{
  "spans": [
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "874a566f544d5232",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680581286965051,
      "end_time": 1767680585999268985,
      "duration": 4.712303934,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}]",
        "llm.input_tokens": 4710,
        "llm.output_tokens": 219,
        "llm.total_tokens": 4929,
        "llm.latency_ms": 4712.312459945679,
        "llm.completion": "REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.\n[Tool Use: Read]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]",
        "llm.reasoning": "REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.",
        "llm.cost_usd": 0.017415
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "ddafa587fe50c700",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680585999623868,
      "end_time": 1767680586000357417,
      "duration": 0.000733549,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/description.md\"}}",
        "tool.output": "{\"content\": \"# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\\\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\\\" target=\\\"_blank\\\">\\\"Join Competition button</a> to create an account and gain access to the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\" target=\\\"_blank\\\">competition data</a>. Then check out <a href=\\\"https://www.kaggle.com/alexisbcook/titanic-tutorial\\\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\\\"699\\\" height=\\\"368\\\" src=\\\"https://www.youtube.com/embed/8yZMXCaFshs\\\" title=\\\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\\\" frameborder=\\\"0\\\" allow=\\\"encrypted-media\\\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\\\"699\\\" align=\\\"center\\\" src=\\\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\\\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\\\"699\\\" align=\\\"center\\\" src=\\\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\\\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\\\"https://www.kaggle.com/c/titanic/notebooks\\\">here</a>.\\n\\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\\\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\\\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\\\"_blank\\\" href=\\\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\\\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\\\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\\\" target=\\\"_blank\\\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\\\"width: 100%;\\\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\\\"https://www.kaggle.com/c/titanic/team\\\" target=\\\"_blank\\\">Team</a> page.</p>\\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\\\"https://www.kaggle.com/c/titanic/notebooks\\\" target=\\\"_blank\\\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\\\"https://www.kaggle.com/learn/overview\\\" target=\\\"&quot;_blank\\\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\\\"I worked so hard to get that score! Give it back!\\\"</i> Read more about our decision to implement a rolling leaderboard <a href=\\\"https://www.kaggle.com/c/titanic/discussion/6240\\\" target=\\\"_blank\\\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\\\"https://www.kaggle.com/contact\\\" target=\\\"_blank\\\">contact us</a>.</p>\", \"is_error\": false}",
        "tool.duration_ms": 0.6506443023681641
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "04eb21a16a6a6d99",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680586000541684,
      "end_time": 1767680586000743093,
      "duration": 0.000201409,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"../research/writeups\"}}",
        "tool.output": "{\"content\": \"Not a directory: /home/code/exploration/../research/writeups\", \"is_error\": true}",
        "tool.duration_ms": 0.26035308837890625
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "19857bef8792113f",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680586000835108,
      "end_time": 1767680586001074166,
      "duration": 0.000239058,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"../research/kernels\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/\", \"is_error\": false}",
        "tool.duration_ms": 0.2505779266357422
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "18c676f90666f555",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680586001172801,
      "end_time": 1767680586001341448,
      "duration": 0.000168647,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)\", \"is_error\": false}",
        "tool.duration_ms": 0.18739700317382812
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "92815f24a6b4e7c8",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680586040322628,
      "end_time": 1767680592008871645,
      "duration": 5.968549017,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}]",
        "llm.input_tokens": 9287,
        "llm.output_tokens": 251,
        "llm.total_tokens": 9538,
        "llm.latency_ms": 5968.568801879883,
        "llm.completion": "REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\n[Tool Use: Read]\n[Tool Use: Read]",
        "llm.reasoning": "REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.",
        "llm.cost_usd": 0.031626
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "fb1e1aa9f85f87f5",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680592009114631,
      "end_time": 1767680592009404795,
      "duration": 0.000290164,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py\"}}",
        "tool.output": "{\"content\": \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\", \"is_error\": true}",
        "tool.duration_ms": 0.33164024353027344
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "3f4b46bf6263dbaa",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680592009519797,
      "end_time": 1767680592009708389,
      "duration": 0.000188592,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py\"}}",
        "tool.output": "{\"content\": \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\", \"is_error\": true}",
        "tool.duration_ms": 0.20813941955566406
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "b0f8e19cc2ef21ea",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680592893274308,
      "end_time": 1767680597222928017,
      "duration": 4.329653709,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}]",
        "llm.input_tokens": 9733,
        "llm.output_tokens": 210,
        "llm.total_tokens": 9943,
        "llm.latency_ms": 4329.676866531372,
        "llm.completion": "REASONING: Let me check the actual file structure in these kernel directories.\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]",
        "llm.reasoning": "REASONING: Let me check the actual file structure in these kernel directories.",
        "llm.cost_usd": 0.032349
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "ff2ae40d11fdeac1",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680597223164359,
      "end_time": 1767680597223454710,
      "duration": 0.000290351,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)\", \"is_error\": false}",
        "tool.duration_ms": 0.33783912658691406
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "66558a5ca0023198",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680597223577247,
      "end_time": 1767680597223754953,
      "duration": 0.000177706,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)\", \"is_error\": false}",
        "tool.duration_ms": 0.21719932556152344
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "565ac49ed2185d64",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767680597223837169,
      "end_time": 1767680597223971215,
      "duration": 0.000134046,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7db40ef9f450>\", \"input\": {\"directory_path\": \"../research/kernels/startupsci_titanic-data-science-solutions\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\", \"is_error\": false}",
        "tool.duration_ms": 0.14662742614746094
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "67d3c002b5a9c236",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680598153970754,
      "end_time": 1767680601148179050,
      "duration": 2.994208296,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}]",
        "llm.input_tokens": 10233,
        "llm.output_tokens": 121,
        "llm.total_tokens": 10354,
        "llm.latency_ms": 2994.258403778076,
        "llm.completion": "REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.\n[Tool Use: Read]",
        "llm.reasoning": "REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.",
        "llm.cost_usd": 0.032514
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "f067d5175b3fc1f8",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680601148401315,
      "end_time": 1767680601149293667,
      "duration": 0.000892352,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\"}}",
        "tool.output": "{\"content\": \"{\\\"cells\\\":[{\\\"source\\\":\\\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\\\\\"statistically significant\\\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ea24845a-4a15-4235-9ed2-4823acbf4317\\\",\\\"_uuid\\\":\\\"89022d89e1603ef3869a96b9da78da38b0b33b14\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch1\\\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ec31d549-537e-4f35-8594-22ef03079081\\\",\\\"_uuid\\\":\\\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch2\\\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\\\"dirty data\\\\\\\" to \\\\\\\"clean data.\\\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\\\"bionic man\\\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\\\" rate.\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"8514a868-6816-494a-9e9c-dba14cb36f42\\\",\\\"_uuid\\\":\\\"bd2a68dcaace0c1371f2994f0204dce65800cc20\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch3\\\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\\\",\\\"_uuid\\\":\\\"ff014109d795c5d1bfcc66c9761730406124ebe4\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch4\\\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ef4059c6-9397-4b72-85da-570f51ba139c\\\",\\\"_uuid\\\":\\\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\\\"}},{\\\"source\\\":\\\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[['PassengerId','Survived']]\\\\nsubmit.to_csv(\\\\\\\"../working/submit.csv\\\\\\\", index=False)\\\\n\\\\nprint('Validation Data Distribution: \\\\\\\\n', data_val['Survived'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\\\",\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"057f369f-a3c6-4718-94f4-db8cb6d59777\\\",\\\"collapsed\\\":true,\\\"_uuid\\\":\\\"29ad86c5413910763316df7fda7753c4b0d0a97d\\\"},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null},{\\\"source\\\":\\\"<a id=\\\\\\\"ch12\\\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\\\",\\\"_uuid\\\":\\\"c476df95925d67809975caca4ea4dcb4b2a0b277\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch90\\\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\\\"super\\\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"973396d8-c37f-4f74-baec-a0c0653c975e\\\",\\\"_uuid\\\":\\\"4c6df5d0a4ecf018846830f1ab4310549977a51d\\\"}},{\\\"source\\\":\\\"<a id=\\\\\\\"ch91\\\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\\\"borrowing\\\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"5678e859-6610-4895-8fe8-0d28f0767b25\\\",\\\"_uuid\\\":\\\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\\\"}},{\\\"source\\\":\\\"\\\\n\\\",\\\"cell_type\\\":\\\"markdown\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"51f197c2-76d7-4503-ade5-f9f35323c4c5\\\",\\\"_uuid\\\":\\\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\\\"}}],\\\"nbformat\\\":4,\\\"nbformat_minor\\\":1,\\\"metadata\\\":{\\\"language_info\\\":{\\\"name\\\":\\\"python\\\",\\\"version\\\":\\\"3.6.3\\\",\\\"codemirror_mode\\\":{\\\"name\\\":\\\"ipython\\\",\\\"version\\\":3},\\\"pygments_lexer\\\":\\\"ipython3\\\",\\\"nbconvert_exporter\\\":\\\"python\\\",\\\"file_extension\\\":\\\".py\\\",\\\"mimetype\\\":\\\"text/x-python\\\"},\\\"kernelspec\\\":{\\\"name\\\":\\\"python3\\\",\\\"language\\\":\\\"python\\\",\\\"display_name\\\":\\\"Python 3\\\"}}}\", \"is_error\": false}",
        "tool.duration_ms": 0.7426738739013672
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "f6e2b1084bc4765a",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680602041360259,
      "end_time": 1767680612139162478,
      "duration": 10.097802219,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}]",
        "llm.input_tokens": 16552,
        "llm.output_tokens": 414,
        "llm.total_tokens": 16966,
        "llm.latency_ms": 10097.86057472229,
        "llm.completion": "REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\n1. Feature engineering: Title extraction from Name, Family size, Age binning\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\n3. Voting classifiers (hard and soft voting)\n4. Grid search for hyperparameter tuning\n5. Best score achieved: ~0.77990 (about 78% accuracy)\n[Tool Use: RecordFinding]\n[Tool Use: Read]",
        "llm.reasoning": "REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:",
        "llm.cost_usd": 0.055866
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "c54ae2fec117a3ae",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767680612139393218,
      "end_time": 1767680612140618796,
      "duration": 0.001225578,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7db40efea250>\", \"input\": {\"finding\": \"Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), Age (children more likely to survive), Family size (FamilySingle, FamilySmall, FamilyLarge), Title extracted from Name (Mr, Mrs, Miss, Master, etc.), Fare, Embarked. Best submission score achieved: ~0.77990 with tuned voting classifier.\", \"source\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.2178421020507812
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "b4744ad6fc74e5e4",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680612140764800,
      "end_time": 1767680612141256643,
      "duration": 0.000491843,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\"}}",
        "tool.output": "{\"content\": \"{\\\"cells\\\":[{\\\"metadata\\\":{\\\"_uuid\\\":\\\"e919d1161f20999e599ba1fd66a5a45b9c82f229\\\",\\\"_cell_guid\\\":\\\"bc64948f-5d6a-078d-085d-1beb58687bd3\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\\\"},{\\\"metadata\\\":{\\\"_execution_state\\\":\\\"idle\\\",\\\"_cell_guid\\\":\\\"14630296-b1aa-759e-bafa-b6a73f3896ed\\\",\\\"_uuid\\\":\\\"2e37a274400cfeb472b6405d524325245588dd66\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings('ignore')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\\\",\\\"execution_count\\\":1,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"8b590aafe06a2ac55daae9d2456155e457914f5f\\\",\\\"_cell_guid\\\":\\\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\\"},{\\\"metadata\\\":{\\\"_execution_state\\\":\\\"idle\\\",\\\"_cell_guid\\\":\\\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\\\",\\\"_uuid\\\":\\\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Load in the train and test datasets\\\\ntrain = pd.read_csv('../input/train.csv')\\\\ntest = pd.read_csv('../input/test.csv')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test['PassengerId']\\\\n\\\\ntrain.head(3)\\\",\\\"execution_count\\\":2,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"81378834770e55c76751347588322fe32acf5737\\\",\\\"_cell_guid\\\":\\\"dff1c1dd-1e59-3907-88fa-1a1d699122be\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\\"},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_execution_state\\\":\\\"idle\\\",\\\"_cell_guid\\\":\\\"e85b2a80-88a4-928f-f2b2-24895dea38f3\\\",\\\"_uuid\\\":\\\"b1c67249f91768ce8e5e2751364d32c87446cf55\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain['Name_length'] = train['Name'].apply(len)\\\\ntest['Name_length'] = test['Name'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain['Has_Cabin'] = train[\\\\\\\"Cabin\\\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest['Has_Cabin'] = test[\\\\\\\"Cabin\\\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset['IsAlone'] = 0\\\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset['Age'].mean()\\\\n    age_std = dataset['Age'].std()\\\\n    age_null_count = dataset['Age'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\\\n    dataset['Age'] = dataset['Age'].astype(int)\\\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(' ([A-Za-z]+)\\\\\\\\.', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\\\"\\\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\\\"Rare\\\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\\\n\\\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\\\"Mr\\\\\\\": 1, \\\\\\\"Miss\\\\\\\": 2, \\\\\\\"Mrs\\\\\\\": 3, \\\\\\\"Master\\\\\\\": 4, \\\\\\\"Rare\\\\\\\": 5}\\\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\\\n    dataset['Title'] = dataset['Title'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\\\",\\\"execution_count\\\":3,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_execution_state\\\":\\\"idle\\\",\\\"_cell_guid\\\":\\\"8fc645ba-4b38-cea7-17b3-02175cb103d9\\\",\\\"_uuid\\\":\\\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Feature selection\\\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\\\",\\\"execution_count\\\":4,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"dd288776321804d99e4e4a7e88594c1d631e4409\\\",\\\"_cell_guid\\\":\\\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \\\"},{\\\"metadata\\\":{\\\"_execution_state\\\":\\\"idle\\\",\\\"_cell_guid\\\":\\\"fc426b8f-873d-6f23-4299-99f174956cca\\\",\\\"_uuid\\\":\\\"1f280a1c11dc35a93b57af494938998e6d0b4544\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"train.head(3)\\\",\\\"execution_count\\\":5,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\\\",\\\"_cell_guid\\\":\\\"41102927-8218-415a-0b73-5129c8f5dd0c\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest',\\\\n#     xaxis= dict(\\\\n#         title= 'Pop',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= 'Feature Importance',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename='scatter2010')\\\",\\\"execution_count\\\":17,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\\\",\\\"_cell_guid\\\":\\\"553828f0-c994-5ee1-695f-9373f11a1a7b\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"f611812e2c9de3773df2264dfb2b13c0995807ac\\\",\\\"_cell_guid\\\":\\\"06847850-a829-0858-b12c-7b66e53e030a\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\\\",\\\"execution_count\\\":18,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1f0aff0896fef90b326fff7816393fef0e0cb992\\\",\\\"_cell_guid\\\":\\\"5645e647-c517-7822-f881-b8d7e38ef5da\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"0bd069388b419fe45306c01825aa3e6f5466ba2b\\\",\\\"_cell_guid\\\":\\\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"y = feature_dataframe['mean'].values\\\\nx = feature_dataframe['features'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe['mean'].values,\\\\n            colorscale='Portland',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= 'Barplots of Mean Feature Importance',\\\\n    hovermode= 'closest',\\\\n#     xaxis= dict(\\\\n#         title= 'Pop',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= 'Feature Importance',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename='bar-direct-labels')\\\",\\\"execution_count\\\":19,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"bbb76d189e8d03921caaacfa9545cef894348c7d\\\",\\\"_cell_guid\\\":\\\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Second-Level Predictions from the First-level Output\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"fed132782b73dda8d265065867e7f57c0aed7f50\\\",\\\"_cell_guid\\\":\\\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"a5945e93337b87a1a8ee5580856768bbb14c07cd\\\",\\\"_cell_guid\\\":\\\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\\\n     'ExtraTrees': et_oof_train.ravel(),\\\\n     'AdaBoost': ada_oof_train.ravel(),\\\\n      'GradientBoost': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\\\",\\\"execution_count\\\":20,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\\\",\\\"_cell_guid\\\":\\\"f69c11db-d84e-8536-4c7e-382fbe67483e\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Correlation Heatmap of the Second Level Training set**\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\\\",\\\"_cell_guid\\\":\\\"4cf590ee-133f-6487-cf5a-53f346893d1c\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale='Viridis',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename='labelled-heatmap')\\\",\\\"execution_count\\\":21,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"4d9ef0298b568e43da6925f385403e0d77bd6e33\\\",\\\"_cell_guid\\\":\\\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\\"},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_uuid\\\":\\\"fef365199854ca3fff754399b4699d941b7e43b8\\\",\\\"_cell_guid\\\":\\\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\\",\\\"execution_count\\\":22,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\\\",\\\"_cell_guid\\\":\\\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\\\",\\\"_cell_guid\\\":\\\"628a03ea-933c-7075-a589-0ff7af237dfd\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\\"},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_uuid\\\":\\\"5155d370069fe6de0fe5105309342ce55130dae8\\\",\\\"_cell_guid\\\":\\\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= 'binary:logistic',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\\\",\\\"execution_count\\\":23,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"0101e6b843f6378838874ccfb844ed464b81d627\\\",\\\"_cell_guid\\\":\\\"0a8152d8-6842-ed00-6bc5-47a511adce1c\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"52ac0cd99cee0099d86a180127da42ff7fff960a\\\",\\\"_cell_guid\\\":\\\"6b4a5c81-e968-d41e-27e4-871481019867\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\\"},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_uuid\\\":\\\"9d607d829dbadd6c72ee01c9735a642435eb53e6\\\",\\\"_cell_guid\\\":\\\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\\\n                            'Survived': predictions })\\\\nStackingSubmission.to_csv(\\\\\\\"StackingSubmission.csv\\\\\\\", index=False)\\\",\\\"execution_count\\\":24,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\\\",\\\"_cell_guid\\\":\\\"1e56c738-b8f3-95e4-d642-c483f9757ed8\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one's score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\\\",\\\"_cell_guid\\\":\\\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\\"},{\\\"metadata\\\":{\\\"collapsed\\\":true,\\\"_uuid\\\":\\\"8549710f41f2ca55f11400e7914ed2f15c380964\\\",\\\"_cell_guid\\\":\\\"74641376-e124-e46f-0e7c-8cbee2a1321b\\\",\\\"trusted\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]}],\\\"metadata\\\":{\\\"kernelspec\\\":{\\\"display_name\\\":\\\"Python 3\\\",\\\"language\\\":\\\"python\\\",\\\"name\\\":\\\"python3\\\"},\\\"_change_revision\\\":0,\\\"_is_fork\\\":false,\\\"language_info\\\":{\\\"name\\\":\\\"python\\\",\\\"version\\\":\\\"3.6.5\\\",\\\"mimetype\\\":\\\"text/x-python\\\",\\\"codemirror_mode\\\":{\\\"name\\\":\\\"ipython\\\",\\\"version\\\":3},\\\"pygments_lexer\\\":\\\"ipython3\\\",\\\"nbconvert_exporter\\\":\\\"python\\\",\\\"file_extension\\\":\\\".py\\\"}},\\\"nbformat\\\":4,\\\"nbformat_minor\\\":1}\", \"is_error\": false}",
        "tool.duration_ms": 0.33736228942871094
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "74d630c29346160a",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680613066074283,
      "end_time": 1767680623784057599,
      "duration": 10.717983316,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}]",
        "llm.input_tokens": 24022,
        "llm.output_tokens": 453,
        "llm.total_tokens": 24475,
        "llm.latency_ms": 10718.005180358887,
        "llm.completion": "REASONING: This is an excellent stacking notebook. Key techniques:\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\n3. Out-of-fold predictions for stacking\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\n5. Age/Fare binning\n6. Public LB score: 0.808 (80.8% accuracy)\n\nLet me record this and also check the data quickly to understand the structure.\n[Tool Use: RecordFinding]\n[Tool Use: Read]",
        "llm.reasoning": "REASONING: This is an excellent stacking notebook. Key techniques:",
        "llm.cost_usd": 0.07886099999999999
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "859a786c227a1d8e",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767680623784332225,
      "end_time": 1767680623785643254,
      "duration": 0.001311029,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7db40efea250>\", \"input\": {\"finding\": \"Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to generate out-of-fold predictions, then use XGBoost as second-level meta-learner. Key: uncorrelated base models produce better ensemble results. Public LB score: 0.808 (80.8% accuracy). Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction, Age/Fare binning.\", \"source\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.3427734375
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "69b2d30c4131b087",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767680623785805209,
      "end_time": 1767680623786304591,
      "duration": 0.000499382,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7db40ef9f390>\", \"input\": {\"file_path\": \"../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\"}}",
        "tool.output": "{\"content\": \"{\\\"cells\\\":[{\\\"metadata\\\":{\\\"_uuid\\\":\\\"fed5696c67bf55a553d6d04313a77e8c617cad99\\\",\\\"_cell_guid\\\":\\\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\\\",\\\"_cell_guid\\\":\\\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\\\",\\\"_cell_guid\\\":\\\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\\\",\\\"_cell_guid\\\":\\\"6b5dc743-15b1-aac6-405e-081def6ecca1\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"13f38775c12ad6f914254a08f0d1ef948a2bd453\\\",\\\"_cell_guid\\\":\\\"e7319668-86fe-8adc-438d-0eef3fd0a982\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"train_df = pd.read_csv('../input/train.csv')\\\\ntest_df = pd.read_csv('../input/test.csv')\\\\ncombine = [train_df, test_df]\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"79282222056237a52bbbb1dbd831f057f1c23d69\\\",\\\"_cell_guid\\\":\\\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"ef106f38a00e162a80c523778af6dcc778ccc1c2\\\",\\\"_cell_guid\\\":\\\"ce473d29-8d19-76b8-24a4-48c217286e42\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"print(train_df.columns.values)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1d7acf42af29a63bc038f14eded24e8b8146f541\\\",\\\"_cell_guid\\\":\\\"cd19a6f6-347f-be19-607b-dca950590b37\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\\\",\\\"_cell_guid\\\":\\\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# preview the data\\\\ntrain_df.head()\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c34fa51a38336d97d5f6a184908cca37daebd584\\\",\\\"_cell_guid\\\":\\\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\\\",\\\"_cell_guid\\\":\\\"7a63bf04-a410-9c81-5310-bdef7963298f\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\\\",\\\"_cell_guid\\\":\\\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"54d86cd45703d459d452f89572771deaa8877999\\\",\\\"_cell_guid\\\":\\\"ca14ae53-f05e-eb73-201c-064d7c3ed610\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1535f18113f851e480cd53e0c612dc05835690f3\\\",\\\"_cell_guid\\\":\\\"810f723d-2313-8dfd-e3e2-26673b9caa90\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"723c835c29e8727bc9bad4b564731f2ca98025d0\\\",\\\"_cell_guid\\\":\\\"50378071-7043-ed8d-a782-70c947520dae\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\\\",\\\"_cell_guid\\\":\\\"1e286e19-b714-385a-fcfa-8cf5ec19956a\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\\\",\\\"_cell_guid\\\":\\\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"52ea4f44dd626448dd2199cb284b592670b1394b\\\",\\\"_cell_guid\\\":\\\"a4d56857-9432-55bb-14c0-52ebeb64d198\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"3a016c1f24da59c85648204302d61ea15920e740\\\",\\\"_cell_guid\\\":\\\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\\\",\\\"_cell_guid\\\":\\\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1f94308b23b934123c03067e84027b507b989e52\\\",\\\"_cell_guid\\\":\\\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"24f4e46f202a858076be91752170cad52aa9aefa\\\",\\\"_cell_guid\\\":\\\"85693668-0cd5-4319-7768-eddb62d2b7d0\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"483c647d2759a2703d20785a44f51b6dee47d0db\\\",\\\"_cell_guid\\\":\\\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2c1428d022430ea594af983a433757e11b47c50c\\\",\\\"_cell_guid\\\":\\\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"06a52babe50e0dd837b553c78fc73872168e1c7d\\\",\\\"_cell_guid\\\":\\\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"models = pd.DataFrame({\\\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\\\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\\\n              'Stochastic Gradient Decent', 'Linear SVC', \\\\n              'Decision Tree'],\\\\n    'Score': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by='Score', ascending=False)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\\\",\\\"_cell_guid\\\":\\\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"submission = pd.DataFrame({\\\\n        \\\\\\\"PassengerId\\\\\\\": test_df[\\\\\\\"PassengerId\\\\\\\"],\\\\n        \\\\\\\"Survived\\\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv('../output/submission.csv', index=False)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\\\",\\\"_cell_guid\\\":\\\"fcfc8d9f-e955-cf70-5843-1fb764c54699\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\\\",\\\"_cell_guid\\\":\\\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\\\"}],\\\"metadata\\\":{\\\"_is_fork\\\":false,\\\"language_info\\\":{\\\"name\\\":\\\"python\\\",\\\"version\\\":\\\"3.6.6\\\",\\\"mimetype\\\":\\\"text/x-python\\\",\\\"codemirror_mode\\\":{\\\"name\\\":\\\"ipython\\\",\\\"version\\\":3},\\\"pygments_lexer\\\":\\\"ipython3\\\",\\\"nbconvert_exporter\\\":\\\"python\\\",\\\"file_extension\\\":\\\".py\\\"},\\\"_change_revision\\\":0,\\\"kernelspec\\\":{\\\"display_name\\\":\\\"Python 3\\\",\\\"language\\\":\\\"python\\\",\\\"name\\\":\\\"python3\\\"}},\\\"nbformat\\\":4,\\\"nbformat_minor\\\":1}\", \"is_error\": false}",
        "tool.duration_ms": 0.3757476806640625
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "130ceb2999851088",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680625603168646,
      "end_time": 1767680635118392493,
      "duration": 9.515223847,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}]",
        "llm.input_tokens": 30746,
        "llm.output_tokens": 295,
        "llm.total_tokens": 31041,
        "llm.latency_ms": 9515.2268409729,
        "llm.completion": "REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.\n[Tool Use: RecordFinding]",
        "llm.reasoning": "REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.",
        "llm.cost_usd": 0.096663
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "a8585b144f3addfc",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767680635118812329,
      "end_time": 1767680635120655775,
      "duration": 0.001843446,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7db40efea250>\", \"input\": {\"finding\": \"Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Rare), 2) Age imputation using median by Pclass/Sex, 3) Age bands (5 bins), 4) Fare bands (4 bins), 5) FamilySize = SibSp + Parch + 1, 6) IsAlone flag, 7) Drop Ticket, Cabin, Name after feature extraction. Models tested: Random Forest best on training (86.76%), followed by Decision Tree, SVM. Key correlations: Sex (female=0.74 survival), Pclass (1st=0.63), Embarked (C=0.55).\", \"source\": \"../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.8618106842041016
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "5e39af005c7e9333",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680636032996882,
      "end_time": 1767680642581758773,
      "duration": 6.548761891,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}]",
        "llm.input_tokens": 31119,
        "llm.output_tokens": 279,
        "llm.total_tokens": 31398,
        "llm.latency_ms": 6548.750162124634,
        "llm.completion": "REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.\n[Tool Use: CreateNotebook]",
        "llm.reasoning": "REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.",
        "llm.cost_usd": 0.09754199999999999
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "012ce6acd9a88c83",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767680642582124286,
      "end_time": 1767680644034166555,
      "duration": 1.452042269,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7db40ef9f410>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint(f'Train shape: {train.shape}')\\nprint(f'Test shape: {test.shape}')\\nprint(f'\\\\nTrain columns: {train.columns.tolist()}')\\nprint(f'\\\\nTarget distribution:\\\\n{train[\\\"Survived\\\"].value_counts(normalize=True)}')\\ntrain.head()\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 1452.071189880371
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "8c653eebc2df06ba",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680645908844426,
      "end_time": 1767680649435548784,
      "duration": 3.526704358,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}]",
        "llm.input_tokens": 31444,
        "llm.output_tokens": 92,
        "llm.total_tokens": 31536,
        "llm.latency_ms": 3526.819944381714,
        "llm.completion": "[Tool Use: ExecuteCellRange]",
        "llm.cost_usd": 0.095712
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "add188156a722069",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767680649435836445,
      "end_time": 1767680650135268326,
      "duration": 0.699431881,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7db40ef9f4d0>\", \"input\": {\"start_cell\": 0, \"end_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint(f'Train shape: {train.shape}')\\nprint(f'Test shape: {test.shape}')\\nprint(f'\\\\nTrain columns: {train.columns.tolist()}')\\nprint(f'\\\\nTarget distribution:\\\\n{train[\\\"Survived\\\"].value_counts(normalize=True)}')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \", \"is_error\": false}",
        "tool.duration_ms": 699.4214057922363
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "600f5cec8ef37214",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680651109486599,
      "end_time": 1767680656008023672,
      "duration": 4.898537073,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}]",
        "llm.input_tokens": 32128,
        "llm.output_tokens": 177,
        "llm.total_tokens": 32305,
        "llm.latency_ms": 4898.573875427246,
        "llm.completion": "[Tool Use: AddNotebookCell]",
        "llm.cost_usd": 0.099039
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "bd740d053f6e169e",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767680656008288251,
      "end_time": 1767680656010887160,
      "duration": 0.002598909,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7db40efdedd0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Check missing values and data types\\nprint(\\\"Missing values:\\\")\\nprint(train.isnull().sum())\\nprint(f\\\"\\\\nData types:\\\\n{train.dtypes}\\\")\\nprint(f\\\"\\\\n\\\\nSurvival by Sex:\\\")\\nprint(train.groupby('Sex')['Survived'].mean())\\nprint(f\\\"\\\\nSurvival by Pclass:\\\")\\nprint(train.groupby('Pclass')['Survived'].mean())\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 2.620220184326172
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "f8734b3f8213b8d1",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680657030045804,
      "end_time": 1767680660644875301,
      "duration": 3.614829497,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \ntool: Added code cell at 1",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}]",
        "llm.input_tokens": 32323,
        "llm.output_tokens": 92,
        "llm.total_tokens": 32415,
        "llm.latency_ms": 3614.8293018341064,
        "llm.completion": "[Tool Use: ExecuteCellRange]",
        "llm.cost_usd": 0.098349
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "eba9b72667c545f3",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767680660645194229,
      "end_time": 1767680660667996182,
      "duration": 0.022801953,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7db40ef9f4d0>\", \"input\": {\"start_cell\": 1, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Check missing values and data types\\nprint(\\\"Missing values:\\\")\\nprint(train.isnull().sum())\\nprint(f\\\"\\\\nData types:\\\\n{train.dtypes}\\\")\\nprint(f\\\"\\\\n\\\\nSurvival by Sex:\\\")\\nprint(train.groupby('Sex')['Survived'].mean())\\nprint(f\\\"\\\\nSurvival by Pclass:\\\")\\nprint(train.groupby('Pclass')['Survived'].mean())\\n```\\nOutput:\\nMissing values:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nData types:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\", \"is_error\": false}",
        "tool.duration_ms": 22.798776626586914
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "ce401a45d340ee20",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680661669405658,
      "end_time": 1767680671569897096,
      "duration": 9.900491438,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \ntool: Added code cell at 1\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Check missing values and data types\nprint(\"Missing values:\")\nprint(train.isnull().sum())\nprint(f\"\\nData types:\\n{train.dtypes}\")\nprint(f\"\\n\\nSurvival by Sex:\")\nprint(train.groupby('Sex')['Survived'].mean())\nprint(f\"\\nSurvival by Pclass:\")\nprint(train.groupby('Pclass')['Survived'].mean())\n```\nOutput:\nMissing values:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nData types:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Check missing values and data types\\nprint(\"Missing values:\")\\nprint(train.isnull().sum())\\nprint(f\"\\\\nData types:\\\\n{train.dtypes}\")\\nprint(f\"\\\\n\\\\nSurvival by Sex:\")\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\nprint(f\"\\\\nSurvival by Pclass:\")\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n```\\nOutput:\\nMissing values:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nData types:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n'}]",
        "llm.input_tokens": 32780,
        "llm.output_tokens": 365,
        "llm.total_tokens": 33145,
        "llm.latency_ms": 9900.493621826172,
        "llm.completion": "REASONING: Now I have a good understanding of the data:\n- 891 training samples, 418 test samples\n- Binary classification (Survived: 0 or 1)\n- Target is imbalanced: 61.6% died, 38.4% survived\n- Key features: Sex (female 74% survival vs male 19%), Pclass (1st class 63% vs 3rd class 24%)\n- Missing values: Age (177), Cabin (687), Embarked (2)\n- Features: Name (can extract Title), Ticket, Fare, SibSp, Parch (family size)\n\nThe target score is 1.0 (100% accuracy), which is extremely high. Let me now write the seed prompt based on all the research.\n[Tool Use: RecordFinding]",
        "llm.reasoning": "REASONING: Now I have a good understanding of the data:",
        "llm.cost_usd": 0.103815
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "a2f87bcb0120c2ba",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767680671570139015,
      "end_time": 1767680671571386178,
      "duration": 0.001247163,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7db40efea250>\", \"input\": {\"finding\": \"Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target imbalanced: 61.6% died, 38.4% survived. Missing values: Age (177/891=20%), Cabin (687/891=77%), Embarked (2). Key survival patterns: Female 74.2% vs Male 18.9%, Pclass 1: 63%, Pclass 2: 47%, Pclass 3: 24%.\", \"source\": \"exploration/eda.ipynb\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\\n  Source: exploration/eda.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.2707710266113281
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "f5d474150683b8e3",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680672601823523,
      "end_time": 1767680704466539865,
      "duration": 31.864716342,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \ntool: Added code cell at 1\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Check missing values and data types\nprint(\"Missing values:\")\nprint(train.isnull().sum())\nprint(f\"\\nData types:\\n{train.dtypes}\")\nprint(f\"\\n\\nSurvival by Sex:\")\nprint(train.groupby('Sex')['Survived'].mean())\nprint(f\"\\nSurvival by Pclass:\")\nprint(train.groupby('Pclass')['Survived'].mean())\n```\nOutput:\nMissing values:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nData types:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\ntool: ‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\n  Source: exploration/eda.ipynb",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Check missing values and data types\\nprint(\"Missing values:\")\\nprint(train.isnull().sum())\\nprint(f\"\\\\nData types:\\\\n{train.dtypes}\")\\nprint(f\"\\\\n\\\\nSurvival by Sex:\")\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\nprint(f\"\\\\nSurvival by Pclass:\")\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n```\\nOutput:\\nMissing values:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nData types:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data:\\n- 891 training samples, 418 test samples\\n- Binary classification (Survived: 0 or 1)\\n- Target is imbalanced: 61.6% died, 38.4% survived\\n- Key features: Sex (female 74% survival vs male 19%), Pclass (1st class 63% vs 3rd class 24%)\\n- Missing values: Age (177), Cabin (687), Embarked (2)\\n- Features: Name (can extract Title), Ticket, Fare, SibSp, Parch (family size)\\n\\nThe target score is 1.0 (100% accuracy), which is extremely high. Let me now write the seed prompt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\\n  Source: exploration/eda.ipynb'}]",
        "llm.input_tokens": 33198,
        "llm.output_tokens": 1577,
        "llm.total_tokens": 34775,
        "llm.latency_ms": 31864.72249031067,
        "llm.completion": "[Tool Use: Write]",
        "llm.cost_usd": 0.123249
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "291d8ae9fefd5d11",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "WriteTool",
      "kind": "INTERNAL",
      "start_time": 1767680704466954936,
      "end_time": 1767680704467772685,
      "duration": 0.000817749,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WriteTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.WriteTool object at 0x7db40ef9f1d0>\", \"input\": {\"file_path\": \"seed_prompt.txt\", \"content\": \"# Titanic Survival Prediction - Techniques Guide\\n\\n## Data Understanding\\n**Reference notebooks for data characteristics:**\\n- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution\\n- Key findings: 891 train samples, 418 test samples, binary classification (Survived 0/1)\\n- Target imbalanced: 61.6% died, 38.4% survived\\n- Missing values: Age (20%), Cabin (77%), Embarked (2 samples)\\n\\n## Feature Engineering (Critical for High Accuracy)\\n\\n### Title Extraction from Name\\nExtract titles from passenger names using regex pattern ` ([A-Za-z]+)\\\\.`:\\n- Common titles: Mr, Mrs, Miss, Master\\n- Rare titles (group together): Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona\\n- Map: Mlle ‚Üí Miss, Ms ‚Üí Miss, Mme ‚Üí Mrs\\n- Title is highly predictive: Master (children) and Miss/Mrs (women) have higher survival\\n\\n### Family Features\\n- **FamilySize** = SibSp + Parch + 1\\n- **IsAlone** = 1 if FamilySize == 1, else 0\\n- Family size categories: Single (1), Small (2-4), Large (5+) have different survival rates\\n- Medium-sized families had better survival (could help each other but not too many to manage)\\n\\n### Cabin Features\\n- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class passengers)\\n- Cabin deck letter (first character) can indicate location on ship\\n\\n### Age Handling\\n- Fill missing Age using median by Pclass and Sex groups (more accurate than global median)\\n- Alternative: Use random values within (mean - std, mean + std)\\n- Create **AgeBand** with 5 bins: 0-16, 16-32, 32-48, 48-64, 64+\\n- Children (Age <= 16) had higher survival priority\\n\\n### Fare Handling\\n- Fill missing Fare with median\\n- Create **FareBand** with 4 quantile bins\\n- Higher fare correlates with higher class and better survival\\n\\n### Name Length\\n- **Name_length** = len(Name) - longer names may indicate nobility/higher status\\n\\n## Models to Use\\n\\n### Ensemble Methods (Best Performance)\\nBased on top kernels achieving 0.77-0.81 accuracy:\\n\\n1. **Stacking (Recommended)**\\n   - First level: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n   - Second level: XGBoost as meta-learner\\n   - Use out-of-fold predictions to avoid leakage\\n   - Key: Use uncorrelated base models for better ensemble\\n\\n2. **Voting Classifier**\\n   - Hard voting: Majority vote from multiple classifiers\\n   - Soft voting: Average predicted probabilities\\n   - Combine: LogisticRegression, SVC, RandomForest, GradientBoosting, XGBoost\\n\\n3. **Gradient Boosting Models**\\n   - XGBoost: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8\\n   - LightGBM: Fast training, good for small datasets\\n   - CatBoost: Handles categorical features natively\\n\\n### Individual Models (Baseline)\\n- RandomForestClassifier: n_estimators=100, good baseline\\n- GradientBoostingClassifier: Often best single model\\n- SVC: Works well with proper scaling\\n- LogisticRegression: Good interpretability\\n\\n## Preprocessing\\n\\n### Feature Encoding\\n- Sex: female=0, male=1 (or vice versa)\\n- Embarked: S=0, C=1, Q=2 (or one-hot encoding)\\n- Title: Numerical mapping (Mr=1, Miss=2, Mrs=3, Master=4, Rare=5)\\n\\n### Features to Drop\\n- PassengerId (identifier only)\\n- Name (after extracting Title)\\n- Ticket (complex alphanumeric, hard to use)\\n- Cabin (after extracting Has_Cabin and deck)\\n\\n### Final Feature Set (Recommended)\\n- Pclass, Sex, Age (binned), Fare (binned), Embarked\\n- Title, FamilySize, IsAlone, Has_Cabin\\n\\n## Validation Strategy\\n\\n### Cross-Validation\\n- Use StratifiedKFold with k=5 or k=10\\n- Stratification important due to class imbalance\\n- CV score may differ from LB score due to train/test distribution differences\\n\\n### Hyperparameter Tuning\\n- GridSearchCV or RandomizedSearchCV\\n- Focus on: n_estimators, max_depth, learning_rate, min_samples_split\\n\\n## Key Insights from Top Kernels\\n\\n1. **Feature engineering matters more than model selection** for this dataset\\n2. **Simple models with good features** can match complex ensembles\\n3. **Title extraction** is one of the most important engineered features\\n4. **Family size** features capture important survival dynamics\\n5. **Stacking with diverse base models** achieves best results (~0.808 LB)\\n6. **Hard voting classifier** with tuned models achieves ~0.78 LB\\n\\n## Achieving High Accuracy\\n\\nTo beat the target score of 1.0 (100% accuracy):\\n- This is an extremely challenging target as even top solutions achieve ~0.80-0.82\\n- Focus on:\\n  1. Comprehensive feature engineering (all features mentioned above)\\n  2. Multiple model ensemble (stacking preferred)\\n  3. Careful hyperparameter tuning\\n  4. Cross-validation to avoid overfitting\\n  5. Consider pseudo-labeling with high-confidence predictions\\n  6. Try different random seeds and ensemble predictions\\n\\n## Implementation Priority\\n\\n1. **First**: Implement all feature engineering (Title, FamilySize, IsAlone, Has_Cabin, Age/Fare bins)\\n2. **Second**: Train multiple diverse models (RF, XGB, LGB, CatBoost, SVC)\\n3. **Third**: Create stacking ensemble with XGBoost meta-learner\\n4. **Fourth**: Tune hyperparameters with cross-validation\\n5. **Fifth**: Try voting ensemble as alternative\\n\"}}",
        "tool.output": "{\"content\": \"Wrote to /home/code/exploration/seed_prompt.txt\", \"is_error\": false}",
        "tool.duration_ms": 0.86212158203125
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "08b7166e99759d2e",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680705584724027,
      "end_time": 1767680710070623584,
      "duration": 4.485899557,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \ntool: Added code cell at 1\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Check missing values and data types\nprint(\"Missing values:\")\nprint(train.isnull().sum())\nprint(f\"\\nData types:\\n{train.dtypes}\")\nprint(f\"\\n\\nSurvival by Sex:\")\nprint(train.groupby('Sex')['Survived'].mean())\nprint(f\"\\nSurvival by Pclass:\")\nprint(train.groupby('Pclass')['Survived'].mean())\n```\nOutput:\nMissing values:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nData types:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\ntool: ‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\n  Source: exploration/eda.ipynb\ntool: Wrote to /home/code/exploration/seed_prompt.txt",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Check missing values and data types\\nprint(\"Missing values:\")\\nprint(train.isnull().sum())\\nprint(f\"\\\\nData types:\\\\n{train.dtypes}\")\\nprint(f\"\\\\n\\\\nSurvival by Sex:\")\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\nprint(f\"\\\\nSurvival by Pclass:\")\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n```\\nOutput:\\nMissing values:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nData types:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data:\\n- 891 training samples, 418 test samples\\n- Binary classification (Survived: 0 or 1)\\n- Target is imbalanced: 61.6% died, 38.4% survived\\n- Key features: Sex (female 74% survival vs male 19%), Pclass (1st class 63% vs 3rd class 24%)\\n- Missing values: Age (177), Cabin (687), Embarked (2)\\n- Features: Name (can extract Title), Ticket, Fare, SibSp, Parch (family size)\\n\\nThe target score is 1.0 (100% accuracy), which is extremely high. Let me now write the seed prompt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\\n  Source: exploration/eda.ipynb'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Wrote to /home/code/exploration/seed_prompt.txt'}]",
        "llm.input_tokens": 34802,
        "llm.output_tokens": 114,
        "llm.total_tokens": 34916,
        "llm.latency_ms": 4485.900640487671,
        "llm.completion": "REASONING: I've written a comprehensive seed prompt. Let me also check if there are any additional kernels or techniques I might have missed, and search for any advanced techniques for achieving very high accuracy on this problem.\n[Tool Use: WebSearch]",
        "llm.reasoning": "REASONING: I've written a comprehensive seed prompt. Let me also check if there are any additional kernels or techniques I might have missed, and search for any advanced techniques for achieving very high accuracy on this problem.",
        "llm.cost_usd": 0.106116
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "5ac63f2a0264cf7d",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767680710070893447,
      "end_time": 1767680714390942623,
      "duration": 4.320049176,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7db40efe84d0>\", \"input\": {\"query\": \"What techniques achieve highest accuracy on Titanic Kaggle competition above 80%?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nTop‚Äëscoring Titanic solutions combine extensive feature engineering (e.g., extracting titles, family size, cabin presence, age bins and imputing missing ages), aggressive hyper‚Äëparameter tuning (grid/random/Bayesian search) and ensemble models such as soft‚Äëvoting or stacking of logistic regression, Random‚ÄØForest and XGBoost, which together have achieved 81‚Äë84‚ÄØ% accuracy on the test set ([github.com](https://github.com/NickKaparinos/Kaggle-Titanic), [atlantis-press.com](https://www.atlantis-press.com/article/126015317.pdf), [python.plainenglish.io](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).\\n\\n--- Result 2 ---\\nTitle: GitHub - NickKaparinos/Kaggle-Titanic: Kaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\nURL: https://github.com/NickKaparinos/Kaggle-Titanic\\n\\nContent:\\n[Skip to content](https://github.com/NickKaparinos/Kaggle-Titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[NickKaparinos](https://github.com/NickKaparinos)/ **[Kaggle-Titanic](https://github.com/NickKaparinos/Kaggle-Titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n- [Star\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE)\\n\\n[0\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers) [0\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks) [Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags) [Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n\\n# NickKaparinos/Kaggle-Titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[10 Commits](https://github.com/NickKaparinos/Kaggle-Titanic/commits/master/) |\\n| [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) | [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) |  |  |\\n| [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) | [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) |  |  |\\n| [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) | [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) |  |  |\\n| [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) | [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) |  |  |\\n| [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) | [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) |  |  |\\n| [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) | [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) |  |  |\\n| [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) | [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) |  |  |\\n| [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) | [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle-Titanic\\n\\n## Machine learning classification competition\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/titanic.jpg)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/titanic.jpg)\\n\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Cross validation results\\n\\nAfter applying the appropriate preprocessing, each model was evaluated using 10-fold cross validation. Each model\\\\`s hyperparameters were tuned using extensive grid search. Afterwards, tuned models can be ensembled together for a boost in accuracy.\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/cv.png)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/cv.png)\\n\\n## Test set results\\n\\nUsing the optimal voting model, a test set accuracy of **0.8110** was achieved, which corresponds to position **349/50092** in the leaderboard (top 0.6%).\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results1.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results1.PNG)\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results2.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results2.PNG)\\n\\n## About\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### Topics\\n\\n[machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle) [classification](https://github.com/topics/classification)\\n\\n### Resources\\n\\n[Readme](https://github.com/NickKaparinos/Kaggle-Titanic#readme-ov-file)\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic#MIT-1-ov-file)\\n\\n[Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n### Stars\\n\\n[**0**\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers)\\n\\n### Watchers\\n\\n[**1**\\\\\\nwatching](https://github.com/NickKaparinos/Kaggle-Titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks)\\n\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNickKaparinos%2FKaggle-Titanic&report=NickKaparinos+%28user%29)\\n\\n## [Releases](https://github.com/NickKaparinos/Kaggle-Titanic/releases)\\n\\nNo releases published\\n\\n## [Packages\\\\ 0](https://github.com/users/NickKaparinos/packages?repo_name=Kaggle-Titanic)\\n\\nNo packages published\\n\\n## Languages\\n\\n- [Python100.0%](https://github.com/NickKaparinos/Kaggle-Titanic/search?l=python)\\n\\nYou can‚Äôt perform that action at this time.\\n\\n--- Result 3 ---\\nURL: https://www.atlantis-press.com/article/126015317.pdf\\n\\nContent:\\nTitanic Survival Prediction Enhanced by Innovative \\nFeature Engineering and Multi-Model Ensemble \\nOptimization \\nHanzhi Li \\n1 School of Software Engineering, South China University of Technology, Guangzhou, \\nGuangdong, 510006, China \\n202230321188@mail.scut.edu.cn\\nAbstract. This study enhances Titanic survival prediction through advanced \\nfeature engineering and ensemble model optimization. The Titanic dataset \\npresents a classic binary classification problem requiring the prediction of \\npassenger survival based on demographic and ticket information. Our \\nmethodology employs systematic preprocessing where missing values are \\nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \\npredictions for age values. Novel features were extracted from passenger \\nnames, cabin information, and family relationships to improve predictive \\npower. Feature importance was evaluated using Random Forest and XGBoost \\nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \\nfor effective feature selection. Three classification algorithms‚Äîlogistic \\nregression, random forest, and XGBoost‚Äîwere systematically optimized using \\ngrid search, random search, and Bayesian optimization techniques. Based on \\nthese optimized models, soft voting and stacking ensemble approaches were \\nimplemented. Cross-validation results demonstrate that ensemble methods \\nachieved superior accuracy (0.8361) compared to individual models: logistic \\nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \\nresearch provides valuable optimization strategies for similar classification \\ntasks, particularly highlighting how feature engineering combined with \\nensemble methods can substantially enhance predictive performance while \\nmaintaining computational efficiency. \\nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \\nLearning, Hyperparameter Tuning. \\n1 Introduction \\nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \\nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \\ntwentieth century. Its historical significance and the multidimensional nature of \\npassenger data (including social class, gender, age, family relationships, etc.) have \\nestablished it as a classic binary classification benchmark dataset in machine learning. \\n¬© The Author(s) 2025\\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\\nhttps://doi.org/10.2991/978-94-6463-823-3_19\\n208 H. Li\\nDespite extensive research on this problem, traditional methods exhibit \\nsignificant limitations. Early studies predominantly focused on the direct application \\nof basic features, achieving prediction accuracy generally below 80% [1]. For \\ninstance, simple rules based on gender and passenger class could rapidly distinguish \\nsurvival groups. However, these methods failed to fully explore nonlinear interactions \\nbetween features and higher-order features. Moreover, approximately 20% of the age \\nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \\napproaches often rely on mean imputation or directly delete missing values, leading to \\ninformation loss and model bias [2]. \\nAccording to previous research, traditional classification methods applied to the \\nTitanic dataset achieved limited accuracy. These methods were constrained by their \\ninability to capture complex feature interactions and their sensitivity to missing values \\n[2]. Simultaneously, traditional methods predominantly depend on single models such \\nas logistic regression and decision trees without conducting feature importance \\nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \\nensemble models but have not systematically compared their performance with single \\nmodels, nor explored how different hyperparameter tuning methods influence \\nprediction model performance in this dataset. \\nTo address these challenges, this study employs advanced feature engineering \\ntechniques to extract new features, utilizes the relationships between features and \\neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \\nselection techniques like random forests and XGBoost to identify key features for \\nsurvival prediction, and performs hyperparameter tuning of logistic regression, \\nrandom forests, and XGBoost models using grid search, random search, and Bayesian \\noptimization. Based on these methods, soft voting and stacking ensemble models are \\ntrained, and the performance of models under various parameter conditions is further \\ncompared. \\n2 Methods \\n2.1 DataSet \\nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \\ndataset consists of two CSV files: the training set and the test set. The training set \\ncontains 891 samples, while the test set contains 418 samples. The data features \\ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \\nEmbarked. Additionally, the training set includes an additional target variable, \\n\\\"Survived\\\". Analysis reveals missing values in the Age, Cabin, and Embarked \\ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \\nAge and Cabin having a relatively high proportion of missing values [2]. \\n2.2 Data Processing \\nTo facilitate data processing, this study first combined the training set and the test set\\nto form a complete dataset, called the \\\"merged set\\\". The missing values in the\\nTitanic Survival Prediction Enhanced by Innovative Feature ‚Ä¶ 209\\nEmbarked and Fare columns, which had relatively few missing values, were filled \\nfirst. By analyzing the sample features corresponding to the missing values in \\nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \\nCabin of B28, and a Sex of female. Considering the correlations between features, a \\nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \\nassumption, box plots of Fare distribution under different combinations of Embarked \\nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \\nPclass is 1, Embarked is more likely to be 'C'. Therefore, the missing Embarked \\nvalues were filled with 'C'. Similarly, for cases where Embarked = 'S' and Pclass = 3, \\nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \\nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \\nNext, the Cabin column was processed. Due to the high number of missing values, the \\nanalysis was based only on available information. Records with Cabin information \\nwere filtered, and the distribution of cabin classes between survivors and \\nnon-survivors was compared. Cabin values containing multiple cabins were split into \\nlists, and the data was expanded row-wise to generate new features. Two new features \\nwere extracted: Deck (the letter part, filled with \\\"U\\\" if none) and Cabin number (the \\nnumeric part, converted to a numeric type). The Cabin number was further binned, \\nand survival rates within different intervals were analyzed [5]. The analysis revealed \\nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \\napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \\naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \\nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \\nsurvival rates for each Deck were calculated, which showed notable variations: Deck \\nB had the highest survival rate at approximately 74%, followed by Decks D and E at \\naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \\nrate at about 30%. Finally, the expanded data was re-aggregated into the original \\nmerged dataset (Fig. 1). \\nThe Title (such as \\\"Mr\\\", \\\"Mrs\\\") was extracted from the Name column as a new\\nfeature. Rare Titl...\\n\\n--- Result 4 ---\\nTitle: Space Titanic Kaggle competition 0.8066 score solution\\nURL: https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6\\n\\nContent:\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# Spaceship Titanic Kaggle competition top 7% score solution\\n\\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\\n\\n7 min read\\n\\n¬∑\\n\\nJan 17, 2024\\n\\n--\\n\\nListen\\n\\nShare\\n\\nPress enter or click to view image in full size\\n\\n## Introduction\\n\\nKaggle‚Äôs Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an ‚Äúanomaly‚Äù using records recovered from the spaceship‚Äôs damaged computer system. The ‚Äúlore‚Äù of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship‚Äôs passengers.\\n\\n## Import libraries\\n\\n```\\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\\n```\\n\\n## Let‚Äôs Start\\n\\nAs you can see, after I loaded the dataset, I removed both the ‚ÄúPassenger Id‚Äù and ‚ÄúName‚Äù columns. They are not going to provide any useful or important information to the prediction. Someone‚Äôs name or Id does not change the probability of being Transported.\\n\\n```\\ndf = pd.read_csv(\\\"train.csv\\\")df.drop(columns=[\\\"PassengerId\\\",\\\"Name\\\"],inplace=True)\\n```\\n\\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the ‚ÄúCabin‚Äù column. Notice that the rows on the ‚ÄúCabin‚Äù column follow a specific pattern. Something like this: ‚ÄúA/5/S‚Äù, ‚ÄúC/1/S‚Äù, ‚ÄúF/7/P‚Äù. And I decided to investigate it. So, to make things simple I split the rows of the ‚ÄúCabin‚Äù into three columns based on both slashes (‚Äù/‚Äù) of the rows. For example, the ‚ÄúA/5/S‚Äù row would be transformed into three new columns: The first one is named ‚Äúcabin\\\\_code‚Äù referring tothe character behind the first slash (A). The second one named ‚Äúid\\\\_cabin‚Äù refers to the character behind the second slash (5). The third one named ‚Äúcabin\\\\_sector‚Äù refers to the character after the second slash (S). And we end up with three new columns.\\n\\n```\\n#Splittingdf[[\\\"cabin_code\\\",\\\"id_cabin\\\",\\\"cabin_sector\\\"]] = df[\\\"Cabin\\\"].str.split(\\\"/\\\", n=2, expand=True)df.head(4)\\n```\\n\\nFirst of all, I noticed that ‚Äúcabin\\\\_code‚Äù only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\\n\\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\\n\\nPress enter or click to view image in full size\\n\\nI did the same thing with the ‚Äúcabin\\\\_sector‚Äù column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\\n\\nPress enter or click to view image in full size\\n\\nThis means that this exploration of the original ‚ÄúCabin‚Äù column is worth it since new insights are being added to the model.\\n\\nNow, we can finally delete the ‚ÄúCabin‚Äù column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\\n\\nI also removed the ‚Äúcabin\\\\_id‚Äù and the column that I had created. As I said before, the Id will not interfere with the model‚Äôs predictive ability.\\n\\nSo used: df.drop(columns=\\\\[‚ÄúCabin‚Äù,‚Äùid\\\\_cabin‚Äù\\\\], inplace=True) to drop both columns\\n\\nBefore splitting our data, the ‚ÄúTransported‚Äù column must be in a binary format. As you can see, I switched ‚ÄúTrue‚Äù for 1 and ‚ÄúFalse‚Äù for 0.\\n\\nBinary transformation: df\\\\[‚ÄúTransported‚Äù\\\\] = df\\\\[‚ÄúTransported‚Äù\\\\].map({True:1, False:0})\\n\\nI also removed every row that had missing values in the ‚Äúcabin\\\\_code‚Äù column.\\n\\n```\\n#BINARY TRANSFORMATIONdf[\\\"Transported\\\"] = df[\\\"Transported\\\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\\\"Cabin\\\",\\\"id_cabin\\\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\\\"cabin_code\\\"], inplace=True)\\n```\\n\\nNow, we can finally split the data and proceed to develop our model.\\n\\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\\n\\n```\\n#Define X and yX = df.iloc[:,0:12]y = df[\\\"Transported\\\"]\\n```\\n\\n```\\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\\n```\\n\\n```\\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\\n```\\n\\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don‚Äôt know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\\n\\n```\\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\\\"imputer_cat\\\", SimpleImputer(strategy=\\\"most_frequent\\\")),(\\\"encoder\\\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\\\"imputer_num\\\", KNNImputer(n_neighbors=3))])\\n```\\n\\nAnd with column transformer, we can attach both transformations to one variable that I named ‚Äútransformer‚Äù. Observe that we also have to specify the type of data to which the pipeline will be applied t...\\n\\n--- Result 5 ---\\nTitle: Titanic dataset Analysis (~80% accuracy) - Analytics Vidhya - Medium\\nURL: https://medium.com/analytics-vidhya/titanic-dataset-analysis-80-accuracy-9480cf3db538\\n\\nContent:\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9480cf3db538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-9480cf3db538---------------------------------------)\\n\\n¬∑\\n\\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\\n\\n# Titanic dataset Analysis (~80% accuracy)\\n\\n[Aishani Basu](https://medium.com/@aishani-81621?source=post_page---byline--9480cf3db538---------------------------------------)\\n\\n11 min read\\n\\n¬∑\\n\\nNov 25, 2021\\n\\n--\\n\\n2\\n\\nListen\\n\\nShare\\n\\n[Source](https://ornjhonnywingsfood.blogspot.com/2021/08/titanic-rms-titanic-wikiwand-murdoch.html)\\n\\n## Introduction\\n\\nThe Titanic or, in full, [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) was part of the one of the most iconic tragedies of all time. RMS Titanic was a British passenger ship that hit an iceberg while on its voyage from Southampton to New York City, and sank in the North Atlantic ocean, leaving hundreds of passengers to die in the aftermath of the deadly incident. Some of the passengers who survived till help arrived were rescued while many lost their lives helplessly waiting for help to arrive.\\n\\nThe legendary Kaggle problem, [Titanic](https://www.kaggle.com/c/titanic), based on the tragedic sinking of the RMS Titanic records data about 891 passengers of RMS Titanic, and we are required to predict if they have survived or not, based on the information we have available about the passengers and the outcome after the sinking of the ship.\\n\\nNote: This notebook is my analysis of the titanic dataset to obtain any meaningful insights from the data and scores an accuracy of ~80 percent (top 5 percent of 14k entries on Kaggle) .\\n\\nLet‚Äôs get started!\\n\\n## Contents:\\n\\n1\\\\. About the data\\n\\n2\\\\. Problem definition and metrics\\n\\n3\\\\. EDA\\n\\n4\\\\. Baseline model performance\\n\\n5\\\\. Model Selection\\n\\n6\\\\. Results and conclusion\\n\\n## About the data:\\n\\nFirst, we have the prediction variable, that is if they survived the sinking or not. Then a bunch of numeric variables, like ids, ages of the passengers, etc. followed by categorical variables like class of the ticket, and strings like name, etc.\\n\\nPress enter or click to view image in full size\\n\\nExample RMS Titanic ticket : [Source](https://www.encyclopedia-titanica.org/titanic-tickets/)\\n\\nPrinting first 5 rows of given dataset..\\n\\n## Problem definition and metrics\\n\\nThe problem is a **binary class classification** problem. We can use **binary crossentropy** or logistic loss as the loss function and any metric like **accuracy** or/and **ROC AUC score** as a metric to evaluate the results.\\n\\n## EDA\\n\\n### Correlation between features:\\n\\nDrawing insights from the correlation between features..\\n\\n- Fare and Pclass are negatively correlated (with a Pearson‚Äôs correlation coefficient of -0.55) ; Obviously, **higher fares imply better better ticket classes** (lower the class number) and vice versa.\\n- Pclass and the target feature are moderately negatively correlated (-0.34) implying **better the ticket class, higher the chance of survival.**\\n- A similar pattern can be observed with the features Parch, SibSp and Age; Both the features, Parch and SibSp are very slightly correlated with all the features except Age feature, and both have a negative correlation (-0.19 and -0.31 respectively) with Age ; **Lower the age, more the number of family accompanying the passenger**\\n- SibSp and Parch features are positively correlated which are both indicative of the number of family members accompanying passenger\\n- PClass and Age are negatively correlated (-0.37) implying **higher the age, better the ticket class**\\n\\n### Missing data\\n\\n**Age** : Contains 177 nan values out of 891 entries. Imputed with median gave best results.\\n\\n**Embarked** : Contains 2 nan values. Imputed with mode of existing data.\\n\\n**Cabin** : 687 out of 891 Cabin entries are nans, i.e. more than 50 percent of the total data exists as missing data or Nans so assumed its better to derive other features from this feature.\\n\\nNow, printing the correlation heatmap after handling missing data and converting categorical strings to encodings (0,1,2..)..\\n\\n- Embarked also has slight correlation with the target variable (-0.17), the **port at which the ship was boarded by the passengers did determine their chance of survival**\\n- Sex is also highly correlated with the target variable (-0.54), indicating the **passenger‚Äôs gender had a high effect on their chance of survival**\\n- Embarked and Fare of the passengers are negatively correlated (-0.22); obviously, fare depends on the port a passenger boards the ship from.\\n- Embarked and Sex also seem slightly correlated (0.11) indicating the **port the passenger boarded from was dependent on the gender of the passenger**; Embarked and Pclass are also correlated (0.16) indicating a **1st class passenger probably boarded at a different port than a 3rd class passenger**.\\n- Sex and Fare, Sex and Parch, SibSp, Sex and Pclass all seem slightly correlated (-0.18, -0.25, -0.11, 0.13 respectively); i.e the **fare the passenger paid, the family aboard, and the class of the ticket was seem slightly dependent on the gender of the passenger**.\\n\\n**Question 1: How do the features ‚ÄòAge‚Äô, ‚ÄòSex‚Äô, ‚ÄòFare‚Äô, ‚ÄòEmbarked‚Äô affect the chance of a passenger‚Äôs survival?**\\n\\nAge :\\n\\nPress enter or click to view image in full size\\n\\n- Both distributions, of people who didn‚Äôt survive and the ones who did, are normal with a spike around age 30 for passengers who did not survive i.e. for **people around the age 30 had a higher chance of not surviving**\\n- People of ages > ~51 are considered outliers in the distribution of people who did not survive. owing to the fact **people above the age of ~51 are very less** and people of age ~55 above are outliers in the distribution of people who did survive, i.e. **very few people above ~55 actually did survive**\\n\\nPress enter or click to view image in full size\\n\\nSex:\\n\\nPress enter or click to view image in full size\\n\\n- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard\\n\\nFare :\\n\\nPress enter or click to view image in full size\\n\\n- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\\n- Lots of outliers in the distribution of both\\n- A huge spike in the distribution of the people who didn‚Äôt survive, or the probability that the person who didn‚Äôt survive had a fare of ~10 units of currency is very high\\n\\nEmbarked:\\n\\nPress enter or click to view image in full size\\n\\n- Maximum passengers seem to have boarded the ship at Southampton port and only 33 percent of them survived; out of the total passengers who boarded the ship at Queenstown, a simi...\\n\\n--- Result 6 ---\\nTitle: Search code, repositories, users, issues, pull requests...\\nURL: https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\\n\\nContent:\\nGitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[Skip to content](#start-of-content)\\n## Navigation Menu\\nToggle navigation\\n[](https://github.com/)\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\n</option></form>\\nSearch\\nClear\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n# Provide feedback\\n</option></form>\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancelSubmit feedback\\n# Saved searches\\n## Use saved searches to filter your results more quickly\\n</option></form>\\nName\\nQuery\\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\nCancelCreate saved search\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nResetting focus\\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\\n{{ message }}\\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\\nmain\\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\nGo to file\\nCode\\nOpen more actions menu\\n## Folders and files\\n|Name|Name|\\nLast commit message\\n|\\nLast commit date\\n|\\n## Latest commit\\n## History\\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n|\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n|\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n|\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n|\\n|\\nView all files\\n|\\n## Repository files navigation\\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\~72% to \\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n# Titanic Survival Prediction\\n[](#titanic-survival-prediction)\\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\\n* **Detailed EDA**\\nA Jupyter notebook that walks through exploratory data analysis:\\n* Summary statistics\\n* Missing-value patterns\\n* Feature distributions and pairwise relationships\\n* Correlation heatmaps\\n* **Feature Engineering**\\nCreation of high-signal features from raw inputs, including:\\n* `Has\\\\_Cabin`(binary cabin indicator)\\n* `FareBin`(quantile-based fare categories)\\n* Group-median imputation for`Age`, then 10-year age bins\\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\\n* `Title`extracted and consolidated from passenger names\\n* **Model Comparison**\\nTraining and evaluation of multiple classifiers on the engineered feature set:\\n* Random Forest\\n* Support Vector Machine\\n* K-Nearest Neighbors (k=3,5,7)\\n* XGBoost\\n* LightGBM\\nResults are compared side-by-side with a clean bar chart, and each model‚Äôs strengths and weaknesses are discussed in detailed Markdown cells.\\n* **Analysis &amp; Interpretation**\\nInline Markdown commentary explains:\\n* Why each feature matters\\n* How missing data were handled\\n* The rationale for model selection\\n* Key takeaways and next-step recommendations\\n## Repository Structure\\n[](#repository-structure)\\ndata/\\n* train.csv # Official Kaggle training set\\n* test.csv # Official Kaggle test set\\nnotebooks/\\n* titanic\\\\_analysis.ipynb\\n‚Ä¢EDA, feature engineering, model training\\n‚Ä¢Detailed Markdown analysis\\n* README.md # Project overview and instructions\\n## Contributing\\n[](#contributing)\\nFeel free to open issues, fork the repository, or submit pull requests with enhancements‚Äîwhether it‚Äôs new feature ideas, alternative modeling approaches, or visualization improvements.\\n## About\\nPredict Titanic survival using t...\\n\\n--- Result 7 ---\\nTitle: GitHub - chrischow/kaggle_titanic: Kaggle Competition - Titanic Dataset\\nURL: https://github.com/chrischow/kaggle_titanic\\n\\nContent:\\n[Skip to content](https://github.com/chrischow/kaggle_titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[chrischow](https://github.com/chrischow)/ **[kaggle\\\\_titanic](https://github.com/chrischow/kaggle_titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n- [Star\\\\\\n1](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n\\nKaggle Competition - Titanic Dataset\\n\\n[1\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers) [0\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks) [Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags) [Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n\\n# chrischow/kaggle\\\\_titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[33 Commits](https://github.com/chrischow/kaggle_titanic/commits/master/) |\\n| [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) | [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) |  |  |\\n| [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) | [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) |  |  |\\n| [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) | [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle Competition - Titanic Dataset\\n\\nThis Kaggle competition requires participants to predict whether passengers will survive.\\n\\n## Introduction\\n\\nThis readme describes the entire process I used for the Kaggle Titanic competition. The notebooks are named in pseudo-chronological order. I say \\\"pseudo\\\" because whenever I revisited a process, I edited the relevant existing notebook without creating a new one. Hence, some code and results in earlier notebooks may not have been used in the next few notebooks, but were used at some point in the entire project.\\n\\n## Exploratory Data Analysis\\n\\nFirst, I performed exploratory data analysis (EDA) on all features in the dataset ( [1\\\\. EDA.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/1.%20EDA.ipynb)). Concurrently, I cleaned the data and generated new features.\\n\\n## Modelling\\n\\n### Random Forest ( `sklearn`)\\n\\nNext, I experimented with the `sklearn` implementation of Random Forest ( [2\\\\. Random Forest - Low Depth.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/2.%20Random%20Forest%20-%20Low%20Depth.ipynb)). By the final iteration in the optimisation process, the accuracy was **82.47%**. However, the `sklearn` implementation ( `RandomForestClassifier`) was extremely slow. Hence, I switched over to the `lightgbm` implementation of the Random Forest algorithm.\\n\\n### Random Forest ( `lightgbm`)\\n\\nThe `lightgbm` implementation of the Random Forest algorithm was **much** faster. The subsequent three notebooks captured my attempts at figuring out the best sequence for the tuning of parameters. The best approach was to:\\n\\n1. Tune the number of trees/estimators\\n2. Perform Recursive Feature Elimination (RFE) with cross-validation (CV)\\n3. Tune the proportion of observations sampled\\n4. Tune the maximum features used per split\\n5. Tune the minimum samples in each terminal node\\n6. Tune alpha, the L1 regularisation parameter\\n7. Tune lambda, the L1 regularisation parameter\\n\\nThis tuning sequence resulted in a 5-fold CV test score of **83.46%**.\\n\\n### Regenerating Features\\n\\nThen, I realised that I made a mistake in the feature generation process. I had generated encoded features using the entire training set. This resulted in contamination, which meant that the pseudo-test sets in CV contained information about the target, and this could have inflated the test scores. Instead, I should have (1) performed data cleaning on the competition training and test sets together, (2) encoded the training set while saving the mappings, and (3) performed mappings on the test set. Hence, I created the [`kaggle_titanic` module](https://github.com/chrischow/kaggle_titanic/blob/master/Modules/kaggle_titanic.py) to perform these three data cleaning functions, and many other functions for machine learning.\\n\\n### Model Tuning I - Improper Cross Validation\\n\\nI created pipelines for performing feature selection for and tuning the various models:\\n\\n1. LightGBM RF with low depth\\n2. LightGBM RF with high depth\\n3. LightGBM GBM with low depth\\n4. LightGBM GBM with high depth\\n5. XGBoost GBM with low depth\\n6. XGBoost GBM with high depth\\n7. Logistic Regression\\n8. Ridge Regression\\n9. Linear Support Vector Classifier (SVC)\\n\\nHaving completed the tuning for the above models, I discovered another source of contamination: in the cross validation process. I was using cross validation to both (1) tune hyperparameters and (2) score models. These two processes should have been separate. The correct way to have performed cross validation was:\\n\\n1. Split data into **meta folds** (e.g. 5 repeats of 5 folds)\\n2. For each **meta fold**:\\na. Split training data into folds (e.g. 5 repeats of 5 folds)\\nb. Run feature selection and hyperparameter tuning algorithms using cross validation\\nc. Score the model on the test set of the **meta fold**\\nd. Move on to next **meta fold** and repeat\\n3. Evaluate results for all **meta folds**\\n\\nThe first important thing I learned was that although feature selection and parameter tuning require cross validation, **all processes from data cleaning to feature generation to feature selection to hyperparameter tuning** require cross validation as well. In other words, **cross validation should be used to validate the full approach**. My mistake was that I saw hyperparameter tuning with cross validation as the end stage of testing. This was wrong. The second important thing I learned was that it was better to place feature selection before hyperparameter tuning. This is because changes in hyperparameter levels do not change how important or unimportant the features are. We only need the initial parameters for running RFE to not prevent the algorithm from letting the important features surface. For example, in tree-based models, increasing the number of trees, depth, or features sampled per tree would help to ensure that sufficient features are evaluated. The tradeoff to increasing these hyperparameters would be the computational cost. These two objectives should be balanced well.\\n\\n## About\\n\\nKaggle Competition - Titanic Dataset\\n\\n### Resources\\n\\n[Readme](https://github.com/chrischow/kaggle_titanic#readme-ov-file)\\n\\n[Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n### Stars\\n\\n[**1**\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers)\\n\\n### Watchers\\n\\n[**2**\\\\\\nwatching](https://github.com/chrischow/kaggle_titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks)\\n\\n[Report repository](https:...\\n\\n--- Result 8 ---\\nTitle: Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\\nURL: https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c\\n\\nContent:\\nBuilding a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n[## Towards AI\\n](https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-40a0aa184c1c---------------------------------------)\\n¬∑Follow publication\\n[\\n![Towards AI](https://miro.medium.com/v2/resize:fill:76:76/1*JyIThO-cLjlChQLb6kSlVQ.png)\\n](https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-40a0aa184c1c---------------------------------------)\\nMaking AI accessible to 100K+ learners. Find the most practical, hands-on and comprehensive AI Engineering and AI for Work certifications at[academy.towardsai.net](http://academy.towardsai.net)- we have pathways for any experience level. Monthly cohorts still open‚Ää‚Äî‚Ääuse COHORT10 for 10% off!\\nFollow publication\\n# Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\\n## This article will walk you through detailed forward feature selection steps and model building from scratch, improving it further with fine-tuning.\\n[\\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\\n](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\\n5 min read\\n¬∑May 24, 2023\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-artificial-intelligence/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--40a0aa184c1c---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=---header_actions--40a0aa184c1c---------------------bookmark_footer------------------)\\nListen\\nShare\\nPress enter or click to view image in full size\\n![]()\\nPhoto by[NASA](https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\\n[\\n## Data pre-processing of spaceship-titanic kaggle dataset for achieving 80+% accuracy.\\n### Data cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact‚Ä¶\\nmedium.com\\n](https://medium.com/@chavdadevang23/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?source=post_page-----40a0aa184c1c---------------------------------------)\\n**We will be building a model in 3 trenches:**\\n1. Building a model with only numerical features.\\n2. Building a model with only categorical features.\\n3. Building a model with all features combined.\\n```\\nNUMS = [&#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,&#x27;Num&#x27;,\\n&#x27;&#x27;Expenditure&#x27;&#x27;,&#x27;&#x27;Group\\\\_size&#x27;&#x27;,&#x27;&#x27;Expenditure&#x27;&#x27;]\\nTARGET = [&#x27;Transported&#x27;]\\n# only numerical feature dataframe\\nnums\\\\_df = df[NUMS+TARGET]\\ndf[NUMS].head(3)\\n```\\nLet‚Äôs find out which features have the most importance in order to classify a data point, we will be using the forward feature selection method.\\n> Forward feature selection: Step-by-step process of adding features to improve a model‚Äôs performance, starting with none, to identify the most relevant ones.\\n```\\ndef evaluate\\\\_model\\\\_kfold\\\\_classification(X,y,k,clf):\\n#kfold base\\nX = X.fillna(0)\\nkf = model\\\\_selection.KFold(n\\\\_splits=k,shuffle=True)\\naccuracies = []\\nfor fold, (train\\\\_index,validation\\\\_index) in enumerate(kf.split(X=X)):\\ntrain\\\\_x = X.loc[train\\\\_index].values\\ntrain\\\\_y = y.loc[train\\\\_index].values\\nvalidation\\\\_x = X.loc[validation\\\\_index].values\\nvalidation\\\\_y = y.loc[validation\\\\_index].values\\nclf.fit(train\\\\_x,train\\\\_y)\\npreds = clf.predict(validation\\\\_x)\\naccuracy = metrics.accuracy\\\\_score(validation\\\\_y, preds)\\nprint(f&quot;Fold={fold}, Accuracy={accuracy}&quot;)\\naccuracies.append(accuracy)\\nreturn sum(accuracies)/len(accuracies)\\ndef feature\\\\_selection\\\\_classification(X,y,k,model):\\ngood\\\\_features = []\\naverage\\\\_eval\\\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\\nbest\\\\_feature= None\\nfor feature in list(X.columns):\\nscore = evaluate\\\\_model\\\\_kfold\\\\_classification(X[[feature]],y,k,model)\\nif score &gt;&gt; average\\\\_eval\\\\_metric:\\nbest\\\\_feature = feature\\naverage\\\\_eval\\\\_metric =score\\nprint(&quot;&quot;best feature--&gt;&gt;&quot;&quot;,best\\\\_feature)\\nfeatures = list(X.columns)\\nfeatures.remove(best\\\\_feature)\\nbest\\\\_feature\\\\_order = [best\\\\_feature]\\nbest\\\\_feature\\\\_order.extend(features)\\nprint(&quot;&quot;best feature order --&gt;&gt;&quot;&quot;,best\\\\_feature\\\\_order)\\ngood\\\\_features = []\\naverage\\\\_eval\\\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\\nscores\\\\_progression = {}\\nfor feature in best\\\\_feature\\\\_order:\\ngood\\\\_features.append(feature)\\nscore = evaluate\\\\_model\\\\_kfold\\\\_classification(X[good\\\\_features],y,k,model)\\nscores\\\\_progression[&#x27;&#x27;|&#x27;&#x27;.join(good\\\\_features)] = score\\nif score &lt;&lt; average\\\\_eval\\\\_metric:\\ngood\\\\_features.remove(feature)\\nelse:\\naverage\\\\_eval\\\\_metric = score\\nreturn good\\\\_features,scores\\\\_progression\\n```\\nThe code performs feature selection for classification. It iterates over features, evaluates their impact on the model, and selects the best ones based on evaluation metrics.\\nHere I am using logistic regression as my base model to select the best features. Why logistic regression: when we did t-sne analysis on this dataset, we found out that data points are separated in a way where drawing a boundary will be easier,\\n```\\n# here any classification model can be chosen in order to get best features\\nclf = LogisticRegression()\\ngood\\\\_features , score\\\\_progression = feature\\\\_selection\\\\_classification(df[NUMS],df[TARGET],5,clf)\\nscore\\\\_progression\\n```\\nPress enter or click to view image in full size\\n![]()\\nStep-by-step progress\\nPress enter or click to view image in full size\\n![]()\\ngood features\\nNow measure accuracy with all numeric features vs with only good features, which we have derived with feature selection.\\n```\\nprint(&quot;&quot;\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*with all features\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*...\\n\\n--- Result 9 ---\\nTitle: How to Predict %80 Accuracy in the Titanic Disaster Competition\\nURL: https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3\\n\\nContent:\\n<div><div><div><a href=\\\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\\\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle‚Äôs Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \\\"core\\\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle‚Äôs competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let‚Äôs begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\\\"ignore\\\")<br/># Input data files are available in the read-only \\\"../input/\\\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There‚Äôre 2 datasets: one is the ‚Äútrain.csv‚Äù dataset, which has the ‚ÄúSurvived‚Äù variable filled with 1‚Äì0, and the other is the \\\"test.csv,\\\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\\\"/kaggle/input/titanic/train.csv\\\")<br/>test = pd.read_csv(\\\"/kaggle/input/titanic/test.csv\\\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the ‚ÄúSurvived‚Äù variable, that is, the target variable.</p><p>We use the function ‚Äúgrab_col_names()‚Äù to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \\\"\\\"\\\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\\\"iris\\\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \\\"\\\"\\\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \\\"O\\\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \\\"O\\\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \\\"O\\\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \\\"O\\\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\\\"Observations: {dataframe.shape[0]}\\\")<br/> print(f\\\"Variables: {dataframe.shape[1]}\\\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Top‚Äëscoring Titanic solutions combine extensive feature engineering (e.g., extracting titles, family size, cabin presence, age bins and imputing missing ages), aggressive hyper‚Äëparameter tuning (grid/random/Bayesian search) and ensemble models such as soft‚Äëvoting or stacking of logistic regression, Random‚ÄØForest and XGBoost, which together have achieved 81‚Äë84‚ÄØ% accuracy on the test set ([github.com](https://github.com/NickKaparinos/Kaggle-Titanic), [atlantis-press.com](https://www.atlantis-press.com/article/126015317.pdf), [python.plainenglish.io](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).\", \"url\": \"\"}, {\"title\": \"GitHub - NickKaparinos/Kaggle-Titanic: Kaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\", \"text\": \"[Skip to content](https://github.com/NickKaparinos/Kaggle-Titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[NickKaparinos](https://github.com/NickKaparinos)/ **[Kaggle-Titanic](https://github.com/NickKaparinos/Kaggle-Titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n- [Star\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE)\\n\\n[0\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers) [0\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks) [Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags) [Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n\\n# NickKaparinos/Kaggle-Titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[10 Commits](https://github.com/NickKaparinos/Kaggle-Titanic/commits/master/) |\\n| [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) | [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) |  |  |\\n| [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) | [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) |  |  |\\n| [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) | [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) |  |  |\\n| [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) | [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) |  |  |\\n| [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) | [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) |  |  |\\n| [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) | [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) |  |  |\\n| [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) | [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) |  |  |\\n| [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) | [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle-Titanic\\n\\n## Machine learning classification competition\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/titanic.jpg)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/titanic.jpg)\\n\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Cross validation results\\n\\nAfter applying the appropriate preprocessing, each model was evaluated using 10-fold cross validation. Each model\\\\`s hyperparameters were tuned using extensive grid search. Afterwards, tuned models can be ensembled together for a boost in accuracy.\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/cv.png)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/cv.png)\\n\\n## Test set results\\n\\nUsing the optimal voting model, a test set accuracy of **0.8110** was achieved, which corresponds to position **349/50092** in the leaderboard (top 0.6%).\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results1.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results1.PNG)\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results2.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results2.PNG)\\n\\n## About\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### Topics\\n\\n[machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle) [classification](https://github.com/topics/classification)\\n\\n### Resources\\n\\n[Readme](https://github.com/NickKaparinos/Kaggle-Titanic#readme-ov-file)\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic#MIT-1-ov-file)\\n\\n[Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n### Stars\\n\\n[**0**\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers)\\n\\n### Watchers\\n\\n[**1**\\\\\\nwatching](https://github.com/NickKaparinos/Kaggle-Titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks)\\n\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNickKaparinos%2FKaggle-Titanic&report=NickKaparinos+%28user%29)\\n\\n## [Releases](https://github.com/NickKaparinos/Kaggle-Titanic/releases)\\n\\nNo releases published\\n\\n## [Packages\\\\ 0](https://github.com/users/NickKaparinos/packages?repo_name=Kaggle-Titanic)\\n\\nNo packages published\\n\\n## Languages\\n\\n- [Python100.0%](https://github.com/NickKaparinos/Kaggle-Titanic/search?l=python)\\n\\nYou can‚Äôt perform that action at this time.\", \"url\": \"https://github.com/NickKaparinos/Kaggle-Titanic\"}, {\"title\": \"\", \"text\": \"Titanic Survival Prediction Enhanced by Innovative \\nFeature Engineering and Multi-Model Ensemble \\nOptimization \\nHanzhi Li \\n1 School of Software Engineering, South China University of Technology, Guangzhou, \\nGuangdong, 510006, China \\n202230321188@mail.scut.edu.cn\\nAbstract. This study enhances Titanic survival prediction through advanced \\nfeature engineering and ensemble model optimization. The Titanic dataset \\npresents a classic binary classification problem requiring the prediction of \\npassenger survival based on demographic and ticket information. Our \\nmethodology employs systematic preprocessing where missing values are \\nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \\npredictions for age values. Novel features were extracted from passenger \\nnames, cabin information, and family relationships to improve predictive \\npower. Feature importance was evaluated using Random Forest and XGBoost \\nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \\nfor effective feature selection. Three classification algorithms‚Äîlogistic \\nregression, random forest, and XGBoost‚Äîwere systematically optimized using \\ngrid search, random search, and Bayesian optimization techniques. Based on \\nthese optimized models, soft voting and stacking ensemble approaches were \\nimplemented. Cross-validation results demonstrate that ensemble methods \\nachieved superior accuracy (0.8361) compared to individual models: logistic \\nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \\nresearch provides valuable optimization strategies for similar classification \\ntasks, particularly highlighting how feature engineering combined with \\nensemble methods can substantially enhance predictive performance while \\nmaintaining computational efficiency. \\nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \\nLearning, Hyperparameter Tuning. \\n1 Introduction \\nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \\nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \\ntwentieth century. Its historical significance and the multidimensional nature of \\npassenger data (including social class, gender, age, family relationships, etc.) have \\nestablished it as a classic binary classification benchmark dataset in machine learning. \\n¬© The Author(s) 2025\\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\\nhttps://doi.org/10.2991/978-94-6463-823-3_19\\n208 H. Li\\nDespite extensive research on this problem, traditional methods exhibit \\nsignificant limitations. Early studies predominantly focused on the direct application \\nof basic features, achieving prediction accuracy generally below 80% [1]. For \\ninstance, simple rules based on gender and passenger class could rapidly distinguish \\nsurvival groups. However, these methods failed to fully explore nonlinear interactions \\nbetween features and higher-order features. Moreover, approximately 20% of the age \\nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \\napproaches often rely on mean imputation or directly delete missing values, leading to \\ninformation loss and model bias [2]. \\nAccording to previous research, traditional classification methods applied to the \\nTitanic dataset achieved limited accuracy. These methods were constrained by their \\ninability to capture complex feature interactions and their sensitivity to missing values \\n[2]. Simultaneously, traditional methods predominantly depend on single models such \\nas logistic regression and decision trees without conducting feature importance \\nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \\nensemble models but have not systematically compared their performance with single \\nmodels, nor explored how different hyperparameter tuning methods influence \\nprediction model performance in this dataset. \\nTo address these challenges, this study employs advanced feature engineering \\ntechniques to extract new features, utilizes the relationships between features and \\neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \\nselection techniques like random forests and XGBoost to identify key features for \\nsurvival prediction, and performs hyperparameter tuning of logistic regression, \\nrandom forests, and XGBoost models using grid search, random search, and Bayesian \\noptimization. Based on these methods, soft voting and stacking ensemble models are \\ntrained, and the performance of models under various parameter conditions is further \\ncompared. \\n2 Methods \\n2.1 DataSet \\nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \\ndataset consists of two CSV files: the training set and the test set. The training set \\ncontains 891 samples, while the test set contains 418 samples. The data features \\ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \\nEmbarked. Additionally, the training set includes an additional target variable, \\n\\\"Survived\\\". Analysis reveals missing values in the Age, Cabin, and Embarked \\ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \\nAge and Cabin having a relatively high proportion of missing values [2]. \\n2.2 Data Processing \\nTo facilitate data processing, this study first combined the training set and the test set\\nto form a complete dataset, called the \\\"merged set\\\". The missing values in the\\nTitanic Survival Prediction Enhanced by Innovative Feature ‚Ä¶ 209\\nEmbarked and Fare columns, which had relatively few missing values, were filled \\nfirst. By analyzing the sample features corresponding to the missing values in \\nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \\nCabin of B28, and a Sex of female. Considering the correlations between features, a \\nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \\nassumption, box plots of Fare distribution under different combinations of Embarked \\nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \\nPclass is 1, Embarked is more likely to be 'C'. Therefore, the missing Embarked \\nvalues were filled with 'C'. Similarly, for cases where Embarked = 'S' and Pclass = 3, \\nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \\nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \\nNext, the Cabin column was processed. Due to the high number of missing values, the \\nanalysis was based only on available information. Records with Cabin information \\nwere filtered, and the distribution of cabin classes between survivors and \\nnon-survivors was compared. Cabin values containing multiple cabins were split into \\nlists, and the data was expanded row-wise to generate new features. Two new features \\nwere extracted: Deck (the letter part, filled with \\\"U\\\" if none) and Cabin number (the \\nnumeric part, converted to a numeric type). The Cabin number was further binned, \\nand survival rates within different intervals were analyzed [5]. The analysis revealed \\nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \\napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \\naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \\nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \\nsurvival rates for each Deck were calculated, which showed notable variations: Deck \\nB had the highest survival rate at approximately 74%, followed by Decks D and E at \\naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \\nrate at about 30%. Finally, the expanded data was re-aggregated into the original \\nmerged dataset (Fig. 1). \\nThe Title (such as \\\"Mr\\\", \\\"Mrs\\\") was extracted from the Name column as a new\\nfeature. Rare Titl...\", \"url\": \"https://www.atlantis-press.com/article/126015317.pdf\"}, {\"title\": \"Space Titanic Kaggle competition 0.8066 score solution\", \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# Spaceship Titanic Kaggle competition top 7% score solution\\n\\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\\n\\n7 min read\\n\\n¬∑\\n\\nJan 17, 2024\\n\\n--\\n\\nListen\\n\\nShare\\n\\nPress enter or click to view image in full size\\n\\n## Introduction\\n\\nKaggle‚Äôs Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an ‚Äúanomaly‚Äù using records recovered from the spaceship‚Äôs damaged computer system. The ‚Äúlore‚Äù of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship‚Äôs passengers.\\n\\n## Import libraries\\n\\n```\\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\\n```\\n\\n## Let‚Äôs Start\\n\\nAs you can see, after I loaded the dataset, I removed both the ‚ÄúPassenger Id‚Äù and ‚ÄúName‚Äù columns. They are not going to provide any useful or important information to the prediction. Someone‚Äôs name or Id does not change the probability of being Transported.\\n\\n```\\ndf = pd.read_csv(\\\"train.csv\\\")df.drop(columns=[\\\"PassengerId\\\",\\\"Name\\\"],inplace=True)\\n```\\n\\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the ‚ÄúCabin‚Äù column. Notice that the rows on the ‚ÄúCabin‚Äù column follow a specific pattern. Something like this: ‚ÄúA/5/S‚Äù, ‚ÄúC/1/S‚Äù, ‚ÄúF/7/P‚Äù. And I decided to investigate it. So, to make things simple I split the rows of the ‚ÄúCabin‚Äù into three columns based on both slashes (‚Äù/‚Äù) of the rows. For example, the ‚ÄúA/5/S‚Äù row would be transformed into three new columns: The first one is named ‚Äúcabin\\\\_code‚Äù referring tothe character behind the first slash (A). The second one named ‚Äúid\\\\_cabin‚Äù refers to the character behind the second slash (5). The third one named ‚Äúcabin\\\\_sector‚Äù refers to the character after the second slash (S). And we end up with three new columns.\\n\\n```\\n#Splittingdf[[\\\"cabin_code\\\",\\\"id_cabin\\\",\\\"cabin_sector\\\"]] = df[\\\"Cabin\\\"].str.split(\\\"/\\\", n=2, expand=True)df.head(4)\\n```\\n\\nFirst of all, I noticed that ‚Äúcabin\\\\_code‚Äù only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\\n\\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\\n\\nPress enter or click to view image in full size\\n\\nI did the same thing with the ‚Äúcabin\\\\_sector‚Äù column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\\n\\nPress enter or click to view image in full size\\n\\nThis means that this exploration of the original ‚ÄúCabin‚Äù column is worth it since new insights are being added to the model.\\n\\nNow, we can finally delete the ‚ÄúCabin‚Äù column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\\n\\nI also removed the ‚Äúcabin\\\\_id‚Äù and the column that I had created. As I said before, the Id will not interfere with the model‚Äôs predictive ability.\\n\\nSo used: df.drop(columns=\\\\[‚ÄúCabin‚Äù,‚Äùid\\\\_cabin‚Äù\\\\], inplace=True) to drop both columns\\n\\nBefore splitting our data, the ‚ÄúTransported‚Äù column must be in a binary format. As you can see, I switched ‚ÄúTrue‚Äù for 1 and ‚ÄúFalse‚Äù for 0.\\n\\nBinary transformation: df\\\\[‚ÄúTransported‚Äù\\\\] = df\\\\[‚ÄúTransported‚Äù\\\\].map({True:1, False:0})\\n\\nI also removed every row that had missing values in the ‚Äúcabin\\\\_code‚Äù column.\\n\\n```\\n#BINARY TRANSFORMATIONdf[\\\"Transported\\\"] = df[\\\"Transported\\\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\\\"Cabin\\\",\\\"id_cabin\\\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\\\"cabin_code\\\"], inplace=True)\\n```\\n\\nNow, we can finally split the data and proceed to develop our model.\\n\\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\\n\\n```\\n#Define X and yX = df.iloc[:,0:12]y = df[\\\"Transported\\\"]\\n```\\n\\n```\\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\\n```\\n\\n```\\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\\n```\\n\\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don‚Äôt know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\\n\\n```\\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\\\"imputer_cat\\\", SimpleImputer(strategy=\\\"most_frequent\\\")),(\\\"encoder\\\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\\\"imputer_num\\\", KNNImputer(n_neighbors=3))])\\n```\\n\\nAnd with column transformer, we can attach both transformations to one variable that I named ‚Äútransformer‚Äù. Observe that we also have to specify the type of data to which the pipeline will be applied t...\", \"url\": \"https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6\"}, {\"title\": \"Titanic dataset Analysis (~80% accuracy) - Analytics Vidhya - Medium\", \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9480cf3db538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-9480cf3db538---------------------------------------)\\n\\n¬∑\\n\\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\\n\\n# Titanic dataset Analysis (~80% accuracy)\\n\\n[Aishani Basu](https://medium.com/@aishani-81621?source=post_page---byline--9480cf3db538---------------------------------------)\\n\\n11 min read\\n\\n¬∑\\n\\nNov 25, 2021\\n\\n--\\n\\n2\\n\\nListen\\n\\nShare\\n\\n[Source](https://ornjhonnywingsfood.blogspot.com/2021/08/titanic-rms-titanic-wikiwand-murdoch.html)\\n\\n## Introduction\\n\\nThe Titanic or, in full, [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) was part of the one of the most iconic tragedies of all time. RMS Titanic was a British passenger ship that hit an iceberg while on its voyage from Southampton to New York City, and sank in the North Atlantic ocean, leaving hundreds of passengers to die in the aftermath of the deadly incident. Some of the passengers who survived till help arrived were rescued while many lost their lives helplessly waiting for help to arrive.\\n\\nThe legendary Kaggle problem, [Titanic](https://www.kaggle.com/c/titanic), based on the tragedic sinking of the RMS Titanic records data about 891 passengers of RMS Titanic, and we are required to predict if they have survived or not, based on the information we have available about the passengers and the outcome after the sinking of the ship.\\n\\nNote: This notebook is my analysis of the titanic dataset to obtain any meaningful insights from the data and scores an accuracy of ~80 percent (top 5 percent of 14k entries on Kaggle) .\\n\\nLet‚Äôs get started!\\n\\n## Contents:\\n\\n1\\\\. About the data\\n\\n2\\\\. Problem definition and metrics\\n\\n3\\\\. EDA\\n\\n4\\\\. Baseline model performance\\n\\n5\\\\. Model Selection\\n\\n6\\\\. Results and conclusion\\n\\n## About the data:\\n\\nFirst, we have the prediction variable, that is if they survived the sinking or not. Then a bunch of numeric variables, like ids, ages of the passengers, etc. followed by categorical variables like class of the ticket, and strings like name, etc.\\n\\nPress enter or click to view image in full size\\n\\nExample RMS Titanic ticket : [Source](https://www.encyclopedia-titanica.org/titanic-tickets/)\\n\\nPrinting first 5 rows of given dataset..\\n\\n## Problem definition and metrics\\n\\nThe problem is a **binary class classification** problem. We can use **binary crossentropy** or logistic loss as the loss function and any metric like **accuracy** or/and **ROC AUC score** as a metric to evaluate the results.\\n\\n## EDA\\n\\n### Correlation between features:\\n\\nDrawing insights from the correlation between features..\\n\\n- Fare and Pclass are negatively correlated (with a Pearson‚Äôs correlation coefficient of -0.55) ; Obviously, **higher fares imply better better ticket classes** (lower the class number) and vice versa.\\n- Pclass and the target feature are moderately negatively correlated (-0.34) implying **better the ticket class, higher the chance of survival.**\\n- A similar pattern can be observed with the features Parch, SibSp and Age; Both the features, Parch and SibSp are very slightly correlated with all the features except Age feature, and both have a negative correlation (-0.19 and -0.31 respectively) with Age ; **Lower the age, more the number of family accompanying the passenger**\\n- SibSp and Parch features are positively correlated which are both indicative of the number of family members accompanying passenger\\n- PClass and Age are negatively correlated (-0.37) implying **higher the age, better the ticket class**\\n\\n### Missing data\\n\\n**Age** : Contains 177 nan values out of 891 entries. Imputed with median gave best results.\\n\\n**Embarked** : Contains 2 nan values. Imputed with mode of existing data.\\n\\n**Cabin** : 687 out of 891 Cabin entries are nans, i.e. more than 50 percent of the total data exists as missing data or Nans so assumed its better to derive other features from this feature.\\n\\nNow, printing the correlation heatmap after handling missing data and converting categorical strings to encodings (0,1,2..)..\\n\\n- Embarked also has slight correlation with the target variable (-0.17), the **port at which the ship was boarded by the passengers did determine their chance of survival**\\n- Sex is also highly correlated with the target variable (-0.54), indicating the **passenger‚Äôs gender had a high effect on their chance of survival**\\n- Embarked and Fare of the passengers are negatively correlated (-0.22); obviously, fare depends on the port a passenger boards the ship from.\\n- Embarked and Sex also seem slightly correlated (0.11) indicating the **port the passenger boarded from was dependent on the gender of the passenger**; Embarked and Pclass are also correlated (0.16) indicating a **1st class passenger probably boarded at a different port than a 3rd class passenger**.\\n- Sex and Fare, Sex and Parch, SibSp, Sex and Pclass all seem slightly correlated (-0.18, -0.25, -0.11, 0.13 respectively); i.e the **fare the passenger paid, the family aboard, and the class of the ticket was seem slightly dependent on the gender of the passenger**.\\n\\n**Question 1: How do the features ‚ÄòAge‚Äô, ‚ÄòSex‚Äô, ‚ÄòFare‚Äô, ‚ÄòEmbarked‚Äô affect the chance of a passenger‚Äôs survival?**\\n\\nAge :\\n\\nPress enter or click to view image in full size\\n\\n- Both distributions, of people who didn‚Äôt survive and the ones who did, are normal with a spike around age 30 for passengers who did not survive i.e. for **people around the age 30 had a higher chance of not surviving**\\n- People of ages > ~51 are considered outliers in the distribution of people who did not survive. owing to the fact **people above the age of ~51 are very less** and people of age ~55 above are outliers in the distribution of people who did survive, i.e. **very few people above ~55 actually did survive**\\n\\nPress enter or click to view image in full size\\n\\nSex:\\n\\nPress enter or click to view image in full size\\n\\n- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard\\n\\nFare :\\n\\nPress enter or click to view image in full size\\n\\n- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\\n- Lots of outliers in the distribution of both\\n- A huge spike in the distribution of the people who didn‚Äôt survive, or the probability that the person who didn‚Äôt survive had a fare of ~10 units of currency is very high\\n\\nEmbarked:\\n\\nPress enter or click to view image in full size\\n\\n- Maximum passengers seem to have boarded the ship at Southampton port and only 33 percent of them survived; out of the total passengers who boarded the ship at Queenstown, a simi...\", \"url\": \"https://medium.com/analytics-vidhya/titanic-dataset-analysis-80-accuracy-9480cf3db538\"}, {\"title\": \"Search code, repositories, users, issues, pull requests...\", \"text\": \"GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[Skip to content](#start-of-content)\\n## Navigation Menu\\nToggle navigation\\n[](https://github.com/)\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\n</option></form>\\nSearch\\nClear\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n# Provide feedback\\n</option></form>\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancelSubmit feedback\\n# Saved searches\\n## Use saved searches to filter your results more quickly\\n</option></form>\\nName\\nQuery\\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\nCancelCreate saved search\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nResetting focus\\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\\n{{ message }}\\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\\nmain\\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\nGo to file\\nCode\\nOpen more actions menu\\n## Folders and files\\n|Name|Name|\\nLast commit message\\n|\\nLast commit date\\n|\\n## Latest commit\\n## History\\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n|\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n|\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n|\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n|\\n|\\nView all files\\n|\\n## Repository files navigation\\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\~72% to \\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n# Titanic Survival Prediction\\n[](#titanic-survival-prediction)\\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\\n* **Detailed EDA**\\nA Jupyter notebook that walks through exploratory data analysis:\\n* Summary statistics\\n* Missing-value patterns\\n* Feature distributions and pairwise relationships\\n* Correlation heatmaps\\n* **Feature Engineering**\\nCreation of high-signal features from raw inputs, including:\\n* `Has\\\\_Cabin`(binary cabin indicator)\\n* `FareBin`(quantile-based fare categories)\\n* Group-median imputation for`Age`, then 10-year age bins\\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\\n* `Title`extracted and consolidated from passenger names\\n* **Model Comparison**\\nTraining and evaluation of multiple classifiers on the engineered feature set:\\n* Random Forest\\n* Support Vector Machine\\n* K-Nearest Neighbors (k=3,5,7)\\n* XGBoost\\n* LightGBM\\nResults are compared side-by-side with a clean bar chart, and each model‚Äôs strengths and weaknesses are discussed in detailed Markdown cells.\\n* **Analysis &amp; Interpretation**\\nInline Markdown commentary explains:\\n* Why each feature matters\\n* How missing data were handled\\n* The rationale for model selection\\n* Key takeaways and next-step recommendations\\n## Repository Structure\\n[](#repository-structure)\\ndata/\\n* train.csv # Official Kaggle training set\\n* test.csv # Official Kaggle test set\\nnotebooks/\\n* titanic\\\\_analysis.ipynb\\n‚Ä¢EDA, feature engineering, model training\\n‚Ä¢Detailed Markdown analysis\\n* README.md # Project overview and instructions\\n## Contributing\\n[](#contributing)\\nFeel free to open issues, fork the repository, or submit pull requests with enhancements‚Äîwhether it‚Äôs new feature ideas, alternative modeling approaches, or visualization improvements.\\n## About\\nPredict Titanic survival using t...\", \"url\": \"https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\"}, {\"title\": \"GitHub - chrischow/kaggle_titanic: Kaggle Competition - Titanic Dataset\", \"text\": \"[Skip to content](https://github.com/chrischow/kaggle_titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[chrischow](https://github.com/chrischow)/ **[kaggle\\\\_titanic](https://github.com/chrischow/kaggle_titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n- [Star\\\\\\n1](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n\\nKaggle Competition - Titanic Dataset\\n\\n[1\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers) [0\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks) [Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags) [Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n\\n# chrischow/kaggle\\\\_titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[33 Commits](https://github.com/chrischow/kaggle_titanic/commits/master/) |\\n| [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) | [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) |  |  |\\n| [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) | [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) |  |  |\\n| [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) | [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle Competition - Titanic Dataset\\n\\nThis Kaggle competition requires participants to predict whether passengers will survive.\\n\\n## Introduction\\n\\nThis readme describes the entire process I used for the Kaggle Titanic competition. The notebooks are named in pseudo-chronological order. I say \\\"pseudo\\\" because whenever I revisited a process, I edited the relevant existing notebook without creating a new one. Hence, some code and results in earlier notebooks may not have been used in the next few notebooks, but were used at some point in the entire project.\\n\\n## Exploratory Data Analysis\\n\\nFirst, I performed exploratory data analysis (EDA) on all features in the dataset ( [1\\\\. EDA.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/1.%20EDA.ipynb)). Concurrently, I cleaned the data and generated new features.\\n\\n## Modelling\\n\\n### Random Forest ( `sklearn`)\\n\\nNext, I experimented with the `sklearn` implementation of Random Forest ( [2\\\\. Random Forest - Low Depth.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/2.%20Random%20Forest%20-%20Low%20Depth.ipynb)). By the final iteration in the optimisation process, the accuracy was **82.47%**. However, the `sklearn` implementation ( `RandomForestClassifier`) was extremely slow. Hence, I switched over to the `lightgbm` implementation of the Random Forest algorithm.\\n\\n### Random Forest ( `lightgbm`)\\n\\nThe `lightgbm` implementation of the Random Forest algorithm was **much** faster. The subsequent three notebooks captured my attempts at figuring out the best sequence for the tuning of parameters. The best approach was to:\\n\\n1. Tune the number of trees/estimators\\n2. Perform Recursive Feature Elimination (RFE) with cross-validation (CV)\\n3. Tune the proportion of observations sampled\\n4. Tune the maximum features used per split\\n5. Tune the minimum samples in each terminal node\\n6. Tune alpha, the L1 regularisation parameter\\n7. Tune lambda, the L1 regularisation parameter\\n\\nThis tuning sequence resulted in a 5-fold CV test score of **83.46%**.\\n\\n### Regenerating Features\\n\\nThen, I realised that I made a mistake in the feature generation process. I had generated encoded features using the entire training set. This resulted in contamination, which meant that the pseudo-test sets in CV contained information about the target, and this could have inflated the test scores. Instead, I should have (1) performed data cleaning on the competition training and test sets together, (2) encoded the training set while saving the mappings, and (3) performed mappings on the test set. Hence, I created the [`kaggle_titanic` module](https://github.com/chrischow/kaggle_titanic/blob/master/Modules/kaggle_titanic.py) to perform these three data cleaning functions, and many other functions for machine learning.\\n\\n### Model Tuning I - Improper Cross Validation\\n\\nI created pipelines for performing feature selection for and tuning the various models:\\n\\n1. LightGBM RF with low depth\\n2. LightGBM RF with high depth\\n3. LightGBM GBM with low depth\\n4. LightGBM GBM with high depth\\n5. XGBoost GBM with low depth\\n6. XGBoost GBM with high depth\\n7. Logistic Regression\\n8. Ridge Regression\\n9. Linear Support Vector Classifier (SVC)\\n\\nHaving completed the tuning for the above models, I discovered another source of contamination: in the cross validation process. I was using cross validation to both (1) tune hyperparameters and (2) score models. These two processes should have been separate. The correct way to have performed cross validation was:\\n\\n1. Split data into **meta folds** (e.g. 5 repeats of 5 folds)\\n2. For each **meta fold**:\\na. Split training data into folds (e.g. 5 repeats of 5 folds)\\nb. Run feature selection and hyperparameter tuning algorithms using cross validation\\nc. Score the model on the test set of the **meta fold**\\nd. Move on to next **meta fold** and repeat\\n3. Evaluate results for all **meta folds**\\n\\nThe first important thing I learned was that although feature selection and parameter tuning require cross validation, **all processes from data cleaning to feature generation to feature selection to hyperparameter tuning** require cross validation as well. In other words, **cross validation should be used to validate the full approach**. My mistake was that I saw hyperparameter tuning with cross validation as the end stage of testing. This was wrong. The second important thing I learned was that it was better to place feature selection before hyperparameter tuning. This is because changes in hyperparameter levels do not change how important or unimportant the features are. We only need the initial parameters for running RFE to not prevent the algorithm from letting the important features surface. For example, in tree-based models, increasing the number of trees, depth, or features sampled per tree would help to ensure that sufficient features are evaluated. The tradeoff to increasing these hyperparameters would be the computational cost. These two objectives should be balanced well.\\n\\n## About\\n\\nKaggle Competition - Titanic Dataset\\n\\n### Resources\\n\\n[Readme](https://github.com/chrischow/kaggle_titanic#readme-ov-file)\\n\\n[Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n### Stars\\n\\n[**1**\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers)\\n\\n### Watchers\\n\\n[**2**\\\\\\nwatching](https://github.com/chrischow/kaggle_titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks)\\n\\n[Report repository](https:...\", \"url\": \"https://github.com/chrischow/kaggle_titanic\"}, {\"title\": \"Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\", \"text\": \"Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n[## Towards AI\\n](https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-40a0aa184c1c---------------------------------------)\\n¬∑Follow publication\\n[\\n![Towards AI](https://miro.medium.com/v2/resize:fill:76:76/1*JyIThO-cLjlChQLb6kSlVQ.png)\\n](https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-40a0aa184c1c---------------------------------------)\\nMaking AI accessible to 100K+ learners. Find the most practical, hands-on and comprehensive AI Engineering and AI for Work certifications at[academy.towardsai.net](http://academy.towardsai.net)- we have pathways for any experience level. Monthly cohorts still open‚Ää‚Äî‚Ääuse COHORT10 for 10% off!\\nFollow publication\\n# Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\\n## This article will walk you through detailed forward feature selection steps and model building from scratch, improving it further with fine-tuning.\\n[\\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\\n](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\\n5 min read\\n¬∑May 24, 2023\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-artificial-intelligence/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--40a0aa184c1c---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=---header_actions--40a0aa184c1c---------------------bookmark_footer------------------)\\nListen\\nShare\\nPress enter or click to view image in full size\\n![]()\\nPhoto by[NASA](https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\\n[\\n## Data pre-processing of spaceship-titanic kaggle dataset for achieving 80+% accuracy.\\n### Data cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact‚Ä¶\\nmedium.com\\n](https://medium.com/@chavdadevang23/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?source=post_page-----40a0aa184c1c---------------------------------------)\\n**We will be building a model in 3 trenches:**\\n1. Building a model with only numerical features.\\n2. Building a model with only categorical features.\\n3. Building a model with all features combined.\\n```\\nNUMS = [&#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,&#x27;Num&#x27;,\\n&#x27;&#x27;Expenditure&#x27;&#x27;,&#x27;&#x27;Group\\\\_size&#x27;&#x27;,&#x27;&#x27;Expenditure&#x27;&#x27;]\\nTARGET = [&#x27;Transported&#x27;]\\n# only numerical feature dataframe\\nnums\\\\_df = df[NUMS+TARGET]\\ndf[NUMS].head(3)\\n```\\nLet‚Äôs find out which features have the most importance in order to classify a data point, we will be using the forward feature selection method.\\n> Forward feature selection: Step-by-step process of adding features to improve a model‚Äôs performance, starting with none, to identify the most relevant ones.\\n```\\ndef evaluate\\\\_model\\\\_kfold\\\\_classification(X,y,k,clf):\\n#kfold base\\nX = X.fillna(0)\\nkf = model\\\\_selection.KFold(n\\\\_splits=k,shuffle=True)\\naccuracies = []\\nfor fold, (train\\\\_index,validation\\\\_index) in enumerate(kf.split(X=X)):\\ntrain\\\\_x = X.loc[train\\\\_index].values\\ntrain\\\\_y = y.loc[train\\\\_index].values\\nvalidation\\\\_x = X.loc[validation\\\\_index].values\\nvalidation\\\\_y = y.loc[validation\\\\_index].values\\nclf.fit(train\\\\_x,train\\\\_y)\\npreds = clf.predict(validation\\\\_x)\\naccuracy = metrics.accuracy\\\\_score(validation\\\\_y, preds)\\nprint(f&quot;Fold={fold}, Accuracy={accuracy}&quot;)\\naccuracies.append(accuracy)\\nreturn sum(accuracies)/len(accuracies)\\ndef feature\\\\_selection\\\\_classification(X,y,k,model):\\ngood\\\\_features = []\\naverage\\\\_eval\\\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\\nbest\\\\_feature= None\\nfor feature in list(X.columns):\\nscore = evaluate\\\\_model\\\\_kfold\\\\_classification(X[[feature]],y,k,model)\\nif score &gt;&gt; average\\\\_eval\\\\_metric:\\nbest\\\\_feature = feature\\naverage\\\\_eval\\\\_metric =score\\nprint(&quot;&quot;best feature--&gt;&gt;&quot;&quot;,best\\\\_feature)\\nfeatures = list(X.columns)\\nfeatures.remove(best\\\\_feature)\\nbest\\\\_feature\\\\_order = [best\\\\_feature]\\nbest\\\\_feature\\\\_order.extend(features)\\nprint(&quot;&quot;best feature order --&gt;&gt;&quot;&quot;,best\\\\_feature\\\\_order)\\ngood\\\\_features = []\\naverage\\\\_eval\\\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\\nscores\\\\_progression = {}\\nfor feature in best\\\\_feature\\\\_order:\\ngood\\\\_features.append(feature)\\nscore = evaluate\\\\_model\\\\_kfold\\\\_classification(X[good\\\\_features],y,k,model)\\nscores\\\\_progression[&#x27;&#x27;|&#x27;&#x27;.join(good\\\\_features)] = score\\nif score &lt;&lt; average\\\\_eval\\\\_metric:\\ngood\\\\_features.remove(feature)\\nelse:\\naverage\\\\_eval\\\\_metric = score\\nreturn good\\\\_features,scores\\\\_progression\\n```\\nThe code performs feature selection for classification. It iterates over features, evaluates their impact on the model, and selects the best ones based on evaluation metrics.\\nHere I am using logistic regression as my base model to select the best features. Why logistic regression: when we did t-sne analysis on this dataset, we found out that data points are separated in a way where drawing a boundary will be easier,\\n```\\n# here any classification model can be chosen in order to get best features\\nclf = LogisticRegression()\\ngood\\\\_features , score\\\\_progression = feature\\\\_selection\\\\_classification(df[NUMS],df[TARGET],5,clf)\\nscore\\\\_progression\\n```\\nPress enter or click to view image in full size\\n![]()\\nStep-by-step progress\\nPress enter or click to view image in full size\\n![]()\\ngood features\\nNow measure accuracy with all numeric features vs with only good features, which we have derived with feature selection.\\n```\\nprint(&quot;&quot;\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*with all features\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*\\\\*...\", \"url\": \"https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c\"}, {\"title\": \"How to Predict %80 Accuracy in the Titanic Disaster Competition\", \"text\": \"<div><div><div><a href=\\\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\\\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle‚Äôs Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \\\"core\\\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle‚Äôs competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let‚Äôs begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\\\"ignore\\\")<br/># Input data files are available in the read-only \\\"../input/\\\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There‚Äôre 2 datasets: one is the ‚Äútrain.csv‚Äù dataset, which has the ‚ÄúSurvived‚Äù variable filled with 1‚Äì0, and the other is the \\\"test.csv,\\\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\\\"/kaggle/input/titanic/train.csv\\\")<br/>test = pd.read_csv(\\\"/kaggle/input/titanic/test.csv\\\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the ‚ÄúSurvived‚Äù variable, that is, the target variable.</p><p>We use the function ‚Äúgrab_col_names()‚Äù to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \\\"\\\"\\\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\\\"iris\\\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \\\"\\\"\\\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \\\"O\\\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \\\"O\\\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \\\"O\\\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \\\"O\\\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\\\"Observations: {dataframe.shape[0]}\\\")<br/> print(f\\\"Variables: {dataframe.shape[1]}\\\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...\", \"url\": \"https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3\"}]}",
        "tool.duration_ms": 4318.942308425903
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "cbabf746e46abeeb",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767680715491124518,
      "end_time": 1767680724350305433,
      "duration": 8.859180915,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 10 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14506\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: {\"cells\":[{\"source\":\"\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \\\"statistically significant\\\" luck!\\n\\n\\n# Table of Contents\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\n1. [Change Log](#ch90)\\n1. [Credits](#ch91)\\n\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\n   \\n*What happens when technology is too smart for its own good?*\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\"dirty data\\\" to \\\"clean data.\\\"\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\n7. **Optimize and Strategize:** This is the \\\"bionic man\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your ‚Äúcurrency exchange\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\n\\nPractice Skills\\n* Binary classification\\n* Python and R basics\\n\\n# Step 2: Gather the Data\\n\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://\n...(truncated)...\nn\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\n#submit_bc = ensemble.BaggingClassifier()\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\\n\\n\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\n#submit_etc = ensemble.ExtraTreesClassifier()\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\\n\\n\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\n#submit_rfc = ensemble.RandomForestClassifier()\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\\n\\n\\n\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\n#submit_abc = ensemble.AdaBoostClassifier()\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\\n\\n\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\\n\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\n#submit_xgb = XGBClassifier()\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\\n\\n\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\\n\\n\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\\n\\n\\n#submit file\\nsubmit = data_val[['PassengerId','Survived']]\\nsubmit.to_csv(\\\"../working/submit.csv\\\", index=False)\\n\\nprint('Validation Data Distribution: \\\\n', data_val['Survived'].value_counts(normalize = True))\\nsubmit.sample(10)\\n\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\"ch12\\\"></a>\\n# Step 7: Optimize and Strategize\\n## Conclusion\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\n\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\n\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\n\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\"ch90\\\"></a>\\n## Change Log:\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\n11/23/17 Cleaned up published notebook and updated through step 3.  \\n11/25/17 Added enhancements to published notebook and started step 4.  \\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\n12/3/17 Update section 4 with improved graphical statistics.  \\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\n12/11/17 Updated section 5 with better how-tos.  \\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\"super\\\" model.  \\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\"ch91\\\"></a>\\n# Credits\\nProgramming is all about \\\"borrowing\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\n\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}\ntool: ‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\n\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\n\\n[Stacking Starter][1] : by Faron \\n\\n\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\n\\n\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n# Create new feature IsAlone from FamilySize\\nfor dataset in full_data:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n# Remove all NULLS in the Embarked column\\nfor dataset in full_data:\\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\nfor dataset in full_data:\\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\\n# Create a New feature CategoricalAge\\nfor dataset in full_data:\\n    age_avg = dataset['Age'].mean()\\n    age_std = dataset['Age'].std()\\n    age_null_count = dataset['Age'].isnull().sum()\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\\n    dataset['Age'] = dataset['Age'].astype(int)\\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\\n# Define function to extract titles from passenger names\\ndef get_title(name):\\n    title_search = re.search(' ([A-Za-z]+)\\\\.', name)\\n    # If the title exists, extract and return it.\\n    if title_search:\\n        return title_search.group(1)\\n    return \\\"\\\"\\n# Create a new feature Title, containing the titles of passenger names\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Name'].apply(get_title)\\n# Group all non-common titles into one single grouping \\\"Rare\\\"\\nfor dataset in full_data:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n\\nfor dataset in full_data:\\n    # Mapping Sex\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\\n    \\n    # Mapping titles\\n    title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n    \\n    # Mapping Embarked\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n    \\n    # Mapping Fare\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \\t\\t\\t\\t\\t\\t        = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \\t\\t\\t\\t\\t\\t\\t        = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n    \\n    # Mapping Age\\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \\t\\t\\t\\t\\t       = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\n...(truncated)...\nlosest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig,filename='scatter2010')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#         gridwidth= 2,\\n#     ),\\n    yaxis=dict(\\n        title= 'Feature Importance',\\n        ticklen= 5,\\n        gridwidth= 2\\n    ),\\n    showlegend= False\\n)\\nfig = go.Figure(data=data, layout=layout)\\npy.iplot(fig, filename='bar-direct-labels')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: ‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nprint(f'\\nTrain columns: {train.columns.tolist()}')\nprint(f'\\nTarget distribution:\\n{train[\"Survived\"].value_counts(normalize=True)}')\ntrain.head()\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTarget distribution:\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \ntool: Added code cell at 1\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Check missing values and data types\nprint(\"Missing values:\")\nprint(train.isnull().sum())\nprint(f\"\\nData types:\\n{train.dtypes}\")\nprint(f\"\\n\\nSurvival by Sex:\")\nprint(train.groupby('Sex')['Survived'].mean())\nprint(f\"\\nSurvival by Pclass:\")\nprint(train.groupby('Pclass')['Survived'].mean())\n```\nOutput:\nMissing values:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nData types:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\ntool: ‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\n  Source: exploration/eda.ipynb\ntool: Wrote to /home/code/exploration/seed_prompt.txt\ntool: SEARCH RESULTS (9 found)\n==================================================\n\n--- Result 1 ---\nTitle: ANSWER\n\nContent:\nTop‚Äëscoring Titanic solutions combine extensive feature engineering (e.g., extracting titles, family size, cabin presence, age bins and imputing missing ages), aggressive hyper‚Äëparameter tuning (grid/random/Bayesian search) and ensemble models such as soft‚Äëvoting or stacking of logistic regression, Random‚ÄØForest and XGBoost, which together have achieved 81‚Äë84‚ÄØ% accuracy on the test set ([github.com](https://github.com/NickKaparinos/Kaggle-Titanic), [atlantis-press.com](https://www.atlantis-press.com/article/126015317.pdf), [python.plainenglish.io](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).\n\n--- Result 2 ---\nTitle: GitHub - NickKaparinos/Kaggle-Titanic: Kaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\nURL: https://github.com/NickKaparinos/Kaggle-Titanic\n\nContent:\n[Skip to content](https://github.com/NickKaparinos/Kaggle-Titanic#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[NickKaparinos](https://github.com/NickKaparinos)/ **[Kaggle-Titanic](https://github.com/NickKaparinos/Kaggle-Titanic)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n- [Star\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n\n\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\n\n### License\n\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE)\n\n[0\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers) [0\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks) [Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags) [Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\n\n[Star](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n\n[Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\n\n# NickKaparinos/Kaggle-Titanic\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[10 Commits](https://github.com/NickKaparinos/Kaggle-Titanic/commits/master/) |\n| [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) | [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) |  |  |\n| [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) | [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) |  |  |\n| [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) | [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) |  |  |\n| [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) | [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) |  |  |\n| [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) | [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) |  |  |\n| [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) | [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) |  |  |\n| [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) | [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) |  |  |\n| [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) | [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle-Titanic\n\n## Machine learning classification competition\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/titanic.jpg)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/titanic.jpg)\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## Cross validation results\n\nAfter applying the appropriate preprocessing, each model was evaluated using 10-fold cross validation. Each model\\`s hyperparameters were tuned using extensive grid search. Afterwards, tuned models can be ensembled together for a boost in accuracy.\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/cv.png)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/cv.png)\n\n## Test set results\n\nUsing the optimal voting model, a test set accuracy of **0.8110** was achieved, which corresponds to position **349/50092** in the leaderboard (top 0.6%).\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results1.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results1.PNG)\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results2.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results2.PNG)\n\n## About\n\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle) [classification](https://github.com/topics/classification)\n\n### Resources\n\n[Readme](https://github.com/NickKaparinos/Kaggle-Titanic#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic#MIT-1-ov-file)\n\n[Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\n\n### Stars\n\n[**0**\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/NickKaparinos/Kaggle-Titanic/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNickKaparinos%2FKaggle-Titanic&report=NickKaparinos+%28user%29)\n\n## [Releases](https://github.com/NickKaparinos/Kaggle-Titanic/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/NickKaparinos/packages?repo_name=Kaggle-Titanic)\n\nNo packages published\n\n## Languages\n\n- [Python100.0%](https://github.com/NickKaparinos/Kaggle-Titanic/search?l=python)\n\nYou can‚Äôt perform that action at this time.\n\n--- Result 3 ---\nURL: https://www.atlantis-press.com/article/126015317.pdf\n\nContent:\nTitanic Survival Prediction Enhanced by Innovative \nFeature Engineering and Multi-Model Ensemble \nOptimization \nHanzhi Li \n1 School of Software Engineering, South China University of Technology, Guangzhou, \nGuangdong, 510006, China \n202230321188@mail.scut.edu.cn\nAbstract. This study enhances Titanic survival prediction through advanced \nfeature engineering and ensemble model optimization. The Titanic dataset \npresents a classic binary classification problem requiring the prediction of \npassenger survival based on demographic and ticket information. Our \nmethodology employs systematic preprocessing where missing values are \nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \npredictions for age values. Novel features were extracted from passenger \nnames, cabin information, and family relationships to improve predictive \npower. Feature importance was evaluated using Random Forest and XGBoost \nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \nfor effective feature selection. Three classification algorithms‚Äîlogistic \nregression, random forest, and XGBoost‚Äîwere systematically optimized using \ngrid search, random search, and Bayesian optimization techniques. Based on \nthese optimized models, soft voting and stacking ensemble approaches were \nimplemented. Cross-validation results demonstrate that ensemble methods \nachieved superior accuracy (0.8361) compared to individual models: logistic \nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \nresearch provides valuable optimization strategies for similar classification \ntasks, particularly highlighting how feature engineering combined with \nensemble methods can substantially enhance predictive performance while \nmaintaining computational efficiency. \nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \nLearning, Hyperparameter Tuning. \n1 Introduction \nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \ntwentieth century. Its historical significance and the multidimensional nature of \npassenger data (including social class, gender, age, family relationships, etc.) have \nestablished it as a classic binary classification benchmark dataset in machine learning. \n¬© The Author(s) 2025\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\nhttps://doi.org/10.2991/978-94-6463-823-3_19\n208 H. Li\nDespite extensive research on this problem, traditional methods exhibit \nsignificant limitations. Early studies predominantly focused on the direct application \nof basic features, achieving prediction accuracy generally below 80% [1]. For \ninstance, simple rules based on gender and passenger class could rapidly distinguish \nsurvival groups. However, these methods failed to fully explore nonlinear interactions \nbetween features and higher-order features. Moreover, approximately 20% of the age \nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \napproaches often rely on mean imputation or directly delete missing values, leading to \ninformation loss and model bias [2]. \nAccording to previous research, traditional classification methods applied to the \nTitanic dataset achieved limited accuracy. These methods were constrained by their \ninability to capture complex feature interactions and their sensitivity to missing values \n[2]. Simultaneously, traditional methods predominantly depend on single models such \nas logistic regression and decision trees without conducting feature importance \nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \nensemble models but have not systematically compared their performance with single \nmodels, nor explored how different hyperparameter tuning methods influence \nprediction model performance in this dataset. \nTo address these challenges, this study employs advanced feature engineering \ntechniques to extract new features, utilizes the relationships between features and \neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \nselection techniques like random forests and XGBoost to identify key features for \nsurvival prediction, and performs hyperparameter tuning of logistic regression, \nrandom forests, and XGBoost models using grid search, random search, and Bayesian \noptimization. Based on these methods, soft voting and stacking ensemble models are \ntrained, and the performance of models under various parameter conditions is further \ncompared. \n2 Methods \n2.1 DataSet \nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \ndataset consists of two CSV files: the training set and the test set. The training set \ncontains 891 samples, while the test set contains 418 samples. The data features \ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \nEmbarked. Additionally, the training set includes an additional target variable, \n\"Survived\". Analysis reveals missing values in the Age, Cabin, and Embarked \ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \nAge and Cabin having a relatively high proportion of missing values [2]. \n2.2 Data Processing \nTo facilitate data processing, this study first combined the training set and the test set\nto form a complete dataset, called the \"merged set\". The missing values in the\nTitanic Survival Prediction Enhanced by Innovative Feature ‚Ä¶ 209\nEmbarked and Fare columns, which had relatively few missing values, were filled \nfirst. By analyzing the sample features corresponding to the missing values in \nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \nCabin of B28, and a Sex of female. Considering the correlations between features, a \nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \nassumption, box plots of Fare distribution under different combinations of Embarked \nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \nPclass is 1, Embarked is more likely to be 'C'. Therefore, the missing Embarked \nvalues were filled with 'C'. Similarly, for cases where Embarked = 'S' and Pclass = 3, \nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \nNext, the Cabin column was processed. Due to the high number of missing values, the \nanalysis was based only on available information. Records with Cabin information \nwere filtered, and the distribution of cabin classes between survivors and \nnon-survivors was compared. Cabin values containing multiple cabins were split into \nlists, and the data was expanded row-wise to generate new features. Two new features \nwere extracted: Deck (the letter part, filled with \"U\" if none) and Cabin number (the \nnumeric part, converted to a numeric type). The Cabin number was further binned, \nand survival rates within different intervals were analyzed [5]. The analysis revealed \nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \nsurvival rates for each Deck were calculated, which showed notable variations: Deck \nB had the highest survival rate at approximately 74%, followed by Decks D and E at \naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \nrate at about 30%. Finally, the expanded data was re-aggregated into the original \nmerged dataset (Fig. 1). \nThe Title (such as \"Mr\", \"Mrs\") was extracted from the Name column as a new\nfeature. Rare Titl...\n\n--- Result 4 ---\nTitle: Space Titanic Kaggle competition 0.8066 score solution\nURL: https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6\n\nContent:\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n¬∑\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle‚Äôs Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an ‚Äúanomaly‚Äù using records recovered from the spaceship‚Äôs damaged computer system. The ‚Äúlore‚Äù of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship‚Äôs passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let‚Äôs Start\n\nAs you can see, after I loaded the dataset, I removed both the ‚ÄúPassenger Id‚Äù and ‚ÄúName‚Äù columns. They are not going to provide any useful or important information to the prediction. Someone‚Äôs name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the ‚ÄúCabin‚Äù column. Notice that the rows on the ‚ÄúCabin‚Äù column follow a specific pattern. Something like this: ‚ÄúA/5/S‚Äù, ‚ÄúC/1/S‚Äù, ‚ÄúF/7/P‚Äù. And I decided to investigate it. So, to make things simple I split the rows of the ‚ÄúCabin‚Äù into three columns based on both slashes (‚Äù/‚Äù) of the rows. For example, the ‚ÄúA/5/S‚Äù row would be transformed into three new columns: The first one is named ‚Äúcabin\\_code‚Äù referring tothe character behind the first slash (A). The second one named ‚Äúid\\_cabin‚Äù refers to the character behind the second slash (5). The third one named ‚Äúcabin\\_sector‚Äù refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that ‚Äúcabin\\_code‚Äù only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the ‚Äúcabin\\_sector‚Äù column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original ‚ÄúCabin‚Äù column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the ‚ÄúCabin‚Äù column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the ‚Äúcabin\\_id‚Äù and the column that I had created. As I said before, the Id will not interfere with the model‚Äôs predictive ability.\n\nSo used: df.drop(columns=\\[‚ÄúCabin‚Äù,‚Äùid\\_cabin‚Äù\\], inplace=True) to drop both columns\n\nBefore splitting our data, the ‚ÄúTransported‚Äù column must be in a binary format. As you can see, I switched ‚ÄúTrue‚Äù for 1 and ‚ÄúFalse‚Äù for 0.\n\nBinary transformation: df\\[‚ÄúTransported‚Äù\\] = df\\[‚ÄúTransported‚Äù\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the ‚Äúcabin\\_code‚Äù column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don‚Äôt know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named ‚Äútransformer‚Äù. Observe that we also have to specify the type of data to which the pipeline will be applied t...\n\n--- Result 5 ---\nTitle: Titanic dataset Analysis (~80% accuracy) - Analytics Vidhya - Medium\nURL: https://medium.com/analytics-vidhya/titanic-dataset-analysis-80-accuracy-9480cf3db538\n\nContent:\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9480cf3db538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-9480cf3db538---------------------------------------)\n\n¬∑\n\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\n\n# Titanic dataset Analysis (~80% accuracy)\n\n[Aishani Basu](https://medium.com/@aishani-81621?source=post_page---byline--9480cf3db538---------------------------------------)\n\n11 min read\n\n¬∑\n\nNov 25, 2021\n\n--\n\n2\n\nListen\n\nShare\n\n[Source](https://ornjhonnywingsfood.blogspot.com/2021/08/titanic-rms-titanic-wikiwand-murdoch.html)\n\n## Introduction\n\nThe Titanic or, in full, [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) was part of the one of the most iconic tragedies of all time. RMS Titanic was a British passenger ship that hit an iceberg while on its voyage from Southampton to New York City, and sank in the North Atlantic ocean, leaving hundreds of passengers to die in the aftermath of the deadly incident. Some of the passengers who survived till help arrived were rescued while many lost their lives helplessly waiting for help to arrive.\n\nThe legendary Kaggle problem, [Titanic](https://www.kaggle.com/c/titanic), based on the tragedic sinking of the RMS Titanic records data about 891 passengers of RMS Titanic, and we are required to predict if they have survived or not, based on the information we have available about the passengers and the outcome after the sinking of the ship.\n\nNote: This notebook is my analysis of the titanic dataset to obtain any meaningful insights from the data and scores an accuracy of ~80 percent (top 5 percent of 14k entries on Kaggle) .\n\nLet‚Äôs get started!\n\n## Contents:\n\n1\\. About the data\n\n2\\. Problem definition and metrics\n\n3\\. EDA\n\n4\\. Baseline model performance\n\n5\\. Model Selection\n\n6\\. Results and conclusion\n\n## About the data:\n\nFirst, we have the prediction variable, that is if they survived the sinking or not. Then a bunch of numeric variables, like ids, ages of the passengers, etc. followed by categorical variables like class of the ticket, and strings like name, etc.\n\nPress enter or click to view image in full size\n\nExample RMS Titanic ticket : [Source](https://www.encyclopedia-titanica.org/titanic-tickets/)\n\nPrinting first 5 rows of given dataset..\n\n## Problem definition and metrics\n\nThe problem is a **binary class classification** problem. We can use **binary crossentropy** or logistic loss as the loss function and any metric like **accuracy** or/and **ROC AUC score** as a metric to evaluate the results.\n\n## EDA\n\n### Correlation between features:\n\nDrawing insights from the correlation between features..\n\n- Fare and Pclass are negatively correlated (with a Pearson‚Äôs correlation coefficient of -0.55) ; Obviously, **higher fares imply better better ticket classes** (lower the class number) and vice versa.\n- Pclass and the target feature are moderately negatively correlated (-0.34) implying **better the ticket class, higher the chance of survival.**\n- A similar pattern can be observed with the features Parch, SibSp and Age; Both the features, Parch and SibSp are very slightly correlated with all the features except Age feature, and both have a negative correlation (-0.19 and -0.31 respectively) with Age ; **Lower the age, more the number of family accompanying the passenger**\n- SibSp and Parch features are positively correlated which are both indicative of the number of family members accompanying passenger\n- PClass and Age are negatively correlated (-0.37) implying **higher the age, better the ticket class**\n\n### Missing data\n\n**Age** : Contains 177 nan values out of 891 entries. Imputed with median gave best results.\n\n**Embarked** : Contains 2 nan values. Imputed with mode of existing data.\n\n**Cabin** : 687 out of 891 Cabin entries are nans, i.e. more than 50 percent of the total data exists as missing data or Nans so assumed its better to derive other features from this feature.\n\nNow, printing the correlation heatmap after handling missing data and converting categorical strings to encodings (0,1,2..)..\n\n- Embarked also has slight correlation with the target variable (-0.17), the **port at which the ship was boarded by the passengers did determine their chance of survival**\n- Sex is also highly correlated with the target variable (-0.54), indicating the **passenger‚Äôs gender had a high effect on their chance of survival**\n- Embarked and Fare of the passengers are negatively correlated (-0.22); obviously, fare depends on the port a passenger boards the ship from.\n- Embarked and Sex also seem slightly correlated (0.11) indicating the **port the passenger boarded from was dependent on the gender of the passenger**; Embarked and Pclass are also correlated (0.16) indicating a **1st class passenger probably boarded at a different port than a 3rd class passenger**.\n- Sex and Fare, Sex and Parch, SibSp, Sex and Pclass all seem slightly correlated (-0.18, -0.25, -0.11, 0.13 respectively); i.e the **fare the passenger paid, the family aboard, and the class of the ticket was seem slightly dependent on the gender of the passenger**.\n\n**Question 1: How do the features ‚ÄòAge‚Äô, ‚ÄòSex‚Äô, ‚ÄòFare‚Äô, ‚ÄòEmbarked‚Äô affect the chance of a passenger‚Äôs survival?**\n\nAge :\n\nPress enter or click to view image in full size\n\n- Both distributions, of people who didn‚Äôt survive and the ones who did, are normal with a spike around age 30 for passengers who did not survive i.e. for **people around the age 30 had a higher chance of not surviving**\n- People of ages > ~51 are considered outliers in the distribution of people who did not survive. owing to the fact **people above the age of ~51 are very less** and people of age ~55 above are outliers in the distribution of people who did survive, i.e. **very few people above ~55 actually did survive**\n\nPress enter or click to view image in full size\n\nSex:\n\nPress enter or click to view image in full size\n\n- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard\n\nFare :\n\nPress enter or click to view image in full size\n\n- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\n- Lots of outliers in the distribution of both\n- A huge spike in the distribution of the people who didn‚Äôt survive, or the probability that the person who didn‚Äôt survive had a fare of ~10 units of currency is very high\n\nEmbarked:\n\nPress enter or click to view image in full size\n\n- Maximum passengers seem to have boarded the ship at Southampton port and only 33 percent of them survived; out of the total passengers who boarded the ship at Queenstown, a simi...\n\n--- Result 6 ---\nTitle: Search code, repositories, users, issues, pull requests...\nURL: https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\n\nContent:\nGitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model‚Äôs strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n‚Ä¢EDA, feature engineering, model training\n‚Ä¢Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements‚Äîwhether it‚Äôs new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...\n\n--- Result 7 ---\nTitle: GitHub - chrischow/kaggle_titanic: Kaggle Competition - Titanic Dataset\nURL: https://github.com/chrischow/kaggle_titanic\n\nContent:\n[Skip to content](https://github.com/chrischow/kaggle_titanic#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[chrischow](https://github.com/chrischow)/ **[kaggle\\_titanic](https://github.com/chrischow/kaggle_titanic)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\n- [Star\\\n1](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\n\n\nKaggle Competition - Titanic Dataset\n\n[1\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers) [0\\\nforks](https://github.com/chrischow/kaggle_titanic/forks) [Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags) [Activity](https://github.com/chrischow/kaggle_titanic/activity)\n\n[Star](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\n\n[Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\n\n# chrischow/kaggle\\_titanic\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[33 Commits](https://github.com/chrischow/kaggle_titanic/commits/master/) |\n| [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) | [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) |  |  |\n| [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) | [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) |  |  |\n| [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) | [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle Competition - Titanic Dataset\n\nThis Kaggle competition requires participants to predict whether passengers will survive.\n\n## Introduction\n\nThis readme describes the entire process I used for the Kaggle Titanic competition. The notebooks are named in pseudo-chronological order. I say \"pseudo\" because whenever I revisited a process, I edited the relevant existing notebook without creating a new one. Hence, some code and results in earlier notebooks may not have been used in the next few notebooks, but were used at some point in the entire project.\n\n## Exploratory Data Analysis\n\nFirst, I performed exploratory data analysis (EDA) on all features in the dataset ( [1\\. EDA.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/1.%20EDA.ipynb)). Concurrently, I cleaned the data and generated new features.\n\n## Modelling\n\n### Random Forest ( `sklearn`)\n\nNext, I experimented with the `sklearn` implementation of Random Forest ( [2\\. Random Forest - Low Depth.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/2.%20Random%20Forest%20-%20Low%20Depth.ipynb)). By the final iteration in the optimisation process, the accuracy was **82.47%**. However, the `sklearn` implementation ( `RandomForestClassifier`) was extremely slow. Hence, I switched over to the `lightgbm` implementation of the Random Forest algorithm.\n\n### Random Forest ( `lightgbm`)\n\nThe `lightgbm` implementation of the Random Forest algorithm was **much** faster. The subsequent three notebooks captured my attempts at figuring out the best sequence for the tuning of parameters. The best approach was to:\n\n1. Tune the number of trees/estimators\n2. Perform Recursive Feature Elimination (RFE) with cross-validation (CV)\n3. Tune the proportion of observations sampled\n4. Tune the maximum features used per split\n5. Tune the minimum samples in each terminal node\n6. Tune alpha, the L1 regularisation parameter\n7. Tune lambda, the L1 regularisation parameter\n\nThis tuning sequence resulted in a 5-fold CV test score of **83.46%**.\n\n### Regenerating Features\n\nThen, I realised that I made a mistake in the feature generation process. I had generated encoded features using the entire training set. This resulted in contamination, which meant that the pseudo-test sets in CV contained information about the target, and this could have inflated the test scores. Instead, I should have (1) performed data cleaning on the competition training and test sets together, (2) encoded the training set while saving the mappings, and (3) performed mappings on the test set. Hence, I created the [`kaggle_titanic` module](https://github.com/chrischow/kaggle_titanic/blob/master/Modules/kaggle_titanic.py) to perform these three data cleaning functions, and many other functions for machine learning.\n\n### Model Tuning I - Improper Cross Validation\n\nI created pipelines for performing feature selection for and tuning the various models:\n\n1. LightGBM RF with low depth\n2. LightGBM RF with high depth\n3. LightGBM GBM with low depth\n4. LightGBM GBM with high depth\n5. XGBoost GBM with low depth\n6. XGBoost GBM with high depth\n7. Logistic Regression\n8. Ridge Regression\n9. Linear Support Vector Classifier (SVC)\n\nHaving completed the tuning for the above models, I discovered another source of contamination: in the cross validation process. I was using cross validation to both (1) tune hyperparameters and (2) score models. These two processes should have been separate. The correct way to have performed cross validation was:\n\n1. Split data into **meta folds** (e.g. 5 repeats of 5 folds)\n2. For each **meta fold**:\na. Split training data into folds (e.g. 5 repeats of 5 folds)\nb. Run feature selection and hyperparameter tuning algorithms using cross validation\nc. Score the model on the test set of the **meta fold**\nd. Move on to next **meta fold** and repeat\n3. Evaluate results for all **meta folds**\n\nThe first important thing I learned was that although feature selection and parameter tuning require cross validation, **all processes from data cleaning to feature generation to feature selection to hyperparameter tuning** require cross validation as well. In other words, **cross validation should be used to validate the full approach**. My mistake was that I saw hyperparameter tuning with cross validation as the end stage of testing. This was wrong. The second important thing I learned was that it was better to place feature selection before hyperparameter tuning. This is because changes in hyperparameter levels do not change how important or unimportant the features are. We only need the initial parameters for running RFE to not prevent the algorithm from letting the important features surface. For example, in tree-based models, increasing the number of trees, depth, or features sampled per tree would help to ensure that sufficient features are evaluated. The tradeoff to increasing these hyperparameters would be the computational cost. These two objectives should be balanced well.\n\n## About\n\nKaggle Competition - Titanic Dataset\n\n### Resources\n\n[Readme](https://github.com/chrischow/kaggle_titanic#readme-ov-file)\n\n[Activity](https://github.com/chrischow/kaggle_titanic/activity)\n\n### Stars\n\n[**1**\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers)\n\n### Watchers\n\n[**2**\\\nwatching](https://github.com/chrischow/kaggle_titanic/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/chrischow/kaggle_titanic/forks)\n\n[Report repository](https:...\n\n--- Result 8 ---\nTitle: Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\nURL: https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c\n\nContent:\nBuilding a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------\n\n[... truncated 15258 characters ...]",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 10 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': 'REASONING: Let me start by reading the competition description and understanding the problem type, then check the pre-fetched competitive intelligence (writeups and kernels) to understand winning approaches.'}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14506\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: This is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target score is 1.0 (100% accuracy), which is extremely high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and the ensembling notebook.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file structure in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': 'REASONING: Let me read the high-accuracy framework notebook and the ensembling notebook to understand winning techniques.'}, {'role': 'tool', 'content': '{\"cells\":[{\"source\":\"\\\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I\\'ll continue to develop.* Thanks, and may you have \\\\\"statistically significant\\\\\" luck!\\\\n\\\\n\\\\n# Table of Contents\\\\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\\\\n1. [Chapter 2 - A Data Science Framework](#ch2)\\\\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\\\\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\\\\n1. [Chapter 5 - The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\\\\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\\\\n1. [Chapter 7 - Step 5: Model Data](#ch7)\\\\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\\\\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\\\\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\\\\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\\\\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\\\\n1. [Change Log](#ch90)\\\\n1. [Credits](#ch91)\\\\n\\\\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don\\'t understand something in the code the print() function is your best friend. In coding, it\\'s okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you\\'ve exhausted all your resources, the Kaggle Community via forums and comments can help too.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ea24845a-4a15-4235-9ed2-4823acbf4317\",\"_uuid\":\"89022d89e1603ef3869a96b9da78da38b0b33b14\"}},{\"source\":\"<a id=\\\\\"ch1\\\\\"></a>\\\\n# How a Data Scientist Beat the Odds\\\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle\\'s Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.\\\\n   \\\\n*What happens when technology is too smart for its own good?*\\\\n![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec31d549-537e-4f35-8594-22ef03079081\",\"_uuid\":\"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df\"}},{\"source\":\"<a id=\\\\\"ch2\\\\\"></a>\\\\n# A Data Science Framework\\\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\\\\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are ‚Äúdrowning in data, yet staving for knowledge.\\\\\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don\\'t have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \\\\\"dirty data\\\\\" to \\\\\"clean data.\\\\\"\\\\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn ‚Äúwild‚Äù data into ‚Äúmanageable‚Äù data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\\\\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\\\\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It\\'s important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that‚Äôs used as actionable intelligence) at worst.\\\\n6. **Validate and Implement Data Model:** After you\\'ve trained your model based on a subset of your data, it\\'s time to test your model. This helps ensure you haven\\'t overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\\\\n7. **Optimize and Strategize:** This is the \\\\\"bionic man\\\\\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you\\'re able to package your ideas, this becomes your ‚Äúcurrency exchange\\\\\" rate.\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8514a868-6816-494a-9e9c-dba14cb36f42\",\"_uuid\":\"bd2a68dcaace0c1371f2994f0204dce65800cc20\"}},{\"source\":\"<a id=\\\\\"ch3\\\\\"></a>\\\\n# Step 1: Define the Problem\\\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\\\n\\\\n......\\\\n\\\\n**Project Summary:**\\\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\\\\n\\\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\\\\n\\\\nPractice Skills\\\\n* Binary classification\\\\n* Python and R basics\\\\n\\\\n# Step 2: Gather the Data\\\\n\\\\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle\\'s Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"22ba49b5-b3b6-4129-8c57-623e0d863ab3\",\"_uuid\":\"ff014109d795c5d1bfcc66c9761730406124ebe4\"}},{\"source\":\"<a id=\\\\\"ch4\\\\\"></a>\\\\n# Step 3: Prepare Data for Consumption\\\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\\\n\\\\n## 3.1 Import Libraries\\\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ef4059c6-9397-4b72-85da-570f51ba139c\",\"_uuid\":\"f1d8b59b37d2ce04a5396a0ab183dc3000817113\"}},{\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\\\n#load packages\\\\nimport sys #access to system parameters https://\\n...(truncated)...\\nn\\\\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\\\\n#submit_bc = ensemble.BaggingClassifier()\\\\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {\\'n_estimators\\':grid_n_estimator, \\'max_samples\\': grid_ratio, \\'oob_score\\': grid_bool, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_bc.best_params_) #Best Parameters:  {\\'max_samples\\': 0.25, \\'n_estimators\\': 500, \\'oob_score\\': True, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_bc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\\\\n#submit_etc = ensemble.ExtraTreesClassifier()\\\\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_etc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_etc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\\\\n#submit_rfc = ensemble.RandomForestClassifier()\\\\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'criterion\\': grid_criterion, \\'max_depth\\': grid_max_depth, \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_rfc.best_params_) #Best Parameters:  {\\'criterion\\': \\'entropy\\', \\'max_depth\\': 6, \\'n_estimators\\': 100, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_rfc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n\\\\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\\\\n#submit_abc = ensemble.AdaBoostClassifier()\\\\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={\\'n_estimators\\': grid_n_estimator, \\'learning_rate\\': grid_ratio, \\'algorithm\\': [\\'SAMME\\', \\'SAMME.R\\'], \\'random_state\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_abc.best_params_) #Best Parameters:  {\\'algorithm\\': \\'SAMME.R\\', \\'learning_rate\\': 0.1, \\'n_estimators\\': 300, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_abc.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\\\\n#submit_gbc = ensemble.GradientBoostingClassifier()\\\\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={\\'learning_rate\\': grid_ratio, \\'n_estimators\\': grid_n_estimator, \\'max_depth\\': grid_max_depth, \\'random_state\\':grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_gbc.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.25, \\'max_depth\\': 2, \\'n_estimators\\': 50, \\'random_state\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_gbc.predict(data_val[data1_x_bin])\\\\n\\\\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\\\\n#submit_xgb = XGBClassifier()\\\\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {\\'learning_rate\\': grid_learn, \\'max_depth\\': [0,2,4,6,8,10], \\'n_estimators\\': grid_n_estimator, \\'seed\\': grid_seed}, scoring = \\'roc_auc\\', cv = cv_split)\\\\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\\\\n#print(\\'Best Parameters: \\', submit_xgb.best_params_) #Best Parameters:  {\\'learning_rate\\': 0.01, \\'max_depth\\': 4, \\'n_estimators\\': 300, \\'seed\\': 0}\\\\n#data_val[\\'Survived\\'] = submit_xgb.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\\\\n#data_val[\\'Survived\\'] = vote_hard.predict(data_val[data1_x_bin])\\\\ndata_val[\\'Survived\\'] = grid_hard.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\\\\n#data_val[\\'Survived\\'] = vote_soft.predict(data_val[data1_x_bin])\\\\n#data_val[\\'Survived\\'] = grid_soft.predict(data_val[data1_x_bin])\\\\n\\\\n\\\\n#submit file\\\\nsubmit = data_val[[\\'PassengerId\\',\\'Survived\\']]\\\\nsubmit.to_csv(\\\\\"../working/submit.csv\\\\\", index=False)\\\\n\\\\nprint(\\'Validation Data Distribution: \\\\\\\\n\\', data_val[\\'Survived\\'].value_counts(normalize = True))\\\\nsubmit.sample(10)\\\\n\\\\n\",\"outputs\":[],\"metadata\":{\"_cell_guid\":\"057f369f-a3c6-4718-94f4-db8cb6d59777\",\"collapsed\":true,\"_uuid\":\"29ad86c5413910763316df7fda7753c4b0d0a97d\"},\"cell_type\":\"code\",\"execution_count\":null},{\"source\":\"<a id=\\\\\"ch12\\\\\"></a>\\\\n# Step 7: Optimize and Strategize\\\\n## Conclusion\\\\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\\\\n\\\\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \\\\n1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\\\\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\\\\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \\\\n\\\\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\\\\n\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6d34edc-1265-41e8-b163-0b817cd3b6a5\",\"_uuid\":\"c476df95925d67809975caca4ea4dcb4b2a0b277\"}},{\"source\":\"<a id=\\\\\"ch90\\\\\"></a>\\\\n## Change Log:\\\\n11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  \\\\n11/23/17 Cleaned up published notebook and updated through step 3.  \\\\n11/25/17 Added enhancements to published notebook and started step 4.  \\\\n11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  \\\\n12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  \\\\n12/3/17 Update section 4 with improved graphical statistics.  \\\\n12/7/17 Updated section 5 with Data Science 101 Lesson.  \\\\n12/8/17 Reorganized section 3 & 4 with cleaner code.  \\\\n12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  \\\\n12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  \\\\n12/11/17 Updated section 5 with better how-tos.  \\\\n12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  \\\\n12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  \\\\n12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for \\\\\"super\\\\\" model.  \\\\n12/23/17 Edited section 1-5 for clarity and more concise code.  \\\\n12/24/17 Updated section 5 with random_state and score for more consistent results.  \\\\n12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  \",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"973396d8-c37f-4f74-baec-a0c0653c975e\",\"_uuid\":\"4c6df5d0a4ecf018846830f1ab4310549977a51d\"}},{\"source\":\"<a id=\\\\\"ch91\\\\\"></a>\\\\n# Credits\\\\nProgramming is all about \\\\\"borrowing\\\\\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \\\\n\\\\n* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas M√ºller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn\\\\n* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization\\\\n* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5678e859-6610-4895-8fe8-0d28f0767b25\",\"_uuid\":\"b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9\"}},{\"source\":\"\\\\n\",\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"51f197c2-76d7-4503-ade5-f9f35323c4c5\",\"_uuid\":\"2e95a403934cf5c0ab1ff9148b62ef14590e84c9\"}}],\"nbformat\":4,\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"name\":\"python\",\"version\":\"3.6.3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\",\"mimetype\":\"text/x-python\"},\"kernelspec\":{\"name\":\"python3\",\"language\":\"python\",\"display_name\":\"Python 3\"}}}'}, {'role': 'assistant', 'content': 'REASONING: This is a comprehensive notebook. Let me record key findings and read the ensembling notebook as well. Key techniques I see:\\n1. Feature engineering: Title extraction from Name, Family size, Age binning\\n2. Multiple models tested: Decision Tree, Random Forest, AdaBoost, GradientBoosting, XGBoost, SVM, KNN, Logistic Regression\\n3. Voting classifiers (hard and soft voting)\\n4. Grid search for hyperparameter tuning\\n5. Best score achieved: ~0.77990 (about 78% accuracy)'}, {'role': 'tool', 'content': '‚úì Recorded finding: Key features for survival prediction: Sex (female survival ~74%), Pclass (1st class ~63% survival), ...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"e919d1161f20999e599ba1fd66a5a45b9c82f229\",\"_cell_guid\":\"bc64948f-5d6a-078d-085d-1beb58687bd3\"},\"cell_type\":\"markdown\",\"source\":\"# Introduction\\\\n\\\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\\\n\\\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\\\\n\\\\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron\\'s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\\\\n\\\\n[Stacking Starter][1] : by Faron \\\\n\\\\n\\\\nNow onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner.  My other standalone Kaggle [script][2] which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\\\\n  [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"14630296-b1aa-759e-bafa-b6a73f3896ed\",\"_uuid\":\"2e37a274400cfeb472b6405d524325245588dd66\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in our libraries\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport re\\\\nimport sklearn\\\\nimport xgboost as xgb\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\nimport plotly.offline as py\\\\npy.init_notebook_mode(connected=True)\\\\nimport plotly.graph_objs as go\\\\nimport plotly.tools as tls\\\\n\\\\nimport warnings\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n# Going to use these 5 base models for the stacking\\\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\\\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\\\nfrom sklearn.svm import SVC\\\\nfrom sklearn.cross_validation import KFold\",\"execution_count\":1,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8b590aafe06a2ac55daae9d2456155e457914f5f\",\"_cell_guid\":\"d647b74c-099b-851a-dcd2-3a58c9e8f10c\"},\"cell_type\":\"markdown\",\"source\":\"# Feature Exploration, Engineering and Cleaning \\\\n\\\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"5937fd72-d1ad-f678-cc82-f08a96e4cad0\",\"_uuid\":\"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Load in the train and test datasets\\\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\\\ntest = pd.read_csv(\\'../input/test.csv\\')\\\\n\\\\n# Store our passenger ID for easy access\\\\nPassengerId = test[\\'PassengerId\\']\\\\n\\\\ntrain.head(3)\",\"execution_count\":2,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"81378834770e55c76751347588322fe32acf5737\",\"_cell_guid\":\"dff1c1dd-1e59-3907-88fa-1a1d699122be\"},\"cell_type\":\"markdown\",\"source\":\"Well it is no surprise that our task is to somehow extract the information out of the categorical variables \\\\n\\\\n**Feature Engineering**\\\\n\\\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\\\n\\\\n[Titanic Best Working Classfier][1] : by Sina\\\\n\\\\n\\\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\"},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"e85b2a80-88a4-928f-f2b2-24895dea38f3\",\"_uuid\":\"b1c67249f91768ce8e5e2751364d32c87446cf55\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"full_data = [train, test]\\\\n\\\\n# Some features of my own that I have added in\\\\n# Gives the length of the name\\\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\\\n# Feature that tells whether a passenger had a cabin on the Titanic\\\\ntrain[\\'Has_Cabin\\'] = train[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\ntest[\\'Has_Cabin\\'] = test[\\\\\"Cabin\\\\\"].apply(lambda x: 0 if type(x) == float else 1)\\\\n\\\\n# Feature engineering steps taken from Sina\\\\n# Create new feature FamilySize as a combination of SibSp and Parch\\\\nfor dataset in full_data:\\\\n    dataset[\\'FamilySize\\'] = dataset[\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\\\n# Create new feature IsAlone from FamilySize\\\\nfor dataset in full_data:\\\\n    dataset[\\'IsAlone\\'] = 0\\\\n    dataset.loc[dataset[\\'FamilySize\\'] == 1, \\'IsAlone\\'] = 1\\\\n# Remove all NULLS in the Embarked column\\\\nfor dataset in full_data:\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].fillna(\\'S\\')\\\\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\\\\nfor dataset in full_data:\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].fillna(train[\\'Fare\\'].median())\\\\ntrain[\\'CategoricalFare\\'] = pd.qcut(train[\\'Fare\\'], 4)\\\\n# Create a New feature CategoricalAge\\\\nfor dataset in full_data:\\\\n    age_avg = dataset[\\'Age\\'].mean()\\\\n    age_std = dataset[\\'Age\\'].std()\\\\n    age_null_count = dataset[\\'Age\\'].isnull().sum()\\\\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\\\\n    dataset[\\'Age\\'][np.isnan(dataset[\\'Age\\'])] = age_null_random_list\\\\n    dataset[\\'Age\\'] = dataset[\\'Age\\'].astype(int)\\\\ntrain[\\'CategoricalAge\\'] = pd.cut(train[\\'Age\\'], 5)\\\\n# Define function to extract titles from passenger names\\\\ndef get_title(name):\\\\n    title_search = re.search(\\' ([A-Za-z]+)\\\\\\\\.\\', name)\\\\n    # If the title exists, extract and return it.\\\\n    if title_search:\\\\n        return title_search.group(1)\\\\n    return \\\\\"\\\\\"\\\\n# Create a new feature Title, containing the titles of passenger names\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Name\\'].apply(get_title)\\\\n# Group all non-common titles into one single grouping \\\\\"Rare\\\\\"\\\\nfor dataset in full_data:\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace([\\'Lady\\', \\'Countess\\',\\'Capt\\', \\'Col\\',\\'Don\\', \\'Dr\\', \\'Major\\', \\'Rev\\', \\'Sir\\', \\'Jonkheer\\', \\'Dona\\'], \\'Rare\\')\\\\n\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mlle\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Ms\\', \\'Miss\\')\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].replace(\\'Mme\\', \\'Mrs\\')\\\\n\\\\nfor dataset in full_data:\\\\n    # Mapping Sex\\\\n    dataset[\\'Sex\\'] = dataset[\\'Sex\\'].map( {\\'female\\': 0, \\'male\\': 1} ).astype(int)\\\\n    \\\\n    # Mapping titles\\\\n    title_mapping = {\\\\\"Mr\\\\\": 1, \\\\\"Miss\\\\\": 2, \\\\\"Mrs\\\\\": 3, \\\\\"Master\\\\\": 4, \\\\\"Rare\\\\\": 5}\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].map(title_mapping)\\\\n    dataset[\\'Title\\'] = dataset[\\'Title\\'].fillna(0)\\\\n    \\\\n    # Mapping Embarked\\\\n    dataset[\\'Embarked\\'] = dataset[\\'Embarked\\'].map( {\\'S\\': 0, \\'C\\': 1, \\'Q\\': 2} ).astype(int)\\\\n    \\\\n    # Mapping Fare\\\\n    dataset.loc[ dataset[\\'Fare\\'] <= 7.91, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 0\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 7.91) & (dataset[\\'Fare\\'] <= 14.454), \\'Fare\\'] = 1\\\\n    dataset.loc[(dataset[\\'Fare\\'] > 14.454) & (dataset[\\'Fare\\'] <= 31), \\'Fare\\']   = 2\\\\n    dataset.loc[ dataset[\\'Fare\\'] > 31, \\'Fare\\'] \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t        = 3\\\\n    dataset[\\'Fare\\'] = dataset[\\'Fare\\'].astype(int)\\\\n    \\\\n    # Mapping Age\\\\n    dataset.loc[ dataset[\\'Age\\'] <= 16, \\'Age\\'] \\\\t\\\\t\\\\t\\\\t\\\\t       = 0\\\\n    dataset.loc[(dataset[\\'Age\\'] > 16) & (dataset[\\'Age\\'] <= 32), \\'Age\\'] = 1\\\\n    dataset.loc[(dataset[\\'Age\\'] > 32) & (dataset[\\'Age\\'] <= 48), \\'Age\\'] = 2\\\\n    dataset.loc[(dataset[\\'Age\\'] > 48) & (dataset[\\'Age\\'] <= 64), \\'Age\\'] = 3\\\\n    dataset.loc[ dataset[\\'Age\\'] > 64, \\'Age\\'] = 4 ;\",\"execution_count\":3,\"outputs\":[]},{\"metadata\":{\"collapsed\":true,\"_execution_state\":\"idle\",\"_cell_guid\":\"8fc645ba-4b38-cea7-17b3-02175cb103d9\",\"_uuid\":\"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Feature selection\\\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\\\ntrain = train.drop(drop_elements, axis = 1)\\\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\\\ntest  = test.drop(drop_elements, axis = 1)\",\"execution_count\":4,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"dd288776321804d99e4e4a7e88594c1d631e4409\",\"_cell_guid\":\"e9814168-b7cd-d4e4-1b1d-e21c6637a663\"},\"cell_type\":\"markdown\",\"source\":\"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\\\n\\\\n## Visualisations \"},{\"metadata\":{\"_execution_state\":\"idle\",\"_cell_guid\":\"fc426b8f-873d-6f23-4299-99f174956cca\",\"_uuid\":\"1f280a1c11dc35a93b57af494938998e6d0b4544\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train.head(3)\",\"execution_count\":5,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3544b03040a1d691f6c48433c84d1e57f3c15e3d\",\"_cell_guid\":\"41102927-8218-415a-0b73-5129c8f5dd0c\"},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation Heatmap**\\\\n\\\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting packag\\n...(truncated)...\\nlosest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig,filename=\\'scatter2010\\')\",\"execution_count\":17,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74\",\"_cell_guid\":\"553828f0-c994-5ee1-695f-9373f11a1a7b\"},\"cell_type\":\"markdown\",\"source\":\"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\"},{\"metadata\":{\"_uuid\":\"f611812e2c9de3773df2264dfb2b13c0995807ac\",\"_cell_guid\":\"06847850-a829-0858-b12c-7b66e53e030a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Create the new column containing the average of values\\\\n\\\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\\\nfeature_dataframe.head(3)\",\"execution_count\":18,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1f0aff0896fef90b326fff7816393fef0e0cb992\",\"_cell_guid\":\"5645e647-c517-7822-f881-b8d7e38ef5da\"},\"cell_type\":\"markdown\",\"source\":\"**Plotly Barplot of Average Feature Importances**\\\\n\\\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\"},{\"metadata\":{\"_uuid\":\"0bd069388b419fe45306c01825aa3e6f5466ba2b\",\"_cell_guid\":\"63d86121-8c29-4b7f-b2ad-12b0a593f1d6\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y = feature_dataframe[\\'mean\\'].values\\\\nx = feature_dataframe[\\'features\\'].values\\\\ndata = [go.Bar(\\\\n            x= x,\\\\n             y= y,\\\\n            width = 0.5,\\\\n            marker=dict(\\\\n               color = feature_dataframe[\\'mean\\'].values,\\\\n            colorscale=\\'Portland\\',\\\\n            showscale=True,\\\\n            reversescale = False\\\\n            ),\\\\n            opacity=0.6\\\\n        )]\\\\n\\\\nlayout= go.Layout(\\\\n    autosize= True,\\\\n    title= \\'Barplots of Mean Feature Importance\\',\\\\n    hovermode= \\'closest\\',\\\\n#     xaxis= dict(\\\\n#         title= \\'Pop\\',\\\\n#         ticklen= 5,\\\\n#         zeroline= False,\\\\n#         gridwidth= 2,\\\\n#     ),\\\\n    yaxis=dict(\\\\n        title= \\'Feature Importance\\',\\\\n        ticklen= 5,\\\\n        gridwidth= 2\\\\n    ),\\\\n    showlegend= False\\\\n)\\\\nfig = go.Figure(data=data, layout=layout)\\\\npy.iplot(fig, filename=\\'bar-direct-labels\\')\",\"execution_count\":19,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bbb76d189e8d03921caaacfa9545cef894348c7d\",\"_cell_guid\":\"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0\"},\"cell_type\":\"markdown\",\"source\":\"# Second-Level Predictions from the First-level Output\"},{\"metadata\":{\"_uuid\":\"fed132782b73dda8d265065867e7f57c0aed7f50\",\"_cell_guid\":\"6b901750-ccdd-38ca-d8ea-1c361121ec4f\"},\"cell_type\":\"markdown\",\"source\":\"**First-level output as new features**\\\\n\\\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\"},{\"metadata\":{\"_uuid\":\"a5945e93337b87a1a8ee5580856768bbb14c07cd\",\"_cell_guid\":\"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"base_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\\\n    })\\\\nbase_predictions_train.head()\",\"execution_count\":20,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200\",\"_cell_guid\":\"f69c11db-d84e-8536-4c7e-382fbe67483e\"},\"cell_type\":\"markdown\",\"source\":\"**Correlation Heatmap of the Second Level Training set**\"},{\"metadata\":{\"_uuid\":\"9714ecaedf7385c5b8ad346ab909215eb9f2abc6\",\"_cell_guid\":\"4cf590ee-133f-6487-cf5a-53f346893d1c\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"data = [\\\\n    go.Heatmap(\\\\n        z= base_predictions_train.astype(float).corr().values ,\\\\n        x=base_predictions_train.columns.values,\\\\n        y= base_predictions_train.columns.values,\\\\n          colorscale=\\'Viridis\\',\\\\n            showscale=True,\\\\n            reversescale = True\\\\n    )\\\\n]\\\\npy.iplot(data, filename=\\'labelled-heatmap\\')\",\"execution_count\":21,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"4d9ef0298b568e43da6925f385403e0d77bd6e33\",\"_cell_guid\":\"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6\"},\"cell_type\":\"markdown\",\"source\":\"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"fef365199854ca3fff754399b4699d941b7e43b8\",\"_cell_guid\":\"6685fa11-497f-3fc2-ab1f-97f92d6eca61\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\",\"execution_count\":22,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c\",\"_cell_guid\":\"a02a94ab-3c9c-a824-7168-e964c5a0f5d5\"},\"cell_type\":\"markdown\",\"source\":\"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\"},{\"metadata\":{\"_uuid\":\"dc4a32e9a8e7c9e611124cba676e5d28240b38be\",\"_cell_guid\":\"628a03ea-933c-7075-a589-0ff7af237dfd\"},\"cell_type\":\"markdown\",\"source\":\"### Second level learning model via XGBoost\\\\n\\\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\\\n\\\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\\\n\\\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"5155d370069fe6de0fe5105309342ce55130dae8\",\"_cell_guid\":\"3a7c7517-b9a3-3a21-3a7b-299ca37c6843\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"gbm = xgb.XGBClassifier(\\\\n    #learning_rate = 0.02,\\\\n n_estimators= 2000,\\\\n max_depth= 4,\\\\n min_child_weight= 2,\\\\n #gamma=1,\\\\n gamma=0.9,                        \\\\n subsample=0.8,\\\\n colsample_bytree=0.8,\\\\n objective= \\'binary:logistic\\',\\\\n nthread= -1,\\\\n scale_pos_weight=1).fit(x_train, y_train)\\\\npredictions = gbm.predict(x_test)\",\"execution_count\":23,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0101e6b843f6378838874ccfb844ed464b81d627\",\"_cell_guid\":\"0a8152d8-6842-ed00-6bc5-47a511adce1c\"},\"cell_type\":\"markdown\",\"source\":\"Just a quick run down of the XGBoost parameters used in the model:\\\\n\\\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\\\n\\\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\\\n\\\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\"},{\"metadata\":{\"_uuid\":\"52ac0cd99cee0099d86a180127da42ff7fff960a\",\"_cell_guid\":\"6b4a5c81-e968-d41e-27e4-871481019867\"},\"cell_type\":\"markdown\",\"source\":\"**Producing the Submission file**\\\\n\\\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"9d607d829dbadd6c72ee01c9735a642435eb53e6\",\"_cell_guid\":\"f5a31787-5fe1-a559-bee9-ad6b6d83ae14\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# Generate Submission File \\\\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\\\n                            \\'Survived\\': predictions })\\\\nStackingSubmission.to_csv(\\\\\"StackingSubmission.csv\\\\\", index=False)\",\"execution_count\":24,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5\",\"_cell_guid\":\"1e56c738-b8f3-95e4-d642-c483f9757ed8\"},\"cell_type\":\"markdown\",\"source\":\"**Steps for Further Improvement**\\\\n\\\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\\\n\\\\nSome additional steps that may be taken to improve one\\'s score could be:\\\\n\\\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\\\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\"},{\"metadata\":{\"_uuid\":\"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e\",\"_cell_guid\":\"9a8f83fd-d0e8-035a-cf7f-25c9012e9373\"},\"cell_type\":\"markdown\",\"source\":\"### Conclusion\\\\n\\\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\\\n\\\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\\\n\\\\nTill next time, Peace Out\\\\n\\\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\"},{\"metadata\":{\"collapsed\":true,\"_uuid\":\"8549710f41f2ca55f11400e7914ed2f15c380964\",\"_cell_guid\":\"74641376-e124-e46f-0e7c-8cbee2a1321b\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"_change_revision\":0,\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.5\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This is an excellent stacking notebook. Key techniques:\\n1. Stacking: Use first-level predictions from multiple classifiers as features for a second-level model (XGBoost)\\n2. Base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC\\n3. Out-of-fold predictions for stacking\\n4. Feature engineering: Name_length, Has_Cabin, FamilySize, IsAlone, Title extraction\\n5. Age/Fare binning\\n6. Public LB score: 0.808 (80.8% accuracy)\\n\\nLet me record this and also check the data quickly to understand the structure.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) to ...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': 'REASONING: This notebook provides a comprehensive workflow. Let me record key findings and now create a quick EDA notebook to understand the data, then write the seed prompt.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Comprehensive feature engineering workflow: 1) Title extraction from Name (Mr, Mrs, Miss, Master, Ra...\\n  Source: ../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb'}, {'role': 'assistant', 'content': 'REASONING: Now let me quickly explore the data to understand its structure and then write the seed prompt. The target score is 1.0 (100% accuracy), which is extremely challenging. Let me create a quick EDA notebook.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv(\\'/home/data/train.csv\\')\\ntest = pd.read_csv(\\'/home/data/test.csv\\')\\n\\nprint(f\\'Train shape: {train.shape}\\')\\nprint(f\\'Test shape: {test.shape}\\')\\nprint(f\\'\\\\nTrain columns: {train.columns.tolist()}\\')\\nprint(f\\'\\\\nTarget distribution:\\\\n{train[\"Survived\"].value_counts(normalize=True)}\\')\\ntrain.head()\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: [\\'PassengerId\\', \\'Survived\\', \\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Ticket\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\']\\n\\nTarget distribution:\\nSurvived\\n0    0.616162\\n1    0.383838\\nName: proportion, dtype: float64\\n\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  '}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Check missing values and data types\\nprint(\"Missing values:\")\\nprint(train.isnull().sum())\\nprint(f\"\\\\nData types:\\\\n{train.dtypes}\")\\nprint(f\"\\\\n\\\\nSurvival by Sex:\")\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\nprint(f\"\\\\nSurvival by Pclass:\")\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n```\\nOutput:\\nMissing values:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nData types:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data:\\n- 891 training samples, 418 test samples\\n- Binary classification (Survived: 0 or 1)\\n- Target is imbalanced: 61.6% died, 38.4% survived\\n- Key features: Sex (female 74% survival vs male 19%), Pclass (1st class 63% vs 3rd class 24%)\\n- Missing values: Age (177), Cabin (687), Embarked (2)\\n- Features: Name (can extract Title), Ticket, Fare, SibSp, Parch (family size)\\n\\nThe target score is 1.0 (100% accuracy), which is extremely high. Let me now write the seed prompt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data characteristics: 891 train samples, 418 test samples. Binary classification (Survived). Target ...\\n  Source: exploration/eda.ipynb'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Wrote to /home/code/exploration/seed_prompt.txt'}, {'role': 'assistant', 'content': \"REASONING: I've written a comprehensive seed prompt. Let me also check if there are any additional kernels or techniques I might have missed, and search for any advanced techniques for achieving very high accuracy on this problem.\"}, {'role': 'tool', 'content': 'SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nTop‚Äëscoring Titanic solutions combine extensive feature engineering (e.g., extracting titles, family size, cabin presence, age bins and imputing missing ages), aggressive hyper‚Äëparameter tuning (grid/random/Bayesian search) and ensemble models such as soft‚Äëvoting or stacking of logistic regression, Random\\u202fForest and XGBoost, which together have achieved 81‚Äë84\\u202f% accuracy on the test set ([github.com](https://github.com/NickKaparinos/Kaggle-Titanic), [atlantis-press.com](https://www.atlantis-press.com/article/126015317.pdf), [python.plainenglish.io](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).\\n\\n--- Result 2 ---\\nTitle: GitHub - NickKaparinos/Kaggle-Titanic: Kaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\nURL: https://github.com/NickKaparinos/Kaggle-Titanic\\n\\nContent:\\n[Skip to content](https://github.com/NickKaparinos/Kaggle-Titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[NickKaparinos](https://github.com/NickKaparinos)/ **[Kaggle-Titanic](https://github.com/NickKaparinos/Kaggle-Titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n- [Star\\\\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE)\\n\\n[0\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers) [0\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks) [Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags) [Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\\n\\n# NickKaparinos/Kaggle-Titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[10 Commits](https://github.com/NickKaparinos/Kaggle-Titanic/commits/master/) |\\n| [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) | [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) |  |  |\\n| [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) | [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) |  |  |\\n| [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) | [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) |  |  |\\n| [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) | [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) |  |  |\\n| [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) | [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) |  |  |\\n| [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) | [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) |  |  |\\n| [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) | [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) |  |  |\\n| [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) | [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle-Titanic\\n\\n## Machine learning classification competition\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/titanic.jpg)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/titanic.jpg)\\n\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships\\n\\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Cross validation results\\n\\nAfter applying the appropriate preprocessing, each model was evaluated using 10-fold cross validation. Each model\\\\`s hyperparameters were tuned using extensive grid search. Afterwards, tuned models can be ensembled together for a boost in accuracy.\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/cv.png)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/cv.png)\\n\\n## Test set results\\n\\nUsing the optimal voting model, a test set accuracy of **0.8110** was achieved, which corresponds to position **349/50092** in the leaderboard (top 0.6%).\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results1.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results1.PNG)\\n\\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results2.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results2.PNG)\\n\\n## About\\n\\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\\n\\n### Topics\\n\\n[machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle) [classification](https://github.com/topics/classification)\\n\\n### Resources\\n\\n[Readme](https://github.com/NickKaparinos/Kaggle-Titanic#readme-ov-file)\\n\\n### License\\n\\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic#MIT-1-ov-file)\\n\\n[Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\\n\\n### Stars\\n\\n[**0**\\\\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers)\\n\\n### Watchers\\n\\n[**1**\\\\\\nwatching](https://github.com/NickKaparinos/Kaggle-Titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks)\\n\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNickKaparinos%2FKaggle-Titanic&report=NickKaparinos+%28user%29)\\n\\n## [Releases](https://github.com/NickKaparinos/Kaggle-Titanic/releases)\\n\\nNo releases published\\n\\n## [Packages\\\\ 0](https://github.com/users/NickKaparinos/packages?repo_name=Kaggle-Titanic)\\n\\nNo packages published\\n\\n## Languages\\n\\n- [Python100.0%](https://github.com/NickKaparinos/Kaggle-Titanic/search?l=python)\\n\\nYou can‚Äôt perform that action at this time.\\n\\n--- Result 3 ---\\nURL: https://www.atlantis-press.com/article/126015317.pdf\\n\\nContent:\\nTitanic Survival Prediction Enhanced by Innovative \\nFeature Engineering and Multi-Model Ensemble \\nOptimization \\nHanzhi Li \\n1 School of Software Engineering, South China University of Technology, Guangzhou, \\nGuangdong, 510006, China \\n202230321188@mail.scut.edu.cn\\nAbstract. This study enhances Titanic survival prediction through advanced \\nfeature engineering and ensemble model optimization. The Titanic dataset \\npresents a classic binary classification problem requiring the prediction of \\npassenger survival based on demographic and ticket information. Our \\nmethodology employs systematic preprocessing where missing values are \\nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \\npredictions for age values. Novel features were extracted from passenger \\nnames, cabin information, and family relationships to improve predictive \\npower. Feature importance was evaluated using Random Forest and XGBoost \\nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \\nfor effective feature selection. Three classification algorithms‚Äîlogistic \\nregression, random forest, and XGBoost‚Äîwere systematically optimized using \\ngrid search, random search, and Bayesian optimization techniques. Based on \\nthese optimized models, soft voting and stacking ensemble approaches were \\nimplemented. Cross-validation results demonstrate that ensemble methods \\nachieved superior accuracy (0.8361) compared to individual models: logistic \\nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \\nresearch provides valuable optimization strategies for similar classification \\ntasks, particularly highlighting how feature engineering combined with \\nensemble methods can substantially enhance predictive performance while \\nmaintaining computational efficiency. \\nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \\nLearning, Hyperparameter Tuning. \\n1 Introduction \\nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \\nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \\ntwentieth century. Its historical significance and the multidimensional nature of \\npassenger data (including social class, gender, age, family relationships, etc.) have \\nestablished it as a classic binary classification benchmark dataset in machine learning. \\n¬© The Author(s) 2025\\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\\nhttps://doi.org/10.2991/978-94-6463-823-3_19\\n208 H. Li\\nDespite extensive research on this problem, traditional methods exhibit \\nsignificant limitations. Early studies predominantly focused on the direct application \\nof basic features, achieving prediction accuracy generally below 80% [1]. For \\ninstance, simple rules based on gender and passenger class could rapidly distinguish \\nsurvival groups. However, these methods failed to fully explore nonlinear interactions \\nbetween features and higher-order features. Moreover, approximately 20% of the age \\nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \\napproaches often rely on mean imputation or directly delete missing values, leading to \\ninformation loss and model bias [2]. \\nAccording to previous research, traditional classification methods applied to the \\nTitanic dataset achieved limited accuracy. These methods were constrained by their \\ninability to capture complex feature interactions and their sensitivity to missing values \\n[2]. Simultaneously, traditional methods predominantly depend on single models such \\nas logistic regression and decision trees without conducting feature importance \\nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \\nensemble models but have not systematically compared their performance with single \\nmodels, nor explored how different hyperparameter tuning methods influence \\nprediction model performance in this dataset. \\nTo address these challenges, this study employs advanced feature engineering \\ntechniques to extract new features, utilizes the relationships between features and \\neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \\nselection techniques like random forests and XGBoost to identify key features for \\nsurvival prediction, and performs hyperparameter tuning of logistic regression, \\nrandom forests, and XGBoost models using grid search, random search, and Bayesian \\noptimization. Based on these methods, soft voting and stacking ensemble models are \\ntrained, and the performance of models under various parameter conditions is further \\ncompared. \\n2 Methods \\n2.1 DataSet \\nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \\ndataset consists of two CSV files: the training set and the test set. The training set \\ncontains 891 samples, while the test set contains 418 samples. The data features \\ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \\nEmbarked. Additionally, the training set includes an additional target variable, \\n\"Survived\". Analysis reveals missing values in the Age, Cabin, and Embarked \\ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \\nAge and Cabin having a relatively high proportion of missing values [2]. \\n2.2 Data Processing \\nTo facilitate data processing, this study first combined the training set and the test set\\nto form a complete dataset, called the \"merged set\". The missing values in the\\nTitanic Survival Prediction Enhanced by Innovative Feature ‚Ä¶ 209\\nEmbarked and Fare columns, which had relatively few missing values, were filled \\nfirst. By analyzing the sample features corresponding to the missing values in \\nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \\nCabin of B28, and a Sex of female. Considering the correlations between features, a \\nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \\nassumption, box plots of Fare distribution under different combinations of Embarked \\nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \\nPclass is 1, Embarked is more likely to be \\'C\\'. Therefore, the missing Embarked \\nvalues were filled with \\'C\\'. Similarly, for cases where Embarked = \\'S\\' and Pclass = 3, \\nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \\nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \\nNext, the Cabin column was processed. Due to the high number of missing values, the \\nanalysis was based only on available information. Records with Cabin information \\nwere filtered, and the distribution of cabin classes between survivors and \\nnon-survivors was compared. Cabin values containing multiple cabins were split into \\nlists, and the data was expanded row-wise to generate new features. Two new features \\nwere extracted: Deck (the letter part, filled with \"U\" if none) and Cabin number (the \\nnumeric part, converted to a numeric type). The Cabin number was further binned, \\nand survival rates within different intervals were analyzed [5]. The analysis revealed \\nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \\napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \\naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \\nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \\nsurvival rates for each Deck were calculated, which showed notable variations: Deck \\nB had the highest survival rate at approximately 74%, followed by Decks D and E at \\naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \\nrate at about 30%. Finally, the expanded data was re-aggregated into the original \\nmerged dataset (Fig. 1). \\nThe Title (such as \"Mr\", \"Mrs\") was extracted from the Name column as a new\\nfeature. Rare Titl...\\n\\n--- Result 4 ---\\nTitle: Space Titanic Kaggle competition 0.8066 score solution\\nURL: https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6\\n\\nContent:\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# Spaceship Titanic Kaggle competition top 7% score solution\\n\\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\\n\\n7 min read\\n\\n¬∑\\n\\nJan 17, 2024\\n\\n--\\n\\nListen\\n\\nShare\\n\\nPress enter or click to view image in full size\\n\\n## Introduction\\n\\nKaggle‚Äôs Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an ‚Äúanomaly‚Äù using records recovered from the spaceship‚Äôs damaged computer system. The ‚Äúlore‚Äù of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship‚Äôs passengers.\\n\\n## Import libraries\\n\\n```\\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\\n```\\n\\n## Let‚Äôs Start\\n\\nAs you can see, after I loaded the dataset, I removed both the ‚ÄúPassenger Id‚Äù and ‚ÄúName‚Äù columns. They are not going to provide any useful or important information to the prediction. Someone‚Äôs name or Id does not change the probability of being Transported.\\n\\n```\\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\\n```\\n\\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the ‚ÄúCabin‚Äù column. Notice that the rows on the ‚ÄúCabin‚Äù column follow a specific pattern. Something like this: ‚ÄúA/5/S‚Äù, ‚ÄúC/1/S‚Äù, ‚ÄúF/7/P‚Äù. And I decided to investigate it. So, to make things simple I split the rows of the ‚ÄúCabin‚Äù into three columns based on both slashes (‚Äù/‚Äù) of the rows. For example, the ‚ÄúA/5/S‚Äù row would be transformed into three new columns: The first one is named ‚Äúcabin\\\\_code‚Äù referring tothe character behind the first slash (A). The second one named ‚Äúid\\\\_cabin‚Äù refers to the character behind the second slash (5). The third one named ‚Äúcabin\\\\_sector‚Äù refers to the character after the second slash (S). And we end up with three new columns.\\n\\n```\\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\\n```\\n\\nFirst of all, I noticed that ‚Äúcabin\\\\_code‚Äù only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\\n\\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\\n\\nPress enter or click to view image in full size\\n\\nI did the same thing with the ‚Äúcabin\\\\_sector‚Äù column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\\n\\nPress enter or click to view image in full size\\n\\nThis means that this exploration of the original ‚ÄúCabin‚Äù column is worth it since new insights are being added to the model.\\n\\nNow, we can finally delete the ‚ÄúCabin‚Äù column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\\n\\nI also removed the ‚Äúcabin\\\\_id‚Äù and the column that I had created. As I said before, the Id will not interfere with the model‚Äôs predictive ability.\\n\\nSo used: df.drop(columns=\\\\[‚ÄúCabin‚Äù,‚Äùid\\\\_cabin‚Äù\\\\], inplace=True) to drop both columns\\n\\nBefore splitting our data, the ‚ÄúTransported‚Äù column must be in a binary format. As you can see, I switched ‚ÄúTrue‚Äù for 1 and ‚ÄúFalse‚Äù for 0.\\n\\nBinary transformation: df\\\\[‚ÄúTransported‚Äù\\\\] = df\\\\[‚ÄúTransported‚Äù\\\\].map({True:1, False:0})\\n\\nI also removed every row that had missing values in the ‚Äúcabin\\\\_code‚Äù column.\\n\\n```\\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\\n```\\n\\nNow, we can finally split the data and proceed to develop our model.\\n\\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\\n\\n```\\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\\n```\\n\\n```\\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\\n```\\n\\n```\\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == \\'object\\'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\\n```\\n\\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don‚Äôt know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\\n\\n```\\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\\n```\\n\\nAnd with column transformer, we can attach both transformations to one variable that I named ‚Äútransformer‚Äù. Observe that we also have to specify the type of data to which the pipeline will be applied t...\\n\\n--- Result 5 ---\\nTitle: Titanic dataset Analysis (~80% accuracy) - Analytics Vidhya - Medium\\nURL: https://medium.com/analytics-vidhya/titanic-dataset-analysis-80-accuracy-9480cf3db538\\n\\nContent:\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9480cf3db538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-9480cf3db538---------------------------------------)\\n\\n¬∑\\n\\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\\n\\n# Titanic dataset Analysis (~80% accuracy)\\n\\n[Aishani Basu](https://medium.com/@aishani-81621?source=post_page---byline--9480cf3db538---------------------------------------)\\n\\n11 min read\\n\\n¬∑\\n\\nNov 25, 2021\\n\\n--\\n\\n2\\n\\nListen\\n\\nShare\\n\\n[Source](https://ornjhonnywingsfood.blogspot.com/2021/08/titanic-rms-titanic-wikiwand-murdoch.html)\\n\\n## Introduction\\n\\nThe Titanic or, in full, [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) was part of the one of the most iconic tragedies of all time. RMS Titanic was a British passenger ship that hit an iceberg while on its voyage from Southampton to New York City, and sank in the North Atlantic ocean, leaving hundreds of passengers to die in the aftermath of the deadly incident. Some of the passengers who survived till help arrived were rescued while many lost their lives helplessly waiting for help to arrive.\\n\\nThe legendary Kaggle problem, [Titanic](https://www.kaggle.com/c/titanic), based on the tragedic sinking of the RMS Titanic records data about 891 passengers of RMS Titanic, and we are required to predict if they have survived or not, based on the information we have available about the passengers and the outcome after the sinking of the ship.\\n\\nNote: This notebook is my analysis of the titanic dataset to obtain any meaningful insights from the data and scores an accuracy of ~80 percent (top 5 percent of 14k entries on Kaggle) .\\n\\nLet‚Äôs get started!\\n\\n## Contents:\\n\\n1\\\\. About the data\\n\\n2\\\\. Problem definition and metrics\\n\\n3\\\\. EDA\\n\\n4\\\\. Baseline model performance\\n\\n5\\\\. Model Selection\\n\\n6\\\\. Results and conclusion\\n\\n## About the data:\\n\\nFirst, we have the prediction variable, that is if they survived the sinking or not. Then a bunch of numeric variables, like ids, ages of the passengers, etc. followed by categorical variables like class of the ticket, and strings like name, etc.\\n\\nPress enter or click to view image in full size\\n\\nExample RMS Titanic ticket : [Source](https://www.encyclopedia-titanica.org/titanic-tickets/)\\n\\nPrinting first 5 rows of given dataset..\\n\\n## Problem definition and metrics\\n\\nThe problem is a **binary class classification** problem. We can use **binary crossentropy** or logistic loss as the loss function and any metric like **accuracy** or/and **ROC AUC score** as a metric to evaluate the results.\\n\\n## EDA\\n\\n### Correlation between features:\\n\\nDrawing insights from the correlation between features..\\n\\n- Fare and Pclass are negatively correlated (with a Pearson‚Äôs correlation coefficient of -0.55) ; Obviously, **higher fares imply better better ticket classes** (lower the class number) and vice versa.\\n- Pclass and the target feature are moderately negatively correlated (-0.34) implying **better the ticket class, higher the chance of survival.**\\n- A similar pattern can be observed with the features Parch, SibSp and Age; Both the features, Parch and SibSp are very slightly correlated with all the features except Age feature, and both have a negative correlation (-0.19 and -0.31 respectively) with Age ; **Lower the age, more the number of family accompanying the passenger**\\n- SibSp and Parch features are positively correlated which are both indicative of the number of family members accompanying passenger\\n- PClass and Age are negatively correlated (-0.37) implying **higher the age, better the ticket class**\\n\\n### Missing data\\n\\n**Age** : Contains 177 nan values out of 891 entries. Imputed with median gave best results.\\n\\n**Embarked** : Contains 2 nan values. Imputed with mode of existing data.\\n\\n**Cabin** : 687 out of 891 Cabin entries are nans, i.e. more than 50 percent of the total data exists as missing data or Nans so assumed its better to derive other features from this feature.\\n\\nNow, printing the correlation heatmap after handling missing data and converting categorical strings to encodings (0,1,2..)..\\n\\n- Embarked also has slight correlation with the target variable (-0.17), the **port at which the ship was boarded by the passengers did determine their chance of survival**\\n- Sex is also highly correlated with the target variable (-0.54), indicating the **passenger‚Äôs gender had a high effect on their chance of survival**\\n- Embarked and Fare of the passengers are negatively correlated (-0.22); obviously, fare depends on the port a passenger boards the ship from.\\n- Embarked and Sex also seem slightly correlated (0.11) indicating the **port the passenger boarded from was dependent on the gender of the passenger**; Embarked and Pclass are also correlated (0.16) indicating a **1st class passenger probably boarded at a different port than a 3rd class passenger**.\\n- Sex and Fare, Sex and Parch, SibSp, Sex and Pclass all seem slightly correlated (-0.18, -0.25, -0.11, 0.13 respectively); i.e the **fare the passenger paid, the family aboard, and the class of the ticket was seem slightly dependent on the gender of the passenger**.\\n\\n**Question 1: How do the features ‚ÄòAge‚Äô, ‚ÄòSex‚Äô, ‚ÄòFare‚Äô, ‚ÄòEmbarked‚Äô affect the chance of a passenger‚Äôs survival?**\\n\\nAge :\\n\\nPress enter or click to view image in full size\\n\\n- Both distributions, of people who didn‚Äôt survive and the ones who did, are normal with a spike around age 30 for passengers who did not survive i.e. for **people around the age 30 had a higher chance of not surviving**\\n- People of ages > ~51 are considered outliers in the distribution of people who did not survive. owing to the fact **people above the age of ~51 are very less** and people of age ~55 above are outliers in the distribution of people who did survive, i.e. **very few people above ~55 actually did survive**\\n\\nPress enter or click to view image in full size\\n\\nSex:\\n\\nPress enter or click to view image in full size\\n\\n- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard\\n\\nFare :\\n\\nPress enter or click to view image in full size\\n\\n- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\\n- Lots of outliers in the distribution of both\\n- A huge spike in the distribution of the people who didn‚Äôt survive, or the probability that the person who didn‚Äôt survive had a fare of ~10 units of currency is very high\\n\\nEmbarked:\\n\\nPress enter or click to view image in full size\\n\\n- Maximum passengers seem to have boarded the ship at Southampton port and only 33 percent of them survived; out of the total passengers who boarded the ship at Queenstown, a simi...\\n\\n--- Result 6 ---\\nTitle: Search code, repositories, users, issues, pull requests...\\nURL: https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\\n\\nContent:\\nGitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[Skip to content](#start-of-content)\\n## Navigation Menu\\nToggle navigation\\n[](https://github.com/)\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\n</option></form>\\nSearch\\nClear\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n# Provide feedback\\n</option></form>\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancelSubmit feedback\\n# Saved searches\\n## Use saved searches to filter your results more quickly\\n</option></form>\\nName\\nQuery\\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\nCancelCreate saved search\\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nAppearance settings\\nResetting focus\\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\\n{{ message }}\\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\\\\\\\~72% to \\\\\\\\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\\nmain\\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\\nGo to file\\nCode\\nOpen more actions menu\\n## Folders and files\\n|Name|Name|\\nLast commit message\\n|\\nLast commit date\\n|\\n## Latest commit\\n## History\\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\\n|\\n|\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\\n|\\n|\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\\n|\\n|\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\\n|\\n|\\n|\\nView all files\\n|\\n## Repository files navigation\\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\~72% to \\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\\n# Titanic Survival Prediction\\n[](#titanic-survival-prediction)\\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\\n* **Detailed EDA**\\nA Jupyter notebook that walks through exploratory data analysis:\\n* Summary statistics\\n* Missing-value patterns\\n* Feature distributions and pairwise relationships\\n* Correlation heatmaps\\n* **Feature Engineering**\\nCreation of high-signal features from raw inputs, including:\\n* `Has\\\\_Cabin`(binary cabin indicator)\\n* `FareBin`(quantile-based fare categories)\\n* Group-median imputation for`Age`, then 10-year age bins\\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\\n* `Title`extracted and consolidated from passenger names\\n* **Model Comparison**\\nTraining and evaluation of multiple classifiers on the engineered feature set:\\n* Random Forest\\n* Support Vector Machine\\n* K-Nearest Neighbors (k=3,5,7)\\n* XGBoost\\n* LightGBM\\nResults are compared side-by-side with a clean bar chart, and each model‚Äôs strengths and weaknesses are discussed in detailed Markdown cells.\\n* **Analysis &amp; Interpretation**\\nInline Markdown commentary explains:\\n* Why each feature matters\\n* How missing data were handled\\n* The rationale for model selection\\n* Key takeaways and next-step recommendations\\n## Repository Structure\\n[](#repository-structure)\\ndata/\\n* train.csv # Official Kaggle training set\\n* test.csv # Official Kaggle test set\\nnotebooks/\\n* titanic\\\\_analysis.ipynb\\n‚Ä¢EDA, feature engineering, model training\\n‚Ä¢Detailed Markdown analysis\\n* README.md # Project overview and instructions\\n## Contributing\\n[](#contributing)\\nFeel free to open issues, fork the repository, or submit pull requests with enhancements‚Äîwhether it‚Äôs new feature ideas, alternative modeling approaches, or visualization improvements.\\n## About\\nPredict Titanic survival using t...\\n\\n--- Result 7 ---\\nTitle: GitHub - chrischow/kaggle_titanic: Kaggle Competition - Titanic Dataset\\nURL: https://github.com/chrischow/kaggle_titanic\\n\\nContent:\\n[Skip to content](https://github.com/chrischow/kaggle_titanic#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/chrischow/kaggle_titanic) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[chrischow](https://github.com/chrischow)/ **[kaggle\\\\_titanic](https://github.com/chrischow/kaggle_titanic)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n- [Star\\\\\\n1](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n\\nKaggle Competition - Titanic Dataset\\n\\n[1\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers) [0\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks) [Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags) [Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n[Star](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic)\\n\\n[Notifications](https://github.com/login?return_to=%2Fchrischow%2Fkaggle_titanic) You must be signed in to change notification settings\\n\\n# chrischow/kaggle\\\\_titanic\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/chrischow/kaggle_titanic/branches) [Tags](https://github.com/chrischow/kaggle_titanic/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[33 Commits](https://github.com/chrischow/kaggle_titanic/commits/master/) |\\n| [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) | [Modules](https://github.com/chrischow/kaggle_titanic/tree/master/Modules) |  |  |\\n| [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) | [Notebooks](https://github.com/chrischow/kaggle_titanic/tree/master/Notebooks) |  |  |\\n| [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) | [README.md](https://github.com/chrischow/kaggle_titanic/blob/master/README.md) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# Kaggle Competition - Titanic Dataset\\n\\nThis Kaggle competition requires participants to predict whether passengers will survive.\\n\\n## Introduction\\n\\nThis readme describes the entire process I used for the Kaggle Titanic competition. The notebooks are named in pseudo-chronological order. I say \"pseudo\" because whenever I revisited a process, I edited the relevant existing notebook without creating a new one. Hence, some code and results in earlier notebooks may not have been used in the next few notebooks, but were used at some point in the entire project.\\n\\n## Exploratory Data Analysis\\n\\nFirst, I performed exploratory data analysis (EDA) on all features in the dataset ( [1\\\\. EDA.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/1.%20EDA.ipynb)). Concurrently, I cleaned the data and generated new features.\\n\\n## Modelling\\n\\n### Random Forest ( `sklearn`)\\n\\nNext, I experimented with the `sklearn` implementation of Random Forest ( [2\\\\. Random Forest - Low Depth.ipynb](https://github.com/chrischow/kaggle_titanic/blob/master/Notebooks/2.%20Random%20Forest%20-%20Low%20Depth.ipynb)). By the final iteration in the optimisation process, the accuracy was **82.47%**. However, the `sklearn` implementation ( `RandomForestClassifier`) was extremely slow. Hence, I switched over to the `lightgbm` implementation of the Random Forest algorithm.\\n\\n### Random Forest ( `lightgbm`)\\n\\nThe `lightgbm` implementation of the Random Forest algorithm was **much** faster. The subsequent three notebooks captured my attempts at figuring out the best sequence for the tuning of parameters. The best approach was to:\\n\\n1. Tune the number of trees/estimators\\n2. Perform Recursive Feature Elimination (RFE) with cross-validation (CV)\\n3. Tune the proportion of observations sampled\\n4. Tune the maximum features used per split\\n5. Tune the minimum samples in each terminal node\\n6. Tune alpha, the L1 regularisation parameter\\n7. Tune lambda, the L1 regularisation parameter\\n\\nThis tuning sequence resulted in a 5-fold CV test score of **83.46%**.\\n\\n### Regenerating Features\\n\\nThen, I realised that I made a mistake in the feature generation process. I had generated encoded features using the entire training set. This resulted in contamination, which meant that the pseudo-test sets in CV contained information about the target, and this could have inflated the test scores. Instead, I should have (1) performed data cleaning on the competition training and test sets together, (2) encoded the training set while saving the mappings, and (3) performed mappings on the test set. Hence, I created the [`kaggle_titanic` module](https://github.com/chrischow/kaggle_titanic/blob/master/Modules/kaggle_titanic.py) to perform these three data cleaning functions, and many other functions for machine learning.\\n\\n### Model Tuning I - Improper Cross Validation\\n\\nI created pipelines for performing feature selection for and tuning the various models:\\n\\n1. LightGBM RF with low depth\\n2. LightGBM RF with high depth\\n3. LightGBM GBM with low depth\\n4. LightGBM GBM with high depth\\n5. XGBoost GBM with low depth\\n6. XGBoost GBM with high depth\\n7. Logistic Regression\\n8. Ridge Regression\\n9. Linear Support Vector Classifier (SVC)\\n\\nHaving completed the tuning for the above models, I discovered another source of contamination: in the cross validation process. I was using cross validation to both (1) tune hyperparameters and (2) score models. These two processes should have been separate. The correct way to have performed cross validation was:\\n\\n1. Split data into **meta folds** (e.g. 5 repeats of 5 folds)\\n2. For each **meta fold**:\\na. Split training data into folds (e.g. 5 repeats of 5 folds)\\nb. Run feature selection and hyperparameter tuning algorithms using cross validation\\nc. Score the model on the test set of the **meta fold**\\nd. Move on to next **meta fold** and repeat\\n3. Evaluate results for all **meta folds**\\n\\nThe first important thing I learned was that although feature selection and parameter tuning require cross validation, **all processes from data cleaning to feature generation to feature selection to hyperparameter tuning** require cross validation as well. In other words, **cross validation should be used to validate the full approach**. My mistake was that I saw hyperparameter tuning with cross validation as the end stage of testing. This was wrong. The second important thing I learned was that it was better to place feature selection before hyperparameter tuning. This is because changes in hyperparameter levels do not change how important or unimportant the features are. We only need the initial parameters for running RFE to not prevent the algorithm from letting the important features surface. For example, in tree-based models, increasing the number of trees, depth, or features sampled per tree would help to ensure that sufficient features are evaluated. The tradeoff to increasing these hyperparameters would be the computational cost. These two objectives should be balanced well.\\n\\n## About\\n\\nKaggle Competition - Titanic Dataset\\n\\n### Resources\\n\\n[Readme](https://github.com/chrischow/kaggle_titanic#readme-ov-file)\\n\\n[Activity](https://github.com/chrischow/kaggle_titanic/activity)\\n\\n### Stars\\n\\n[**1**\\\\\\nstar](https://github.com/chrischow/kaggle_titanic/stargazers)\\n\\n### Watchers\\n\\n[**2**\\\\\\nwatching](https://github.com/chrischow/kaggle_titanic/watchers)\\n\\n### Forks\\n\\n[**0**\\\\\\nforks](https://github.com/chrischow/kaggle_titanic/forks)\\n\\n[Report repository](https:...\\n\\n--- Result 8 ---\\nTitle: Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\\nURL: https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c\\n\\nContent:\\nBuilding a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------\\n\\n[... truncated 15258 characters ...]'}]",
        "llm.input_tokens": 49553,
        "llm.output_tokens": 272,
        "llm.total_tokens": 49825,
        "llm.latency_ms": 8859.20262336731,
        "llm.completion": "REASONING: The search confirms that top solutions achieve 81-84% accuracy using ensemble methods, feature engineering, and hyperparameter tuning. The target of 1.0 (100%) is extremely challenging. Let me record this finding and update the seed prompt with additional techniques.\n[Tool Use: RecordFinding]",
        "llm.reasoning": "REASONING: The search confirms that top solutions achieve 81-84% accuracy using ensemble methods, feature engineering, and hyperparameter tuning. The target of 1.0 (100%) is extremely challenging. Let me record this finding and update the seed prompt with additional techniques.",
        "llm.cost_usd": 0.15273899999999999
      }
    },
    {
      "trace_id": "6dcdebd881343d793c91292a4bb43d2e",
      "span_id": "d5ae8a2dad3493c5",
      "parent_span_id": "4d7ad84c75cfb7df",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767680724350582832,
      "end_time": 1767680724351851076,
      "duration": 0.001268244,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7db40efea250>\", \"input\": {\"finding\": \"Top Titanic solutions achieve 81-84% accuracy (not 100%). Best techniques: 1) XGBoost for Age imputation, 2) Novel features from Cabin (Deck letter, cabin number bins), 3) Soft voting and stacking ensembles, 4) Bayesian optimization for hyperparameters, 5) Feature selection with SelectFromModel and RFE. Cross-validation shows ensemble methods achieve 0.8361 vs individual models (LR: 0.8339, RF: 0.8350, XGB: 0.8204).\", \"source\": \"Web search: highest accuracy Titanic Kaggle\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Top Titanic solutions achieve 81-84% accuracy (not 100%). Best techniques: 1) XGBoost for Age imputa...\\n  Source: Web search: highest accuracy Titanic Kaggle\", \"is_error\": false}",
        "tool.duration_ms": 1.3103485107421875
      }
    }
  ],
  "metadata": {
    "span_count": 38,
    "last_updated": "2026-01-06T06:25:24.469279"
  }
}