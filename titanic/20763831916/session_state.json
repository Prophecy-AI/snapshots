{
  "workspace_dir": "/home/code",
  "competition_id": "titanic",
  "metric_direction": false,
  "start_time": "2026-01-06T22:22:32.078089",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Key feature engineering: Title extraction from Name (Mr, Mrs, Miss, Master, Rare), FamilySize = SibSp + Parch + 1, IsAlone flag, Has_Cabin flag, Name_length, Age/Fare binning into categories",
      "source": "../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Stacking approach: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) with out-of-fold predictions, then XGBoost as second-level meta-learner. Achieved 0.808 public LB score (top 9%)",
      "source": "../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/introduction-to-ensembling-stacking-in-python.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model comparison: Tested 23+ algorithms including ensemble methods (AdaBoost, Bagging, ExtraTrees, GradientBoosting, RandomForest), GLM (LogisticRegression), SVM, KNN, NaiveBayes, DecisionTree, XGBoost. Voting classifier (hard/soft) used to combine predictions.",
      "source": "../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/a-data-science-framework-to-achieve-99-accuracy.ipynb",
      "agent": "explorer"
    }
  ],
  "web_research": [],
  "max_submissions": 10,
  "remaining_submissions": 8
}