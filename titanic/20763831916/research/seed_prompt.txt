# Titanic Survival Prediction - Loop 3 Strategy

## Current Status
- Best CV score: 0.8316 from exp_000 (Baseline XGBoost)
- Best LB score: 0.7584 from exp_000
- CV-LB gap: +7.3% â†’ **SEVERE OVERFITTING**
- Latest experiment: exp_001 (Simple RF) CV 0.8238, NOT SUBMITTED
- Submissions used: 1/10 (7 remaining)

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY** - execution is sound
- Evaluator's top priority: "Submit exp_001 to validate hypothesis" - **AGREE**
- Key concerns raised:
  1. Experiment not submitted - can't validate simpler model hypothesis
  2. Title feature dropped - loses valuable signal
  3. Survival rate too low (31.3% vs 38.4% training)
  4. Ensemble methods unexplored
- **My synthesis**: The evaluator is correct. We need to:
  1. Submit exp_001 to get LB feedback (validates hypothesis)
  2. Implement voting ensemble (proven approach)
  3. Consider adding Title back (captures sex + social status)

## CRITICAL: Target is Impossible
**The target of 1.0 (100% accuracy) is fundamentally impossible for Titanic.**
- State-of-the-art: 81-85% accuracy
- Top 0.6% achieved 81.1% with voting ensemble
- 100% accuracy would require perfect knowledge of who survived

**Realistic goals:**
- Beat 0.7584 (current LB): Achievable
- Reach 0.78-0.80: Good progress
- Reach 0.81-0.82: Excellent (top 10%)
- Reach 0.83-0.85: State-of-the-art (unlikely without extensive effort)

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, survival patterns
- `exploration/evolver_loop1_lb_feedback.ipynb` - CV-LB gap analysis
- `exploration/evolver_loop2_analysis.ipynb` - Prediction comparison

Key patterns:
- Sex is dominant: Female 74.2% survival vs Male 18.9%
- Pclass matters: 1st class 63%, 3rd class 24%
- Title captures both sex AND social status (Mr vs Master, Mrs vs Miss)
- FamilySize: Medium families (2-4) have highest survival
- Distribution shift: Embarked C is 24.4% in test vs 18.9% in train

## Recommended Approaches (Priority Order)

### Priority 1: Voting Ensemble with Simple Features
**Rationale**: Ensembles are proven to work (0.808 LB in kernels). Diversity reduces overfitting.

**Action**:
- Use 7-8 features: Pclass, Sex_Code, Age, SibSp, Parch, Fare, Embarked_Code, Title_Code
- Combine diverse models: RandomForest, LogisticRegression, GradientBoosting, SVC
- Use soft voting (probability averaging)
- Expected CV: ~0.82-0.83, Expected LB: ~0.78-0.80

### Priority 2: Add Title Feature Back
**Rationale**: Title was dropped in exp_001 but has 0.08 importance. It captures:
- Sex (Mr/Master = male, Mrs/Miss = female)
- Social status (Master = young boy, Mr = adult male)
- Marital status (Mrs vs Miss)

**Action**:
- Extract Title from Name using regex
- Map to categories: Mr, Mrs, Miss, Master, Rare
- Add to feature set (now 8 features)

### Priority 3: Stacking Ensemble
**Rationale**: More sophisticated than voting, achieved 0.808 LB in kernels.

**Action**:
- Level 1: RF, ExtraTrees, AdaBoost, GradientBoosting, SVC
- Generate out-of-fold predictions (5-fold CV)
- Level 2: XGBoost or LogisticRegression on stacked features
- This is more complex - try after voting ensemble

### Priority 4: Hyperparameter Tuning
**Rationale**: Fine-tuning after ensemble is established.

**Action**:
- GridSearchCV on best performing models
- Focus on regularization parameters
- Only after ensemble baseline is working

## What NOT to Try
- More complex single models (will overfit)
- More than 10-12 features (noise risk)
- Stacking before voting ensemble works
- Chasing CV score without LB validation

## Validation Notes
- Use 5-fold Stratified CV (current approach is sound)
- **Expected CV-LB gap: 5-7%** (based on current data)
- A model with CV 0.82 might achieve LB 0.75-0.77
- Submit periodically to calibrate CV-LB relationship

## Experiment Plan for Loop 3
1. **exp_002**: Voting Ensemble with 8 features (including Title)
   - Models: RF, LogisticRegression, GradientBoosting, SVC
   - Soft voting
   - Target CV: 0.82+

2. **exp_003**: Stacking Ensemble (if voting works)
   - 5 base models with out-of-fold predictions
   - XGBoost meta-learner

## Key Insight
The simple RF (exp_001) predicts 31.3% survival vs 38.4% in training. This is too conservative. Adding Title back and using ensemble should help identify more survivors correctly.

**Prediction comparison**: XGBoost and Simple RF agree on 91.4% of predictions. The 8.6% disagreement (36 passengers) is where ensemble diversity can help.
