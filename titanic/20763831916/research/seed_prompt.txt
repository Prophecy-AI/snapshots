# Titanic Survival Prediction - Loop 6 Strategy

## Current Status
- Best CV score: 0.8373 from exp_003 (Threshold-Tuned Ensemble)
- Best LB score: 0.7847 from exp_003 (Threshold-Tuned Ensemble)
- CV-LB gap: +5.26% (stable for best model)
- Submissions used: 4/10 (4 remaining)

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY** - stacking implementation was correct
- Evaluator's top priority was: "SUBMIT THE STACKING MODEL to validate whether it improves over threshold-tuned ensemble"
- **RESULT: STACKING FAILED** - LB dropped from 0.7847 to 0.7631 (-2.16%, 9 passengers worse)
- Key insight: Same CV (0.8373) but stacking's CV-LB gap increased from 5.26% to 7.42%
- **My synthesis**: Stacking overfits on this small dataset. Simpler models work better.
  - Abandon stacking approach entirely
  - Focus on feature engineering to improve the Threshold-Tuned Ensemble
  - The 15 differing predictions were mostly WRONG for stacking

## Submission History Analysis (Critical Learning)
| Exp | Model | CV | LB | Gap | Survivors | Key Insight |
|-----|-------|-----|-----|-----|-----------|-------------|
| exp_000 | XGBoost (13 features) | 0.8316 | 0.7584 | +7.32% | 157 (37.6%) | Too complex, overfits |
| exp_001 | Simple RF (7 features) | 0.8238 | 0.7775 | +4.63% | 131 (31.3%) | Simpler = better |
| exp_003 | Threshold-Tuned Ensemble | 0.8373 | 0.7847 | +5.26% | 130 (31.1%) | **BEST LB** |
| exp_004 | Stacking (5 base + XGB) | 0.8373 | 0.7631 | +7.42% | 131 (31.3%) | Stacking overfits |

**Pattern Confirmed**:
1. Lower survival rate (~31%) outperforms training rate (38.4%)
2. Simpler models have smaller CV-LB gap
3. Stacking/complex ensembles overfit on small datasets
4. CV score alone is NOT predictive of LB performance

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, survival patterns
- `exploration/evolver_loop5_lb_feedback.ipynb` - Stacking failure analysis

Key patterns:
- Sex is dominant: Female 74.2% survival vs Male 18.9%
- Pclass matters: 1st class 63%, 3rd class 24%
- Title is highly predictive: Mr 15.7%, Mrs 79.2%, Miss 69.8%, Master 57.5%
- **Stacking wrongly predicted survival for 8 young 3rd class females (ages 18-36, mean fare $12.9) who likely died**
- Test set has lower survival rate than training (distribution shift)

## Recommended Approaches (Priority Order)

### Priority 1: FEATURE ENGINEERING (Highest Impact)
**Rationale**: Research shows 80%+ accuracy is achievable with proper feature engineering. Current model uses only 8 basic features. Adding FamilySize, IsAlone, Has_Cabin could improve predictions.

**Features to Add**:
1. **FamilySize** = SibSp + Parch + 1
   - Medium families (2-4) have 55-72% survival vs solo (30%) or large (0-33%)
2. **IsAlone** = 1 if FamilySize == 1, else 0
   - Solo travelers have lower survival (30.4%)
3. **Has_Cabin** = 1 if Cabin is not null, else 0
   - With cabin: 66.7% survival vs without: 30%
4. **TicketFreq** = Number of passengers sharing same ticket
   - Groups traveling together may have different survival patterns
5. **FareBin** = Quartile-based fare categories
   - Higher fare correlates with higher survival

**Action**:
1. Add these features to the Threshold-Tuned Ensemble
2. Use same threshold calibration (~31% survival rate)
3. Compare CV and submit if CV improves

### Priority 2: BLENDING BEST MODELS
**Rationale**: Simple RF (LB 0.7775) and Threshold-Tuned (LB 0.7847) have different strengths. Blending might capture both.

**Action**:
1. Load predictions from exp_001 (Simple RF) and exp_003 (Threshold-Tuned)
2. Create weighted average: 0.6 * Threshold-Tuned + 0.4 * Simple RF
3. Apply threshold to achieve ~31% survival rate
4. Compare to best LB

### Priority 3: ANALYZE DISAGREEMENT PATTERNS
**Rationale**: Understanding where models disagree can inform feature engineering.

**Key Observations from Analysis**:
- All 4 models agree on 370/418 passengers (88.5%)
- 9 passengers have 2-2 split (most uncertain)
- Focus on these uncertain passengers for feature engineering

### Priority 4: TRY CATBOOST (If Feature Engineering Doesn't Help)
**Rationale**: CatBoost handles categorical features natively and may generalize better than XGBoost.

**Action**:
1. Use same features as Threshold-Tuned Ensemble
2. Let CatBoost handle categorical encoding
3. Apply threshold calibration
4. Compare CV-LB gap

## What NOT to Try
- **Stacking**: Proven to overfit on this dataset (LB dropped 2.16%)
- **More complex models**: XGBoost with 13 features had worst LB (0.7584)
- **Chasing CV score**: CV 0.8496 (stacking at threshold 0.5) gave LB 0.7631
- **Neural networks**: Too complex for 891 samples
- **Hyperparameter tuning**: Diminishing returns, focus on features first

## Validation Notes
- Use 5-fold Stratified CV with random_state=42 (consistent)
- **Trust survival rate pattern**: ~31% survival rate (130-133 survivors) performs best on LB
- Expected LB = CV - 0.05 (based on recent submissions)
- **CV alone is NOT reliable**: Same CV (0.8373) gave LB 0.7847 vs 0.7631

## Target Reality Check
- **Impossible target**: 1.0 (100% accuracy)
- **State-of-the-art**: 81-85% LB (top solutions)
- **Current best LB**: 0.7847 (78.47%)
- **Realistic goal**: 0.80-0.81 (80-81%)
- **Gap to close**: 1.5-2.5% (6-10 passengers)

## Experiment Plan for Loop 6
1. **Add FamilySize, IsAlone, Has_Cabin features** to Threshold-Tuned Ensemble
2. Apply threshold calibration for ~31% survival rate
3. If CV improves AND survival rate is ~31%, submit
4. If no improvement, try blending Simple RF + Threshold-Tuned

## Submission Strategy
- 4 submissions remaining
- Best LB: 0.7847 (exp_003)
- Only submit if:
  1. CV improves over 0.8373 AND
  2. Survival rate is ~31% (130-133 survivors) AND
  3. Approach is fundamentally different (new features, not just hyperparameters)
- Save 2 submissions for final ensemble tuning

## Critical Success Factors
1. **Feature engineering**: Add FamilySize, IsAlone, Has_Cabin
2. **Keep it simple**: Avoid complex stacking/ensembles
3. **Threshold calibration**: ~31% survival rate is key
4. **Trust LB patterns**: Simpler models with lower survival rate work better
5. **Don't chase CV**: CV 0.8373 can give LB 0.7847 or 0.7631 depending on model