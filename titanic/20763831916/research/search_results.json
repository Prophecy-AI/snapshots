{
  "query": "What feature engineering techniques achieve 80%+ accuracy on Titanic Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering tricks such as extracting passenger titles from the\u202fName field, creating\u202fFamilySize\u202fand\u202fIsAlone\u202fflags, encoding cabin presence or deck, binning\u202fAge\u202fand parsing ticket prefixes have been shown to lift Titanic model accuracy above\u202f80%\u202f([github.com](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708), [ahmedbesbes.com](https://www.ahmedbesbes.com/blog/kaggle-titanic-competition)).",
      "url": ""
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge | Ahmed BESBES",
      "text": "September 10, 2016\u00a0\u00a0\u00a0\u00a0\u00a033min read\n\n# How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge\n\nThe [Titanic challenge](https://www.kaggle.com/c/titanic/) hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing him such as his age, his sex, or his passenger class on the boat.\n\nI have been playing with the Titanic dataset for a while, and I have recently achieved an accuracy score of 0.8134 on the public leaderboard. As I'm writing this post, I am ranked among the top 4% of all Kagglers.\n\nThis post is the opportunity to share my solution with you.\n\nTo make this tutorial more \"academic\" so that anyone could benefit, I will first start with an exploratory data analysis (EDA) then I'll follow with feature engineering and finally present the predictive model I set up.\n\nThroughout this jupyter notebook, I will be using Python at each level of the pipeline.\nThe main libraries involved in this tutorial are:\n\n- **Pandas** for data manipulation and ingestion\n- **Matplotlib** and **seaborn** for data visualization\n- **Numpy** for multidimensional array computing\n- **sklearn** for machine learning and predictive modeling\n\n### Installation procedure\n\nA very easy way to install these packages is to download and install the [Conda](http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install) distribution that encapsulates them all. This distribution is available on all platforms (Windows, Linux and Mac OSX).\n\n### Nota Bene\n\nThis is my first attempt as a blogger and as a machine learning practitioner.\n\nIf you have a question about the code or the hypotheses I made, do not hesitate to post a comment in the comment section below.\nIf you also have a suggestion on how this notebook could be improved, please reach out to me.\nThis tutorial is available on my [github](https://github.com/ahmedbesbes/How-to-score-0.8134-in-Titanic-Kaggle-Challenge) account.\n\nHope you've got everything set on your computer. Let's get started.\n\n## I - Exploratory data analysis\n\nAs in different data projects, we'll first start diving into the data and build up our first intuitions.\n\nIn this section, we'll be doing four things.\n\n- Data extraction : we'll load the dataset and have a first look at it.\n- Cleaning : we'll fill in missing values.\n- Plotting : we'll create some interesting charts that'll (hopefully) spot correlations and hidden insights out of the data.\n- Assumptions : we'll formulate hypotheses from the charts.\n\nWe tweak the style of this notebook a little bit to have centered plots.\n\n```\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\");\n```\n\nWe import the useful libraries.\n\n```\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nimport pylab as plot\nparams = {\n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n```\n\nTwo datasets are available: a training set and a test set.\nWe'll be using the training set to build our predictive model and the testing set to score it and generate an output file to submit on the Kaggle evaluation system.\n\nWe'll see how this procedure is done at the end of this post.\n\nNow let's start by loading the training set.\n\n```\ndata = pd.read_csv('./data/train.csv')\nprint(data.shape)\n#(891, 12)\n```\n\nWe have:\n\n- 891 rows\n- 12 columns\n\nPandas allows you to have a sneak peak at your data.\n\n```\ndata.head()\n```\n\n| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n\nThe Survived column is the **target variable**. If Suvival = 1 the passenger survived, otherwise he's dead. The is the variable we're going to predict.\n\nThe other variables describe the passengers. They are the **features**.\n\n- PassengerId: and id given to each traveler on the boat\n- Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n- The Name of the passeger\n- The Sex\n- The Age\n- SibSp: number of siblings and spouses traveling with the passenger\n- Parch: number of parents and children traveling with the passenger\n- The ticket number\n- The ticket Fare\n- The cabin number\n- The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q\n\nPandas allows you to a have a high-level simple statistical description of the numerical features.\nThis can be done using the describe method.\n\n```\ndata.describe()\n```\n\n| PassengerId | Survived | Pclass | Age | SibSp | Parch | Fare |\n| --- | --- | --- | --- | --- | --- | --- |\n| count | 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 |\n| mean | 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 |\n| std | 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 |\n| min | 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 |\n| 25% | 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 |\n| 50% | 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 |\n| 75% | 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 |\n| max | 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 |\n\nThe count variable shows that 177 values are missing in the Age column.\n\nOne solution is to fill in the null values with the median age. We could also impute with the mean age but the median is more robust to outliers.\n\n```\ndata['Age'] = data['Age'].fillna(data['Age'].median())\n```\n\nLet's now make some charts.\n\nLet's visualize survival based on the gender.\n\n```\ndata['Died'] = 1 - data['Survived']\ndata.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, colors=['g', 'r']);\n```\n\nIt looks like male passengers are more likely to succumb.\n\nLet's plot the same graph but with ratio instead.\n\n```\ndata.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                           stacked=True, colors=['g', 'r']);\n```\n\nThe Sex variable seems to be a discriminative feature. Women are more likely to survive.\n\nLet's now correlate the survival with the age variable.\n\n```\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Sex', y='Age',\n               hue='Survived', data=data,\n               split=True,\n               palette={0: \"r\", 1: \"g\"}\n              );\n```\n\nAs we saw in the chart above and validate by the following:\n\n- Women survive more than men, as depicted by the larger female green histogram\n\nNow, we see that:\n\n- The age conditions the survival for male passengers:\n\n  - Younger male tend to survive\n  - A large number of passengers between 20 and 40 succumb\n- The age doesn't seem to have a direct impact on the female survival\n\nThese violin plots confirm that one old code of conduct that sailors and captains follow in case of threatening situations: **\"Women and children first !\"**.\n\nRight?\n\nLet's now focus on the Fare ticket of each passenger and see how it could impact the survival.\n\n```\nfigure = plt.figure(figsize=(25, 7))\nplt.hist([dat...",
      "url": "https://www.ahmedbesbes.com/blog/kaggle-titanic-competition"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    },
    {
      "title": "Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy.",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff090e722fd42&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Python in Plain English**](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-f090e722fd42---------------------------------------)\n\n\u00b7\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---publication_nav-78073def27b8-f090e722fd42---------------------publication_nav------------------)\n\n[![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-f090e722fd42---------------------------------------)\n\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fdata-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---post_publication_sidebar-78073def27b8-f090e722fd42---------------------post_publication_sidebar------------------)\n\n# Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy.\n\n[![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n\nFollow\n\n7 min read\n\n\u00b7\n\nMay 20, 2023\n\n--\n\nListen\n\nShare\n\nData cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact the quality and effectiveness of analytical models.\n\nPhoto by [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n[Competition Link](https://www.kaggle.com/competitions/spaceship-titanic) / [Dataset Link](https://www.kaggle.com/competitions/spaceship-titanic/data)\n\nScored 80.336%, Top 20% on leader-board (Image by Author)\n\nAlready got the preprocessed data and want to directly jump into the model-building part here is the part-2 of this article.\n\n[**Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset** \\\n\\\n**This article will walk you through detailed forward feature selection steps and model building from scratch, improving\u2026**\\\n\\\npub.towardsai.net](https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c?source=post_page-----f090e722fd42---------------------------------------)\n\nLet's start with exploring the dataset. Below are the features we have, and the definition of each feature you can find [here](https://www.kaggle.com/competitions/spaceship-titanic/data).\n\n```\n['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n       'Name', 'Transported']\n```\n\n```\ndf['Transported'].value_counts() # balanced dataset\n\n# True     4378\n# False    4315\n\n```\n\nThe target class is well balanced here..! as this is a practice dataset otherwise in real-life data this is often not true.\n\nThe below function will help us to know what all unique values a categorical column contains and its count and portion in the dataset, it can give us a good understanding of the categorical features and we could also find some features with imbalance or features with low variance.\n\n```\ndef showDetails(column):\n    print('------------------------------------------')\n    print(column +' & TRANSPORTED')\n    print(df[column].value_counts())\n    tempDict = dict(df[column][df['Transported'] == True].value_counts())\n    for i in tempDict.keys():\n        tempDict[i] = (tempDict[i]/len(df[df[column] == i]))*100\n    print(tempDict)\n    print('------------------------------------------')\n\nshowDetails('HomePlanet')\nshowDetails('CryoSleep')\nshowDetails('Destination')\nshowDetails('VIP')\n```\n\n```\nOutput :\n\nHomePlanet & TRANSPORTED\nEarth     4602\nEuropa    2131\nMars      1759\nName: HomePlanet, dtype: int64\n{'Earth': 42.39461103867884, 'Europa': 65.884561238855, 'Mars': 52.30244457077885}\n------------------------------------------\n------------------------------------------\nCryoSleep & TRANSPORTED\nFalse    5439\nTrue     3037\nName: CryoSleep, dtype: int64\n{True: 81.75831412578202, False: 32.892075749218606}\n------------------------------------------\n------------------------------------------\nDestination & TRANSPORTED\nTRAPPIST-1e      5915\n55 Cancri e      1800\nPSO J318.5-22     796\nName: Destination, dtype: int64\n{'TRAPPIST-1e': 47.11749788672866, '55 Cancri e': 61.0, 'PSO J318.5-22': 50.37688442211056}\n------------------------------------------\n------------------------------------------\nVIP & TRANSPORTED\nFalse    8291\nTrue      199\nName: VIP, dtype: int64\n{False: 50.63321674104451, True: 38.19095477386934}\n```\n\nExample:{\u2018Earth\u2019: 42.39461103867884, \u2018Europa\u2019: 65.884561238855, \u2018Mars\u2019: 52.30244457077885}\n\nso, 42.4% of passengers from Earth here have been transported successfully.\n\nLet us see if there exists any geometric pattern in the data, which can give us a clue on which type of feature engineering or which type of models will perform better for this dataset. Will be using t-sne for visualization.\n\n```\ndef plot_tsne(X,y,p=30,step=1000):\n    \"\"\"\n    X-features number (pandas dataframe)\n    y-targets int (pandas series)\n    p-perplexity - points in neighourhood\n    step-Maximum number of iterations for the optimization. Should be at least 250.\n    \"\"\"\n    targets = y.astype(int)\n    tsne = manifold.TSNE(n_components=2,perplexity=p,n_iter=step ,random_state=42)\n    transformed_data = tsne.fit_transform(X.iloc[:100,:])\n    tsne_df = pd.DataFrame(np.column_stack((transformed_data, targets[:100])),columns=[\"x\", \"y\", \"targ...",
      "url": "https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?gi=82d63c7a3356"
    },
    {
      "title": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa324ba577812&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Python in Plain English**](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\n\u00b7\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---publication_nav-78073def27b8-a324ba577812---------------------publication_nav------------------)\n\n[![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\n\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------post_publication_sidebar------------------)\n\n# Kaggle Titanic Challenge: Features Creation\n\n## Better Data, Better Model!\n\n[![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\nFollow\n\n5 min read\n\n\u00b7\n\nApr 7, 2022\n\n--\n\nListen\n\nShare\n\nPhoto by [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n# Overview\n\nHey folks,\n\nIn the previous section of [**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),we looked at the distribution of different features. And their relationship with the prediction label, **Survived**, as well as among each other.\n\nWe noticed that some of the features like **Name** contain extra information about a passenger\u2019s **title** which can be useful for our model.\n\n**In this section,** we will extract useful details from different features and create new features. Let\u2019s get started!\n\nYou can find the complete notebook here:\n\n[**kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7 AmanKrishna/kaggle\\_titanic** \\\n\\\n**Machine Learning model build for survivor prediction for Kaggle's titanic competition \u2026**\\\n\\\ngithub.com](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\n\nFirst, let\u2019s import and combine our train & test data to avoid redoing the same steps for both separately.\n\n## Code\n\n# **New Feature:** Family Name\n\nThe **Name** feature looks something like this:\n\nFamily Names\n\n**Family Name** followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n\n## Why Family Name?\n\n_Hypothesis:_ If some members of a family survive then chances are others will too.\n\n## Code\n\n# New Feature: Title\n\nEach passenger has a title in our database ( **Mr./Mrs./Master** etc.). We can extract them and add them as a new feature **Title**.\n\nTitle associated with Name\n\nLet\u2019s group some of these Titles together using the below dictionary:\n\nTitle Dictionary\n\nHere is the final list of titles we will be using:\n\nList of Titles\n\n## Why Title?\n\nLooking at the below graph we can see that survival probability changes with Title\n\nTitle vs Survival\n\n## **Code**\n\n# New Feature: Cabin Section\n\nIf we look at the **Cabin** feature we can see that each cabin starts with an alphabet. These could be **Cabin sections** similar to the ones we have on trains or flights.\n\nCabin Starts with Alphabet\n\nAfter extracting and combining a few we get the following set of **Cabin Sections**\n\nSet of Cabins\n\n## Code\n\n# New Feature: Family Size\n\nIn [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we discovered that **family size** played a pivotal role in determining survival.\n\nFamily Size vs Survival Rate\n\nLet\u2019s create a new feature **Family\\_Size** by summing **Sibsp**& **Parch** features.\n\n## Code\n\n# New Feature: Family Size Grouped\n\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n\n## Code\n\n## Why grouping?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Binning family sizes into groups will help our model learn better.\n\n# New Feature: Grouping Ticket\n\nDuring [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we noticed that the number of Unique **Tickets** was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of **passengers per ticket** we get the following survival rates:\n\nTicket\\_Group\\_Size vs Survival\n\n## Why group tickets?\n\n_Hypothesis:_ The same ticket will be shared among the family group as well as the friends group.\n\n## Code\n\n# Age\\_Bin & Fare\\_Bin\n\nWe have observed during [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) that Survival probability varied across **Age** and **Fare** segments. Let\u2019s bin them into separate features.\n\nAge/Fare Bins vs Survival Rates\n\n## Why Binning?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n\n**Note:** The **age** feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n\n## Code\n\n# New Feature: Survival Rates\n\nThis feature will contain the survival probability for each **Ticket**, **Family Name** & **Cabin number**. **We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_Rate & Name\\_Survival\\_Rate**\n\nFor all the passengers travelling on the titanic the **survival probability is around 0.38**. But this probability varies according to Sex, Pclass & other f...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812?gi=750ac193f43c"
    },
    {
      "title": "Kaggle Titanic Challenge: Features Creation",
      "text": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English\n[Sitemap](https://python.plainenglish.io/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Python in Plain English\n](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\u00b7Follow publication\n[\n![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)\n](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\nFollow publication\n# Kaggle Titanic Challenge: Features Creation\n## Better Data, Better Model!\n[\n![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)\n](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n5 min read\n\u00b7Apr 7, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/python-in-plain-english/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;user=Aman+Krishna&amp;userId=bda5c8d97e1a&amp;source=---header_actions--a324ba577812---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=---header_actions--a324ba577812---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)## Overview\nHey folks,\nIn the previous section of[**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),****we looked at the distribution of different features. And their relationship with the prediction label,**Survived**, as well as among each other.\nWe noticed that some of the features like**Name**contain extra information about a passenger\u2019s**title**which can be useful for our model.\n**In this section,**we will extract useful details from different features and create new features. Let\u2019s get started!\nYou can find the complete notebook here:\n[\n## kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7AmanKrishna/kaggle\\_titanic\n### Machine Learning model build for survivor prediction for Kaggle&#x27;s titanic competition \u2026github.com\n](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\nFirst, let\u2019s import and combine our train &amp; test data to avoid redoing the same steps for both separately.\n### Code\n## **New Feature:**Family Name\nThe**Name**feature looks something like this:\nPress enter or click to view image in full size\n![]()\nFamily Names\n**Family Name**followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n### Why Family Name?\n*Hypothesis:*If some members of a family survive then chances are others will too.\n### Code\n## New Feature: Title\nEach passenger has a title in our database (**Mr./Mrs./Master**etc.). We can extract them and add them as a new feature**Title**.\nPress enter or click to view image in full size\n![]()\nTitle associated with Name\nLet\u2019s group some of these Titles together using the below dictionary:\nTitle Dictionary\nHere is the final list of titles we will be using:\n![]()\nList of Titles### Why Title?\nLooking at the below graph we can see that survival probability changes with Title\nPress enter or click to view image in full size\n![]()\nTitle vs Survival### **Code**\n## New Feature: Cabin Section\nIf we look at the**Cabin**feature we can see that each cabin starts with an alphabet. These could be**Cabin sections**similar to the ones we have on trains or flights.\nPress enter or click to view image in full size\n![]()\nCabin Starts with Alphabet\nAfter extracting and combining a few we get the following set of**Cabin Sections**\n![]()\nSet of Cabins### Code\n## New Feature: Family Size\nIn[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we discovered that**family size**played a pivotal role in determining survival.\nPress enter or click to view image in full size\n![]()\nFamily Size vs Survival Rate\nLet\u2019s create a new feature**Family\\_Size**by summing**Sibsp**&amp;**Parch**features.\n### Code\n## New Feature: Family Size Grouped\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n### Code\n### Why grouping?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Binning family sizes into groups will help our model learn better.\n## New Feature: Grouping Ticket\nDuring[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we noticed that the number of Unique**Tickets**was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of**passengers per ticket**we get the following survival rates:\nPress enter or click to view image in full size\n![]()\nTicket\\_Group\\_Size vs Survival### Why group tickets?\n*Hypothesis:*The same ticket will be shared among the family group as well as the friends group.\n### Code\n## Age\\_Bin &amp;&amp; Fare\\_Bin\nWe have observed during[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)that Survival probability varied across**Age**and**Fare**segments. Let\u2019s bin them into separate features.\nPress enter or click to view image in full size![]()\nPress enter or click to view image in full size![]()\nAge/Fare Bins vs Survival Rates\n### Why Binning?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n## GetAman Krishna\u2019s stories in\u00a0your\u00a0inbox\nJoin Medium for free to get updates from\u00a0this\u00a0writer.\nSubscribe\nSubscribe\n**Note:**The**age**feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n### Code\n## New Feature: Survival Rates\nThis feature will contain the survival probability for each**Ticket**,**Family Name**&amp;**Cabin number**.**We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_R...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset",
      "text": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Towards AI\n](https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-40a0aa184c1c---------------------------------------)\n\u00b7Follow publication\n[\n![Towards AI](https://miro.medium.com/v2/resize:fill:76:76/1*JyIThO-cLjlChQLb6kSlVQ.png)\n](https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-40a0aa184c1c---------------------------------------)\nMaking AI accessible to 100K+ learners. Find the most practical, hands-on and comprehensive AI Engineering and AI for Work certifications at[academy.towardsai.net](http://academy.towardsai.net)- we have pathways for any experience level. Monthly cohorts still open\u200a\u2014\u200ause COHORT10 for 10% off!\nFollow publication\n# Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\n## This article will walk you through detailed forward feature selection steps and model building from scratch, improving it further with fine-tuning.\n[\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\n](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n5 min read\n\u00b7May 24, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-artificial-intelligence/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--40a0aa184c1c---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=---header_actions--40a0aa184c1c---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[NASA](https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\n[\n## Data pre-processing of spaceship-titanic kaggle dataset for achieving 80+% accuracy.\n### Data cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact\u2026\nmedium.com\n](https://medium.com/@chavdadevang23/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?source=post_page-----40a0aa184c1c---------------------------------------)\n**We will be building a model in 3 trenches:**\n1. Building a model with only numerical features.\n2. Building a model with only categorical features.\n3. Building a model with all features combined.\n```\nNUMS = [&#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,&#x27;Num&#x27;,\n&#x27;&#x27;Expenditure&#x27;&#x27;,&#x27;&#x27;Group\\_size&#x27;&#x27;,&#x27;&#x27;Expenditure&#x27;&#x27;]\nTARGET = [&#x27;Transported&#x27;]\n# only numerical feature dataframe\nnums\\_df = df[NUMS+TARGET]\ndf[NUMS].head(3)\n```\nLet\u2019s find out which features have the most importance in order to classify a data point, we will be using the forward feature selection method.\n> Forward feature selection: Step-by-step process of adding features to improve a model\u2019s performance, starting with none, to identify the most relevant ones.\n```\ndef evaluate\\_model\\_kfold\\_classification(X,y,k,clf):\n#kfold base\nX = X.fillna(0)\nkf = model\\_selection.KFold(n\\_splits=k,shuffle=True)\naccuracies = []\nfor fold, (train\\_index,validation\\_index) in enumerate(kf.split(X=X)):\ntrain\\_x = X.loc[train\\_index].values\ntrain\\_y = y.loc[train\\_index].values\nvalidation\\_x = X.loc[validation\\_index].values\nvalidation\\_y = y.loc[validation\\_index].values\nclf.fit(train\\_x,train\\_y)\npreds = clf.predict(validation\\_x)\naccuracy = metrics.accuracy\\_score(validation\\_y, preds)\nprint(f&quot;Fold={fold}, Accuracy={accuracy}&quot;)\naccuracies.append(accuracy)\nreturn sum(accuracies)/len(accuracies)\ndef feature\\_selection\\_classification(X,y,k,model):\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nbest\\_feature= None\nfor feature in list(X.columns):\nscore = evaluate\\_model\\_kfold\\_classification(X[[feature]],y,k,model)\nif score &gt;&gt; average\\_eval\\_metric:\nbest\\_feature = feature\naverage\\_eval\\_metric =score\nprint(&quot;&quot;best feature--&gt;&gt;&quot;&quot;,best\\_feature)\nfeatures = list(X.columns)\nfeatures.remove(best\\_feature)\nbest\\_feature\\_order = [best\\_feature]\nbest\\_feature\\_order.extend(features)\nprint(&quot;&quot;best feature order --&gt;&gt;&quot;&quot;,best\\_feature\\_order)\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nscores\\_progression = {}\nfor feature in best\\_feature\\_order:\ngood\\_features.append(feature)\nscore = evaluate\\_model\\_kfold\\_classification(X[good\\_features],y,k,model)\nscores\\_progression[&#x27;&#x27;|&#x27;&#x27;.join(good\\_features)] = score\nif score &lt;&lt; average\\_eval\\_metric:\ngood\\_features.remove(feature)\nelse:\naverage\\_eval\\_metric = score\nreturn good\\_features,scores\\_progression\n```\nThe code performs feature selection for classification. It iterates over features, evaluates their impact on the model, and selects the best ones based on evaluation metrics.\nHere I am using logistic regression as my base model to select the best features. Why logistic regression: when we did t-sne analysis on this dataset, we found out that data points are separated in a way where drawing a boundary will be easier,\n```\n# here any classification model can be chosen in order to get best features\nclf = LogisticRegression()\ngood\\_features , score\\_progression = feature\\_selection\\_classification(df[NUMS],df[TARGET],5,clf)\nscore\\_progression\n```\nPress enter or click to view image in full size\n![]()\nStep-by-step progress\nPress enter or click to view image in full size\n![]()\ngood features\nNow measure accuracy with all numeric features vs with only good features, which we have derived with feature selection.\n```\nprint(&quot;&quot;\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*with all features\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*...",
      "url": "https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c"
    }
  ]
}