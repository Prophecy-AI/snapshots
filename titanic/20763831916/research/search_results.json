{
  "query": "How to reduce overfitting and CV-LB gap in Kaggle competitions with small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on proven Kaggle\u2011specific practices, you can keep a small\u2011dataset competition from over\u2011fitting and narrow the public\u2011leaderboard vs. private\u2011leaderboard (CV\u2011LB) gap with the following concise workflow\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions); [Chioka](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio); [Baru\u0161auskas](https://www.slideshare.net/DariusBaruauskas/tips-and-tricks-to-win-kaggle-data-science-competitions); [Kaggle 2025 guide](https://www.kaggle.com/discussions/general/561951)):\n\n1. **Set up a robust cross\u2011validation scheme**  \n   - Use stratified\u202fk\u2011fold (\u2265\u202f5 folds) and repeat the CV several times to average scores.  \n   - If full CV is too slow, run it on a representative subset, but keep the subset large enough to reflect the whole distribution.  \n   - Treat the CV metric as the primary performance indicator; ignore short\u2011term public\u2011LB fluctuations.\u202f([Chioka](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio))\n\n2. **Apply regularization and early stopping**  \n   - Limit model complexity (e.g., max depth, number of leaves).  \n   - Add L1/L2 penalties or dropout (for neural nets).  \n   - Monitor validation loss and stop training when it stops improving.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n3. **Engineer and select features carefully**  \n   - Remove noisy or highly correlated columns; keep only those that improve CV score.  \n   - Create domain\u2011specific features that capture signal without inflating dimensionality.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n4. **Use adversarial validation to detect train\u2011test distribution shift**  \n   - Train a binary classifier to separate the original training set from the public test set.  \n   - Features that the classifier exploits indicate leakage; drop or re\u2011engineer them.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n5. **Ensemble diverse models (stacking)**  \n   - Combine at least two fundamentally different algorithms (e.g., LightGBM + a simple neural net).  \n   - Blend their predictions with a meta\u2011learner or weighted average to reduce variance.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n6. **Tune hyper\u2011parameters with Bayesian optimization**  \n   - Use tools like Optuna or Hyperopt to search the space efficiently, focusing on regularization\u2011related parameters (eta, subsample, colsample_bytree, etc.).  \n   - Run the optimizer inside the CV loop to ensure tuned settings generalize.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n7. **Validate on a \u201cprivate\u2011like\u201d holdout**  \n   - After finalizing the model, create a final holdout that mimics the size and distribution of the private test set (e.g., 10\u201115\u202f% of the data).  \n   - Compare its score to the CV average; if the gap is large, revisit steps\u202f1\u20116.\u202f([Baru\u0161auskas](https://www.slideshare.net/DariusBaruauskas/tips-and-tricks-to-win-kaggle-data-science-competitions))\n\nFollowing these steps\u2014especially strong CV, regularization, adversarial validation, and diverse ensembling\u2014will markedly reduce over\u2011fitting and keep the public\u2011leaderboard score aligned with the private leaderboard, even when the dataset is tiny.",
      "url": ""
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "How to Select Your Final Models in a Kaggle Competition",
      "text": "Did your rank just drop sharp in the private leaderboard in a Kaggle\u00a0competition?\n[![picard palm](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)\nI\u2019ve been through that, too. We all learn about overfitting when we started machine learning,\u00a0but Kaggle makes\u00a0you really feel the pain of overfitting.\u00a0Should I have been more careful in the [Higgs Boson Machine Learning competition](http://www.kaggle.com/c/higgs-boson/), I would have selected a solution that would gave me a rank 4 than rank 22.\nI vow to come out with some principles systematically select\u00a0final models. Here are the lessons learnt:\n- **Always do cross-validation to get a reliable metric.**\u00a0If you don\u2019t, the validation score you get on a single\u00a0validation set\u00a0is unlikely to reflect the model performance in general. Then, you will likely see\u00a0a model improvement in that single validation set, but actually performs worse in general. **_Keep in mind the CV score can be optimistic, but your model is still overfitting._**\n- **Trust your CV\u00a0score, and not LB\u00a0score.** The leaderboard score\u00a0is scored only on a small percentage of the full test set. In some cases, it\u2019s only a few hundred test cases. Your cross-validation score will be much more reliable in general.\n- If your CV score is not stable (perhaps due to ensembling methods), you can\u00a0run your CV with more folds and multiple times to\u00a0take average.\n- If a single\u00a0CV\u00a0run is very slow, use a subset of the data to run\u00a0the CV. This will help your CV loop to run faster. Of course, the subset should not be too small or else the CV score will not be representative.\n- **For the final 2 models, pick very different models.** Picking two very similar solutions means that your solutions either fail together or win together, effectively meaning that you only pick one model. You should reduce your risk by picking two confident but very different models.\u00a0_**You should not depend on the leaderboard score at all.**_\n- Try to group your solutions by methodologies. Then, pick the best CV score model from each group. Then compare these best candidates of each group, pick two.\n- Example: I have different groups 1)\u00a0Bagging of SVMs 2) RandomForest 3) Neural Networks 4) LinearModels. Then, each group should produce one single best model, then you pick 2 out of these.\n- **Pick a robust methodology.** Here is the tricky part\u00a0which depends on experience, even if you have done cross validation, you can still get burned:\u00a0Sketchy methods of improving the CV score like\u00a0making cubic features, cubic root features, boosting like crazy, magical\u00a0numbers(without understanding it), etc, will likely be a bad model to pick even if the CV score is good. Unfortunately, you will probably have to make this mistake once to know what this means. =\\]\nApplying the above\u00a0principles to the recent competition\u00a0[Africa Soil Property Prediction Challenge](http://www.kaggle.com/c/afsis-soil-properties), plus a bit of luck, I picked the top 1 and top 2 models.\n[![top score](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)\nSorted by private score\nI ended up Top 10% with a rank of\u00a090 by spending just\u00a0a week time and mostly in Mexico in a vacation. I guess,\u00a0not too bad?",
      "url": "https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio"
    },
    {
      "title": "A Beginners Guide to Winning Kaggle Competitions in 2025 | Kaggle",
      "text": "Kaggle competitions have become more competitive than ever, with larger prize pools, stronger competitors, and increasingly complex datasets. Many participants struggle to break into the top 10 percent, while experienced competitors are always looking for ways to optimize their approach and secure a gold medal.\n\nThis guide outlines key strategies for improving performance in Kaggle competitions, covering everything from model selection to leaderboard tactics. Whether you are a beginner or an experienced competitor, these principles will help you maximize your ranking.\n\nChoosing the Right Competition:\n\nNot all competitions are equally difficult. Some require extensive computational resources, while others reward creative feature engineering over raw model performance. The best competitions for improving your ranking include:\n\n\u2022 Tabular data competitions, which often rely on feature engineering rather than GPU-intensive training.\n\n\u2022 Structured data competitions, where gradient boosting methods frequently outperform deep learning models.\n\n\u2022 Competitions with well-defined public baselines, allowing participants to start with a strong reference point.\n\n\u2022 Smaller datasets, which allow for rapid experimentation and iteration.\n\nFor newcomers, it is advisable to start with past featured competitions or playground competitions, as these often have extensive public notebooks and discussions that can provide valuable insights.\n\nModel Selection and Best Practices:\n\nThe choice of model depends on the type of data in the competition.\n\nFor tabular data, gradient boosting frameworks such as XGBoost, LightGBM, and CatBoost remain the most effective models in most cases. These methods handle missing data well and can be optimized using hyperparameter tuning libraries such as Optuna or Hyperopt.\n\nFor computer vision problems, smaller datasets can often be tackled effectively with ResNet50 or EfficientNet-B0. For larger datasets, fine-tuning vision transformers (ViTs) or ConvNeXt models can yield strong results. Segmentation tasks benefit from U-Net architectures with ResNet encoders.\n\nFor natural language processing (NLP), simpler datasets often perform well with TF-IDF and logistic regression. Longer text sequences benefit from transformer-based models such as BERT, GPT-3.5, or LLaMA. For multilingual text, models like mT5 or XLM-RoBERTa are strong options.\n\nIt is always beneficial to benchmark simple models first before moving on to more complex deep learning approaches. In many cases, simple models can outperform deep learning, particularly when well-engineered features are used.\n\nKey Kaggle Techniques for Higher Rankings:\n\nFeature engineering remains one of the most important factors in achieving a high ranking in tabular data competitions. Common techniques include:\n\n\u2022 Target encoding for categorical variables.\n\n\u2022 Polynomial feature generation for capturing non-linear relationships.\n\n\u2022 Extracting useful features from dates and timestamps, such as the day of the week or seasonal patterns.\n\n\u2022 Generating lag-based features for time-series forecasting.\n\nFor computer vision and NLP competitions, data augmentation can be critical. In image classification tasks, techniques such as random cropping, CutMix, and MixUp improve generalization. In NLP, methods such as back translation and synonym replacement can help models learn more robust representations.\n\nAnother essential strategy is ensembling. Many competition winners combine multiple models to improve performance. For tabular data, blending XGBoost, LightGBM, and CatBoost models is common. In deep learning, ensembling multiple CNN architectures or averaging different transformer models often yields improvements.\n\nThe most effective ensembles combine models with different architectures to introduce diversity into predictions.\n\nAvoiding Overfitting to the Public Leaderboard\n\nOne of the most common pitfalls in Kaggle competitions is overfitting to the public leaderboard. Since the public leaderboard score is based on only a portion of the test set, it does not always reflect final rankings.\n\nTo mitigate this risk:\n\n\u2022 Always prioritize cross-validation performance over public leaderboard scores.\n\n\u2022 Use k-fold or stratified k-fold cross-validation for stability.\n\n\u2022 Regularly check for data leakage, as leaked information can artificially inflate scores.\n\nCompetitors should also be strategic about final-day submissions. Even small differences in scores can lead to significant rank changes. Testing different random seeds and ensembling top models can optimize performance before the final submission deadline.\n\nWinning Kaggle competitions requires more than just technical skills. Success comes from selecting the right competitions, optimizing feature engineering and model selection, using ensembling techniques, and avoiding leaderboard overfitting. Those who consistently apply these strategies improve their rankings over time.\n\nFeel free to share your own insights in the discussion below!\ud83d\udc47",
      "url": "https://www.kaggle.com/discussions/general/561951"
    },
    {
      "title": "Don\u2019t Overfit! II \u2014 How to avoid Overfitting in your Machine Learning and Deep Learning Models",
      "text": "Image by author\n\n* * *\n\nOne of the main objectives of predictive modeling is to build a model that would give accurate predictions on unseen data which will only be possible when we make sure that they have not overfitted the training data. Unfortunately, Overfitting is a common stumbling block that every machine learners face in their career. There are many reasons for the same, yet we would like to point out some major reasons. First is the presence of fewer data points in training samples, second is the dataset being imbalanced, and last but not the least, is the complex nature of the model.\n\nIn this blog, we are going to try out almost all the machine learning and deep learning heuristic methods that are available to avoid overfitting in the dataset from one of the Kaggle competition **[Don\u2019t Overfit II](https://www.kaggle.com/c/dont-overfit-ii/).**\n\nHere is the list of contents that are followed in this blog. We would try to explore each, one by one.\n\n## Table of Contents\n\n01. [Kaggle Problem statement, Dataset, and Evaluation metric](http://towardsdatascience.com/towardsdatascience.com#b8db)\n02. [Existing Approaches](http://towardsdatascience.com/towardsdatascience.com#e69d)\n03. [Research works on Overfitting](http://towardsdatascience.com/towardsdatascience.com#bd68)\n04. [Our Contribution](http://towardsdatascience.com/towardsdatascience.com#d0f5)\n05. [Exploratory Data Analysis (EDA)](http://towardsdatascience.com/towardsdatascience.com#4abb)\n06. [Feature Engineering](http://towardsdatascience.com/towardsdatascience.com#3324)\n07. [First cut Approach](http://towardsdatascience.com/towardsdatascience.com#7818)\n08. [Machine Learning Models](http://towardsdatascience.com/towardsdatascience.com#588d)\n09. [Deep Learning Models](http://towardsdatascience.com/towardsdatascience.com#8f0f)\n10. [Results](http://towardsdatascience.com/towardsdatascience.com#df0c)\n11. [Conclusion](http://towardsdatascience.com/towardsdatascience.com#a94f)\n12. [Future Work](http://towardsdatascience.com/towardsdatascience.com#7e01)\n13. [References](http://towardsdatascience.com/towardsdatascience.com#efa2)\n\n## Kaggle Problem statement, Dataset, and Evaluation metric:\n\n### Problem statement\n\nTo make a model with 250 data points in the train set and predict the binary target accurately for 19750 unseen data points in the test set. The dataset has 300 features of a continuous variable.\n\n### Dataset\n\nThe dataset can be obtained from [here](https://www.kaggle.com/c/dont-overfit-ii/data).\n\nFiles:\n\u25cf train.csv \u2013 the training set. 250 rows.\n\u25cf test.csv \u2013 the test set. 19,750 rows.\n\u25cf sample\\_submission.csv \u2013 a sample submission file in the correct format\nColumns:\n\u25cf id- sample id\n\u25cf target- a binary target of mysterious origin.\n\u25cf 0\u2013299- continuous variables.\n\n> _**NOTE:**_\n>\n> The dataset that is currently available on kaggle site is a new version but the evaluation is still being done on an older version. you may get the older version [here](https://www.kaggle.com/mdmub0587/older-dataset-for-dont-overfit-ii-challenge)\n\n### Evaluation Metric\n\n[AUCROC](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)(Receiver Operating Characteristic/Area Under the Curve)\n\nROC Curves summarize the trade-off between the true positive rate and false-positive rate for a predictive model using different probability thresholds. But, this metric is not preferable when the dataset is highly imbalanced. The area under the ROC curve is called AUC, higher the AUC higher the performance. For no skill model, AUC will be 0.5.\n\n## Existing Approaches:\n\nWe already know that this competition had been consummated a year ago on Kaggle and tons of methods found their applications. Some of the finest approaches are as elucidated below:\n\n### Just Don\u2019t Overfit [\\[1\\]](http://towardsdatascience.com/towardsdatascience.com\\#e794):\n\nThe crux approach of this blog is to use LASSOCV (Least Absolute Shrinkage and Selection Operator Cross-Validation). It is made up of two terms LASSO and CV (where CV is Cross-Validation). The main function of the LASSO regressor is to add some bias term to minimize the variance of the model. It performs L1 regularization, i.e. adds a penalty equivalent to an absolute value of the magnitude of the coefficients. Its objective function is:\n\nImage by author\n\nWhere the only parameter is alpha. The more the value of alpha, the more the feature\u2019s weight approaches to zero. The use of this algorithm leads to a score of **0.843** on the Kaggle leader board.\n\n### Don\u2019t Overfit! \u2013 How to prevent Overfitting in your Deep Learning Models [\\[2\\]](http://towardsdatascience.com/towardsdatascience.com\\#8754):\n\nThis blog has tried to train a Deep Neural Network model to avoid the overfitting of the same dataset we have. First, a feature selection using RFE (Recursive Feature Elimination) algorithm is performed. Next, a simple NN model with 2 hidden layers and 2 dropout layers with early stopping is trained, which resulted in **80%** test accuracy.\n\n## Research works on Overfitting\n\n### An Overview of Overfitting and its Solutions [\\[3\\]](http://towardsdatascience.com/towardsdatascience.com\\#1bd6):\n\nThis paper presented some of the heuristic techniques to avoid overfitting in great detail and gave reasons for how and when to apply them.\n\n_**Network-reduction:**_\nThe complexity of a model can be reduced by eliminating the lesser significant and irrelevant data (i.e. noise), which in turn, would aid the model to prevent overfitting and perform well on unseen data. This is called pruning, which is widely used in the decision tree algorithm. There are two types of _pruning_ methods that have been proposed, one being the _pre-pruning_ and the other, _post-pruning_.\n\nThe pre-pruning method tries to stop the tree-building process early before it produces leaves with very small samples. At each stage of splitting the tree, we check the cross-validation error. If the error does not decrease significantly enough, we stop. This is also called Early-Stopping in the learning algorithm.\n\nPost-pruning involves cutting back the tree. After a tree has been built, it is pruned back to the point where the cross-validated error is minimum.\n\n_**Expansion of the training data:**_\nAs limited data leads the model to overfit, we should expand it out which might help the model to avoid overfitting. The techniques that have been proposed in this paper include adding some random noise to an existing dataset, re-acquiring some data from existing data set through some processing, and Producing some new data based on the distribution of the data set.\n\n_**Regularization:**_\nA large number of features makes the model more complex. Instead of discarding some features arbitrarily, if we add a regularization term into cost function, that would cut down the effect of useless features by minimizing their weights. L1 regularization term sets the weight of the less important feature to zero, whereas L2 regularization makes that type of weight too small.\n\n### Overfitting in Making Comparisons Between Variable Selection Methods [\\[4\\]](http://towardsdatascience.com/towardsdatascience.com\\#5070):\n\nThis paper has covered two major methods of feature-selection, namely sequential forward selection (SFS) and sequential forward floating selection (SFFS). The results obtained on the sonar and the mushroom dataset were covered, which consequently showed that SFFS produces a better result than SFS. In the mushroom dataset, SFFS gave better results on test data even by half of the features, as compared to several features selected by SFS.\n\n_**Sequential Forward Selection(SFS):**_\nIn this method, the best single feature was first selected, and paired with unselected features, again following the selection of the best pair features. Again, a triplet with the best pair and unselected features were made, and then the best triplet features were selected. This process went on until the desired number of features got selected.\n\nImage by author\n\nThe drawback o...",
      "url": "https://towardsdatascience.com/dont-overfit-ii-how-to-avoid-overfitting-in-your-machine-learning-and-deep-learning-models-2ff903f4b36a?gi=a489843bbfd5"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    },
    {
      "title": "Don\u2019t Overfit II- A Kaggle Competition. - Ayush Khandelwal - Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd087344b845e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akwsir96%2Fdont-overfit-ii-a-kaggle-competition-d087344b845e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akwsir96%2Fdont-overfit-ii-a-kaggle-competition-d087344b845e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Don\u2019t Overfit II- A Kaggle Competition.\n\n[Ayush Khandelwal](https://medium.com/@akwsir96?source=post_page---byline--d087344b845e---------------------------------------)\n\n12 min read\n\n\u00b7\n\nAug 2, 2022\n\n--\n\nListen\n\nShare\n\nOverfitting\n\n# Table Of Contents\n\n01. Introduction\n02. Prerequisites\n03. Kaggle Competition\n04. Existing Approaches\n05. My Approach\n06. Exploratory Data Analysis(EDA)\n07. Feature Engineering\n08. First Cut Solution\n09. Modeling\n10. Best Model\n11. Video Representation\n12. Conclusion\n13. Future Work\n14. References\n\n# 1\\. Introduction\n\nOverfitting is a trivial problem when building any machine learning or deep neural network model.Overfitting means when a statistical model or neural network learns patterns present in the training data but is unable to perform on unseen test data thereby defeating the algorithm purpose.There are several reasons for overfitting but the common reasons are as:\n\n1. Data used for training is not cleaned and contains noise in it.\n2. The model has a high variance\n3. The size of the training dataset used is not enough\n4. The model is too complex\n\nThe Kaggle competition Don\u2019t Overfit II is based on the above third point.This blog uses machine learning as well as deep learning models to tackle the competition.\n\n# 2\\. Prerequisites\n\nThis blog assumes readers have basic understanding of Overfitting,Machine Learning Algorithms,Scaling Methods,Transformation,Oversampling, Basic Neural Network,Python,Libraries.\n\n# 3\\. Kaggle Competition\n\nDon\u2019t overfit II is kaggle problem where model is made with 250 training data points and tested on 19750 test data points given a very small amount of training data.\n\nAccording to kaggle,\n\n> _\u201cIt was not just any competition._\n>\n> _It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples\u2026 without overfitting.\u201d_\n\n## Dataset\n\nThe dataset can be downloaded from [here](https://www.kaggle.com/c/dont-overfit-ii)\n\nThe dataset consist of train and test csv.\n\nTraining data:\n\nThe training data consist of 250 points and 302 features including id and target variable\n\nTesting data:\n\nThe testing data consist of 19750 points and 300 features including id\n\n## 3\\. Evaluation Metric\n\nThe score will be evaluated on the [AUCROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between actual values and predicted values.\n\nThe AUCROC is a performance measurement for various threshold settings.The ROC curve is created by plotting the TPR(true positive rate) against the FPR(false positive rate) at various threshold settings.\n\n# 4\\. Existing Approaches\n\n## [1\\. Just Don\u2019t Overfit II](https://medium.com/analytics-vidhya/just-dont-overfit-e2fddd28eb29)\n\nThe approach this blog uses is LASSOCV (Least Absolute Shrinkage and Selection Operator Cross-Validation). It is made up of two terms LASSO and CV (where CV is Cross-Validation).This algorithm performs L1 regularisation as it creates sparsity and add some bias term to minimize the variance of the model.\n\n## 2\\. [Don\u2019t Overfit-How to prevent overfitting in your deep learning models](https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323)\n\nThe approach in this blog uses basic MLP with 128 and 64 neurons as two hidden layers.Since it\u2019s a two class classification problem,binary\\_crossentropy is used as loss function.This leads to overfitting of model.In the next approach author uses 8 neuron network and use early stop and add dropout layer with dropout of (0.4)to avoid overfitting.For this model the kaggle score was **80%**\n\n## Research Paper\n\n## 3\\. [An Overview on Overfitting and its solutions](https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022/pdf)\n\nThis paper tells about the overfitting problems and its solutions.There are various ways in which overfitting can happen viz is limited training dataset,data has some noise in it, complexity of classifiers.\n\nThe author proposed various solutions to overcome it.\n\n1. Early stopping- It means you can stop your training early when your accuracy is not improving and your gap between training error and validation error is increasing after some epochs\n\nOverfitting (Loss increases between training error and test error) Blue Line -Training Error ,Red Line-Test Error\n\n2.Network Reduction- This method is proposed to reduce noise in our dataset.There are basically a method to reduce noise in classifiers is Pruning. Again is pruning is of two types.\n\na. pre-pruning-It functions learning during process.It adds conditions and rules in the model while learning the model.\n\nb. post-pruning-It prevents overfitting by deleting the rules and conditions generated during the model.\n\n3\\. Expansion of Training data: overfitting can happen when the training data is small. Therefore the dataset must be large enough so that model cannot be adjusted to noise and outliers. Approaches to add data in existing data:\n\na.Acquire more training data\n\nb.Add noise to the data\n\nc. Re-acquire some data from existing data set through some processing\n\n4.Regularization:Add L2(Ridge) and L1(Lasso) regularization to prevent overfitting.Regularization is done to remove useless features(L1) or to give less weight to useless features(L2) when features are large.\n\n# 5\\. My Approach\n\n1. Apply Feature Engineering\n2. Selected top features using **RFECV(Recursive feature elimination with cross-validation**) to make models.\n3. Uses **Feature Transformation**(box cox transformation and log transformation) on features.\n4. Apply Oversampling technique called SMOTE(S **ynthetic Minority Oversampling Techniques**).\n5. Uses 2 feature scaling techniques : **Standardization and Robust Scaler**.\n6. Work on 7 datasets which are as follows:\n\na. **DATASET 1**-Reduced featureset using RFECV along with standardization/Robust Scaler.\n\nb. **DATASET 2**-Dataset with only engineered features along with Standardization/Robust Scaler\n\nc. **DATASET 3** \u2014 Dataset with transformed features on the reduced dimensionality along with standardization/Robust Scaler\n\nd. **DATASET 4**\u2014 PCA on the original features. Taken only those many principal components which would explain 95% of the variance in the data.\n\ne. **DATASET 5**\u2014 Oversampling the data to make it balanced and also on top of that reduced feature set along with feature transformation and standardization/Robust Scaler\n\nf. **DATASET 6**\u2014 PCA on the reduced feature set.\n\ng. **DATASET 7**\u2014 Dataset with transformed features on the reduced dimensionality along with standardization/Robust Scaler and oversampling.\n\n7\\. Models used in this competition are:\n\na. Logistic Regression\n\nb. Support Vector Machine\n\nc. Decision Tree classifier\n\nd. Random Forest Classifier\n\ne. Xgboost Classifier\n\nf. SGDClassifier\n\ng. Voting and Stacking Classifier\n\n## SMOTE\n\nSMOTE is an algorithm that creates **s...",
      "url": "https://medium.com/@akwsir96/dont-overfit-ii-a-kaggle-competition-d087344b845e"
    },
    {
      "title": "General Tips for participating Kaggle Competitions",
      "text": "# General Tips for participating Kaggle Competitions\n\n\u2022\n\n196 likes\u202286,040 views\n\n![Mark Peng](https://cdn.slidesharecdn.com/profile-photo-markpeng-48x48.jpg?cb=1709552824)\n\n[Mark Peng](https://www.slideshare.net/markpeng?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview) Follow\n\nThe slides of a talk at Spark Taiwan User Group to share my experience and some general tips for participating kaggle competitions. Read less\n\nRead more\n\nReport\n\nShare\n\nReport\n\nShare\n\n1 of 74\n\nDownload nowDownload to read offline\n\n![General Tips for participating Competitions Mark Peng 2015.12.16 @ Spark Taiwan User Group https://tw.linkedin.com/in/markpeng  ](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/75/General-Tips-for-participating-Kaggle-Competitions-1-2048.jpg)\n\n![Kaggle Profile (Master Tier) https://www.kaggle.com/markpeng 2 Big Data Scientist  ](data:image/gif;base64)\n\n![3 Things I Want to Share \u2022 Quick overview to Kaggle rules \u2022 Why cross-validation matters \u2022 Mostly used ML models \u2022 Feature engineering methods \u2022 Ensemble learning \u2022 Team up \u2022 Recommended books, MOOCs and resources  ](data:image/gif;base64)\n\n![Kaggle Competition Dataset and Rules 4 Training Dataset Private LBPublic LB Validation feedback but sometimes misleading Testing Dataset Might be different from public LB (used to determine final prize winners!)  ](data:image/gif;base64)\n\n![5 How to become a Kaggle Master \u2022 To achieve Master tier, you must fulfill 2 criteria \u2022 Consistency: at least 2 Top 10% finishes in public competitions \u2022 Excellence: at least 1 of those finishes in the top 10 positions \u2022 Note that not all competitions count toward earning Master tier! Reference: https://www.kaggle.com/wiki/UserRankingAndTierSystem  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 6 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![Cross Validation The key to avoid overfitting 7  ](data:image/gif;base64)\n\n![Underfitting and Overfitting We want to find a model with lowest generalization error (hopefully) 8 Model Complexity HighLow PredictionError Training Error Validation Error (Local CV) Testing Error (Private LB) High Variance Low Bias High Bias Low Variance Lowest Generalization Error OverfittingUnderfitting  ](data:image/gif;base64)\n\n![9 Big Shake Up on Private LB! Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/leaderboard/private  ](data:image/gif;base64)\n\n![10 Who is the King of Overfitting? Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission Public LB RMSE: Private LB RMSE: They even wrote a post to show off their perfect overfitting! Num. of Features: 41 Training Data Size: 137 Testing Data Size: 10,000  ](data:image/gif;base64)\n\n![11 K-fold Cross Validation (K = 5) fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 Round 1 Round 2 Round 3 Round 4 Round 5 Validation Data Training Data score(CV) = the average of evaluation scores from each fold You can also repeat the process many times!  ](data:image/gif;base64)\n\n![12 K-fold Cross Validation Tips \u2022 It is normal to experience big shake up on private LB if not using local CV correctly \u2022 5 or 10 folds are not always the best choices (you need to consider the cost of computation time for training models) \u2022 Which K to choose? \u2022 Depends on your data \u2022 Mimic the ratio of training and testing in validation process \u2022 Find a K with lowest gap between local CV and public LB scores \u2022 Standard deviation of K-fold CV score matters more than mean! \u2022 Stratified K-fold CV is important for imbalanced dataset, especially for classification problems  ](data:image/gif;base64)\n\n![13 Stratified K-fold Cross Validation (K = 5) Round 1 Round 2 Round 3 Round 4 Round 5Class Distributions F M Keep the distribution of classes in each fold Validation Data Training Data  ](data:image/gif;base64)\n\n![14 Should I Trust Public LB? \u2022 Yes if you can find a K-fold CV that follows the same trend with public LB \u2022 High positive correlation between local CV and public LB \u2022 The score increases and decreases in both local CV and public LB \u2022 Trust more in your local CV!  ](data:image/gif;base64)\n\n![Data Cleaning Making data more analyzable 15  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 16 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![17 Data Cleaning Techniques \u2022 Data cleaning is the removal of duplicates, useless data, or fixing of missing data \u2022 Reduce dimensional complexity of dataset \u2022 Make training faster without hurting the performance \u2022 Apply imputation methods to help (hopefully) utilize incomplete rows \u2022 Incomplete rows may contain relevant features (don\u2019t just drop them!) \u2022 In the risk of distorting original data, so be cautious!  ](data:image/gif;base64)\n\n![18 Data Cleaning Techniques (cont.) \u2022 Remove duplicate features \u2022 Columns having the same value distribution or variance \u2022 Only need to keep one of them \u2022 Remove constant features \u2022 Columns with only one unique value \u2022 R: sapply(data, function(x) length(unique(x))) \u2022 Remove features with near-zero variance \u2022 R: nearZeroVar(data, saveMetrics=T) in caret package \u2022 Be sure to know what you are doing before removing any features  ](data:image/gif;base64)\n\n![19 Data Cleaning Techniques (cont.) \u2022 Some machine learning tools cannot accept NAs in the input \u2022 Encode missing values to avoid NAs \u2022 Binary features \u2022 -1 for negatives, 0 for missing values and 1 for positives \u2022 Categorical features \u2022 Encode as an unique category \u2022 \u201cUnknown\u201d, \u201cMissing\u201d, \u2026. \u2022 Numeric features \u2022 Encode as a big positive or negative number \u2022 999, -99999, \u2026.  ](data:image/gif;base64)\n\n![20 Data Cleaning Techniques (cont.) \u2022 Basic ways to impute missing values \u2022 mean, median or most frequent value of feature \u2022 R: impute(x, fun = mean) in Hmisc package \u2022 Python: Imputer(strategy='mean', axis=0) in scikit-learn package  ](data:image/gif;base64)\n\n![21 Data Cleaning Techniques (cont.) \u2022 Advanced: multiple imputation \u2022 Impute incomplete columns based on other columns in the data \u2022 R: mice package (Multivariate Imputation by Chained Equations) \u2022 Imputation would not always give you positive improvements, thus you have to validate it cautiously  ](data:image/gif;base64)\n\n![Mostly Used Models What models to use? 22  ](data:image/gif;base64)\n\n![Model Type Name R Python Regression Linear Regression \u2022 glm, glmnet \u2022 sklearn.linear_model.LinearRegression Ridge Regression \u2022 glmnet \u2022 sklearn.linear_model.Ridge Lasso Regression \u2022 glmnet \u2022 sklearn.linear_model.Lasso Instance-based K-nearest Neighbor (KNN) \u2022 knn \u2022 sklearn.neighbors.KNeighborsClassifier Support Vector Machines (SVM) \u2022 svm {e1071} \u2022 LiblinearR \u2022 sklearn.svm.SVC, sklearn.svm.SVR \u2022 sklearn.svm.LinearSVC, sklearn.svm.LinearSVR Hyperplane-based Naive Bayes \u2022 naiveBayes {e1071} \u2022 sklearn.naive_bayes.GaussianNB \u2022 sklearn.naive_bayes.MultinomialNB \u2022 sklearn.naive_bayes.BernoulliNB Logistic Regression \u2022 glm, glmnet \u2022 LiblinearR \u2022 sklearn.linear_model.LogisticRegression Ensemble Trees Random Forests \u2022 randomForest \u2022 sklearn.ensemble.RandomForestClassifier \u2022 sklearn.ensemble.RandomForestRegressor Extremely Randomized Trees \u2022 extraTrees \u2022 sklearn.ensemble.ExtraTreesClassifier \u2022 sklearn.ensemble.ExtraTreesRegressor Gradient Boosting Machines (GBM) \u2022 gbm \u2022 xgboost \u2022 sklearn.ensemble.GradientBoostingClassifier \u2022 sklearn.ensemble.GradientBoostingRegressor \u2022 xgboost Neural Network Multi-layer Neural Netwo...",
      "url": "https://www.slideshare.net/slideshow/general-tips-for-participating-kaggle-competitions/56209561"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) [2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2) 3 [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Model Training**\n\nWe can improve a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forset is the number of trees in the forest and the maximum number of features used in developing each tree.\u00a0**We need to understand how models work and what impact does each parameter have to the model\u2019s performance, be it accuracy, robustness or speed.**\n\nNormally we would find the best set of parameters by a process called\u00a0**[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**. Actually what it does is simply iterating through all the possible combinations and find the best one.\n\nBy the way, random forest usually reach optimum when\u00a0`max_features`\u00a0is set to the square root of the total number of features.\n\nHere I\u2019d like to stress some points about tuning XGB. These parameters are generally considered to have real impacts on its performance:\n\n- `eta`: Step size used in updating weights. Lower\u00a0`eta`\u00a0means slower training but better convergence.\n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration. This is to combat overfitting.\n- `colsample_bytree`: The ratio of features used in each iteration. This is like\u00a0`max_features`\u00a0in\u00a0`RandomForestClassifier`.\n- `max_depth`: The maximum depth of each tree. Unlike random forest,\u00a0**gradient boosting would eventually overfit if we do not limit its depth**.\n- `early_stopping_rounds`: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n2. Set\u00a0`eta`\u00a0to a relatively high value (e.g. 0.05 ~ 0.1),\u00a0`num_round`\u00a0to 300 ~ 500.\n3. Use grid search to find the best combination of other parameters.\n4. Gradually lower\u00a0`eta`\u00a0until we reach the optimum.\n5. **Use the validation set as\u00a0`watch_list`\u00a0to re-train the model with the best parameters. Observe how score changes on validation set in each iteration. Find the optimal value for\u00a0`early_stopping_rounds`.**\n\nFinally, note that models with randomness all have a parameter like\u00a0`seed`\u00a0or\u00a0`random_state`\u00a0to control the random seed.\u00a0**You must record this**\u00a0with all other parameters when you get a good model. Otherwise you wouldn\u2019t be able to reproduce it.\n\n**Cross Validation**\n\n**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**\u00a0is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse.\u00a0**It is widely believed that we should trust our CV scores under such situation.** Ideally we would want\u00a0**CV scores obtained by different approaches to improve in sync with each other and with the LB score**, but this is not always possible.\n\nUsually\u00a0**5-fold CV**\u00a0is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nHow to do CV properly is not a trivial problem. It requires constant experiment and case-by-case discussion. Many Kagglers share their CV approaches (like\u00a0[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)) after competitions when they feel that reliable CV is not easy.\n\n**Ensemble Generation**\n\n[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\u00a0refers to the technique of combining different models. It\u00a0**reduces both bias and variance of the final model**\u00a0(you can find a proof\u00a0[here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)), thus\u00a0**increasing the score and reducing the risk of overfitting**. Recently it became virtually impossible to win prize without using ensemble in Kaggle competitions.\n\nCommon approaches of ensemble learning are:\n\n- **Bagging**: Use different random subsets of training data to train each base model. Then all the base models vote to generate the final predictions. This is how random forest works.\n- **Boosting**: Train base models iteratively, modify the weights of training samples according to the last iteration. This is how gradient boosted trees work. (Actually it\u2019s not the whole story. Apart from boosting, GBTs try to learn the residuals of earlier iterations.) It performs better than bagging but is more prone to overfitting.\n- **Blending**: Use non-overlapping data to train different base models and take a weighted average of them to obtain the final predictions. This is easy to implement but uses less data.\n- **Stacking**: To be discussed next.\n\nIn theory, for the ensemble to perform well, two factors matter:\n\n- **Base models should be as unrelated as possibly**. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.\n- **Performance of base models shouldn\u2019t differ to much.**\n\nActually we have a\u00a0**trade-off**\u00a0here. In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.\n\n**Stacking**\n\nCompared with blending, stacking makes better use of training data. Here\u2019s a diagram of how it works:\n\n[![Stacking](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2099%200'%3E%3C/svg%3E)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)\n\n_(Taken from\u00a0[Faron](https://www.kaggle.com/mmueller). Many thanks!)_\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example. First we split the training data into 5 folds. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold.\u00a0**You have to keep the predictions on the testing data as well.**\u00a0This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data. After 5 iterations we will obtain a matrix of shape\u00a0`#(samples in training data) X #(base models)`. This matrix is then fed to the stacker (it\u2019s just another model) in the second level. After the stacker is fitted, use the predictions on testing data by base models ( **each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape**) as the input for the stacker and obtain our final predictions.\n\nMaybe it\u2019s better to just show the codes:\n\nPrize winners usually have larger and much more complicated ensembles. For beginner, implementing a correct 5-fold stacking is good enough.\n\n**\\*Pipeline**\n\nWe can see that the workflow for a Kaggle competition is quite complex, especially for model selection and ensemble. Ideally, we need a highly automated pipeline capable of:\n\n- **Modularized feature transformations**. We only need to write a few lines of codes (or better, rules / DSLs) and the new feature is added to the training set.\n- **Automated grid search**. We only need to set up models and parameter grid, the search w...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3"
    }
  ]
}