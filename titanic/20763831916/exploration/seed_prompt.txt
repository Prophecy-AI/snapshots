# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival patterns by feature

## Problem Overview
- Binary classification: Predict survival (0/1)
- Evaluation metric: Accuracy
- Small dataset: 891 training samples, 418 test samples
- Target is slightly imbalanced: 38.4% survived, 61.6% did not survive

## Feature Engineering (Critical for this problem)

### Title Extraction from Name
Extract titles from passenger names using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Mrs, Miss, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into "Rare"
- Map Mlle/Ms → Miss, Mme → Mrs
- Title is highly predictive of survival (Master = young boys, Miss/Mrs = females)

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Medium family sizes (2-4) have higher survival rates than solo travelers or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/class)
- Cabin deck letter can be extracted (first character) but 77% missing

### Name Length
- Name_length = len(Name) - can be weakly predictive

### Age and Fare Binning
- Bin Age into categories: 0-16, 16-32, 32-48, 48-64, 64+
- Bin Fare into quartiles or categories: 0-7.91, 7.91-14.45, 14.45-31, 31+
- Binning helps tree models and reduces noise

## Missing Value Handling

### Age (20% missing)
- Fill with median grouped by Pclass and Sex
- Or use random values within (mean - std, mean + std)
- Or use model-based imputation (predict age from other features)

### Cabin (77% missing)
- Create Has_Cabin binary feature
- Don't try to impute - too much missing

### Embarked (2 missing in train)
- Fill with mode ('S' - Southampton)

### Fare (1 missing in test)
- Fill with median

## Feature Selection
Drop these columns after feature engineering:
- PassengerId (identifier only)
- Name (after extracting Title)
- Ticket (too many unique values, hard to use)
- Cabin (after creating Has_Cabin)
- SibSp (after creating FamilySize)

Keep/create these features:
- Sex_Code (0=female, 1=male)
- Pclass
- Embarked_Code
- Title_Code
- FamilySize
- IsAlone
- AgeBin_Code
- FareBin_Code
- Has_Cabin

## Models

### Recommended Models (in order of typical performance)
1. **Gradient Boosting (XGBoost/LightGBM)** - Usually best single model
2. **Random Forest** - Robust, handles feature interactions well
3. **Extra Trees** - Similar to RF, more randomness
4. **Logistic Regression** - Good baseline, interpretable
5. **SVM with RBF kernel** - Can work well with proper scaling

### Ensemble Approaches (Highly Recommended)
**Voting Classifier:**
- Combine 5-10 diverse models
- Hard voting (majority) or soft voting (probability averaging)
- Include: RF, ExtraTrees, GradientBoosting, AdaBoost, LogisticRegression, SVC, XGBoost

**Stacking (Best approach for this competition):**
- Level 1: Use 5 base models (RF, ExtraTrees, AdaBoost, GradientBoosting, SVC)
- Generate out-of-fold predictions using 5-fold CV
- Level 2: Train XGBoost on the stacked predictions
- This approach achieved 0.808 LB score (top 9%)

## Hyperparameter Tuning

### Random Forest / Extra Trees
```
n_estimators: 100-500
max_depth: 4-10
min_samples_leaf: 2-5
max_features: 'sqrt' or 0.3-0.5
```

### Gradient Boosting / XGBoost
```
n_estimators: 100-500
max_depth: 3-6
learning_rate: 0.01-0.1
min_samples_leaf: 2-5
subsample: 0.8
```

### SVC
```
kernel: 'rbf' or 'linear'
C: 0.1-10
gamma: 'scale' or 'auto'
```

## Validation Strategy
- Use Stratified K-Fold CV (k=5) to maintain class distribution
- ShuffleSplit with 60/30 train/test split (leaving 10% out) also works
- Cross-validation score is more reliable than single train/test split

## Key Insights from Top Kernels
1. Sex is the most important feature (females 74% survival vs males 19%)
2. Pclass strongly correlates with survival (1st class 63%, 3rd class 24%)
3. Title extraction captures both sex and social status information
4. Family size has non-linear relationship with survival (medium families best)
5. Feature engineering is more important than model selection for this dataset
6. Ensemble methods consistently outperform single models

## Submission Tips
- Always check submission format: PassengerId, Survived (0 or 1)
- Ensure predictions are integers (0 or 1), not floats
- Verify 418 rows in submission
