# Titanic Survival Prediction - Loop 2 Strategy

## Current Status
- Best CV score: 0.8316 from exp_000 (Baseline XGBoost)
- Best LB score: 0.7584 (exp_000)
- CV-LB gap: +0.0732 (7.3%) â†’ **SEVERE OVERFITTING**
- Submissions used: 1/10 (7 remaining)

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY** - execution is sound, but results show overfitting
- Evaluator's top priority: "Submit baseline then implement ensemble" - We submitted and got 0.7584 LB, which is BELOW the gender-only baseline (~76-77%)
- Key concerns raised: 
  1. Target of 1.0 is impossible (agree - state-of-the-art is 81-85%)
  2. Single model approach (agree - ensembles are more robust)
  3. CV-LB gap (now confirmed at 7.3% - this is the critical issue)
- **My synthesis**: The 7.3% CV-LB gap is the #1 problem. Before trying ensembles, we need to fix the overfitting issue. A complex ensemble of overfitting models will still overfit.

## Critical Issue: Severe Overfitting
The LB score of 0.7584 is:
- Below the gender-only baseline (~76-77%)
- Below even the worst CV fold (0.7921)
- 7.3% lower than CV score

**Root causes identified:**
1. Too many features (13) - some may be noise
2. XGBoost may be memorizing training patterns
3. Age imputation on combined train+test (minor leakage)
4. Distribution shift: Embarked C is 24.4% in test vs 18.9% in train

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, survival patterns
- `exploration/evolver_loop1_lb_feedback.ipynb` - CV-LB gap analysis
- `research/kernels/alexisbcook_titanic-tutorial/` - Simple baseline with 4 features

Key patterns (from EDA):
- Sex is dominant: Female 74.2% survival vs Male 18.9%
- Pclass matters: 1st class 63%, 3rd class 24%
- Title captures both sex and status
- FamilySize has non-linear relationship (medium families best)

## Recommended Approaches (Priority Order)

### Priority 1: Simpler Model with Fewer Features
**Rationale**: The official Titanic tutorial uses just 4 features (Pclass, Sex, SibSp, Parch) with Random Forest and achieves reasonable LB scores. Our 13-feature XGBoost is overfitting.

**Action**:
- Use only core features: Sex, Pclass, SibSp, Parch, Fare, Embarked
- Try Random Forest with max_depth=5, n_estimators=100
- Compare CV and expected LB performance

### Priority 2: Voting Ensemble with Simple Models
**Rationale**: Ensembles of diverse simple models are more robust than a single complex model.

**Action**:
- Combine: LogisticRegression, RandomForest, GradientBoosting, SVC
- Use soft voting (probability averaging)
- Keep features simple (6-8 features max)

### Priority 3: Regularization and Hyperparameter Tuning
**Rationale**: If using XGBoost, increase regularization to prevent overfitting.

**Action**:
- Reduce max_depth to 3
- Increase reg_alpha and reg_lambda (L1/L2 regularization)
- Reduce n_estimators
- Use early stopping

### Priority 4: Feature Engineering Refinement
**Rationale**: Some features may be adding noise rather than signal.

**Action**:
- Keep: Sex, Pclass, Title, FamilySize
- Test: Has_Cabin, Deck (may be noisy due to 77% missing)
- Remove: Name_length (weak signal)
- Impute Age on train only, not combined data

## What NOT to Try
- More complex models (will overfit more)
- More features without validation
- Hyperparameter tuning before fixing overfitting
- Stacking (premature - need robust base models first)

## Validation Notes
- Use 5-fold Stratified CV (current approach is sound)
- **Trust CV trend, but calibrate for LB gap**
- Expected LB = CV - 0.07 (based on current gap)
- A model with CV 0.80 might achieve LB 0.73-0.77

## Target Recalibration
- **Impossible target**: 1.0 (100% accuracy)
- **State-of-the-art**: 81-85% LB
- **Realistic goal**: 78-80% LB (would be a significant improvement)
- **Immediate goal**: Beat 76% LB (gender baseline)

## Experiment Plan for Loop 2
1. **exp_001**: Simple Random Forest with 4-6 features (match official tutorial)
2. **exp_002**: Voting ensemble with simple models
3. **exp_003**: Regularized XGBoost with fewer features

Focus on reducing the CV-LB gap before optimizing CV score.
