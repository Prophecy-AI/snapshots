# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival patterns by feature

## Problem Overview
- Binary classification: Predict survival (0/1)
- Evaluation metric: Accuracy
- Small dataset: 891 training samples, 418 test samples
- Target is slightly imbalanced: 38.4% survived, 61.6% did not survive
- **Realistic target: 80-85% accuracy is state-of-the-art** (top solutions achieve ~81-85%)
- 100% accuracy is not achievable due to inherent noise in the data

## Feature Engineering (Critical for this problem)

### Title Extraction from Name (MOST IMPORTANT)
Extract titles from passenger names using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Mrs, Miss, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into "Rare"
- Map Mlle/Ms → Miss, Mme → Mrs
- Title is highly predictive of survival (Master = young boys, Miss/Mrs = females)

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Medium family sizes (2-4) have higher survival rates than solo travelers or large families
- Consider FamilySize bins: 1 (alone), 2-4 (small), 5+ (large)

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/class)
- **Deck** = First character of Cabin (A, B, C, D, E, F, G, T) - fill missing with 'U' (unknown)
- Deck B, D, E have higher survival rates (~74-75%)
- Deck U (unknown) has lowest survival rate (~30%)

### Name Length
- Name_length = len(Name) - can be weakly predictive

### Age and Fare Binning
- Bin Age into categories: 0-16, 16-32, 32-48, 48-64, 64+
- Bin Fare into quartiles or categories: 0-7.91, 7.91-14.45, 14.45-31, 31+
- Binning helps tree models and reduces noise

### Ticket Features (Advanced)
- Extract ticket prefix (letters before numbers)
- Ticket frequency (how many passengers share same ticket)

## Missing Value Handling

### Age (20% missing) - Critical
**Option 1:** Fill with median grouped by Pclass, Sex, and Title
**Option 2:** Use random values within (mean - std, mean + std)
**Option 3 (Best):** Use XGBoost/RandomForest to predict Age from other features

### Cabin (77% missing)
- Create Has_Cabin binary feature
- Extract Deck letter, fill missing with 'U'
- Don't try to impute cabin numbers - too much missing

### Embarked (2 missing in train)
- Fill with mode ('S' - Southampton)
- Or analyze: if Pclass=1 and Fare=80, likely 'C' (Cherbourg)

### Fare (1 missing in test)
- Fill with median grouped by Pclass and Embarked

## Feature Selection
**Drop these columns after feature engineering:**
- PassengerId (identifier only)
- Name (after extracting Title and Name_length)
- Ticket (after extracting features)
- Cabin (after creating Has_Cabin and Deck)

**Keep/create these features:**
- Sex_Code (0=female, 1=male)
- Pclass
- Embarked_Code
- Title_Code
- FamilySize
- IsAlone
- AgeBin_Code or Age (continuous)
- FareBin_Code or Fare (continuous)
- Has_Cabin
- Deck_Code
- Name_length (optional)

## Models

### Recommended Models (in order of typical performance)
1. **XGBoost** - Usually best single model, handles missing values
2. **LightGBM** - Fast, good performance
3. **Random Forest** - Robust, handles feature interactions well
4. **Gradient Boosting** - Strong performer
5. **Extra Trees** - Similar to RF, more randomness
6. **Logistic Regression** - Good baseline, interpretable
7. **SVM with RBF kernel** - Can work well with proper scaling

### Ensemble Approaches (HIGHLY RECOMMENDED - Key to Top Scores)

**Voting Classifier:**
- Combine 5-10 diverse models
- Hard voting (majority) or soft voting (probability averaging)
- Include: RF, ExtraTrees, GradientBoosting, AdaBoost, LogisticRegression, SVC, XGBoost
- Soft voting often performs better

**Stacking (Best approach for this competition):**
- Level 1: Use 5 base models (RF, ExtraTrees, AdaBoost, GradientBoosting, SVC)
- Generate out-of-fold predictions using 5-fold CV to avoid data leakage
- Level 2: Train XGBoost or LogisticRegression on the stacked predictions
- This approach achieved 0.808 LB score (top 9%)

**Blending:**
- Average predictions from multiple models
- Weight models by their CV performance

## Hyperparameter Tuning

### XGBoost (Primary Model)
```
n_estimators: 100-500
max_depth: 3-6
learning_rate: 0.01-0.1
subsample: 0.8
colsample_bytree: 0.8
reg_alpha: 0-1
reg_lambda: 0-1
scale_pos_weight: 1 or class_weight
```

### Random Forest / Extra Trees
```
n_estimators: 100-500
max_depth: 4-10
min_samples_leaf: 2-5
max_features: 'sqrt' or 0.3-0.5
```

### Gradient Boosting
```
n_estimators: 100-500
max_depth: 3-6
learning_rate: 0.01-0.1
min_samples_leaf: 2-5
subsample: 0.8
```

### SVC
```
kernel: 'rbf' or 'linear'
C: 0.1-10
gamma: 'scale' or 'auto'
probability: True (for soft voting)
```

### Tuning Methods
- GridSearchCV for exhaustive search
- RandomizedSearchCV for faster exploration
- Bayesian optimization (Optuna) for efficient tuning

## Validation Strategy
- Use Stratified K-Fold CV (k=5 or k=10) to maintain class distribution
- ShuffleSplit with 60/30 train/test split (leaving 10% out) also works
- Cross-validation score is more reliable than single train/test split
- **Important:** CV score ~83-84% typically translates to ~80-81% LB score

## Key Insights from Top Kernels
1. **Sex is the most important feature** (females 74% survival vs males 19%)
2. **Pclass strongly correlates with survival** (1st class 63%, 3rd class 24%)
3. **Title extraction captures both sex and social status** - very powerful feature
4. **Family size has non-linear relationship** with survival (medium families best)
5. **Feature engineering is MORE important than model selection** for this dataset
6. **Ensemble methods consistently outperform single models**
7. **Hyperparameter tuning with grid search** provides incremental improvements

## Advanced Techniques

### Transformer-based Approach (Experimental)
- Render each row as a string (concatenate all features)
- Use pre-trained DistilBERT to extract 768-dim feature vector
- Train XGBoost on transformer features
- Achieved 80.1% (top 3%) with minimal feature engineering

### Feature Interactions
- Create interaction features: Sex * Pclass, Age * Pclass
- Polynomial features for continuous variables

### Target Encoding (Use with Caution)
- Encode categorical features with target mean
- Use K-fold target encoding to avoid leakage

## Submission Tips
- Always check submission format: PassengerId, Survived (0 or 1)
- Ensure predictions are integers (0 or 1), not floats
- Verify 418 rows in submission
- Try multiple random seeds and average predictions
- Submit best CV model, not necessarily best single-fold model

## Expected Performance
- Baseline (gender only): ~76-77%
- Good feature engineering: ~79-80%
- Tuned single model: ~80-82%
- Ensemble methods: ~81-85%
- State-of-the-art: ~84-85%
