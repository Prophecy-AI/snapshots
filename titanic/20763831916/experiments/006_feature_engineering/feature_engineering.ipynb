{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d115ca",
   "metadata": {},
   "source": [
    "# Feature Engineering for Threshold-Tuned Ensemble\n",
    "\n",
    "## Goal: Improve LB by adding proven features\n",
    "\n",
    "Current best: LB 0.7847 (Threshold-Tuned Ensemble, CV 0.8373)\n",
    "\n",
    "## Key Learnings from Previous Experiments:\n",
    "1. Stacking FAILED - simpler models work better\n",
    "2. ~31% survival rate (130-133 survivors) is optimal for LB\n",
    "3. CV alone is NOT reliable - same CV gave LB 0.7847 vs 0.7631\n",
    "\n",
    "## Features to Add:\n",
    "1. FamilySize = SibSp + Parch + 1\n",
    "2. IsAlone = 1 if FamilySize == 1\n",
    "3. Has_Cabin = 1 if Cabin is not null\n",
    "4. TicketFreq = Number of passengers sharing same ticket\n",
    "5. FareBin = Quartile-based fare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training survival rate: {train['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced preprocessing with NEW features\n",
    "def preprocess_enhanced(train_df, test_df):\n",
    "    train_data = train_df.copy()\n",
    "    test_data = test_df.copy()\n",
    "    \n",
    "    # ============ TITLE EXTRACTION ============\n",
    "    for df in [train_data, test_data]:\n",
    "        df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "        title_mapping = {\n",
    "            'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "            'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
    "            'Lady': 'Rare', 'Countess': 'Rare', 'Capt': 'Rare', 'Col': 'Rare',\n",
    "            'Don': 'Rare', 'Dr': 'Rare', 'Major': 'Rare', 'Rev': 'Rare',\n",
    "            'Sir': 'Rare', 'Jonkheer': 'Rare', 'Dona': 'Rare'\n",
    "        }\n",
    "        df['Title'] = df['Title'].map(title_mapping).fillna('Rare')\n",
    "    \n",
    "    title_order = ['Mr', 'Miss', 'Mrs', 'Master', 'Rare']\n",
    "    title_map = {t: i for i, t in enumerate(title_order)}\n",
    "    train_data['Title_Code'] = train_data['Title'].map(title_map)\n",
    "    test_data['Title_Code'] = test_data['Title'].map(title_map)\n",
    "    \n",
    "    # ============ SEX ENCODING ============\n",
    "    train_data['Sex_Code'] = (train_data['Sex'] == 'male').astype(int)\n",
    "    test_data['Sex_Code'] = (test_data['Sex'] == 'male').astype(int)\n",
    "    \n",
    "    # ============ EMBARKED ============\n",
    "    train_data['Embarked'] = train_data['Embarked'].fillna('S')\n",
    "    test_data['Embarked'] = test_data['Embarked'].fillna('S')\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_data['Embarked_Code'] = train_data['Embarked'].map(embarked_map)\n",
    "    test_data['Embarked_Code'] = test_data['Embarked'].map(embarked_map)\n",
    "    \n",
    "    # ============ NEW FEATURE 1: FamilySize ============\n",
    "    train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n",
    "    test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n",
    "    \n",
    "    # ============ NEW FEATURE 2: IsAlone ============\n",
    "    train_data['IsAlone'] = (train_data['FamilySize'] == 1).astype(int)\n",
    "    test_data['IsAlone'] = (test_data['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # ============ NEW FEATURE 3: Has_Cabin ============\n",
    "    train_data['Has_Cabin'] = train_data['Cabin'].notna().astype(int)\n",
    "    test_data['Has_Cabin'] = test_data['Cabin'].notna().astype(int)\n",
    "    \n",
    "    # ============ NEW FEATURE 4: TicketFreq ============\n",
    "    # Count how many passengers share the same ticket\n",
    "    combined_tickets = pd.concat([train_data['Ticket'], test_data['Ticket']])\n",
    "    ticket_counts = combined_tickets.value_counts()\n",
    "    train_data['TicketFreq'] = train_data['Ticket'].map(ticket_counts)\n",
    "    test_data['TicketFreq'] = test_data['Ticket'].map(ticket_counts)\n",
    "    \n",
    "    # ============ FARE IMPUTATION (from train only) ============\n",
    "    train_fare_median = train_data['Fare'].median()\n",
    "    train_data['Fare'] = train_data['Fare'].fillna(train_fare_median)\n",
    "    test_data['Fare'] = test_data['Fare'].fillna(train_fare_median)\n",
    "    \n",
    "    # ============ NEW FEATURE 5: FareBin ============\n",
    "    # Use quartiles from training data - handle edge cases\n",
    "    fare_bins = [-0.001, 7.91, 14.45, 31, 1000]\n",
    "    train_data['FareBin'] = pd.cut(train_data['Fare'], bins=fare_bins, labels=[0, 1, 2, 3])\n",
    "    test_data['FareBin'] = pd.cut(test_data['Fare'], bins=fare_bins, labels=[0, 1, 2, 3])\n",
    "    # Fill any NaN with median bin\n",
    "    train_data['FareBin'] = train_data['FareBin'].fillna(1).astype(int)\n",
    "    test_data['FareBin'] = test_data['FareBin'].fillna(1).astype(int)\n",
    "    \n",
    "    # ============ AGE IMPUTATION (from train only) ============\n",
    "    age_medians = train_data.groupby(['Title', 'Pclass'])['Age'].median()\n",
    "    train_age_median = train_data['Age'].median()\n",
    "    \n",
    "    def fill_age(row, medians, fallback):\n",
    "        if pd.isna(row['Age']):\n",
    "            try:\n",
    "                return medians[(row['Title'], row['Pclass'])]\n",
    "            except KeyError:\n",
    "                return fallback\n",
    "        return row['Age']\n",
    "    \n",
    "    train_data['Age'] = train_data.apply(lambda x: fill_age(x, age_medians, train_age_median), axis=1)\n",
    "    test_data['Age'] = test_data.apply(lambda x: fill_age(x, age_medians, train_age_median), axis=1)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_processed, test_processed = preprocess_enhanced(train, test)\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify new features\n",
    "print(\"NEW FEATURES SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nFamilySize distribution (train):\")\n",
    "print(train_processed['FamilySize'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nIsAlone distribution (train):\")\n",
    "print(train_processed['IsAlone'].value_counts())\n",
    "\n",
    "print(f\"\\nHas_Cabin distribution (train):\")\n",
    "print(train_processed['Has_Cabin'].value_counts())\n",
    "\n",
    "print(f\"\\nTicketFreq distribution (train):\")\n",
    "print(train_processed['TicketFreq'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nFareBin distribution (train):\")\n",
    "print(train_processed['FareBin'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check survival rates for new features\n",
    "print(\"\\nSURVIVAL RATES BY NEW FEATURES:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nFamilySize survival rates:\")\n",
    "for size in sorted(train_processed['FamilySize'].unique()):\n",
    "    mask = train_processed['FamilySize'] == size\n",
    "    rate = train_processed.loc[mask, 'Survived'].mean()\n",
    "    count = mask.sum()\n",
    "    print(f\"  Size {size}: {rate:.3f} (n={count})\")\n",
    "\n",
    "print(\"\\nIsAlone survival rates:\")\n",
    "for alone in [0, 1]:\n",
    "    mask = train_processed['IsAlone'] == alone\n",
    "    rate = train_processed.loc[mask, 'Survived'].mean()\n",
    "    count = mask.sum()\n",
    "    label = 'Alone' if alone == 1 else 'With family'\n",
    "    print(f\"  {label}: {rate:.3f} (n={count})\")\n",
    "\n",
    "print(\"\\nHas_Cabin survival rates:\")\n",
    "for cabin in [0, 1]:\n",
    "    mask = train_processed['Has_Cabin'] == cabin\n",
    "    rate = train_processed.loc[mask, 'Survived'].mean()\n",
    "    count = mask.sum()\n",
    "    label = 'Has cabin' if cabin == 1 else 'No cabin'\n",
    "    print(f\"  {label}: {rate:.3f} (n={count})\")\n",
    "\n",
    "print(\"\\nFareBin survival rates:\")\n",
    "for bin_val in sorted(train_processed['FareBin'].unique()):\n",
    "    mask = train_processed['FareBin'] == bin_val\n",
    "    rate = train_processed.loc[mask, 'Survived'].mean()\n",
    "    count = mask.sum()\n",
    "    print(f\"  Bin {bin_val}: {rate:.3f} (n={count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features - ORIGINAL 8 features (baseline)\n",
    "original_features = ['Pclass', 'Sex_Code', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Code', 'Title_Code']\n",
    "\n",
    "# ENHANCED features (original + new)\n",
    "enhanced_features = original_features + ['FamilySize', 'IsAlone', 'Has_Cabin', 'TicketFreq', 'FareBin']\n",
    "\n",
    "print(f\"Original features ({len(original_features)}): {original_features}\")\n",
    "print(f\"\\nEnhanced features ({len(enhanced_features)}): {enhanced_features}\")\n",
    "\n",
    "# Prepare data\n",
    "X_orig = train_processed[original_features].values\n",
    "X_enhanced = train_processed[enhanced_features].values\n",
    "y = train_processed['Survived'].values\n",
    "\n",
    "X_test_orig = test_processed[original_features].values\n",
    "X_test_enhanced = test_processed[enhanced_features].values\n",
    "test_ids = test_processed['PassengerId'].values\n",
    "\n",
    "print(f\"\\nOriginal X shape: {X_orig.shape}\")\n",
    "print(f\"Enhanced X shape: {X_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Voting Ensemble (same as exp_003 that achieved LB 0.7847)\n",
    "def create_voting_ensemble():\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=5, min_samples_leaf=5,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    svc = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "    \n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[('rf', rf), ('lr', lr), ('gb', gb), ('svc', svc)],\n",
    "        voting='soft'\n",
    "    )\n",
    "    return ensemble\n",
    "\n",
    "print(\"Voting Ensemble defined (same as exp_003)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ORIGINAL vs ENHANCED features with CV\n",
    "print(\"COMPARING ORIGINAL vs ENHANCED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Test ORIGINAL features\n",
    "print(\"\\n1. ORIGINAL FEATURES (8 features):\")\n",
    "oof_probs_orig = np.zeros(len(X_orig))\n",
    "fold_scores_orig = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_orig, y)):\n",
    "    X_train, X_val = X_orig[train_idx], X_orig[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train\n",
    "    model = create_voting_ensemble()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    val_prob = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    oof_probs_orig[val_idx] = val_prob\n",
    "    \n",
    "    val_pred = (val_prob >= 0.5).astype(int)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_orig.append(fold_acc)\n",
    "    print(f\"  Fold {fold+1}: {fold_acc:.4f}\")\n",
    "\n",
    "cv_orig = np.mean(fold_scores_orig)\n",
    "print(f\"  Mean CV (threshold 0.5): {cv_orig:.4f} (+/- {np.std(fold_scores_orig):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ENHANCED features\n",
    "print(\"\\n2. ENHANCED FEATURES (13 features):\")\n",
    "oof_probs_enhanced = np.zeros(len(X_enhanced))\n",
    "fold_scores_enhanced = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_enhanced, y)):\n",
    "    X_train, X_val = X_enhanced[train_idx], X_enhanced[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train\n",
    "    model = create_voting_ensemble()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    val_prob = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    oof_probs_enhanced[val_idx] = val_prob\n",
    "    \n",
    "    val_pred = (val_prob >= 0.5).astype(int)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_enhanced.append(fold_acc)\n",
    "    print(f\"  Fold {fold+1}: {fold_acc:.4f}\")\n",
    "\n",
    "cv_enhanced = np.mean(fold_scores_enhanced)\n",
    "print(f\"  Mean CV (threshold 0.5): {cv_enhanced:.4f} (+/- {np.std(fold_scores_enhanced):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal (8 features):  CV = {cv_orig:.4f} (+/- {np.std(fold_scores_orig):.4f})\")\n",
    "print(f\"Enhanced (13 features): CV = {cv_enhanced:.4f} (+/- {np.std(fold_scores_enhanced):.4f})\")\n",
    "print(f\"\\nDifference: {cv_enhanced - cv_orig:+.4f}\")\n",
    "\n",
    "if cv_enhanced > cv_orig:\n",
    "    print(\"\\n✓ Enhanced features IMPROVED CV!\")\n",
    "else:\n",
    "    print(\"\\n✗ Enhanced features did NOT improve CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0db8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis for ENHANCED features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THRESHOLD ANALYSIS FOR ENHANCED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train final model on all data for test predictions\n",
    "X_all_scaled = scaler.fit_transform(X_enhanced)\n",
    "X_test_scaled = scaler.transform(X_test_enhanced)\n",
    "\n",
    "final_model = create_voting_ensemble()\n",
    "final_model.fit(X_all_scaled, y)\n",
    "test_probs = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "thresholds = [0.45, 0.50, 0.52, 0.55, 0.58, 0.60, 0.62, 0.65]\n",
    "\n",
    "print(f\"\\n{'Threshold':<12} {'Survivors':<12} {'Survival Rate':<15} {'OOF Accuracy':<15}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    test_preds = (test_probs >= thresh).astype(int)\n",
    "    oof_preds = (oof_probs_enhanced >= thresh).astype(int)\n",
    "    survivors = test_preds.sum()\n",
    "    survival_rate = test_preds.mean()\n",
    "    oof_acc = accuracy_score(y, oof_preds)\n",
    "    \n",
    "    marker = \"\"\n",
    "    if 125 <= survivors <= 135:\n",
    "        marker = \" <- TARGET\"\n",
    "    \n",
    "    print(f\"{thresh:<12.2f} {survivors:<12} {survival_rate:<15.3f} {oof_acc:<15.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2644fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold for ~31% survival rate (130 survivors)\n",
    "target_survivors = 130\n",
    "\n",
    "low, high = 0.4, 0.7\n",
    "while high - low > 0.001:\n",
    "    mid = (low + high) / 2\n",
    "    survivors = (test_probs >= mid).sum()\n",
    "    if survivors > target_survivors:\n",
    "        low = mid\n",
    "    else:\n",
    "        high = mid\n",
    "\n",
    "optimal_threshold = (low + high) / 2\n",
    "optimal_survivors = (test_probs >= optimal_threshold).sum()\n",
    "\n",
    "print(f\"\\nOptimal threshold for ~{target_survivors} survivors: {optimal_threshold:.3f}\")\n",
    "print(f\"Actual survivors: {optimal_survivors}\")\n",
    "print(f\"Survival rate: {optimal_survivors/len(test_probs):.3f}\")\n",
    "\n",
    "# Calculate OOF accuracy at optimal threshold\n",
    "oof_preds_opt = (oof_probs_enhanced >= optimal_threshold).astype(int)\n",
    "oof_acc_opt = accuracy_score(y, oof_preds_opt)\n",
    "print(f\"OOF accuracy at optimal threshold: {oof_acc_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd15d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission with optimal threshold\n",
    "test_preds_final = (test_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_ids,\n",
    "    'Survived': test_preds_final\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved with {len(submission)} rows\")\n",
    "print(f\"\\nSurvived distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nSurvival rate: {submission['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY: Feature Engineering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFeatures added:\")\n",
    "print(f\"  - FamilySize (SibSp + Parch + 1)\")\n",
    "print(f\"  - IsAlone (1 if FamilySize == 1)\")\n",
    "print(f\"  - Has_Cabin (1 if Cabin is not null)\")\n",
    "print(f\"  - TicketFreq (passengers sharing same ticket)\")\n",
    "print(f\"  - FareBin (quartile-based fare categories)\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Original (8 features):  CV = {cv_orig:.4f}\")\n",
    "print(f\"  Enhanced (13 features): CV = {cv_enhanced:.4f}\")\n",
    "print(f\"  Improvement: {cv_enhanced - cv_orig:+.4f}\")\n",
    "\n",
    "print(f\"\\nThreshold-tuned submission:\")\n",
    "print(f\"  Threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"  OOF accuracy: {oof_acc_opt:.4f}\")\n",
    "print(f\"  Survivors: {submission['Survived'].sum()} ({submission['Survived'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nComparison to previous best (exp_003):\")\n",
    "print(f\"  exp_003: CV 0.8373, LB 0.7847, 130 survivors (31.1%)\")\n",
    "print(f\"  This:    CV {oof_acc_opt:.4f}, ?? LB, {submission['Survived'].sum()} survivors ({submission['Survived'].mean()*100:.1f}%)\")\n",
    "\n",
    "if oof_acc_opt > 0.8373:\n",
    "    print(f\"\\n✓ CV IMPROVED! Consider submitting.\")\n",
    "else:\n",
    "    print(f\"\\n✗ CV did not improve. May not be worth submitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save candidate submission\n",
    "import shutil\n",
    "shutil.copy('/home/submission/submission.csv', '/home/code/submission_candidates/candidate_005.csv')\n",
    "print(\"Saved candidate_005.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
