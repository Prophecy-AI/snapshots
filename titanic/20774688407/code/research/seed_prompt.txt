## Current Status
- Best CV score: 0.8361 from exp_002 (RF without Age) and exp_003 (XGBoost)
- Best LB score: 0.7799 from exp_000 (RF with Age)
- CV-LB gap: +5.4% to +6.6% → **CV is NOT a reliable predictor of LB performance**
- Submissions remaining: 2

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Execution was sound.
- Evaluator's top priority: Create ensemble before submitting exp_003. **AGREE - analysis shows majority vote differs from exp_000 by only 2 predictions, which is too conservative.**
- Key concerns raised:
  1. XGBoost has higher CV variance (0.026 vs 0.009) - **Valid concern, but variance alone doesn't predict LB**
  2. 27 predictions changed (6.5%) - **Analyzed: 18 changed 1→0, 9 changed 0→1. Most are 3rd class females.**
  3. Only 2 submissions remaining - **Critical constraint. Must be strategic.**
- Evaluator suggested analyzing which predictions changed - **DONE in loop4_analysis.ipynb**

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, survival rates
- `exploration/evolver_loop1_lb_feedback.ipynb` - Adversarial validation (Age 56.9% shift)
- `exploration/evolver_loop3_lb_feedback.ipynb` - Why removing Age hurt LB
- `exploration/evolver_loop4_analysis.ipynb` - **NEW: XGBoost vs RF prediction comparison**

Key findings from loop 4 analysis:
1. XGBoost (exp_003) differs from RF (exp_000) by 27 predictions (6.5%)
2. XGBoost predicts fewer survivors (145 vs 154)
3. Changes: 18 changed 1→0, 9 changed 0→1
4. Most changes are 3rd class females (14 of 17 female changes)
5. Majority vote ensemble differs from exp_000 by only 2 predictions (too conservative)

## Critical Strategic Insight
**The target of 1.0 (100% accuracy) is IMPOSSIBLE.** 
- Typical best Kaggle Titanic scores are 80-82%
- Our best LB is 0.7799 (78%)
- We are within ~2% of the realistic ceiling

**CV is misleading us.** Higher CV does NOT mean higher LB:
- exp_000: CV 0.8339 → LB 0.7799 (BEST LB)
- exp_001: CV 0.8271 → LB 0.7727
- exp_002: CV 0.8361 → LB 0.7703 (WORST LB despite BEST CV)
- exp_003: CV 0.8361 → LB ??? (untested)

## Recommended Approaches (Priority Order)

### 1. **SUBMIT exp_003 (XGBoost) to test hypothesis**
**Rationale**: With 2 submissions remaining, we need LB feedback to guide final attempt.
- exp_003 uses SAME features as exp_000 (best LB) with different model
- XGBoost gives lower importance to Age (3.3% vs 8%) - may help with distribution shift
- 27 prediction changes is enough to potentially improve LB
- Expected outcomes:
  - If LB > 0.78: XGBoost is better, use for final submission
  - If LB ≈ 0.78: Similar performance, try ensemble for final
  - If LB < 0.77: XGBoost hurts, revert to RF variations for final

### 2. **If exp_003 fails: Try RF with different hyperparameters**
**Rationale**: exp_000 has best LB. Small improvements may help.
- Try n_estimators=300-500, max_depth=5-7
- Try different random seeds and average predictions
- Try bootstrap=False for more stable predictions

### 3. **If exp_003 succeeds: Try XGBoost tuning**
**Rationale**: If XGBoost improves LB, tune it further.
- Try lower learning_rate (0.01-0.03) with more trees (300-500)
- Try different max_depth (3, 5, 6)
- Try feature selection based on XGBoost importance

### 4. **Alternative: Probability-based ensemble**
**Rationale**: Instead of hard voting, use prediction probabilities.
- Get probabilities from RF and XGBoost
- Average probabilities, then threshold at 0.5
- This is more nuanced than majority voting

## What NOT to Try
- **Removing Age entirely** - PROVEN to hurt LB (exp_002)
- **Complex stacking** - exp_001 showed stacking hurt both CV and LB
- **Chasing CV improvements** - CV is NOT correlated with LB for this problem
- **Conservative majority voting** - Only differs by 2 predictions, too similar to exp_000
- **Adding many new features** - Risk of overfitting with limited data

## Validation Notes
- **DO NOT trust CV** - Use LB as the primary signal
- CV-LB gap is ~5-7% and NOT consistent
- With only 2 submissions remaining:
  1. Submit exp_003 to get XGBoost LB feedback
  2. Use final submission for best approach based on feedback

## Submission Strategy
**2 submissions remaining** - Strategic plan:
1. **Next submission**: exp_003 (XGBoost) - tests if different model improves on best features
2. **Final submission**: Based on exp_003 result:
   - If exp_003 > 0.78: Submit tuned XGBoost or XGBoost ensemble
   - If exp_003 ≈ 0.78: Submit probability-based ensemble
   - If exp_003 < 0.77: Submit RF with different hyperparameters

## Expected Outcomes
- Realistic best LB: 0.80-0.82 (top Kaggle scores)
- Current best: 0.7799
- Target: 1.0 (IMPOSSIBLE - this is a learning exercise)

The goal should be incremental improvement toward 0.80+, not 1.0.

## Key Experiment to Run
**exp_004: Submit exp_003 (XGBoost) for LB feedback**
- This is the most informative use of a submission
- We need to know if XGBoost improves on RF with same features
- The 27 prediction changes (6.5%) are significant enough to potentially change LB
- XGBoost's lower reliance on Age (3.3% vs 8%) might help with distribution shift
