## Current Status
- Best CV score: 0.8361 from exp_002 (RF without Age)
- Best LB score: 0.7799 from exp_000 (RF with Age)
- CV-LB gap: +5.4% to +6.6% → **CV is NOT a reliable predictor of LB performance**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Execution was sound.
- Evaluator's top priority was to submit exp_002 to validate the hypothesis. **DONE - hypothesis FAILED.**
- Key concern raised: Fare still in features despite distribution shift. **Valid - should address.**
- **Critical new finding**: Removing Age WORSENED LB (0.7799→0.7703) despite improving CV (0.8339→0.8361). The CV-LB gap WIDENED from 5.4% to 6.6%. This invalidates the distribution shift hypothesis as a solution.

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, survival rates
- `exploration/evolver_loop1_lb_feedback.ipynb` - Adversarial validation (Age 56.9% shift)
- `exploration/evolver_loop3_lb_feedback.ipynb` - **NEW: Analysis of why removing Age hurt LB**

Key findings from loop 3 analysis:
1. Only 8 predictions changed (1.9%) between exp_000 and exp_002
2. These 8 changes HURT LB by ~1% (0.7799→0.7703)
3. Within Title groups, Age still matters for survival:
   - Mr Child: 6.7% survival vs Mr Young: 17.6%
   - Miss Child: 66% vs Miss Middle: 84%
4. Age provides signal that Title cannot fully capture
5. Agreement between exp_000 and exp_002: 98.1%

## Critical Strategic Insight
**The target of 1.0 (100% accuracy) is IMPOSSIBLE.** 
- Typical best Kaggle Titanic scores are 80-82%
- Our best LB is 0.7799 (78%)
- We are within ~2% of the realistic ceiling

**CV is misleading us.** Higher CV does NOT mean higher LB:
- exp_000: CV 0.8339 → LB 0.7799 (BEST LB)
- exp_001: CV 0.8271 → LB 0.7727
- exp_002: CV 0.8361 → LB 0.7703 (WORST LB despite BEST CV)

## Recommended Approaches (Priority Order)

### 1. Return to exp_000 baseline and tune hyperparameters
**Rationale**: exp_000 has the best LB (0.7799). Small improvements here are more likely to help than new approaches.
- Try different RF hyperparameters: n_estimators (300, 500), max_depth (5, 7, 8)
- Try different random seeds and ensemble predictions

### 2. Try XGBoost or LightGBM on exp_000 features
**Rationale**: Research shows XGBoost often achieves top-4% on Titanic. Same features as exp_000.
- XGBoost with learning_rate=0.01-0.05, max_depth=4-6
- LightGBM with num_leaves=20-50

### 3. Add FarePerPerson feature
**Rationale**: Research suggests `FarePerPerson = Fare / FamilySize` normalizes fare distribution.
- This addresses Fare distribution shift without removing Fare

### 4. Try ensemble/blending of exp_000 and exp_002
**Rationale**: 98.1% agreement means they're very similar, but the 1.9% difference matters.
- Simple voting: if both agree, use that; if disagree, use exp_000 (better LB)
- This is low-risk since exp_000 dominates

### 5. Ticket prefix feature
**Rationale**: Research shows 'PC' prefix has 65% survival (vs 10% for 'A/5').
- Extract ticket prefix as categorical feature

## What NOT to Try
- **Removing Age entirely** - PROVEN to hurt LB (exp_002)
- **Complex stacking** - exp_001 showed stacking hurt both CV and LB
- **Chasing CV improvements** - CV is NOT correlated with LB for this problem
- **Distribution shift mitigation by feature removal** - Doesn't work

## Validation Notes
- **DO NOT trust CV** - Use LB as the primary signal
- CV-LB gap is ~5-7% and NOT consistent
- With only 2 submissions remaining, prioritize:
  1. Small improvements to exp_000 (best LB)
  2. Different model (XGBoost) on same features

## Submission Budget
**2 submissions remaining** - Be strategic:
- Submit only if there's a clear hypothesis to test
- Focus on approaches that build on exp_000 (best LB)

## Expected Outcomes
- Realistic best LB: 0.80-0.82 (top Kaggle scores)
- Current best: 0.7799
- Target: 1.0 (IMPOSSIBLE - this is a learning exercise)

The goal should be incremental improvement toward 0.80+, not 1.0.