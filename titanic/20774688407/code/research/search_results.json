{
  "query": "How to handle distribution shift between train and test data in machine learning competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the\u202fWILDS\u202fbenchmark\u202fand recent research on covariate\u2011shift adjustment, here\u2019s a concise workflow for tackling train\u2011test distribution shift in machine\u2011learning competitions\u202f([WILDS](https://proceedings.mlr.press/v139/koh21a),\u202f[Stanford AI Lab blog](https://ai.stanford.edu/blog/wilds),\u202f[Robust covariate\u2011shift adjustment (RCSA)](https://proceedings.mlr.press/v32/wen14.pdf)):\n\n1. **Diagnose the shift**  \n   - Determine whether the mismatch is *covariate shift* (\u202f\\(p_{\\text{train}}(x) \\neq p_{\\text{test}}(x)\\)\u202f), *label shift*, or *concept shift*.  \n   - Visualize feature distributions (e.g., histograms, t\u2011SNE) for train vs. test to confirm the type of drift.\n\n2. **Pick a realistic evaluation set**  \n   - Use a benchmark like **WILDS** that contains real\u2011world shifts (hospital, camera\u2011trap, satellite, etc.) to prototype methods and gauge out\u2011of\u2011distribution performance before the competition deadline\u202f([WILDS paper](https://proceedings.mlr.press/v139/koh21a)).\n\n3. **Re\u2011weight training examples**  \n   - Apply importance\u2011weighting or the **Robust Covariate\u2011Shift Adjustment (RCSA)** algorithm to compute weights \\(w_i = \\frac{p_{\\text{test}}(x_i)}{p_{\\text{train}}(x_i)}\\) that make the weighted training distribution resemble the test distribution\u202f([RCSA paper](https://proceedings.mlr.press/v32/wen14.pdf)).  \n   - Train the model with these weights (e.g., via a weighted loss function).\n\n4. **Incorporate domain\u2011adaptation techniques**  \n   - **Adversarial feature alignment**: train a discriminator to make feature representations indistinguishable across domains while preserving task performance.  \n   - **Self\u2011training / pseudo\u2011labeling**: generate labels for unlabeled test\u2011like data and fine\u2011tune the model iteratively.  \n   - **Ensemble or test\u2011time adaptation**: at inference, adjust batch\u2011norm statistics or apply a small adaptation step on a held\u2011out \u201cshifted\u201d validation set.\n\n5. **Validate with a protected holdout**  \n   - Split a small portion of the test\u2011like data (or use the WILDS validation split) that is *never* queried during model development.  \n   - Track performance on this holdout to avoid overfitting to the public leaderboard, similar to the **Ladder** approach for reliable leaderboards\u202f([Hardt & Blum, 2015](https://proceedings.mlr.press/v37/blum15.pdf)).\n\n6. **Iterate and monitor**  \n   - After each major change (re\u2011weighting, adaptation, architecture), re\u2011evaluate on the holdout.  \n   - Keep a log of hyper\u2011parameters and their impact on out\u2011of\u2011distribution metrics to guide final submissions.\n\nFollowing these steps\u2014diagnosing the shift, using a realistic benchmark, applying robust re\u2011weighting (RCSA), leveraging domain\u2011adaptation tricks, and protecting against leaderboard over\u2011fitting\u2014will help you build models that remain accurate when the test distribution differs from the training data.",
      "url": ""
    },
    {
      "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
      "text": "WILDS: A Benchmark of in-the-Wild Distribution Shifts\n[![[International Conference on Machine Learning Logo]](https://proceedings.mlr.press/v139/assets/images/logo-pmlr.svg)](https://proceedings.mlr.press/)Proceedings of Machine Learning Research\n[[edit](https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-koh21a.md)]\n# WILDS: A Benchmark of in-the-Wild Distribution Shifts\nPang Wei Koh,Shiori Sagawa,Henrik Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard Lanas Phillips,Irena Gao,Tony Lee,Etienne David,Ian Stavness,Wei Guo,Berton Earnshaw,Imran Haque,Sara M Beery,Jure Leskovec,Anshul Kundaje,Emma Pierson,Sergey Levine,Chelsea Finn,Percy Liang\n*Proceedings of the 38th International Conference on Machine Learning*,PMLR 139:5637-5664,2021.\n#### Abstract\nDistribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.\n#### Cite this Paper\nBibTeX\n`@InProceedings{pmlr-v139-koh21a,\ntitle = {WILDS: A Benchmark of in-the-Wild Distribution Shifts},\nauthor = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},\nbooktitle = {Proceedings of the 38th International Conference on Machine Learning},\npages = {5637--5664},\nyear = {2021},\neditor = {Meila, Marina and Zhang, Tong},\nvolume = {139},\nseries = {Proceedings of Machine Learning Research},\nmonth = {18--24 Jul},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v139/koh21a/koh21a.pdf},\nurl = {https://proceedings.mlr.press/v139/koh21a.html},\nabstract = {Distribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.}\n}`\nCopy to ClipboardDownload\nEndnote\n`%0 Conference Paper\n%T WILDS: A Benchmark of in-the-Wild Distribution Shifts\n%A Pang Wei Koh\n%A Shiori Sagawa\n%A Henrik Marklund\n%A Sang Michael Xie\n%A Marvin Zhang\n%A Akshay Balsubramani\n%A Weihua Hu\n%A Michihiro Yasunaga\n%A Richard Lanas Phillips\n%A Irena Gao\n%A Tony Lee\n%A Etienne David\n%A Ian Stavness\n%A Wei Guo\n%A Berton Earnshaw\n%A Imran Haque\n%A Sara M Beery\n%A Jure Leskovec\n%A Anshul Kundaje\n%A Emma Pierson\n%A Sergey Levine\n%A Chelsea Finn\n%A Percy Liang\n%B Proceedings of the 38th International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2021\n%E Marina Meila\n%E Tong Zhang\t%F pmlr-v139-koh21a\n%I PMLR\n%P 5637--5664\n%U https://proceedings.mlr.press/v139/koh21a.html\n%V 139\n%X Distribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.`\nCopy to ClipboardDownload\nAPA\n`Koh, P.W., Sagawa, S., Marklund, H., Xie, S.M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R.L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B., Haque, I., Beery, S.M., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C. & Liang, P.. (2021). WILDS: A Benchmark of in-the-Wild Distribution Shifts.*Proceedings of the 38th International Conference on Machine Learning*, in*Proceedings of Machine Learning Research*139:5637-5664 Available from https://proceedings.mlr.press/v139/koh21a.html.`\nCopy to ClipboardDownload\n#### Related Material\n* [Download PDF](http://proceedings.mlr.press/v139/koh21a/koh21a.pdf)\n* [Supplementary PDF](http://proceedings.mlr.press/v139/koh21a/koh21a-supp.pdf)",
      "url": "https://proceedings.mlr.press/v139/koh21a"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "",
      "text": "Robust Learning under Uncertain Test Distributions:\nRelating Covariate Shift to Model Misspecification\nJunfeng Wen1JUNFENG.WEN@UALBERTA.CA\nChun-Nam Yu2 CHUN-NAM.YU@ALCATEL-LUCENT.COM\nRussell Greiner1 RGREINER@UALBERTA.CA\n1Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA\n2Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA\nAbstract\nMany learning situations involve learning the\nconditional distribution ppy|xq when the training\ninstances are drawn from the training distribu\u0002tion ptrpxq, even though it will later be used to\npredict for instances drawn from a different test\ndistribution ptepxq. Most current approaches fo\u0002cus on learning how to reweigh the training ex\u0002amples, to make them resemble the test distribu\u0002tion. However, reweighing does not always help,\nbecause (we show that) the test error also de\u0002pends on the correctness of the underlying model\nclass. This paper analyses this situation by view\u0002ing the problem of learning under changing dis\u0002tributions as a game between a learner and an ad\u0002versary. We characterize when such reweighing\nis needed, and also provide an algorithm, robust\ncovariate shift adjustment (RCSA), that provides\nrelevant weights. Our empirical studies, on UCI\ndatasets and a real-world cancer prognostic pre\u0002diction dataset, show that our analysis applies,\nand that our RCSA works effectively.\n1. Introduction\nTraditional machine learning often explicitly or implicitly\nassumes that the data used for training a model come from\nthe same distribution as that of the test data. However, this\nassumption is violated in many real-world applications. For\nexample, biostatisticians often try to collect a large and di\u0002verse training set, perhaps for building prognostic predic\u0002tors for patients with different diseases. When clinicians\ndeploy these predictors, they do not know whether the lo\u0002cal test patient population will be even close to that training\npopulation. Sometimes we can collect a small sample from\nthe target test population, but in most cases we have noth\u0002Proceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy\u0002right 2014 by the author(s).\ning more than weak prior knowledge about how the test\ndistribution may shift, such as anticipated changes in gen\u0002der ratio or age distribution. It is useful to build predictors\nthat are robust against such changes in test distributions.\nIn this work, we investigate the problem of distribu\u0002tion change under covariate shift assumption (Shimodaira,\n2000), in which both training and test distributions share\nthe same conditional distribution ppy|xq, while their\nmarginal distributions, ptrpxq and ptepxq, are different. To\ncorrect the shifted distribution, major efforts have been\ndedicated to importance reweighing (Quionero-Candela\net al., 2009; Sugiyama & Kawanabe, 2012). However,\nreweighing methods will not necessarily improve the per\u0002formance in test set, as prediction accuracy under covariate\nshift is also dependent on model misspecification (White,\n1981). Fig. 1 shows three examples of misspecified mod\u0002els, where we are considering the model class of straight\nlines of the form y\u201cax`b, for xP r\u00b41.5, 2.5s. In Fig. 1(a),\nno straight line is a good fit for the cubic curve across\nthe whole interval, but Model 2 fits the curve reasonably\nwell in the small interval r\u00b40.5, 0.5s. If training data is\nspread all over r\u00b41.5, 2.5s while test data concentrates on\nr\u00b40.5, 0.5s, improvement via reweighing could be signif\u0002icant. The situation in Fig. 1(b) is different: although the\ntrue model is a curve and not a straight line, the best linear\nfit is no more than \u000f away from the value of the true model.\nIn this case, no matter what test distributions we see in the\ninterval r\u00b41.5, 2.5s, the regression loss of the best linear\nmodel will never be more than \u000f from the Bayes optimal\nloss. In Fig. 1(c), the true model is a straight line except at\nx \u201c 0; perhaps this outlier is a cancer patient whose tumour\nspontaneously disappeared on its own. Unless the test dis\u0002tribution concentrates most of its mass at x \u201c 0, the straight\nline fit learned from the training data over the interval will\nstill be a very good predictor. Sometimes we can rule out\nthis type of covariate shift through prior knowledge. If such\noutliers are extremely rare during training time, we would\nnot expect the test population to have many such patients.\nReweighing will not help much in cases 1(b) and 1(c).\nRobust Learning under Uncertain Test Distributions\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n0\n2\n4\n6\n8\n10\n12\n14\n16\nInput\nOutput\nTrue model\nModel 1\nModel 2\n(a) Large misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\nInput\nOutput\n\u2191\n\u2193\n\u03b5\nTrue model\nBest linear fit\n(b) Small misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nInput\nOutput\nTrue model\n(c) Single point misspecification.\nFigure 1. Three different scenarios of model misspecifications.\nIn this paper, we relate covariate shift to model misspecifi\u0002cation and investigate when reweighing can help a learner\ndeal with covariate shift. We introduce a game between a\nlearner and an adversary that performs robust learning. The\nlearner chooses a model \u03b8 from a set \u0398 to minimize the\nloss, while the adversary chooses a reweighing function \u03b1\nfrom a set A to create new test distributions to maximize the\nloss. There are two major contributions in this paper: First,\nwe provide an improved understanding of the relation be\u0002tween covariate shift and model misspecification through\nthis game analysis. If the learner can find a \u03b8 that min\u0002imizes the loss against any possible \u03b1 that the adversary\ncan play, then it is not necessary to perform reweighing\nagainst covariate shift scenarios represented by A. Sec\u0002ond, we provide a systematic method for checking a model\nclass \u0398 against different covariate shift scenarios, such as\nchanging gender ratio and age distributions in the prognos\u0002tic predictor example, to help user decide whether impor\u0002tance reweighing would be beneficial.\nFor practical use, our method can be used to decide if the\nmodel class is sufficient against shifts that are close to a test\nsample; or robust against a known range of potential shifts\nif test sample is unavailable. If the model class is insuffi\u0002cient, we can consider different ways to deal with covariate\nshifts, such as reweighing using unlabelled test samples, or\nexploring a different model class for the problem.\n2. Related Work\nOur work is inspired by Grunwald & Dawid \u00a8 (2004), who\ninterpret maximum entropy as a game between an adver\u0002sary and a learner on minimizing the worst case expected\nlog loss. Teo et al. (2008) and Globerson & Roweis (2006)\nalso consider an adversarial scenario under changing test\nset conditions, but they are concerned with corruption or\ndeletion of features rather than covariate shift.\nMany results on covariate shift correction involve density\nratio estimation. Shimodaira (2000) showed that, given co\u0002variate shift and model misspecification, reweighing each\ninstance with ptepxq{ptrpxq is asymptotically optimal for\nlog-likelihood estimation, where ptrpxq and ptepxq are as\u0002sumed to be known or estimated in advance. Sugiyama\n& Muller \u00a8 (2005) extended this work by proposing an\n(almost) unbiased estimator for L2 generalization error.\nThere are several works focusing on minimizing differ\u0002ent types of divergence between distributions in the liter\u0002ature (Kanamori et al., 2008; Sugiyama et al., 2008; Ya\u0002mada et al., 2011). Kernel mean matching (KMM) (Huang\net al., 2007) reweighs instances to match means in a\nRKHS (Scholkopf & Smola \u00a8 , 2002). Our work and some\nother approaches (Pan et al., 2009) adapt the idea of match\u0002ing means of the datasets to correct shifted distribution,\nbut we extend their approaches from a two-step optimiza\u0002tion to a game framework that jointly learns a model\nand weights with covariate shift correction. Some other\napproaches (Zadrozny, 2004; Bickel et al.,...",
      "url": "https://proceedings.mlr.press/v32/wen14.pdf"
    },
    {
      "title": "The Stanford AI Lab Blog",
      "text": "WILDS: A Benchmark of in-the-Wild Distribution Shifts | SAIL BlogShiori Sagawa</a>and[Pang Wei Koh](https://cs.stanford.edu/~pangwei/)\" /\\>WILDS: A Benchmark of in-the-Wild Distribution Shifts | The Stanford AI Lab Blog\n# WILDS: A Benchmark of in-the-Wild Distribution Shifts\n[Shiori Sagawa](https://cs.stanford.edu/~ssagawa/)and[Pang Wei Koh](https://cs.stanford.edu/~pangwei/)\nJuly 19, 2021\n![](https://ai.stanford.edu/blog/assets/img/posts/2021-07-19-wilds/image3.png)\nOne of the most common assumptions in machine learning (ML) is that the training and test data are independently and identically distributed (i.i.d.). For example, we might collect some number of data points and then randomly split them, assigning half to the training set and half to the test set.\nHowever, this assumption is often broken in ML systems deployed in the wild. In real-world applications, distribution shifts\u2014 instances where a model is trained on data from one distribution but then deployed on data from a different distribution\u2014 are ubiquitous. For example, in medical applications, we might train a diagnosis model on patients from a few hospitals, and then deploy it more broadly to hospitals outside the training set[1](#fn:zech2018); and in wildlife monitoring, we might train an animal recognition model on images from one set of camera traps and then deploy it to new camera traps[2](#fn:beery2018).\nA large body of prior work has shown that these distribution shifts can significantly degrade model performance in a variety of real-world ML applications: models can perform poorly out-of-distribution, despite achieving high in-distribution performance[3](#fn:quinonero2009). To be able to reliably deploy ML models in the wild, we urgently need to develop methods for training models that are robust to real-world distribution shifts.\n## The WILDS benchmark\n![](https://ai.stanford.edu/blog/assets/img/posts/2021-07-19-wilds/image2.png)\nTo facilitate the development of ML models that are robust to real-world distribution shifts, our[ICML 2021 paper](https://www.google.com/url?q=https://arxiv.org/abs/2012.07421&amp;sa=D&amp;source=editors&amp;ust=1626558949287000&amp;usg=AOvVaw3L_bSKdMpjGZlKj9K4839a)presents WILDS, a curated benchmark of 10 datasets that reflect natural distribution shifts arising from different cameras, hospitals, molecular scaffolds, experiments, demographics, countries, time periods, users, and codebases.\n![](https://ai.stanford.edu/blog/assets/img/posts/2021-07-19-wilds/image4.png)\nThe WILDS datasets cover two common types of distribution shifts: domain generalization and subpopulation shift.\nIn domain generalization, the training and test distributions comprise data from related but distinct domains. The figure shows an example from the OGB-MolPCBA dataset[4](#fn:hu2020)in WILDS, where the task is to predict the biochemical properties of molecules, and the goal is to generalize to molecules with different molecular scaffolds that have not been seen in the training set.\nIn subpopulation shift, we consider test distributions that are subpopulations of the training distribution, and seek to perform well even on the worst-case subpopulation. As an example, consider the CivilComments-WILDS dataset[5](#fn:borkan2019), where the task is toxicity classification on online text comments. Standard models perform well on average but poorly on comments that mention certain minority demographic groups (e.g., they might be likely to erroneously flag innocuous comments mentioning Black people as toxic), and we seek to train models that can perform equally well on comments that correspond to different demographic subpopulations.\nFinally, some datasets exhibit both types of distribution shifts. For example, the second example in the figure above is from the FMoW-WILDS dataset[6](#fn:christie2018), where there is both a domain generalization problem over time (the training set consists of satellite images taken before 2013, while the test images were taken after 2016) as well as a subpopulation shift problem over different geographical regions (we seek to do well over all regions).\n## Selection criteria for WILDS datasets\nWILDS builds on extensive data collection efforts by domain experts working on applying ML methods in their application areas, and who are often forced to grapple with distribution shifts to make progress in their applications. To design WILDS, we worked with these experts to identify, select, and adapt datasets that fulfilled the following criteria:\n1. **Real-world relevance.**The training/test splits and evaluation metrics are motivated by real-world scenarios and chosen in conjunction with domain experts. By focusing on realistic distribution shifts, WILDS complements existing distribution shift benchmarks, which have largely studied shifts that are cleanly characterized but are not likely to arise in real-world deployments. For example, many recent papers have studied datasets with shifts induced by synthetic transformations, such as changing the color of MNIST digits[7](#fn:kim2019). Though these are important testbeds for systematic studies, model robustness need not transfer across shifts\u2014e.g., a method that improves robustness on a standard vision dataset can consistently harm robustness on real-world satellite imagery datasets[8](#fn:xie2021). So, in order to evaluate and develop methods for real-world distribution shifts, benchmarks like WILDS that capture shifts in the wild serve as an important complement to more synthetic benchmarks.\n2. **Distribution shifts with large performance gaps.**The train/test splits reflect shifts that substantially degrade model performance, i.e., with a large gap between in-distribution and out-of-distribution performance. Measuring the in-distribution versus out-of-distribution gap is an important but subtle problem, as it relies on carefully constructing an appropriate in-distribution setting. We discuss its complexities and our approach in more detail in the paper.\n![](https://ai.stanford.edu/blog/assets/img/posts/2021-07-19-wilds/image1.png)\nApart from the 10 datasets in WILDS, we also survey distribution shifts that occur in other application areas\u2014algorithmic fairness and policing, medicine and healthcare, genomics, natural language and speech processing, education, and robotics\u2014and discuss examples of datasets from these areas that we considered but did not include in WILDS. We investigated datasets in autonomous driving, fairness in policing, and computational biology, but either did not observe substantial performance drops or found that performance disparities arose from factors beyond distribution shifts.\n## Using WILDS\nTo make it easy to work with WILDS and to enable systematic comparisons between approaches, we developed an open-source Python package that fully automates data loading and evaluation. This package also contains default models and hyperparameters that can easily reproduce all of the baseline numbers we have in our paper. The package is simple to install\u2014just run`pip install wilds`\u2014and straightforward to use with any PyTorch-based algorithms and models:\n![](https://ai.stanford.edu/blog/assets/img/posts/2021-07-19-wilds/image5.png)\nWe are also hosting a public leaderboard at[https://wilds.stanford.edu/leaderboard/](https://www.google.com/url?q=https://wilds.stanford.edu/leaderboard/&amp;sa=D&amp;source=editors&amp;ust=1626558949290000&amp;usg=AOvVaw0VSnjWmDCRZKw2mBo0U0_v)to track the state of the art in algorithms for learning robust models. In our paper, we benchmarked several existing algorithms for learning robust models, but found that they did not consistently improve upon standard models trained with empirical risk minimization (i.e., minimizing the average loss). We thus believe that there is substantial room for developing algorithms and model architectures that can close the gaps between in-distribution and out-of-distribution performance on the WILDS datasets.\nJust in the past few months,...",
      "url": "https://ai.stanford.edu/blog/wilds"
    },
    {
      "title": "",
      "text": "WILDS: A Benchmark of in-the-Wild Distribution Shifts\nPang Wei Koh * 1 Shiori Sagawa * 1 Henrik Marklund 1 Sang Michael Xie 1 Marvin Zhang 2\nAkshay Balsubramani 1 Weihua Hu 1 Michihiro Yasunaga 1 Richard Lanas Phillips 3Irena Gao 1 Tony Lee 1\nEtienne David 4Ian Stavness 5 Wei Guo 6 Berton A. Earnshaw 7Imran S. Haque 7 Sara Beery 8\nJure Leskovec 1 Anshul Kundaje 1 Emma Pierson 3 9 Sergey Levine 2 Chelsea Finn 1 Percy Liang 1\nAbstract\nDistribution shifts\u2014where the training distribu\u0002tion differs from the test distribution\u2014can sub\u0002stantially degrade the accuracy of machine learn\u0002ing (ML) systems deployed in the wild. De\u0002spite their ubiquity in the real-world deployments,\nthese distribution shifts are under-represented in\nthe datasets widely used in the ML community\ntoday. To address this gap, we present WILDS,\na curated benchmark of 10 datasets reflecting\na diverse range of distribution shifts that natu\u0002rally arise in real-world applications, such as\nshifts across hospitals for tumor identification;\nacross camera traps for wildlife monitoring; and\nacross time and location in satellite imaging and\npoverty mapping. On each dataset, we show that\nstandard training yields substantially lower out\u0002of-distribution than in-distribution performance.\nThis gap remains even with models trained by\nexisting methods for tackling distribution shifts,\nunderscoring the need for new methods for train\u0002ing models that are more robust to the types of\ndistribution shifts that arise in practice. To facil\u0002itate method development, we provide an open\u0002source package that automates dataset loading,\ncontains default model architectures and hyperpa\u0002rameters, and standardizes evaluations. The full\npaper, code, and leaderboards are available at\nhttps://wilds.stanford.edu.\n1. Introduction\nDistribution shifts\u2014where the training distribution differs\nfrom the test distribution\u2014pose significant challenges for\n*Equal contribution 1\nStanford 2UC Berkeley 3Cornell 4INRAE\n5USask 6UTokyo 7Recursion 8Caltech 9Microsoft Research. Cor\u0002respondence to: Shiori Sagawa <ssagawa@cs.stanford.edu>,\nPang Wei Koh <pangwei@cs.stanford.edu>, Percy Liang <pli\u0002ang@cs.stanford.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\ny = mall\nx = \nd = Americas\ndrawn from\ny = residential\nx = \nd = Africa\ndrawn from\n\u2026\nTrain (mixture of domains) Test (Americas) Test (Africa)\ndrawn from\ny = rec facility\nx = \nd = Americas\ndrawn from\ny = school\nx = \nd = Africa\naccuracy = 55.3% accuracy = 32.8%\nworst-region accuracy = 32.8%\n\u2026\n\u2026\nTrain (mixture of domains) Test (unseen domains)\naverage precision = 27.2%\ny = active\nd = scaffold 1\nx =\ndrawn from\ny = inactive\nx = \nd = scaffold \n44,930 \ndrawn from\n\u2026\ny = active\nx = \nd = scaffold\n44,931\ndrawn from\ny = inactive\nx = \nd = scaffold\n90,124\ndrawn from\nDomain generalization\nSubpopulation shift Figure 1: In each WILDS dataset, each data point (x, y, d) is asso\u0002ciated with a domain d. Each domain corresponds to a distribution\nPd over data points which are similar in some way, e.g., molecules\nwith the same scaffold, or satellite images from the same region.\nWe study two types of distribution shifts. Top: In domain general\u0002ization, we train and test on disjoint sets of domains. The goal is to\ngeneralize to domains unseen during training, e.g., molecules with\na new scaffold in OGB-MOLPCBA (Hu et al., 2020b). Bottom:\nIn subpopulation shift, the training and test domains overlap, but\ntheir relative proportions differ. We typically assess models by\ntheir worst performance over test domains, each of which corre\u0002spond to a subpopulation of interest, e.g., different geographical\nregions in FMOW-WILDS (Christie et al., 2018).\nmachine learning (ML) systems deployed in the wild. In this\nwork, we consider two common types of distribution shifts:\ndomain generalization and subpopulation shift (Figure 1).\nBoth of these shifts arise naturally in many real-world sce\u0002narios, and prior work has shown that they can substantially\ndegrade model performance. In domain generalization, the\ntraining and test distributions comprise data from related\nbut distinct domains, such as patients from different hospi\u0002tals (Zech et al., 2018), images taken by different cameras\n(Beery et al., 2018), bioassays from different cell types (Li\net al., 2019a), or satellite images from different countries\nand time periods (Jean et al., 2016). In subpopulation shift,\nDomain generalization Subpopulation \nshift Domain generalization + subpopulation shift\nTrain example\nWhat do Black \nand LGBT \npeople have to \ndo with bicycle \nlicensing? \nimport \nnumpy as np\n\u2026\nnorm=np.___\nOverall a solid \npackage that \nhas a good \nquality of \nconstruction \nfor the price.\nTrain Val (OOD) Test (OOD)\nExperiment 1 Experiment 2 Experiment 3 Experiment 4\nsiRNA A\nsiRNA B\nTest example\nAs a Christian, \nI will not be \npatronizing \nany of those \nbusinesses.\nimport \nsubprocess \nas sp\np=sp.Popen()\nstdout=p.___\nI *loved* my \nFrench press, \nit\u2019s so perfect \nand came with \nall this fun \nstuff!\nTrain Val (OOD) Test (OOD)\nExperiment 1 Experiment 2 Experiment 3 Experiment 4\nsiRNA A\nsiRNA B\nDomain (d) camera hospital batch scaffold location, time demographic time, region country, rural-urban user git repository\nAdapted from Bandi et al.\n2018\nBeery et al.\n2020\nYeh et al.\n2020\nBorkan et al.\n2019\nHu et al.\n2020\nChristie et al.\n2018\nRaychev et al.\n2016\nNi et al.\n2019\nDavid et al.\n2021\nTaylor et al.\n2019\nDataset iWildCam Camelyon17 RxRx1 OGB-MolPCBA GlobalWheat CivilComments FMoW PovertyMap Amazon Py150\nPrediction (y) animal species tumor perturbed gene bioassays wheat head bbox toxicity land use asset wealth sentiment autocomplete\nInput (x) camera trap photo tissue slide cell image molecular graph wheat image online comment satellite image satellite image product review code\n# examples 203,029 455,954 125,510 437,929 6,515 448,000 523,846 19,669 539,502 150,000\n# domains 323 5 51 120,084 47 16 16 x 5 23 x 2 2,586 8,421\nFigure 2: The WILDS benchmark contains 10 datasets across a diverse set of application areas, data modalities, and dataset sizes. Each\ndataset comprises data from different domains, and the benchmark is set up to evaluate models on distribution shifts across these domains.\nwe consider test distributions that are subpopulations of the\ntraining distribution, with the goal of doing well even on the\nworst-case subpopulation; e.g., we might seek models that\nperform well on all demographic subpopulations, including\nminority individuals (Buolamwini & Gebru, 2018).\nDespite their ubiquity in real-world deployments, these\ntypes of distribution shifts are under-represented in the\ndatasets widely used in the ML community today (Geirhos\net al., 2020). Most of these datasets were designed for\nthe standard i.i.d. setting, with training and test sets from\nthe same distribution, and prior work on retrofitting them\nwith distribution shifts has focused on shifts that are cleanly\ncharacterized but not always likely to arise in real-world\ndeployments. For instance, many recent papers have studied\ndatasets with shifts induced by synthetic transformations,\nsuch as changing the color of MNIST digits (Arjovsky et al.,\n2019), or by disparate data splits, such as generalizing from\ncartoons to photos (Li et al., 2017a). Datasets like these are\nimportant testbeds for systematic studies; but to develop and\nevaluate methods for real-world shifts, we need to comple\u0002ment them with datasets that capture shifts in the wild.\nIn this paper, we present WILDS, a curated benchmark of 10\ndatasets with evaluation metrics and train/test splits repre\u0002senting a broad array of distribution shifts that ML models\nface in the wild (Figure 2). WILDS datasets span many im\u0002portant applications: animal species categorization (Beery\net al., 2020a), tumor identification (Bandi et al., 2018), bioas\u0002say prediction (Wu et al., 2018; Hu et al., 2020b), genetic\nperturbation classification (Taylor et al., 2019), wheat head\ndetection (David et al., 2020), ...",
      "url": "https://proceedings.mlr.press/v139/koh21a/koh21a.pdf"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2107.00643] Mandoline: Model Evaluation under Distribution Shift\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2107.00643\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2107.00643**(cs)\n[Submitted on 1 Jul 2021 ([v1](https://arxiv.org/abs/2107.00643v1)), last revised 11 Apr 2022 (this version, v2)]\n# Title:Mandoline: Model Evaluation under Distribution Shift\nAuthors:[Mayee Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M),[Karan Goel](https://arxiv.org/search/cs?searchtype=author&amp;query=Goel,+K),[Nimit S. Sohoni](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohoni,+N+S),[Fait Poms](https://arxiv.org/search/cs?searchtype=author&amp;query=Poms,+F),[Kayvon Fatahalian](https://arxiv.org/search/cs?searchtype=author&amp;query=Fatahalian,+K),[Christopher R\u00e9](https://arxiv.org/search/cs?searchtype=author&amp;query=R\u00e9,+C)\nView a PDF of the paper titled Mandoline: Model Evaluation under Distribution Shift, by Mayee Chen and 5 other authors\n[View PDF](https://arxiv.org/pdf/2107.00643)> > Abstract:\n> Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple &#34;slicing functions&#34; - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines. Comments:|33 pages. Published as a conference paper at ICML 2021|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2107.00643](https://arxiv.org/abs/2107.00643)[cs.LG]|\n|(or[arXiv:2107.00643v2](https://arxiv.org/abs/2107.00643v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2107.00643](https://doi.org/10.48550/arXiv.2107.00643)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Nimit Sohoni [[view email](https://arxiv.org/show-email/c336d188/2107.00643)]\n**[[v1]](https://arxiv.org/abs/2107.00643v1)**Thu, 1 Jul 2021 17:57:57 UTC (650 KB)\n**[v2]**Mon, 11 Apr 2022 00:14:55 UTC (489 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Mandoline: Model Evaluation under Distribution Shift, by Mayee Chen and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2107.00643)\n* [TeX Source](https://arxiv.org/src/2107.00643)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2107.00643&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2107.00643&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-07](https://arxiv.org/list/cs.LG/2021-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2107.00643?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.00643)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.00643)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.00643)\n### [1 blog link](https://arxiv.org/tb/2107.00643)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-00643)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-00643)\n[Karan Goel](<https://dblp.uni-trier.de/search/author?author=Karan Goel>)\n[Nimit Sharad Sohoni](<https://dblp.uni-trier.de/search/author?author=Nimit Sharad Sohoni>)\n[Kayvon Fatahalian](<https://dblp.uni-trier.de/search/author?author=Kayvon Fatahalian>)\n[Christopher R\u00e9](<https://dblp.uni-trier.de/search/author?author=Christopher R\u00e9>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2107.00643&amp;description=Mandoline: Model Evaluation under Distribution Shift>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2107.00643&amp;title=Mandoline: Model Evaluation under Distribution Shift>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencema...",
      "url": "https://arxiv.org/abs/2107.00643"
    },
    {
      "title": "",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/pdf/2507.21160"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2206.15407] Shifts 2.0: Extending The Dataset of Real Distributional Shifts\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2206.15407\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2206.15407**(cs)\n[Submitted on 30 Jun 2022 ([v1](https://arxiv.org/abs/2206.15407v1)), last revised 15 Sep 2022 (this version, v2)]\n# Title:Shifts 2.0: Extending The Dataset of Real Distributional Shifts\nAuthors:[Andrey Malinin](https://arxiv.org/search/cs?searchtype=author&amp;query=Malinin,+A),[Andreas Athanasopoulos](https://arxiv.org/search/cs?searchtype=author&amp;query=Athanasopoulos,+A),[Muhamed Barakovic](https://arxiv.org/search/cs?searchtype=author&amp;query=Barakovic,+M),[Meritxell Bach Cuadra](https://arxiv.org/search/cs?searchtype=author&amp;query=Cuadra,+M+B),[Mark J. F. Gales](https://arxiv.org/search/cs?searchtype=author&amp;query=Gales,+M+J+F),[Cristina Granziera](https://arxiv.org/search/cs?searchtype=author&amp;query=Granziera,+C),[Mara Graziani](https://arxiv.org/search/cs?searchtype=author&amp;query=Graziani,+M),[Nikolay Kartashev](https://arxiv.org/search/cs?searchtype=author&amp;query=Kartashev,+N),[Konstantinos Kyriakopoulos](https://arxiv.org/search/cs?searchtype=author&amp;query=Kyriakopoulos,+K),[Po-Jui Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+P),[Nataliia Molchanova](https://arxiv.org/search/cs?searchtype=author&amp;query=Molchanova,+N),[Antonis Nikitakis](https://arxiv.org/search/cs?searchtype=author&amp;query=Nikitakis,+A),[Vatsal Raina](https://arxiv.org/search/cs?searchtype=author&amp;query=Raina,+V),[Francesco La Rosa](https://arxiv.org/search/cs?searchtype=author&amp;query=La+Rosa,+F),[Eli Sivena](https://arxiv.org/search/cs?searchtype=author&amp;query=Sivena,+E),[Vasileios Tsarsitalidis](https://arxiv.org/search/cs?searchtype=author&amp;query=Tsarsitalidis,+V),[Efi Tsompopoulou](https://arxiv.org/search/cs?searchtype=author&amp;query=Tsompopoulou,+E),[Elena Volf](https://arxiv.org/search/cs?searchtype=author&amp;query=Volf,+E)\nView a PDF of the paper titled Shifts 2.0: Extending The Dataset of Real Distributional Shifts, by Andrey Malinin and 17 other authors\n[View PDF](https://arxiv.org/pdf/2206.15407)> > Abstract:\n> Distributional shift, or the mismatch between training and deployment data, is a significant obstacle to the usage of machine learning in high-stakes industrial applications, such as autonomous driving and medicine. This creates a need to be able to assess how robustly ML models generalize as well as the quality of their uncertainty estimates. Standard ML baseline datasets do not allow these properties to be assessed, as the training, validation and test data are often identically distributed. Recently, a range of dedicated benchmarks have appeared, featuring both distributionally matched and shifted data. Among these benchmarks, the Shifts dataset stands out in terms of the diversity of tasks as well as the data modalities it features. While most of the benchmarks are heavily dominated by 2D image classification tasks, Shifts contains tabular weather forecasting, machine translation, and vehicle motion prediction tasks. This enables the robustness properties of models to be assessed on a diverse set of industrial-scale tasks and either universal or directly applicable task-specific conclusions to be reached. In this paper, we extend the Shifts Dataset with two datasets sourced from industrial, high-risk applications of high societal importance. Specifically, we consider the tasks of segmentation of white matter Multiple Sclerosis lesions in 3D magnetic resonance brain images and the estimation of power consumption in marine cargo vessels. Both tasks feature ubiquitous distributional shifts and a strict safety requirement due to the high cost of errors. These new datasets will allow researchers to further explore robust generalization and uncertainty estimation in new situations. In this work, we provide a description of the dataset and baseline results for both tasks. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nCite as:|[arXiv:2206.15407](https://arxiv.org/abs/2206.15407)[cs.LG]|\n|(or[arXiv:2206.15407v2](https://arxiv.org/abs/2206.15407v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2206.15407](https://doi.org/10.48550/arXiv.2206.15407)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Andrey Malinin Dr. [[view email](https://arxiv.org/show-email/4e4d1470/2206.15407)]\n**[[v1]](https://arxiv.org/abs/2206.15407v1)**Thu, 30 Jun 2022 16:51:52 UTC (4,126 KB)\n**[v2]**Thu, 15 Sep 2022 09:52:12 UTC (4,352 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Shifts 2.0: Extending The Dataset of Real Distributional Shifts, by Andrey Malinin and 17 other authors\n* [View PDF](https://arxiv.org/pdf/2206.15407)\n* [TeX Source](https://arxiv.org/src/2206.15407)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2206.15407&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2206.15407&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-06](https://arxiv.org/list/cs.LG/2022-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2206.15407?context=cs)\n[cs.AI](https://arxiv.org/abs/2206.15407?context=cs.AI)\n[stat](https://arxiv.org/abs/2206.15407?context=stat)\n[stat.ML](https://arxiv.org/abs/2206.15407?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2206.15407)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2206.15407)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2206.15407)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2206.15407&amp;description=Shifts 2.0: Extending The Dataset of Real Distributional Shifts>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2206.15407&amp;title=Shifts 2.0: Extending The Dataset of Real Distributional Shifts>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and...",
      "url": "https://arxiv.org/abs/2206.15407"
    }
  ]
}