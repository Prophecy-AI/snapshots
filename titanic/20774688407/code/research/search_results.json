{
  "query": "What are the best feature engineering techniques for Titanic survival prediction that achieve 80%+ accuracy on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most reliable way to push a Titanic model past the 80\u202f% public\u2011score threshold is to turn the raw passenger fields into a handful of high\u2011impact engineered features \u2013 things that capture family structure, social status, and missing\u2011value patterns \u2013 and then feed them to a strong learner such as XGBoost or a tuned Random\u202fForest.  In recent Kaggle notebooks, the combination of the following transformations consistently lifts validation accuracy from the low\u201170\u202f% range up to and beyond 80\u202f%\u202f([\u00d6zg\u00fcr\u202fSanl\u0131\u202f2024](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)):\n\n* **Title extraction** \u2013 parse the passenger\u2019s name to obtain a \u201cTitle\u201d (Mr, Mrs, Miss, Master, etc.) and group rare titles; this captures social class and gender nuances and is one of the top\u2011ranked features in the GitHub Random\u2011Forest pipeline that raised accuracy to ~78\u202f% ([Samantha\u202fZhang\u202f2025](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  \n* **FamilySize & IsAlone** \u2013 compute `FamilySize = SibSp + Parch + 1` and a binary `IsAlone` flag; larger families and solitary travelers have distinct survival patterns and are explicitly modelled in the same repo ([Samantha\u202fZhang\u202f2025](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  \n* **Cabin deck & Cabin\u2011presence indicator** \u2013 keep only the first letter of the Cabin string (A\u2011G) as a \u201cDeck\u201d feature and add a boolean \u201cHasCabin\u201d flag; the Alpha6849 end\u2011to\u2011end pipeline shows this improves the public score to 0.76555 ([Alpha6849\u202f2025](https://github.com/Alpha6849/-Titanic-Survival-Prediction)).  \n* **Ticket prefix** \u2013 split the Ticket field and use the alphabetic prefix (or \u201cTicketGroup\u201d) as a categorical feature; this captures crew vs. passenger ticketing schemes.  \n* **Fare per person & fare bins** \u2013 create `FarePerPerson = Fare / FamilySize` and bucket fares into quantiles; this normalises the wide fare distribution and adds predictive power.  \n* **Age imputation & Age bins** \u2013 fill missing ages with the median age grouped by `Pclass`\u202f+\u202f`Sex`, then bin ages (child, teen, adult, senior); age stratification is repeatedly highlighted as a key driver of performance in the XGBoost notebook that achieved a top\u20114\u202f% leaderboard position ([Tristan\u202fNguyen\u202f2020](https://scatta.cc/2020/08/15/prediction-titanic-dataset-kaggle.html)).  \n* **Embarked mode fill & one\u2011hot encoding** \u2013 replace missing `Embarked` values with the mode and one\u2011hot encode all categorical variables (Sex, Embarked, Title, Deck, TicketGroup) so the model can treat each level separately.  \n\nWhen these engineered columns are combined with robust preprocessing (scaling numeric fields, using `LabelEncoder` or `OneHotEncoder`) and a high\u2011capacity algorithm such as XGBoost (or a well\u2011tuned Random\u202fForest), public\u2011leaderboard scores regularly exceed 0.80\u202faccuracy, as demonstrated in the 2024 \u201c80\u202f% accuracy\u201d tutorial ([\u00d6zg\u00fcr\u202fSanl\u0131\u202f2024](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).  The consensus across the cited notebooks is that thoughtful extraction of titles, family metrics, cabin/deck information, and fare/age normalisation provides the biggest lift, while proper handling of missing values and categorical encoding lets the model fully exploit those signals.",
      "url": ""
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "GitHub - Alpha6849/-Titanic-Survival-Prediction: End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.",
      "text": "<div><div><article><p></p><h2>-Titanic-Survival-Prediction</h2><a href=\"#-titanic-survival-prediction\"></a><p></p>\n<p>End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.</p>\n<p></p><h2>\ud83d\udea2 Titanic Survival Prediction - Random Forest Based ML Pipeline</h2><a href=\"#-titanic-survival-prediction---random-forest-based-ml-pipeline\"></a><p></p>\n<p>This is a complete end-to-end machine learning project built on the famous <strong>Titanic: Machine Learning from Disaster</strong> dataset on Kaggle. The goal is to predict passenger survival using classification models, with a focus on <strong>data preprocessing</strong>, <strong>feature engineering</strong>, and <strong>model tuning</strong>.</p>\n<hr/>\n<p></p><h2>\ud83d\udccc Project Description</h2><a href=\"#-project-description\"></a><p></p>\n<p>The Titanic dataset is a widely used beginner dataset for classification problems in machine learning. In this project, we went through all key stages of the ML pipeline:</p>\n<ul>\n<li>Exploratory Data Analysis</li>\n<li>Data Cleaning &amp; Imputation</li>\n<li>Feature Engineering</li>\n<li>Model Selection and Hyperparameter Tuning</li>\n<li>Final Prediction and Kaggle Submission</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udd0d Phases Breakdown</h2><a href=\"#-phases-breakdown\"></a><p></p>\n<p></p><h3>\u2705 Phase 1: Data Cleaning</h3><a href=\"#-phase-1-data-cleaning\"></a><p></p>\n<ul>\n<li>Dropped non-informative columns: <code>Name</code>, <code>Ticket</code>, <code>Cabin</code></li>\n<li>Filled missing values in:\n<ul>\n<li><code>Age</code>: with median grouped by <code>Pclass</code> and <code>Sex</code></li>\n<li><code>Embarked</code>: with mode</li>\n<li><code>Fare</code> in test set: with median</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 2: Feature Engineering</h3><a href=\"#-phase-2-feature-engineering\"></a><p></p>\n<ul>\n<li>Created <code>FamilySize</code> as <code>SibSp + Parch + 1</code></li>\n<li>Extracted <code>Title</code> from the <code>Name</code> column before dropping it</li>\n<li>Mapped categorical columns (<code>Sex</code>, <code>Embarked</code>, <code>Title</code>) using <strong>Label Encoding</strong></li>\n</ul>\n<p></p><h3>\u2705 Phase 3: Model Training &amp; Tuning</h3><a href=\"#-phase-3-model-training--tuning\"></a><p></p>\n<ul>\n<li>Tried baseline models including Logistic Regression and XGBoost</li>\n<li>Observed XGBoost underperformed (~0.60 accuracy)</li>\n<li>Final model: <strong>Random Forest Classifier</strong>\n<ul>\n<li>Tuned using <code>GridSearchCV</code></li>\n<li>Best parameters selected based on cross-validation</li>\n<li>Trained on full training set and predicted on test set</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 4: Final Submission</h3><a href=\"#-phase-4-final-submission\"></a><p></p>\n<ul>\n<li>Predictions were exported as <code>submission_final.csv</code></li>\n<li>Submitted to Kaggle for evaluation</li>\n</ul>\n<p></p><h2>\ud83c\udfc6 Kaggle Performance</h2><a href=\"#-kaggle-performance\"></a><p></p>\n<ul>\n<li><strong>Final Kaggle Score</strong>: <code>0.76555</code></li>\n<li><strong>Kaggle Rank</strong>: ~11,000 (Top 30\u201335% range at time of submission)</li>\n<li><strong>Final Model</strong>: Random Forest Classifier with GridSearchCV tuning</li>\n</ul>\n<p></p><h2>\ud83d\udcc1 Project Structure</h2><a href=\"#-project-structure\"></a><p></p>\n<p>\u251c\u2500\u2500 data/\n\u2502 \u251c\u2500\u2500 train.csv\n\u2502 \u2514\u2500\u2500 test.csv\n\u251c\u2500\u2500 titanic_model.ipynb &lt;- All model development in Jupyter\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission_final.csv\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt</p>\n<p></p><h2>\ud83c\udf93 What I Learned</h2><a href=\"#-what-i-learned\"></a><p></p>\n<ul>\n<li>How to clean and preprocess real-world datasets with missing values</li>\n<li>Feature extraction using domain knowledge (like <code>Title</code>, <code>FamilySize</code>)</li>\n<li>Label encoding vs OneHotEncoding choices</li>\n<li>Why some models (e.g. XGBoost) may not always outperform others</li>\n<li>Hyperparameter tuning using <code>GridSearchCV</code></li>\n<li>Submitting predictions on Kaggle and interpreting scores</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcbc Future Improvements</h2><a href=\"#-future-improvements\"></a><p></p>\n<ul>\n<li>Try ensemble techniques (Voting, Stacking)</li>\n<li>Add feature importance plots for interpretability</li>\n<li>Experiment with other preprocessing pipelines</li>\n</ul>\n<hr/>\n<blockquote>\n<p>Built as part of machine learning practice. Feel free to fork, reuse, or collaborate!</p>\n</blockquote>\n</article></div></div>",
      "url": "https://github.com/Alpha6849/-Titanic-Survival-Prediction"
    },
    {
      "title": "Predicting Titanic survival with feature engineering and XGBoost",
      "text": "# Predicting Titanic survival with feature engineering and XGBoost\n\n![](https://scatta.cc/assets/images/titanic_ship.png)\n\nThe classic Titanic machine learning competition is an fantastic way to apply your machine learning skills and compare your work with others.\n\nEven though the full dataset is available and you can cheat your way to perfect score, it is very satisfying to compete fairly and achieve good result.\n\nIn the notebook, I focus on the preprocessing aspect of machine learning, i.e., the cleaning and feature engineering of the dataset, which resulted in a top 4% position in the leaderboard with XGBoost.\n\nThis dataset is an excellent example to illustrate the power of understanding your dataset and making it more useful before diving into specific ML algorithms.\n\nThe notebook is divided into five parts:\n\n**Chapter 1: Missing values**\n\n**Chapter 2: Feature engineering**\n\n**Chapter 3: Assessing the features**\n\n**Chapter 4: Prepare data for training**\n\n**Chapter 5: Build a model to predict Titanic survival**\n\nThe success of the prediction boils down to judicious selection of useful features in addtion to creating new ones. Below is the correlation matrix of existing and engineered features in this dataset.\n\n![](data:image/png;base64,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...",
      "url": "https://scatta.cc/2020/08/15/prediction-titanic-dataset-kaggle.html"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Kaggle Challenge - Titanic Survival Prediction Feature Engineering | Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd48875ccba21&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Vishal Gupta](https://medium.com/@vishal8092009?source=post_page---byline--d48875ccba21---------------------------------------)\n\n10 min read\n\n\u00b7\n\nJun 2, 2020\n\n--\n\nListen\n\nShare\n\nTITANIC SURVIVAL PREDICTION\n\nIn this tutorial, we will learn about one of the most popular datasets in data science. It will give you idea about how to analyze and relate with real conditions.\n\n## The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive compared to others.\n\nIn this challenge, we will need to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n_This post will help you start with data science and familiarize yourself with Machine Learning. The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck._\n\n## A. Variable Information here\n\n**PassengerId** is the unique id of the row (no effect on target)\n\n**Survived** is the target we are trying to predict 1 : Survived 0 : Not Survived\n\n**Pclass** (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has three unique values (1, 2 or 3)\n\n**Name**, **Sex** and **Age** are self-explanatory\n\n**SibSp** is the total number of the passengers\u2019 siblings and spouse\n\n**Parch** is the total number of the passengers\u2019 parents and children\n\n**Ticket** is the ticket number of the passenger\n\n**Fare** is the passenger fare\n\n**Cabin** is the cabin number of the passenger\n\n**Embarked** is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n\n```\nC = CherbourgQ = QueenstownS = Southampton\n```\n\n## Variable Notes\n\n**Pclass:** 1st = Upper 2nd = Middle 3rd = Lower\n\n**SibSp:** The dataset defines relationship as, Sibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**Parch:** The dataset defines relationship as, Parent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n## Below are the few questions we would like to answer after our analysis\n\n- Is there any relation between given info of passengers and their survival?\n- What is the survival rate for different age groups?\n- Was preference given to women and children for saving?\n- Did having higher social status help people improve survival chances?\n- What are the effects of being alone or with family?\n- Was the survival rate affected by passenger class\n- Can we predict if a passenger survived from the disaster with using machine-learning techniques?\n\n## Let\u2019s Get Started\n\n```\n# importing basic librariesimport warningswarnings.filterwarnings(\u2018ignore\u2019)import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport plotly as plyfrom scipy import statsimport math# reading the titanic train and test datatrain = pd.read_csv(\u2018https://bit.ly/kaggletrain')test = pd.read_csv(\u2018https://bit.ly/kaggletest')train.shape # (891,12)test.shape # (418,11)\n```\n\nWe can check the data head to see what kind of variables we have. Mainly we have two kinds of variables i.e. continuous and discrete.\n\nHere Survived is our target variable, so we save it in the variable y and save the _PassengerID_ for sake of result submission on _kaggle_.\n\n```\ny , testpassenger = train[\u2018Survived\u2019], test[\u2018PassengerID\u2019]\n```\n\nWe also drop the _passenger id_, as it is unique for each record and doesn\u2019t contribute anything for survival prediction. Also concatenate the train and test into one for the ease of performing EDA and missing value operations and feature engineering.\n\n```\ntrain = train.drop(\u2018PassengerId\u2019, axis = 1)test = test.drop(\u2018PassengerId\u2019, axis = 1)total = pd.concat([train,test],axis = 0, sort=False)total = total.reset_index(drop=True)# Saving variables into different listscat_var = [\u2018Pclass\u2019,\u2019Sex\u2019,\u2019SibSp\u2019,\u2019Parch\u2019,\u2019Embarked\u2019] # categoricaltarget = [\u2018Survived\u2019] # targetcon_var = [\u2018Age\u2019,\u2019Fare\u2019] # continuous\n```\n\n## B. Visualization\n\nTo plot categorical variables:\n\n```\ndef catplot(x, df):   fig, ax = plt.subplots(math.ceil(len(x)/3), 3, figsize = (20,10))   ax = ax.flatten()   for axes, cat in zip(ax, x):   if (cat == \u2018Survived\u2019):      sns.countplot(df[cat], ax = axes)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)   else:      sns.countplot(df[cat], hue=\u2019Survived\u2019, data = df, ax = axes)      axes.legend(title=\u2019Survived ?\u2019, labels = [\u2018No\u2019,\u2019Yes\u2019],                      loc= \u2018upper right\u2019)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)# call the plotcatplot(target + cat_var, total)\n```\n\nPress enter or click to view image in full size\n\nCategorical Variable Plots\n\nTo plot the continuous variables:\n\n```\ndef cont(x, df):   fig, ax = plt.subplots(1,3, figsize = (18,4))   sns.distplot( df[df[x].notna()][x] , fit = stats.norm,ax =ax[0],                     bins = 12)   # fit parameter in distplot fits a function of distribution   # passed in argumentstats.probplot( df[x].fillna(df[x].median()), plot=ax[1])   sns.boxplot(y = df[x], ax = ax[2])   plt.show()# call the plotcont(\u2018Age\u2019, total)\n```\n\n```\ncont(\u2018Fare\u2019, total)\n```\n\nDensity plots:\n\nFor Age and Fare variables we make density plots, to see if they have any impact on the survival\n\n```\nplot_con = [\u2018Age\u2019,\u2019Fare\u2019]for i in plot_con:   plt.figure(figsize=(12,4))   sns.distplot(train[train[\u2018Survived\u2019]==0][i], color=\u2019r\u2019, bins = 5)   sns.distplot(train[train[\u2018Survived\u2019]==1][i], color=\u2019g\u2019, bins = 5)   plt.legend(title=\u2019Survived vs \u2018+i, labels = [\u2018Dead\u2019,\u2019Alive\u2019],                    loc=\u2019upper right\u2019)   plt.show()\n```\n\n- People with lower age survived more\n- It can be concluded that higher fare leads to better survival chance, the binning of fare might improve decision boundary.\n- However that relationship might be result of other variables also.\n\n## C. Feature Engineering\n\nNow our next steps will be towards feature engineering\n\n**1\\. Feature Engineering \u2014 Name**:\n\nAfter a glimpse we can see that name itself cannot help us in any way, so we extract new variables from it i.e. Title and Last Name.\n\n```\ndef map_title(x):   if x in [\u2018Mr\u2019]:      return \u2018Mr\u2019   elif x in [\u2018Mrs\u2019, \u2018Mme\u2019]:      return \u2018Mrs\u2019   elif x in [\u2018Miss\u2019, \u2018Mlle\u2019, \u2018Ms\u2019]:      return \u2018Miss\u2019   # Master taken separate as all have children ageel...",
      "url": "https://medium.com/@vishal8092009/in-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    },
    {
      "title": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset",
      "text": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Towards AI\n](https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-40a0aa184c1c---------------------------------------)\n\u00b7Follow publication\n[\n![Towards AI](https://miro.medium.com/v2/resize:fill:76:76/1*JyIThO-cLjlChQLb6kSlVQ.png)\n](https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-40a0aa184c1c---------------------------------------)\nMaking AI accessible to 100K+ learners. Find the most practical, hands-on and comprehensive AI Engineering and AI for Work certifications at[academy.towardsai.net](http://academy.towardsai.net)- we have pathways for any experience level. Monthly cohorts still open\u200a\u2014\u200ause COHORT10 for 10% off!\nFollow publication\n# Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\n## This article will walk you through detailed forward feature selection steps and model building from scratch, improving it further with fine-tuning.\n[\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\n](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n5 min read\n\u00b7May 24, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-artificial-intelligence/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--40a0aa184c1c---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=---header_actions--40a0aa184c1c---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[NASA](https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\n[\n## Data pre-processing of spaceship-titanic kaggle dataset for achieving 80+% accuracy.\n### Data cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact\u2026\nmedium.com\n](https://medium.com/@chavdadevang23/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?source=post_page-----40a0aa184c1c---------------------------------------)\n**We will be building a model in 3 trenches:**\n1. Building a model with only numerical features.\n2. Building a model with only categorical features.\n3. Building a model with all features combined.\n```\nNUMS = [&#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,&#x27;Num&#x27;,\n&#x27;&#x27;Expenditure&#x27;&#x27;,&#x27;&#x27;Group\\_size&#x27;&#x27;,&#x27;&#x27;Expenditure&#x27;&#x27;]\nTARGET = [&#x27;Transported&#x27;]\n# only numerical feature dataframe\nnums\\_df = df[NUMS+TARGET]\ndf[NUMS].head(3)\n```\nLet\u2019s find out which features have the most importance in order to classify a data point, we will be using the forward feature selection method.\n> Forward feature selection: Step-by-step process of adding features to improve a model\u2019s performance, starting with none, to identify the most relevant ones.\n```\ndef evaluate\\_model\\_kfold\\_classification(X,y,k,clf):\n#kfold base\nX = X.fillna(0)\nkf = model\\_selection.KFold(n\\_splits=k,shuffle=True)\naccuracies = []\nfor fold, (train\\_index,validation\\_index) in enumerate(kf.split(X=X)):\ntrain\\_x = X.loc[train\\_index].values\ntrain\\_y = y.loc[train\\_index].values\nvalidation\\_x = X.loc[validation\\_index].values\nvalidation\\_y = y.loc[validation\\_index].values\nclf.fit(train\\_x,train\\_y)\npreds = clf.predict(validation\\_x)\naccuracy = metrics.accuracy\\_score(validation\\_y, preds)\nprint(f&quot;Fold={fold}, Accuracy={accuracy}&quot;)\naccuracies.append(accuracy)\nreturn sum(accuracies)/len(accuracies)\ndef feature\\_selection\\_classification(X,y,k,model):\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nbest\\_feature= None\nfor feature in list(X.columns):\nscore = evaluate\\_model\\_kfold\\_classification(X[[feature]],y,k,model)\nif score &gt;&gt; average\\_eval\\_metric:\nbest\\_feature = feature\naverage\\_eval\\_metric =score\nprint(&quot;&quot;best feature--&gt;&gt;&quot;&quot;,best\\_feature)\nfeatures = list(X.columns)\nfeatures.remove(best\\_feature)\nbest\\_feature\\_order = [best\\_feature]\nbest\\_feature\\_order.extend(features)\nprint(&quot;&quot;best feature order --&gt;&gt;&quot;&quot;,best\\_feature\\_order)\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nscores\\_progression = {}\nfor feature in best\\_feature\\_order:\ngood\\_features.append(feature)\nscore = evaluate\\_model\\_kfold\\_classification(X[good\\_features],y,k,model)\nscores\\_progression[&#x27;&#x27;|&#x27;&#x27;.join(good\\_features)] = score\nif score &lt;&lt; average\\_eval\\_metric:\ngood\\_features.remove(feature)\nelse:\naverage\\_eval\\_metric = score\nreturn good\\_features,scores\\_progression\n```\nThe code performs feature selection for classification. It iterates over features, evaluates their impact on the model, and selects the best ones based on evaluation metrics.\nHere I am using logistic regression as my base model to select the best features. Why logistic regression: when we did t-sne analysis on this dataset, we found out that data points are separated in a way where drawing a boundary will be easier,\n```\n# here any classification model can be chosen in order to get best features\nclf = LogisticRegression()\ngood\\_features , score\\_progression = feature\\_selection\\_classification(df[NUMS],df[TARGET],5,clf)\nscore\\_progression\n```\nPress enter or click to view image in full size\n![]()\nStep-by-step progress\nPress enter or click to view image in full size\n![]()\ngood features\nNow measure accuracy with all numeric features vs with only good features, which we have derived with feature selection.\n```\nprint(&quot;&quot;\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*with all features\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*...",
      "url": "https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c"
    },
    {
      "title": "",
      "text": "# Feature Engineering, Data Exploration, and Classification using Titanic Dataset [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#Feature-Engineering,-Data-Exploration,-and-Classification-using-Titanic-Dataset)\n\nIn machine learing, what perhaps is more important than defining a model or a neural network is data exploration. After we get the dataset, that is hopefully representative of the problem we are trying to solve, the first thing we do is to examine the data and see what characteristics and features are present. Are there any problems in the data? Are some categories under-represented? Are there missing values? Are there significant correlations between some features and outcome? Can we easily engineer new features from existing ones that will correlate better with the outcome? Data exploration not only helps to discover and eliminates some potential problems before model training begins, but will also makes our model more accurate by injecting some human intuition into the solution.\n\nIn this tutorial, we will use the Titanic passenger survival dataset to illustrate the concept of data exploration and feature engineering. We choose this dataset because it is simple, and thus allows us to focus on engineering the features. We will show how engineered features play an important role in prediction by being the most important feature in machine learning models. Then, we will illustrate compare several methods of binary classification that can used to predict whether a passenger survives the disaster.\n\nThe Titanic dataset can be [download from this website](https://storage.googleapis.com/kaggle-forum-message-attachments/66979/2180/titanic.csv). Prediction Titanic passenger survival is also a [Kaggle challege](https://www.kaggle.com/c/titanic). Note that because the passenger survival information is public, the Kaggle challenge leaderboard is spammed with people submitting actual real-world data as machine learning results, making it meaningless.\n\n## 1\\. Data Exploration [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#1.-Data-Exploration)\n\nWe first load the \"titanic.csv\" file into a Pandas dataframe. We will immediately print the first few lines of the file.\n\nIn\u00a0\\[8\\]:\n\n```\nimport pandas as pd\n\ndata = pd.read_csv('titanic.csv')\ndata.head(10)\n\n```\n\nOut\\[8\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked | boat | body | home.dest |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C | NaN | 22.0 | Montevideo, Uruguay |\n\nWe immediately notice that the second column \"survived\" is our prediction result. The \"name\" column contains not only the first and last name, but also the **title**, which could be an important indicator of socio-economic status, which could be a factor in the survival probability. The \"sibsp\" columns contains the number of siblings and spouses a passenger have on the ship. The \"parch\" column is the number of parent or guardian the passenger have on the ship. The two columns added together is the size of the family together on the ship. The \"ticket\" column is the ticket number, which is unique for vast majority of the passengers. Some tickets have alphabets in front. The \"embarked\" column is the port of departure. This could be another indicator of socio-economic status along with the \"fare\" column. The \"boat\" and \"body\" column is information that should not be used to predict survival rate, since it is statistics gathered after the event. These two columns should be dropped. The \"cabin\" column is very interesting, it contains the **deck** information and the room number. Which deck the passenger resides in could be an indicator of survival rate, and should be extracted. Room number on the other hand, whithout knowing the actual layout of the ship, is not very useful. Finally, the \"home.dest\" column shows the home and destination of the passenger, which could be another indicator of socio-economic status.\n\nWe also immdediatly see that some data is missing. We need to examine in detail which data is missing and determine the best way to fill in these missing data without skewing the prediction results.\n\nWe can use the folloing command to list the categories in the dataset.\n\nIn\u00a0\\[9\\]:\n\n```\nprint(data.columns.values)\n\n```\n\n```\n['pclass' 'survived' 'name' 'sex' 'age' 'sibsp' 'parch' 'ticket' 'fare'\n 'cabin' 'embarked' 'boat' 'body' 'home.dest']\n\n```\n\nFrom these categories, will will drop \"boat\", \"body\" from the dataset since they cannot be used as predictors and they are also not the outcome. We will also drop the \"home.dest\" column since we already have many other good indicators of socio-economic status. Keeping the \"home.dest\" column will not cause harm, we just want to keep our model a simpler.\n\nIn\u00a0\\[10\\]:\n\n```\ndrop_cat=['boat','body','home.dest']\ndata.drop(drop_cat, inplace=True, axis=1)\ndata.head(10)\n\n```\n\nOut\\[10\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C |\n\nLet use directly goto the point and see how many passengers survived the disaster.\n\nIn\u00a0\\[12\\]:\n\n```\ndata['survived'].mean()\n\n```\n\nOut\\[12\\]:\n\n```\n0.3819709702062643\n```\n\nAbout 40% of the passengers survived. We know from historical records that women and children were given priority on the lifeboats. Was this true, and did it reflect in their survival rates? Lets take a look.\n\nIn\u00a0\\[70\\]:\n\n```\nprint('Survival Rate by Sex')\nprint(data['survived'].groupby(data['sex']).mean())\nprint('\\n\\nSex Ratio of Passengers')\nprint(data['sex...",
      "url": "https://xiangyutang2.github.io/feature-engineering"
    }
  ]
}