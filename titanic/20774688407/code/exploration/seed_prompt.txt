# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, survival rates by features

## Problem Overview
- Binary classification task: predict survival (0/1)
- Evaluation metric: Accuracy
- Small dataset: 891 training samples, 418 test samples
- Target distribution: ~38% survived, ~62% did not survive

## Critical Feature Engineering (from top Kaggle kernels)

### 1. Title Extraction from Name
Extract titles from passenger names using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Miss, Mrs, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into 'Rare'
- Map Mlle/Ms → Miss, Mme → Mrs
- Title is highly predictive of survival (Master = young boys, Miss = unmarried women)

### 2. Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size has non-linear relationship with survival (medium families survive better)

### 3. Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class)
- Can extract deck letter from Cabin (first character)

### 4. Name Length
- **Name_length** = len(Name) - longer names may indicate higher status

### 5. Age and Fare Binning
- Bin Age into categories: 0-16, 16-32, 32-48, 48-64, 64+
- Bin Fare into quartiles or categories based on thresholds (7.91, 14.454, 31)

## Missing Value Handling

### Age (20% missing in train, 20% in test)
- Option 1: Fill with median by Pclass and Sex
- Option 2: Random values within mean ± std
- Option 3: Predict using other features (Name title is useful - Master indicates young boy)

### Cabin (77% missing)
- Create Has_Cabin binary feature
- Can impute deck based on Pclass

### Embarked (2 missing in train)
- Fill with mode ('S' - Southampton)

### Fare (1 missing in test)
- Fill with median

## Models to Use

### Single Models (baseline ~78% accuracy)
1. **Random Forest** - Most robust, handles feature interactions well
   - n_estimators: 100-500
   - max_depth: 4-8 (prevent overfitting on small dataset)
   
2. **Gradient Boosting** - Strong performance
   - learning_rate: 0.01-0.1
   - n_estimators: 100-300
   - max_depth: 2-4

3. **XGBoost** - Often best single model
   - learning_rate: 0.01-0.05
   - max_depth: 4-6
   - n_estimators: 200-500

4. **SVM** - Works well with proper scaling
   - Use RBF kernel
   - Tune C and gamma

5. **Logistic Regression** - Good baseline, interpretable

### Ensemble Methods (can achieve ~80-82% accuracy)

#### Voting Classifier
- Combine 5-7 diverse models
- Hard voting often outperforms soft voting for this dataset
- Use: RF, ExtraTrees, AdaBoost, GradientBoosting, SVC

#### Stacking (achieved 80.8% in top kernels)
**Two-level stacking:**
- Level 1 (base models): RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
- Level 2 (meta-learner): XGBoost
- Use out-of-fold predictions to prevent leakage
- Uncorrelated base models produce better results

## Feature Selection
Drop these columns before modeling:
- PassengerId (identifier)
- Name (after extracting Title)
- Ticket (high cardinality, limited value)
- Cabin (after extracting Has_Cabin/Deck)

Keep/Create:
- Pclass, Sex, Age (binned), SibSp, Parch, Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin

## Validation Strategy
- Use Stratified K-Fold (k=5) to maintain class distribution
- Small dataset means high variance in CV scores
- Focus on robust features rather than complex models

## Key Insights from Top Solutions
1. Feature engineering is more important than model selection for this dataset
2. Simple models with good features often beat complex models
3. Title extraction is one of the most powerful features
4. Family size features capture important survival patterns
5. Avoid overfitting - use regularization and limit tree depth
6. The train/test distributions may differ slightly, causing CV-LB gap

## Submission Strategy
1. Start with baseline Random Forest (~78%)
2. Add feature engineering (Title, FamilySize, IsAlone) → ~80%
3. Try ensemble/stacking → ~80-82%
4. Tune hyperparameters carefully
5. Consider multiple submissions with different random seeds

## Code Patterns

### Feature Engineering Pipeline
```python
def engineer_features(df):
    # Title extraction
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')
    df['Title'] = df['Title'].replace('Mme', 'Mrs')
    
    # Family features
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    
    # Cabin
    df['Has_Cabin'] = df['Cabin'].notna().astype(int)
    
    return df
```

### Stacking Implementation
Use sklearn's StackingClassifier or implement custom out-of-fold stacking:
- Split data into K folds
- For each fold, train base models on K-1 folds, predict on held-out fold
- Concatenate predictions as features for meta-learner
