{
  "query": "What techniques achieve highest accuracy on Titanic Kaggle competition beyond 80%?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Aggressive feature engineering (e.g., extracting Title, creating FamilySize/IsAlone, binning Age, encoding Cabin) combined with strong gradient\u2011boosting models such as XGBoost or GradientBoostingClassifier tuned via grid/random/Bayesian search, and multi\u2011model ensembles (soft\u2011voting or stacking of XGBoost, Random\u202fForest, Logistic Regression, etc.) consistently push Titanic Kaggle scores above 80\u202f%\u2014reports show XGBoost reaching 93\u202f% accuracy, ensembles achieving 81.1\u202f% and 83.6\u202f% accuracy, and tuned GradientBoosting/Random\u202fForest hitting 82\u201384\u202f% ([GitHub\u202fmrankitgupta](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost)), ([GitHub\u202fNickKaparinos](https://github.com/NickKaparinos/Kaggle-Titanic)), ([Atlantis\u2011Press\u202fpaper](https://www.atlantis-press.com/article/126015317.pdf)), ([Daniel\u202fPollithy\u202fblog](https://danielpollithy.github.io/python/numpy/scikit/machine-learning/kaggle-titanic-83)), ([Medium\u202f\u00d6zg\u00fcr\u202fSanl\u0131](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).",
      "url": ""
    },
    {
      "title": "GitHub - mrankitgupta/titanic-survival-prediction-93-xgboost: Titanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[mrankitgupta](https://github.com/mrankitgupta)/ **[titanic-survival-prediction-93-xgboost](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n- [Star\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### License\n\n[MIT license](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE)\n\n[1\\\nstar](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/stargazers) [1\\\nfork](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/forks) [Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags) [Activity](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/activity)\n\n[Star](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n[Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n\n# mrankitgupta/titanic-survival-prediction-93-xgboost\n\nmain\n\n[Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[6 Commits](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/commits/main/) |\n| [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) | [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) |\n| [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) | [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) |\n| [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) | [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# Titanic - Machine Learning from Disaster \\| (Accuracy: 93%) XGBoost \ud83d\udef3\ufe0f\n\n### Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### ML Models used: XGBoost, Random Forest, Logistic Regression\n\nIn this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model and Hyperparameter tunning.\n\n### **Prerequisites:**\n\n`Data Analyst Roadmap` \u231b\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### **Overview**\n\n1. **Understand the shape of the data (Histograms, box plots, etc.)**\n\n2. **Data Cleaning**\n\n3. **Data Exploration**\n\n4. **Feature Engineering**\n\n5. **Data Preprocessing for Model**\n\n6. **Basic Model Building**\n\n7. **Model Tuning**\n\n8. **Ensemble Modle Building**\n\n9. **Results**\n\n\n### **About the Project** \ud83d\udef3\ufe0f\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\nKnowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## **Workflow stages**\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\n## Technologies used \u2699\ufe0f\n\n- [Python](https://github.com/mrankitgupta/Python-Lessons)\n\n- [Statistics](https://github.com/mrankitgupta/Statistics-for-Data-Science-using-Python)\n\n- [Jupyter](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-92-74-xgboost)\n\n\n##### Python Libraries :\n\n- [Pandas](https://github.com/mrankitgupta/Kaggle-Pandas-Solved-Exercises) \\| [NumPy](https://numpy.org/) \\| [Matplotlib](https://matplotlib.org/) \\| [Seaborn](https://seaborn.pydata.org) \\|\n\n- [Scikit-Learn](https://scikit-learn.org/) [\\|](https://scikit-learn.org/) [XGBoost](https://xgboost.readthedocs.io/en/stable/)\n\n\n## Project - Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### **Kaggle Project Link:** **[Titanic Survival Prediction](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** \ud83d\udef3\ufe0f \ud83d\udd17\n\n### Datasets\n\nKaggle Titanic Datasets: `Titanic Train` & `Titanic Test`\n\n## Related Projects\u2753 \ud83d\udc68\u200d\ud83d\udcbb \ud83d\udef0\ufe0f\n\n`Spotify Data Analysis using Python` \ud83d\udcca\n\n`Data Analyst Roadmap` \u231b\n\n`Statistics for Data Science using Python` \ud83d\udcca\n\n`Sales Insights - Data Analysis using Tableau & SQL` \ud83d\udcca\n\n`Kaggle - Pandas Solved Exercises` \ud83d\udcca\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### Liked my Contributions\u2753 Follow Me\ud83d\udc49 [Kaggle](https://www.kaggle.com/MrAnkitGupta) and [GitHub](https://github.com/MrAnkitGupta)\n\n[Nominate Me for GitHub Stars](https://stars.github.com/nominate/) \u2b50 \u2728\n\n## For any queries/doubts \ud83d\udd17 \ud83d\udc47\n\n### [Ankit Gupta](https://bio.link/AnkitGupta)\n\n[aggle](https://kaggle.com/mrankitgupta)\n\n## About\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### Topics\n\n[nlp](https://github.com/topics/nlp) [data-science](https://github.com/topics/data-science) [machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [linear-regression](https://github.com/topics/linear-regression) [machine-learning-algorithms](https://github.com/topics/machine-learning-algorithms) [ml](https://github.com/topics/ml) [prediction](https://github.com/topi...",
      "url": "https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost"
    },
    {
      "title": "GitHub - NickKaparinos/Kaggle-Titanic: Kaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).",
      "text": "[Skip to content](https://github.com/NickKaparinos/Kaggle-Titanic#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/NickKaparinos/Kaggle-Titanic) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[NickKaparinos](https://github.com/NickKaparinos)/ **[Kaggle-Titanic](https://github.com/NickKaparinos/Kaggle-Titanic)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n- [Star\\\n0](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n\n\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\n\n### License\n\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE)\n\n[0\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers) [0\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks) [Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags) [Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\n\n[Star](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic)\n\n[Notifications](https://github.com/login?return_to=%2FNickKaparinos%2FKaggle-Titanic) You must be signed in to change notification settings\n\n# NickKaparinos/Kaggle-Titanic\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/NickKaparinos/Kaggle-Titanic/branches) [Tags](https://github.com/NickKaparinos/Kaggle-Titanic/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[10 Commits](https://github.com/NickKaparinos/Kaggle-Titanic/commits/master/) |\n| [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) | [Plots](https://github.com/NickKaparinos/Kaggle-Titanic/tree/master/Plots) |  |  |\n| [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) | [.gitignore](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/.gitignore) |  |  |\n| [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) | [LICENSE](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/LICENSE) |  |  |\n| [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) | [README.md](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/README.md) |  |  |\n| [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) | [main.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/main.py) |  |  |\n| [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) | [stacking.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/stacking.py) |  |  |\n| [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) | [utilities.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/utilities.py) |  |  |\n| [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) | [voting.py](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/voting.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle-Titanic\n\n## Machine learning classification competition\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/titanic.jpg)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/titanic.jpg)\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## Cross validation results\n\nAfter applying the appropriate preprocessing, each model was evaluated using 10-fold cross validation. Each model\\`s hyperparameters were tuned using extensive grid search. Afterwards, tuned models can be ensembled together for a boost in accuracy.\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/cv.png)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/cv.png)\n\n## Test set results\n\nUsing the optimal voting model, a test set accuracy of **0.8110** was achieved, which corresponds to position **349/50092** in the leaderboard (top 0.6%).\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results1.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results1.PNG)\n\n[![drawing](https://github.com/NickKaparinos/Kaggle-Titanic/raw/master/Plots/kaggle_results2.PNG)](https://github.com/NickKaparinos/Kaggle-Titanic/blob/master/Plots/kaggle_results2.PNG)\n\n## About\n\nKaggle Titanic Competition. Using ensemble techniques 81.1% accuracy was achieved in the test set (top 0.6%).\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle) [classification](https://github.com/topics/classification)\n\n### Resources\n\n[Readme](https://github.com/NickKaparinos/Kaggle-Titanic#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/NickKaparinos/Kaggle-Titanic#MIT-1-ov-file)\n\n[Activity](https://github.com/NickKaparinos/Kaggle-Titanic/activity)\n\n### Stars\n\n[**0**\\\nstars](https://github.com/NickKaparinos/Kaggle-Titanic/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/NickKaparinos/Kaggle-Titanic/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/NickKaparinos/Kaggle-Titanic/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNickKaparinos%2FKaggle-Titanic&report=NickKaparinos+%28user%29)\n\n## [Releases](https://github.com/NickKaparinos/Kaggle-Titanic/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/NickKaparinos/packages?repo_name=Kaggle-Titanic)\n\nNo packages published\n\n## Languages\n\n- [Python100.0%](https://github.com/NickKaparinos/Kaggle-Titanic/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/NickKaparinos/Kaggle-Titanic"
    },
    {
      "title": "Kaggle Titanic 83%",
      "text": "Author Kaggle Titanic 83% https://danielpollithy.github.io/python/numpy/scikit/machine-learning/kaggle-titanic-83\nKaggle Titanic 83%\nAuthor\n2018-06-02T00:00:00+02:00\nTrigger\n## How I achieved 83% accuracy in the Kaggle Titanic challenge\nAfter trying different things (a custom linear model, different cvs, different random forests, randomized searches, pca) I found the GradientBoostingClassifier with {\u2018max_depth\u2019: 4, \u2018n_estimators\u2019: 200} to perform the best with 0.8282 with StratifiedCV on the training data.\nThis model resulted in a public score of 0.78947 on Kaggle.\nAnother interesting result was 0.84 accuracy with a RandomForest with CV=10, {\u2018max_features\u2019: 5, \u2018min_samples_leaf\u2019: 7, \u2018min_samples_split\u2019: 3, \u2018n_estimators\u2019: 41}). It led to the same public score of 0.78947.\nI have been searching for better parameters for hours using GridSearch but it did not return anything better than what the initial randomized search returned.\nThe most work intensive part of this challenge was the preprocessing.\nIn the end I had the following pipeline running:\nnum_attribs = [u'Pclass', u'Age', u'SibSp', u'Parch', u'Fare', u'Name']\ncat_attribs = [u'Sex', u'Embarked', u'Name']\nclass NumAttributesAdder(BaseEstimator, TransformerMixin):\ndef __init__(self): # no *args or **kargs\npass\ndef fit(self, X, y=None):\nreturn self # nothing else to do\ndef transform(self, X, y=None):\nX['FamilySize'] = X['SibSp'] + X['Parch'] + 1\nX['IsAlone'] = 1 # initialize to yes/1 is alone\nX['IsAlone'].loc[X['FamilySize'] > 1] = 0\nreturn X\nclass CatAttributesAdder(BaseEstimator, TransformerMixin):\ndef __init__(self, stat_min=10): # no *args or **kargs\nself.stat_min = stat_min\ndef fit(self, X, y=None):\nreturn self # nothing else to do\ndef transform(self, X, y=None):\n# create new features\nX['Title'] = X['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntitle_names = (X['Title'].value_counts() < self.stat_min)\nX['Title'] = X['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nX = X.drop(['Name'], axis=1)\nreturn X\nnum_pipeline = Pipeline([\n('selector1', DataFrameSelector(num_attribs)),\n('name_length', NameLengthAdder()),\n('pandas_median_fill', PandasMedianFill()),\n('num_attribs_adder', NumAttributesAdder()),\n('std_scaler', StandardScaler()),\ncat_pipeline = Pipeline([\n('selector2', DataFrameSelector(cat_attribs)),\n('pandas_mode_fill', PandasModeFill()),\n('cat_attribs_adder', CatAttributesAdder()),\n('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\nfull_pipeline = FeatureUnion(transformer_list=[\n(\"num_pipeline\", num_pipeline),\n(\"cat_pipeline\", cat_pipeline),\nLoading the data and transforming it by tunneling it through the pipeline is easy:\n# load the train and test data\ndata_train = pd.read_csv('../input/train.csv')\ntrain = data_train.copy()\ntrain_labels = data_train['Survived'].copy()\ntrain = full_pipeline.fit_transform(data_train)\nSearching for a RandomForestClassifier looked like this\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30],\n'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]},\n{'bootstrap': [False], 'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]},\nforest_reg = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=10,\nscoring='accuracy', return_train_score=True)\ngrid_search.fit(train, train_labels)\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\nprint(mean_score, params)\nwinner = sorted(zip(cvres[\"mean_test_score\"], cvres[\"params\"], ), key=lambda x: x[0], reverse=True)[0]\nPredict the test data with a model and write to a kaggle submission compatible csv\n# use the forest on the test data\nbest_forest = rnd_search.best_estimator_\ndata_test = pd.read_csv('../input/test.csv')\ntest = full_pipeline.fit_transform(data_test.copy())\ntest_predictions = best_forest.predict(test)\ndata_test['Survived'] = test_predictions\ndata_test.to_csv('solution.csv', columns = ['PassengerId', 'Survived'], index=False)\nAuthor\n# Daniel Pollithy\n[daniel@pollithy.com](mailto:daniel@pollithy.com)\nDeveloper based in Munich, Germany",
      "url": "https://danielpollithy.github.io/python/numpy/scikit/machine-learning/kaggle-titanic-83"
    },
    {
      "title": "",
      "text": "Titanic Survival Prediction Enhanced by Innovative \nFeature Engineering and Multi-Model Ensemble \nOptimization \nHanzhi Li \n1 School of Software Engineering, South China University of Technology, Guangzhou, \nGuangdong, 510006, China \n202230321188@mail.scut.edu.cn\nAbstract. This study enhances Titanic survival prediction through advanced \nfeature engineering and ensemble model optimization. The Titanic dataset \npresents a classic binary classification problem requiring the prediction of \npassenger survival based on demographic and ticket information. Our \nmethodology employs systematic preprocessing where missing values are \nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \npredictions for age values. Novel features were extracted from passenger \nnames, cabin information, and family relationships to improve predictive \npower. Feature importance was evaluated using Random Forest and XGBoost \nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \nfor effective feature selection. Three classification algorithms\u2014logistic \nregression, random forest, and XGBoost\u2014were systematically optimized using \ngrid search, random search, and Bayesian optimization techniques. Based on \nthese optimized models, soft voting and stacking ensemble approaches were \nimplemented. Cross-validation results demonstrate that ensemble methods \nachieved superior accuracy (0.8361) compared to individual models: logistic \nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \nresearch provides valuable optimization strategies for similar classification \ntasks, particularly highlighting how feature engineering combined with \nensemble methods can substantially enhance predictive performance while \nmaintaining computational efficiency. \nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \nLearning, Hyperparameter Tuning. \n1 Introduction \nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \ntwentieth century. Its historical significance and the multidimensional nature of \npassenger data (including social class, gender, age, family relationships, etc.) have \nestablished it as a classic binary classification benchmark dataset in machine learning. \n\u00a9 The Author(s) 2025\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\nhttps://doi.org/10.2991/978-94-6463-823-3_19\n208 H. Li\nDespite extensive research on this problem, traditional methods exhibit \nsignificant limitations. Early studies predominantly focused on the direct application \nof basic features, achieving prediction accuracy generally below 80% [1]. For \ninstance, simple rules based on gender and passenger class could rapidly distinguish \nsurvival groups. However, these methods failed to fully explore nonlinear interactions \nbetween features and higher-order features. Moreover, approximately 20% of the age \nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \napproaches often rely on mean imputation or directly delete missing values, leading to \ninformation loss and model bias [2]. \nAccording to previous research, traditional classification methods applied to the \nTitanic dataset achieved limited accuracy. These methods were constrained by their \ninability to capture complex feature interactions and their sensitivity to missing values \n[2]. Simultaneously, traditional methods predominantly depend on single models such \nas logistic regression and decision trees without conducting feature importance \nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \nensemble models but have not systematically compared their performance with single \nmodels, nor explored how different hyperparameter tuning methods influence \nprediction model performance in this dataset. \nTo address these challenges, this study employs advanced feature engineering \ntechniques to extract new features, utilizes the relationships between features and \neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \nselection techniques like random forests and XGBoost to identify key features for \nsurvival prediction, and performs hyperparameter tuning of logistic regression, \nrandom forests, and XGBoost models using grid search, random search, and Bayesian \noptimization. Based on these methods, soft voting and stacking ensemble models are \ntrained, and the performance of models under various parameter conditions is further \ncompared. \n2 Methods \n2.1 DataSet \nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \ndataset consists of two CSV files: the training set and the test set. The training set \ncontains 891 samples, while the test set contains 418 samples. The data features \ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \nEmbarked. Additionally, the training set includes an additional target variable, \n\"Survived\". Analysis reveals missing values in the Age, Cabin, and Embarked \ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \nAge and Cabin having a relatively high proportion of missing values [2]. \n2.2 Data Processing \nTo facilitate data processing, this study first combined the training set and the test set\nto form a complete dataset, called the \"merged set\". The missing values in the\nTitanic Survival Prediction Enhanced by Innovative Feature \u2026 209\nEmbarked and Fare columns, which had relatively few missing values, were filled \nfirst. By analyzing the sample features corresponding to the missing values in \nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \nCabin of B28, and a Sex of female. Considering the correlations between features, a \nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \nassumption, box plots of Fare distribution under different combinations of Embarked \nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \nPclass is 1, Embarked is more likely to be 'C'. Therefore, the missing Embarked \nvalues were filled with 'C'. Similarly, for cases where Embarked = 'S' and Pclass = 3, \nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \nNext, the Cabin column was processed. Due to the high number of missing values, the \nanalysis was based only on available information. Records with Cabin information \nwere filtered, and the distribution of cabin classes between survivors and \nnon-survivors was compared. Cabin values containing multiple cabins were split into \nlists, and the data was expanded row-wise to generate new features. Two new features \nwere extracted: Deck (the letter part, filled with \"U\" if none) and Cabin number (the \nnumeric part, converted to a numeric type). The Cabin number was further binned, \nand survival rates within different intervals were analyzed [5]. The analysis revealed \nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \nsurvival rates for each Deck were calculated, which showed notable variations: Deck \nB had the highest survival rate at approximately 74%, followed by Decks D and E at \naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \nrate at about 30%. Finally, the expanded data was re-aggregated into the original \nmerged dataset (Fig. 1). \nThe Title (such as \"Mr\", \"Mrs\") was extracted from the Name column as a new\nfeature. Rare Titl...",
      "url": "https://www.atlantis-press.com/article/126015317.pdf"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "Space Titanic Kaggle competition 0.8066 score solution",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n\u00b7\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle\u2019s Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an \u201canomaly\u201d using records recovered from the spaceship\u2019s damaged computer system. The \u201clore\u201d of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship\u2019s passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let\u2019s Start\n\nAs you can see, after I loaded the dataset, I removed both the \u201cPassenger Id\u201d and \u201cName\u201d columns. They are not going to provide any useful or important information to the prediction. Someone\u2019s name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the \u201cCabin\u201d column. Notice that the rows on the \u201cCabin\u201d column follow a specific pattern. Something like this: \u201cA/5/S\u201d, \u201cC/1/S\u201d, \u201cF/7/P\u201d. And I decided to investigate it. So, to make things simple I split the rows of the \u201cCabin\u201d into three columns based on both slashes (\u201d/\u201d) of the rows. For example, the \u201cA/5/S\u201d row would be transformed into three new columns: The first one is named \u201ccabin\\_code\u201d referring tothe character behind the first slash (A). The second one named \u201cid\\_cabin\u201d refers to the character behind the second slash (5). The third one named \u201ccabin\\_sector\u201d refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that \u201ccabin\\_code\u201d only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the \u201ccabin\\_sector\u201d column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original \u201cCabin\u201d column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the \u201cCabin\u201d column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the \u201ccabin\\_id\u201d and the column that I had created. As I said before, the Id will not interfere with the model\u2019s predictive ability.\n\nSo used: df.drop(columns=\\[\u201cCabin\u201d,\u201did\\_cabin\u201d\\], inplace=True) to drop both columns\n\nBefore splitting our data, the \u201cTransported\u201d column must be in a binary format. As you can see, I switched \u201cTrue\u201d for 1 and \u201cFalse\u201d for 0.\n\nBinary transformation: df\\[\u201cTransported\u201d\\] = df\\[\u201cTransported\u201d\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the \u201ccabin\\_code\u201d column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don\u2019t know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named \u201ctransformer\u201d. Observe that we also have to specify the type of data to which the pipeline will be applied t...",
      "url": "https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6"
    },
    {
      "title": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset",
      "text": "Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset | by Devang Chavda | Towards AI\n[Sitemap](https://pub.towardsai.net/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Towards AI\n](https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-40a0aa184c1c---------------------------------------)\n\u00b7Follow publication\n[\n![Towards AI](https://miro.medium.com/v2/resize:fill:76:76/1*JyIThO-cLjlChQLb6kSlVQ.png)\n](https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-40a0aa184c1c---------------------------------------)\nMaking AI accessible to 100K+ learners. Find the most practical, hands-on and comprehensive AI Engineering and AI for Work certifications at[academy.towardsai.net](http://academy.towardsai.net)- we have pathways for any experience level. Monthly cohorts still open\u200a\u2014\u200ause COHORT10 for 10% off!\nFollow publication\n# Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\n## This article will walk you through detailed forward feature selection steps and model building from scratch, improving it further with fine-tuning.\n[\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\n](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--40a0aa184c1c---------------------------------------)\n5 min read\n\u00b7May 24, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-artificial-intelligence/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--40a0aa184c1c---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/40a0aa184c1c&amp;operation=register&amp;redirect=https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c&amp;source=---header_actions--40a0aa184c1c---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[NASA](https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\n[\n## Data pre-processing of spaceship-titanic kaggle dataset for achieving 80+% accuracy.\n### Data cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact\u2026\nmedium.com\n](https://medium.com/@chavdadevang23/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42?source=post_page-----40a0aa184c1c---------------------------------------)\n**We will be building a model in 3 trenches:**\n1. Building a model with only numerical features.\n2. Building a model with only categorical features.\n3. Building a model with all features combined.\n```\nNUMS = [&#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,&#x27;Num&#x27;,\n&#x27;&#x27;Expenditure&#x27;&#x27;,&#x27;&#x27;Group\\_size&#x27;&#x27;,&#x27;&#x27;Expenditure&#x27;&#x27;]\nTARGET = [&#x27;Transported&#x27;]\n# only numerical feature dataframe\nnums\\_df = df[NUMS+TARGET]\ndf[NUMS].head(3)\n```\nLet\u2019s find out which features have the most importance in order to classify a data point, we will be using the forward feature selection method.\n> Forward feature selection: Step-by-step process of adding features to improve a model\u2019s performance, starting with none, to identify the most relevant ones.\n```\ndef evaluate\\_model\\_kfold\\_classification(X,y,k,clf):\n#kfold base\nX = X.fillna(0)\nkf = model\\_selection.KFold(n\\_splits=k,shuffle=True)\naccuracies = []\nfor fold, (train\\_index,validation\\_index) in enumerate(kf.split(X=X)):\ntrain\\_x = X.loc[train\\_index].values\ntrain\\_y = y.loc[train\\_index].values\nvalidation\\_x = X.loc[validation\\_index].values\nvalidation\\_y = y.loc[validation\\_index].values\nclf.fit(train\\_x,train\\_y)\npreds = clf.predict(validation\\_x)\naccuracy = metrics.accuracy\\_score(validation\\_y, preds)\nprint(f&quot;Fold={fold}, Accuracy={accuracy}&quot;)\naccuracies.append(accuracy)\nreturn sum(accuracies)/len(accuracies)\ndef feature\\_selection\\_classification(X,y,k,model):\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nbest\\_feature= None\nfor feature in list(X.columns):\nscore = evaluate\\_model\\_kfold\\_classification(X[[feature]],y,k,model)\nif score &gt;&gt; average\\_eval\\_metric:\nbest\\_feature = feature\naverage\\_eval\\_metric =score\nprint(&quot;&quot;best feature--&gt;&gt;&quot;&quot;,best\\_feature)\nfeatures = list(X.columns)\nfeatures.remove(best\\_feature)\nbest\\_feature\\_order = [best\\_feature]\nbest\\_feature\\_order.extend(features)\nprint(&quot;&quot;best feature order --&gt;&gt;&quot;&quot;,best\\_feature\\_order)\ngood\\_features = []\naverage\\_eval\\_metric = 0 #only positive evalution metrics , loss metric where inverse is good cant work in this code\nscores\\_progression = {}\nfor feature in best\\_feature\\_order:\ngood\\_features.append(feature)\nscore = evaluate\\_model\\_kfold\\_classification(X[good\\_features],y,k,model)\nscores\\_progression[&#x27;&#x27;|&#x27;&#x27;.join(good\\_features)] = score\nif score &lt;&lt; average\\_eval\\_metric:\ngood\\_features.remove(feature)\nelse:\naverage\\_eval\\_metric = score\nreturn good\\_features,scores\\_progression\n```\nThe code performs feature selection for classification. It iterates over features, evaluates their impact on the model, and selects the best ones based on evaluation metrics.\nHere I am using logistic regression as my base model to select the best features. Why logistic regression: when we did t-sne analysis on this dataset, we found out that data points are separated in a way where drawing a boundary will be easier,\n```\n# here any classification model can be chosen in order to get best features\nclf = LogisticRegression()\ngood\\_features , score\\_progression = feature\\_selection\\_classification(df[NUMS],df[TARGET],5,clf)\nscore\\_progression\n```\nPress enter or click to view image in full size\n![]()\nStep-by-step progress\nPress enter or click to view image in full size\n![]()\ngood features\nNow measure accuracy with all numeric features vs with only good features, which we have derived with feature selection.\n```\nprint(&quot;&quot;\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*with all features\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*...",
      "url": "https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    }
  ]
}