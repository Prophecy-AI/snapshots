# Titanic Survival Prediction - Strategy Guide (Loop 3)

## Current Status
- Best CV score: 0.8372 from exp_001 (Voting Ensemble)
- Best LB score: 0.7727 from exp_001 (Voting Ensemble)
- CV-LB gap: +0.0645 (narrowed from +0.0732)
- Target: 1.0 (UNREALISTIC - top Kaggle solutions achieve 0.80-0.82)
- Realistic target: 0.80 LB (need +0.0273 improvement)
- Submissions remaining: 5/10

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator confirmed:
- Validation methodology is sound (10-fold Stratified CV)
- No leakage detected in implementation
- Score integrity verified

**Evaluator's top priority: Submit ensemble and implement stacking.**
- DONE: Submitted exp_001, got LB 0.7727 (+0.0143 improvement)
- AGREE: Stacking is the clear next step - reference kernels achieved 0.808 LB

**Key concerns raised:**
1. Target of 1.0 is unrealistic → ACKNOWLEDGED. Realistic target is 0.80 LB.
2. Stacking not attempted → ADDRESSING in this loop
3. Ticket features unexplored → Will add to stacking experiment

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Basic EDA, survival patterns
- `exploration/evolver_loop1_analysis.ipynb` - Deck, Ticket, Name_Length analysis
- `exploration/evolver_loop2_lb_feedback.ipynb` - CV-LB calibration analysis
- `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` - Stacking reference (0.808 LB)

**Key patterns to exploit:**
1. **CV-LB Calibration**: LB = 2.55*CV - 1.37. Need CV ~0.8479 to achieve LB 0.80
2. **Prediction calibration**: Baseline under-predicted survival. Ensemble correctly identified:
   - 21 Pclass 3 females as survivors
   - 10 Pclass 1 males as non-survivors
3. **Ticket frequency**: 2-3 shared tickets have 57-70% survival, 5+ have 0%
4. **Name_Length**: 0.33 correlation with survival (VeryLong: 62.6% vs Short: 23%)

## Recommended Approaches (Priority Order)

### Priority 1: STACKING (Highest Impact)
Reference kernel achieved 0.808 LB with stacking. Implementation:

**Base models (5-7 diverse models):**
- RandomForestClassifier (n_estimators=200, max_depth=6)
- ExtraTreesClassifier (n_estimators=200, max_depth=6)
- AdaBoostClassifier (n_estimators=100, learning_rate=0.5)
- GradientBoostingClassifier (n_estimators=100, max_depth=3)
- SVC (kernel='rbf', C=1.0, probability=True)
- XGBClassifier (n_estimators=100, max_depth=3)
- LogisticRegression (C=0.1)

**Meta-learner:**
- XGBClassifier (n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8)
- OR LogisticRegression (simpler, may generalize better)

**Implementation steps:**
1. Use StratifiedKFold (n_splits=5 or 10)
2. For each fold, train all base models on training fold
3. Generate out-of-fold predictions for training data
4. Generate test predictions (average across folds)
5. Stack OOF predictions as features for meta-learner
6. Train meta-learner on stacked features
7. Predict test using stacked test predictions

### Priority 2: Add Ticket Features
Strong signal identified but not yet exploited:
- **Ticket_Frequency**: Count passengers sharing same ticket
- **Ticket_Survival_Rate**: Mean survival rate of ticket group (use OOF to avoid leakage)
- Ticket frequency 2-3 has 57-70% survival, 5+ has 0%

### Priority 3: Add Name_Length Feature
- 0.33 correlation with survival
- VeryLong names (>40 chars): 62.6% survival
- Short names (<20 chars): 23% survival
- Simple to add, may provide additional signal

### Priority 4: Feature Selection
With 14+ features, some may be redundant:
- Use feature importance from base models
- Consider removing low-importance features
- Deck has 77% unknown - may add noise

## What NOT to Try

1. **Simple voting ensemble variations** - Already implemented, diminishing returns
2. **Deeper trees** - Caused overfitting (CV-LB gap was 0.0732 with deeper trees)
3. **More complex hyperparameter tuning** - Low ROI at this stage
4. **Target of 1.0** - Impossible. Focus on 0.80 LB.

## Validation Notes

- Use StratifiedKFold (k=5 or 10) for all experiments
- CV-LB calibration: LB ≈ 2.55*CV - 1.37
- Current gap (+0.0645) is acceptable but watch for increases
- Lower variance correlates with better generalization
- Track prediction distributions (baseline under-predicted survival)

## Implementation Checklist for Stacking

```python
# Stacking implementation outline
from sklearn.model_selection import StratifiedKFold

# 1. Define base models
base_models = [
    ('rf', RandomForestClassifier(...)),
    ('et', ExtraTreesClassifier(...)),
    ('ada', AdaBoostClassifier(...)),
    ('gb', GradientBoostingClassifier(...)),
    ('svc', SVC(probability=True, ...)),
    ('xgb', XGBClassifier(...)),
    ('lr', LogisticRegression(...))
]

# 2. Generate OOF predictions
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof_train = np.zeros((len(X_train), len(base_models)))
oof_test = np.zeros((len(X_test), len(base_models)))

for i, (name, model) in enumerate(base_models):
    oof_test_fold = np.zeros((len(X_test), kfold.n_splits))
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
        model.fit(X[train_idx], y[train_idx])
        oof_train[val_idx, i] = model.predict_proba(X[val_idx])[:, 1]
        oof_test_fold[:, fold] = model.predict_proba(X_test)[:, 1]
    oof_test[:, i] = oof_test_fold.mean(axis=1)

# 3. Train meta-learner
meta_learner = XGBClassifier(n_estimators=2000, max_depth=4, ...)
meta_learner.fit(oof_train, y)
predictions = meta_learner.predict(oof_test)
```

## Expected Outcome

If stacking works as in reference kernel:
- CV should improve to ~0.84-0.85
- LB should improve to ~0.78-0.80
- This would be competitive with top 10% solutions

## Submission Strategy

- Submit stacking result if CV improves significantly (>0.84)
- 5 submissions remaining - use wisely
- Focus on CV improvement, trust the calibration