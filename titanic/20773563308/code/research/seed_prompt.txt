# Titanic Survival Prediction - Evolved Strategy (Loop 3)

## Current Status
- Best CV score: 0.8372 from exp_001 (Voting Ensemble)
- Best LB score: 0.7584 from exp_000 (XGBoost baseline)
- **CV-LB gap: +0.0732** → Significant overfitting. CV overestimates by 7.3%
- **CRITICAL**: exp_001 submitted this loop - awaiting LB feedback

## Response to Evaluator
- **Technical verdict was TRUSTWORTHY.** Implementation is sound, no leakage detected.
- **Evaluator's top priority**: Submit exp_001 NOW and implement stacking. **AGREED** - Submitting exp_001 this loop.
- **Key concerns raised**: 
  1. No LB submission for ensemble yet - **ADDRESSING NOW** with submission
  2. Target of 1.0 is unrealistic - **Acknowledged.** Realistic ceiling is ~0.80-0.82 LB.
  3. Stacking not attempted - **NEXT PRIORITY** after LB feedback
  4. Prediction distribution shifted (255/163 vs 267/151) - **MONITORING** - 21 females in Pclass 3 changed to survived

## Data Understanding
**Reference notebooks:**
- `exploration/eda.ipynb` - Initial EDA with survival patterns
- `exploration/evolver_loop1_analysis.ipynb` - Feature engineering analysis
- `exploration/evolver_loop1_lb_feedback.ipynb` - Distribution shift analysis
- `exploration/evolver_loop2_analysis.ipynb` - Prediction comparison analysis

**Key patterns to exploit (with evidence):**
1. **Sex** - Strongest predictor: Female 74.2% vs Male 18.9% survival
2. **Pclass** - Clear gradient: 1st=63%, 2nd=47%, 3rd=24%
3. **Title** - Captures Sex+Age+Status: Master (young boys) high survival
4. **Deck feature** - Strong signal: Deck D/E/B have 74-76% survival vs U (unknown) at 30%
5. **FamilySize** - Non-linear: Medium families (2-4) survive better than alone or large

## Recommended Approaches (Priority Order)

### Priority 1: Stacking Ensemble (MAIN FOCUS)
Reference: `../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` achieved 0.808 LB

Implementation:
```python
# Base models (diverse)
base_models = [
    RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4),
    ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4),
    AdaBoostClassifier(n_estimators=100, learning_rate=0.5),
    GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1),
    SVC(kernel='rbf', C=1.0, probability=True)
]

# Meta-learner (simple to avoid overfitting)
meta_learner = LogisticRegression(C=0.1, max_iter=1000)

# Use 5-fold CV for out-of-fold predictions
# Stack OOF predictions as meta-features
```

Key points:
- Use out-of-fold predictions to avoid leakage
- Simple meta-learner (LogisticRegression) to reduce overfitting
- Diverse base models for better stacking

### Priority 2: Feature Selection/Reduction
With 14 features, some may be redundant or noisy:
- Analyze feature importance from voting ensemble
- Consider removing low-importance features
- Test with fewer features to reduce overfitting

### Priority 3: Ticket Features (If Stacking Works)
From analysis: Ticket frequency has strong signal
- 2-3 shared tickets: 57-70% survival
- 5+ shared tickets: 0% survival
- Only add if stacking shows improvement

### Priority 4: Hyperparameter Tuning (Last Resort)
Only if stacking doesn't help:
- GridSearchCV on best model
- Focus on regularization parameters

## What NOT to Try
- **Complex hyperparameter tuning** - Focus on ensemble diversity first
- **Neural networks** - Overkill for 891 samples
- **Name_Length feature** - May not generalize (correlation could be spurious)
- **More complex models** - Overfitting is the issue, not underfitting

## Validation Notes
- Use 10-fold Stratified CV (consistent with baseline)
- random_state=42 for reproducibility
- **Key insight**: CV-LB gap of +0.073 means we should expect LB to be ~7% lower than CV
- If CV improves by 0.01, LB might improve by ~0.007 (assuming similar gap)
- Target CV of ~0.85 to achieve LB of ~0.78-0.79

## Conditional Strategy Based on LB Feedback

### If exp_001 LB > 0.7584 (improvement):
- Proceed with stacking (Priority 1)
- The ensemble approach is working
- Focus on model diversity

### If exp_001 LB ≤ 0.7584 (no improvement):
- The CV-LB gap may be due to distribution shift
- Consider:
  1. More aggressive regularization
  2. Removing features that may not generalize (e.g., Deck has 77% unknown)
  3. Simpler models (LogisticRegression alone)
  4. Adversarial validation to identify drifting features

## Success Criteria
- CV improvement over 0.8372 (current best)
- LB improvement over 0.7584 (current best)
- Reduced CV-LB gap (target gap < 0.06)

## Key Insights from Research
- Stacking with diverse base models achieves 0.808+ LB (from kernels)
- Simpler models often generalize better on small datasets
- Ensemble diversity is key - use different model families
- Feature engineering matters more than model choice
- Title, Sex, Pclass are top features - already captured
