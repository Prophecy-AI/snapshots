# Titanic Survival Prediction - Evolved Strategy (Loop 1)

## Current Status
- Best CV score: 0.8316 from exp_000 (XGBoost baseline)
- Best LB score: None (no submissions yet)
- CV-LB gap: Unknown - need to submit to calibrate
- Target: 1.0 (100% accuracy)

## Response to Evaluator
- **Technical verdict was TRUSTWORTHY.** Implementation is sound, no leakage detected.
- **Evaluator's top priority**: Submit baseline and implement ensemble methods. **AGREE** - we need LB feedback for calibration.
- **Key concerns raised**: 
  1. Target of 1.0 is unrealistic - **Acknowledged but proceeding anyway.** Our job is to pursue the target. Best legitimate solutions achieve ~0.80-0.82 LB.
  2. No submissions made - **Addressing this loop.** Will submit baseline first.
  3. Single model approach - **Addressing this loop.** Will build voting ensemble.
  4. Unexplored features (Deck, Ticket) - **Addressing this loop.** Analysis shows strong signal.

## Data Understanding
**Reference notebooks:**
- `exploration/eda.ipynb` - Initial EDA with survival patterns
- `exploration/evolver_loop1_analysis.ipynb` - Feature engineering analysis

**Key patterns to exploit (with evidence):**
1. **Deck feature** - Strong signal: Deck D/E/B have 74-76% survival vs U (unknown) at 30%. See analysis notebook.
2. **Name_Length** - 0.33 correlation with survival (stronger than expected). VeryLong names: 62.6% survival vs Short: 23%.
3. **Ticket_Freq** - Non-linear pattern: 2-3 shared tickets have 57-70% survival, but 5+ shared tickets have 0% (large groups perished).
4. **Age binning** - Children (0-16) have 55% survival vs Young adults (16-32) at 37%.
5. **Sex + Pclass interaction** - Already captured by Title but explicit interaction may help.

## Recommended Approaches (Priority Order)

### Priority 1: Submit Baseline for LB Calibration
- Submit exp_000 (XGBoost baseline, CV=0.8316) to get LB feedback
- This establishes CV-LB correlation for future experiments
- Use Submit tool with experiment_id="exp_000"

### Priority 2: Enhanced Feature Engineering
Add these features to the baseline:
1. **Deck** = First character of Cabin, 'U' for unknown
2. **Name_Length** = len(Name)
3. **Ticket_Freq** = Count of passengers sharing same ticket
4. **Age_Bin** = Categorical bins: Child (0-16), Young (16-32), Middle (32-48), Senior (48-64), Elderly (64+)
5. **FamilySize_Bin** = 1 (alone), 2-4 (small), 5+ (large) - captures non-linear relationship

### Priority 3: Voting Ensemble
Build hard/soft voting ensemble with diverse models:
- XGBoost (current best)
- RandomForest (n_estimators=200, max_depth=8)
- GradientBoosting (n_estimators=200, max_depth=4)
- ExtraTrees (n_estimators=200)
- AdaBoost (n_estimators=200)
- LogisticRegression (regularized)
- SVC (kernel='rbf', probability=True for soft voting)

Use soft voting (average probabilities) - typically better than hard voting.

### Priority 4: Stacking Ensemble (After Voting)
If voting ensemble improves score, implement stacking:
- Base models: RF, ExtraTrees, AdaBoost, GradientBoosting, SVC
- Meta-learner: XGBoost or LogisticRegression
- Use out-of-fold predictions for meta-features
- Reference: `../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` achieved 0.808 LB

## What NOT to Try
- **Hyperparameter tuning** - Premature. Focus on features and ensembles first.
- **Neural networks** - Overkill for 891 samples. Tree-based models dominate.
- **Complex ticket prefix encoding** - Too many categories, weak signal.

## Validation Notes
- Use 10-fold Stratified CV (consistent with baseline)
- random_state=42 for reproducibility
- This is 100% public LB - LB score is on entire test set
- CV-LB correlation should be strong given 100% public LB

## Implementation Order for Next Experiment
1. First: Submit baseline (exp_000) to get LB feedback
2. Then: Create exp_001 with enhanced features + voting ensemble
3. Compare CV scores and submit if improved

## Key Insights from Research
- Stacking with diverse base models achieves 0.808+ LB (from kernels)
- XGBoost with proper tuning can achieve 82-84% CV
- Feature engineering matters more than model choice
- Title, Sex, Pclass are top features - already captured
- Deck and Name_Length are underutilized features with strong signal
