{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a6f13e",
   "metadata": {},
   "source": [
    "# Stacking Ensemble with Enhanced Features\n",
    "\n",
    "**Goal:** Implement stacking following reference kernel that achieved 0.808 LB\n",
    "\n",
    "**Approach:**\n",
    "1. 7 diverse base models generate out-of-fold predictions\n",
    "2. OOF predictions become features for meta-learner\n",
    "3. Add Ticket_Frequency and Name_Length features\n",
    "4. Use XGBoost or LogisticRegression as meta-learner\n",
    "\n",
    "**Reference:** `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c68a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(name):\n",
    "    \"\"\"Extract title from name using regex\"\"\"\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def process_features(df, ticket_freq_map=None, is_train=True):\n",
    "    \"\"\"Process features with enhanced feature set including Ticket_Frequency and Name_Length\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Title extraction\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', \n",
    "                                        'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    # 2. Family features\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df['FamilySize_Bin'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 11], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # 3. Cabin features\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'U')\n",
    "    \n",
    "    # 4. Name_Length (NEW - 0.33 correlation with survival)\n",
    "    df['Name_Length'] = df['Name'].apply(len)\n",
    "    \n",
    "    # 5. Ticket_Frequency (NEW - strong signal: 2-3 shared = 57-70% survival, 5+ = 0%)\n",
    "    if ticket_freq_map is None:\n",
    "        ticket_freq_map = df['Ticket'].value_counts().to_dict()\n",
    "    df['Ticket_Frequency'] = df['Ticket'].map(ticket_freq_map).fillna(1)\n",
    "    \n",
    "    # 6. Embarked - fill missing with mode\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # 7. Fare - fill missing with median by Pclass\n",
    "    if df['Fare'].isna().any():\n",
    "        df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return df, ticket_freq_map\n",
    "\n",
    "# Process train first to get ticket frequency map\n",
    "train_processed, ticket_freq_map = process_features(train, is_train=True)\n",
    "# Use train's ticket frequency map for test (to avoid leakage)\n",
    "test_processed, _ = process_features(test, ticket_freq_map=ticket_freq_map, is_train=False)\n",
    "\n",
    "print(\"New features:\")\n",
    "print(f\"Name_Length range: {train_processed['Name_Length'].min()} - {train_processed['Name_Length'].max()}\")\n",
    "print(f\"Ticket_Frequency distribution:\")\n",
    "print(train_processed['Ticket_Frequency'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e46afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age imputation using median by Pclass, Sex, Title\n",
    "def impute_age(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    age_medians = train_df.groupby(['Pclass', 'Sex', 'Title'])['Age'].median()\n",
    "    \n",
    "    def get_median_age(row, medians, fallback_median):\n",
    "        if pd.isna(row['Age']):\n",
    "            try:\n",
    "                return medians.loc[(row['Pclass'], row['Sex'], row['Title'])]\n",
    "            except KeyError:\n",
    "                return fallback_median\n",
    "        return row['Age']\n",
    "    \n",
    "    fallback = train_df['Age'].median()\n",
    "    train_df['Age'] = train_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    test_df['Age'] = test_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_processed, test_processed = impute_age(train_processed, test_processed)\n",
    "\n",
    "# Age_Bin feature\n",
    "def add_age_bin(df):\n",
    "    df = df.copy()\n",
    "    df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "    return df\n",
    "\n",
    "train_processed = add_age_bin(train_processed)\n",
    "test_processed = add_age_bin(test_processed)\n",
    "\n",
    "print(f\"Age missing: Train={train_processed['Age'].isna().sum()}, Test={test_processed['Age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "def encode_features(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Sex encoding\n",
    "    train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    test_df['Sex'] = test_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    # Embarked encoding\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_df['Embarked'] = train_df['Embarked'].map(embarked_map)\n",
    "    test_df['Embarked'] = test_df['Embarked'].map(embarked_map)\n",
    "    \n",
    "    # Title encoding\n",
    "    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "    train_df['Title'] = train_df['Title'].map(title_map)\n",
    "    test_df['Title'] = test_df['Title'].map(title_map)\n",
    "    \n",
    "    # Deck encoding\n",
    "    deck_map = {'U': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "    train_df['Deck'] = train_df['Deck'].map(deck_map)\n",
    "    test_df['Deck'] = test_df['Deck'].map(deck_map)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_encoded, test_encoded = encode_features(train_processed, test_processed)\n",
    "\n",
    "# Feature set (16 features now with Name_Length and Ticket_Frequency)\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "            'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', \n",
    "            'Deck', 'FamilySize_Bin', 'Age_Bin',\n",
    "            'Name_Length', 'Ticket_Frequency']  # NEW features\n",
    "\n",
    "X = train_encoded[features].values\n",
    "y = train_encoded['Survived'].values\n",
    "X_test = test_encoded[features].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features ({len(features)}): {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc91f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models for stacking (same as voting ensemble)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, \n",
    "                          use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
    "    ('lr', LogisticRegression(C=0.1, max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "print(f\"Base models ({len(base_models)}):\")\n",
    "for name, model in base_models:\n",
    "    print(f\"  - {name}: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACKING: Generate out-of-fold predictions\n",
    "print(\"Generating out-of-fold predictions for stacking...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to store OOF predictions\n",
    "oof_train = np.zeros((len(X), len(base_models)))  # OOF predictions for training data\n",
    "oof_test = np.zeros((len(X_test), len(base_models)))  # Test predictions\n",
    "\n",
    "# Track individual model CV scores\n",
    "model_cv_scores = {name: [] for name, _ in base_models}\n",
    "\n",
    "for i, (name, model_template) in enumerate(base_models):\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "    oof_test_folds = np.zeros((len(X_test), kfold.n_splits))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Clone model for each fold\n",
    "        from sklearn.base import clone\n",
    "        model = clone(model_template)\n",
    "        \n",
    "        # Scale for SVC and LR\n",
    "        if name in ['svc', 'lr']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train_fold)\n",
    "            oof_train[val_idx, i] = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            oof_test_folds[:, fold] = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            val_pred = model.predict(X_val_scaled)\n",
    "        else:\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            oof_train[val_idx, i] = model.predict_proba(X_val_fold)[:, 1]\n",
    "            oof_test_folds[:, fold] = model.predict_proba(X_test)[:, 1]\n",
    "            val_pred = model.predict(X_val_fold)\n",
    "        \n",
    "        model_cv_scores[name].append(accuracy_score(y_val_fold, val_pred))\n",
    "    \n",
    "    # Average test predictions across folds\n",
    "    oof_test[:, i] = oof_test_folds.mean(axis=1)\n",
    "    print(f\"  {name} CV: {np.mean(model_cv_scores[name]):.4f} (+/- {np.std(model_cv_scores[name]):.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Base model CV scores:\")\n",
    "for name, scores in model_cv_scores.items():\n",
    "    print(f\"  {name:5s}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACKING: Train meta-learner on OOF predictions\n",
    "print(\"\\nTraining meta-learner on stacked features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try both XGBoost and LogisticRegression as meta-learners\n",
    "# XGBoost meta-learner (as in reference kernel)\n",
    "meta_xgb = XGBClassifier(\n",
    "    n_estimators=2000, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.01,\n",
    "    gamma=0.9, \n",
    "    subsample=0.8, \n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# LogisticRegression meta-learner (simpler, may generalize better)\n",
    "meta_lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "# Evaluate meta-learners with CV\n",
    "kfold_meta = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# XGBoost meta-learner CV\n",
    "xgb_cv_scores = []\n",
    "for train_idx, val_idx in kfold_meta.split(oof_train, y):\n",
    "    meta_xgb_clone = clone(meta_xgb)\n",
    "    meta_xgb_clone.fit(oof_train[train_idx], y[train_idx])\n",
    "    val_pred = meta_xgb_clone.predict(oof_train[val_idx])\n",
    "    xgb_cv_scores.append(accuracy_score(y[val_idx], val_pred))\n",
    "\n",
    "print(f\"Meta XGBoost CV: {np.mean(xgb_cv_scores):.4f} (+/- {np.std(xgb_cv_scores):.4f})\")\n",
    "\n",
    "# LogisticRegression meta-learner CV\n",
    "lr_cv_scores = []\n",
    "for train_idx, val_idx in kfold_meta.split(oof_train, y):\n",
    "    meta_lr_clone = clone(meta_lr)\n",
    "    meta_lr_clone.fit(oof_train[train_idx], y[train_idx])\n",
    "    val_pred = meta_lr_clone.predict(oof_train[val_idx])\n",
    "    lr_cv_scores.append(accuracy_score(y[val_idx], val_pred))\n",
    "\n",
    "print(f\"Meta LR CV: {np.mean(lr_cv_scores):.4f} (+/- {np.std(lr_cv_scores):.4f})\")\n",
    "\n",
    "# Choose best meta-learner\n",
    "if np.mean(xgb_cv_scores) >= np.mean(lr_cv_scores):\n",
    "    best_meta = 'xgb'\n",
    "    best_cv = np.mean(xgb_cv_scores)\n",
    "    best_std = np.std(xgb_cv_scores)\n",
    "    print(f\"\\nBest meta-learner: XGBoost\")\n",
    "else:\n",
    "    best_meta = 'lr'\n",
    "    best_cv = np.mean(lr_cv_scores)\n",
    "    best_std = np.std(lr_cv_scores)\n",
    "    print(f\"\\nBest meta-learner: LogisticRegression\")\n",
    "\n",
    "print(f\"\\nStacking CV: {best_cv:.4f} (+/- {best_std:.4f})\")\n",
    "print(f\"Previous best (Voting Ensemble): 0.8372 (+/- 0.0239)\")\n",
    "print(f\"Improvement: {best_cv - 0.8372:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e22a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final meta-learner on all OOF predictions and generate test predictions\n",
    "print(\"\\nGenerating final predictions...\")\n",
    "\n",
    "if best_meta == 'xgb':\n",
    "    final_meta = XGBClassifier(\n",
    "        n_estimators=2000, max_depth=4, learning_rate=0.01,\n",
    "        gamma=0.9, subsample=0.8, colsample_bytree=0.8,\n",
    "        use_label_encoder=False, eval_metric='logloss', random_state=42\n",
    "    )\n",
    "else:\n",
    "    final_meta = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "final_meta.fit(oof_train, y)\n",
    "test_predictions = final_meta.predict(oof_test)\n",
    "\n",
    "# Also get probabilities for potential threshold tuning\n",
    "test_proba = final_meta.predict_proba(oof_test)[:, 1]\n",
    "\n",
    "print(f\"Test predictions generated using {best_meta} meta-learner\")\n",
    "print(f\"Prediction distribution: {(test_predictions == 0).sum()} died, {(test_predictions == 1).sum()} survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Baseline (XGBoost): 267 died, 151 survived\")\n",
    "print(f\"  Voting Ensemble:    255 died, 163 survived\")\n",
    "print(f\"  Stacking:           {(test_predictions == 0).sum()} died, {(test_predictions == 1).sum()} survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852322a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFeatures used: {len(features)}\")\n",
    "print(f\"  - Original: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\")\n",
    "print(f\"  - Engineered: Title, FamilySize, IsAlone, Has_Cabin, Deck, FamilySize_Bin, Age_Bin\")\n",
    "print(f\"  - NEW: Name_Length, Ticket_Frequency\")\n",
    "print(f\"\\nBase models: {len(base_models)}\")\n",
    "print(f\"Meta-learner: {best_meta}\")\n",
    "print(f\"\\nCV Scores:\")\n",
    "print(f\"  Baseline (XGBoost):    0.8316 (+/- 0.0324)\")\n",
    "print(f\"  Voting Ensemble:       0.8372 (+/- 0.0239)\")\n",
    "print(f\"  Stacking:              {best_cv:.4f} (+/- {best_std:.4f})\")\n",
    "print(f\"\\nImprovement over baseline: {best_cv - 0.8316:+.4f}\")\n",
    "print(f\"Improvement over voting:   {best_cv - 0.8372:+.4f}\")\n",
    "print(f\"\\nExpected LB (using calibration LB = 2.55*CV - 1.37):\")\n",
    "print(f\"  Predicted LB: {2.55 * best_cv - 1.37:.4f}\")\n",
    "print(f\"  Previous best LB: 0.7727\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
