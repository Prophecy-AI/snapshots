{
  "query": "What techniques achieve highest accuracy on Titanic Kaggle competition beyond 80%?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most recent source (Dataquest,\u202fMar\u202f2025) reports only an 81\u202f% score using standard preprocessing and a basic model, and no newer techniques achieving higher than 80\u202f% have been documented up to that date ([dataquest.io](https://www.dataquest.io/blog/kaggle-fundamentals)).",
      "url": ""
    },
    {
      "title": "Kaggle Fundamentals: The Titanic Competition - Dataquest",
      "text": "Kaggle Fundamentals: The Titanic Competition &#8211; Dataquest\n[](https://www.dataquest.io)\n* [Profile](https://app.dataquest.io/profile)\n* [Account](https://app.dataquest.io/settings/account)\n* [Subscription](https://app.dataquest.io/settings/subscription)\n* [Teams](https://www.dataquest.io/for-business/)\n* [Help](https://support.dataquest.io/en)\n* [Logout](https://app.dataquest.io/logout)\n[](https://www.dataquest.io/#)\nNew Year Launchpad: Lift Off Your Data Career with 57% Off Lifetime Plan.\n[Save Now](https://app.dataquest.io/payment?plan_interval=lifetime)\nMarch 25, 2025\n# Kaggle Fundamentals: The Titanic Competition\n[Kaggle](https://www.kaggle.com/)is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\nIn this tutorial we'll learn learn how to:\n* Approach a Kaggle competition\n* Explore the competition data and learn about the competition topic\n* Prepare data for machine learning\n* Train a model\n* Measure the accuracy of your model\n* Prepare and make your first Kaggle submission\nThis tutorial presumes you have an understanding of Python and the pandas library. If you need to learn about these, we recommend our[pandas tutorial](https://www.dataquest.io/blog/pandas-python-tutorial/)blog post.\n## The Titanic competition\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the[sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic).\nIn this[Titanic ML competition](https://www.kaggle.com/competitions/titanic), we have data about passengers onboard the Titanic, and we'll see if we can use that information to predict whether those people survived or not. Before we start looking at this specific competition, let's take a moment to understand how Kaggle competitions work.\nEach Kaggle competition has two key data files that you will work with - a**training**set and a**testing**set.\nThe training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict: in this case,`Survival`.\nThe testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.\nThis is useful because we want as much data as we can to train our model on. Once we have trained our model on the training set, we will use that model to make predictions on the data from the testing set, and submit those predictions to Kaggle.\nIn this competition, the two files are named`test.csv`and`train.csv`. We'll start by using[`pandas.read\\_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)to read both files and then inspect their size.\n```\n`import pandas as pd\ntest = pd.read\\_csv(\"test.csv\")\ntrain = pd.read\\_csv(\"train.csv\")\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))`\n```\n```\n`Dimensions of train: (891, 12)\nDimensions of test: (418, 11)`\n```\n## Exploring the data\nThe files we just opened are available on[the data page for the Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/data). That page also has a**data dictionary**, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n* `PassengerID`\u2014 A column added by Kaggle to identify each row and make submissions easier\n* `Survived`\u2014 Whether the passenger survived or not and the value we are predicting*(0=No, 1=Yes)*\n* `Pclass`\u2014 The class of the ticket the passenger purchased*(1=1st, 2=2nd, 3=3rd)*\n* `Sex`\u2014 The passenger's sex\n* `Age`\u2014 The passenger's age in years\n* `SibSp`\u2014 The number of siblings or spouses the passenger had aboard the Titanic\n* `Parch`\u2014 The number of parents or children the passenger had aboard the Titanic\n* `Ticket`\u2014 The passenger's ticket number\n* `Fare`\u2014 The fare the passenger paid\n* `Cabin`\u2014 The passenger's cabin number\n* `Embarked`\u2014 The port where the passenger embarked*(C=Cherbourg, Q=Queenstown, S=Southampton)*\nThe data page on Kaggle has some additional notes about some of the columns. It's always worth exploring this in detail to get a full understanding of the data.\nLet's take a look at the first few rows of the`train`dataframe.\n```\n`train.head()`\n```\n||PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n0|1|0|3|Braund, Mr. Owen Harris|male|22.0|1|0|A/5 21171|7.2500|NaN|S|\n1|2|1|1|Cumings, Mrs. John Bradley (Florence Briggs Th...|female|38.0|1|0|PC 17599|71.2833|C85|C|\n2|3|1|3|Heikkinen, Miss. Laina|female|26.0|0|0|STON/O2. 3101282|7.9250|NaN|S|\n3|4|1|1|Futrelle, Mrs. Jacques Heath (Lily May Peel)|female|35.0|1|0|113803|53.1000|C123|S|\n4|5|0|3|Allen, Mr. William Henry|male|35.0|0|0|373450|8.0500|NaN|S|\nThe type of machine learning we will be doing is called**classification**, because when we make predictions we are classifying each passenger as 'survived' or not. More specifically, we are performing**binary classification**, which means that there are only two different states we are classifying.\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie[Titanic](https://en.wikipedia.org/wiki/Titanic_(1997_film))would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\nThis indicates that`Age`,`Sex`, and`PClass`may be good predictors of survival. We'll start by exploring`Sex`and`Pclass`by visualizing the data.\nBecause the`Survived`column contains`0`if the passenger did not survive and`1`if they did, we can segment our data by sex and calculate the mean of this column. We can use`DataFrame.pivot\\_table()`to easily do this:\n```\n`import matplotlib.pyplot as plt\n%matplotlib inline\nsex\\_pivot = train.pivot\\_table(index=\"Sex\",values=\"Survived\")\nsex\\_pivot.plot.bar()\nplt.show()`\n```\nWe can immediately see that females survived in much higher proportions than males did. Let's do the same with the`Pclass`column.\n```\n`class\\_pivot = train.pivot\\_table(index=\"Pclass\",values=\"Survived\")\nclass\\_pivot.plot.bar()\nplt.show()`\n```\n## Exploring and converting the age column\nThe`Sex`and`PClass`columns are what we call**categorical**features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\nLet's take a look at the`Age`column using[`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html).\n```\n`train[\"Age\"].describe()`\n```\n```\n`count 714.000000\nmean 29.699118\nstd 14.526497\nmin 0.420000\n25% 20.125000\n50% 28.000000\n75% 38.000000\nmax 80.000000\nName: Age, dtype: float64`\n```\nThe`Age`column contains numbers ranging from`0.42`to`80.0`(If you look at Kaggle's data page, it informs us that`Age`is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the`train`data set had earlier which indicates we have some missing values.\nAll of this means that the`Age`column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visuall...",
      "url": "https://www.dataquest.io/blog/kaggle-fundamentals"
    },
    {
      "title": "How To Score ~80% Accuracy in Kaggle's Spaceship Titanic ...",
      "text": "<div><div><div><h2>This is a step-by-step guide to walk you through submitting a \u201c.csv\u201d file of predictions to Kaggle for the new titanic competition.</h2><div><div><a href=\"https://medium.com/@ZaynabAwofeso?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/codex?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div></div></div><figure><figcaption>image by <a href=\"http://unsplash.com\">unsplash</a></figcaption></figure><h2>Introduction</h2><p>Kaggle recently launched a fun competition called Spaceship Titanic. It is designed to be an update of the popular Titanic competition which helps people new to data science learn the basics of machine learning, get acquainted with Kaggle\u2019s platform, and meet others in the community. This article is a beginner-friendly analysis of the Spaceship Titanic Kaggle Competition. It covers steps to obtain any meaningful insights from the data and to predict the \u201cground truth\u201d for the test set with an accuracy of ~80% using RandomForestClassifier.</p><h2>Index</h2><ol><li>Problem definition and metrics</li><li>About the data</li><li>Exploratory Data Analysis</li><li>Data Cleaning and preprocessing</li><li>Feature Extraction and Feature Selection</li><li>Baseline Model Performance and Model Building</li><li>Submission and Feature Importance</li></ol><h2>1. Problem definition and metrics</h2><p>As the first thing, we have to understand the problem. It\u2019s the year 2912 and the interstellar passenger liner Spaceship Titanic has collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension! To help rescue crews retrieve the lost passengers, we are challenged to use records recovered from the spaceship\u2019s damaged computer system to predict which passengers were transported to another dimension.</p><blockquote><p>This problem is a binary class classification problem where we have to predict which passengers were transported to an alternate dimension or not, and we will be using accuracy as a metric to evaluate our results.</p></blockquote><h2>2. About the data</h2><p>We will be using 3 CSV files:</p><ul><li><strong>train file</strong> (spaceship_titanic_train.csv) \u2014 contains personal records of the passengers that would be used to build the machine learning model.</li><li><strong>test file</strong> (spaceship_titanic_test.csv) \u2014 contains personal records for the remaining one-third (~4300) of the passengers, but not the target variable (i.e. the value of Transported for the passengers). It will be used to see how well our model performs on unseen data.</li><li><strong>sample submission file</strong> (sample_submission.csv) \u2014 contains the format in which we have to submit our predictions.</li></ul><p>We will be using python for this problem. You can download the dataset from Kaggle <a href=\"https://www.kaggle.com/competitions/spaceship-titanic/data\">here</a>.</p><p><strong>Import required libraries</strong></p><figure></figure><p><strong>Reading Data</strong></p><figure></figure><p>Let\u2019s make a copy of the train and test data so that even if we make any changes to these datasets it would not affect the original datasets.</p><figure></figure><p>We will look at the structure of the train and test dataset next. We will first check the features present, then we will look at their data types.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name', 'Transported'],<br/> dtype='object')</span></pre><p>We have 13 independent variables and 1 target variable (Transported) in the training dataset. Let\u2019s also look at the columns of the test dataset.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name'],<br/> dtype='object')</span></pre><p>We have similar features in the test dataset as the training dataset except Transported that we will predict using the model built by the train data.</p><p>Given below is the description for each variable.</p><ul><li><strong>PassengerId</strong> \u2014 A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li><li><strong>HomePlanet</strong> \u2014 The planet the passenger departed from, typically their planet of permanent residence.</li><li><strong>CryoSleep </strong>\u2014 Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li><li><strong>Cabin</strong> \u2014 The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li><li><strong>Destination</strong> \u2014 The planet the passenger will be debarking to.</li><li><strong>Age</strong> \u2014 The age of the passenger.</li><li><strong>VIP </strong>\u2014 Whether the passenger has paid for special VIP service during the voyage.</li><li><strong>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</strong> \u2014 Amount the passenger has billed at each of the Spaceship Titanic\u2019s many luxury amenities.</li><li><strong>Name</strong> \u2014 The first and last names of the passenger.</li><li><strong>Transported</strong> \u2014 Whether the passenger was transported to another dimension. This is the target, the column we are trying to predict.</li></ul><p>Let\u2019s print data types for each variable of the training dataset.</p><figure></figure><pre><span>PassengerId object<br/>HomePlanet object<br/>CryoSleep object<br/>Cabin object<br/>Destination object<br/>Age float64<br/>VIP object<br/>RoomService float64<br/>FoodCourt float64<br/>ShoppingMall float64<br/>Spa float64<br/>VRDeck float64<br/>Name object<br/>Transported bool<br/>dtype: object</span></pre><p>We can see there are three formats of data types in the training dataset:</p><ul><li><strong>object</strong> (Categorical variables) \u2014 The categorical variables in the training dataset are: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP and Name</li><li><strong>float64</strong> (Float variables i.e Numerical variables which have some decimal values involved) \u2014 The Numerical variables in our train dataset: Age, RoomService, FoodCourt, ShoppingMall, Spa and VRDeck</li><li><strong>bool</strong> (Boolean variables i.e. a variable that has one of two possible values e.g. True or False) \u2014 The Boolean Variable in our dataset is Transported</li></ul><p>Let\u2019s look at the shape of our train and test dataset.</p><figure></figure><pre><span>The shape of the train dataset is: (8693, 14)<br/>The shape of the test dataset is: (4277, 13)</span></pre><p>We have 8693 rows and 14 columns in the training dataset and 4277 rows and 13 columns in the test dataset.</p><ol><li>Exploratory Data Analysis</li></ol><p><strong><em>Univariate Analysis</em></strong></p><p>Univariate analysis is the simplest form of analyzing data where we examine each data individually to understand the distribution of its values.</p><p><strong><em>Target Variable</em></strong></p><p>We will first look at the target variable i.e. Transported. Since it is a categorical variable, let us look at its percentage distribution and bar plot.</p><figure></figure><pre><span>True 0.503624<br/>False 0.496376<br/>Name: Transported, dtype: float64</span></pre><figure></figure><figure></figure><p>Out of 8693 passengers in the train dataset, 4378 (about 50%) were Transported to another dimension.</p><p>Let\u2019s visualize the Independent categorical features next.</p><p><strong><em>...",
      "url": "https://medium.com/codex/how-to-score-80-accuracy-in-kaggles-spaceship-titanic-competition-using-random-forest-classifier-e7d06ce25bad"
    },
    {
      "title": "Kaggle Titanic submission score is higher than local accuracy score",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Kaggle Titanic submission score is higher than local accuracy score](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 6 months ago\n\nModified [1 month ago](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score?lastactivity)\n\nViewed\n506 times\n\n3\n\n$\\\\begingroup$\n\nThis is the starter challenge, Titanic. The original question I posted on Kaggle is [here](https://www.kaggle.com/c/titanic/discussion/122512). However, nobody really gives any insightful advice so I am turning to the powerful Stackoverflow community.\n\nBased on this [Notebook](https://www.kaggle.com/tarunpaparaju/how-top-lb-got-their-score-use-titanic-to-learn), we can download the ground truth for this challenge and get a perfect score.\n\nI tested it and it does give me 100% on LB for the purpose of confirming it is the ground truth as it claims. ( **side question here**: how do I remove this perfect submission because now it shows I have 100% on this challenge but I want to show my real score, which is roughly 80% and I will keep improving)\n\n_Sometimes submission on Kaggle takes several minutes to get back the score so I used the ground truth locally to test my different models to save time._ However, they always give me different results. See the following:\n\n[![enter image description here](https://i.sstatic.net/hOytL.png)](https://i.sstatic.net/hOytL.png)\n\nThese are the code I use, **what's wrong? You can use my code to try your submission and do you also have the same problem?**\n\n```\ndef mark(pred):\n    solution = os.path.join(dirname, './output/solution.csv')\n    submission = os.path.join(dirname, './output/'+pred)\n    solution = pd.read_csv(solution)\n    submission = pd.read_csv(submission)\n\n    solution.columns = ['PassengerId', 'Sol']\n    submission.columns = ['PassengerId', 'Pred']\n\n    df = pd.concat([solution[['Sol']], submission[['Pred']]], axis=1)\n    num_row = df.shape[0]\n    print(pred[:-4], '==', (df[(df['Sol'] == df['Pred'])]).shape[0] / num_row)\n\nif __name__== \"__main__\":\n    mark('achieve_99_dtree_rfe.csv')\n    mark('advanced_feature_with_stacking_5_fold.csv')\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/65196)\n\n[Improve this question](https://datascience.stackexchange.com/posts/65196/edit)\n\nFollow\n\nasked Dec 20, 2019 at 17:08\n\n[![Kenny's user avatar](https://www.gravatar.com/avatar/4af6be0843af8500c96d36e355deeff5?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/83148/kenny)\n\n[Kenny](https://datascience.stackexchange.com/users/83148/kenny) Kenny\n\n3111 bronze badge\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nFirst question: on certain competitions on kaggle you can select your submission when you go to the submissions window. There you may not be able to on titanic one so you are stuck with 100 percent.\n\nRegarding your second question and why do you have different results locally, there is a couple of explanations, first its randomization did you set all the seeds, second all of the modules/library versions you have locally sam as the ones on kaggle? Third different hardware could also be the reason\n\n[Share](https://datascience.stackexchange.com/a/65203)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/65203/edit)\n\nFollow\n\nanswered Dec 20, 2019 at 21:18\n\nuser87235user87235\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Thanks for replying. As I understand, I only submit my predication.csv, they count how many predications do I get right for same PassengerId, for example, if PassengerId 1000 is 1 but my predication is 0, then this will not be counted towards my final score, they don't see my model/algorithm, so how is that related to seeds/library/hardware?$\\\\endgroup$\n\n\u2013\u00a0[Kenny](https://datascience.stackexchange.com/users/83148/kenny)\n\nCommentedDec 21, 2019 at 4:14\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f65196%2fkaggle-titanic-submission-score-is-higher-than-local-accuracy-score%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[1](https://datascience.stackexchange.com/q/5119) [Kaggle Titanic Survival Table an example of Naive Bayes?](https://datascience.stackexchange.com/questions/5119/kaggle-titanic-survival-table-an-example-of-naive-bayes)\n\n[5](https://datascience.stackexchange.com/q/10307) [2 stage ensemble -- CV MSE valid in 1st stage but not in 2nd](https://datascience.stackexchange.com/questions/10307/2-stage-ensemble-cv-mse-valid-in-1st-stage-but-not-in-2nd)\n\n[4](https://datascience.stackexchange.com/q/13104) [How to further improve the kaggle titanic submission accuracy?](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\n\n[2](https://datascience.stackexchange.com/q/45351) [kaggle Titanic what is GP?](https://datascience.stackexchange.com/questions/45351/kaggle-titanic-what-is-gp)\n\n[7](https://datascience.stackexchange.com/q/58422) [Why do we use the F1 score instead of mutual information?](https://datascience.stackexchange.com/questions/58422/why-do-we-use-the-f1-score-instead-of-mutual-information)\n\n[2](https://datascience.stackexchange.com/q/65239) [Importing .ipnyb file from Kaggle into local Jupyter](https://datascience.stackexchange.com/questions/65239/importing-ipnyb-file-from-kaggle-into-local-jupyter)\n\n[0](https://datascience.stackexchange.com/q/77028) [Dropping attributes leads to better classifier accuracy? (Titanic Set)](https://datascience.stackexchange.com/questions/77028/dropping-attributes-leads-to-better-classifier-accuracy-titanic-set)\n\n[1](https://datascience.stackexchange.com/q/104097) [Difference between model score on test part and Kaggle public score](https://datascience.stackexchange.com/questions/104097/difference-between-model-score-on-test-part-and-kaggle-public-score)\n\n[1](https://datascience.stackexchange.co...",
      "url": "https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score"
    },
    {
      "title": "Titanic Data Set \u2013 How I increased my score from 79% to 82%",
      "text": "<div><div>\n\t \n\t<p>The Titanic data set is said to be the starter for every aspiring data scientist. So it was that I sat down two years ago, after having taken an econometrics course in a university which introduced me to R, thinking to give the competition a shot. My goal was to achieve an accuracy of 80% or higher.</p>\n<p>However, my first try in this competition ended up with me producing some summary statistics and trying to solve the challenge with a linear regression model :). In short, I misinterpreted what the data science community meant by: \u201c<strong>a competition for beginners</strong>\u201d.</p>\n<p>Now, two years later, I again tried my luck. Knowing about terms like training and testing data sets, overfitting, cross-validation, bias-variance trade-off, regular expressions, and about different classification models makes me hopefully better prepared this time. After almost having completed a Statistics degree, countless hours on <a href=\"https://www.coursera.org/\">Coursera</a>, <a href=\"https://www.datacamp.com/\">Data Camp</a> and <a href=\"https://stackoverflow.com/\">Stackoverflow</a>, and after having a data science internship under my belt I finally declared myself a \u201cbeginner\u201d in the data science community and ready for the <a href=\"https://www.kaggle.com/c/titanic\">Titanic Kaggle Competition</a>.</p>\n<p>Long story short, don\u2019t be discouraged if you hear the words \u201cbeginner competition\u201d, but are not yet able to understand everything or produce the results other people showcase in their kernels.</p>\n<p>This blog post is an attempt to make the titanic data set easier to understand. Throughout it, I will be explaining my code.</p>\n<p>What we will be covering in this blog post:</p>\n<ul>\n<li>An Exploratory Data Analysis with <code>ggplot2</code>\u00a0and <code>dplyr</code>.</li>\n<li>Feature Engineering for some Variables.</li>\n<li>Dealing with Missing Values.</li>\n<li>Model Building and Model Evaluation:\n<ol>\n<li>Logistic Regression</li>\n<li>Random Forest</li>\n<li>Linear Discriminant Analysis</li>\n<li>K-Nearest\u00a0Neighbors</li>\n</ol>\n</li>\n</ul>\n<p>In the end, I am ending up with a score of around <strong>79%</strong>. Pretty disappointing for me and I didn\u2019t achieve my initial goal of 80%. This is why in the second part of this tutorial, I evaluated the so-called <strong>gender model</strong> with which I achieved a score of <strong>81.82%</strong>.</p>\n<p>Let\u2019s get started!</p>\n<p>First, we are loading the libraries we need. The <code>tidyverse</code> consists of various packages (<code>dplyr</code>, <code>ggplot</code>, etc.) and is perfect for data manipulations. The <code>ggbubr</code> package is nice for visualizations and gives us some extra flexibility. The arsenal package is for easily creating some nice looking tables. The other packages are for building predictive models.</p>\n<p>After having loaded the packages, we are loading the data sets. In order to <code>rbind()</code>\u00a0them, the train and test data sets have to have equivalent columns. That is why I am creating the <code>Survived</code> column in the test data set. After that, I am transforming some variables with <code>mutate()</code>\u00a0to characters and factors.</p>\n<pre>library(tidyverse) # data manipulation\nlibrary(ggpubr) # data visualization\nlibrary(arsenal) # table creation\nlibrary(randomForest) # model buliding\nlibrary(caret) # model building\nlibrary(class) # model building\nlibrary(MASS) # model building\n#Loading and combining the data sets\ntrain &lt;- read.csv(\"train.csv\")\ntest &lt;- read.csv(\"test.csv\")\ntest$Survived &lt;- NA\ntitanic &lt;- rbind(train, test) # combining data sets\n# data manipulation\ntitanic %&gt;%\n dplyr::mutate(\n Name = as.character(Name),\n Ticket = as.character(Ticket),\n Cabin = as.character(Cabin),\n Survived = as.factor(Survived),\n Pclass = as.factor(Pclass)\n ) -&gt; titanic</pre>\n<h2>Exploratory Data Analysis of the Titanic Data Set</h2>\n<h3>Investigating Gender for the Titanic Data Set</h3>\n<p>For all my plots, I am using <code>ggplot2</code>. If you are unfamiliar with the syntax, the <a href=\"http://r4ds.had.co.nz/\">R for Data Science</a> book, <a href=\"https://www.datacamp.com/\">Data Camp</a>, and the <a href=\"https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\">ggplot cheat sheet</a> are great resources that you can refer to.</p>\n<pre>plot_count &lt;- ggplot(titanic[1:891, ], aes(x = Sex, fill = Survived)) +\n geom_bar() +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n ggtitle(\"Most of the Titanic Passengers are Male.\\n \nMost Passengers Who Survived Were Female\") +\n theme(\n plot.title = element_text(hjust = 0.5),\n legend.position = \"bottom\"\n )\nplot_percent &lt;- ggplot(titanic[1:891, ], aes(x = Sex, fill = Survived)) +\n geom_bar(position = \"fill\") +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n ggtitle(\"75% of all Female Passengers Survived whereas only \\n around 20% of the Male Passengers Survived\") +\n theme(\n plot.title = element_text(hjust = 0.5),\n legend.position = \"bottom\"\n )\nggarrange(plot_count, plot_percent)</pre>\n<p><a href=\"https://i0.wp.com/thatdatatho.com/wp-content/uploads/2018/09/gender-1.jpeg?ssl=1\"></a></p>\n<p>It looks like most of the female titanic passengers survived and most of the male passengers died. This becomes especially visible when looking at the percentages on the right plot. 75% of all female passengers survived whereas less than 25% of male passengers survived.</p>\n<p>This is a very crucial finding and is key for the 81.82% success of the <strong>gender model</strong> I am discussing in <strong>part 2</strong>.</p>\n<h3>Investigating Gender and Class for the Titanic Data Set</h3>\n<pre>plot_count &lt;- ggplot(titanic[1:891, ], aes(x = Sex, fill = Survived)) +\n geom_bar() +\n facet_wrap(~Pclass) +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n theme(legend.position = \"bottom\")\nplot_percent &lt;- ggplot(titanic[1:891, ], aes(x = Sex, fill = Survived)) +\n geom_bar(position = \"fill\") +\n facet_wrap(~Pclass) +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n theme(legend.position = \"bottom\") +\n ylab(\"%\")\ncombined_figure &lt;- ggarrange(plot_count, plot_percent)\nannotate_figure(combined_figure,\n top = text_grob(\"Almost All Female Passengers Who are Class One and Two Survived. The Big Proportion of Men not Surviving \\n Mainly Comes From Male Class 3 Passengers\",\n color = \"black\",\n face = \"bold\",\n size = 14\n )\n)</pre>\n<p><a href=\"https://i0.wp.com/thatdatatho.com/wp-content/uploads/2018/09/gender_class_header.jpeg?ssl=1\"></a></p>\n<p>Almost all female passengers in classes 1 and 2 survived whereas, for males, the passenger class is not a great predictor for survival. This is because regardless of class, male passengers do not really seem to benefit much from being in higher classes.</p>\n<h3>Investigating Age, Fare, and Embarked for the Titanic Data Set</h3>\n<pre>plot_age &lt;- ggplot(titanic[1:891, ], aes(x = Age, fill = Survived)) +\n geom_histogram() +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n theme(legend.position = \"bottom\")\nplot_fare &lt;- ggplot(titanic[1:891, ], aes(x = Fare, fill = Survived)) +\n geom_histogram() +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n theme(legend.position = \"bottom\")\nplot_embarked &lt;- ggplot(titanic[1:891, ], aes(x = Embarked, fill = Survived)) +\n geom_bar() +\n scale_fill_manual(\n name = \"Survived\",\n values = c(\"red\", \"blue\"),\n labels = c(\"No\", \"Yes\"),\n breaks = c(\"0\", \"1\")\n ) +\n theme(legend.position = \"bottom\")\nggarrange(plot_age, \n plot_fare, \n plot_embarked, common.legend = TRUE, ncol = 3)</pre>\n<p><a href=\"https://i0.wp.com/that...",
      "url": "https://thatdatatho.com/titanic-data-set-increased-prediction-scores-82"
    },
    {
      "title": "Titanic:Deep Learning Model With 80% accuracy | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/saife245/titanic-deep-learning-model-with-80-accuracy"
    },
    {
      "title": "Titanic dataset Analysis (~80% accuracy) | by Aishani Basu - Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9480cf3db538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ftitanic-dataset-analysis-80-accuracy-9480cf3db538&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-9480cf3db538---------------------------------------)\n\n\u00b7\n\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\n\n# Titanic dataset Analysis (~80% accuracy)\n\n[Aishani Basu](https://medium.com/@aishani-81621?source=post_page---byline--9480cf3db538---------------------------------------)\n\n11 min read\n\n\u00b7\n\nNov 25, 2021\n\n--\n\n2\n\nListen\n\nShare\n\n[Source](https://ornjhonnywingsfood.blogspot.com/2021/08/titanic-rms-titanic-wikiwand-murdoch.html)\n\n## Introduction\n\nThe Titanic or, in full, [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) was part of the one of the most iconic tragedies of all time. RMS Titanic was a British passenger ship that hit an iceberg while on its voyage from Southampton to New York City, and sank in the North Atlantic ocean, leaving hundreds of passengers to die in the aftermath of the deadly incident. Some of the passengers who survived till help arrived were rescued while many lost their lives helplessly waiting for help to arrive.\n\nThe legendary Kaggle problem, [Titanic](https://www.kaggle.com/c/titanic), based on the tragedic sinking of the RMS Titanic records data about 891 passengers of RMS Titanic, and we are required to predict if they have survived or not, based on the information we have available about the passengers and the outcome after the sinking of the ship.\n\nNote: This notebook is my analysis of the titanic dataset to obtain any meaningful insights from the data and scores an accuracy of ~80 percent (top 5 percent of 14k entries on Kaggle) .\n\nLet\u2019s get started!\n\n## Contents:\n\n1\\. About the data\n\n2\\. Problem definition and metrics\n\n3\\. EDA\n\n4\\. Baseline model performance\n\n5\\. Model Selection\n\n6\\. Results and conclusion\n\n## About the data:\n\nFirst, we have the prediction variable, that is if they survived the sinking or not. Then a bunch of numeric variables, like ids, ages of the passengers, etc. followed by categorical variables like class of the ticket, and strings like name, etc.\n\nPress enter or click to view image in full size\n\nExample RMS Titanic ticket : [Source](https://www.encyclopedia-titanica.org/titanic-tickets/)\n\nPrinting first 5 rows of given dataset..\n\n## Problem definition and metrics\n\nThe problem is a **binary class classification** problem. We can use **binary crossentropy** or logistic loss as the loss function and any metric like **accuracy** or/and **ROC AUC score** as a metric to evaluate the results.\n\n## EDA\n\n### Correlation between features:\n\nDrawing insights from the correlation between features..\n\n- Fare and Pclass are negatively correlated (with a Pearson\u2019s correlation coefficient of -0.55) ; Obviously, **higher fares imply better better ticket classes** (lower the class number) and vice versa.\n- Pclass and the target feature are moderately negatively correlated (-0.34) implying **better the ticket class, higher the chance of survival.**\n- A similar pattern can be observed with the features Parch, SibSp and Age; Both the features, Parch and SibSp are very slightly correlated with all the features except Age feature, and both have a negative correlation (-0.19 and -0.31 respectively) with Age ; **Lower the age, more the number of family accompanying the passenger**\n- SibSp and Parch features are positively correlated which are both indicative of the number of family members accompanying passenger\n- PClass and Age are negatively correlated (-0.37) implying **higher the age, better the ticket class**\n\n### Missing data\n\n**Age** : Contains 177 nan values out of 891 entries. Imputed with median gave best results.\n\n**Embarked** : Contains 2 nan values. Imputed with mode of existing data.\n\n**Cabin** : 687 out of 891 Cabin entries are nans, i.e. more than 50 percent of the total data exists as missing data or Nans so assumed its better to derive other features from this feature.\n\nNow, printing the correlation heatmap after handling missing data and converting categorical strings to encodings (0,1,2..)..\n\n- Embarked also has slight correlation with the target variable (-0.17), the **port at which the ship was boarded by the passengers did determine their chance of survival**\n- Sex is also highly correlated with the target variable (-0.54), indicating the **passenger\u2019s gender had a high effect on their chance of survival**\n- Embarked and Fare of the passengers are negatively correlated (-0.22); obviously, fare depends on the port a passenger boards the ship from.\n- Embarked and Sex also seem slightly correlated (0.11) indicating the **port the passenger boarded from was dependent on the gender of the passenger**; Embarked and Pclass are also correlated (0.16) indicating a **1st class passenger probably boarded at a different port than a 3rd class passenger**.\n- Sex and Fare, Sex and Parch, SibSp, Sex and Pclass all seem slightly correlated (-0.18, -0.25, -0.11, 0.13 respectively); i.e the **fare the passenger paid, the family aboard, and the class of the ticket was seem slightly dependent on the gender of the passenger**.\n\n**Question 1: How do the features \u2018Age\u2019, \u2018Sex\u2019, \u2018Fare\u2019, \u2018Embarked\u2019 affect the chance of a passenger\u2019s survival?**\n\nAge :\n\nPress enter or click to view image in full size\n\n- Both distributions, of people who didn\u2019t survive and the ones who did, are normal with a spike around age 30 for passengers who did not survive i.e. for **people around the age 30 had a higher chance of not surviving**\n- People of ages > ~51 are considered outliers in the distribution of people who did not survive. owing to the fact **people above the age of ~51 are very less** and people of age ~55 above are outliers in the distribution of people who did survive, i.e. **very few people above ~55 actually did survive**\n\nPress enter or click to view image in full size\n\nSex:\n\nPress enter or click to view image in full size\n\n- From the above stacked plots, it is clear **more percentage of female passengers survived** i.e. 74.2 % while only 18.9 % of male passengers survived, even though more male passesngers were aboard\n\nFare :\n\nPress enter or click to view image in full size\n\n- Distribution of fares of survivors clearly has a higher median value, hence **passengers who survived had higher average fares**\n- Lots of outliers in the distribution of both\n- A huge spike in the distribution of the people who didn\u2019t survive, or the probability that the person who didn\u2019t survive had a fare of ~10 units of currency is very high\n\nEmbarked:\n\nPress enter or click to view image in full size\n\n- Maximum passengers seem to have boarded the ship at Southampton port and only 33 percent of them survived; out of the total passengers who boarded the ship at Queenstown, a simi...",
      "url": "https://medium.com/analytics-vidhya/titanic-dataset-analysis-80-accuracy-9480cf3db538"
    }
  ]
}