{
  "query": "What is the correct Titanic Kaggle test set ground truth with 100% accuracy submission?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The only way to achieve a 100\u202f%\u2011accuracy Titanic submission is to download the hidden \u201csolution.csv\u201d (the true test\u2011set labels) provided in the Kaggle notebook linked in the DataScience\u202fStackExchange answer, which supplies the ground\u2011truth file for a perfect score ([datascience.stackexchange.com](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)).",
      "url": ""
    },
    {
      "title": "Kaggle Titanic submission score is higher than local accuracy score",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Kaggle Titanic submission score is higher than local accuracy score](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 6 months ago\n\nModified [1 month ago](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score?lastactivity)\n\nViewed\n506 times\n\n3\n\n$\\\\begingroup$\n\nThis is the starter challenge, Titanic. The original question I posted on Kaggle is [here](https://www.kaggle.com/c/titanic/discussion/122512). However, nobody really gives any insightful advice so I am turning to the powerful Stackoverflow community.\n\nBased on this [Notebook](https://www.kaggle.com/tarunpaparaju/how-top-lb-got-their-score-use-titanic-to-learn), we can download the ground truth for this challenge and get a perfect score.\n\nI tested it and it does give me 100% on LB for the purpose of confirming it is the ground truth as it claims. ( **side question here**: how do I remove this perfect submission because now it shows I have 100% on this challenge but I want to show my real score, which is roughly 80% and I will keep improving)\n\n_Sometimes submission on Kaggle takes several minutes to get back the score so I used the ground truth locally to test my different models to save time._ However, they always give me different results. See the following:\n\n[![enter image description here](https://i.sstatic.net/hOytL.png)](https://i.sstatic.net/hOytL.png)\n\nThese are the code I use, **what's wrong? You can use my code to try your submission and do you also have the same problem?**\n\n```\ndef mark(pred):\n    solution = os.path.join(dirname, './output/solution.csv')\n    submission = os.path.join(dirname, './output/'+pred)\n    solution = pd.read_csv(solution)\n    submission = pd.read_csv(submission)\n\n    solution.columns = ['PassengerId', 'Sol']\n    submission.columns = ['PassengerId', 'Pred']\n\n    df = pd.concat([solution[['Sol']], submission[['Pred']]], axis=1)\n    num_row = df.shape[0]\n    print(pred[:-4], '==', (df[(df['Sol'] == df['Pred'])]).shape[0] / num_row)\n\nif __name__== \"__main__\":\n    mark('achieve_99_dtree_rfe.csv')\n    mark('advanced_feature_with_stacking_5_fold.csv')\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/65196)\n\n[Improve this question](https://datascience.stackexchange.com/posts/65196/edit)\n\nFollow\n\nasked Dec 20, 2019 at 17:08\n\n[![Kenny's user avatar](https://www.gravatar.com/avatar/4af6be0843af8500c96d36e355deeff5?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/83148/kenny)\n\n[Kenny](https://datascience.stackexchange.com/users/83148/kenny) Kenny\n\n3111 bronze badge\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nFirst question: on certain competitions on kaggle you can select your submission when you go to the submissions window. There you may not be able to on titanic one so you are stuck with 100 percent.\n\nRegarding your second question and why do you have different results locally, there is a couple of explanations, first its randomization did you set all the seeds, second all of the modules/library versions you have locally sam as the ones on kaggle? Third different hardware could also be the reason\n\n[Share](https://datascience.stackexchange.com/a/65203)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/65203/edit)\n\nFollow\n\nanswered Dec 20, 2019 at 21:18\n\nuser87235user87235\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Thanks for replying. As I understand, I only submit my predication.csv, they count how many predications do I get right for same PassengerId, for example, if PassengerId 1000 is 1 but my predication is 0, then this will not be counted towards my final score, they don't see my model/algorithm, so how is that related to seeds/library/hardware?$\\\\endgroup$\n\n\u2013\u00a0[Kenny](https://datascience.stackexchange.com/users/83148/kenny)\n\nCommentedDec 21, 2019 at 4:14\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f65196%2fkaggle-titanic-submission-score-is-higher-than-local-accuracy-score%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[1](https://datascience.stackexchange.com/q/5119) [Kaggle Titanic Survival Table an example of Naive Bayes?](https://datascience.stackexchange.com/questions/5119/kaggle-titanic-survival-table-an-example-of-naive-bayes)\n\n[5](https://datascience.stackexchange.com/q/10307) [2 stage ensemble -- CV MSE valid in 1st stage but not in 2nd](https://datascience.stackexchange.com/questions/10307/2-stage-ensemble-cv-mse-valid-in-1st-stage-but-not-in-2nd)\n\n[4](https://datascience.stackexchange.com/q/13104) [How to further improve the kaggle titanic submission accuracy?](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\n\n[2](https://datascience.stackexchange.com/q/45351) [kaggle Titanic what is GP?](https://datascience.stackexchange.com/questions/45351/kaggle-titanic-what-is-gp)\n\n[7](https://datascience.stackexchange.com/q/58422) [Why do we use the F1 score instead of mutual information?](https://datascience.stackexchange.com/questions/58422/why-do-we-use-the-f1-score-instead-of-mutual-information)\n\n[2](https://datascience.stackexchange.com/q/65239) [Importing .ipnyb file from Kaggle into local Jupyter](https://datascience.stackexchange.com/questions/65239/importing-ipnyb-file-from-kaggle-into-local-jupyter)\n\n[0](https://datascience.stackexchange.com/q/77028) [Dropping attributes leads to better classifier accuracy? (Titanic Set)](https://datascience.stackexchange.com/questions/77028/dropping-attributes-leads-to-better-classifier-accuracy-titanic-set)\n\n[1](https://datascience.stackexchange.com/q/104097) [Difference between model score on test part and Kaggle public score](https://datascience.stackexchange.com/questions/104097/difference-between-model-score-on-test-part-and-kaggle-public-score)\n\n[1](https://datascience.stackexchange.co...",
      "url": "https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score"
    },
    {
      "title": "Titanic - Machine Learning from Disaster | Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.",
      "url": "https://www.kaggle.com/c/titanic"
    },
    {
      "title": "Unveiling Hidden Insights: A Deep Dive into the Titanic Dataset",
      "text": "[Skip to content](https://talent500.com/talent500.com#content)\n\n## Uncovering the Secrets of the Titanic: Data-Driven Insights and Surprising Discoveries\n\n- [Prachi Kothiyal](https://talent500.com/blog/author/prachi/)\n- [October 29, 2024](https://talent500.com/blog/2024/10/29/)\n\n#### Jump to\n\nThe Titanic dataset, a cornerstone in the world of data science, continues to captivate researchers and aspiring analysts alike. Despite its seemingly simple structure, this dataset harbors a wealth of intriguing insights that challenge conventional wisdom and reveal unexpected patterns in survival rates among passengers.\n\n## Surprising Survival Rates\n\nOne of the most startling revelations from the dataset is the counterintuitive survival rates among different passenger classes. Contrary to popular belief, adult male passengers in Third Class had double the chance of survival compared to their Second Class counterparts. This finding challenges the narrative often portrayed in Hollywood films and raises questions about the factors influencing survival during the disaster.\n\n## The Name Game\n\nIntriguingly, the length of a passenger\u2019s name appears to correlate with their chances of survival. Passengers with longer names exhibited significantly higher survival rates compared to those with shorter names. While this correlation may seem arbitrary, it potentially reveals underlying socio-economic factors that influenced passenger demographics and, consequently, survival rates.\n\n## Acts of Altruism\n\nThe Titanic dataset provides evidence of multiple instances of altruism during the tragedy. One notable observation is the higher survival rate of younger First Class passengers compared to their older counterparts, suggesting a possible act of self-sacrifice by older passengers. Additionally, the data reveals that a significant portion of Second Class male passengers may have voluntarily given up their chances of survival to ensure the safety of women and children from all classes.\n\n## Gender Disparities in Ticket Pricing\n\nAn unexpected finding emerges when analyzing ticket prices across genders. On average, women\u2019s tickets were priced higher than men\u2019s, with the disparity most pronounced in First Class (20% higher), followed by Second Class (8% higher), and Third Class (4% higher). While the reasons for this pricing difference remain speculative, it adds an intriguing dimension to the analysis of passenger demographics.\n\n## Debunking Group Survival Myths\n\nA popular notion suggests that passengers traveling in groups of 2-4 had better survival chances, while those in larger groups or traveling solo faced higher risks. However, deeper analysis reveals this to be a case of correlation rather than causation. The apparent relationship between group size and survival rates is more closely tied to passenger class than to the size of the traveling party itself.\n\n## Solo Female Travelers: An Unexpected Advantage\n\nContrary to the general belief that solo travelers faced higher mortality rates, the data shows that solo female passengers, particularly in Third Class, had significantly better survival rates compared to women traveling in groups or with families. This surprising trend may be attributed to the selfless actions of women who chose to remain with their husbands and male children, sacrificing their own chances of survival.\n\n## The Importance of Names and Tickets\n\nAnalysis of the Name and Ticket columns provides valuable insights into passenger groupings and relationships. By combining this information, researchers can more accurately predict survival rates within groups, taking into account factors such as sex and age. This approach offers a more nuanced understanding of survival patterns beyond the broader categories of class and gender.\n\n## Addressing Missing Age Data\n\nThe Titanic dataset presents challenges with missing age information for many passengers. While various methods exist for imputing these values, from simple averages to more sophisticated machine learning models, the most critical factor is determining whether a passenger was a child, adult, or senior citizen. This categorization plays a crucial role in predicting survival chances.\n\nA novel approach to identifying female children among passengers with missing age data involves examining the \u201cParch\u201d (Parents/Children) flag. Passengers with the title \u201cMiss\u201d and a Parch value greater than zero are likely to be female children, allowing for more accurate age imputation and survival prediction.\n\n## Conclusion\n\nThe Titanic dataset, despite its age, continues to offer new insights and challenges to data scientists. From uncovering hidden acts of altruism to debunking long-held myths about survival rates, the dataset serves as a testament to the power of thorough data analysis. As technology and analytical techniques advance, there remains potential for further discoveries and improved predictive models based on this iconic dataset.\n\nThe enduring fascination with the Titanic dataset underscores its value as a training ground for aspiring data scientists. It provides a rich playground for exploring various aspects of data science, from exploratory data analysis and visualization to feature engineering and machine learning model development. As new generations of analysts approach this dataset with fresh perspectives and advanced tools, the potential for pushing the boundaries of predictive accuracy remains high.\n\nRead more such articles from our newsletter\u00a0[here](https://talent500.com/blog/category/newsletters/).\n\n#### Prachi Kothiyal\n\n[PrevVodafone Enhances Customer Experience with Azure AI](https://talent500.com/blog/vodafone-azure-ai-customer-service-innovation/)\n\n[NextDevOps at Netflix: Embracing Chaos for Unparalleled ReliabilityNext](https://talent500.com/blog/netflix-devops-chaos-engineering-reliability/)\n\n### Leave a Comment [Cancel Reply](https://talent500.com/blog/titanic-dataset-analysis-hidden-insights/\\#respond)\n\nYour email address will not be published.Required fields are marked \\*\n\nType here..\n\nName\\*\n\nEmail\\*\n\nWebsite\n\nSave my name, email, and website in this browser for the next time I comment.\n\n## You may also like\n\n## [AI Revolution October 2025: How AI Agents, Security, and Investment Trends Are Reshaping Software Development](https://talent500.com/blog/ai-revolution-october-2025-software-development-trends/)\n\nPrachi Kothiyal October 10, 2025 9:50 am No Comments\n\nOctober 2025 has marked a pivotal moment in the evolution of software development. Artificial intelligence has become the foundation of how teams build, secure, and scale digital solutions. New releases\n\n## [How Walmart Is Combining Human Developers and AI Agents to Shape the Future of Work](https://talent500.com/blog/walmart-ai-developers-automation-future-work/)\n\nPrachi Kothiyal October 10, 2025 9:39 am No Comments\n\nWalmart is accelerating its transformation into an AI-powered enterprise by blending human expertise with artificial intelligence. The retail titan is expanding its recruitment of software engineers while simultaneously developing an\n\n## [ChatGPT\u2019s Next Evolution: OpenAI\u2019s Vision for an AI Operating System](https://talent500.com/blog/openai-chatgpt-ai-operating-system/)\n\nPrachi Kothiyal October 10, 2025 9:11 am No Comments\n\nOpenAI is taking a bold step forward in redefining the role of artificial intelligence in everyday computing. At its recent Developer Day event, the company unveiled its latest vision\u2014to transform\n\n##### Categories\n\nCategoriesSelect CategoryAgile & Project ManagementAIAnalyticsAutomation TestingBackend\u00a0\u00a0\u00a0.Net\u00a0\u00a0\u00a0java\u00a0\u00a0\u00a0PythoncareerCloud Computingcyber securityData Analytics\u00a0\u00a0\u00a0data sciencedata architecturedata engineeringEnterprise\u00a0\u00a0\u00a0Diversity & inclusionFrontend\u00a0\u00a0\u00a0Angular\u00a0\u00a0\u00a0JavaScript\u00a0\u00a0\u00a0React\u00a0\u00a0\u00a0VueFullstack\u00a0\u00a0\u00a0Java + React\u00a0\u00a0\u00a0Node + Reactgeneral techHuman Resources ManagementInformation SecurityIT Infra and NetworkingJavascriptJob Interview GuidesJob SearchManual TestingMobile\u00a0\u00a0\u00a0android\u00a0\u00a0\u00a0iOSNewslettersProd...",
      "url": "https://talent500.com/blog/titanic-dataset-analysis-hidden-insights"
    },
    {
      "title": "How To Score ~80% Accuracy in Kaggle's Spaceship Titanic ...",
      "text": "<div><div><div><h2>This is a step-by-step guide to walk you through submitting a \u201c.csv\u201d file of predictions to Kaggle for the new titanic competition.</h2><div><div><a href=\"https://medium.com/@ZaynabAwofeso?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/codex?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div></div></div><figure><figcaption>image by <a href=\"http://unsplash.com\">unsplash</a></figcaption></figure><h2>Introduction</h2><p>Kaggle recently launched a fun competition called Spaceship Titanic. It is designed to be an update of the popular Titanic competition which helps people new to data science learn the basics of machine learning, get acquainted with Kaggle\u2019s platform, and meet others in the community. This article is a beginner-friendly analysis of the Spaceship Titanic Kaggle Competition. It covers steps to obtain any meaningful insights from the data and to predict the \u201cground truth\u201d for the test set with an accuracy of ~80% using RandomForestClassifier.</p><h2>Index</h2><ol><li>Problem definition and metrics</li><li>About the data</li><li>Exploratory Data Analysis</li><li>Data Cleaning and preprocessing</li><li>Feature Extraction and Feature Selection</li><li>Baseline Model Performance and Model Building</li><li>Submission and Feature Importance</li></ol><h2>1. Problem definition and metrics</h2><p>As the first thing, we have to understand the problem. It\u2019s the year 2912 and the interstellar passenger liner Spaceship Titanic has collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension! To help rescue crews retrieve the lost passengers, we are challenged to use records recovered from the spaceship\u2019s damaged computer system to predict which passengers were transported to another dimension.</p><blockquote><p>This problem is a binary class classification problem where we have to predict which passengers were transported to an alternate dimension or not, and we will be using accuracy as a metric to evaluate our results.</p></blockquote><h2>2. About the data</h2><p>We will be using 3 CSV files:</p><ul><li><strong>train file</strong> (spaceship_titanic_train.csv) \u2014 contains personal records of the passengers that would be used to build the machine learning model.</li><li><strong>test file</strong> (spaceship_titanic_test.csv) \u2014 contains personal records for the remaining one-third (~4300) of the passengers, but not the target variable (i.e. the value of Transported for the passengers). It will be used to see how well our model performs on unseen data.</li><li><strong>sample submission file</strong> (sample_submission.csv) \u2014 contains the format in which we have to submit our predictions.</li></ul><p>We will be using python for this problem. You can download the dataset from Kaggle <a href=\"https://www.kaggle.com/competitions/spaceship-titanic/data\">here</a>.</p><p><strong>Import required libraries</strong></p><figure></figure><p><strong>Reading Data</strong></p><figure></figure><p>Let\u2019s make a copy of the train and test data so that even if we make any changes to these datasets it would not affect the original datasets.</p><figure></figure><p>We will look at the structure of the train and test dataset next. We will first check the features present, then we will look at their data types.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name', 'Transported'],<br/> dtype='object')</span></pre><p>We have 13 independent variables and 1 target variable (Transported) in the training dataset. Let\u2019s also look at the columns of the test dataset.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name'],<br/> dtype='object')</span></pre><p>We have similar features in the test dataset as the training dataset except Transported that we will predict using the model built by the train data.</p><p>Given below is the description for each variable.</p><ul><li><strong>PassengerId</strong> \u2014 A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li><li><strong>HomePlanet</strong> \u2014 The planet the passenger departed from, typically their planet of permanent residence.</li><li><strong>CryoSleep </strong>\u2014 Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li><li><strong>Cabin</strong> \u2014 The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li><li><strong>Destination</strong> \u2014 The planet the passenger will be debarking to.</li><li><strong>Age</strong> \u2014 The age of the passenger.</li><li><strong>VIP </strong>\u2014 Whether the passenger has paid for special VIP service during the voyage.</li><li><strong>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</strong> \u2014 Amount the passenger has billed at each of the Spaceship Titanic\u2019s many luxury amenities.</li><li><strong>Name</strong> \u2014 The first and last names of the passenger.</li><li><strong>Transported</strong> \u2014 Whether the passenger was transported to another dimension. This is the target, the column we are trying to predict.</li></ul><p>Let\u2019s print data types for each variable of the training dataset.</p><figure></figure><pre><span>PassengerId object<br/>HomePlanet object<br/>CryoSleep object<br/>Cabin object<br/>Destination object<br/>Age float64<br/>VIP object<br/>RoomService float64<br/>FoodCourt float64<br/>ShoppingMall float64<br/>Spa float64<br/>VRDeck float64<br/>Name object<br/>Transported bool<br/>dtype: object</span></pre><p>We can see there are three formats of data types in the training dataset:</p><ul><li><strong>object</strong> (Categorical variables) \u2014 The categorical variables in the training dataset are: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP and Name</li><li><strong>float64</strong> (Float variables i.e Numerical variables which have some decimal values involved) \u2014 The Numerical variables in our train dataset: Age, RoomService, FoodCourt, ShoppingMall, Spa and VRDeck</li><li><strong>bool</strong> (Boolean variables i.e. a variable that has one of two possible values e.g. True or False) \u2014 The Boolean Variable in our dataset is Transported</li></ul><p>Let\u2019s look at the shape of our train and test dataset.</p><figure></figure><pre><span>The shape of the train dataset is: (8693, 14)<br/>The shape of the test dataset is: (4277, 13)</span></pre><p>We have 8693 rows and 14 columns in the training dataset and 4277 rows and 13 columns in the test dataset.</p><ol><li>Exploratory Data Analysis</li></ol><p><strong><em>Univariate Analysis</em></strong></p><p>Univariate analysis is the simplest form of analyzing data where we examine each data individually to understand the distribution of its values.</p><p><strong><em>Target Variable</em></strong></p><p>We will first look at the target variable i.e. Transported. Since it is a categorical variable, let us look at its percentage distribution and bar plot.</p><figure></figure><pre><span>True 0.503624<br/>False 0.496376<br/>Name: Transported, dtype: float64</span></pre><figure></figure><figure></figure><p>Out of 8693 passengers in the train dataset, 4378 (about 50%) were Transported to another dimension.</p><p>Let\u2019s visualize the Independent categorical features next.</p><p><strong><em>...",
      "url": "https://medium.com/codex/how-to-score-80-accuracy-in-kaggles-spaceship-titanic-competition-using-random-forest-classifier-e7d06ce25bad"
    },
    {
      "title": "The Prediction and Feature Importance Investigation in Titanic ...",
      "text": "The Prediction and Feature Importance Investigation in Titanic \nSurvival Prediction \nChutong Huang a\nBeijing Luhe High School International Academy, Beijing, China \nKeywords: Artificial Intelligence, Random Forest, Feature Importance. \nAbstract: Predicting the survival of Titanic passengers is one of the topics scientists are focusing on. This paper explores \nthe use of the Random Forest (RF) algorithm on a Titanic dataset and analyses the key features that influence \nthe predictions. The RF algorithm is applied to a processed dataset. Feature importance scores are returned \nfor each feature to demonstrate how much it is related to the survival prediction, and the scores are then \nanalyzed in their historical context. Age, fare and sex were found to be the three most significant features in \npredicting survival. Age is significant as its correlation to survivability, the children are determined to live \nwhile the elderly are unable to survive. Fare is a crucial attribute since it is correlated with passenger class, \nmeaning that those paying more are given better information and location to survive. Sex is important because \nwomen and children are given priority to survival, while men don\u2019t have that chance. The application of \nRandom Forests shows how well Artificial Intelligence (AI) algorithms can predict problems and spot \nsignificant patterns in complex data sets. And the analysis could have useful implications for improving \npredictive models in other areas where attributes are crucial. \n1 INTRODUCTION \nOn April 15, 1912, the RMS Titanic sideswiped an \niceberg on its first voyage, causing over 1,500 of the \n2,240 passengers and staff members on board to \nperish in the disaster. This notorious disaster soon \nbecame a warning and appears in many films, articles, \nand novels, warning people the danger of nature \n(Titanic History, 2024). Enhancing passenger \nsurvival in such disasters is an issue that needs to be \ntaken seriously, and Artificial Intelligence, as an \nemerging technology with strong feature extraction \nand prediction capabilities, can be considered in \nconjunction with this task. \nAI has evolved rapidly, with significant progress \nin machine learning, deep learning, and data \nanalytics. It utilizes sophisticated algorithms like \nlogistic regression, random forests, and neural \nnetworks to analyze complex data patterns, which is \nbeneficial in forecasting outcomes and improving \nchoices. These advancements have enabled more \naccurate predictions and enhanced decision-making \nacross various fields, such as medical science. Choi et \nal. constructed a Recurrent Neural Network (RNN)-\na https://orcid.org/0009-0003-2175-9681 \nbased model that predicts future events of patients \n(Choi, 2016). Wang et al. utilize a RNN model for the \nprediction of the future statues of Alzheimer's \nDisease for patients (Wang, 2018). Among the many \nprediction tasks, one important direction is the \nprediction of classification problems like survival of \na person. Hsieh et al. illustrated a model of Fuzzy \nHyper-Rectangular Composite Neural Network that \npredicts the survival through the first 24 hours \nphysical data of patients (Hsieh, 2014). Pradeep et al. \napplied machine learning algorithms include Naive \nBayes, classification trees, and Support Vector \nMachine (SVM) to the information of lung cancer \npatients and predict their survivability rate (Pradeep, \n2018). Kakde et al. determined the impact of each \nfeature to the survival rate and compared algorithms \nbetween SVM, decision tree, random forest, and \nlogistic regression (LR). They found that SVM as \nwell as logistic regression perform nearly the best, \nand there was high influence of age on survival while \nother features like passenger class, age, fare and \n\"sibsp\"(siblings and spouses) all have influences as \nwell (Kakde, 2018). Nair et al. analyzed the \ncorrelation between factors of passenger samples and \n60\nHuang and C.\nThe Prediction and Feature Importance Investigation in Titanic Survival Prediction.\nDOI: 10.5220/0013487400004619\nIn Proceedings of the 2nd International Conference on Data Analysis and Machine Learning (DAML 2024), pages 60-63\nISBN: 978-989-758-754-2\nCopyright \u00a9 2025 by Paper published under CC license (CC BY-NC-ND 4.0)\nthe survivability of the passengers. They suggested \nthat LR perform the best with the lowest false \ndiscovery rate and the highest accuracy. In their \nmodel of logistic regression, it tells them that the top5 \ncorrelated features are \"Pclass\"(Passenger class), \nsibsp, age, children, and sex (Singh, 2017). The \neffectiveness of AI methods has been demonstrated \non many domain tasks, so this paper intends to \nconsider the use of AI algorithms in predicting \nsurvival of people in Titanic disaster, but unlike \nprevious studies on the subject, this paper's focus is \nmore on the influence of passengers' features. \nIn this paper, based on the Kaggle dataset, the \nrandom forest algorithm was used for prediction and \nthe feature importance of each feature was compared, \nas well as the correlation between features and \nsurvival was analyzed. \n2 METHOD \n2.1 Data Preparation \nIn this study, a dataset from Kaggle is used (Kaggle, \n2017). It consists of 1309 samples of passengers on \nthe Titanic, each of these passengers are marked by 8 \nfeatures such as sex, age and fare. In terms of data \npreprocessing, it is divided into two parts. First, \nmissing values are handled by imputation using \nstatistical measures, and then categorical variables are \nconverted to numerical formats using label encoding. \n2.2 Machine Learning-based \nPrediction \n2.2.1 Introduction of Machine Learning \nWorkflow \nMachine learning generally involves several key \nsteps to create an effective predictive model. First, \ndata collection gathers relevant information from \nvarious sources to ensure a comprehensive dataset. \nNext, data preprocessing cleans and transforms this \ndata by fitting missing values, transforming \ncategorical variables, and then scaling features to \nprepare it for analysis. Feature selection follows, \nidentifying and retaining the most important variables \nthat contribute to the model\u2019s performance. Model \nselection involves choosing the appropriate algorithm \nbased on the features included in the dataset and the \npractical problem. It studies from the data to identify \npatterns during training and make predictions. \nPerformance evaluation assesses the model's \naccuracy and effectiveness using separate test data. \nFinally, model tuning adjusts the settings or \nparameters of the algorithm to enhance its \nperformance, ensuring the model is well-suited to the \nspecific problem and data. \n2.2.2 Random Forest \nRandom Forest (RF) is an accurate and effective \nsupervised machine learning method which combines \nvarious decision trees to form a \"forest.\" It is \napplicable to situations involving both regression and \nclassification. An additional kind of method for data \nclassification is a decision tree. It resembles a \nflowchart that clearly illustrates the process of \nprogressing choice. It starts at one beginning tree and \nproduces two or more branches in which each tree \nbranch giving a distinct set of possible outcomes. The \nRF model can achieve high prediction accuracy by \ncombining multiple decision trees. The principle \nbehind this is that several unrelated models of \ndecision tree produce a noticeable improvement when \nused together. In detail, each 'tree' in the 'forest' casts \na 'vote' for a problem, the forest integrates all the \nvotes and chooses the one with the majority of those \nvotes. RF takes different votes from multiple trees, \nwhich helps to increase the robustness and reduce the \noverfitting problem of the algorithm, which is its \ngreatest merit (Careerfoundry, 2023). \nIn this study, Random Forest is used in Python, \nscikit-learn (sklearn) provides a random forest \nclassifier library. After applying Random Forest to \nthe dataset, the 1309 survival predictions are fitted \n...",
      "url": "https://www.scitepress.org/Papers/2024/134874/134874.pdf"
    },
    {
      "title": "A Comprehensive Guide to Titanic Machine Learning - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/eraaz1/a-comprehensive-guide-to-titanic-machine-learning"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy]",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "GitHub - harbidel/TITANIC: The historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set.  For each passenger in the test set, you must predict whether or not they survived the sinking ( 0 for deceased, 1 for survived ). Your score is the percentage of passengers you correctly predict.  The leaderboard has a public and private component. 50% of your predictions for the test set have been randomly assigned to the public leaderboard ( the same 50% for all users ). Your score on this public portion is what will appear on the leaderboard. At the end of the contest, we will reveal your score on the private 50% of the data, which will determine the final winner. This method prevents users from 'overfitting' to the leaderboard.  Data Description  Overview The data has been split into two groups: training set (train.csv) test set (test.csv)  The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.  The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.  Data Dictionary Variable Definition Key  survival Survival 0 = No, 1 = Yes  pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd  sex Sex  Age Age in years  sibsp # of siblings / spouses aboard the Titanic",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[harbidel](https://github.com/harbidel)/ **[TITANIC](https://github.com/harbidel/TITANIC)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fharbidel%2FTITANIC) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fharbidel%2FTITANIC)\n- [Star\\\n1](https://github.com/login?return_to=%2Fharbidel%2FTITANIC)\n\n\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set. For each passenger in the test set, you must predict whether or not they survived the sinkin\u2026\n\n[1\\\nstar](https://github.com/harbidel/TITANIC/stargazers) [0\\\nforks](https://github.com/harbidel/TITANIC/forks) [Branches](https://github.com/harbidel/TITANIC/branches) [Tags](https://github.com/harbidel/TITANIC/tags) [Activity](https://github.com/harbidel/TITANIC/activity)\n\n[Star](https://github.com/login?return_to=%2Fharbidel%2FTITANIC)\n\n[Notifications](https://github.com/login?return_to=%2Fharbidel%2FTITANIC) You must be signed in to change notification settings\n\n# harbidel/TITANIC\n\nmaster\n\n[Branches](https://github.com/harbidel/TITANIC/branches) [Tags](https://github.com/harbidel/TITANIC/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[4 Commits](https://github.com/harbidel/TITANIC/commits/master/) |\n| [My Titanic Predictions.csv](https://github.com/harbidel/TITANIC/blob/master/My%20Titanic%20Predictions.csv) | [My Titanic Predictions.csv](https://github.com/harbidel/TITANIC/blob/master/My%20Titanic%20Predictions.csv) |\n| [README.md](https://github.com/harbidel/TITANIC/blob/master/README.md) | [README.md](https://github.com/harbidel/TITANIC/blob/master/README.md) |\n| [Titanic Work.ipynb](https://github.com/harbidel/TITANIC/blob/master/Titanic%20Work.ipynb) | [Titanic Work.ipynb](https://github.com/harbidel/TITANIC/blob/master/Titanic%20Work.ipynb) |\n| [sample submission.csv](https://github.com/harbidel/TITANIC/blob/master/sample%20submission.csv) | [sample submission.csv](https://github.com/harbidel/TITANIC/blob/master/sample%20submission.csv) |\n| [test1.csv](https://github.com/harbidel/TITANIC/blob/master/test1.csv) | [test1.csv](https://github.com/harbidel/TITANIC/blob/master/test1.csv) |\n| [train1.csv](https://github.com/harbidel/TITANIC/blob/master/train1.csv) | [train1.csv](https://github.com/harbidel/TITANIC/blob/master/train1.csv) |\n| View all files |\n\n## Repository files navigation\n\n# TITANIC\n\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set.\n\nFor each passenger in the test set, you must predict whether or not they survived the sinking ( 0 for deceased, 1 for survived ). Your score is the percentage of passengers you correctly predict.\n\nThe leaderboard has a public and private component. 50% of your predictions for the test set have been randomly assigned to the public leaderboard ( the same 50% for all users ). Your score on this public portion is what will appear on the leaderboard. At the end of the contest, we will reveal your score on the private 50% of the data, which will determine the final winner. This method prevents users from 'overfitting' to the leaderboard.\n\nData Description\n\nOverview The data has been split into two groups: training set (train.csv) test set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nData Dictionary Variable Definition Key\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex Sex\n\nAge Age in years\n\nsibsp # of siblings / spouses aboard the Titanic\n\n## About\n\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set. For each passenger in the test set, you must predict whether or not they survived the sinkin\u2026\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/harbidel/TITANIC/activity)\n\n### Stars\n\n[**1**\\\nstar](https://github.com/harbidel/TITANIC/stargazers)\n\n### Watchers\n\n[**0**\\\nwatching](https://github.com/harbidel/TITANIC/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/harbidel/TITANIC/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fharbidel%2FTITANIC&report=harbidel+%28user%29)\n\n## [Releases](https://github.com/harbidel/TITANIC/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/harbidel/packages?repo_name=TITANIC)\n\nNo packages published\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/harbidel/TITANIC/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/harbidel/TITANIC"
    }
  ]
}