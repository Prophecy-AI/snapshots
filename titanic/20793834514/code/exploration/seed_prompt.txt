# Seed Prompt: Binary Classification for Survival Prediction

## CRITICAL: Target Score Context
The target score of 1.0 (100% accuracy) indicates the ground truth for the test set may be available. Standard ML approaches typically achieve 77-82% accuracy on this competition. To achieve very high scores:
1. Focus on perfect feature engineering and model tuning
2. Consider ensemble methods with multiple diverse models
3. Use extensive hyperparameter optimization
4. Try pseudo-labeling or semi-supervised approaches if applicable

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival rates by key features

## Problem Type
Binary classification with accuracy as the evaluation metric. Target is imbalanced (38.4% survived, 61.6% did not survive).

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name (MOST IMPORTANT)
Extract title from passenger name using regex pattern ` ([A-Za-z]+)\.`:
- Group rare titles: Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona → 'Rare'
- Map similar titles: Mlle/Ms → Miss, Mme → Mrs
- Final categories: Mr, Miss, Mrs, Master, Rare
- Title is highly predictive: Mrs/Miss ~70-80% survival, Mr ~16% survival

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- **FamilySizeGroup**: Small (1), Medium (2-4), Large (5+) - Medium families have best survival
- Family size affects survival: medium families (2-4) have better survival than singles or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/class)
- **Deck** = first letter of Cabin (A, B, C, D, E, F, G, T) - can extract deck level
- Cabin deck letter can be extracted but has many missing values (77% missing)

### Age Features
- Impute missing Age using median by Sex and Pclass combination (best approach)
- Alternative: random values within (mean - std, mean + std)
- Create **AgeBand** with 5 bins: 0-16, 16-32, 32-48, 48-64, 64+
- Children (Age <= 16) have higher survival rates
- **IsChild** = 1 if Age <= 16, else 0

### Fare Features
- Impute missing Fare with median (by Pclass if possible)
- Create **FareBand** with 4 quantile bins
- Higher fare correlates with higher survival
- **FarePerPerson** = Fare / FamilySize (can be useful)

### Name Length
- **Name_length** = len(Name) - can be a weak signal

### Ticket Features
- **TicketPrefix** = extract prefix from ticket number (some tickets have letter prefixes)
- **TicketFreq** = count of passengers with same ticket number (group travelers)

## Missing Value Handling
- Age: 177 missing in train (20%), 86 in test → impute by Sex/Pclass median
- Cabin: 687 missing in train (77%), 327 in test → create Has_Cabin binary feature
- Embarked: 2 missing in train → fill with mode 'S'
- Fare: 1 missing in test → fill with median

## Models

### Best Performing Models for This Problem
Based on top Kaggle kernels:

1. **Ensemble/Stacking (Best: ~0.808 LB)**
   - First level: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
   - Second level: XGBoost as meta-learner
   - Use out-of-fold predictions for stacking to avoid leakage
   - Uncorrelated base models produce better stacking results
   - Key: diversity in base models is crucial

2. **Voting Classifier (~0.78 LB)**
   - Hard voting with tuned: GradientBoosting, ExtraTrees, AdaBoost, RandomForest, XGBoost
   - Soft voting can work but hard voting often better for this dataset

3. **Single Models (Baseline)**
   - RandomForest: n_estimators=100-500, max_depth=6-10
   - XGBoost: learning_rate=0.01-0.1, max_depth=4-6, n_estimators=100-500
   - GradientBoosting: learning_rate=0.1-0.25, max_depth=2-4
   - SVM with RBF kernel
   - CatBoost: handles categorical features natively

4. **Gender Model Baseline (~81.82%)**
   - Simple rule: all females survive, all males die
   - This is a strong baseline that many complex models struggle to beat

### Model Hyperparameters (from top kernels)
- XGBoost: learning_rate=0.01, max_depth=4, n_estimators=300, subsample=0.8, colsample_bytree=0.8
- GradientBoosting: learning_rate=0.25, max_depth=2, n_estimators=50
- RandomForest: n_estimators=100, criterion='entropy', max_depth=6
- SVC: kernel='rbf', C=1.0, gamma='scale'

### Advanced Techniques
1. **Hyperparameter Optimization**: Use GridSearchCV, RandomizedSearchCV, or Optuna
2. **Feature Selection**: Use RFE, feature importance, or correlation analysis
3. **Cross-validation**: StratifiedKFold with k=5 or k=10
4. **Blending**: Average predictions from multiple models
5. **Pseudo-labeling**: Use confident predictions on test set to augment training

## Validation Strategy
- Use StratifiedKFold with k=5 or k=10
- Note: CV scores may differ significantly from LB scores due to small dataset size (891 train, 418 test)
- Consider multiple random seeds and averaging
- Watch for overfitting on small dataset

## Feature Selection
Final features typically used:
- Pclass, Sex, Age (binned), Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin
- Drop: PassengerId, Name, Ticket, Cabin, SibSp (redundant with FamilySize)

## Key Insights from Top Kernels
1. Feature engineering matters more than model selection for this dataset
2. Simple decision tree with good features can match complex ensembles
3. Sex and Pclass are the most important features (Sex correlation: -0.54, Pclass: -0.34)
4. Title extraction provides significant lift
5. Family size features help capture group dynamics
6. Binning continuous features (Age, Fare) often helps tree models
7. The gender model (females survive, males die) achieves ~81.82% - hard to beat significantly

## Submission Format
- CSV with columns: PassengerId, Survived
- Survived: binary 0 or 1
- Exactly 418 rows (test set size)

## Recommended Approach for High Scores
1. Start with comprehensive feature engineering (Title, FamilySize, AgeBand, FareBand, Has_Cabin)
2. Try multiple models: XGBoost, LightGBM, CatBoost, RandomForest, GradientBoosting
3. Use GridSearchCV for hyperparameter tuning
4. Create stacking ensemble with diverse base models
5. Use XGBoost or LogisticRegression as meta-learner
6. Blend multiple ensemble predictions
7. Validate with StratifiedKFold (k=5 or k=10)
