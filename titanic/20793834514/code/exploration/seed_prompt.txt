# Seed Prompt: Binary Classification for Survival Prediction

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival rates by key features

## Problem Type
Binary classification with accuracy as the evaluation metric. Target is imbalanced (38.4% survived, 61.6% did not survive).

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title from passenger name using regex pattern ` ([A-Za-z]+)\.`:
- Group rare titles: Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona → 'Rare'
- Map similar titles: Mlle/Ms → Miss, Mme → Mrs
- Final categories: Mr, Miss, Mrs, Master, Rare
- Title is highly predictive: Mrs/Miss ~70-80% survival, Mr ~16% survival

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size affects survival: medium families (2-4) have better survival than singles or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/class)
- Cabin deck letter can be extracted but has many missing values

### Age Features
- Impute missing Age using median by Sex and Pclass combination
- Alternative: random values within (mean - std, mean + std)
- Create **AgeBand** with 5 bins: 0-16, 16-32, 32-48, 48-64, 64+
- Children (Age <= 16) have higher survival rates

### Fare Features
- Impute missing Fare with median
- Create **FareBand** with 4 quantile bins
- Higher fare correlates with higher survival

### Name Length
- **Name_length** = len(Name) - can be a weak signal

## Missing Value Handling
- Age: 177 missing in train, 86 in test → impute by Sex/Pclass median
- Cabin: 687 missing in train, 327 in test → create Has_Cabin binary feature
- Embarked: 2 missing in train → fill with mode 'S'
- Fare: 1 missing in test → fill with median

## Models

### Best Performing Models for This Problem
Based on top Kaggle kernels:

1. **Ensemble/Stacking (Best: ~0.808 LB)**
   - First level: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
   - Second level: XGBoost as meta-learner
   - Use out-of-fold predictions for stacking to avoid leakage
   - Uncorrelated base models produce better stacking results

2. **Voting Classifier (~0.78 LB)**
   - Hard voting with tuned: GradientBoosting, ExtraTrees, AdaBoost, RandomForest, XGBoost
   - Soft voting can work but hard voting often better for this dataset

3. **Single Models (Baseline)**
   - RandomForest: n_estimators=100-500, max_depth=6-10
   - XGBoost: learning_rate=0.01-0.1, max_depth=4-6, n_estimators=100-500
   - GradientBoosting: learning_rate=0.1-0.25, max_depth=2-4
   - SVM with RBF kernel

### Model Hyperparameters (from top kernels)
- XGBoost: learning_rate=0.01, max_depth=4, n_estimators=300, subsample=0.8, colsample_bytree=0.8
- GradientBoosting: learning_rate=0.25, max_depth=2, n_estimators=50
- RandomForest: n_estimators=100, criterion='entropy', max_depth=6

## Validation Strategy
- Use StratifiedKFold with k=5 or k=10
- Note: CV scores may differ significantly from LB scores due to small dataset size
- Consider multiple random seeds and averaging

## Feature Selection
Final features typically used:
- Pclass, Sex, Age (binned), Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin
- Drop: PassengerId, Name, Ticket, Cabin, SibSp (redundant with FamilySize)

## Key Insights from Top Kernels
1. Feature engineering matters more than model selection for this dataset
2. Simple decision tree with good features can match complex ensembles
3. Sex and Pclass are the most important features
4. Title extraction provides significant lift
5. Family size features help capture group dynamics
6. Binning continuous features (Age, Fare) often helps tree models

## Submission Format
- CSV with columns: PassengerId, Survived
- Survived: binary 0 or 1
- Exactly 418 rows (test set size)
