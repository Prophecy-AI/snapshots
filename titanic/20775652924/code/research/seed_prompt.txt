# Titanic Survival Prediction - Strategy Guide (Loop 4)

## Current Status
- Best CV score: 0.8372 from exp_001 (Voting Ensemble)
- Best LB score: 0.7727 from exp_001 (Voting Ensemble)
- CV-LB gap: +0.0645 (CV overestimates LB by ~6.5%)
- Latest experiment: exp_003 (Adversarial + Regularized) CV 0.8327 - WORSE than voting
- **Target: 1.0 is IMPOSSIBLE** - Top Kaggle solutions achieve 0.80-0.82
- **Realistic target: 0.80 LB** (need +0.0273 improvement, CV ~0.8479)
- Submissions remaining: 1/10 today (SAVE IT)

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator confirmed:
- Adversarial validation properly implemented (AUC = 0.5196)
- No leakage detected
- Score integrity verified

**Evaluator's top priority: DO NOT SUBMIT exp_003. Focus on hyperparameter optimization.**
- AGREE: exp_003 (CV 0.8327) is worse than exp_001 (CV 0.8372)
- AGREE: Save the remaining submission for a more promising candidate
- AGREE: Target CV > 0.84 through hyperparameter optimization

**Key insights from exp_003:**
1. **Adversarial AUC = 0.5196** → Minimal distribution shift between train/test
2. **Regularization HURT performance** → Models are NOT overfitting in traditional sense
3. **The CV-LB gap is NOT due to distribution shift** → Likely small sample variance (418 test samples)
4. **Original voting ensemble remains best** → Well-tuned hyperparameters

**Synthesis:** The CV-LB gap cannot be reduced by regularization or feature selection. The gap is likely inherent to the small test set size. Focus should shift to:
1. Improving CV score through hyperparameter optimization (target: 0.84+)
2. Multi-seed ensembling to reduce variance
3. Interaction features that capture non-linear relationships

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Basic EDA, survival patterns
- `exploration/evolver_loop1_analysis.ipynb` - Deck, Ticket, Name_Length analysis
- `exploration/evolver_loop2_lb_feedback.ipynb` - CV-LB calibration: LB = 2.55*CV - 1.37
- `exploration/evolver_loop3_analysis.ipynb` - OOF correlation analysis
- `experiments/004_adversarial_regularized/adversarial_regularized.ipynb` - Adversarial validation results

**Key findings:**
1. **Adversarial AUC = 0.5196** - No distribution shift (features don't distinguish train/test)
2. **CV-LB calibration**: LB = 2.55*CV - 1.37. To achieve LB 0.80, need CV ~0.8479
3. **OOF predictions highly correlated** (mean 0.851) - Stacking doesn't help
4. **Feature importance (RF)**: Title (0.246), Sex (0.202), Fare (0.090), Name_Length (0.081)
5. **Regularization hurts** - CV dropped from 0.8372 to 0.8294 with stronger regularization

## Recommended Approaches (Priority Order)

### Priority 1: HYPERPARAMETER OPTIMIZATION WITH OPTUNA
The voting ensemble uses reasonable but not optimized hyperparameters. Use Bayesian optimization to find better settings.

**Implementation:**
```python
import optuna
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

def objective(trial):
    # XGBoost hyperparameters
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'gamma': trial.suggest_float('gamma', 0, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),
    }
    
    model = XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric='logloss')
    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')
    return scores.mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

**Target:** CV > 0.84 (current: 0.8372)

### Priority 2: MULTI-SEED ENSEMBLE
Train the voting ensemble with multiple random seeds and average predictions. This reduces variance without increasing bias.

**Implementation (from TF-DF kernel):**
```python
predictions = np.zeros(len(X_test))
n_seeds = 20

for seed in range(n_seeds):
    # Create voting ensemble with different seed
    models = [
        ('rf', RandomForestClassifier(n_estimators=100, max_depth=6, random_state=seed)),
        ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=seed)),
        ('xgb', XGBClassifier(n_estimators=100, max_depth=3, random_state=seed)),
        # ... other models
    ]
    
    # Train and predict
    for name, model in models:
        model.fit(X_train, y_train)
        predictions += model.predict_proba(X_test)[:, 1] / (len(models) * n_seeds)

final_predictions = (predictions >= 0.5).astype(int)
```

**Why this works:** Different random seeds create slightly different decision boundaries. Averaging reduces variance and improves generalization.

### Priority 3: INTERACTION FEATURES
Create interaction features that capture non-linear relationships:

```python
# Sex × Pclass interaction (most important)
df['Sex_Pclass'] = df['Sex'].astype(str) + '_' + df['Pclass'].astype(str)

# Title × Pclass interaction
df['Title_Pclass'] = df['Title'].astype(str) + '_' + df['Pclass'].astype(str)

# Age × Sex interaction
df['Age_Sex'] = df['Age_Bin'].astype(str) + '_' + df['Sex'].astype(str)

# Fare per person (for families)
df['Fare_Per_Person'] = df['Fare'] / df['FamilySize']
```

**Why:** The voting ensemble may be missing interaction effects. Sex × Pclass is particularly important (Pclass 3 females have different survival than Pclass 1 females).

### Priority 4: TRY LIGHTGBM
LightGBM sometimes outperforms XGBoost on small datasets due to different tree-building strategy.

```python
from lightgbm import LGBMClassifier

lgbm = LGBMClassifier(
    n_estimators=200,
    max_depth=4,
    learning_rate=0.05,
    num_leaves=15,  # Key parameter for LightGBM
    min_child_samples=10,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
```

## What NOT to Try

1. **Stronger regularization** - Already tested, HURT performance (CV 0.8294 < 0.8372)
2. **Feature selection** - Already tested, didn't help (CV 0.8327 < 0.8372)
3. **Pure stacking** - OOF predictions too correlated (0.851 mean correlation)
4. **Passthrough stacking** - Already tested, CV 0.8271 < voting 0.8372
5. **Adversarial reweighting** - No distribution shift to correct (AUC = 0.5196)
6. **Target of 1.0** - IMPOSSIBLE. Focus on realistic target of 0.80 LB.

## Validation Notes

- Use StratifiedKFold (k=10) for all experiments
- CV-LB calibration: LB ≈ 2.55*CV - 1.37 (from 2 submissions)
- To achieve LB 0.80, need CV ~0.8479 (current: 0.8372, need +0.0107)
- Lower variance correlates with better generalization

## Submission Strategy

- **DO NOT SUBMIT** unless CV improves significantly (>0.84)
- Only 1 submission remaining today - SAVE IT
- Focus on hyperparameter optimization and multi-seed ensembling
- If CV reaches 0.84+, consider submitting to verify CV-LB calibration

## Expected Outcome

If hyperparameter optimization and multi-seed ensembling work:
- Target CV: 0.84+ (current: 0.8372)
- Expected LB: 0.77-0.78 (using calibration formula)
- Improvement: +0.01-0.02 LB over current best (0.7727)

## Key Insight

The CV-LB gap is NOT fixable through regularization or feature selection. It's likely due to:
1. Small sample variance (418 test samples)
2. Inherent difficulty of the Titanic prediction problem
3. The calibration formula may not be perfectly accurate

**Focus on improving CV score through hyperparameter optimization.** This is the most direct path to improving LB score.
