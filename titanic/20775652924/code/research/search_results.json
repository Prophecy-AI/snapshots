{
  "query": "How to reduce CV-LB gap in Kaggle competitions when test distribution differs from train?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent research on covariate shift and best\u2011practice guidance for Kaggle competitions, you can shrink the public\u2011vs\u2011private leaderboard (CV\u2011LB) gap when the test distribution differs from the training data by re\u2011weighting the training set, using a validation scheme that mirrors the target distribution, and carefully blending weighted and unweighted models\u202f([hal.science](https://hal.science/hal-01213965/file/Covariate_shift_ECML2015.pdf); [imbalanced\u2011learn.org](https://imbalanced-learn.org/stable/common_pitfalls.html); [chioka.in](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Detect the shift** \u2013 Compare marginal feature statistics (means, histograms, KS tests) between the training set and the public\u2011test (or a hold\u2011out slice of it). This confirms that\u202f\\(P_{\\text{train}}(X) \\neq P_{\\text{test}}(X)\\).  \n\n2. **Estimate importance weights** \u2013 Fit a density\u2011ratio model (e.g., logistic regression or a kernel\u2011based estimator) that predicts whether a row comes from the test or train split. The predicted odds give the weight\u202f\\(w_i = \\frac{p_{\\text{test}}(x_i)}{p_{\\text{train}}(x_i)}\\).  \n\n3. **Train two models**  \n   * **Weighted model** \u2013 Train your primary algorithm (e.g., XGBoost, LightGBM) using the importance weights\u202f\\(w_i\\) as sample weights.  \n   * **Unweighted model** \u2013 Train the same algorithm on the original data without weights.  \n\n4. **Blend the models** \u2013 Combine predictions with a simple linear blend (e.g.,\u202f\\(\\hat y = \\alpha \\hat y_{\\text{weighted}} + (1-\\alpha)\\hat y_{\\text{unweighted}}\\)). Cross\u2011validate different\u202f\\(\\alpha\\)\u202fvalues to find the blend that minimizes validation loss. This exploits the finding that weighted models reduce global bias while unweighted models can be locally better on low\u2011importance instances\u202f([hal.science](https://hal.science/hal-01213965/file/Covariate_shift_ECML2015.pdf)).  \n\n5. **Use a distribution\u2011aware CV scheme** \u2013  \n   * Split the data **before** any resampling or weighting to avoid data leakage\u202f([imbalanced\u2011learn.org](https://imbalanced-learn.org/stable/common_pitfalls.html)).  \n   * Apply *rebalanced* or *stratified* folds that preserve the estimated importance\u2011weight distribution (e.g., use the same weighting scheme inside each fold or the RebalancedCV utility\u202f([github.com/korem-lab/RebalancedCV](https://github.com/korem-lab/rebalancedcv))).  \n   * Run multiple folds (e.g., 5\u2011fold repeated 3 times) and average scores to obtain a stable CV estimate.  \n\n6. **Monitor the CV\u2011LB gap** \u2013 After each blend/parameter change, compare the CV score to the public leaderboard score. If the gap widens, reduce model complexity or increase regularization to curb over\u2011fitting to the public split\u202f([chioka.in](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio)).  \n\n7. **Finalize and submit** \u2013 Once the blended model shows a minimal CV\u2011LB gap on the validation folds, generate predictions for the full test set and submit. Keep a backup of the unblended weighted model in case the private test distribution deviates further.  \n\nFollowing these steps\u2014detecting shift, importance\u2011weighting, blending weighted/unweighted models, and using leakage\u2011free, distribution\u2011aware cross\u2011validation\u2014will systematically reduce the CV\u2011LB discrepancy when the test data distribution differs from the training data.",
      "url": ""
    },
    {
      "title": "",
      "text": "A Practical Approach to Reduce the Learning\nBias under Covariate Shift\nVan-Tinh Tran and Alex Aussem\nLIRIS, UMR 5205\nUniversity of Lyon 1\n69622 Lyon, France\n{van-tinh.tran,aaussem}@univ-lyon1.fr\nAbstract. Covariate shift is a specific class of selection bias that arises\nwhen the marginal distributions of the input features X are different in\nthe source and the target domains while the conditional distributions\nof the target Y given X are the same. A common technique to deal\nwith this problem, called importance weighting, amounts to reweighting\nthe training instances in order to make them resemble the test distribu\u0002tion. However this usually comes at the expense of a reduction of the\neffective sample size. In this paper, we show analytically that, while the\nunweighted model is globally more biased than the weighted one, it may\nlocally be less biased on low importance instances. In view of this result,\nwe then discuss a manner to optimally combine the weighted and the\nunweighted models in order to improve the predictive performance in\nthe target domain. We conduct a series of experiments on synthetic and\nreal-world data to demonstrate the efficiency of this approach.\n1 Introduction\nSelection bias, also termed dataset shift or domain adaptation in the litera\u0002ture [8], occurs when the training distribution P(x, y) and the test distribution\nP\n0\n(x, y) are different. It is pervasive in almost all empirical studies, including\nMachine Learning, Statistics, Social Sciences, Economics, Bioinformatics, Bio\u0002statistics, Epidemiology, Medicine, etc. Selection bias is prevalent in many real\u0002world machine learning problems because the common assumption in machine\nlearning is that the training and the test data are drawn independently and\nidentically from the same distribution. The term \u201ddomain adaptation\u201d is used\nwhen one builds a model from some fixed source domain, but wishes to deploy it\nacross one or more different target domains. The term \u201dselection bias\u201d is slightly\nmore specific as it assumes implicitly that there exists a binary variable S that\ncontrols the selection of examples in the training set, in other words we only\nhave access to the examples that have S = 1. For instance, case-control stud\u0002ies in Epidemiology are particularly susceptible to selection bias, including bias\nresulting from inappropriate selection of controls in case-control studies, bias\nresulting from differential loss-to-follow-up, incidence-prevalence bias, volunteer\nbias, healthy-worker bias, and nonresponse bias [4].\nIt is well known that one may account for the difference between P(x, y)\nand P\n0\n(x, y) by re-weighting the training points using the so-called importance\nweight, denoted as \u03b2(x, y) = P\n0\n(x, y)/P(x, y). Formally, let {h\u03b8\n\u2217 }\u03b8\u2208\u0398 be a model\nfamily from which we want to select an optimal model h\u03b8\n\u2217 (x) = h(x, \u03b8\u2217\n) for our\nlearning task and let l(y, h(x, \u03b8)) be the loss function we would like to minimize,\nthe optimal model we are searching for is the one that minimizes the expected\nloss over the test (or target) distribution:\n\u03b8\n\u2217 = argmin\n\u03b8\u2208\u0398\nX\n(x,y)\u223cP\n\u03b2(x, y)P(x, y)l(y, h(\u03b8, x))\nSo in practice, weighting the empirical loss of the training instances by \u03b2(x, y)\nprovides a well-justified solution to the selection bias problem.\nIn general, the estimation of \u03b2(x, y) with two different distributions P(x, y)\nand P\n0\n(x, y) is unsolvable, as the two terms could be arbitrarily far apart. One\nsimple assumption we can make about the connection between the distributions\nof the source and the target domains is that P(x, y) and P\n0\n(x, y) differ only in\nP(x) and P\n0\n(x) while their conditional distribution P(y|x) remains unchanged.\nThis specific selection bias is known as covariate shift in the literature [10]. In this\ncase, the weighting term reduces to \u03b2(x) = P\n0\n(x)/P(x) and effective adaptation\nis possible. At first glance, it may appear that covariate shift is not a problem\nbecause, for classification, we are only interested in P(Y |X) which remains un\u0002changed. In fact, Shimodaira [10] showed that there are circumstances under\nwhich the predictive performance is jeopardized by covariate shift. This happens\ntypically when the parametric model family {P(Y |X, \u03b8)}\u03b8\u2208\u0398 is misspecified, that\nis, there does not exist any \u03b8 \u2208 \u0398 such that P(Y |X = x, \u03b8) = P(Y |X = x) for\nall x \u2208 X , so none of the models in the model family can exactly match the true\nrelation between X and Y .\nThe intuitive reason why covariate shift under model misspecification is a\nproblem is that the optimal (misspecified) model performs better in dense regions\nof the input space than in sparse regions, because the dense regions dominate\nthe average classification error, which is what we want to minimize. If the dense\nregions of X are different in the training and test sets, the optimal model on\nthe training set will no longer be optimal on the test set. In other words, the\noptimal model depends on P(x), and if P\n0\n(x) 6= P(x), then the optimal model\nfor the target domain differs from that for the source domain. It was proven\nthat, if the support of P\n0\n(x) (the set of x for which P\n0\n(x) > 0) is contained in\nthe support of P(x), then the optimal model that maximizes this re-weighted log\nlikelihood function asymptotically converges to the optimal model for the target\ndomain [10] and a large body of research has been devoted to the estimation of\nP\n0\n(x)/P(x) e.g. [13], [5], [11], [2], [1], [6], [7], [9]. However, reweighting methods\ndo not necessarily improve the prediction accuracy as they also dependent on\nthe extent to which the model is misspecified [12].\nIn this paper, we show analytically that, despite the fact that the unweighted\nmodel is globally more biased than the weighted one, the former may locally be\nless biased on low importance instances. In view of this result, we design a simple\nalgorithm that combines the weighted and the unweighted models in order to\nimprove the predictive performance in the target domain. More specifically, we\nprove that an optimal B? always exists such that, in the region where \u03b2(x) \u2264 B?,\nthe biased model trained on the unweighted sample should be preferred to the\nunbiased one, and vice-versa. We propose a practical procedure to estimate this\nthreshold value from training data.\nThe remainder of this paper is structured as follows. In Section 2, we define\nsome key concepts used along the paper and state some results that will support\nour analysis. Then in Section 3, we conduct a theoretical analysis to prove that\nan optimal (but not necessarily unique) B? always exists and discuss a manner\nto optimally combine the weighted and the unweighted models in order to im\u0002prove the predictive performance in the target domain. In section 4, a series of\nexperiments are carried out on toy problems and real-world data sets to assess\nthe effectiveness of this approach.\n2 Preliminaries\nIn this section, we define some key concepts used along the paper and state\nsome results that will support our analysis. Consider the supervised learning\nproblem where we observed n training samples, denoted by ((xt; yt) : t = 1, ..., n),\nwhere xt \u2208 X \u2282 Rd are i.i.d training input points drawn from some probability\ndistribution p(x) and yt \u2208 Y \u2282 R are the corresponding training output values\ndrawn from a conditional probability distribution p(y|x). We are interested in\npredicting the output value y at an input point x using a model h\u03b8(x) = h(x, \u03b8)\nparameterized by \u03b8 \u2208 \u0398 \u2282 Rm. Under covariate shift assumption, the test inputs\nfollow a different probability distribution p\n0\n(x) while the conditional probability\ndistribution of test output p(y|x) remains unchanged. The ratio \u03b2(x) = p\n0\n(x)\np(x)\nis\ncalled the importance of x. Given a loss function l(y, h(x, \u03b8)) : X \u00d7 Y \u00d7 Y \u2192\n[0, \u221e), we shall consider throughout this paper, the following loss functions:\n\u2013 EL-Tr: Expectation of loss over training distribution p(x, y) = p(x)p(y|x)\nLoss0(h\u03b8) = Ex,y\u223cp[l(y, h(x, \u03b8))] = Zp(x)\nZ\np(y|x)l(y, h(x, \u03b8))dydx\n\u2013 EL-Te: Expectation of ...",
      "url": "https://hal.science/hal-01213965/file/Covariate_shift_ECML2015.pdf"
    },
    {
      "title": "9.  Common pitfalls and recommended practices #",
      "text": "9. Common pitfalls and recommended practices &#8212; Version 0.14.1\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![Version 0.14.1 - Home](_static/logo_wide.png)![Version 0.14.1 - Home](https://imbalanced-learn.org/stable/_static/img/logo_wide_dark.png)](index.html)\n**SearchCtrl+K\n******\n* [**GitHub](https://github.com/scikit-learn-contrib/imbalanced-learn)\n**SearchCtrl+K\n******\n* [**GitHub](https://github.com/scikit-learn-contrib/imbalanced-learn)\n# 9.Common pitfalls and recommended practices[#](#common-pitfalls-and-recommended-practices)\nThis section is a complement to the documentation given[[here]](https://scikit-learn.org/dev/common_pitfalls.html)in scikit-learn.\nIndeed, we will highlight the issue of misusing resampling, leading to a**data leakage**. Due to this leakage, the performance of a model reported\nwill be over-optimistic.\n## 9.1.Data leakage[#](#data-leakage)\nAs mentioned in the scikit-learn documentation, data leakage occurs when\ninformation that would not be available at prediction time is used when\nbuilding the model.\nIn the resampling setting, there is a common pitfall that corresponds to\nresample the**entire**dataset before splitting it into a train and a test\npartitions. Note that it would be equivalent to resample the train and test\npartitions as well.\nSuch of a processing leads to two issues:\n* the model will not be tested on a dataset with class distribution similar\nto the real use-case. Indeed, by resampling the entire dataset, both the\ntraining and testing set will be potentially balanced while the model should\nbe tested on the natural imbalanced dataset to evaluate the potential bias\nof the model;\n* the resampling procedure might use information about samples in the dataset\nto either generate or select some of the samples. Therefore, we might use\ninformation of samples which will be later used as testing samples which\nis the typical data leakage issue.\nWe will demonstrate the wrong and right ways to do some sampling and emphasize\nthe tools that one should use, avoiding to fall in the trap.\nWe will use the adult census dataset. For the sake of simplicity, we will only\nuse the numerical features. Also, we will make the dataset more imbalanced to\nincrease the effect of the wrongdoings:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportfetch\\_openml&gt;&gt;&gt;fromimblearn.datasetsimportmake\\_imbalance&gt;&gt;&gt;X,y=fetch\\_openml(...data\\_id=1119,as\\_frame=True,return\\_X\\_y=True...)&gt;&gt;&gt;X=X.select\\_dtypes(include=&quot;number&quot;)&gt;&gt;&gt;X,y=make\\_imbalance(...X,y,sampling\\_strategy={&quot;&gt;50K&quot;:300},random\\_state=1...)\n```\nLet\u2019s first check the balancing ratio on this dataset:\n```\n&gt;&gt;&gt;fromcollectionsimportCounter&gt;&gt;&gt;{key:value/len(y)forkey,valueinCounter(y).items()}{&#39;&lt;=50K&#39;: 0.988..., &#39;&gt;50K&#39;: 0.011...}\n```\nTo later highlight some of the issue, we will keep aside a left-out set that we\nwill not use for the evaluation of the model:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;X,X\\_left\\_out,y,y\\_left\\_out=train\\_test\\_split(...X,y,stratify=y,random\\_state=0...)\n```\nWe will use a`sklearn.ensemble.HistGradientBoostingClassifier`as a\nbaseline classifier. First, we will train and check the performance of this\nclassifier, without any preprocessing to alleviate the bias toward the majority\nclass. We evaluate the generalization performance of the classifier via\ncross-validation:\n```\n&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.model\\_selectionimportcross\\_validate&gt;&gt;&gt;model=HistGradientBoostingClassifier(random\\_state=0)&gt;&gt;&gt;cv\\_results=cross\\_validate(...model,X,y,scoring=&quot;&quot;balanced\\_accuracy&quot;&quot;,...return\\_train\\_score=True,return\\_estimator=True,...n\\_jobs=-1...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/- std. dev.: &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].mean():.3f}+/- &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].std():.3f}&quot;...)Balanced accuracy mean +/- std. dev.: 0.609 +/- 0.024\n```\nWe see that the classifier does not give good performance in terms of balanced\naccuracy mainly due to the class imbalance issue.\nIn the cross-validation, we stored the different classifiers of all folds. We\nwill show that evaluating these classifiers on the left-out data will give\nclose statistical performance:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.metricsimportbalanced\\_accuracy\\_score&gt;&gt;&gt;scores=[]&gt;&gt;&gt;forfold\\_id,cv\\_modelinenumerate(cv\\_results[&quot;estimator&quot;]):...scores.append(...balanced\\_accuracy\\_score(...y\\_left\\_out,cv\\_model.predict(X\\_left\\_out)...)...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/- std. dev.: &quot;...f&quot;{np.mean(scores):.3f}+/-{np.std(scores):.3f}&quot;...)Balanced accuracy mean +/- std. dev.: 0.628 +/- 0.009\n```\nLet\u2019s now show the**wrong**pattern to apply when it comes to resampling to\nalleviate the class imbalance issue. We will use a sampler to balance the**entire**dataset and check the statistical performance of our classifier via\ncross-validation:\n```\n&gt;&gt;&gt;fromimblearn.under\\_samplingimportRandomUnderSampler&gt;&gt;&gt;sampler=RandomUnderSampler(random\\_state=0)&gt;&gt;&gt;X\\_resampled,y\\_resampled=sampler.fit\\_resample(X,y)&gt;&gt;&gt;model=HistGradientBoostingClassifier(random\\_state=0)&gt;&gt;&gt;cv\\_results=cross\\_validate(...model,X\\_resampled,y\\_resampled,scoring=&quot;&quot;balanced\\_accuracy&quot;&quot;,...return\\_train\\_score=True,return\\_estimator=True,...n\\_jobs=-1...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/- std. dev.: &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].mean():.3f}+/- &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].std():.3f}&quot;...)Balanced accuracy mean +/- std. dev.: 0.724 +/- 0.042\n```\nThe cross-validation performance looks good, but evaluating the classifiers\non the left-out data shows a different picture:\n```\n&gt;&gt;&gt;scores=[]&gt;&gt;&gt;forfold\\_id,cv\\_modelinenumerate(cv\\_results[&quot;estimator&quot;]):...scores.append(...balanced\\_accuracy\\_score(...y\\_left\\_out,cv\\_model.predict(X\\_left\\_out)...)...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/- std. dev.: &quot;...f&quot;{np.mean(scores):.3f}+/-{np.std(scores):.3f}&quot;...)Balanced accuracy mean +/- std. dev.: 0.698 +/- 0.014\n```\nWe see that the performance is now worse than the cross-validated performance.\nIndeed, the data leakage gave us too optimistic results due to the reason\nstated earlier in this section.\nWe will now illustrate the correct pattern to use. Indeed, as in scikit-learn,\nusing a[`Pipeline`](references/generated/imblearn.pipeline.Pipeline.html#imblearn.pipeline.Pipeline)avoids to make any data leakage\nbecause the resampling will be delegated to imbalanced-learn and does not\nrequire any manual steps:\n```\n&gt;&gt;&gt;fromimblearn.pipelineimportmake\\_pipeline&gt;&gt;&gt;model=make\\_pipeline(...RandomUnderSampler(random\\_state=0),...HistGradientBoostingClassifier(random\\_state=0)...)&gt;&gt;&gt;cv\\_results=cross\\_validate(...model,X,y,scoring=&quot;&quot;balanced\\_accuracy&quot;&quot;,...return\\_train\\_score=True,return\\_estimator=True,...n\\_jobs=-1...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/- std. dev.: &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].mean():.3f}+/- &quot;...f&quot;{cv\\_results[&#39;&#39;test\\_score&#39;&#39;].std():.3f}&quot;...)Balanced accuracy mean +/- std. dev.: 0.732 +/- 0.019\n```\nWe observe that we get good statistical performance as well. However, now we\ncan check the performance of the model from each cross-validation fold to\nensure that we have similar performance:\n```\n&gt;&gt;&gt;scores=[]&gt;&gt;&gt;forfold\\_id,cv\\_modelinenumerate(cv\\_results[&quot;estimator&quot;]):...scores.append(...balanced\\_accuracy\\_score(...y\\_left\\_out,cv\\_model.predict(X\\_left\\_out)...)...)&gt;&gt;&gt;print(...f&quot;Balanced accuracy mean +/...",
      "url": "https://imbalanced-learn.org/stable/common_pitfalls.html"
    },
    {
      "title": "How to Select Your Final Models in a Kaggle Competition",
      "text": "Did your rank just drop sharp in the private leaderboard in a Kaggle\u00a0competition?\n[![picard palm](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)\nI\u2019ve been through that, too. We all learn about overfitting when we started machine learning,\u00a0but Kaggle makes\u00a0you really feel the pain of overfitting.\u00a0Should I have been more careful in the [Higgs Boson Machine Learning competition](http://www.kaggle.com/c/higgs-boson/), I would have selected a solution that would gave me a rank 4 than rank 22.\nI vow to come out with some principles systematically select\u00a0final models. Here are the lessons learnt:\n- **Always do cross-validation to get a reliable metric.**\u00a0If you don\u2019t, the validation score you get on a single\u00a0validation set\u00a0is unlikely to reflect the model performance in general. Then, you will likely see\u00a0a model improvement in that single validation set, but actually performs worse in general. **_Keep in mind the CV score can be optimistic, but your model is still overfitting._**\n- **Trust your CV\u00a0score, and not LB\u00a0score.** The leaderboard score\u00a0is scored only on a small percentage of the full test set. In some cases, it\u2019s only a few hundred test cases. Your cross-validation score will be much more reliable in general.\n- If your CV score is not stable (perhaps due to ensembling methods), you can\u00a0run your CV with more folds and multiple times to\u00a0take average.\n- If a single\u00a0CV\u00a0run is very slow, use a subset of the data to run\u00a0the CV. This will help your CV loop to run faster. Of course, the subset should not be too small or else the CV score will not be representative.\n- **For the final 2 models, pick very different models.** Picking two very similar solutions means that your solutions either fail together or win together, effectively meaning that you only pick one model. You should reduce your risk by picking two confident but very different models.\u00a0_**You should not depend on the leaderboard score at all.**_\n- Try to group your solutions by methodologies. Then, pick the best CV score model from each group. Then compare these best candidates of each group, pick two.\n- Example: I have different groups 1)\u00a0Bagging of SVMs 2) RandomForest 3) Neural Networks 4) LinearModels. Then, each group should produce one single best model, then you pick 2 out of these.\n- **Pick a robust methodology.** Here is the tricky part\u00a0which depends on experience, even if you have done cross validation, you can still get burned:\u00a0Sketchy methods of improving the CV score like\u00a0making cubic features, cubic root features, boosting like crazy, magical\u00a0numbers(without understanding it), etc, will likely be a bad model to pick even if the CV score is good. Unfortunately, you will probably have to make this mistake once to know what this means. =\\]\nApplying the above\u00a0principles to the recent competition\u00a0[Africa Soil Property Prediction Challenge](http://www.kaggle.com/c/afsis-soil-properties), plus a bit of luck, I picked the top 1 and top 2 models.\n[![top score](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)\nSorted by private score\nI ended up Top 10% with a rank of\u00a090 by spending just\u00a0a week time and mostly in Mexico in a vacation. I guess,\u00a0not too bad?",
      "url": "https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - korem-lab/RebalancedCV\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/korem-lab/rebalancedcv)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/korem-lab/rebalancedcv)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=korem-lab/RebalancedCV)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[korem-lab](https://github.com/korem-lab)/**[RebalancedCV](https://github.com/korem-lab/RebalancedCV)**Public\n* [Notifications](https://github.com/login?return_to=/korem-lab/RebalancedCV)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/korem-lab/RebalancedCV)\n* [Star4](https://github.com/login?return_to=/korem-lab/RebalancedCV)\n### License\n[BSD-3-Clause license](https://github.com/korem-lab/RebalancedCV/blob/main/LICENSE)\n[4stars](https://github.com/korem-lab/RebalancedCV/stargazers)[0forks](https://github.com/korem-lab/RebalancedCV/forks)[Branches](https://github.com/korem-lab/RebalancedCV/branches)[Tags](https://github.com/korem-lab/RebalancedCV/tags)[Activity](https://github.com/korem-lab/RebalancedCV/activity)\n[Star](https://github.com/login?return_to=/korem-lab/RebalancedCV)\n[Notifications](https://github.com/login?return_to=/korem-lab/RebalancedCV)You must be signed in to change notification settings\n# korem-lab/RebalancedCV\nmain\n[Branches](https://github.com/korem-lab/RebalancedCV/branches)[Tags](https://github.com/korem-lab/RebalancedCV/tags)\n[](https://github.com/korem-lab/RebalancedCV/branches)[](https://github.com/korem-lab/RebalancedCV/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[24 Commits](https://github.com/korem-lab/RebalancedCV/commits/main/)\n[](https://github.com/korem-lab/RebalancedCV/commits/main/)\n|\n[.github/workflows](https://github.com/korem-lab/RebalancedCV/tree/main/.github/workflows)\n|\n[.github/workflows](https://github.com/korem-lab/RebalancedCV/tree/main/.github/workflows)\n|\n|\n|\n[ci](https://github.com/korem-lab/RebalancedCV/tree/main/ci)\n|\n[ci](https://github.com/korem-lab/RebalancedCV/tree/main/ci)\n|\n|\n|\n[rebalancedcv](https://github.com/korem-lab/RebalancedCV/tree/main/rebalancedcv)\n|\n[rebalancedcv](https://github.com/korem-lab/RebalancedCV/tree/main/rebalancedcv)\n|\n|\n|\n[vignettes](https://github.com/korem-lab/RebalancedCV/tree/main/vignettes)\n|\n[vignettes](https://github.com/korem-lab/RebalancedCV/tree/main/vignettes)\n|\n|\n|\n[.DS\\_Store](https://github.com/korem-lab/RebalancedCV/blob/main/.DS_Store)\n|\n[.DS\\_Store](https://github.com/korem-lab/RebalancedCV/blob/main/.DS_Store)\n|\n|\n|\n[LICENSE](https://github.com/korem-lab/RebalancedCV/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/korem-lab/RebalancedCV/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/korem-lab/RebalancedCV/blob/main/README.md)\n|\n[README.md](https://github.com/korem-lab/RebalancedCV/blob/main/README.md)\n|\n|\n|\n[setup.py](https://github.com/korem-lab/RebalancedCV/blob/main/setup.py)\n|\n[setup.py](https://github.com/korem-lab/RebalancedCV/blob/main/setup.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# **RebalancedCV**\n[](#rebalancedcv)\n[![main](https://github.com/korem-lab/RebalancedCV/actions/workflows/main.yml/badge.svg)](https://github.com/korem-lab/RebalancedCV/actions/workflows/main.yml)\n[![](https://github.com/korem-lab/RebalancedCV/raw/main/vignettes/RLOOCV-logo.png)](https://github.com/korem-lab/RebalancedCV/blob/main/vignettes/RLOOCV-logo.png)\nThis is a python package designed to facilitate correcting for distributional bias during cross valiation. It was recently shown that removing a fraction of a dataset into a testing fold can artificially create a shift in label averages across training folds that is inversely correlated with that of their corresponding test folds. We have demonstrated that most machine learning models' results suffer from this bias, which this package resolves by subsampling points from within the trianing set to remove any differences in label average across training folds. to begin using RebalancedCV, we recommend reading it's[documentation pages](https://korem-lab.github.io/RebalancedCV/).\nAll classes from this package provide train/test indices to split data in train/test sets while rebalancing the training set to account for distributional bias. This package is designed to enable automated rebalancing for the cross-valition implementations in formats similar to scikit-learn's`LeaveOneOut`,`StratifiedKFold`, and`LeavePOut`, through the`RebalancedCV`classes`RebalancedLeaveOneOut`,`RebalancedLeaveOneOutRegression`,`RebalancedKFold`, and`RebalancedLeavePOut`. These Rebalanced classes are designed to work in the exact same code structure and implementation use cases as their scikit-learn equivalents, with the only difference being a subsampling within the provided training indices.\nFor any support using RebalancedCV, please use our[issues page](https://github.com/korem-lab/RebalancedCV/issues)or email:[gia2105@columbia.edu](mailto:gia2105@columbia.edu).\n## **Installation**\n[](#installation)\n```\npip install RebalancedCV\n```\nThe dependencies for RebalancedCV are python, numpy, and scikit-learn. Is has been developped and tested using python 3.6 - 3.12. Only standard hardware is required for RebalancedCV. The typical install time for RebalancedCV is less that 15 seconds.\n## **Example**\n[](#example)\nWe demonstrate the following snippet of code to utilize out rebalanced leave-one-out implementation, using an observation matrix`X`and a binary outcome vector`y`. We demonstrate it using scikit-learn's`LogisticRegressionCV`, although this can be replaced with any training/tuning/predicting scheme. The expected runtime for the RebalancedCV package's operations are less than 5 seconds, wihle the overall example is expected to complete in less than one minute on most machines.\n```\nimportnumpyasnpimportpandasaspdfromsklearn.linear\\_modelimportLogisticRegressionCVfromsklearn.model\\_selectionimportLeaveOneOutfromrebalancedcvimportRebalancedLeaveOneOutfromsklearn.metricsimportroc\\_auc\\_scorenp.random.seed(1)## given some random `X` matrix, and a `y` binary vectorX=np.random.rand(100,10)y=np.random.rand(100)&gt;0.5## Leave-one-out evaluationloo=LeaveOneOut()loocv\\_predictions=[LogisticRegressionCV()\\\\\n.fit(X[train\\_index],y[train\\_index])\\\\\n.predict\\_proba(X[test\\_index]\n)[:,1][0]fortrain\\_index,test\\_indexinloo.split(X,y) ]## Since all the data is random, a fair evaluation## should yield au auROC close to 0.5print('Leave One Out auROC: {:.2f}'\\\\\n.format(roc\\_auc\\_score(y,loocv\\_predictions) ) )## Rebalanced leave-one-out evaluationrloo=RebalancedLeaveOneOut()rloocv\\_predictions=[LogisticRegressionCV()\\\\\n.fit(X[train\\_index],y[train\\_index])\\\\\n.predict\\_proba(X[test\\_index]\n)[:,1][0]fortrain\\_index,...",
      "url": "https://github.com/korem-lab/rebalancedcv"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    },
    {
      "title": "General Tips for participating Kaggle Competitions",
      "text": "# General Tips for participating Kaggle Competitions\n\n\u2022\n\n196 likes\u202286,040 views\n\n![Mark Peng](https://cdn.slidesharecdn.com/profile-photo-markpeng-48x48.jpg?cb=1709552824)\n\n[Mark Peng](https://www.slideshare.net/markpeng?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview) Follow\n\nThe slides of a talk at Spark Taiwan User Group to share my experience and some general tips for participating kaggle competitions. Read less\n\nRead more\n\nReport\n\nShare\n\nReport\n\nShare\n\n1 of 74\n\nDownload nowDownload to read offline\n\n![General Tips for participating Competitions Mark Peng 2015.12.16 @ Spark Taiwan User Group https://tw.linkedin.com/in/markpeng  ](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/75/General-Tips-for-participating-Kaggle-Competitions-1-2048.jpg)\n\n![Kaggle Profile (Master Tier) https://www.kaggle.com/markpeng 2 Big Data Scientist  ](data:image/gif;base64)\n\n![3 Things I Want to Share \u2022 Quick overview to Kaggle rules \u2022 Why cross-validation matters \u2022 Mostly used ML models \u2022 Feature engineering methods \u2022 Ensemble learning \u2022 Team up \u2022 Recommended books, MOOCs and resources  ](data:image/gif;base64)\n\n![Kaggle Competition Dataset and Rules 4 Training Dataset Private LBPublic LB Validation feedback but sometimes misleading Testing Dataset Might be different from public LB (used to determine final prize winners!)  ](data:image/gif;base64)\n\n![5 How to become a Kaggle Master \u2022 To achieve Master tier, you must fulfill 2 criteria \u2022 Consistency: at least 2 Top 10% finishes in public competitions \u2022 Excellence: at least 1 of those finishes in the top 10 positions \u2022 Note that not all competitions count toward earning Master tier! Reference: https://www.kaggle.com/wiki/UserRankingAndTierSystem  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 6 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![Cross Validation The key to avoid overfitting 7  ](data:image/gif;base64)\n\n![Underfitting and Overfitting We want to find a model with lowest generalization error (hopefully) 8 Model Complexity HighLow PredictionError Training Error Validation Error (Local CV) Testing Error (Private LB) High Variance Low Bias High Bias Low Variance Lowest Generalization Error OverfittingUnderfitting  ](data:image/gif;base64)\n\n![9 Big Shake Up on Private LB! Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/leaderboard/private  ](data:image/gif;base64)\n\n![10 Who is the King of Overfitting? Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission Public LB RMSE: Private LB RMSE: They even wrote a post to show off their perfect overfitting! Num. of Features: 41 Training Data Size: 137 Testing Data Size: 10,000  ](data:image/gif;base64)\n\n![11 K-fold Cross Validation (K = 5) fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 Round 1 Round 2 Round 3 Round 4 Round 5 Validation Data Training Data score(CV) = the average of evaluation scores from each fold You can also repeat the process many times!  ](data:image/gif;base64)\n\n![12 K-fold Cross Validation Tips \u2022 It is normal to experience big shake up on private LB if not using local CV correctly \u2022 5 or 10 folds are not always the best choices (you need to consider the cost of computation time for training models) \u2022 Which K to choose? \u2022 Depends on your data \u2022 Mimic the ratio of training and testing in validation process \u2022 Find a K with lowest gap between local CV and public LB scores \u2022 Standard deviation of K-fold CV score matters more than mean! \u2022 Stratified K-fold CV is important for imbalanced dataset, especially for classification problems  ](data:image/gif;base64)\n\n![13 Stratified K-fold Cross Validation (K = 5) Round 1 Round 2 Round 3 Round 4 Round 5Class Distributions F M Keep the distribution of classes in each fold Validation Data Training Data  ](data:image/gif;base64)\n\n![14 Should I Trust Public LB? \u2022 Yes if you can find a K-fold CV that follows the same trend with public LB \u2022 High positive correlation between local CV and public LB \u2022 The score increases and decreases in both local CV and public LB \u2022 Trust more in your local CV!  ](data:image/gif;base64)\n\n![Data Cleaning Making data more analyzable 15  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 16 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![17 Data Cleaning Techniques \u2022 Data cleaning is the removal of duplicates, useless data, or fixing of missing data \u2022 Reduce dimensional complexity of dataset \u2022 Make training faster without hurting the performance \u2022 Apply imputation methods to help (hopefully) utilize incomplete rows \u2022 Incomplete rows may contain relevant features (don\u2019t just drop them!) \u2022 In the risk of distorting original data, so be cautious!  ](data:image/gif;base64)\n\n![18 Data Cleaning Techniques (cont.) \u2022 Remove duplicate features \u2022 Columns having the same value distribution or variance \u2022 Only need to keep one of them \u2022 Remove constant features \u2022 Columns with only one unique value \u2022 R: sapply(data, function(x) length(unique(x))) \u2022 Remove features with near-zero variance \u2022 R: nearZeroVar(data, saveMetrics=T) in caret package \u2022 Be sure to know what you are doing before removing any features  ](data:image/gif;base64)\n\n![19 Data Cleaning Techniques (cont.) \u2022 Some machine learning tools cannot accept NAs in the input \u2022 Encode missing values to avoid NAs \u2022 Binary features \u2022 -1 for negatives, 0 for missing values and 1 for positives \u2022 Categorical features \u2022 Encode as an unique category \u2022 \u201cUnknown\u201d, \u201cMissing\u201d, \u2026. \u2022 Numeric features \u2022 Encode as a big positive or negative number \u2022 999, -99999, \u2026.  ](data:image/gif;base64)\n\n![20 Data Cleaning Techniques (cont.) \u2022 Basic ways to impute missing values \u2022 mean, median or most frequent value of feature \u2022 R: impute(x, fun = mean) in Hmisc package \u2022 Python: Imputer(strategy='mean', axis=0) in scikit-learn package  ](data:image/gif;base64)\n\n![21 Data Cleaning Techniques (cont.) \u2022 Advanced: multiple imputation \u2022 Impute incomplete columns based on other columns in the data \u2022 R: mice package (Multivariate Imputation by Chained Equations) \u2022 Imputation would not always give you positive improvements, thus you have to validate it cautiously  ](data:image/gif;base64)\n\n![Mostly Used Models What models to use? 22  ](data:image/gif;base64)\n\n![Model Type Name R Python Regression Linear Regression \u2022 glm, glmnet \u2022 sklearn.linear_model.LinearRegression Ridge Regression \u2022 glmnet \u2022 sklearn.linear_model.Ridge Lasso Regression \u2022 glmnet \u2022 sklearn.linear_model.Lasso Instance-based K-nearest Neighbor (KNN) \u2022 knn \u2022 sklearn.neighbors.KNeighborsClassifier Support Vector Machines (SVM) \u2022 svm {e1071} \u2022 LiblinearR \u2022 sklearn.svm.SVC, sklearn.svm.SVR \u2022 sklearn.svm.LinearSVC, sklearn.svm.LinearSVR Hyperplane-based Naive Bayes \u2022 naiveBayes {e1071} \u2022 sklearn.naive_bayes.GaussianNB \u2022 sklearn.naive_bayes.MultinomialNB \u2022 sklearn.naive_bayes.BernoulliNB Logistic Regression \u2022 glm, glmnet \u2022 LiblinearR \u2022 sklearn.linear_model.LogisticRegression Ensemble Trees Random Forests \u2022 randomForest \u2022 sklearn.ensemble.RandomForestClassifier \u2022 sklearn.ensemble.RandomForestRegressor Extremely Randomized Trees \u2022 extraTrees \u2022 sklearn.ensemble.ExtraTreesClassifier \u2022 sklearn.ensemble.ExtraTreesRegressor Gradient Boosting Machines (GBM) \u2022 gbm \u2022 xgboost \u2022 sklearn.ensemble.GradientBoostingClassifier \u2022 sklearn.ensemble.GradientBoostingRegressor \u2022 xgboost Neural Network Multi-layer Neural Netwo...",
      "url": "https://www.slideshare.net/slideshow/general-tips-for-participating-kaggle-competitions/56209561"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    },
    {
      "title": "Tips and tricks to win kaggle data science competitions",
      "text": "# Tips and tricks to win kaggle data science competitions\n\nDownload as PPTX, PDF65 likes14,661 viewsAI-enhanced description\n\n[D\\\nDarius Baru\u0161auskas](https://www.slideshare.net/DariusBaruauskas)\n\nThe document serves as a guide for participating in predictive modeling competitions on platforms like Kaggle, emphasizing strategies for building competitive models and avoiding common pitfalls. It highlights the importance of cross-validation, feature engineering, and effective ensembling techniques while cautioning against overfitting and reliance on public leaderboard scores. Key tips include trusting cross-validation over public scores, using consistent CV splits, and the necessity of a robust data processing pipeline for improved model performance.\n\n[Data & Analytics](https://www.slideshare.net/category/data-analytics)\n\nRelated topics:\n\n[Data Science Insights](https://www.slideshare.net/tag/data-science) \u2022 [Predictive Modeling](https://www.slideshare.net/tag/predictive-modeling)\n\nRead more\n\n1 of 38\n\nDownload nowDownloaded 606 times\n\n[1](https://www.slideshare.net/www.slideshare.net#1)\n\n[2](https://www.slideshare.net/www.slideshare.net#2)\n\n[3](https://www.slideshare.net/www.slideshare.net#3)\n\n[4](https://www.slideshare.net/www.slideshare.net#4) Most read\n\n[5](https://www.slideshare.net/www.slideshare.net#5)\n\n[6](https://www.slideshare.net/www.slideshare.net#6)\n\n[7](https://www.slideshare.net/www.slideshare.net#7)\n\n[8](https://www.slideshare.net/www.slideshare.net#8)\n\n[9](https://www.slideshare.net/www.slideshare.net#9)\n\n[10](https://www.slideshare.net/www.slideshare.net#10)\n\n[11](https://www.slideshare.net/www.slideshare.net#11)\n\n[12](https://www.slideshare.net/www.slideshare.net#12)\n\n[13](https://www.slideshare.net/www.slideshare.net#13) Most read\n\n[14](https://www.slideshare.net/www.slideshare.net#14)\n\n[15](https://www.slideshare.net/www.slideshare.net#15)\n\n[16](https://www.slideshare.net/www.slideshare.net#16)\n\n[17](https://www.slideshare.net/www.slideshare.net#17)\n\n[18](https://www.slideshare.net/www.slideshare.net#18)\n\n[19](https://www.slideshare.net/www.slideshare.net#19) Most read\n\n[20](https://www.slideshare.net/www.slideshare.net#20)\n\n[21](https://www.slideshare.net/www.slideshare.net#21)\n\n[22](https://www.slideshare.net/www.slideshare.net#22)\n\n[23](https://www.slideshare.net/www.slideshare.net#23)\n\n[24](https://www.slideshare.net/www.slideshare.net#24)\n\n[25](https://www.slideshare.net/www.slideshare.net#25)\n\n[26](https://www.slideshare.net/www.slideshare.net#26)\n\n[27](https://www.slideshare.net/www.slideshare.net#27)\n\n[28](https://www.slideshare.net/www.slideshare.net#28)\n\n[29](https://www.slideshare.net/www.slideshare.net#29)\n\n[30](https://www.slideshare.net/www.slideshare.net#30)\n\n[31](https://www.slideshare.net/www.slideshare.net#31)\n\n[32](https://www.slideshare.net/www.slideshare.net#32)\n\n[33](https://www.slideshare.net/www.slideshare.net#33)\n\n[34](https://www.slideshare.net/www.slideshare.net#34)\n\n[35](https://www.slideshare.net/www.slideshare.net#35)\n\n[36](https://www.slideshare.net/www.slideshare.net#36)\n\n[37](https://www.slideshare.net/www.slideshare.net#37)\n\n[38](https://www.slideshare.net/www.slideshare.net#38)\n\n## More Related Content\n\nPDF\n\nGeneral Tips for participating Kaggle Competitions\n\n[Mark Peng](https://www.slideshare.net/markpeng)\n\nPDF\n\nTips for data science competitions\n\n[Owen Zhang](https://www.slideshare.net/OwenZhang2)\n\nPPTX\n\nKaggle winning solutions: Retail Sales Forecasting\n\n[Yan Xu](https://www.slideshare.net/xuyangela)\n\nPDF\n\nLearned Embeddings for Search and Discovery at Instacart\n\n[Sharath Rao](https://www.slideshare.net/SharathRao6)\n\nPDF\n\nKaggle Winning Solution Xgboost algorithm -- Let us learn from its author\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nPDF\n\nKaggle presentation\n\n[HJ van Veen](https://www.slideshare.net/HJvanVeen)\n\nPPTX\n\nLearning machine learning with Yellowbrick\n\n[Rebecca Bilbro](https://www.slideshare.net/RebeccaBilbro)\n\nPDF\n\nAn Introduction to Supervised Machine Learning and Pattern Classification: Th...\n\n[Sebastian Raschka](https://www.slideshare.net/SebastianRaschka)\n\nGeneral Tips for participating Kaggle Competitions\n\n[Mark Peng](https://www.slideshare.net/markpeng)\n\nTips for data science competitions\n\n[Owen Zhang](https://www.slideshare.net/OwenZhang2)\n\nKaggle winning solutions: Retail Sales Forecasting\n\n[Yan Xu](https://www.slideshare.net/xuyangela)\n\nLearned Embeddings for Search and Discovery at Instacart\n\n[Sharath Rao](https://www.slideshare.net/SharathRao6)\n\nKaggle Winning Solution Xgboost algorithm -- Let us learn from its author\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nKaggle presentation\n\n[HJ van Veen](https://www.slideshare.net/HJvanVeen)\n\nLearning machine learning with Yellowbrick\n\n[Rebecca Bilbro](https://www.slideshare.net/RebeccaBilbro)\n\nAn Introduction to Supervised Machine Learning and Pattern Classification: Th...\n\n[Sebastian Raschka](https://www.slideshare.net/SebastianRaschka)\n\n### What's hot (20)\n\nPDF\n\nWinning data science competitions, presented by Owen Zhang\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nPDF\n\nFeature Engineering\n\n[HJ van Veen](https://www.slideshare.net/HJvanVeen)\n\nPDF\n\nXGBoost & LightGBM\n\n[Gabriel Cypriano Saca](https://www.slideshare.net/GabrielCyprianoSaca)\n\nPPTX\n\nHow to Win Machine Learning Competitions ?\n\n[HackerEarth](https://www.slideshare.net/HackerEarth)\n\nPDF\n\nWinning Kaggle 101: Introduction to Stacking\n\n[Ted Xiao](https://www.slideshare.net/TedXiao)\n\nPDF\n\nIntroduction to XGBoost\n\n[Joonyoung Yi](https://www.slideshare.net/ssuser62b35f)\n\nPDF\n\nXgboost\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nPDF\n\nFeature Engineering - Getting most out of data for predictive models\n\n[Gabriel Moreira](https://www.slideshare.net/gabrielspmoreira)\n\nPPTX\n\nFeature Engineering\n\n[odsc](https://www.slideshare.net/odsc)\n\nPDF\n\nFeature Engineering\n\n[Sri Ambati](https://www.slideshare.net/0xdata)\n\nPDF\n\nFeature Engineering - Getting most out of data for predictive models - TDC 2017\n\n[Gabriel Moreira](https://www.slideshare.net/gabrielspmoreira)\n\nPPTX\n\nMachine Learning - Dataset Preparation\n\n[Andrew Ferlitsch](https://www.slideshare.net/AndrewFerlitsch)\n\nPDF\n\nDeep Dive into Hyperparameter Tuning\n\n[Shubhmay Potdar](https://www.slideshare.net/ShubhmayPotdar1)\n\nPDF\n\nLSTM\n\n[\u4f73\u84c9 \u502a](https://www.slideshare.net/ssuser2e52e8)\n\nPDF\n\nWinning Data Science Competitions\n\n[Jeong-Yoon Lee](https://www.slideshare.net/jeongyoonlee)\n\nPPTX\n\nData Analysis: Evaluation Metrics for Supervised Learning Models of Machine L...\n\n[Md. Main Uddin Rony](https://www.slideshare.net/MdMainUddinRony)\n\nPDF\n\nUnderstanding Bagging and Boosting\n\n[Mohit Rajput](https://www.slideshare.net/mohitrajput901)\n\nPDF\n\nData Science - Part V - Decision Trees & Random Forests\n\n[Derek Kane](https://www.slideshare.net/DerekKane)\n\nPPTX\n\nAttention Is All You Need\n\n[Illia Polosukhin](https://www.slideshare.net/ilblackdragon)\n\nPPTX\n\nMachine Learning - Splitting Datasets\n\n[Andrew Ferlitsch](https://www.slideshare.net/AndrewFerlitsch)\n\nWinning data science competitions, presented by Owen Zhang\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nFeature Engineering\n\n[HJ van Veen](https://www.slideshare.net/HJvanVeen)\n\nXGBoost & LightGBM\n\n[Gabriel Cypriano Saca](https://www.slideshare.net/GabrielCyprianoSaca)\n\nHow to Win Machine Learning Competitions ?\n\n[HackerEarth](https://www.slideshare.net/HackerEarth)\n\nWinning Kaggle 101: Introduction to Stacking\n\n[Ted Xiao](https://www.slideshare.net/TedXiao)\n\nIntroduction to XGBoost\n\n[Joonyoung Yi](https://www.slideshare.net/ssuser62b35f)\n\nXgboost\n\n[Vivian S. Zhang](https://www.slideshare.net/ShangxuanZhang)\n\nFeature Engineering - Getting most out of data for predictive models\n\n[Gabriel Moreira](https://www.slideshare.net/gabrielspmoreira)\n\nFeature Engineering\n\n[odsc](https://www.slideshare.net/odsc)\n\nFeature Engineering\n\n[Sri Ambati](https://www.slideshare.net/0xdata)\n\nFeature Engineering - Getting most out o...",
      "url": "https://www.slideshare.net/DariusBaruauskas/tips-and-tricks-to-win-kaggle-data-science-competitions?next_slideshow=1"
    }
  ]
}