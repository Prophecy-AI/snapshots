{
  "query": "What hyperparameter tuning strategies work best for XGBoost and Random Forest on small tabular datasets like Titanic?",
  "results": [
    {
      "title": "ANSWER",
      "text": "For a tiny, fully\u2011tabular problem such as the Titanic data (\u2248\u202f800\u202frows,\u202f\u2248\u202f10\u202ffeatures) the most effective tuning strategy is to keep the search space small, use a robust search method (e.g., Bayesian or model\u2011based optimization), and guard against over\u2011fitting with cross\u2011validation and built\u2011in randomness.\n\n**XGBoost** \u2013 focus on the bias\u2011variance knobs described in the official guide:\u202f`max_depth`, `min_child_weight`, `gamma` (tree complexity) and the sampling parameters `subsample` and `colsample_bytree` to inject randomness and curb over\u2011fit on limited data\u3010Result\u202f1\u3011.\u202fA practical workflow is to run a Bayesian optimizer (Optuna, Hyperopt, Ray Tune, etc.) over this compact space, letting the optimizer evaluate a few hundred trials with early\u2011stopping on a validation fold.\u202fThe XGBoost\u2011specific \u201ctune\u201d portal lists ready\u2011made Optuna, Hyperopt and other Bayesian recipes that work well on small datasets\u3010Result\u202f2\u3011, and an Optuna\u2011based tutorial further emphasizes starting with the two groups of hyperparameters (tree vs. boosting) and narrowing the ranges for `learning_rate` (\u2248\u202f0.01\u20130.3), `n_estimators` (\u2248\u202f50\u2013300) and `max_depth` (\u2264\u202f5) to avoid overly complex trees\u3010Result\u202f10\u3011.\n\n**Random Forest** \u2013 the default settings are often adequate, but modest tuning can lift performance.\u202fKey parameters are `n_estimators`, `max_features` (or `max_features`\u202f=\u202f\u221ap for small p), `min_samples_leaf` and `max_depth`.\u202fThe 2018 study on RF tuning recommends model\u2011based optimization (MBO) as a highly efficient strategy and provides the `tuneRanger` R package that automatically searches this reduced space with Bayesian surrogates\u3010Result\u202f6\u3011.\u202fOn a dataset the size of Titanic, a 5\u2011fold CV evaluation of 30\u201350 MBO iterations typically yields a well\u2011regularized forest without over\u2011fitting.\n\n**Bottom line** \u2013 for small tabular data, limit the hyperparameter grid to the most influential complexity controls, use a Bayesian/MBO search (Optuna, Hyperopt, tuneRanger) with cross\u2011validation and early\u2011stopping, and rely on sampling\u2011based regularization (`subsample`, `colsample_bytree` for XGBoost; `max_features`, `min_samples_leaf` for Random Forest) to keep the models from memorising the few training points.",
      "url": ""
    },
    {
      "title": "Notes on Parameter Tuning \uf0c1",
      "text": "Notes on Parameter Tuning &mdash; xgboost 3.1.1 documentation\n* [](../index.html)\n* [XGBoost Tutorials](index.html)\n* Notes on Parameter Tuning\n* [View page source](../_sources/tutorials/param_tuning.rst.txt)\n# Notes on Parameter Tuning[\uf0c1](#notes-on-parameter-tuning)\nParameter tuning is a dark art in machine learning, the optimal parameters\nof a model can depend on many scenarios. So it is impossible to create a\ncomprehensive guide for doing so.\nThis document tries to provide some guideline for parameters in XGBoost.\n## Understanding Bias-Variance Tradeoff[\uf0c1](#understanding-bias-variance-tradeoff)\nIf you take a machine learning or statistics course, this is likely to be one\nof the most important concepts.\nWhen we allow the model to get more complicated (e.g. more depth), the model\nhas better ability to fit the training data, resulting in a less biased model.\nHowever, such complicated model requires more data to fit.\nMost of parameters in XGBoost are about bias variance tradeoff. The best model\nshould trade the model complexity with its predictive power carefully.[Parameters Documentation](../parameter.html)will tell you whether each parameter\nwill make the model more conservative or not. This can be used to help you\nturn the knob between complicated model and simple model.\n## Control Overfitting[\uf0c1](#control-overfitting)\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\nThere are in general two ways that you can control overfitting in XGBoost:\n* The first way is to directly control model complexity.\n* This includes`max\\_depth`,`min\\_child\\_weight`,`gamma`,`max\\_cat\\_threshold`and other similar regularization parameters. See[XGBoost Parameters](../parameter.html)for a comprehensive\nset of parameters.\n* Set a constant`base\\_score`based on your own criteria. See[Intercept](intercept.html)for more info.\n* The second way is to add randomness to make training robust to noise.\n* This includes`subsample`and`colsample\\_bytree`, which may be used with boosting\nRF`num\\_parallel\\_tree`.\n* You can also reduce stepsize`eta`, possibly with a training callback. Remember to\nincrease`num\\_round`when you do so.\n## Handle Imbalanced Dataset[\uf0c1](#handle-imbalanced-dataset)\nFor common cases such as ads clickthrough log, the dataset is extremely imbalanced.\nThis can affect the training of XGBoost model, and there are two ways to improve it.\n* If you care only about the overall performance metric (AUC) of your prediction\n* Balance the positive and negative weights via`scale\\_pos\\_weight`\n* Use AUC for evaluation\n* If you care about predicting the right probability\n* In such a case, you cannot re-balance the dataset\n* Set parameter`max\\_delta\\_step`to a finite number (say 1) to help convergence\n## Use Hyper Parameter Optimization (HPO) Frameworks[\uf0c1](#use-hyper-parameter-optimization-hpo-frameworks)\nTuning models is a sophisticated task and there are advanced frameworks to help you. For\nexamples, some meta estimators in scikit-learn like[`sklearn.model\\_selection.HalvingGridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV)can help guide the search\nprocess. Optuna is another great option and there are many more based on different\nbranches of statistics.\n## Know Your Data[\uf0c1](#know-your-data)\nIt cannot be stressed enough the importance of understanding the data, sometimes that\u2019s\nall it takes to get a good model. Many solutions use a simple XGBoost tree model without\nmuch tuning and emphasize the data pre-processing step. XGBoost can help feature selection\nby providing both a global feature importance score and sample feature importance with\nSHAP value. Also, there are parameters specifically targeting categorical features, and\ntasks like survival and ranking. Feel free to explore them.\n## Reducing Memory Usage[\uf0c1](#reducing-memory-usage)\nIf you are using a HPO library like[`sklearn.model\\_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV),\nplease control the number of threads it can use. It\u2019s best to let XGBoost to run in\nparallel instead of asking> GridSearchCV\nto run multiple experiments at the same\ntime. For instance, creating a fold of data for cross validation can consume a significant\namount of memory:\n```\n# This creates a copy of dataset. X and X\\_train are both in memory at the same time.# This happens for every thread at the same time if you run `GridSearchCV` with# `n\\_jobs` larger than 1X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X,y)\n```\n```\ndf=pd.DataFrame()# This creates a new copy of the dataframe, even if you specify the inplace parameternew\\_df=df.drop(...)\n```\n```\narray=np.array(...)# This may or may not make a copy of the data, depending on the type of the dataarray.astype(np.float32)\n```\n```\n# np by default uses double, do you actually need it?array=np.array(...)\n```\nYou can find some more specific memory reduction practices scattered through the documents\nFor instances:[Distributed XGBoost with Dask](dask.html),[XGBoost GPU Support](../gpu/index.html). However, before going into\nthese, being conscious about making data copies is a good starting point. It usually\nconsumes a lot more memory than people expect.",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html"
    },
    {
      "title": "Tune | XGBoosting",
      "text": "Helpful examples for tuning XGBoost model parameters (hyperparameter).\n\nXGBoost hyperparameter tuning involves adjusting specific hyperparameters, such as learning rate, max depth, and number of estimators, to find the optimal settings that improve model performance and predictive accuracy.\n\n| Examples | Tags |\n| --- | --- |\n| [Bayesian Optimization of XGBoost Hyperparameters with Ax](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-ax/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Bayesian Optimization of XGBoost Hyperparameters with bayes\\_opt](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-bayes_opt/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Bayesian Optimization of XGBoost Hyperparameters with hyperopt](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-hyperopt/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Bayesian Optimization of XGBoost Hyperparameters with optuna](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-optuna/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Bayesian Optimization of XGBoost Hyperparameters with Ray Tune](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-ray-tune/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Bayesian Optimization of XGBoost Hyperparameters with scikit-optimize](https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-scikit-optimize/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Grid Search XGBoost Hyperparameters](https://xgboosting.com/grid-search-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Halving Grid Search for XGBoost Hyperparameters](https://xgboosting.com/halving-grid-search-for-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/) |\n| [Halving Random Search for XGBoost Hyperparameters](https://xgboosting.com/halving-random-search-for-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Improve XGBoost Model Accuracy (Skill)](https://xgboosting.com/improve-xgboost-model-accuracy-skill/) | - [Prediction](https://xgboosting.com/prediction/)<br>- [Tune](https://xgboosting.com/tune/) |\n| [Manually Search XGBoost Hyperparameters with For Loops](https://xgboosting.com/manually-search-xgboost-hyperparameters-with-for-loops/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Most Important XGBoost Hyperparameters to Tune](https://xgboosting.com/most-important-xgboost-hyperparameters-to-tune/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Optimal Order for Tuning XGBoost Hyperparameters](https://xgboosting.com/optimal-order-for-tuning-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Random Search XGBoost Hyperparameters](https://xgboosting.com/random-search-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Suggested Ranges for Tuning XGBoost Hyperparameters](https://xgboosting.com/suggested-ranges-for-tuning-xgboost-hyperparameters/) | - [Tune](https://xgboosting.com/tune/)<br>- [Search](https://xgboosting.com/search/) |\n| [Tune \"num\\_boost\\_round\" Parameter to xgboost.train()](https://xgboosting.com/tune-num_boost_round-parameter-to-xgboost.train/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Early Stopping](https://xgboosting.com/early-stopping/) |\n| [Tune XGBoost \"alpha\" Parameter](https://xgboosting.com/tune-xgboost-alpha-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"booster\" Parameter](https://xgboosting.com/tune-xgboost-booster-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/) |\n| [Tune XGBoost \"colsample\\_bylevel\" Parameter](https://xgboosting.com/tune-xgboost-colsample_bylevel-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"colsample\\_bynode\" Parameter](https://xgboosting.com/tune-xgboost-colsample_bynode-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"colsample\\_bytree\" Parameter](https://xgboosting.com/tune-xgboost-colsample_bytree-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"early\\_stopping\\_rounds\" Parameter](https://xgboosting.com/tune-xgboost-early_stopping_rounds-parameter/) | - [Plot](https://xgboosting.com/plot/)<br>- [Regularization](https://xgboosting.com/regularization/)<br>- [Early Stopping](https://xgboosting.com/early-stopping/)<br>- [Tune](https://xgboosting.com/tune/) |\n| [Tune XGBoost \"eta\" Parameter](https://xgboosting.com/tune-xgboost-eta-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"gamma\" Parameter](https://xgboosting.com/tune-xgboost-gamma-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"grow\\_policy\" Parameter](https://xgboosting.com/tune-xgboost-grow_policy-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/) |\n| [Tune XGBoost \"learning\\_rate\" Parameter](https://xgboosting.com/tune-xgboost-learning_rate-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"max\\_bin\" Parameter](https://xgboosting.com/tune-xgboost-max_bin-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"max\\_delta\\_step\" Parameter](https://xgboosting.com/tune-xgboost-max_delta_step-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"max\\_depth\" Parameter](https://xgboosting.com/tune-xgboost-max_depth-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"max\\_leaves\" Parameter](https://xgboosting.com/tune-xgboost-max_leaves-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"min\\_child\\_weight\" Parameter](https://xgboosting.com/tune-xgboost-min_child_weight-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"min\\_split\\_loss\" Parameter](https://xgboosting.com/tune-xgboost-min_split_loss-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot](https://xgboosting.com/plot/) |\n| [Tune XGBoost \"n\\_estimators\" Parameter](https://xgboosting.com/tune-xgboost-n_estimators-parameter/) | - [Tune](https://xgboosting.com/tune/)<br>- [Parameters](https://xgboosting.com/parameters/)<br>- [Plot]...",
      "url": "https://xgboosting.com/tune"
    },
    {
      "title": "Beyond Grid Search: Hypercharge Hyperparameter Tuning for XGBoost",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7c78f7a2929d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Beyond Grid Search: Hypercharge Hyperparameter Tuning for XGBoost\n\n## Using Hyperopt, Optuna, and Ray Tune to Accelerate Machine Learning Hyperparameter Optimization\n\n[![Druce Vertes](https://miro.medium.com/v2/resize:fill:88:88/2*_UoZqNJIrKbOqPvAdAG3pA.png)](https://medium.com/@druce?source=post_page-----7c78f7a2929d--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----7c78f7a2929d--------------------------------)\n\n[Druce Vertes](https://medium.com/@druce?source=post_page-----7c78f7a2929d--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fffea89542b9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d&user=Druce+Vertes&userId=ffea89542b9f&source=post_page-ffea89542b9f----7c78f7a2929d---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7c78f7a2929d--------------------------------)\n\n\u00b7\n\n21 min read\n\n\u00b7\n\nOct 30, 2020\n\n--\n\n1\n\nListen\n\nShare\n\nBayesian optimization of machine learning model hyperparameters works faster and better than grid search. Here\u2019s how we can speed up hyperparameter tuning with 1) Bayesian optimization with Hyperopt and Optuna, running on\u2026 2) the [Ray](https://ray.io/) distributed machine learning framework, with a [unified Ray Tune API to many hyperparameter search algos](https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf) and early stopping schedulers, and\u2026 3) a distributed cluster of cloud instances for even faster tuning.\n\nImage by Author\n\n## Outline:\n\n01. Results\n02. Hyperparameter tuning overview\n03. Bayesian optimization\n04. Early stopping\n05. Implementation details\n06. Baseline linear regression\n07. ElasticNetCV (Linear regression with L1 and L2 regularization)\n08. ElasticNet with GridSearchCV\n09. XGBoost: sequential grid search over hyperparameter subsets with early stopping\n10. XGBoost: Hyperopt and Optuna search algorithms\n11. LightGBM: Hyperopt and Optuna search algorithms\n12. XGBoost on a Ray cluster\n13. LightGBM on a Ray cluster\n14. Concluding remarks\n\n## 1\\. Results\n\nBottom line up front: Here are results on the Ames housing data set, predicting Iowa home prices:\n\nRMSE and search time for XGB and LightGBM using various hyperparameter optimization methodologiesRMSE and fit time for baseline linear models\n\n## Baseline linear models\n\n_Times for single-instance are on a local desktop with 12 threads, comparable to EC2 4xlarge. Times for cluster are on m5.large x 32 (1 head node + 31 workers)._\n\n- We obtain a big speedup when using Hyperopt and Optuna locally, compared to grid search. The sequential search performed about 261 trials, so the XGB/Optuna search performed about 3x as many trials in half the time and got a similar result.\n- The cluster of 32 instances (64 threads) gave a modest RMSE improvement vs. the local desktop with 12 threads. I tried to set this up so we would get some improvement in RMSE vs. local Hyperopt/Optuna (which we did with 2048 trials), and some speedup in training time (which we did not get with 64 threads). It ran twice the number of trials in slightly less than twice the time. The comparison is imperfect, local desktop vs. AWS, running Ray 1.0 on local and 1.1 on the cluster, different number of trials (better hyperparameter configs don\u2019t get early-stopped and take longer to train). But the point was to see what kind of improvement one might obtain in practice, leveraging a cluster vs. a local desktop or laptop. Bottom line, modest benefit here from a 32-node cluster.\n- RMSEs are similar across the board. XGB with 2048 trials is best by a small margin among the boosting models.\n- LightGBM doesn\u2019t offer an improvement over XGBoost here in RMSE or run time. In my experience, LightGBM is often faster, so you can train and tune more in a given time. But we don\u2019t see that here. Possibly XGB interacts better with ASHA early stopping.\n- Similar RMSE between Hyperopt and Optuna. Optuna is consistently faster (up to 35% with LGBM/cluster).\n\nOur simple ElasticNet baseline yields slightly better results than boosting, in seconds. This may be because our feature engineering was intensive and designed to fit the linear model. Not shown, SVR and KernelRidge outperform ElasticNet, and an ensemble improves over all individual algos.\n\nFull notebooks are on [GitHub](https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb).\n\n## 2\\. Hyperparameter Tuning Overview\n\n(If you are not a data scientist ninja, here is some context. If you are, you can safely skip to the _Bayesian Optimization_ section and the implementations below.)\n\nAny sufficiently advanced machine learning model is indistinguishable from magic, and any sufficiently advanced machine learning model needs good tuning.\n\nBacking up a step, here is a typical modeling workflow:\n\n- Exploratory data analysis: understand your data.\n- Feature engineering and feature selection: clean, transform and engineer the best possible features\n- Modeling: model selection and hyperparameter tuning to identify the best model architecture, and ensembling to combine multiple models\n- Evaluation: Describe the out-of-sample error and its expected distribution.\n\nTo minimize the out-of-sample error, you minimize the error from _bias_, meaning the model isn\u2019t sufficiently sensitive to the signal in the data, and _variance_, meaning the model is too sensitive to the signal specific to the training data in ways that don\u2019t generalize out-of-sample. Modeling is 90% data prep, the other half is all finding the [optimal bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html).\n\nHyperparameters help you tune the bias-variance tradeoff. For a simple logistic regression predicting survival on the Titanic, a regularization parameter lets you control overfitting by penalizing sensitivity to any individual feature. For a massive neural network doing machine translation, the number and types of layers, units, activation function, in addition to regularization, are hyperparameters. We select the best hyperparameters using [_k-fold cross-validation_](https://machinelearningmastery.com/k-fold...",
      "url": "https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d?gi=60835d2081cc"
    },
    {
      "title": "Training XGBoost with MLflow Experiments and HyperOpt Tuning",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# Training XGBoost with MLflow Experiments and HyperOpt Tuning\n\nA starting point on your MLOps Journey\n\n[Ani Madurkar](https://towardsdatascience.com/author/animadurkar/)\n\nJan 9, 2023\n\n12 min read\n\nShare\n\n![Colors of the Adirondacks. Image by author](https://towardsdatascience.com/wp-content/uploads/2023/01/1LLw56Cg9m07Zp_Rda4Xg2Q-scaled.jpeg)Colors of the Adirondacks. Image by author\n\n## Introduction\n\nAs you evolve in your journey in [Machine Learning](https://towardsdatascience.com/tag/machine-learning/), you\u2019ll soon find yourself gravitating closer and closer to MLOps whether you like it or not. Building efficient, scalable, and resilient machine learning systems is a challenge and the real job of a Data Scientist (in my opinion) as opposed to just doing modeling.\n\nThe modeling part has been largely figured out for most use cases. Unless you\u2019re trying to be at the bleeding edge of the craft, you\u2019re likely dealing with structured, tabular datasets. The choice of model can vary depending on the dataset size, assumptions, and technical restrictions, but for the most part, it is fairly repeatable. My workflow for supervised learning ML during the experimentation phase has converged to using XGBoost with HyperOpt and MLflow. XGBoost for the model of choice, HyperOpt for the hyperparameter tuning, and MLflow for the experimentation and tracking.\n\nThis also represents a phenomenal step 1 as you embark on the [Mlops](https://towardsdatascience.com/tag/mlops/) journey because I think it\u2019s easiest to start doing more MLOps work during the experimentation phase (model tracking, versioning, registry, etc.). It\u2019s lightweight and highly configurable which makes it easy to scale up and down as you may need.\n\nAlthough I briefly discuss XGBoost, MLflow, and HyperOpt, this isn\u2019t a deep walkthrough of each. Initial hands-on familiarity with each would be really helpful to understand how some pieces here are working in more depth. I\u2019ll be working with the [UCI ML Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) dataset (CC BY 4.0).\n\nTo start, we can start an MLflow server (I discuss what\u2019s happening here a bit later):\n\n```\nmlflow server\n - backend-store-uri sqlite:///mlflow.db\n - default-artifact-root ./mlruns\n - host 0.0.0.0\n - port 5000\n```\n\n![](https://towardsdatascience.com/wp-content/uploads/2023/01/1OKMpZ0h9WEzK1sU2MI4N9Q.png)\n\nPandasProfiler is a fantastic open-source library to run a quick exploratory data analysis report on a dataset. Tracking descriptive statistics, finding nulls, anomaly detection, distribution analysis, and more are all shown in the report.\n\n![Sample view of the HTML Profiler output](https://towardsdatascience.com/wp-content/uploads/2023/01/1sC-lG5Y-VH3BnklxGlKnw.png)Sample view of the HTML Profiler output\n\nI save the file as an HTML to interact with the analysis on a webpage instead of in a Jupyter Notebook which could yield memory errors depending on dataset size. See the Quickstart guide here for how PandasProfiler works: [https://pandas-profiling.ydata.ai/docs/master/pages/getting\\_started/quickstart.html](https://pandas-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html)\n\nLastly, we can create training, validation, and testing datasets.\n\n## XGBoost\n\nXGBoost, eXtreme Gradient Boosted Decision Trees, has become the de facto model of choice for a large number of tabular modeling tasks. It\u2019s still highly recommended to try out simpler models like Linear/Logistic Regression first but almost all structured tabular modeling projects with >50\u2013100K rows have resulted in these models winning out by a significant margin.\n\n_How do they work?_\n\nXGBoost is an open-source, gradient-boosting algorithm that uses decision trees as weak learners to build up a stronger model \u2013 considered an ensemble model due to its nature to combine multiple models together. There are two common ensemble methods: Bagging and Boosting.\n\nBagging, or bootstrap aggregating, typically is low variance but can be high bias. It can lead to better training stability, stronger ML accuracy, and a lower tendency to overfit. Random Forest models leverage bagging by combining decision trees where each tree can pick only from a random subset of features to use.\n\n![An illustration of the concept of bootstrap aggregating. Open Domain, Wikipedia](https://towardsdatascience.com/wp-content/uploads/2023/01/07oH1wrbEZMJXVjRo.png)An illustration of the concept of bootstrap aggregating. Open Domain, [Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n\nBoosting, in contrast, works to convert weak learners to strong ones. Each learner, or model, is trained on the same set of samples but each sample is weighted differently in each iteration. This results in weak learners getting better at learning the right weights and parameters for strong model performance over time.\n\n![An illustration of the concept of boosting. Open Domain, Wikipedia](https://towardsdatascience.com/wp-content/uploads/2023/01/0il7KId1w99wh-tr9.png)An illustration of the concept of boosting. Open Domain, [Wikipedia](https://commons.wikimedia.org/wiki/File:Ensemble_Boosting.svg)\n\nThe gradient boosting part of this algorithm refers to the fact it uses [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) to minimize the loss function. The loss function for XGBoost is a regularised (L1 and L2) objective function that incorporates a function of convex loss and a model complexity penalty term.\n\nXGBoost took fame as it became the standard for winning a multitude of tournaments on Kaggle, but lately, people have also used Microsoft\u2019s LightGBM as it can be faster for large datasets. I\u2019ve typically found phenomenal performance using both \u2013 can just be dependent on your needs.\n\n## MLflow\n\nMlflow is an open-source machine learning experiment tracking software. It makes it incredibly easy to spin up a local web interface to monitor your machine learning models, compare them, and stage them.\n\nAs you\u2019re experimenting between the right modeling algorithm and architecture, it can be a nightmare to efficiently evaluate the best one especially when you\u2019re running hundreds of experiments at once. Ideally, you want a way to store each model run, its hyperparameters, its evaluation criteria, and more. MLflow makes this all possible with minimal code around your training code.\n\nAs you experiment with different modeling architectures you can add new experiments and compare each one with the same criteria. Logging artifacts is extremely easy and fast. There are two main components that it looks to track and store: entities and artifacts.\n\nEntities: runs, parameters, metrics, tags, notes, metadata, etc. These are stored in the backend store.\n\nArtifacts: files, models, images, in-memory objects, or model summaries, etc. These are stored in the artifact store.\n\nThe default storage location for these are local files, but you can specify the backend store to instead be an SQLite database (this is minimally needed if you want to stage models to Staging or Production), a PostgreSQL database (this enables user authentication to access), or an S3 database. This page clarifies the different store configurations really well: [https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded)\n\nI won\u2019t be going into the basics of how to use MLflow because their documentation covers a lot of what you\u2019d need already. In case you need a quick tutorial for how to spin up MLflow, I highly recommend starting here: [https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n\nI will be leveraging a lightweight architecture that is independent of the cloud but know that as long as you have read/write access to an S3 bu...",
      "url": "https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6"
    },
    {
      "title": "A Guide to Find the Best Boosting Model using Bayesian Hyperparameter Tuning but without\u2026",
      "text": "# A Guide to Find the Best Boosting Model using Bayesian Hyperparameter Tuning but without\u2026\n\nBoosted decision tree algorithms may outperform other models but overfitting is a real danger. Fit your model using the HGBoost library.\n\n[Erdogan Taskesen](https://towardsdatascience.com/author/erdogant/)\n\nAug 29, 2022\n\n26 min read\n\nShare\n\n### With boosted decision tree algorithms, such as XGBoost, CatBoost, and LightBoost you may outperform other models but overfitting is a real danger. Learn how to split the data, optimize hyperparameters, and find the best-performing model without overtraining it using the HGBoost library.\n\n![Image from the author.](https://towardsdatascience.com/wp-content/uploads/2022/08/1Tepkj5NPqypeej26uXDV4g.png)Image from the author.\n\nGradient boosting techniques gained much popularity in recent years for classification and regression tasks. An important part is the tuning of hyperparameters to gain the best performance in predictions. This requires searching across thousands of parameter combinations which is not only a computationally-intensive task but can quickly lead to overtrained models. As a result, a model may not generalize on new (unseen) data, and the performance accuracy can be worse than expected. Luckily there are _Bayesian optimization_ techniques that can help to optimize the grid search and reduce the computational burden. But there is more to it because an optimized grid search may still result in overtrained models. Carefully splitting your data into a train set, a test set, and an independent validation set is another important part that should be incorporated when optimizing hyperparameters. _**Here comes the HGBoost library into play!** HGBoost stands for Hyperoptimized Gradient Boosting and is a Python package for hyperparameter optimization for XGBoost, LightBoost, and CatBoost. It will carefully split the dataset into a train, test, and independent validation set. Within the train-test set, there is the inner loop for optimizing the hyperparameters using Bayesian optimization (with hyperopt) and, the outer loop to score how well the top performing models can generalize based on k-fold cross validation. As such, it will make the best attempt to select the most robust model with the best performance. **In this blog, I will first briefly discuss boosting algorithms and hyperparameter optimization. Then I will jump into how to train a model with optimized hyperparameters. I will demonstrate how to interpret and visually explain the optimized hyperparameter space and how to evaluate the model performance.**_\n\n* * *\n\n## A very brief introduction to Boosting Algorithms.\n\nGradient boosting algorithms such as Extreme Gradient Boosting ( _XGboost_), Light Gradient Boosting ( _Lightboost_), and _CatBoost_ are powerful ensemble machine learning algorithms for predictive modeling that can be applied on tabular and continuous data, and for both [classification](https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d) and regression tasks.\n\nIt is not surprising that boosted decision tree algorithms are very popular because these algorithms were involved in more than half of the winning solutions in machine learning challenges hosted at Kaggle \\[ [1](https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)\\]. _It is the combination of gradient boosting with decision trees that provides state-of-the-art results in many applications._ It is also _this_ combination that makes the difference between the _XGboost, CatBoost, and Lightboost_. The common theme is that each boosting algorithm needs to find the best split for each leaf, and needs to minimize the [computational cost](https://arxiv.org/abs/1706.08359). The high computational cost is because the model needs to find the [exact split](https://arxiv.org/abs/1603.02754) for each leaf, and that requires iterative scanning through all the data. This process is challenging to optimize.\n\nRoughly, there are two different strategies to compute the trees: _**level-wise and leaf-wise. The level-wise strategy**_ _grows the tree level by level. In this strategy, each node splits the data and prioritizes the nodes closer to the tree root._ _**The leaf-wise strategy**_ _grows the tree by splitting the data at the nodes with the highest loss change._ Level-wise growth is usually better for smaller datasets whereas leaf-wise tends to overfit in small datasets. However, leaf-wise growth [tends to excel in larger datasets](http://researchcommons.waikato.ac.nz/handle/10[2](https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)89/2317) where it is considerably faster than level-wise growth \\[2\\]. Let\u2019s summarize the _XGboost, LightBoost, and CatBoost_ algorithms in terms of tree splitting, and computational costs.\n\n### XGBoost.\n\nExtreme Gradient Boosting _(XGboost)_ is one of the most popular types of gradient boosting techniques for which the boosted [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) are internally made up of an ensemble of weak decision trees. Originally _XGBoost_ was based on a level-wise growth algorithm but recently has added an option for leaf-wise growth that implements split approximation using histograms. **The advantages** are that _XGBoost_ is effective in learning and has strong generalization performance. In addition, it can also capture nonlinear relationships. **The disadvantages** are that the [Hyperparameter Tuning](https://towardsdatascience.com/tag/hyperparameter-tuning/) can be complex, when using sparse datasets it can quickly result in high computational costs (both memory-wise and time-wise) as many trees are needed when using very large datasets \\[ [2](https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)\\].\n\n### LightBoost.\n\n_LightBoost_ or _LightGBM_ is a gradient boosting framework that uses tree-based learning algorithms for which the trees are split vertically (or leaf-wise). Such an approach can be more efficient in loss reduction compared to a level-wise algorithm when growing the same leaf. In addition, _LightBoost_ uses histogram-based algorithms, which buckets continuous feature values into discrete bins. This speeds up training and reduces memory usage. It is designed to be efficient and has many **advantages**, such as fast training, high efficiency, low memory usage, better accuracy, support of parallel and GPU learning, and capability of handling large-scale data \\[ [3](https://lightgbm.readthedocs.io/)\\]. The computation and memory efficient advantages make _LightBoost_ applicable for large amounts of data. **The disadvantages** are that it is sensitive to [Overfitting](https://towardsdatascience.com/tag/overfitting/) due to leaf-wise splitting, and high complexity due to hyperparameter tuning.\n\n### CatBoost.\n\n_CatBoost_ is also a high-performance method for gradient boosting on decision trees. \\_Catboost grows a balanced tree using [oblivious](https://en.wikipedia.org/wiki/Oblivious_data_structure) trees or symmetric trees for faster execution\\_. This means that p\u00e9r feature, the values are divided into buckets by creating feature-split pairs, such as (temperature, <0), (temperature, 1\u201330), (temperature, >30), and so on. In each level of an oblivious tree, the feature-split pair that brings the lowest loss (according to a penalty function) is selected and is used for all the level nodes. It is described that _CatBoost_ provides great results with default parameters and will therefore require less time on parameter tuning \\[\\[\\[4\\]( [http://CatBoost](http://CatBoost) is a high-performance open source library for gradient boosting on decision trees)\\]( [http://CatBoost](http://CatBoost) is a high-performance open source library for gradient boosting on decision trees)\\]. Another **advantage** is that _CatBoost_ allows...",
      "url": "https://towardsdatascience.com/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8"
    },
    {
      "title": "Hyperparameters and Tuning Strategies for Random Forest - ADS",
      "text": "Hyperparameters and Tuning Strategies for Random Forest - ADS\nNow on home page\n## ADS\n## Hyperparameters and Tuning Strategies for Random Forest[]()\n* [Probst, Philipp](https://ui.adsabs.harvard.edu/search/?q=author:%22Probst,+Philipp%22);\n* [Wright, Marvin](https://ui.adsabs.harvard.edu/search/?q=author:%22Wright,+Marvin%22);\n* [Boulesteix, Anne-Laure](https://ui.adsabs.harvard.edu/search/?q=author:%22Boulesteix,+Anne-Laure%22)\n#### Abstract\nThe random forest algorithm (RF) has several hyperparameters that have to be set by the user, e.g., the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a brief overview of tuning strategies we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters.\nPublication:\narXiv e-prints\nPub Date:April 2018DOI:\n[10.48550/arXiv.1804.03515](https://ui.adsabs.harvard.edu/link_gateway/2018arXiv180403515P/doi:10.48550/arXiv.1804.03515)**\narXiv:[arXiv:1804.03515](https://ui.adsabs.harvard.edu/link_gateway/2018arXiv180403515P/arXiv:1804.03515)**Bibcode:[2018arXiv180403515P](https://ui.adsabs.harvard.edu/abs/2018arXiv180403515P/abstract)**Keywords:\n* Statistics - Machine Learning;\n* Computer Science - Machine LearningE-Print:19 pages, 2 figures\n**\nfull text sources\nPreprint\n[**](https://ui.adsabs.harvard.edu/link_gateway/2018arXiv180403515P/EPRINT_PDF)\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/2018arXiv180403515P/EPRINT_HTML)",
      "url": "https://ui.adsabs.harvard.edu/abs/2018arXiv180403515P/abstract"
    },
    {
      "title": "Xgboost performs significantly worse than Random Forest",
      "text": "Join Stack Overflow\u2019s first live community AMA on February 26th, at 3 PM ET.\n[Learn more](https://meta.stackexchange.com/questions/406399/join-us-for-our-first-community-wide-ama-ask-me-anything-with-stack-overflow-s?utm_medium=ppc&utm_source=stackoverflow-community&utm_campaign=community-ama&utm_content=announcement-banner1)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Xgboost performs significantly worse than Random Forest](https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked6 years, 4 months ago\n\nModified [6 years ago](https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest?lastactivity)\n\nViewed\n16k times\n\n8\n\n$\\\\begingroup$\n\nI have a dataset of 3500 observations x 70 features which is my training set and I also have a dataset of 600 observations x 70 features which is the test set.\n\nThe target is to classify observations correctly either as 0 or 1. 2000 observations of the training set are 0 and the rest 1600 of them are 1.\n\nI aim at the highest possible recall for precision>=90%.\n\nI did grid search for ensemble algorithms only in relation to number of trees (from 50 to 650 trees). Analytically the best recall results for precision >= 90% for each of the algorithms are the following:\n\nRandom Forest (375 trees)\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state=0, n_estimators=375, class_weight='balanced')\nclassifier.fit(X_train, y_train)\n\n```\n\n- Precision: 90%\n- Recall: 24%\n\nXgboost (550 trees)\n\n```\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators=n_trees, seed=0, scale_pos_weight=1.5)\nclassifier.fit(X_train, y_train, eval_metric='map')\n\n```\n\n- Precision: 90%\n- Recall: 15%\n\n**Why Xgboost is performing so much worse than the Random Forest?**\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [random-forest](https://datascience.stackexchange.com/questions/tagged/random-forest)\n- [xgboost](https://datascience.stackexchange.com/questions/tagged/xgboost)\n\n[Share](https://datascience.stackexchange.com/q/38535)\n\n[Improve this question](https://datascience.stackexchange.com/posts/38535/edit)\n\nFollow\n\n[edited Oct 18, 2018 at 10:24](https://datascience.stackexchange.com/posts/38535/revisions)\n\nOutcast\n\nasked Sep 20, 2018 at 12:44\n\n[![Outcast's user avatar](https://www.gravatar.com/avatar/1d516e928a7402d372cc9b562e7bd2d7?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/45610/outcast)\n\n[Outcast](https://datascience.stackexchange.com/users/45610/outcast) Outcast\n\n1,09722 gold badges1414 silver badges2929 bronze badges\n\n$\\\\endgroup$\n\n8\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Different algorithms, different parameters (depth, number of nodes, minimum samples in node, number of samples to consider split, etc.), different ways of handling imbalance.$\\\\endgroup$\n\n\u2013\u00a0[user2974951](https://datascience.stackexchange.com/users/59085/user2974951)\n\nCommentedSep 20, 2018 at 13:36\n\n- 2\n\n\n\n\n\n$\\\\begingroup$Slightly tangential, but did I understand correctly that you grid-searched over the number of trees to use? This is extremely inefficient in XGBoost. I strongly recommend creating an evaluating set and then using early stopping. Also, surely you found, when using a random forest, that your results just get better and better (tailing off asymptotically) as you increase the number of trees?$\\\\endgroup$\n\n\u2013\u00a0[gazza89](https://datascience.stackexchange.com/users/49143/gazza89)\n\nCommentedSep 21, 2018 at 10:00\n\n- $\\\\begingroup$I am not sure that number of trees \"is extremely inefficient in XGBoost\". But in any case, only the fact that xgboost needs so much tuning in order to reach closer, if it finally does, to the performance of a way less tuned Random Forest is a sign that it is not probably the right algorithm for this case.$\\\\endgroup$\n\n\u2013\u00a0[Outcast](https://datascience.stackexchange.com/users/45610/outcast)\n\nCommentedSep 21, 2018 at 10:49\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Yes, I meant grid search over the other parameters and use early stopping to determine the number of trees. What it means, is that you won't have iterations where you use high numbers of trees, which take a long time to train, and give you terrible results.$\\\\endgroup$\n\n\u2013\u00a0[gazza89](https://datascience.stackexchange.com/users/49143/gazza89)\n\nCommentedOct 18, 2018 at 10:41\n\n- 1\n\n\n\n\n\n$\\\\begingroup$@gazza89, I have actually performed some very deep grid searches (without early stopping) with both Random Forest and Xgboost and for now I get 37% & 28% recall respectively for precision 90% (at around 400 trees for both). Therefore, still things are more or less the same in terms of the comparative performance of these algorithms. (Please keep in mind that my aim is to maximise recall for precision>=90%). But I am going to get some new data sometime soon and see which one is really performing better in a entirely new test set because finally we may that Xgboost is significantly better.$\\\\endgroup$\n\n\u2013\u00a0[Outcast](https://datascience.stackexchange.com/users/45610/outcast)\n\nCommentedOct 18, 2018 at 10:53\n\n\n\\|\u00a0[Show **3** more comments](https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest)\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n8\n\n$\\\\begingroup$\n\nDifferent algorithms need to be tuned in different ways. For example, one important parameter in boosting is ETA or learning rate. This determines the change in weights after each boosting step. This parameter is important to reduce overfitting in boosting.\n\nTo understand the different parameters you can refer to [this](https://xgboost.readthedocs.io/en/latest/parameter.html).\n\nAnother important thing to look at is the objective function. This will determine what your algorithm is trying to optimize.\n\nLastly, please make sure you are selecting the model that actually maximises the metric that you care about. You can do this by selecting an appropriate eval\\_metric. In this case, you probably want to use something like F1-Score or precision at a fixed recall.\n\n[Share](https://datascience.stackexchange.com/a/38593)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/38593/edit)\n\nFollow\n\n[edited Sep 21, 2018 at 9:56](https://datascience.stackexchange.com/posts/38593/revisions)\n\n[![ebrahimi's user avatar](https://www.gravatar.com/avatar/f6be44a8b92f23792714a8241eb6c327?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/26019/ebrahimi)\n\n[ebrahimi](https://datascience.stackexchange.com/users/26019/ebrahimi)\n\n1,30577 gold badges2020 silver badges4040 bronze badges\n\nanswered Sep 21, 2018 at 7:05\n\n[![amanbirs's user avatar](https://www.gravatar.com/avatar/90285dfabf602c67592747e29f53acf6?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/42559/amanbirs)\n\n[amanbirs](https://datascience.stackexchange.com/users/42559/amanbirs) amanbirs\n\n18144 bronze badges\n\n$\\\\endgroup$\n\n8\n\n- $\\\\begingroup$Thanks for your answer @amanbirs (upvote). I have added a small part of my source code to my post to show that I have picked some reasonable parameters for the training so probably the problem is not that I have not specified the right metric etc. In any case, finally, it may not be so unreasonable that (sometimes) the random forest performs so much better than Xgboost but I just wanted the opinion of other...",
      "url": "https://datascience.stackexchange.com/questions/38535/xgboost-performs-significantly-worse-than-random-forest"
    },
    {
      "title": "Hyper parameters tuning XGBClassifier",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Hyper parameters tuning XGBClassifier](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked5 years, 1 month ago\n\nModified [4 years, 11 months ago](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier?lastactivity)\n\nViewed\n3k times\n\n3\n\n$\\\\begingroup$\n\nI am working on a highly imbalanced dataset for a competition.\n\nThe training data shape is : (166573, 14)\n\n```\ntrain['outcome'].value_counts()\n\n0    159730\n1      6843\n\n```\n\nI am using XGBClassifier for building model and the only parameter I manually set is **_scale\\_pos\\_weight : 23.34_** (0 value counts / 1 value counts)\n\nand it's giving around 82% under AUC metric.\n\nI guess I can get much accuracy if I hypertune all other parameters.\n\n```\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=23.4, seed=None,\n       silent=True, subsample=1)\n\n```\n\nI tried GridSearchCV but it's taking a lot of time to complete on my local machine and I am not able to get any result back.\n\n```\nclf = XGBClassifier()\ngrid = GridSearchCV(clf,\n                    params, n_jobs=-1,\n                    scoring=\"roc_auc\",\n                    cv=3)\n\ngrid.fit(X_train, y_train)\nprint(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n\n```\n\nWhat others parameters should I target to tune considering higly imbalanced dataset and how to run it so that I can actually get some results back?\n\n- [xgboost](https://datascience.stackexchange.com/questions/tagged/xgboost)\n- [cross-validation](https://datascience.stackexchange.com/questions/tagged/cross-validation)\n- [hyperparameter-tuning](https://datascience.stackexchange.com/questions/tagged/hyperparameter-tuning)\n\n[Share](https://datascience.stackexchange.com/q/49746)\n\n[Improve this question](https://datascience.stackexchange.com/posts/49746/edit)\n\nFollow\n\nasked Apr 23, 2019 at 5:51\n\n[![Praveenks's user avatar](https://www.gravatar.com/avatar/c85379da7665efb05e33f777bde1daca?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/61728/praveenks)\n\n[Praveenks](https://datascience.stackexchange.com/users/61728/praveenks) Praveenks\n\n15111 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$You can often get a lift by also tuning `scale_pos_weight` rather than using the default value, for example: [xgboosting.com/xgboost-tune-scale\\_pos\\_weight-parameter](https://xgboosting.com/xgboost-tune-scale_pos_weight-parameter/)$\\\\endgroup$\n\n\u2013\u00a0[jasonb](https://datascience.stackexchange.com/users/912/jasonb)\n\nCommentedMay 19 at 19:46\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\n1)Random search if often better than grid\n[https://www.analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/](https://www.analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/)\nTry:\n\n```\n#clf = RandomizedSearchCV(clf ,param_distributions = params, cv=kfold, scoring=\"accuracy\", n_jobs= 10, verbose = 1)\n\n```\n\n2) For unbalanced data set:\n\n- Resampling: undersampling or oversampling\n- create new data points\n\n[https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n\n[Share](https://datascience.stackexchange.com/a/55945)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/55945/edit)\n\nFollow\n\nanswered Jul 18, 2019 at 17:35\n\n[![fuwiak's user avatar](https://www.gravatar.com/avatar/23cb41cee0394217cebb85d242c46502?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/56354/fuwiak)\n\n[fuwiak](https://datascience.stackexchange.com/users/56354/fuwiak) fuwiak\n\n1,37388 gold badges1313 silver badges2626 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier)\u00a0\\|\n\n0\n\n$\\\\begingroup$\n\nI think you are tackling 2 different problems here:\n\n1. Imbalanced dataset\n2. Hyperparameter optimization for XGBoost\n\nThere are many techniques for dealing with Imbalanced datasets, one of it could be adding higher weights to your small class or another way could be resampling your data giving more chance to the small class.\n\nFor XGBoost I suggest fixing the learning rate so that the early stopping number of trees goes to around 300 and then dealing with the number of trees and the min child weight first, those are the most important parameters.\n\n[Share](https://datascience.stackexchange.com/a/49755)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/49755/edit)\n\nFollow\n\nanswered Apr 23, 2019 at 6:42\n\n[![Franco Piccolo's user avatar](https://i.sstatic.net/adxe1.jpg?s=64)](https://datascience.stackexchange.com/users/62069/franco-piccolo)\n\n[Franco Piccolo](https://datascience.stackexchange.com/users/62069/franco-piccolo) Franco Piccolo\n\n15777 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f49746%2fhyper-parameters-tuning-xgbclassifier%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [xgboost](https://datascience.stackexchange.com/questions/tagged/xgboost) - [cross-validation](https://datascience.stackexchange.com/questions/tagged/cross-validation) - [hyperparameter-tuning](https://datascience.stackexchange.com/questions/tagged/hyperparameter-tuning)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[6](https://datascience.stackexchange.com/q/31614) [Which parameters are hyper parameters in a linear regression?](https://datascience.stackexchange.com/questions/31614/which-parameters-are-hyper-parameters-in-a-linear-regression)\n\n[3](https://datascience.stackexchange.com/q/32766) [Hyper parameters and ValidationSet](https://datascience.stackexchange.com/questions/32766/hyper-parameters-and-validationset)\n\n[2](https://datascience.stackexchange.com/q/38766) [A way to Identify tuning parameters and their possible range](https://datascience.stackexchange.com/questions/38766/a-way-to-identify-tuning-parameters-and-their-possible-range)\n\n[10](https://datascience.stackexchange.com/q/41758) [Which is first ? Tuning the parameters or selecting the model](...",
      "url": "https://datascience.stackexchange.com/questions/49746/hyper-parameters-tuning-xgbclassifier"
    }
  ]
}