{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617f8f3c",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: Why Stacking Underperformed\n",
    "\n",
    "**Goal:** Understand why stacking (CV 0.8293) performed worse than voting (CV 0.8372) and identify the best path forward.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How correlated are the OOF predictions from base models?\n",
    "2. Would passthrough features help?\n",
    "3. What's the potential of the new features (Name_Length, Ticket_Frequency)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick feature engineering (same as experiments)\n",
    "import re\n",
    "\n",
    "def extract_title(name):\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    return title_search.group(1) if title_search else \"\"\n",
    "\n",
    "def process_data(df, ticket_freq_map=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Title\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    # Family\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df['FamilySize_Bin'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 11], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Cabin\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'U')\n",
    "    \n",
    "    # Name_Length\n",
    "    df['Name_Length'] = df['Name'].apply(len)\n",
    "    \n",
    "    # Ticket_Frequency\n",
    "    if ticket_freq_map is None:\n",
    "        ticket_freq_map = df['Ticket'].value_counts().to_dict()\n",
    "    df['Ticket_Frequency'] = df['Ticket'].map(ticket_freq_map).fillna(1)\n",
    "    \n",
    "    # Embarked\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # Fare\n",
    "    if df['Fare'].isna().any():\n",
    "        df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return df, ticket_freq_map\n",
    "\n",
    "train_proc, ticket_map = process_data(train)\n",
    "test_proc, _ = process_data(test, ticket_map)\n",
    "\n",
    "# Age imputation\n",
    "age_medians = train_proc.groupby(['Pclass', 'Sex', 'Title'])['Age'].median()\n",
    "fallback = train_proc['Age'].median()\n",
    "\n",
    "def impute_age(row, medians, fallback):\n",
    "    if pd.isna(row['Age']):\n",
    "        try:\n",
    "            return medians.loc[(row['Pclass'], row['Sex'], row['Title'])]\n",
    "        except KeyError:\n",
    "            return fallback\n",
    "    return row['Age']\n",
    "\n",
    "train_proc['Age'] = train_proc.apply(lambda x: impute_age(x, age_medians, fallback), axis=1)\n",
    "test_proc['Age'] = test_proc.apply(lambda x: impute_age(x, age_medians, fallback), axis=1)\n",
    "\n",
    "# Age_Bin\n",
    "train_proc['Age_Bin'] = pd.cut(train_proc['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "test_proc['Age_Bin'] = pd.cut(test_proc['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "print(\"Features processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "def encode_features(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    test_df['Sex'] = test_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_df['Embarked'] = train_df['Embarked'].map(embarked_map)\n",
    "    test_df['Embarked'] = test_df['Embarked'].map(embarked_map)\n",
    "    \n",
    "    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "    train_df['Title'] = train_df['Title'].map(title_map)\n",
    "    test_df['Title'] = test_df['Title'].map(title_map)\n",
    "    \n",
    "    deck_map = {'U': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "    train_df['Deck'] = train_df['Deck'].map(deck_map)\n",
    "    test_df['Deck'] = test_df['Deck'].map(deck_map)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_enc, test_enc = encode_features(train_proc, test_proc)\n",
    "\n",
    "# Feature sets\n",
    "features_base = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "                 'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', 'Deck', 'FamilySize_Bin', 'Age_Bin']\n",
    "\n",
    "features_enhanced = features_base + ['Name_Length', 'Ticket_Frequency']\n",
    "\n",
    "X_base = train_enc[features_base].values\n",
    "X_enhanced = train_enc[features_enhanced].values\n",
    "y = train_enc['Survived'].values\n",
    "\n",
    "print(f\"Base features: {len(features_base)}\")\n",
    "print(f\"Enhanced features: {len(features_enhanced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze OOF prediction correlation\n",
    "# This is why stacking underperformed - let's quantify it\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
    "    ('lr', LogisticRegression(C=0.1, max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "# Generate OOF predictions to analyze correlation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros((len(X_enhanced), len(base_models)))\n",
    "\n",
    "for i, (name, model_template) in enumerate(base_models):\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_enhanced, y)):\n",
    "        X_train_fold, X_val_fold = X_enhanced[train_idx], X_enhanced[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        \n",
    "        model = clone(model_template)\n",
    "        \n",
    "        if name in ['svc', 'lr']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_scaled = scaler.transform(X_val_fold)\n",
    "            model.fit(X_train_scaled, y_train_fold)\n",
    "            oof_preds[val_idx, i] = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            oof_preds[val_idx, i] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"OOF predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b592e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of OOF predictions\n",
    "oof_df = pd.DataFrame(oof_preds, columns=[name for name, _ in base_models])\n",
    "corr_matrix = oof_df.corr()\n",
    "\n",
    "print(\"OOF Prediction Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "print(f\"\\nMean pairwise correlation: {corr_matrix.values[np.triu_indices(7, k=1)].mean():.3f}\")\n",
    "print(f\"Min correlation: {corr_matrix.values[np.triu_indices(7, k=1)].min():.3f}\")\n",
    "print(f\"Max correlation: {corr_matrix.values[np.triu_indices(7, k=1)].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the key insight: OOF predictions are HIGHLY correlated\n",
    "# The meta-learner sees 7 features that are all saying the same thing\n",
    "# This is why stacking didn't help\n",
    "\n",
    "# Let's visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0.5, vmin=0.5, vmax=1.0)\n",
    "plt.title('OOF Prediction Correlations\\n(High correlation = limited diversity for meta-learner)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/oof_correlation.png', dpi=100)\n",
    "plt.show()\n",
    "print(\"Saved: /home/code/exploration/oof_correlation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d25d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Voting ensemble with enhanced features vs base features\n",
    "# Does adding Name_Length and Ticket_Frequency help?\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "def create_voting_ensemble():\n",
    "    return VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "            ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "            ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "# Test with base features (14 features - same as exp_001)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "voting_base = create_voting_ensemble()\n",
    "scores_base = cross_val_score(voting_base, X_base, y, cv=kfold, scoring='accuracy')\n",
    "print(f\"Voting (base 14 features): {scores_base.mean():.4f} (+/- {scores_base.std():.4f})\")\n",
    "\n",
    "# Test with enhanced features (16 features - with Name_Length, Ticket_Frequency)\n",
    "voting_enhanced = create_voting_ensemble()\n",
    "scores_enhanced = cross_val_score(voting_enhanced, X_enhanced, y, cv=kfold, scoring='accuracy')\n",
    "print(f\"Voting (enhanced 16 features): {scores_enhanced.mean():.4f} (+/- {scores_enhanced.std():.4f})\")\n",
    "\n",
    "print(f\"\\nImprovement from new features: {scores_enhanced.mean() - scores_base.mean():+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Passthrough stacking (original features + OOF predictions)\n",
    "# This is what the evaluator recommended\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Create stacked features: original 16 features + 7 OOF predictions = 23 features\n",
    "X_stacked = np.hstack([X_enhanced, oof_preds])\n",
    "print(f\"Stacked feature shape: {X_stacked.shape}\")\n",
    "\n",
    "# Test meta-learner on stacked features\n",
    "meta_lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "scores_passthrough = cross_val_score(meta_lr, X_stacked, y, cv=kfold, scoring='accuracy')\n",
    "print(f\"Passthrough stacking (LR): {scores_passthrough.mean():.4f} (+/- {scores_passthrough.std():.4f})\")\n",
    "\n",
    "meta_xgb = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "scores_passthrough_xgb = cross_val_score(meta_xgb, X_stacked, y, cv=kfold, scoring='accuracy')\n",
    "print(f\"Passthrough stacking (XGB): {scores_passthrough_xgb.mean():.4f} (+/- {scores_passthrough_xgb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b225284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. OOF PREDICTION CORRELATION:\")\n",
    "print(f\"   Mean pairwise correlation: {corr_matrix.values[np.triu_indices(7, k=1)].mean():.3f}\")\n",
    "print(f\"   This explains why pure stacking underperformed - meta-learner sees redundant info\")\n",
    "\n",
    "print(\"\\n2. FEATURE ENHANCEMENT IMPACT:\")\n",
    "print(f\"   Base features (14): {scores_base.mean():.4f}\")\n",
    "print(f\"   Enhanced features (16): {scores_enhanced.mean():.4f}\")\n",
    "print(f\"   Improvement: {scores_enhanced.mean() - scores_base.mean():+.4f}\")\n",
    "\n",
    "print(\"\\n3. PASSTHROUGH STACKING:\")\n",
    "print(f\"   LR meta-learner: {scores_passthrough.mean():.4f}\")\n",
    "print(f\"   XGB meta-learner: {scores_passthrough_xgb.mean():.4f}\")\n",
    "\n",
    "print(\"\\n4. BEST APPROACH:\")\n",
    "best_score = max(scores_base.mean(), scores_enhanced.mean(), scores_passthrough.mean(), scores_passthrough_xgb.mean())\n",
    "if best_score == scores_enhanced.mean():\n",
    "    print(f\"   Voting with enhanced features: {best_score:.4f}\")\n",
    "elif best_score == scores_passthrough_xgb.mean():\n",
    "    print(f\"   Passthrough stacking (XGB): {best_score:.4f}\")\n",
    "else:\n",
    "    print(f\"   Best: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\n5. COMPARISON TO PREVIOUS EXPERIMENTS:\")\n",
    "print(f\"   exp_000 (XGBoost baseline): 0.8316\")\n",
    "print(f\"   exp_001 (Voting ensemble): 0.8372\")\n",
    "print(f\"   exp_002 (Pure stacking): 0.8293\")\n",
    "print(f\"   Current best: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The new features (Name_Length, Ticket_Frequency) may not be helping\n",
    "# Let's check their individual importance\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=42)\n",
    "rf.fit(X_enhanced, y)\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'feature': features_enhanced,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances (RF):\")\n",
    "print(importances.to_string(index=False))\n",
    "\n",
    "print(f\"\\nName_Length importance: {importances[importances['feature']=='Name_Length']['importance'].values[0]:.4f}\")\n",
    "print(f\"Ticket_Frequency importance: {importances[importances['feature']=='Ticket_Frequency']['importance'].values[0]:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
