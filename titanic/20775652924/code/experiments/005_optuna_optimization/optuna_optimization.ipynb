{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b1134b",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "\n",
    "**Goal:** Improve CV score above 0.84 through Bayesian optimization\n",
    "\n",
    "**Current best:** Voting Ensemble CV 0.8372, LB 0.7727\n",
    "**Target:** CV > 0.84 (need +0.0028 improvement)\n",
    "\n",
    "**Approach:**\n",
    "1. Optimize XGBoost hyperparameters with Optuna\n",
    "2. Optimize RandomForest hyperparameters\n",
    "3. Optimize GradientBoosting hyperparameters\n",
    "4. Combine optimized models in voting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f20fe25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:04:47.796307Z",
     "iopub.status.busy": "2026-01-07T09:04:47.795553Z",
     "iopub.status.idle": "2026-01-07T09:04:49.450302Z",
     "shell.execute_reply": "2026-01-07T09:04:49.449677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (891, 12)\n",
      "Test shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b733262e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:04:49.452541Z",
     "iopub.status.busy": "2026-01-07T09:04:49.452314Z",
     "iopub.status.idle": "2026-01-07T09:04:49.511817Z",
     "shell.execute_reply": "2026-01-07T09:04:49.511217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features processed.\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering (same as voting ensemble exp_001)\n",
    "def extract_title(name):\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def process_features(df):\n",
    "    df = df.copy()\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', \n",
    "                                        'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df['FamilySize_Bin'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 11], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'U')\n",
    "    \n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    if df['Fare'].isna().any():\n",
    "        df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_processed = process_features(train)\n",
    "test_processed = process_features(test)\n",
    "\n",
    "# Age imputation\n",
    "def impute_age(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    age_medians = train_df.groupby(['Pclass', 'Sex', 'Title'])['Age'].median()\n",
    "    \n",
    "    def get_median_age(row, medians, fallback_median):\n",
    "        if pd.isna(row['Age']):\n",
    "            try:\n",
    "                return medians.loc[(row['Pclass'], row['Sex'], row['Title'])]\n",
    "            except KeyError:\n",
    "                return fallback_median\n",
    "        return row['Age']\n",
    "    \n",
    "    fallback = train_df['Age'].median()\n",
    "    train_df['Age'] = train_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    test_df['Age'] = test_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_processed, test_processed = impute_age(train_processed, test_processed)\n",
    "\n",
    "# Age_Bin\n",
    "train_processed['Age_Bin'] = pd.cut(train_processed['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "test_processed['Age_Bin'] = pd.cut(test_processed['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "print(\"Features processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3f41ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:04:49.513890Z",
     "iopub.status.busy": "2026-01-07T09:04:49.513333Z",
     "iopub.status.idle": "2026-01-07T09:04:49.529424Z",
     "shell.execute_reply": "2026-01-07T09:04:49.528849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (891, 14)\n",
      "Features: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', 'Deck', 'FamilySize_Bin', 'Age_Bin']\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "def encode_features(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    test_df['Sex'] = test_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_df['Embarked'] = train_df['Embarked'].map(embarked_map)\n",
    "    test_df['Embarked'] = test_df['Embarked'].map(embarked_map)\n",
    "    \n",
    "    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "    train_df['Title'] = train_df['Title'].map(title_map)\n",
    "    test_df['Title'] = test_df['Title'].map(title_map)\n",
    "    \n",
    "    deck_map = {'U': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "    train_df['Deck'] = train_df['Deck'].map(deck_map)\n",
    "    test_df['Deck'] = test_df['Deck'].map(deck_map)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_encoded, test_encoded = encode_features(train_processed, test_processed)\n",
    "\n",
    "# Feature set (same as voting ensemble)\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "            'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', \n",
    "            'Deck', 'FamilySize_Bin', 'Age_Bin']\n",
    "\n",
    "X = train_encoded[features].values\n",
    "y = train_encoded['Survived'].values\n",
    "X_test = test_encoded[features].values\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb345c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:04:49.531312Z",
     "iopub.status.busy": "2026-01-07T09:04:49.531103Z",
     "iopub.status.idle": "2026-01-07T09:05:16.758952Z",
     "shell.execute_reply": "2026-01-07T09:05:16.758329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZING XGBOOST\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best XGBoost CV: 0.8372\n",
      "Best params: {'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.03149834299080512, 'subsample': 0.7547458558957117, 'colsample_bytree': 0.7726130043755545, 'min_child_weight': 4, 'gamma': 0.1987168339648069, 'reg_alpha': 0.48095482073340695, 'reg_lambda': 0.6462179360882688}\n"
     ]
    }
   ],
   "source": [
    "# Optuna optimization for XGBoost\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZING XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "xgb_study = optuna.create_study(direction='maximize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(f\"\\nBest XGBoost CV: {xgb_study.best_value:.4f}\")\n",
    "print(f\"Best params: {xgb_study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac8e7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:05:16.761367Z",
     "iopub.status.busy": "2026-01-07T09:05:16.760889Z",
     "iopub.status.idle": "2026-01-07T09:08:01.612373Z",
     "shell.execute_reply": "2026-01-07T09:08:01.611720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZING RANDOM FOREST\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RandomForest CV: 0.8406\n",
      "Best params: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 19, 'min_samples_leaf': 1, 'max_features': None}\n"
     ]
    }
   ],
   "source": [
    "# Optuna optimization for RandomForest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZING RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def rf_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "rf_study = optuna.create_study(direction='maximize')\n",
    "rf_study.optimize(rf_objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(f\"\\nBest RandomForest CV: {rf_study.best_value:.4f}\")\n",
    "print(f\"Best params: {rf_study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8a9617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:08:01.614652Z",
     "iopub.status.busy": "2026-01-07T09:08:01.614423Z",
     "iopub.status.idle": "2026-01-07T09:11:00.637946Z",
     "shell.execute_reply": "2026-01-07T09:11:00.637205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZING GRADIENT BOOSTING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best GradientBoosting CV: 0.8417\n",
      "Best params: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.03179473190235844, 'min_samples_split': 15, 'min_samples_leaf': 10, 'subsample': 0.962115034882979}\n"
     ]
    }
   ],
   "source": [
    "# Optuna optimization for GradientBoosting\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZING GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def gb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingClassifier(**params, random_state=42)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "gb_study = optuna.create_study(direction='maximize')\n",
    "gb_study.optimize(gb_objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(f\"\\nBest GradientBoosting CV: {gb_study.best_value:.4f}\")\n",
    "print(f\"Best params: {gb_study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3947d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:11:00.640341Z",
     "iopub.status.busy": "2026-01-07T09:11:00.639769Z",
     "iopub.status.idle": "2026-01-07T09:11:00.645405Z",
     "shell.execute_reply": "2026-01-07T09:11:00.644842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Optimized single model CV scores:\n",
      "  XGBoost:          0.8372\n",
      "  RandomForest:     0.8406\n",
      "  GradientBoosting: 0.8417\n",
      "\n",
      "Previous voting ensemble: 0.8372\n",
      "Best optimized single model: 0.8417\n"
     ]
    }
   ],
   "source": [
    "# Summary of optimized models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOptimized single model CV scores:\")\n",
    "print(f\"  XGBoost:          {xgb_study.best_value:.4f}\")\n",
    "print(f\"  RandomForest:     {rf_study.best_value:.4f}\")\n",
    "print(f\"  GradientBoosting: {gb_study.best_value:.4f}\")\n",
    "print(f\"\\nPrevious voting ensemble: 0.8372\")\n",
    "print(f\"Best optimized single model: {max(xgb_study.best_value, rf_study.best_value, gb_study.best_value):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f869fdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:11:00.647604Z",
     "iopub.status.busy": "2026-01-07T09:11:00.647093Z",
     "iopub.status.idle": "2026-01-07T09:11:00.660495Z",
     "shell.execute_reply": "2026-01-07T09:11:00.659964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZED VOTING ENSEMBLE\n",
      "============================================================\n",
      "Optimized ensemble models:\n",
      "  - lr\n",
      "  - rf\n",
      "  - gb\n",
      "  - et\n",
      "  - ada\n",
      "  - svc\n",
      "  - xgb\n"
     ]
    }
   ],
   "source": [
    "# Create optimized voting ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZED VOTING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models with optimized hyperparameters\n",
    "optimized_xgb = XGBClassifier(**xgb_study.best_params, random_state=42, \n",
    "                              use_label_encoder=False, eval_metric='logloss')\n",
    "optimized_rf = RandomForestClassifier(**rf_study.best_params, random_state=42, n_jobs=-1)\n",
    "optimized_gb = GradientBoostingClassifier(**gb_study.best_params, random_state=42)\n",
    "\n",
    "# Keep other models with original params (they weren't the bottleneck)\n",
    "models_optimized = [\n",
    "    ('lr', LogisticRegression(C=0.1, max_iter=1000, random_state=42)),\n",
    "    ('rf', optimized_rf),\n",
    "    ('gb', optimized_gb),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "    ('xgb', optimized_xgb)\n",
    "]\n",
    "\n",
    "print(\"Optimized ensemble models:\")\n",
    "for name, model in models_optimized:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837c3daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:11:00.662329Z",
     "iopub.status.busy": "2026-01-07T09:11:00.662127Z",
     "iopub.status.idle": "2026-01-07T09:11:16.100675Z",
     "shell.execute_reply": "2026-01-07T09:11:16.099806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.8778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 0.8652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7: Accuracy = 0.8652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8: Accuracy = 0.7978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9: Accuracy = 0.8427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10: Accuracy = 0.8090\n",
      "\n",
      "==================================================\n",
      "Optimized Ensemble CV: 0.8361 (+/- 0.0274)\n",
      "Previous Voting Ensemble: 0.8372 (+/- 0.0239)\n",
      "Change: -0.0011\n"
     ]
    }
   ],
   "source": [
    "# 10-fold CV for optimized ensemble\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores_opt = []\n",
    "oof_predictions_opt = np.zeros(len(X))\n",
    "test_predictions_opt = np.zeros(len(X_test))\n",
    "\n",
    "individual_scores_opt = {name: [] for name, _ in models_optimized}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    fold_probs = np.zeros((len(X_val), 2))\n",
    "    test_fold_probs = np.zeros((len(X_test), 2))\n",
    "    \n",
    "    for name, model in models_optimized:\n",
    "        from sklearn.base import clone\n",
    "        m = clone(model)\n",
    "        \n",
    "        if name in ['lr', 'svc']:\n",
    "            m.fit(X_train_scaled, y_train)\n",
    "            val_prob = m.predict_proba(X_val_scaled)\n",
    "            test_prob = m.predict_proba(X_test_scaled)\n",
    "            val_pred = m.predict(X_val_scaled)\n",
    "        else:\n",
    "            m.fit(X_train, y_train)\n",
    "            val_prob = m.predict_proba(X_val)\n",
    "            test_prob = m.predict_proba(X_test)\n",
    "            val_pred = m.predict(X_val)\n",
    "        \n",
    "        fold_probs += val_prob\n",
    "        test_fold_probs += test_prob\n",
    "        individual_scores_opt[name].append(accuracy_score(y_val, val_pred))\n",
    "    \n",
    "    fold_probs /= len(models_optimized)\n",
    "    test_fold_probs /= len(models_optimized)\n",
    "    \n",
    "    val_pred_ensemble = (fold_probs[:, 1] >= 0.5).astype(int)\n",
    "    oof_predictions_opt[val_idx] = val_pred_ensemble\n",
    "    test_predictions_opt += test_fold_probs[:, 1] / kfold.n_splits\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, val_pred_ensemble)\n",
    "    cv_scores_opt.append(fold_acc)\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Optimized Ensemble CV: {np.mean(cv_scores_opt):.4f} (+/- {np.std(cv_scores_opt):.4f})\")\n",
    "print(f\"Previous Voting Ensemble: 0.8372 (+/- 0.0239)\")\n",
    "print(f\"Change: {np.mean(cv_scores_opt) - 0.8372:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a977c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:11:16.103154Z",
     "iopub.status.busy": "2026-01-07T09:11:16.102533Z",
     "iopub.status.idle": "2026-01-07T09:11:16.108305Z",
     "shell.execute_reply": "2026-01-07T09:11:16.107615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Model Performance (Optimized):\n",
      "==================================================\n",
      "lr   : 0.8136 (+/- 0.0329)\n",
      "rf   : 0.8406 (+/- 0.0294)\n",
      "gb   : 0.8417 (+/- 0.0235)\n",
      "et   : 0.8316 (+/- 0.0216)\n",
      "ada  : 0.8193 (+/- 0.0279)\n",
      "svc  : 0.8282 (+/- 0.0249)\n",
      "xgb  : 0.8372 (+/- 0.0313)\n",
      "\n",
      "Ensemble: 0.8361 (+/- 0.0274)\n"
     ]
    }
   ],
   "source": [
    "# Individual model performance\n",
    "print(\"\\nIndividual Model Performance (Optimized):\")\n",
    "print(\"=\"*50)\n",
    "for name, scores in individual_scores_opt.items():\n",
    "    print(f\"{name:5s}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "\n",
    "print(f\"\\nEnsemble: {np.mean(cv_scores_opt):.4f} (+/- {np.std(cv_scores_opt):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535b577f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:11:16.110516Z",
     "iopub.status.busy": "2026-01-07T09:11:16.110069Z",
     "iopub.status.idle": "2026-01-07T09:11:16.122789Z",
     "shell.execute_reply": "2026-01-07T09:11:16.122103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING SUBMISSION\n",
      "============================================================\n",
      "Submission saved to /home/submission/submission.csv\n",
      "\n",
      "Prediction distribution:\n",
      "Survived\n",
      "0    262\n",
      "1    156\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Comparison:\n",
      "  Original Voting: 255 died, 163 survived\n",
      "  Optimized:       262 died, 156 survived\n"
     ]
    }
   ],
   "source": [
    "# Generate submission\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_pred_binary = (test_predictions_opt >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Original Voting: 255 died, 163 survived\")\n",
    "print(f\"  Optimized:       {(test_pred_binary == 0).sum()} died, {(test_pred_binary == 1).sum()} survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9344cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nCV Score Comparison:\")\n",
    "print(f\"  Baseline XGBoost (exp_000):     0.8316\")\n",
    "print(f\"  Voting Ensemble (exp_001):      0.8372\")\n",
    "print(f\"  Stacking (exp_002):             0.8293\")\n",
    "print(f\"  Adversarial+Regularized (exp_003): 0.8327\")\n",
    "print(f\"  Optuna Optimized (exp_004):     {np.mean(cv_scores_opt):.4f}\")\n",
    "\n",
    "print(f\"\\nBest CV: {max(0.8372, np.mean(cv_scores_opt)):.4f}\")\n",
    "print(f\"\\nExpected LB (using calibration LB = 2.55*CV - 1.37):\")\n",
    "print(f\"  Optimized: {2.55 * np.mean(cv_scores_opt) - 1.37:.4f}\")\n",
    "print(f\"  Previous best LB: 0.7727\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
