{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77edee46",
   "metadata": {},
   "source": [
    "# Voting Ensemble with Enhanced Features\n",
    "\n",
    "**Goal:** Reduce CV-LB gap (currently +0.0732) by:\n",
    "1. Using diverse models in soft voting ensemble\n",
    "2. Simpler hyperparameters to reduce overfitting\n",
    "3. Adding robust features: Deck, FamilySize_Bin, Age_Bin\n",
    "\n",
    "**Reference:** Strategy recommends simpler models to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407afdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:01.451160Z",
     "iopub.status.busy": "2026-01-07T07:26:01.450306Z",
     "iopub.status.idle": "2026-01-07T07:26:02.520007Z",
     "shell.execute_reply": "2026-01-07T07:26:02.519366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (891, 12)\n",
      "Test shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4946c0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:02.522199Z",
     "iopub.status.busy": "2026-01-07T07:26:02.521973Z",
     "iopub.status.idle": "2026-01-07T07:26:02.554090Z",
     "shell.execute_reply": "2026-01-07T07:26:02.553478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title distribution:\n",
      "Title\n",
      "Mr        517\n",
      "Miss      185\n",
      "Mrs       126\n",
      "Master     40\n",
      "Rare       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Deck distribution:\n",
      "Deck\n",
      "U    687\n",
      "C     59\n",
      "B     47\n",
      "D     33\n",
      "E     32\n",
      "A     15\n",
      "F     13\n",
      "G      4\n",
      "T      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FamilySize_Bin distribution:\n",
      "FamilySize_Bin\n",
      "0    537\n",
      "1    292\n",
      "2     62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_title(name):\n",
    "    \"\"\"Extract title from name using regex\"\"\"\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def process_features(df, is_train=True):\n",
    "    \"\"\"Process features for train/test data with enhanced features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Title extraction\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', \n",
    "                                        'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    # 2. Family features\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # 3. FamilySize_Bin (NEW - captures non-linear relationship)\n",
    "    df['FamilySize_Bin'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 11], labels=[0, 1, 2]).astype(int)\n",
    "    # 0 = alone, 1 = small (2-4), 2 = large (5+)\n",
    "    \n",
    "    # 4. Has_Cabin feature\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    \n",
    "    # 5. Deck feature (NEW - strong signal from analysis)\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'U')\n",
    "    \n",
    "    # 6. Embarked - fill missing with mode\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # 7. Fare - fill missing with median by Pclass\n",
    "    if df['Fare'].isna().any():\n",
    "        df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process train and test\n",
    "train_processed = process_features(train, is_train=True)\n",
    "test_processed = process_features(test, is_train=False)\n",
    "\n",
    "print(\"Title distribution:\")\n",
    "print(train_processed['Title'].value_counts())\n",
    "print(\"\\nDeck distribution:\")\n",
    "print(train_processed['Deck'].value_counts())\n",
    "print(\"\\nFamilySize_Bin distribution:\")\n",
    "print(train_processed['FamilySize_Bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df6874e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:02.555925Z",
     "iopub.status.busy": "2026-01-07T07:26:02.555700Z",
     "iopub.status.idle": "2026-01-07T07:26:02.593215Z",
     "shell.execute_reply": "2026-01-07T07:26:02.592607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "Train Age missing: 0\n",
      "Test Age missing: 0\n",
      "\n",
      "Age_Bin distribution:\n",
      "Age_Bin\n",
      "0    104\n",
      "1    490\n",
      "2    216\n",
      "3     81\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Age imputation using median by Pclass, Sex, Title (from training data only)\n",
    "def impute_age(train_df, test_df):\n",
    "    \"\"\"Impute age using median by Pclass, Sex, Title\"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Calculate medians from training data\n",
    "    age_medians = train_df.groupby(['Pclass', 'Sex', 'Title'])['Age'].median()\n",
    "    \n",
    "    def get_median_age(row, medians, fallback_median):\n",
    "        if pd.isna(row['Age']):\n",
    "            try:\n",
    "                return medians.loc[(row['Pclass'], row['Sex'], row['Title'])]\n",
    "            except KeyError:\n",
    "                return fallback_median\n",
    "        return row['Age']\n",
    "    \n",
    "    fallback = train_df['Age'].median()\n",
    "    train_df['Age'] = train_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    test_df['Age'] = test_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_processed, test_processed = impute_age(train_processed, test_processed)\n",
    "\n",
    "# Age_Bin feature (NEW - captures child survival advantage)\n",
    "def add_age_bin(df):\n",
    "    df = df.copy()\n",
    "    df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "    # 0 = Child (0-16), 1 = Young (16-32), 2 = Middle (32-48), 3 = Senior (48+)\n",
    "    return df\n",
    "\n",
    "train_processed = add_age_bin(train_processed)\n",
    "test_processed = add_age_bin(test_processed)\n",
    "\n",
    "print(f\"Missing values after imputation:\")\n",
    "print(f\"Train Age missing: {train_processed['Age'].isna().sum()}\")\n",
    "print(f\"Test Age missing: {test_processed['Age'].isna().sum()}\")\n",
    "print(\"\\nAge_Bin distribution:\")\n",
    "print(train_processed['Age_Bin'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fc06cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:02.594905Z",
     "iopub.status.busy": "2026-01-07T07:26:02.594696Z",
     "iopub.status.idle": "2026-01-07T07:26:02.609707Z",
     "shell.execute_reply": "2026-01-07T07:26:02.609119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (891, 14)\n",
      "Test matrix shape: (418, 14)\n",
      "\n",
      "Features (14): ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', 'Deck', 'FamilySize_Bin', 'Age_Bin']\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "def encode_features(train_df, test_df):\n",
    "    \"\"\"Encode categorical features\"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Sex encoding\n",
    "    train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    test_df['Sex'] = test_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    # Embarked encoding\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_df['Embarked'] = train_df['Embarked'].map(embarked_map)\n",
    "    test_df['Embarked'] = test_df['Embarked'].map(embarked_map)\n",
    "    \n",
    "    # Title encoding\n",
    "    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "    train_df['Title'] = train_df['Title'].map(title_map)\n",
    "    test_df['Title'] = test_df['Title'].map(title_map)\n",
    "    \n",
    "    # Deck encoding (U=0, A-G=1-7, T=8)\n",
    "    deck_map = {'U': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "    train_df['Deck'] = train_df['Deck'].map(deck_map)\n",
    "    test_df['Deck'] = test_df['Deck'].map(deck_map)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_encoded, test_encoded = encode_features(train_processed, test_processed)\n",
    "\n",
    "# Select features for modeling (enhanced feature set)\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "            'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', \n",
    "            'Deck', 'FamilySize_Bin', 'Age_Bin']  # NEW features\n",
    "\n",
    "X = train_encoded[features].values\n",
    "y = train_encoded['Survived'].values\n",
    "X_test = test_encoded[features].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Test matrix shape: {X_test.shape}\")\n",
    "print(f\"\\nFeatures ({len(features)}): {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc06eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:02.611551Z",
     "iopub.status.busy": "2026-01-07T07:26:02.611345Z",
     "iopub.status.idle": "2026-01-07T07:26:02.619621Z",
     "shell.execute_reply": "2026-01-07T07:26:02.619032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in ensemble:\n",
      "  - lr: LogisticRegression\n",
      "  - rf: RandomForestClassifier\n",
      "  - gb: GradientBoostingClassifier\n",
      "  - et: ExtraTreesClassifier\n",
      "  - ada: AdaBoostClassifier\n",
      "  - svc: SVC\n",
      "  - xgb: XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "# Define voting ensemble with simpler hyperparameters (to reduce overfitting)\n",
    "# Following strategy: use diverse models with regularization\n",
    "\n",
    "models = [\n",
    "    ('lr', LogisticRegression(C=0.1, max_iter=1000, random_state=42)),  # Simple, regularized\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, \n",
    "                          use_label_encoder=False, eval_metric='logloss', random_state=42))  # Simpler than baseline\n",
    "]\n",
    "\n",
    "print(\"Models in ensemble:\")\n",
    "for name, model in models:\n",
    "    print(f\"  - {name}: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa01f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:02.621523Z",
     "iopub.status.busy": "2026-01-07T07:26:02.621303Z",
     "iopub.status.idle": "2026-01-07T07:26:14.195411Z",
     "shell.execute_reply": "2026-01-07T07:26:14.194763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.8778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 0.7978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7: Accuracy = 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8: Accuracy = 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9: Accuracy = 0.8427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10: Accuracy = 0.8427\n",
      "\n",
      "==================================================\n",
      "Mean CV Accuracy: 0.8372 (+/- 0.0239)\n",
      "Overall OOF Accuracy: 0.8373\n"
     ]
    }
   ],
   "source": [
    "# 10-fold Stratified Cross-Validation with Voting Ensemble\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "# Also track individual model scores for analysis\n",
    "individual_scores = {name: [] for name, _ in models}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale features for SVC and LogisticRegression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train individual models and collect predictions for soft voting\n",
    "    fold_probs = np.zeros((len(X_val), 2))\n",
    "    test_fold_probs = np.zeros((len(X_test), 2))\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Use scaled data for SVC and LR\n",
    "        if name in ['lr', 'svc']:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            val_prob = model.predict_proba(X_val_scaled)\n",
    "            test_prob = model.predict_proba(X_test_scaled)\n",
    "            val_pred = model.predict(X_val_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            val_prob = model.predict_proba(X_val)\n",
    "            test_prob = model.predict_proba(X_test)\n",
    "            val_pred = model.predict(X_val)\n",
    "        \n",
    "        fold_probs += val_prob\n",
    "        test_fold_probs += test_prob\n",
    "        \n",
    "        # Track individual model accuracy\n",
    "        individual_scores[name].append(accuracy_score(y_val, val_pred))\n",
    "    \n",
    "    # Average probabilities (soft voting)\n",
    "    fold_probs /= len(models)\n",
    "    test_fold_probs /= len(models)\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    val_pred_ensemble = (fold_probs[:, 1] >= 0.5).astype(int)\n",
    "    oof_predictions[val_idx] = val_pred_ensemble\n",
    "    \n",
    "    # Accumulate test predictions\n",
    "    test_predictions += test_fold_probs[:, 1] / kfold.n_splits\n",
    "    \n",
    "    # Calculate fold accuracy\n",
    "    fold_acc = accuracy_score(y_val, val_pred_ensemble)\n",
    "    cv_scores.append(fold_acc)\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"Overall OOF Accuracy: {accuracy_score(y, oof_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b02b3dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:14.197527Z",
     "iopub.status.busy": "2026-01-07T07:26:14.197274Z",
     "iopub.status.idle": "2026-01-07T07:26:14.202289Z",
     "shell.execute_reply": "2026-01-07T07:26:14.201725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Model Performance:\n",
      "==================================================\n",
      "lr   : 0.8136 (+/- 0.0329)\n",
      "rf   : 0.8282 (+/- 0.0204)\n",
      "gb   : 0.8260 (+/- 0.0211)\n",
      "et   : 0.8316 (+/- 0.0216)\n",
      "ada  : 0.8193 (+/- 0.0279)\n",
      "svc  : 0.8282 (+/- 0.0249)\n",
      "xgb  : 0.8260 (+/- 0.0288)\n",
      "\n",
      "Ensemble: 0.8372 (+/- 0.0239)\n",
      "\n",
      "Baseline (XGBoost alone): 0.8316 (+/- 0.0324)\n",
      "Improvement: +0.0056\n"
     ]
    }
   ],
   "source": [
    "# Individual model performance analysis\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "print(\"=\"*50)\n",
    "for name, scores in individual_scores.items():\n",
    "    print(f\"{name:5s}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "\n",
    "print(f\"\\nEnsemble: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"\\nBaseline (XGBoost alone): 0.8316 (+/- 0.0324)\")\n",
    "print(f\"Improvement: {np.mean(cv_scores) - 0.8316:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80c4502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T07:26:14.204357Z",
     "iopub.status.busy": "2026-01-07T07:26:14.203753Z",
     "iopub.status.idle": "2026-01-07T07:26:14.217068Z",
     "shell.execute_reply": "2026-01-07T07:26:14.216485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n",
      "5          897         0\n",
      "6          898         1\n",
      "7          899         0\n",
      "8          900         1\n",
      "9          901         0\n",
      "\n",
      "Prediction distribution:\n",
      "Survived\n",
      "0    255\n",
      "1    163\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Comparison with baseline:\n",
      "Baseline: 267 died, 151 survived\n",
      "Ensemble: 255 died, 163 survived\n"
     ]
    }
   ],
   "source": [
    "# Generate submission\n",
    "test_pred_binary = (test_predictions >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nComparison with baseline:\")\n",
    "print(f\"Baseline: 267 died, 151 survived\")\n",
    "print(f\"Ensemble: {(test_pred_binary == 0).sum()} died, {(test_pred_binary == 1).sum()} survived\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
