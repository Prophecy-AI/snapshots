# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, survival rates by key features
- Key findings: 891 train samples, 418 test samples, binary classification (accuracy metric)
- Strong predictors: Sex (female 74% survival vs male 19%), Pclass (1st class 63% vs 3rd class 24%), Title

## Feature Engineering (Critical for High Scores)

### Must-Have Features:
1. **Title extraction from Name**: Extract title using regex ` ([A-Za-z]+)\.`
   - Group rare titles: Map Mlle/Ms to Miss, Mme to Mrs, rare titles to 'Rare'
   - Master (young boys) has 57.5% survival - important signal

2. **Family features**:
   - FamilySize = SibSp + Parch + 1
   - IsAlone = 1 if FamilySize == 1, else 0
   - Family survival rates vary significantly

3. **Has_Cabin**: Binary feature (1 if Cabin is not null, 0 otherwise)
   - Cabin presence indicates higher class/survival

4. **Age binning**: Use pd.qcut() for quantile-based bins (4-5 bins)
   - Handle missing Age with median or model-based imputation

5. **Fare binning**: Use pd.qcut() for quantile-based bins (4 bins)
   - Handle missing Fare with median

6. **Name_length**: Length of passenger name (correlates with social status)

### Missing Value Handling:
- Age: Fill with median (by Pclass+Sex groups for better accuracy)
- Embarked: Fill with mode ('S')
- Fare: Fill with median
- Cabin: Create Has_Cabin feature, then drop Cabin column

## Models

### Single Models (Baseline ~82% accuracy):
- **RandomForestClassifier**: Good baseline, handles feature interactions
- **GradientBoostingClassifier**: Often best single model
- **XGBoost**: Fast, handles missing values natively
- **LogisticRegression**: Simple baseline with regularization

### Ensemble Methods (Recommended for Higher Scores):

1. **Voting Classifier** (Hard/Soft voting):
   - Combine: AdaBoost, Bagging, ExtraTrees, GradientBoosting, RandomForest, LogisticRegression
   - Soft voting often better than hard voting

2. **Stacking (Best approach for this competition)**:
   - First level (5 base models): RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVM
   - Use out-of-fold predictions to avoid overfitting
   - Second level: XGBoost trained on first-level predictions
   - Key: Base models should be diverse/uncorrelated

### Hyperparameter Tuning:
- Use GridSearchCV or RandomizedSearchCV
- Key parameters for tree models:
  - n_estimators: [50, 100, 300]
  - max_depth: [2, 4, 6, 8, None]
  - min_samples_split: [5, 10, 0.03, 0.05]
  - learning_rate (boosting): [0.01, 0.03, 0.05, 0.1]

## Validation Strategy
- Use StratifiedKFold (k=5) to maintain class balance
- Cross-validation score ~82-84% is typical for good models
- Leaderboard scores around 77-80% are competitive

## Preprocessing Pipeline

1. **Categorical encoding**:
   - LabelEncoder for ordinal features (Pclass)
   - One-hot encoding for nominal features (Sex, Embarked, Title)

2. **Feature selection**:
   - Use RFECV (Recursive Feature Elimination with CV)
   - Or use feature_importances_ from tree models

3. **Drop columns**: PassengerId, Name, Ticket, Cabin (after extracting features)

## Code Structure Recommendations

```
1. Load and combine train/test for consistent preprocessing
2. Feature engineering (apply to both train and test)
3. Handle missing values
4. Encode categorical features
5. Split train for validation
6. Train multiple models with CV
7. Ensemble predictions
8. Generate submission
```

## Key Insights from Top Kernels
- Simple decision tree with good feature engineering can achieve ~78% on leaderboard
- Stacking with XGBoost as meta-learner is a proven winning approach
- Feature engineering matters more than model complexity for this dataset
- Uncorrelated base models produce better ensemble results
