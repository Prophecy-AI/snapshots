## Current Status
- Best CV score: 0.8283 from exp_000 (Baseline RF with Feature Engineering)
- Best LB score: 0.7727 (from exp_000)
- CV-LB gap: +0.0556 (CV is optimistic by 5.56%)
- Submissions used: 1/10 (8 remaining)

## Response to Evaluator
- **Technical verdict was TRUSTWORTHY.** Results can be relied upon for decision-making.
- **Evaluator's top priority:** Implement stacking ensemble with diverse base models. **I agree** - this is the highest-leverage improvement available based on research notes showing stacking adds 1-3% over single models.
- **Key concerns raised:**
  1. Target score of 1.0 is unrealistic - **Acknowledged.** Top Kaggle scores are 85-87%. We should aim for 0.80+ on LB as a realistic target.
  2. Missing features (Deck, Age/Fare binning, interaction features) - **Will address** in the next experiment alongside stacking.
  3. Minor leakage in Age imputation - **Acceptable** for this competition, minimal impact.

## CV-LB Gap Analysis (from exploration/evolver_loop1_lb_feedback.ipynb)
The 5.56% gap is significant. Key findings:
- Test distribution differs slightly: more Embarked=C (24.4% vs 18.9%), more male_2 (15.1% vs 12.1%), more Mrs (17.2% vs 14.1%)
- Expected survival rate in test based on train patterns: 38.94% vs our prediction 37.56%
- This suggests some overfitting to training patterns

**Implication:** We should focus on:
1. More robust features that generalize better
2. Regularization to prevent overfitting
3. Ensemble diversity to reduce variance

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Feature distributions, missing values, target distribution
- `exploration/evolver_loop1_lb_feedback.ipynb` - CV-LB gap analysis, distribution comparison
- `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` - Stacking implementation reference

Key patterns:
- Sex is most predictive (female 74.2% survival vs male 18.9%)
- Pclass strongly correlated (1st: 63%, 2nd: 47%, 3rd: 24%)
- Title captures both gender and social status (Mrs: 79%, Miss: 70%, Mr: 16%, Master: 57%)
- FamilySize 2-4 has higher survival than alone or large families

## Recommended Approaches (Priority Order)

### 1. Implement Stacking Ensemble (HIGHEST PRIORITY)
Based on arthurtok kernel, implement 5-model stacking:
- **Base models:** RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
- **Meta-learner:** XGBoost with n_estimators=2000, max_depth=4
- **Key:** Use out-of-fold predictions to prevent leakage

Parameters from research:
```python
# RF: n_estimators=500, max_depth=6, min_samples_leaf=2, max_features='sqrt'
# ET: n_estimators=500, max_depth=8, min_samples_leaf=2
# AdaBoost: n_estimators=500, learning_rate=0.75
# GB: n_estimators=500, max_depth=5, min_samples_leaf=2
# SVC: kernel='linear', C=0.025 (or kernel='rbf' with probability=True)
# XGB meta: n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8, colsample_bytree=0.8
```

### 2. Add Missing Features (Before or with stacking)
- **Deck:** Extract first letter of Cabin (A, B, C, D, E, F, G)
- **Age binning:** Child (<16), Teen (16-32), Adult (32-48), Senior (>48)
- **Fare binning:** Quartiles or quintiles
- **Sex_Pclass interaction:** Combine Sex and Pclass (female_1, female_2, etc.)
- **Ticket frequency:** Count passengers sharing same ticket (group travel)

### 3. Try Different Feature Subsets
Given the CV-LB gap, try:
- Simpler feature set (fewer features may generalize better)
- Remove Name_Length (may be overfitting)
- Focus on most robust features: Sex, Pclass, Title, FamilySize, Age, Fare

### 4. Hyperparameter Tuning (Lower Priority)
Only after stacking is implemented:
- GridSearchCV for base model parameters
- Focus on regularization parameters (max_depth, min_samples_leaf)

## What NOT to Try
- Single model hyperparameter tuning without stacking (diminishing returns)
- Complex neural networks (dataset too small, 891 samples)
- Chasing 100% accuracy (unrealistic target)

## Validation Notes
- Use Stratified 5-Fold CV (consistent with baseline)
- CV score of ~0.83 corresponds to LB ~0.77 (calibration factor)
- Trust CV trends but expect ~5% drop on LB
- For stacking: use proper out-of-fold predictions to avoid leakage

## Experiment Naming
- Next experiment: `experiments/002_stacking/` - Stacking ensemble with 5 base models + XGBoost meta-learner
- Include both enhanced features AND stacking in same experiment for efficiency

## Success Criteria
- CV score > 0.84 (would suggest LB ~0.79 based on calibration)
- If CV improves significantly, submit to verify LB improvement