{
  "query": "What are the best techniques to achieve highest accuracy on Titanic Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on several community tutorials and top\u2011scoring notebooks, the most effective way to push Titanic\u2011competition accuracy into the high\u201180\u202f% range is to combine thorough data cleaning, targeted feature engineering, and a well\u2011tuned tree\u2011based model (often Random\u202fForest or a gradient\u2011boosting variant) with cross\u2011validated hyper\u2011parameter search\u30104\u2020https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\u3011\u30109\u2020https://python.plainenglish.io/kaggle-titanic-challenge-training-fine-tuning-the-model-42310439ae7\u3011.  \n\n**Step\u2011by\u2011step recipe**\n\n1. **Load & inspect the data** \u2013 read `train.csv` and `test.csv` with pandas, list missing values in *Age*, *Cabin*, *Embarked* and examine basic statistics.  \n\n2. **Impute & create missing\u2011value flags**  \n   * Fill missing *Age* with the median (or a model\u2011based estimate).  \n   * Fill missing *Embarked* with the mode and create a binary *CabinPresent* flag (1\u202fif Cabin not null, else\u202f0).  \n   * Optionally encode the original *Cabin* letter as a categorical feature.  \n\n3. **Feature engineering** \u2013 the biggest accuracy gains come from engineered variables:  \n   * **FamilySize** = `SibSp + Parch + 1`  \n   * **IsAlone** = 1\u202fif *FamilySize*\u202f=\u202f1 else\u202f0  \n   * **Title** extracted from the *Name* field (e.g., Mr, Mrs, Miss, Master, Rare)  \n   * **Age bins** (e.g., child, teen, adult, senior) and **Fare bins** to reduce outlier impact  \n   * Encode *Sex*, *Embarked*, *Title*, and *CabinPresent* as numeric (LabelEncoder or one\u2011hot).  \n\n   These engineered columns lifted baseline scores from ~72\u202f% to ~78\u202f% in a recent repo\u30104\u2020https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\u3011.  \n\n4. **Model selection & baseline** \u2013 train a **Random Forest** (or XGBoost/LightGBM) on the processed data. Random Forest alone has achieved 84.35\u202f% accuracy in a public notebook\u30103\u2020https://github.com/alicevillar/titanic-kaggle\u3011.  \n\n5. **Hyper\u2011parameter fine\u2011tuning** \u2013 use stratified K\u2011fold cross\u2011validation (e.g., 5\u2011fold) to search over:  \n   * `n_estimators` (200\u20111000)  \n   * `max_depth` (5\u201115)  \n   * `min_samples_split` / `min_samples_leaf`  \n   * `max_features` (sqrt, log2, or a fraction)  \n\n   The fine\u2011tuned Random Forest in the \u201cJourney to 100\u202f% Accuracy\u201d notebook placed the author in the top\u202f3\u202f%\u30109\u2020https://python.plainenglish.io/kaggle-titanic-challenge-training-fine-tuning-the-model-42310439ae7\u3011.  \n\n6. **Ensemble / stacking (optional)** \u2013 blend predictions from the tuned Random Forest with a Gradient Boosting model (XGBoost/LightGBM) and a simple Logistic Regression. Weighted averaging or a meta\u2011learner often nudges the public\u2011leaderboard score a few points higher, as demonstrated in multiple community solutions (e.g., 0.8134 public score)\u30102\u2020https://www.ahmedbesbes.com/blog/kaggle-titanic-competition\u3011.  \n\n7. **Final validation & submission** \u2013 after selecting the best hyper\u2011parameters, train on the full training set, generate predictions for `test.csv`, and submit. Verify that the cross\u2011validated accuracy on the hold\u2011out split exceeds your target (\u2265\u202f0.84) before uploading.  \n\nFollowing these concise steps\u2014clean missing data, engineer family\u2011size, title, and age/fare bins, encode categoricals, and fine\u2011tune a Random Forest (or boosted\u2011tree) model\u2014has consistently produced the highest accuracies reported for the Titanic Kaggle competition.",
      "url": ""
    },
    {
      "title": "How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge | Ahmed BESBES",
      "text": "September 10, 2016\u00a0\u00a0\u00a0\u00a0\u00a033min read\n\n# How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge\n\nThe [Titanic challenge](https://www.kaggle.com/c/titanic/) hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing him such as his age, his sex, or his passenger class on the boat.\n\nI have been playing with the Titanic dataset for a while, and I have recently achieved an accuracy score of 0.8134 on the public leaderboard. As I'm writing this post, I am ranked among the top 4% of all Kagglers.\n\nThis post is the opportunity to share my solution with you.\n\nTo make this tutorial more \"academic\" so that anyone could benefit, I will first start with an exploratory data analysis (EDA) then I'll follow with feature engineering and finally present the predictive model I set up.\n\nThroughout this jupyter notebook, I will be using Python at each level of the pipeline.\nThe main libraries involved in this tutorial are:\n\n- **Pandas** for data manipulation and ingestion\n- **Matplotlib** and **seaborn** for data visualization\n- **Numpy** for multidimensional array computing\n- **sklearn** for machine learning and predictive modeling\n\n### Installation procedure\n\nA very easy way to install these packages is to download and install the [Conda](http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install) distribution that encapsulates them all. This distribution is available on all platforms (Windows, Linux and Mac OSX).\n\n### Nota Bene\n\nThis is my first attempt as a blogger and as a machine learning practitioner.\n\nIf you have a question about the code or the hypotheses I made, do not hesitate to post a comment in the comment section below.\nIf you also have a suggestion on how this notebook could be improved, please reach out to me.\nThis tutorial is available on my [github](https://github.com/ahmedbesbes/How-to-score-0.8134-in-Titanic-Kaggle-Challenge) account.\n\nHope you've got everything set on your computer. Let's get started.\n\n## I - Exploratory data analysis\n\nAs in different data projects, we'll first start diving into the data and build up our first intuitions.\n\nIn this section, we'll be doing four things.\n\n- Data extraction : we'll load the dataset and have a first look at it.\n- Cleaning : we'll fill in missing values.\n- Plotting : we'll create some interesting charts that'll (hopefully) spot correlations and hidden insights out of the data.\n- Assumptions : we'll formulate hypotheses from the charts.\n\nWe tweak the style of this notebook a little bit to have centered plots.\n\n```\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\");\n```\n\nWe import the useful libraries.\n\n```\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nimport pylab as plot\nparams = {\n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n```\n\nTwo datasets are available: a training set and a test set.\nWe'll be using the training set to build our predictive model and the testing set to score it and generate an output file to submit on the Kaggle evaluation system.\n\nWe'll see how this procedure is done at the end of this post.\n\nNow let's start by loading the training set.\n\n```\ndata = pd.read_csv('./data/train.csv')\nprint(data.shape)\n#(891, 12)\n```\n\nWe have:\n\n- 891 rows\n- 12 columns\n\nPandas allows you to have a sneak peak at your data.\n\n```\ndata.head()\n```\n\n| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n\nThe Survived column is the **target variable**. If Suvival = 1 the passenger survived, otherwise he's dead. The is the variable we're going to predict.\n\nThe other variables describe the passengers. They are the **features**.\n\n- PassengerId: and id given to each traveler on the boat\n- Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n- The Name of the passeger\n- The Sex\n- The Age\n- SibSp: number of siblings and spouses traveling with the passenger\n- Parch: number of parents and children traveling with the passenger\n- The ticket number\n- The ticket Fare\n- The cabin number\n- The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q\n\nPandas allows you to a have a high-level simple statistical description of the numerical features.\nThis can be done using the describe method.\n\n```\ndata.describe()\n```\n\n| PassengerId | Survived | Pclass | Age | SibSp | Parch | Fare |\n| --- | --- | --- | --- | --- | --- | --- |\n| count | 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 |\n| mean | 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 |\n| std | 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 |\n| min | 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 |\n| 25% | 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 |\n| 50% | 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 |\n| 75% | 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 |\n| max | 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 |\n\nThe count variable shows that 177 values are missing in the Age column.\n\nOne solution is to fill in the null values with the median age. We could also impute with the mean age but the median is more robust to outliers.\n\n```\ndata['Age'] = data['Age'].fillna(data['Age'].median())\n```\n\nLet's now make some charts.\n\nLet's visualize survival based on the gender.\n\n```\ndata['Died'] = 1 - data['Survived']\ndata.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, colors=['g', 'r']);\n```\n\nIt looks like male passengers are more likely to succumb.\n\nLet's plot the same graph but with ratio instead.\n\n```\ndata.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                           stacked=True, colors=['g', 'r']);\n```\n\nThe Sex variable seems to be a discriminative feature. Women are more likely to survive.\n\nLet's now correlate the survival with the age variable.\n\n```\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Sex', y='Age',\n               hue='Survived', data=data,\n               split=True,\n               palette={0: \"r\", 1: \"g\"}\n              );\n```\n\nAs we saw in the chart above and validate by the following:\n\n- Women survive more than men, as depicted by the larger female green histogram\n\nNow, we see that:\n\n- The age conditions the survival for male passengers:\n\n  - Younger male tend to survive\n  - A large number of passengers between 20 and 40 succumb\n- The age doesn't seem to have a direct impact on the female survival\n\nThese violin plots confirm that one old code of conduct that sailors and captains follow in case of threatening situations: **\"Women and children first !\"**.\n\nRight?\n\nLet's now focus on the Fare ticket of each passenger and see how it could impact the survival.\n\n```\nfigure = plt.figure(figsize=(25, 7))\nplt.hist([dat...",
      "url": "https://www.ahmedbesbes.com/blog/kaggle-titanic-competition"
    },
    {
      "title": "GitHub - alicevillar/titanic-kaggle: Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%",
      "text": "[Skip to content](https://github.com/alicevillar/titanic-kaggle#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[alicevillar](https://github.com/alicevillar)/ **[titanic-kaggle](https://github.com/alicevillar/titanic-kaggle)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle) You must be signed in to change notification settings\n- [Fork\\\n3](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n- [Star\\\n6](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n\n\nTitanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%\n\n[6\\\nstars](https://github.com/alicevillar/titanic-kaggle/stargazers) [3\\\nforks](https://github.com/alicevillar/titanic-kaggle/forks) [Branches](https://github.com/alicevillar/titanic-kaggle/branches) [Tags](https://github.com/alicevillar/titanic-kaggle/tags) [Activity](https://github.com/alicevillar/titanic-kaggle/activity)\n\n[Star](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n\n[Notifications](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle) You must be signed in to change notification settings\n\n# alicevillar/titanic-kaggle\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/alicevillar/titanic-kaggle/branches) [Tags](https://github.com/alicevillar/titanic-kaggle/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[13 Commits](https://github.com/alicevillar/titanic-kaggle/commits/main/) |\n| ### [README.md](https://github.com/alicevillar/titanic-kaggle/blob/main/README.md) | ### [README.md](https://github.com/alicevillar/titanic-kaggle/blob/main/README.md) |  |  |\n| ### [Titanic\\_DecisionTree.ipynb](https://github.com/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) | ### [Titanic\\_DecisionTree.ipynb](https://github.com/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) |  |  |\n| ### [accuracy\\_graph\\_titanic.png](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png) | ### [accuracy\\_graph\\_titanic.png](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png) |  |  |\n| ### [titanic.csv](https://github.com/alicevillar/titanic-kaggle/blob/main/titanic.csv) | ### [titanic.csv](https://github.com/alicevillar/titanic-kaggle/blob/main/titanic.csv) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN\n\nThe objective of Kaggle's Titanic Challenge was to build a classification model that could successfully predict the survival or the death of a given passenger based on a set of variables. The purpose of this repository is to document the process I went through to create a Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN.\n\n### Quick Start:\n\n[Check out](https://nbviewer.jupyter.org/github/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) a static version of the notebook with Jupyter NBViewer from the comfort of your web browser.\n\n### Dependencies:\n\n- [Numpy](https://numpy.org/)\n- [Pandas](https://pandas.pydata.org/)\n- [SciKit-Learn](https://scikit-learn.org/)\n- [Matplotlib](https://matplotlib.org/)\n- [Seaborn](https://seaborn.pydata.org/)\n\n### Approach\n\n- PART 1: Data Handling -> Importing Data with Pandas, cleaning data, data description.\n- PART 2: Data Analysis -> Supervised ML Techniques: Decision Tree, SVM, Logistic Regression, Random Forest and KNN\n- PART 3: Valuation of the Analysis -> Performance measurement + K-folds cross validation to evaluate results locally + Accuracy comparison graph\n\n### Kaggle Competition \\| Titanic Machine Learning from Disaster\n\nKaggle's challenge provides information about a subset of the Titanic population and asks a predictive model that tells whether or not a given passenger survived.\nWe are given basic explanatory variables, including passenger gender, age, passenger class, among others. More details about the competition can be found on Kaggle's [Competition Page](https://www.kaggle.com/c/titanic):\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. In this contest, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\n### Results\n\nIn this repository I documented the process to create a Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. To all these modeles I did a Valuation Analisis (Performance Measurement and K-Fold). I have found the accuracy score with Random forest (0.8435754189944135) and Decision Tree (0.8212290502793296).Finally, I created a graph to compare the accuracy of the different models.\n\n[![print](https://github.com/alicevillar/titanic-kaggle/raw/main/accuracy_graph_titanic.png)](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png)\n\n## About\n\nTitanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [numpy](https://github.com/topics/numpy) [sklearn](https://github.com/topics/sklearn) [pandas](https://github.com/topics/pandas) [seaborn](https://github.com/topics/seaborn) [titanic-kaggle](https://github.com/topics/titanic-kaggle) [logistic-regression](https://github.com/topics/logistic-regression) [matplotlib](https://github.com/topics/matplotlib) [decision-trees](https://github.com/topics/decision-trees) [kfold-cross-validation](https://github.com/topics/kfold-cross-validation) [machine-learning-projects](https://github.com/topics/machine-learning-projects) [knn-algorithm](https://github.com/topics/knn-algorithm) [support-vector-classification](https://github.com/topics/support-vector-classification) [performance-measurements](https://github.com/topics/performance-measurements)\n\n### Resources\n\n[Readme](https://github.com/alicevillar/titanic-kaggle#readme-ov-file)\n\n[Activity](https://github.com/alicevillar/titanic-kaggle/activity)\n\n### Stars\n\n[**6**\\\nstars](https://github.com/alicevillar/titanic-kaggle/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/alicevillar/titanic-kaggle/watchers)\n\n### Forks\n\n[**3**\\\nforks](https://github.com/alicevillar/titanic-kaggle/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Falicev...",
      "url": "https://github.com/alicevillar/titanic-kaggle"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "Kaggle Titanic Challenge: Training & Fine-Tuning the Model",
      "text": "Aman Krishna Kaggle Titanic Challenge: Training & Fine-Tuning the Model https://python.plainenglish.io/kaggle-titanic-challenge-training-fine-tuning-the-model-42310439ae7\nKaggle Titanic Challenge: Training & Fine-Tuning the Model\nAman Krishna\n2022-04-20T07:23:42Z\n# Kaggle Titanic Challenge: Training & Fine-Tuning the Model\n\n## The art of fine-tuning a model\n\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--42310439ae7---------------------------------------)\n\n7 min read\n\n\nApr 19, 2022\n\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nPhoto by [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n**To get into the top 5% of this competition, you need two main ingredients. First, good features that represent the data. Second, a model fine-tuned to the problem.**\n\n**We have already created our** [**_features_**](https://medium.com/python-in-plain-english/kaggle-titanic-challenge-create-them-features-a324ba577812) **and now we shall create an excellent model that got me in the top 3%.**\n\nYou can find the complete code on Github **_:_**\n\n[**kaggle_titanic/Model Training.ipynb at main \u00b7 AmanKrishna/kaggle_titanic** \n\n**Machine Learning model build for survivor prediction for Kaggle's titanic competition - kaggle_titanic/Model\u2026**\n\ngithub.com]\n\n## Which Model to use?\n\n**We have a very small dataset of 891 passengers.** This eliminates all the Deep learning approaches. What we are left with are simpler models like Logistic regression, Bagging Trees (Random Forest), Boosted Trees (XGBoost, AdaBoost & CatBoost), KNN (K-Nearest Neighbours), and SVMs.\n\nI have experimented with all of them. **For me, the one that worked the best was Random Forest.** You can play around with others for learning.\n\nHere I will teach you how to **fine-tune** a Random Forest tree to extract the best results. The same methodology can be applied to other models as well.\n\n## Pre-Requisite\n\nFirst, we need to set up our training and testing data by [**_feature creation_**](https://medium.com/python-in-plain-english/kaggle-titanic-challenge-create-them-features-a324ba577812) & [**_handling missing values_**](https://medium.com/python-in-plain-english/kaggle-titanic-competition-missing-values-f3280267b361) **_._** You can find the combined code on my [**_Github repo_**](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/Model%20Training.ipynb).\n\n### Dropping Features\n\n**Not all features are created equal.** I experimented with a combination of features to see which one works best and which can be dropped. We definitely do not need **PassengerId** & **Name** of passengers.\n\nMoreover, we create **Age** & **Fare bins** in our feature engineering articles. But I found that using **Age**& **Fare** directly was far more useful.\n\n### Code\n\n### Dummify Data\n\nWe have many categorical features like **Sex**. Now Sex takes two values _male_ and _female_. But o **ur model cannot directly use these values.** We can handle categorical data in two ways:\n\n- **Label Encoding**: Where each categorical value is assigned a number like _0, 1, 2,_ etc. This is useful mostly for **Ordinal features** which have an inherent ranking. For example, if we have a feature **Review** for movie rating which takes values _Good_, _Average_ & _Bad_. We can assign _Good_: 3, _Average_: 2 & _Bad_: 1.\n- **One-Hot Encoding:** This is used mostly for **Nominal features** where values are not related to each other. Like in our case **Sex**. Now the value _male_ is not lesser or greater than _female_. So we create 2 new features from **Sex**- **Sex_Female**- **Sex_Male** Now **Sex_Female** has a value of _1_ if the passenger was a female else it is _0_. Similarly for **Sex_Male**. This way we split the original feature into N new features where N = the Number of Unique Values of that feature.\n\n**We will be using One-Hot encoding on our categorical features.**\n\n### Code\n\n### Split Data\n\nFinally, we split our data into train & Test. We started with **12 Features**. After creating new features and adding dummies for categorical Features we finally have **1986 features**. Most of them are due to one-hot encoding of Family Names, Ticket Numbers & Cabin Numbers.\n\nI experimented with dropping most of them and keeping only a few. But my best result came when I used all of them.\n\n## Training\n\nWe will be using the **Random forest** model for classification. The following are the most important parameters of a Random Forest.\n\n- **n_estimators**: Number of trees in our **Random Forest (RF)** Value: _1 to Infinity_\n- **max_depth**: Maximum depth of a tree in RFValue: _1 to Infinity_\n- **max_samples**: Maximum Samples used while creating a tree in RFValue: _0.1 to 1_\n- **max_features**: Maximum Features used while creating a tree in RFValue: _[\u201csqrt\u201d, \u201cauto\u201d, \u201clog\u201d]_\n- **min_samples_split**: The minimum number of samples required to split an internal node of a tree in RFValue: _1 to infinity_\n- **min_samples_leaf**: The minimum number of samples required to be at a leaf node of a tree in RFValue: _1 to infinity_\n\n**We will start with a simple Random Forest with n_estimators = 101.** First fine-tuning other hyper-parameters and then fine-tuning the n_estimators in the end.\n\nThe logic is that more estimators in our forest will result in a better representation of the data. Hence if our other hyper-parameters are fine-tuned for a smaller forest they will perform even better when the forest size is increased! (Not always true)\n\n### Code\n\n## Fine Tuning\n\nFirst, we shall fine-tune **max_depth, max_samples, max_features & criterion.** **Sklearn** offers an excellent tool **Grid Search** to search through different values of hyper-parameters & give us the best one!\n\n**For example**, to fine-tune **max_depth** we can search through values 2 to 20. **GridSearch** will simply use **max_depth** values 2, 3, 4 \u2026. 20 to fit the model to our training data. It will then give users the values which are best for the current data.\n\n### Code\n\n### Output\n\nOur Grid Search results say that out of all the combination the ones which gave the best accuracy (0.8608) are **- criterion:** _gini_ **- max_depth:** _19_ **- max_features:** _auto_ **- max_samples:** _0.5_\n\nPress enter or click to view image in full size\n\nGrid Search Results\n\n## Refining Search Space: Max Depth & Max Samples\n\nWe shall fix Criterion & Max Features and try to refine search space for Max Depth & Max samples.\n\n### How?\n\nEarlier the search space for Max Samples was 0.1 to 1 with steps of 0.1. Meaning our grid search was looking at 10 possible values for Max Samples:[0.1, 0.2, 0.3, \u2026]. The result it gave us was 0.5\n\nNow we will further refine our search between 0.4 to 0.6 with a step size of 0.01 i.e. now our Grid Search will look at 20 possible values[0.4, 0.41, \u2026, 0.].\n\nSimilarly, we will fine-tune our search for Max Depth between 18 to 25 with a step size of 1.\n\n### Code:\n\n### Output:\n\nPress enter or click to view image in full size\n\nBest max_Depth & max_samples\n\n## Fine Tune: Min Sample Split & Min Sample Leaf\n\nFor both **min_samples_leaf**& **min_sample_split,** the search space is from 2 to 21 with a step size of 1.\n\n### Code:\n\n### Output:\n\nPress enter or click to view image in full size\n\nBest min_samples_leaf & min_samples_split\n\n**The abrupt drop in Mean Accuracy scared me as well. But turns out the model performed better on Tests with these values rather than the default values of min sample leaf/split which gave a higher mean accuracy on training data. I think the reason is that the model is generalizing better!**\n\n## Fine Tune: Number of Estimators\n\nWe will start with a broad range for the Number of estimators. Starting from 11 all the way to 5001 with a step size of 500. Later we shall refine our search.\n\n### Code:\n\n### Output:\n\nPress enter or click to view image in full size\n\nBest n_estimator value\n\n## Refining Search Space: Number of estima...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-training-fine-tuning-the-model-42310439ae7"
    },
    {
      "title": "How to score top 3% in Kaggle\u2019s ,,Titanic \u2014 Machine Learning from Disaster\u201d competition",
      "text": "<div><div><div><a href=\"https://anelmusic13.medium.com/?source=post_page---byline--13d056e262b1---------------------------------------\"><div><p></p></div></a></div><p>Hey, fellow machine learning enthusiasts. If you\u2019ve ever had a try at a Kaggle competition, chances are you\u2019re already familiar with the Titanic dataset. <br/>This competition is about predicting whether a passenger will survive the Titanic disaster or not.</p><p>With relatively little effort it is possible to rank among the top 30% of participants. Unfortunately, many of the top scorers train their model on an external dataset and thus obtain a model that classifies the test data with 100% accuracy. I couldn\u2019t tell you why someone would want to cheat in such an obvious way but what this essentially means is that you have to make an extra effort to get into the top 3%.</p><p><strong>Aim</strong> of this article:</p><ul><li>Explain step by step the end-to-end data pipeline that is needed to score among top 3%.</li><li>Discuss the thought process of a Machine Learning Engineer / Data Scientist in Data Cleaning and Feature Engineering.</li><li>Make it more difficult for future participants to stand out :)</li></ul><p><strong>You can find the complete code at:</strong></p><p><a href=\"https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis\">https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis</a></p><h2>1. Getting the data:</h2><figure><figcaption>Import needed modules for handling tabular data, plotting and accessing common ML</figcaption></figure><figure><figcaption>Provide data paths</figcaption></figure><p>In my case the train.csv as well as test.csv are located in the current working directory. Using pandas we can read and convert the csv-files to pandas dataframes:</p><figure><figcaption>Read data csv and convert it to pandas dataframe</figcaption></figure><p>Next, we can investigate the data to get a better understanding of the available features:</p><figure><figcaption>Show train.csv pandas dataframe</figcaption></figure><h2>2. Exploratory Data Analysis</h2><p>Typically, it is not necessary to use each available feature. Many of them do not provide additional information for the model and increase the training time unnecessarily. However, a more crucial problem that is often overlooked is that additional features inevitably require more data. Now why is that?</p><p><strong>Why additional features call for additional data:</strong></p><p>If you think about it, each additional feature adds a new dimension to the feature space. This in turn increases the distance between observations because each new dimension adds a non-negative term to the calculation of the euclidean distance. Thus the feature space becomes sparse. In a sparse feature space ML algorithms tend to overfit to noise and the only way to populate your feature space is by adding additional (and rich) data.</p><p>As you can see it is essential to explore which features should be considered and which should not.</p><p>I would argue that most important skill of a Machine Learning Engineer/Data Scientist is to be unbiased, not assume things and to ask good questions.</p><p>So, let\u2019s ask some questions:</p><h2>2.1 Question 1: Does the dataset contain missing values?</h2><p>In the very first step it is helpful to check if and how many entries are missing in the dataset.</p><figure><figcaption>Dataset missing values summary</figcaption></figure><figure><figcaption>Define function that creates missing value heatmap</figcaption></figure><figure><figcaption>Missing values heatmap</figcaption></figure><p>From the above plots, it can be seen that the training and similarly the test datasets contain features with missing values. The sparsest features are \u201cage\u201d and \u201ccabin\u201d.</p><p>A naive approach to solve this problem would be to remove the feature completely from the dataset. However, since we do not know how much information these they provide they further investigation is needed. Maybe we find that it would make sense to impute the missing values using sophisticated data imputation methods.</p><h2>2.1 Question 1: How many passengers survived?</h2><figure><figcaption>Check survival rate</figcaption></figure><p>If you prefer plots you can define the function below to plot a bar chart:</p><figure><figcaption>Define function for plotting bar charts</figcaption></figure><figure><figcaption>Bar chart survival rate</figcaption></figure><p>As expected the majority of passengers in the training data died and only 38% survived. This means that the training data suffers from data imbalance but it is not severe which is why I will not consider techniques like sampling to tackle the imbalance.</p><figure><figcaption>Degree of minority class imbalance [shorturl.at/fTV37]</figcaption></figure><h2>2.2 Question 2: Is the likelihood of survival dependent on gender?</h2><p>In 1912, about 110 years ago, women were generally considered to be the weaker sex and should be protected. Let\u2019s investigate if the men of that time were so dutiful even under the fear of death.</p><figure><figcaption>Passenger count based on gender</figcaption></figure><figure><figcaption>Survival ratio based on gender</figcaption></figure><figure><figcaption>Survival rate bar chart</figcaption></figure><p>Here, we can clearly see that even though the majority of passenger were men, the majority of survivors were women. The key observation here is that the survival rate for female passengers is 4 times higher than the survival rate of male passengers. This seems to confirm that the phrase \u201cwomen and children first\u201d does indeed seem to have been a rule to which men adhered to.</p><h2>2.3 Question 3: Could it be that the class to which a passenger belonged correlates with the probability of survival??</h2><figure><figcaption>Passenger count with respect to class</figcaption></figure><figure><figcaption>Passenger class distribution</figcaption></figure><figure><figcaption>Bar chart passenger class vs dependent (target) variable ,,survived\u201d</figcaption></figure><p>From the plots and tables above it becomes clear that the Pclass is an important factor to consider.</p><p><strong>Key observations:</strong></p><ul><li>Most passenger had class 3 tickets, yet only 24% of class 3 passengers survived.</li><li>Almost 63% of the passenger from class 1 survived.</li><li>Approx 50% of the class 2 passenger survived.</li></ul><p>However, it is not clear yet weather the class or the gender is the underlying and deciding factor. Which brings another important question:</p><h2>2.4 Question 4: Is the higher survival rate in Class 1 due to the class itself or due to a skewed gender distribution in which female passengers dominate?</h2><figure><figcaption>Function that plots bar chart with multiple features</figcaption></figure><figure><figcaption>Bar chart of gender vs surival rate</figcaption></figure><figure><figcaption>Percentage of male and female survivors with respect to gender</figcaption></figure><p>Here, we can see that the question raised above was justified. Irrespective of the class the most important factor when it comes to survival was gender. (At least between the two features Sex and Pclass). However, men had a significantly higher chance of survival if they bought class 1 tickets. This just shows that we should keep both features as both yield insightful information that should help our model.</p><p><strong>Key observations:</strong></p><ul><li>Survival Rate females 1. Class: 96,8%</li><li>Survival Rate females 2. Class: 92,1%</li><li>Survival Rate females 3. Class: 50%</li><li>Survival Rate male 1. Class: 36.8% <br/>(still significantly lower than 3. class females)</li></ul><h2>2.5 Question 5: Did a passengers age influence the chance of survival?</h2><figure><figcaption>Utility function for plotting a histogram and the kernel density estimate</figcaption></figure><figure><figcaption>Age distribution</figcaption></figure><figure><figcaption>Age kernel density</f...",
      "url": "https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1"
    },
    {
      "title": "Titanic - Machine Learning from Disaster",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><div><div><a href=\"https://www.kaggle.com/organizations/kaggle\"></a><p><span><span><span>Kaggle </span></span><span> \u00b7 Getting Started Prediction Competition \u00b7 Ongoing</span></span></p></div><div><div><p><span></span></p><p>Start here! Predict survival on the Titanic and get familiar with ML basics</p><p></p></div><div><p></p></div></div></div><div><p></p><h2>Titanic - Machine Learning from Disaster</h2><p></p></div><div><div><p></p><h2>Notebooks</h2><p></p></div><div><ul><p>Pinned notebooks</p><li><div><div><p>Titanic Tutorial</p><p><span><span>Updated <span>3y ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/alexisbcook/titanic-tutorial/comments\">30259 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div><div><p><span>18036</span></p><p><span><span> Gold</span></span></p></div></div></li><li><div><p><span>1215</span></p><p><span><span> Gold</span></span></p></div></li><p>Unpinned notebooks</p><li><div><p>Titanic - Machine Learning from Disaster</p><p><span><span>Updated <span>6h ago</span></span> </span><span><span><span>Score: 0.77511</span> \u00b7 <a href=\"https://www.kaggle.com/code/mohibulhimu/titanic-machine-learning-from-disaster/comments\">1 comment</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Exercise: Arithmetic and Variables</p><p><span><span>Notebook copied with edits from <a href=\"https://www.kaggle.com/code/alexisbcook/exercise-arithmetic-and-variables\">Alexis Cook</a></span> \u00b7 <span>Updated <span>15h ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/abraham35/exercise-arithmetic-and-variables/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanic</p><p><span><span>Updated <span>4h ago</span></span> </span><span><span><span>Score: 0.7799</span> \u00b7 <a href=\"https://www.kaggle.com/code/maita27/titanic/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Survived_Titanic</p><p><span><span>Updated <span>16h ago</span></span> </span><span><span><span>Score: 0.78229</span> \u00b7 <a href=\"https://www.kaggle.com/code/thangnguyen12939445/survived-titanic/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Exercise: Arithmetic and Variables</p><p><span><span>Notebook copied with edits from <a href=\"https://www.kaggle.com/code/alexisbcook/exercise-arithmetic-and-variables\">Alexis Cook</a></span> \u00b7 <span>Updated <span>21h ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/zubairamuti/exercise-arithmetic-and-variables/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Exercise: Arithmetic and Variables</p><p><span><span>Notebook copied with edits from <a href=\"https://www.kaggle.com/code/alexisbcook/exercise-arithmetic-and-variables\">Alexis Cook</a></span> \u00b7 <span>Updated <span>12h ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/muhammadabdullah1612/exercise-arithmetic-and-variables/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanic Survival Prediction - Beginner Guide</p><p><span><span>Updated <span>12h ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/faizankhandeshmukh/titanic-survival-prediction-beginner-guide/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>LightGBMclassifierUm</p><p><span><span>Updated <span>13h ago</span></span> </span><span><span><span>Score: 0.77033</span> \u00b7 <a href=\"https://www.kaggle.com/code/usamamulla/lightgbmclassifierum/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>titanic_disaster </p><p><span><span>Updated <span>1d ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/trishoolin/titanic-disaster/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>titanic_problem</p><p><span><span>Updated <span>1d ago</span></span> </span><span><span><span>Score: 0.77511</span> \u00b7 <a href=\"https://www.kaggle.com/code/karansinghchauhann/titanic-problem/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanic | 83.14% Accu| Ensemble (Bagging + DT)</p><p><span><span>Updated <span>1d ago</span></span> </span><span><span><span>Score: 0.77272</span> \u00b7 <a href=\"https://www.kaggle.com/code/khushiirathod/titanic-83-14-accu-ensemble-bagging-dt/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanic - Machine Learning from Disaster - xgboost</p><p><span><span>Updated <span>8h ago</span></span> </span><span><span><span>Score: 0.72488</span> \u00b7 <a href=\"https://www.kaggle.com/code/mohibulhimu/titanic-machine-learning-from-disaster-xgboost/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>notebook2de88c4bdc</p><p><span><span>Updated <span>2h ago</span></span> </span><span><span><span>Score: 0.77033</span> \u00b7 <a href=\"https://www.kaggle.com/code/ahzaradsh/notebook2de88c4bdc/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li></li><li><div><div><p>Building an Interactive GUI in Kaggle with ipywidg</p><p><span><span>Updated <span>5d ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/zohaib123/building-an-interactive-gui-in-kaggle-with-ipywidg/comments\">3 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span><span><span><span> +9</span></span></span></span></span></span></p></div><div><p><span>13</span></p><p><span><span> Bronze</span></span></p></div></div></li><li><div><p>Titanic predict using R</p><p><span><span>Updated <span>2d ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/resious/titanic-predict-using-r/comments\">3 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanic Competition Code File No 554488522</p><p><span><span>Updated <span>2d ago</span></span> </span><span><span><span>Score: 0.77272</span> \u00b7 <a href=\"https://www.kaggle.com/code/hmdanyal/titanic-competition-code-file-no-554488522/comments\">1 comment</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>simple</p><p><span><span>Updated <span>1d ago</span></span> </span><span><span><span>Score: 0.77751</span> \u00b7 <a href=\"https://www.kaggle.com/code/blue68/simple/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><p>Titanik_E\u011fitim</p><p><span><span>Updated <span>2d ago</span></span> </span><span><span><a href=\"https://www.kaggle.com/code/kadirkrtls/titanik-e-itim/comments\">0 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></span></p></div></li><li><div><div><p>My Titanic Model (XGBoost,LogReg,GridSearchCV)</p><p><span><span>Updated <span>3d ago</span></span> </span><span><span><span>Score: 0.78708</span> \u00b7 <a href=\"https://www.kaggle.com/code/emirhngull/my-titanic-model-xgboost-logreg-gridsearchcv/comments\">5 comments</a> \u00b7 <span><span>Titanic - Machine Learning from Disaster</span></span></span></sp...",
      "url": "https://www.kaggle.com/c/titanic/code"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy]",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    }
  ]
}