{
  "query": "What techniques achieve highest accuracy on Titanic Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Extensive feature engineering (e.g., creating family\u2011size, title variables) combined with strong ensemble learners such as Random\u202fForest and rigorously tuned via k\u2011fold cross\u2011validation pushes Titanic competition accuracy above 92%\u202fas of\u202fMarch\u202f25\u202f2025\u202f([dataquest.io](https://www.dataquest.io/blog/kaggle-fundamentals), [kaggle.com](https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy)).",
      "url": ""
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "Kaggle Fundamentals: The Titanic Competition - Dataquest",
      "text": "March 25, 2025\n\n# Kaggle Fundamentals: The Titanic Competition\n\n[Kaggle](https://www.kaggle.com/) is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\n\nIn this tutorial we'll learn learn how to:\n\n- Approach a Kaggle competition\n- Explore the competition data and learn about the competition topic\n- Prepare data for machine learning\n- Train a model\n- Measure the accuracy of your model\n- Prepare and make your first Kaggle submission\n\nThis tutorial presumes you have an understanding of Python and the pandas library. If you need to learn about these, we recommend our [pandas tutorial](https://www.dataquest.io/blog/pandas-python-tutorial/) blog post.\n\n## The Titanic competition\n\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the [sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic).\n\nIn this [Titanic ML competition](https://www.kaggle.com/competitions/titanic), we have data about passengers onboard the Titanic, and we'll see if we can use that information to predict whether those people survived or not. Before we start looking at this specific competition, let's take a moment to understand how Kaggle competitions work.\n\nEach Kaggle competition has two key data files that you will work with - a **training** set and a **testing** set.\n\nThe training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict: in this case, `Survival`.\n\nThe testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.\n\nThis is useful because we want as much data as we can to train our model on. Once we have trained our model on the training set, we will use that model to make predictions on the data from the testing set, and submit those predictions to Kaggle.\n\nIn this competition, the two files are named `test.csv` and `train.csv`. We'll start by using [`pandas.read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read both files and then inspect their size.\n\n```\nimport pandas as pd\n\ntest = pd.read_csv(\"test.csv\")\ntrain = pd.read_csv(\"train.csv\")\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))\n\n```\n\n```\nDimensions of train: (891, 12)\nDimensions of test: (418, 11)\n\n```\n\n## Exploring the data\n\nThe files we just opened are available on [the data page for the Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/data). That page also has a **data dictionary**, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n\n- `PassengerID`\u2014 A column added by Kaggle to identify each row and make submissions easier\n- `Survived`\u2014 Whether the passenger survived or not and the value we are predicting _(0=No, 1=Yes)_\n- `Pclass`\u2014 The class of the ticket the passenger purchased _(1=1st, 2=2nd, 3=3rd)_\n- `Sex`\u2014 The passenger's sex\n- `Age`\u2014 The passenger's age in years\n- `SibSp`\u2014 The number of siblings or spouses the passenger had aboard the Titanic\n- `Parch`\u2014 The number of parents or children the passenger had aboard the Titanic\n- `Ticket`\u2014 The passenger's ticket number\n- `Fare`\u2014 The fare the passenger paid\n- `Cabin`\u2014 The passenger's cabin number\n- `Embarked`\u2014 The port where the passenger embarked _(C=Cherbourg, Q=Queenstown, S=Southampton)_\n\nThe data page on Kaggle has some additional notes about some of the columns. It's always worth exploring this in detail to get a full understanding of the data.\n\nLet's take a look at the first few rows of the `train` dataframe.\n\n```\ntrain.head()\n```\n\n| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n| 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S |\n| 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S |\n\nThe type of machine learning we will be doing is called **classification**, because when we make predictions we are classifying each passenger as 'survived' or not. More specifically, we are performing **binary classification**, which means that there are only two different states we are classifying.\n\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\n\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie [Titanic](https://en.wikipedia.org/wiki/Titanic_%281997_film%29) would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\n\nThis indicates that `Age`, `Sex`, and `PClass` may be good predictors of survival. We'll start by exploring `Sex` and `Pclass` by visualizing the data.\n\nBecause the `Survived` column contains `0` if the passenger did not survive and `1` if they did, we can segment our data by sex and calculate the mean of this column. We can use `DataFrame.pivot_table()` to easily do this:\n\n```\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot.plot.bar()\nplt.show()\n\n```\n\nWe can immediately see that females survived in much higher proportions than males did. Let's do the same with the `Pclass` column.\n\n```\nclass_pivot = train.pivot_table(index=\"Pclass\",values=\"Survived\")\nclass_pivot.plot.bar()\nplt.show()\n\n```\n\n## Exploring and converting the age column\n\nThe `Sex` and `PClass` columns are what we call **categorical** features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\n\nLet's take a look at the `Age` column using [`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html).\n\n```\ntrain[\"Age\"].describe()\n```\n\n```\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64\n```\n\nThe `Age` column contains numbers ranging from `0.42` to `80.0` (If you look at Kaggle's data page, it informs us that `Age` is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the `train` data set had earlier which indicates we have some missing values.\n\nAll of this means that the `Age` column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visually the those that survived vs those who died across different age ranges:\n\n```\nsurvived = train[train[\"Survived\"] == 1]\ndied = train[train[\"Survived\"] == 0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='red...",
      "url": "https://www.dataquest.io/blog/kaggle-fundamentals"
    },
    {
      "title": "How to Achieve more than 98% of accuracy on Titanic dataset - Reddit",
      "text": "Reddit - The heart of the internet[Skip to main content](#main-content)\n[\nGo to kaggle](https://www.reddit.com/r/kaggle/)\n[r/kaggle](https://www.reddit.com/r/kaggle/)\u2022\n[Rutvij07](https://www.reddit.com/user/Rutvij07/)\n[\u7e41\u9ad4\u4e2d\u6587](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=zh-hant)[Fran\u00e7ais](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=fr)[\u65e5\u672c\u8a9e](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=ja)[Ti\u1ebfng Vi\u1ec7t](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=vi)[\u0939\u093f\u0928\u094d\u0926\u0940](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=hi)[\u7b80\u4f53\u4e2d\u6587](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=zh-hans)[Espa\u00f1ol](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=es)[Polski](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=pl)[Deutsch](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=de)[T\u00fcrk\u00e7e](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=tr)[\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=el)[Magyar](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=hu)[Italiano](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=it)[Norsk (Bokm\u00e5l)](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=no)[Rom\u00e2n\u0103](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=ro)[Nederlands](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=nl)[Espa\u00f1ol (Espa\u00f1a)](https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on/?tl=es-es)\n# How to Achieve more than 98% of accuracy on Titanic dataset\n[How to Achieve more than 98% of accuracy on Titanic dataset](https://medium.com/@rutvij.bhutaiya_82611/how-to-achieve-more-than-98-of-accuracy-on-titanic-dataset-87241c18161a)\nMany machine learning enthusiast starts with Kaggle data and the commonplace is Titanic. Here you can take tips to increase the accuracy.\nProject Link:[https://github.com/RutvijBhutaiya/The-Famous-Titanic-Study](https://github.com/RutvijBhutaiya/The-Famous-Titanic-Study)\n[![r/kaggle - How to Achieve more than 98% of accuracy on Titanic dataset](https://preview.redd.it/7n735llkv1x31.png?width=590&amp;format=png&amp;auto=webp&amp;s=63ac3ec124b13c849b8751458d2b14fd8f80cf8c)](https://preview.redd.it/7n735llkv1x31.png?width=590&amp;format=png&amp;auto=webp&amp;s=63ac3ec124b13c849b8751458d2b14fd8f80cf8c)\nRead more\nShare\n# Related Answers Section\nRelated Answers\n[\nTop Kaggle competitions to watch this year\n](https://www.reddit.com/answers/460069dc-39ae-4c12-9c03-dbc425324d00/?q=Top%20Kaggle%20competitions%20to%20watch%20this%20year&amp;source=PDP)\n[\nBest datasets for beginners in data science\n](https://www.reddit.com/answers/85f7ff62-2b6f-43a7-b3d8-5db11bb47444/?q=Best%20datasets%20for%20beginners%20in%20data%20science&amp;source=PDP)\n[\nTips for winning Kaggle competitions\n](https://www.reddit.com/answers/bec5e2ca-5fa8-4325-9934-d2fd48e5ab95/?q=Tips%20for%20winning%20Kaggle%20competitions&amp;source=PDP)\n[\nLatest trends in machine learning on Kaggle\n](https://www.reddit.com/answers/8a4dd77a-0685-4b8e-9761-742c633ac271/?q=Latest%20trends%20in%20machine%20learning%20on%20Kaggle&amp;source=PDP)\n[\nHow to effectively use Kaggle Notebooks\n](https://www.reddit.com/answers/099624e9-83b0-4cd5-8e78-d43b7513ae6a/?q=How%20to%20effectively%20use%20Kaggle%20Notebooks&amp;source=PDP)\nNew to Reddit?\nCreate your account and connect with a world of communities.\nContinue with Email\nContinue With Phone Number\nBy continuing, you agree to our[User Agreement](https://www.redditinc.com/policies/user-agreement)and acknowledge that you understand the[Privacy Policy](https://www.redditinc.com/policies/privacy-policy).\nPublic\nAnyone can view, post, and comment to this community\n00\n## Top Posts\n* [\nReddit\nreReddit: Top posts of November 6, 2019\n](https://www.reddit.com/posts/2019/november-6-1/global/)\n* [\nReddit\nreReddit: Top posts of November 2019\n](https://www.reddit.com/posts/2019/november/global/)\n* [\nReddit\nreReddit: Top posts of 2019\n](https://www.reddit.com/posts/2019/global/)\n[Reddit Rules](https://www.redditinc.com/policies/content-policy)[Privacy Policy](https://www.reddit.com/policies/privacy-policy)[User Agreement](https://www.redditinc.com/policies/user-agreement)[Accessibility](https://support.reddithelp.com/hc/sections/38303584022676-Accessibility)[Reddit, Inc. \u00a92025. All rights reserved.](https://redditinc.com)\nExpand NavigationCollapse Navigation",
      "url": "https://www.reddit.com/r/kaggle/comments/dsf4gx/how_to_achieve_more_than_98_of_accuracy_on"
    },
    {
      "title": "How to win your first Kaggle competition? - dataroots",
      "text": "# How to win your first Kaggle competition?\n\n[Get In Touch ->](https://dataroots.io/contact-us)\n\n[Careers](https://dataroots.io/careers)\n\n[Our DNA](https://dataroots.io/our-dna)\n\n[Blog](https://dataroots.io/blog)\n\n[Podcast](https://dataroots.io/datatopics)\n\nBy Adrien Debray, Johannes Lootens\n\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\n\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n\n## All you need to know about Kaggle competitions\n\nKaggle competition overview page\n\n\ud83d\udca1 Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\n\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\n\nFirstly there are the **getting-started competitions**, such as the [Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io) or [Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io) ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the **community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\n\nWhat actually tends to attract people to Kaggle are the **competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\n\nEvery single one of those competitions is defined by a **dataset** and an **evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\n\nWhile the **train set** is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the **public leaderboard test set** which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the **private leaderboard test set.** This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\n\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n\n### Notebooks\n\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\n\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\n\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n\n## How to take the W in a Kaggle competition ?\n\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\n\nWe participated in the [Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io) competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n\n### 1\\. Have a good understanding of the competition and how to tackle the problem\n\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n\n- Read the competition overview and linked resources thoroughly\n- Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n- Check existing literature on approaches that were tried/succeeded in solving this or similar problems\n\n### 2\\. Get inspired by other participants\u2019 work to get started\n\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n\nGo in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\n\nBased on your readings, choose a clear and simple notebook with a decent LB score as **baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n\n### 3\\. Improve your model in an efficient way\n\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n\n##### Create datasets for intermediate results / preprocessed data\n\nSaved preprocessed datasets and trained models will make your results comparison more \u201cfair\u201d and will save you precious GPU time by avoiding repetitive tasks.\n\nAccordingly, your work structure should avoid having big complicated notebooks but rather simple training and inference notebooks taking the processed data as input.\n\nBasic inputs for the inference notebook\n\n**Efficient GPU Usage**\n\nKaggle provides 30 hours / week of free access to several accel...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    },
    {
      "title": "How to further improve the kaggle titanic submission accuracy?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How to further improve the kaggle titanic submission accuracy?](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 10 months ago\n\nModified [7 years, 10 months ago](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?lastactivity)\n\nViewed\n7k times\n\n4\n\n$\\\\begingroup$\n\nI am working on the Titanic dataset. So far my submission has 0.78 score using soft majority voting with logistic regression and random forest. As for the features, I used Pclass, Age, SibSp, Parch, Fare, Sex, Embarked.\n\nMy question is how to further boost the score for this classification problem?\n\nOne thing I tried is to add more classifiers for the majority voting, but it does not help, it even worthens the result. How do I understand this worthening effect?\n\nThanks for your insight.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)\n\n[Share](https://datascience.stackexchange.com/q/13104)\n\n[Improve this question](https://datascience.stackexchange.com/posts/13104/edit)\n\nFollow\n\n[edited Jul 30, 2016 at 14:03](https://datascience.stackexchange.com/posts/13104/revisions)\n\nnos\n\nasked Jul 30, 2016 at 13:15\n\n[![nos's user avatar](https://www.gravatar.com/avatar/4ba32c06ed067564a6c6512de5217657?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21955/nos)\n\n[nos](https://datascience.stackexchange.com/users/21955/nos) nos\n\n15311 silver badge66 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n5\n\n$\\\\begingroup$\n\nbig question.\n\nOk so here's a few things I'd look at if I was you..\n\n1. Have you tried any feature engineering ?(it sounds like you've just used the features in the training set but I can't be 100%)\n2. Random Forests should do pretty well, but maybe try xgboost too? It's quite good at everything on Kaggle. SVM's could be worth a go also if you're thinking of stacking/ensembling.\n3. Check out some of the tutorials around this competition. There's hundreds of them and most of them are great.\n\n**Links:**\n\n[R #1 (my favourite)](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/)\n\n[R #2](http://www.numbrcrunch.com/blog/the-good-ol-titanic-kaggle-competition-pt-1)\n\n[Python #1](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)\n\n[Python #2](https://github.com/agconti/kaggle-titanic)\n\n...Hopefully this helps\n\n[Share](https://datascience.stackexchange.com/a/13139)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13139/edit)\n\nFollow\n\nanswered Aug 2, 2016 at 2:52\n\n[![plumbus_bouquet's user avatar](https://www.gravatar.com/avatar/031741853441d7c93e9a50ffbabffb86?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21702/plumbus-bouquet)\n\n[plumbus\\_bouquet](https://datascience.stackexchange.com/users/21702/plumbus-bouquet) plumbus\\_bouquet\n\n34011 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nOkay, I am currently at 0.81340 in the competiton. And I will just clear out certain things. I would suggest that you try feature engineering before you go for ensemble methods. There are actually quite a decent tutorials as mentioned before.\nOne can actually score upto at least 0.82 just relying on feature engineering and a ten-fold cross validated RandomForest.\nCertain things for you to ponder at:\n\n- Look at the Age, what other information does it give you.\n- Do SibSp and Parch actually represent different things?\n- Can you get something out of the Name of the passengers?\n\nAll the Best.\n\nCheers.\n\n[Share](https://datascience.stackexchange.com/a/13223)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13223/edit)\n\nFollow\n\nanswered Aug 5, 2016 at 7:35\n\n[![Himanshu Rai's user avatar](https://graph.facebook.com/10205672827392400/picture?type=large)](https://datascience.stackexchange.com/users/21608/himanshu-rai)\n\n[Himanshu Rai](https://datascience.stackexchange.com/users/21608/himanshu-rai) Himanshu Rai\n\n1,8481212 silver badges1010 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f13104%2fhow-to-further-improve-the-kaggle-titanic-submission-accuracy%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [classification](https://datascience.stackexchange.com/questions/tagged/classification) - [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Linked\n\n[0](https://datascience.stackexchange.com/q/57708) [What should be the criteria to select features using correlation factors between features?](https://datascience.stackexchange.com/questions/57708/what-should-be-the-criteria-to-select-features-using-correlation-factors-between?noredirect=1)\n\n#### Related\n\n[1](https://datascience.stackexchange.com/q/5119) [Kaggle Titanic Survival Table an example of Naive Bayes?](https://datascience.stackexchange.com/questions/5119/kaggle-titanic-survival-table-an-example-of-naive-bayes)\n\n[6](https://datascience.stackexchange.com/q/8339) [Voting combined results from different classifiers gave bad accuracy](https://datascience.stackexchange.com/questions/8339/voting-combined-results-from-different-classifiers-gave-bad-accuracy)\n\n[1](https://datascience.stackexchange.com/q/16349) [How to improve accuracy further for forest cover prediction](https://datascience.stackexchange.com/questions/16349/how-to-improve-accuracy-further-for-forest-cover-prediction)\n\n[3](https://datascience.stackexchange.com/q/65196) [Kaggle Titanic submission score is higher than local accuracy score](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\n\n[1](https://datascience.stackexchange.com/q/69193) [Feature Engineering - correlation with binary outcome - Titanic Dataset - Ticket feature](https://datascience.stackexchange.com/questions/69193...",
      "url": "https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy"
    },
    {
      "title": "How to Use Kaggle: Competitions",
      "text": "Getting Started on Kaggle | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n# How to Use Kaggle\nkeyboard\\_arrow\\_down\nCompetitions\nkeyboard\\_arrow\\_right\n[Types of Competitions](#types-of-competitions)\nkeyboard\\_arrow\\_right\n[Competition Formats](#competition-formats)\n[Joining a Competition](#joining-a-competition)\nkeyboard\\_arrow\\_right\n[Forming a Team](#forming-a-team)\nkeyboard\\_arrow\\_right\n[Making a Submission](#making-a-submission)\nkeyboard\\_arrow\\_right\n[Leakage](#leakage)\nkeyboard\\_arrow\\_right\n[Resources for Getting Started](#resources-for-getting-started)\n[Cheating](#cheating)\nkeyboard\\_arrow\\_right[Datasets](https://www.kaggle.com/docs/datasets)\nkeyboard\\_arrow\\_right[Public API](https://www.kaggle.com/docs/api)\n[Efficient GPU Usage Tips](https://www.kaggle.com/docs/efficient-gpu-usage)\nkeyboard\\_arrow\\_right[Tensor Processing Units (TPUs)](https://www.kaggle.com/docs/tpu)\nkeyboard\\_arrow\\_right[Models](https://www.kaggle.com/docs/models)\nkeyboard\\_arrow\\_right[Competitions Setup](https://www.kaggle.com/docs/competitions-setup)\nkeyboard\\_arrow\\_right[Organizations](https://www.kaggle.com/docs/organizations)\nkeyboard\\_arrow\\_right[Groups](https://www.kaggle.com/docs/groups)\nkeyboard\\_arrow\\_right[Kaggle Packages](https://www.kaggle.com/docs/packages)\nkeyboard\\_arrow\\_right[Notebooks](https://www.kaggle.com/docs/notebooks)\nkeyboard\\_arrow\\_right[MCP Server](https://www.kaggle.com/docs/mcp)\nkeyboard\\_arrow\\_right[Benchmarks](https://www.kaggle.com/docs/benchmarks)\n## Competitions\nFind challenges for every interest level\n### Types of Competitions\nKaggle Competitions are designed to provide challenges for competitors at all different stages of their machine learning careers. As a result, they are very diverse, with a range of broad types.\n#### Featured\nFeatured competitions are the types of competitions that Kaggle is probably best known for. These are full-scale machine learning challenges which pose difficult, generally commercially-purposed prediction problems. For example, past featured competitions have included:\n* [Allstate Claim Prediction Challenge](https://www.kaggle.com/c/allstate-purchase-prediction-challenge)- Use customers\u2019 shopping history to predict which insurance policy they purchase\n* [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)- Predict the existence and type of toxic comments on Wikipedia\n* [Zillow Prize](https://www.kaggle.com/c/zillow-prize-1)- Build a machine learning algorithm that can challenge Zestimates, the Zillow real estate price estimation algorithm\nFeatured competitions attract some of the most formidable experts, and offer prize pools going as high as a million dollars. However, they remain accessible to anyone and everyone. Whether you\u2019re an expert in the field or a complete novice, featured competitions are a valuable opportunity to learn skills and techniques from the very best in the field.\n#### Research\nResearch competitions are another common type of competition on Kaggle. Research competitions feature problems which are more experimental than featured competition problems. For example, some past research competitions have included:\n* [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge)- Given an image, can you find all the same landmarks in a dataset?\n* [Right Whale Recognition](https://www.kaggle.com/c/noaa-right-whale-recognition)- Identify endangered right whales in aerial photographs\n* [Large Scale Hierarchical Text Classification](https://www.kaggle.com/c/lshtc)- Classify Wikipedia documents into one of \\~300,000 categories\nResearch competitions do not usually offer prizes or points due to their experimental nature. But they offer an opportunity to work on problems which may not have a clean or easy solution and which are integral to a specific domain or area in a slightly less competitive environment.\n#### Getting Started\nGetting Started competitions are the easiest, most approachable competitions on Kaggle. These are semi-permanent competitions that are meant to be used by new users just getting their foot in the door in the field of machine learning. They offer no prizes or points. Because of their long-running nature, Getting Started competitions are perhaps the most heavily tutorialized problems in machine learning - just what a newcomer needs to get started!\n* [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)\n* [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)- Predict survival on the Titanic\n* [Housing Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\nGetting Started competitions have two-month rolling leaderboards. Once a submission is more than two months old, it is automatically invalidated and no longer counts towards the leaderboard. Similarly, your team will drop from the leaderboard if all its submissions are older than two months. This gives new Kagglers the opportunity to see how their scores stack up against a cohort of competitors, rather than many tens of thousands of users. If your team is removed from a Getting Started competition due to the rolling expiry and wishes to rejoin, creating a new submission will cause it to show again on the leaderboard.\nAdditionally, the[Kaggle Learn](https://www.kaggle.com/learn/overview)platform has several tracks for beginners interested in free hands-on data science learning from pandas to deep learning. Lessons within a track are separated into easily digestible chunks and contain Notebook exercises for you to practise building models and new techniques. You\u2019ll learn all the skills you need to dive into Kaggle Competitions.\n#### Playground\nPlayground competitions are a \u201cfor fun\u201d type of Kaggle competition that is one step above Getting Started in difficulty. These are competitions which often provide relatively simple machine learning tasks, and are similarly targeted at newcomers or Kagglers interested in practicing a new type of problem in a lower-stakes setting. Prizes range from kudos to small cash prizes. Some examples of Playground competitions are:\n* [Dogs versus Cats](https://www.kaggle.com/c/dogs-vs-cats)- Create an algorithm to distinguish dogs from cats\n* [Leaf Classification](https://www.kaggle.com/c/leaf-classification)- Can you see the random forest for the leaves?\n* [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)- Share code and data to improve ride time predictions\n### Competition Formats\nIn addition to the different categories of competitions (e.g., \u201cfeatured\u201d), there are also a handful of different formats competitions are run in.\n#### Simple Competitions\nSimple (or \u201cclassic\u201d) competitions are those which follow the standard Kaggle format. In a simple competition, users can access the complete datasets at the beginning of the competition, after accepting the competition\u2019s rules. As a competitor you will download the data, build models on it locally or in[Notebooks](https://www.kaggle.com/notebooks), generate a prediction file, then upload your predictions as a submission on Kaggle. By far most competitions on Kaggle follow this format.\nOne example of a simple competition is the[Porto Seguro Safe Driver Prediction Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction).\n#### Two-stage Competitions\nIn two-stage competitions the challenge is split into two parts: Stage 1 and Stage 2, with the second stage building on the results teams achieved in Stage 1. Stage 2 involves a new test dataset that is released at the start of the stage. Eligibility for Stage 2 typically requires making a submission in Stage 1. In two-stage competitions, it\u2019s especially important to read and understand the competition\u2019s specific rules and timeline.\nOne example of ...",
      "url": "https://www.kaggle.com/docs/competitions"
    }
  ]
}