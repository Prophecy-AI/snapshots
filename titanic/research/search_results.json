{
  "query": "How is 100% accuracy achieved on Kaggle Titanic competition - is the test set labels available somewhere?",
  "results": [
    {
      "title": "ANSWER",
      "text": "100% accuracy on the Titanic competition is only possible by cheating (e.g., using the hidden test\u2011set labels), because Kaggle never releases the test labels \u2013 they remain private and are only used for leaderboard scoring ([freecodecamp.org](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401), Sep\u202f2023).",
      "url": ""
    },
    {
      "title": "The Titanic ML project on Kaggle - Python - The freeCodeCamp Forum",
      "text": "[Skip to main content](https://forum.freecodecamp.org/forum.freecodecamp.org#main-container)\n\n\u200b\n\nHey everyone,\n\nYou may have already noticed that we made the decision to remove the lecture videos from our full-stack curriculum. Please note that this was not a decision we took lightly. Video-based coursework is a pretty significant process to create, and maintaining them (keeping them updated) as our curriculum improves and expands is proving to be quite the task for our small community of contributors.\n\nAdditionally, while some of you have expressed that you enjoyed the videos, we are not seeing a trend in the data to indicate that the videos are particularly beneficial. With all of that in mind, we have decided that removing them and allowing our focus to remain on delivering quality coursework is the best path forward.\n\nFor those of you who are interested, the videos are currently still available on [https://www.youtube.com/@freecodecampconcepts/videos](https://www.youtube.com/@freecodecampconcepts/videos). We may or may not unlist them in the future, so be sure to check out the ones you want to watch.\n\nAs always, if you have any questions you are welcome to reach out to me in Discord, on the forum, on our subreddit, via DMs, or at `naomi@freecodecamp.org`.\n\n## ADVERTISEMENT\n\n# [The Titanic ML project on Kaggle](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401)\n\n[Python](https://forum.freecodecamp.org/c/python/424)\n\nYou have selected **0** posts.\n\nselect all\n\ncancel selecting\n\n1.5kviews\n6links\n\n[2](https://forum.freecodecamp.org/u/SzeYeung1)\n\n[2](https://forum.freecodecamp.org/u/Atrox)\n\n[Sep 2023](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401/1)\n\n2 / 5\n\nSep 2023\n\n[Mar 2024](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401/5)\n\n[Atrox](https://forum.freecodecamp.org/u/atrox)\n\n[Sep 2023](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401)\n\nGreetings,\n\nI hope everyone is doing well,\n\nI am trying to do the titanic challenge in kaggle. Can you please assist me with the following queries:\n\n1. do I need to categories for the age to start?\n\n2. I dropped the following columns I think they are completely unnecessary and cannot be used: Cabin, Ticket, Name . Am I right here ? Should I drop the Embarked as well?\n\n3. How do I determine which model and feautures to use to me it seems like a regression task, are there metrics that I can use?\n\n\nthe data and my jupyter notebook are here:\n\nThanks.\n\nKind Regards,\nAtrox.\n\n1.5kviews\n6links\n\n[2](https://forum.freecodecamp.org/u/SzeYeung1)\n\n[2](https://forum.freecodecamp.org/u/Atrox)\n\n[SzeYeung1](https://forum.freecodecamp.org/u/szeyeung1)\n\n[Sep 2023](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401/2)\n\n1. You may try and compare whether binning the Age variable can lead to better performance of the model, or models.\n\n2. Again you can compare the effects of dropping certain columns by experimenting. When one decide which features to be dropped, common sense might tell us some features are unrelated to the survival chance. But is it really the case? We have to examine the data. If you look into the survival rates of pessengers from different point of Embarked, do you see some pattern emerges? Basic data exploration will tell Gender and PC Class are two big factors influencing the survival chance, but some may argue the titles extracted from the Names (Mr, Mrs, Miss, Sir, Lady, etc) contain similarily important information, and to a lesser extent, so does Cabin (As the first letter of Cabin indicates which deck the cabin is located, and related to the PC Class). There is no definitive answer for feature selection. Sometimes seemingly useless features can yield important information after good feature engineering, but sometimes promising looking features do not improve model performance significantly after inclusion for training.\n\n3. While the model is to \u2018predict\u2019 which passengers survive, why do you see it looks like a regression task?\n\n\n[Atrox](https://forum.freecodecamp.org/u/atrox)\n\n[Sep 2023](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401/3)\n\n1. How do I see if it is effective?\n\n2. Okay I understand. Are there some data analysis methods that you can advice me in this case to use? For example some articles maybe?\n\n3. I am not sure about it being regression model, that is why I wanted to ask how to determine which model to use. I thought it is a regression model since I saw some correlations between age and survival rate and so on. I thought I can just fit a regression line that would fit this data.\n\n\n## ADVERTISEMENT\n\n[SzeYeung1](https://forum.freecodecamp.org/u/szeyeung1)\n\n[Sep 2023](https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401/4)\n\n1. You train a model with Age variable as it is, consists of numeric values, make predictions with validation dataset or test set, get the score of accuracy. Than train another model with Age variable binned(in one or more than one way), get another score(s) and compare.\n\n[This tutorial](https://www.kaggle.com/code/alexisbcook/categorical-variables) from Kaggle\u2019s mini course [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) is using this comparative approach on the effect of different treatments of categorical variables, though with different dataset.\n\n1. For the Titanic challenge, there\u2019s ton of notebooks and tutorials in [Kaggle\u2019s competition page](https://www.kaggle.com/competitions/titanic/code?competitionId=3136&sortBy=voteCount). You may look into the ones with most votes, but not the ones with 100% accuracy score, which are using cheating means for vanity.\n\n2. I think there\u2019s a confusion of the term used. In supervised learning we have two major types of task: classification and regression. The following description is taken from [scikit-learn Tutorial](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#introduction):\n\n\n- [classification](https://en.wikipedia.org/wiki/Classification_in_machine_learning): samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n- [regression](https://en.wikipedia.org/wiki/Regression_analysis): if the desired output consists of one or more continuous variables, then the task is called _regression_. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n\nWhile you are not wrong in saying that a regression line may fit between certain variables and survival rate (in fact this is how logistic regression is operating), at the end the final predictions are not about survival rate, but concrete answers that whether a passgener with given features was going to be survived or not. So it is a classification problem.\n\nI am not sure whether you are referring to machine learning algorithms (like logistic regression, svm, knn, decision trees, random forest) when you talk about which model to use. Any algorithm that can do classification can be used, of course you would pick the one with the best performance. For different performance metrics for classification, you may consult the [Metrics and scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) section of scilit-learn\u2019s documentation or [this article](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/), but Kaggle\u2019s Titanic ...",
      "url": "https://forum.freecodecamp.org/t/the-titanic-ml-project-on-kaggle/636401"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "Kaggle Fundamentals: The Titanic Competition - Dataquest",
      "text": "March 25, 2025\n\n# Kaggle Fundamentals: The Titanic Competition\n\n[Kaggle](https://www.kaggle.com/) is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\n\nIn this tutorial we'll learn learn how to:\n\n- Approach a Kaggle competition\n- Explore the competition data and learn about the competition topic\n- Prepare data for machine learning\n- Train a model\n- Measure the accuracy of your model\n- Prepare and make your first Kaggle submission\n\nThis tutorial presumes you have an understanding of Python and the pandas library. If you need to learn about these, we recommend our [pandas tutorial](https://www.dataquest.io/blog/pandas-python-tutorial/) blog post.\n\n## The Titanic competition\n\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the [sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic).\n\nIn this [Titanic ML competition](https://www.kaggle.com/competitions/titanic), we have data about passengers onboard the Titanic, and we'll see if we can use that information to predict whether those people survived or not. Before we start looking at this specific competition, let's take a moment to understand how Kaggle competitions work.\n\nEach Kaggle competition has two key data files that you will work with - a **training** set and a **testing** set.\n\nThe training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict: in this case, `Survival`.\n\nThe testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.\n\nThis is useful because we want as much data as we can to train our model on. Once we have trained our model on the training set, we will use that model to make predictions on the data from the testing set, and submit those predictions to Kaggle.\n\nIn this competition, the two files are named `test.csv` and `train.csv`. We'll start by using [`pandas.read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read both files and then inspect their size.\n\n```\nimport pandas as pd\n\ntest = pd.read_csv(\"test.csv\")\ntrain = pd.read_csv(\"train.csv\")\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))\n\n```\n\n```\nDimensions of train: (891, 12)\nDimensions of test: (418, 11)\n\n```\n\n## Exploring the data\n\nThe files we just opened are available on [the data page for the Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/data). That page also has a **data dictionary**, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n\n- `PassengerID`\u2014 A column added by Kaggle to identify each row and make submissions easier\n- `Survived`\u2014 Whether the passenger survived or not and the value we are predicting _(0=No, 1=Yes)_\n- `Pclass`\u2014 The class of the ticket the passenger purchased _(1=1st, 2=2nd, 3=3rd)_\n- `Sex`\u2014 The passenger's sex\n- `Age`\u2014 The passenger's age in years\n- `SibSp`\u2014 The number of siblings or spouses the passenger had aboard the Titanic\n- `Parch`\u2014 The number of parents or children the passenger had aboard the Titanic\n- `Ticket`\u2014 The passenger's ticket number\n- `Fare`\u2014 The fare the passenger paid\n- `Cabin`\u2014 The passenger's cabin number\n- `Embarked`\u2014 The port where the passenger embarked _(C=Cherbourg, Q=Queenstown, S=Southampton)_\n\nThe data page on Kaggle has some additional notes about some of the columns. It's always worth exploring this in detail to get a full understanding of the data.\n\nLet's take a look at the first few rows of the `train` dataframe.\n\n```\ntrain.head()\n```\n\n| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n| 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S |\n| 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S |\n\nThe type of machine learning we will be doing is called **classification**, because when we make predictions we are classifying each passenger as 'survived' or not. More specifically, we are performing **binary classification**, which means that there are only two different states we are classifying.\n\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\n\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie [Titanic](https://en.wikipedia.org/wiki/Titanic_%281997_film%29) would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\n\nThis indicates that `Age`, `Sex`, and `PClass` may be good predictors of survival. We'll start by exploring `Sex` and `Pclass` by visualizing the data.\n\nBecause the `Survived` column contains `0` if the passenger did not survive and `1` if they did, we can segment our data by sex and calculate the mean of this column. We can use `DataFrame.pivot_table()` to easily do this:\n\n```\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot.plot.bar()\nplt.show()\n\n```\n\nWe can immediately see that females survived in much higher proportions than males did. Let's do the same with the `Pclass` column.\n\n```\nclass_pivot = train.pivot_table(index=\"Pclass\",values=\"Survived\")\nclass_pivot.plot.bar()\nplt.show()\n\n```\n\n## Exploring and converting the age column\n\nThe `Sex` and `PClass` columns are what we call **categorical** features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\n\nLet's take a look at the `Age` column using [`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html).\n\n```\ntrain[\"Age\"].describe()\n```\n\n```\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64\n```\n\nThe `Age` column contains numbers ranging from `0.42` to `80.0` (If you look at Kaggle's data page, it informs us that `Age` is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the `train` data set had earlier which indicates we have some missing values.\n\nAll of this means that the `Age` column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visually the those that survived vs those who died across different age ranges:\n\n```\nsurvived = train[train[\"Survived\"] == 1]\ndied = train[train[\"Survived\"] == 0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='red...",
      "url": "https://www.dataquest.io/blog/kaggle-fundamentals"
    },
    {
      "title": "kaggle-titanic-dataset-example-submission-workflow.ipynb - GitHub",
      "text": "[Skip to content](https://github.com/mrdbourke/your-first-kaggle-submission/blob/master/kaggle-titanic-dataset-example-submission-workflow.ipynb#start-of-content)\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n{{ message }}\n[mrdbourke](https://github.com/mrdbourke)/ **[your-first-kaggle-submission](https://github.com/mrdbourke/your-first-kaggle-submission)** Public\n- [Notifications](https://github.com/login?return_to=%2Fmrdbourke%2Fyour-first-kaggle-submission) You must be signed in to change notification settings\n- [Fork\\\n149](https://github.com/login?return_to=%2Fmrdbourke%2Fyour-first-kaggle-submission)\n- [Star\\\n222](https://github.com/login?return_to=%2Fmrdbourke%2Fyour-first-kaggle-submission)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/mrdbourke/your-first-kaggle-submission/blob/master/kaggle-titanic-dataset-example-submission-workflow.ipynb"
    },
    {
      "title": "Spaceship Titanic: A complete guide - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p><div><a href=\"https://www.kaggle.com/cookies\"><p>Learn more</p></a><p>OK, Got it.</p></div></div><div><div><div><a href=\"https://www.kaggle.com/samuelcortinhas\"></a><p><span><span><span>Samuel Cortinhas </span></span><span> \u00b7 <span>3y ago</span> \u00b7 70,147 views</span></span></p><div><p></p></div></div></div><div><div><p></p><h2>\ud83d\ude80 Spaceship Titanic: A complete guide \ud83c\udfc6</h2><p></p></div><div><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/notebook\"><span>Notebook</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/input\"><span>Input</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/output\"><span>Output</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/log\"><span>Logs</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/comments\"><span>Comments (239)</span><span></span></a></p></div></div><div><div><div><h2>Runtime</h2><div><p>22m 26s</p></div><div><h2>Input</h2><div><p><span>COMPETITIONS</span></p><div><p><span></span></p><p>Spaceship Titanic</p><p></p></div></div></div><h2>Language</h2><p>Python</p><div><p></p><h2>Table of Contents</h2><p></p><div><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Introduction\"><span>Introduction</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Libraries\"><span>Libraries</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Data\"><span>Data</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#EDA\"><span>EDA</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Feature-engineering\"><span>Feature engineering</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Missing-values\"><span>Missing values</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Preprocessing\"><span>Preprocessing</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Model-selection\"><span>Model selection</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Modelling\"><span>Modelling</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Submission\"><span>Submission</span></a></p></div></div></div><div><div><div><p></p><div><p>Competition Notebook</p><p><a href=\"https://www.kaggle.com/competitions/spaceship-titanic\">Spaceship Titanic</a></p></div></div><div><p>Public Score</p><p>0.80874</p></div><div><p>Best Score</p><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide?scriptVersionId=92521620\">0.80874 V47</a></p></div></div><div></div></div></div><div><div><h2>License</h2><p>This Notebook has been released under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">Apache 2.0</a> open source license.</p></div><div><h2>Continue exploring</h2><ul><li><div><a><div><div><p></p></div><div><p>Input</p><p><span>1 file</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Output</p><p><span>5 files</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Logs</p><p><span>1345.5 second run - successful</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Comments</p><p><span>239 comments</span></p></div></div></a></div></li></ul></div></div></div></div></div></div>",
      "url": "https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide"
    },
    {
      "title": "Introduction to Kaggle and Scoring Top 7% in the Titanic Competition",
      "text": "Introduction to Kaggle and Scoring Top 7% in the Titanic Competition | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Data Science](https://towardsdatascience.com/category/data-science/)\n# Introduction to Kaggle and Scoring Top 7% in the Titanic Competition\nGet started with Kaggle and submit a (good) first solution\n[Oliver S](https://towardsdatascience.com/author/hrmnmichaels/)\nApr 23, 2024\n16 min read\nShare\n[Kaggle](https://www.kaggle.com/)is a fun platform hosting a variety of data science and machine learning competitions &#8211; covering topics such as[sports](https://www.kaggle.com/competitions/nfl-big-data-bowl-2024),[energy](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers)or[autonomous driving](https://www.kaggle.com/competitions/lyft-motion-prediction-autonomous-vehicles).\nIn this post we will give an introduction to Kaggle, and tackle the introductory[&quot;Titanic&quot; challenge](https://www.kaggle.com/competitions/titanic). We will explain how to approach and solve such a challenge, and demonstrate this with a top 7% solution for &quot;Titanic&quot;.\n![Photo by Derek Oyen on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/04/0BguEdndpTyZl35ae.jpg)Photo by[Derek Oyen](https://unsplash.com/@goosegrease?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash)on[Unsplash](https://unsplash.com/photos/glacier-during-daytime-4ReskwNsh68?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash)\nYou can find the full code on[Github](https://github.com/hermanmichaels/titanic), and with that following along while reading this article, as well as reproduce my exact score. In it, we follow some things I consider[best practice for Python](https://towardsdatascience.com/best-practices-for-python-development-bf74c2880f87)and use helpful tools, such as[mypy](https://towardsdatascience.com/introduction-to-mypy-3d32fc96db75)and[poetry](https://towardsdatascience.com/dependency-management-with-poetry-f1d598591161). With that being said, let&#8217;s dive right into it.\n## Kaggle\n[Kaggle](https://www.kaggle.com/)offers a wide variety of data science / machine learning competitions, see the intro for examples. It is a great way to test and improve your data science / ML knowledge and learn how to solve problems hands-on. Plus, you can even win monetary prices! However, Kaggle is populated by some of the best data scientists and ML people out there &#8211; and prices are only given to the few top solutions (out of several hundreds or thousands) &#8211; thus winning here is extremely hard and rare, and should not be your main motivation when starting.\nEach (most?) competition comes with a story &#8211; a purpose &#8211; and a dataset. You are then tasked to understand the data, and solve the desired problem. If you want, you can submit your solutions to the platform, and get ranked on a public leaderboard &#8211; that is, your solution is ranked on a held-out test set. However, to avoid cheating or optimizing against this by simply spamming submissions, once the competition time (usually a few weeks to months) has expired, all competitors / teams are ranked vs a private test set &#8211; deciding the ultimate winners.\nIn the following, we will show how to understand the data, create a model, and submit to Kaggle following the introductory[Titanic competition](https://www.kaggle.com/competitions/titanic).\n## Titanic &#8211; Machine Learning from Disaster\nMost probably know the story of the cruise ship[Titanic](https://en.wikipedia.org/wiki/Titanic)and its infamous demise: a long time ago, although sadly still a tragedy with many lives lost.\nKaggle offers &quot;tutorial&quot; competitions, in which you can learn and train without any time constraints. One of these is the mentioned &quot;Titanic&quot; competition, for which you have to predict which passengers will survive.\nWe get two csv files &#8211; train.csv and test.csv. Both contain different features, such as Sex and Name of the passengers, and train contains an extra column &quot;Survived&quot;. Thus, we are tasked to learn a model, which, given the test features, can predict whether that passenger survived or not.\nTo make a submission, we simply make our own csv file consisting of PassengerID: survived, and upload this to Kaggle (either via drag &amp; drop, or from the command line &#8211; as we will see later).\nIn details, we have the following features:\n* **pclass**: ticket class (1, 2 or 3) &#8211; something similar to economy or business class in today&#8217;s travel\n* **Sex**: sex of the passenger (male / female)\n* **Age**: age in years\n* **sibsp**: # siblings / spouses aboard\n* **parch**: # parents / children aboard\n* **ticket**: the ticket number\n* **fare**: passenger fare &#8211; how much the passenger paid for the ticket\n* **cabin**: cabin number\n* **embarked**: port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)### Data Analysis and Feature Selection\nFirst step of every data science / ML problem, every competition &#8230; you tackle, should always be understanding and visualizing the data &#8211; also known as[Exploratory Data Analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis). Every good ML project stands and falls with the amount and quality of data available, and engineers spend a large amount of time collecting and preparing this data &#8211; and not necessarily working on complex models.\nThus, let&#8217;s do exactly that with the Titanic dataset. I have downloaded the materials from the Kaggle competition into my project folder, in a subfolder called &quot;titanic&quot;.\nLet&#8217;s start with the discrete data. For this, we create a function plotting the survival ratio per class (reminder, you can find the full code on[Github](https://github.com/hermanmichaels/titanic)):\n```\n`def plot\\_survival\\_ratio(df: pd.DataFrame, column: str) -&gt;&gt; None:\n# Replace NaN tokens by &quot;&quot;Unk(nown)&quot;&quot;\ndf[column].fillna(&quot;&quot;Unk&quot;&quot;, inplace=True)\n# Calculate survival ratio per existing value in the column\ntotal\\_counts = df[column].value\\_counts()\nsurvival\\_counts = (\ndf.groupby([column, &quot;&quot;Survived&quot;&quot;]).size().unstack().fillna(0)\n)\ncolumn\\_values = df[column].unique()\nsurvival\\_ratios = [\nsurvival\\_counts.loc[value, 1] / total\\_counts[value]\nfor value in column\\_values\n]\ncolor\\_mapping = plt.cm.get\\_cmap(&quot;&quot;tab10&quot;&quot;, len(survival\\_counts))\nbar\\_colors = color\\_mapping(np.arange(len(column\\_values)))\n# Plot the survival ratios\nplt.bar(column\\_values, survival\\_ratios, color=bar\\_colors)\nplt.title(f&quot;&quot;Survival Ratio by {column}&quot;&quot;)\nplt.xlabel(column)\nplt.ylabel(&quot;&quot;Survival Ratio&quot;&quot;)\nplt.ylim(0, 1)\nplt.show()`\n```\nNow, let&#8217;s exemplary look at a few available features, starting with**Sex**:\n```\n`df = pd.read\\_csv(&quot;&quot;titanic/train.csv&quot;&quot;)\nplot\\_survival\\_ratio(df, &quot;&quot;Sex&quot;&quot;)`\n```\n![Figure by author](https://towardsdatascience.com/wp-content/uploads/2024/04/1dr4w5JNnpXInfZheO6dvlA.png)Figure by author\nAs we can see, Sex makes a big difference on survival &#8211; seemingly the famous line of &quot;Women (and children)&quot; first was followed flawlessly.\nNext, let&#8217;s look at**passenger class**:\n![Figure by author](https://towardsdatascience.com/wp-content/uploads/2024/04/186WISQQsSc5Cvnla5n3-Qg.png)Figure by author\nAs one could suspect, passengers with a higher (lower in numbers) class were probably given priority treatment.\nNow, let&#82...",
      "url": "https://towardsdatascience.com/introduction-to-kaggle-and-scoring-top-7-in-the-titanic-competition-7a29ce9c24ae"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy]",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "Titanic dataset",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/datasets/brendan45774/test-file#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/datasets/brendan45774/test-file)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/datasets/brendan45774/test-file#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdatasets%2Fbrendan45774%2Ftest-file)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdatasets%2Fbrendan45774%2Ftest-file)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/datasets/brendan45774/test-file"
    }
  ]
}