{
  "query": "What are the best feature engineering techniques for Titanic survival prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering is the process of turning raw passenger data into richer, model\u2011friendly variables that capture the underlying patterns driving survival. In the Titanic competition, simple transformations such as handling missing values and one\u2011hot encoding categorical fields (e.g.,\u202fSex,\u202fEmbarked,\u202fPclass) are essential, but the biggest gains come from creating new attributes that combine or extract information from existing columns\u202f([xiangyutang2](https://xiangyutang2.github.io/feature-engineering)).\n\nThe most effective techniques reported by recent Kaggle\u2011focused tutorials include:  \n\n* **Family\u2011size features** \u2013 sum\u202f`SibSp`\u202fand\u202f`Parch`\u202fto get total family members (`family_size`) and derive a binary\u202f`IsAlone`\u202fflag for passengers traveling solo\u202f([codesignal](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/feature-engineering-enhancing-the-titanic-dataset-for-survival-predictions),\u202f[GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  \n* **Title extraction** \u2013 parse the passenger\u2019s name to pull honorifics (Mr,\u202fMrs,\u202fMiss,\u202fMaster,\u202fDr, etc.) and map rare titles to a \u201cRare\u201d group; titles are strong survival predictors\u202f([GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708),\u202f[Trevor Stephens](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering)).  \n* **Cabin/Deck information** \u2013 create a binary\u202f`HasCabin`\u202ffeature indicating whether a cabin number is present and extract the deck letter (first character of\u202f`Cabin`) as a categorical variable\u202f([GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708),\u202f[Trevor Stephens](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering)).  \n* **Age binning** \u2013 bucket ages into meaningful intervals (e.g.,\u202fchild\u202f<\u202f12,\u202fteen\u202f12\u201118,\u202fadult\u202f19\u201160,\u202fsenior\u202f>\u202f60) to reduce noise from exact ages and handle missing values with median imputation per title\u202f([GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  \n* **Ticket prefix** \u2013 extract the alphabetic prefix of the\u202f`Ticket`\u202ffield, which can capture groupings or travel class nuances\u202f([Trevor Stephens](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering)).  \n* **Fare per person** \u2013 divide\u202f`Fare`\u202fby\u202f`family_size\u202f+\u202f1`\u202fto normalize fare across families, then optionally bin the resulting value\u202f([GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  \n\nWhen these engineered features are combined with proper encoding (one\u2011hot or ordinal) and scaling, they often become the most important variables in tree\u2011based models and can lift validation accuracy from the low\u201170\u202f% range to the high\u201170\u202f% or even low\u201180\u202f% range\u202f([GitHub](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)). Together, they illustrate why thoughtful feature construction is usually more impactful than model complexity for the Titanic survival prediction task.",
      "url": ""
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "Feature Engineering, Data Exploration, and Classification using Titanic Dataset\u00b6",
      "text": "# Feature Engineering, Data Exploration, and Classification using Titanic Dataset [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#Feature-Engineering,-Data-Exploration,-and-Classification-using-Titanic-Dataset)\n\nIn machine learing, what perhaps is more important than defining a model or a neural network is data exploration. After we get the dataset, that is hopefully representative of the problem we are trying to solve, the first thing we do is to examine the data and see what characteristics and features are present. Are there any problems in the data? Are some categories under-represented? Are there missing values? Are there significant correlations between some features and outcome? Can we easily engineer new features from existing ones that will correlate better with the outcome? Data exploration not only helps to discover and eliminates some potential problems before model training begins, but will also makes our model more accurate by injecting some human intuition into the solution.\n\nIn this tutorial, we will use the Titanic passenger survival dataset to illustrate the concept of data exploration and feature engineering. We choose this dataset because it is simple, and thus allows us to focus on engineering the features. We will show how engineered features play an important role in prediction by being the most important feature in machine learning models. Then, we will illustrate compare several methods of binary classification that can used to predict whether a passenger survives the disaster.\n\nThe Titanic dataset can be [download from this website](https://storage.googleapis.com/kaggle-forum-message-attachments/66979/2180/titanic.csv). Prediction Titanic passenger survival is also a [Kaggle challege](https://www.kaggle.com/c/titanic). Note that because the passenger survival information is public, the Kaggle challenge leaderboard is spammed with people submitting actual real-world data as machine learning results, making it meaningless.\n\n## 1\\. Data Exploration [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#1.-Data-Exploration)\n\nWe first load the \"titanic.csv\" file into a Pandas dataframe. We will immediately print the first few lines of the file.\n\nIn\u00a0\\[8\\]:\n\n```\nimport pandas as pd\n\ndata = pd.read_csv('titanic.csv')\ndata.head(10)\n\n```\n\nOut\\[8\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked | boat | body | home.dest |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C | NaN | 22.0 | Montevideo, Uruguay |\n\nWe immediately notice that the second column \"survived\" is our prediction result. The \"name\" column contains not only the first and last name, but also the **title**, which could be an important indicator of socio-economic status, which could be a factor in the survival probability. The \"sibsp\" columns contains the number of siblings and spouses a passenger have on the ship. The \"parch\" column is the number of parent or guardian the passenger have on the ship. The two columns added together is the size of the family together on the ship. The \"ticket\" column is the ticket number, which is unique for vast majority of the passengers. Some tickets have alphabets in front. The \"embarked\" column is the port of departure. This could be another indicator of socio-economic status along with the \"fare\" column. The \"boat\" and \"body\" column is information that should not be used to predict survival rate, since it is statistics gathered after the event. These two columns should be dropped. The \"cabin\" column is very interesting, it contains the **deck** information and the room number. Which deck the passenger resides in could be an indicator of survival rate, and should be extracted. Room number on the other hand, whithout knowing the actual layout of the ship, is not very useful. Finally, the \"home.dest\" column shows the home and destination of the passenger, which could be another indicator of socio-economic status.\n\nWe also immdediatly see that some data is missing. We need to examine in detail which data is missing and determine the best way to fill in these missing data without skewing the prediction results.\n\nWe can use the folloing command to list the categories in the dataset.\n\nIn\u00a0\\[9\\]:\n\n```\nprint(data.columns.values)\n\n```\n\n```\n['pclass' 'survived' 'name' 'sex' 'age' 'sibsp' 'parch' 'ticket' 'fare'\n 'cabin' 'embarked' 'boat' 'body' 'home.dest']\n\n```\n\nFrom these categories, will will drop \"boat\", \"body\" from the dataset since they cannot be used as predictors and they are also not the outcome. We will also drop the \"home.dest\" column since we already have many other good indicators of socio-economic status. Keeping the \"home.dest\" column will not cause harm, we just want to keep our model a simpler.\n\nIn\u00a0\\[10\\]:\n\n```\ndrop_cat=['boat','body','home.dest']\ndata.drop(drop_cat, inplace=True, axis=1)\ndata.head(10)\n\n```\n\nOut\\[10\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C |\n\nLet use directly goto the point and see how many passengers survived the disaster.\n\nIn\u00a0\\[12\\]:\n\n```\ndata['survived'].mean()\n\n```\n\nOut\\[12\\]:\n\n```\n0.3819709702062643\n```\n\nAbout 40% of the passengers survived. We know from historical records that women and children were given priority on the lifeboats. Was this true, and did it reflect in their survival rates? Lets take a look.\n\nIn\u00a0\\[70\\]:\n\n```\nprint('Survival Rate by Sex')\nprint(data['survived'].groupby(data['sex']).mean())\nprint('\\n\\nSex Ratio of Passengers')\nprint(data['sex...",
      "url": "https://xiangyutang2.github.io/feature-engineering"
    },
    {
      "title": "Feature Engineering: Enhancing the Titanic Dataset for Survival Predictions",
      "text": "[Skip to main content](https://codesignal.com/codesignal.com#main-content)\n\nLesson Launch: Feature Engineering for Survival Rate Predictors\n\nWelcome back to our course - **Data Cleaning and Preprocessing in Machine Learning**. Today's mission revolves around Feature Engineering on the Titanic dataset. By the end of today's lesson, your toolkit will be loaded with skills that revolve around feature creation, modification, and encoding. Your expertise in Python and Pandas will also be put into practice, reinforcing your knowledge in the process.\n\nIntroduction to Feature Engineering\n\nFeature engineering is the process of creating optimized features that improve the effectiveness of machine learning algorithms. This process utilizes the data to create new features and modify existing ones. This might involve creating new features, transforming existing features, or identifying and removing irrelevant ones. For instance, in our Titanic dataset, we have properties or indicators like `age`, `sex`, `pclass`, etc., which might need some optimizing.\n\nLet's take `sibsp` and `parch` as an example: `sibsp` shows the number of siblings/spouses aboard while `parch` shows the number of parents/children onboard. Because these features both indicate the number of family members onboard for each individual, one might see them as similar features or even overlapping. Hence, we can combine these two features to create a new feature: `family_size`.\n\nJoin the 1M+ learners on CodeSignal\n\nBe a part of our community of 1M+ users who develop and demonstrate their skills on CodeSignal\n\nStart learning today!\n\nCreating New Features\n\nTo start creating new features, we need to have the Titanic dataset in our hands. This can be quickly done in Python using Seaborn, as shown below:\n\nWith the dataset ready, let's compute the `family_size` feature using the number of siblings/spouses ( `sibsp`) and parents/children ( `parch`).\n\nThis snippet of code will create a new column `'family_size'` and add it to the dataset `titanic_df`.\n\nModification of Existing Features\n\nAnother valuable task in feature engineering is to modify existing features. For instance, the `fare` feature in our Titanic dataset may contain values that are significantly larger than the average, making the distribution of `fare` values skewed. This skewness could be unhelpful for our machine-learning algorithms and might induce overfitting.\n\nIn such a case, we can consider applying a log transformation to the `fare` column, with the aim of reducing skewness. The code snippet for this is shown below:\n\nHere, we add 0.1 to the `fare` column to adjust for zero values, then apply a logarithmic transformation.\n\nHandling and Encoding Categorical Values\n\nMachine Learning algorithms generally deal with numbers and not text. Therefore, in handling categorical features, it is important to change text values to numerical ones. The process of doing this is called encoding. For our Titanic dataset, we can perform One-Hot Encoding on the `'sex'` column:\n\nHere, we are using the `pd.get_dummies()` function to perform One-Hot Encoding on the `sex` column. This results in a new dataframe `sex_dummies` where each category in the `sex` column has its own column in the new dataframe.\n\nThat's a Wrap For Now\n\nFantastic work today! You have delved into feature engineering with Python and Pandas, and you've been introduced to creating new features from scratch, modifying existing ones, and handling categorical features. Combining all these techniques, the cleaned, processed data can now be appropriately structured and ready to feed into a machine learning model for training. I almost forgot - let's go do some practice!\n\n[Previous Lesson](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/data-preprocessing-mastering-normalization-and-standardization-techniques)\n\n[Previous](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/data-preprocessing-mastering-normalization-and-standardization-techniques)\n\n[Next Lesson: Training a Machine Learning Model with the Titanic Dataset](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/training-a-machine-learning-model-with-the-titanic-dataset)\n\n[Next Lesson: Training a Machine Learning Model with the Titanic Dataset](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/training-a-machine-learning-model-with-the-titanic-dataset)\n\n[Next](https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/training-a-machine-learning-model-with-the-titanic-dataset)\n\nLog in\n\nStart learning\n\nCompany\n\nCollections\n\nPlatform\n\nRoles\n\nResources\n\nSupport",
      "url": "https://codesignal.com/learn/courses/data-cleaning-and-preprocessing-in-machine-learning/lessons/feature-engineering-enhancing-the-titanic-dataset-for-survival-predictions"
    },
    {
      "title": "Titanic: Getting Started With R - Part 4: Feature Engineering",
      "text": "![Titanic: Getting Started With R - Part 4: Feature Engineering](https://trevorstephens.com/images/titanic-header.png)\n\n[![Trevor Stephens](https://trevorstephens.com/images/blue_avatar.png)](https://trevorstephens.com/)\n\n### Trevor Stephens\n\nRegular Data Scientist, Occasional Blogger.\n\nFollow\n\n- Melbourne, Australia\n- [Website](https://trevorstephens.com)\n- [Twitter](http://twitter.com/trevs)\n- [LinkedIn](http://linkedin.com/in/trevorstephens)\n- [Github](http://github.com/trevorstephens)\n\n[Tutorial index](https://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r)\n\nFeature engineering is so important to how your model performs, that even a simple model with great features can outperform a complicated algorithm with poor ones. In fact, feature engineering has been described as [easily the most important factor](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) in determining the success or failure of your predictive model. Feature engineering really boils down to the human element in machine learning. How much you understand the data, with your human intuition and creativity, can make the difference.\n\nSo what is feature engineering? It can mean many things to different problems, but in the Titanic competition it could mean chopping, and combining different attributes that we were given by the good folks at Kaggle to squeeze a little bit more value from them. In general, an engineered feature may be easier for a machine learning algorithm to digest and make rules from than the variables it was derived from.\n\nThe initial suspects for gaining more machine learning mojo from are the three text fields that we never sent into our decision trees [last time](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-3-decision-trees). While the ticket number, cabin, and name were all unique to each passenger; perhaps parts of those text strings could be extracted to build a new predictive attribute. Let\u2019s start with the name field. If we take a glance at the first passenger\u2019s name we see the following:\n\n```\n>train$Name[1][1]Braund,Mr.OwenHarris891Levels:Abbing,Mr.AnthonyAbbott,Mr.RossmoreEdward...Zimmerman,Mr.Leo\n```\n\nPreviously we have only accessed passenger groups by subsetting, now we access an individual by using the row number, 1, as an index instead. Okay, no one else on the boat had that name, that\u2019s pretty much certain, but what else might they have shared? Well, I\u2019m sure there were plenty of Mr\u2019s aboard. Perhaps the persons title might give us a little more insight.\n\nIf we scroll through the dataset we see many more titles including Miss, Mrs, Master, and even the Countess! The title \u201cMaster\u201d is a bit outdated now, but back in these days, it was reserved for unmarried boys. Additionally, the nobility such as our Countess would probably act differently to the lowly proletariat too. There seems to be a fair few possibilities of patterns in this that may dig deeper than the combinations of age, gender, etc that we looked at before.\n\nIn order to extract these titles to make new variables, we\u2019ll need to perform the same actions on both the training and testing set, so that the features are available for growing our decision trees, and making predictions on the unseen testing data. An easy way to perform the same processes on both datasets at the same time is to merge them. In R we can use `rbind`, which stands for row bind, so long as both dataframes have the same columns as each other. Since we obviously lack the Survived column in our test set, let\u2019s create one full of missing values (NAs) and then row bind the two datasets together:\n\n```\n>test$Survived<-NA>combi<-rbind(train,test)\n```\n\nNow we have a new dataframe called \u201ccombi\u201d with all the same rows as the original two datasets, stacked in the order in which we specified: train first, and test second.\n\nIf you look back at the output of our inquiry on Owen, his name is still encoded as a factor. As we mentioned earlier in the tutorial series, strings are automatically imported as factors in R, even if it doesn\u2019t make sense. So we need to cast this column back into a text string. To do this we use `as.character`. Let\u2019s do this and then take another look at Owen:\n\n```\n>combi$Name<-as.character(combi$Name)>combi$Name[1][1]\"Braund, Mr. Owen Harris\"\n```\n\nExcellent, no more levels, now it\u2019s just pure text. In order to break apart a string, we need some hooks to tell the program to look for. Nicely, we see that there is a comma right after the person\u2019s last name, and a full stop after their title. We can easily use the function `strsplit`, which stands for string split, to break apart our original name over these two symbols. Let\u2019s try it out on Mr. Braund:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][1]\"Braund\"\" Mr\"\" Owen Harris\"\n```\n\nOkay, good. Here we have sent `strsplit` the cell of interest, and given it some symbols to chose from when splitting the string up, either a comma or period. Those symbols in the square brackets are called regular expressions, though this is a very simple one, and if you plan on working with a lot of text I would certainly recommend getting used to using them!\n\nWe see that the title has been broken out on its own, though there\u2019s a strange space before it begins because the comma occurred at the end of the surname. But how do we get to that title piece and clear out the rest of the stuff we don\u2019t want? An index `[[1]]` is printed before the text portions. Let\u2019s try to dig into this new type of container by appending all those square brackets to the original command:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][1]\"Braund\"\" Mr\"\" Owen Harris\"\n```\n\nGetting there! String split uses a doubly stacked matrix because it can never be sure that a given regex will have the same number of pieces. If there were more commas or periods in the name, it would create more segments, so it hides them a level deeper to maintain the rectangular types of containers that we are used to in things like spreadsheets, or now dataframes! Let\u2019s go a level deeper into the indexing mess and extract the title. It\u2019s the second item in this nested list, so let\u2019s dig in to index number 2 of this new container:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][2][1]\" Mr\"\n```\n\nGreat. We have isolated the title we wanted at last. But how to _apply_ this transformation to every row of the combined train/test dataframe? Luckily, R has some extremely useful functions that apply more complicated functions one row at a time. As we had to dig into this container to get the title, simply trying to run `combi$Title <- strsplit(combi$Name, split='[,.]')[[1]][2]` over the whole name vector would result in all of our rows having the same value of Mr., so we need to work a bit harder. Unsurprisingly applying a function to a lot of cells in a dataframe or vector uses the `apply` suite of functions of R:\n\n```\n>combi$Title<-sapply(combi$Name,FUN=function(x){strsplit(x,split='[,.]')[[1]][2]})\n```\n\nR\u2019s apply functions all work in slightly different ways, but `sapply` will work great here. We feed `sapply` our vector of names and our function that we just came up with. It runs through the rows of the vector of names, and sends each name to the function. The results of all these string splits are all combined up into a vector as output from the `sapply` function, which we then store to a new column in our original dataframe, called Title.\n\nFinally, we may wish to strip off those spaces from the beginning of the titles. Here we can just substitute the first occurrence of a space with nothing. We can use `sub` for this ( `gsub` would replace all spaces, poor \u201cthe Countess\u201d would look strange then though):\n\n```\n>combi$Title<-sub(' ','',combi$Title)\n```\n\nAlright, we now have a nice new column of titles, let\u2019s have a look at it:\n\n```\n>table(combi$Title)CaptColDonDonaDrJonkheerLady1411811MajorMasterMissMlleMmeMrMrs26126021757197MsRevSirtheCountess2811\n```\n\nHmm, there are a few very rare titles in here that won\u2019t...",
      "url": "https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering"
    },
    {
      "title": "Exploratory Data Analysis and Feature Engineering Techniques",
      "text": "Hira Akram Exploratory Data Analysis and Feature Engineering Techniques https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46\nExploratory Data Analysis and Feature Engineering Techniques\nHira Akram\n2024-07-31T19:37:05Z\n# Exploratory Data Analysis and Feature Engineering Techniques\n\n## Strategies for enhanced predictive modelling\n\n[Hira Akram](https://hirakram.medium.com/?source=post_page---byline--bf5a04afff46---------------------------------------)\n\n5 min read\n\n\nJan 13, 2021\n\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n[Image Source](https://dribbble.com/ousama_alsaadi)\n\n[_Titanic Kaggle competition_](https://www.kaggle.com/c/titanic) is a great place to understand the machine learning pipeline. In this article, we will discuss the preliminary steps involved in building a predictive model. Let\u2019s begin!\n\n**_Competition Summary:_**\n\nOur aim is to determine from a set of features whether a passenger would have survived the wreck. In other words, we will build a binary classification model that outputs \u20181\u2019 stating that the passenger had survived the Titanic sinking and \u20180\u2019 for not surviving.\n\nBelow is a visualization of the survival percentage in the provided training dataset. Based on this given percentage of ground truth; we will train **five** typical classification models and compare their performances:\n\n**_Exploratory Data Analysis:_**\n\nFirst and foremost step for building a machine learning model is EDA. This involves the following:\n\n- Comprehend the underlying information like numbers of rows and columns, data types, look at a few samples from the data, general figures like mean median mode values.\n- Indicate the proportion of missing values and decide how to address them.\n- Data visualization.\n\nSo let\u2019s explore the training set and establish an understanding of our data:\n\nWe gather the following information from the above code:\n\n- Training data comprises a total of 891 examples and 12 features; including the \u2018 _Survived_\u2019 flag.\n- We have a mix of discrete and continuous features.\n- There are three columns with missing values (Age, Cabin, Embarked)\n\nMoving on, now let\u2019s discuss each feature in detail to get a better intuition.\n\n1. **_Pclass_**\n\nOur first feature to consider is **Pclass.** This is a categorical feature and it represents the socio-economic status of the passengers. We have plotted a treemap which clearly illustrates that the majority of the passengers belonged to the _upper_ socio-economic status.\n\n. **_Name_**\n\nSince the passenger names can not be used as is; therefore we have extracted the titles within the data to make more sense of this information.\n\n. **_Embarked_**\n\nThere were two missing values in this columns. We can either remove these rows or replace them with the mode value. Deleting rows would ultimately lead to date loss and given dataset is already very small; therefore we will go with option two. We also noticed that a high number of passengers embarked on _Southampton._\n\nPress enter or click to view image in full size\n\n. **_Cabin_**\n\nAlthough a huge number of values were missing from this column, but we extracted the alphabets specifying the cabin category. I will shortly discuss how we can make use of this to recreate features.\n\n. **_Age_**\n\nBelow code approximates the missing age values using _IterativeImputer()_ function _._ Later on, we will utilize this column to build more complex features.\n\n**_Feature Engineering_**\n\nThis method is used for adding complexity to the existing data; by constructing new features on top of the pre-existing columns. In order to better represent the underlying structure of the data; we can either split or merge information from different columns. This is one of the many techniques that can enhance our model performance altogether.\n\nIn the Titanic dataset we will include **six** additional columns using this method. Just like we carefully transformed all the passenger names into their titles _;_ let\u2019s look at some more columns.\n\nFirstly, we will make use of the _Parch_ and _SibSp_ columns to count the number of family members that are on board, as below:\n\nMoreover, an attribute like _Fare per Family_ can also help understand the link between these two columns:\n\n_Ticket_ column in itself doesn\u2019t seem quite useful, so we will construct a categorical column based on the nature of this data:\n\nConcatenate the above-created column with the _Cabin_ column that we transformed earlier, to gain some more value out of these features:\n\nThe following two features can also be built using the _Age_ column. For _ageBins,_ we will split the ages into four discrete equal-sized buckets using the pandas _qcut()_ function.\n\nLastly, we will multiply _Age_ with _Fare_ to make a new _numerical_ column.\n\nInitially, the training dataset contained very simple features which were insufficient for the algorithm to search for patterns. Newly constructed features are designed to establish a correspondence between the dataset which ultimately enhances the model accuracy.\n\nNow that our features are ready, we can move on to the next steps that are involved in data preparation.\n\n**_Categorical Encoding_**\n\nSince many algorithms can only interpret numerical data, therefore, encoding the categorical features is an essential step. There are three common workarounds for encoding such features:\n\n1. One Hot Encoding (binary split)\n2. Label Encoding (numerical assignment to each category)\n3. Ordinal Encoding (ordered assignment to each category)\n\nIn our case, we will implement **label encoding** using sklearn _LabelEncoder()_ function. We will label encode the below-mentioned list of columns that contain categorical information in form of words, alphabets or alphanumeric. This task can be achieved by using the following code:\n\n\n['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'familyMembers', 'ticketInfo', 'Cabin', 'Cabin_ticketInfo']\n\n\n**_Standard Scaling_**\n\nContinuous features can largely vary, resulting in very slow convergence hence impacting the model performance on the whole. The term scaling suggests, that we redistribute the feature values upon a fixed range so that the data dispersion is standardized. Once this is achieved, the model will efficiently converge to the global minima.\n\nOriginally, the Titanic data only had a single feature i.e. Fare which required scaling. While constructing new advanced features we included two more. Here\u2019s how we\u2019ll carry out standard scaling on all the continuous data:\n\nBelow is a comparison between unscaled and scaled _\u2018Fare\u2019_ values. We can see that new _x range_ has approximately shrunk to \u00b12, also data is now distributed on zero _mean:_\n\nPress enter or click to view image in full size\n\n**_Conclusion_**\n\nNow that we feel confident about our data, let\u2019s use it to train a bunch of models and pick the algorithm which suits best! In the next article, we will make survival predictions on the Titanic dataset using five binary classification algorithms.\n\nHere are a few samples from the finalized training data:\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Titanic](https://medium.com/tag/titanic?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Data Preprocessing](https://medium.com/tag/data-preprocessing?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Feature Engineering](https://medium.com/tag/feature-engineering?source=post_page-----bf5a04afff46---------------------------------------)\n\nFollow\n\n[**Published in Towards AI**](https://pub.towardsai.net/?source=post_page---post_publication_info--bf5a04afff46---------------------------------------)\n\n[86K followers](https://pub.towardsai.net/followers?source=post_page---post_publication_info--bf5a04afff46----------------------...",
      "url": "https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46"
    },
    {
      "title": "Titanic Survival Prediction\u200a\u2014\u200aI",
      "text": "- [Latest](https://towardsai.net/p/)\n- [Trends](https://medium.com/towards-artificial-intelligence/trending)\n- [Shop](https://gumroad.com/towardsai)\n\nName: Towards AI\nLegal Name: Towards AI, Inc.\nDescription: Towards AI is the world's leading artificial intelligence (AI) and technology publication. Read by thought-leaders and decision-makers around the world.\nPhone Number: +1-650-246-9381\nEmail: pub@towardsai.net\n\n228 Park Avenue SouthNew York,\nNY10003United States\n\nWebsite: [https://towardsai.net/](https://towardsai.net/)\nPublisher: [https://towardsai.net/#publisher](https://towardsai.net/#publisher)\nDiversity Policy: [https://towardsai.net/about](https://towardsai.net/about)\nEthics Policy: [https://towardsai.net/about](https://towardsai.net/about)\nMasthead: [https://towardsai.net/about](https://towardsai.net/about)\n\nName: Towards AI\nLegal Name: Towards AI, Inc.\nDescription: Towards AI is the world's leading artificial intelligence (AI) and technology publication.\nFounders:\nRoberto Iriondo,\n[Website](https://www.robertoiriondo.com/),\nJob Title: Co-founder and AdvisorWorks for: Towards AI, Inc.Follow Roberto:\n[X](https://x.com/@robiriondo),\n[LinkedIn](https://www.linkedin.com/in/robiriondo),\n[GitHub](https://github.com/robiriondo),\n[Google Scholar](https://scholar.google.com/citations?user=dTn7NEcAAAAJ),\n[Towards AI Profile](https://towardsai.net/p/author/robiriondo),\n[Medium](https://medium.com/@robiriondo),\n[ML@CMU](https://www.ml.cmu.edu/robiriondo),\n[FreeCodeCamp](https://www.freecodecamp.org/news/author/robiriondo/),\n[Crunchbase](https://www.crunchbase.com/person/roberto-iriondo),\n[Bloomberg](https://www.bloomberg.com/profile/person/22994840),\n[Roberto Iriondo, Generative AI Lab](https://roberto.generativeailab.org/),\n[Generative AI Lab](https://generativeailab.org/) [VeloxTrend](https://veloxtrend.com/) [Ultrarix Capital Partners](https://ultrarix.com/)Denis Piffaretti,\nJob Title: Co-founderWorks for: Towards AI, Inc.Louie Peters,\nJob Title: Co-founderWorks for: Towards AI, Inc.Louis-Fran\u00e7ois Bouchard,\nJob Title: Co-founderWorks for: Towards AI, Inc.\nCover:\nLogo:\nAreas Served: Worldwide\nAlternate Name: Towards AI, Inc.\nAlternate Name: Towards AI Co.\nAlternate Name: towards ai\nAlternate Name: towardsai\nAlternate Name: towards.ai\nAlternate Name: tai\nAlternate Name: toward ai\nAlternate Name: toward.ai\nAlternate Name: Towards AI, Inc.\nAlternate Name: towardsai.net\nAlternate Name: pub.towardsai.net\n\nFollow us on:\n[Facebook](https://www.facebook.com/towardsAl/) [X](https://x.com/towards_ai) [LinkedIn](https://www.linkedin.com/company/towards-artificial-intelligence) [Instagram](https://www.instagram.com/towards_ai/) [Youtube](https://www.youtube.com/channel/UCQNjFuhOJM1YqFTPY1Q_kYQ) [Github](https://github.com/towardsai) [Google My Business](https://local.google.com/place?id=4955254490173856159&use=srp&ved=1t%3A65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376#fpstate=lie) [Google Search](https://www.google.com/search?ved=1t:65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376&q=Towards+AI&ludocid=4955254490173856159&lsig=AB86z5Ur3DZsSmdOFFUwd-bMHTIe#fpstate=lie) [Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw?oc=3&ceid=US:en) [Google Maps](https://g.page/TowardsAI?gm) [Discord](https://discord.com/invite/V7RGX9XKAW) [Shop](https://gumroad.com/towardsai) [Towards AI, Medium Editorial](https://towardsai.medium.com) [Medium](https://medium.com/towards-artificial-intelligence) [Flipboard](https://flipboard.com/@Towards_AI) [Publication](https://pub.towardsai.net/) [Feed](https://feed.towardsai.net/) [Sponsors](https://sponsors.towardsai.net/) [Sponsors](https://members.towardsai.net/) [Contribute](https://contribute.towardsai.net/)\n\n5 stars \u2013 based on\n497 reviews\n\n#### Frequently Used, Contextual References\n\nTODO: Remember to copy unique IDs whenever it needs used. i.e., URL: 304b2e42315e\n\n## Resources\n\nOur 15 AI experts built the most comprehensive, practical, 90+ lesson courses to master AI Engineering - we have pathways for any experience at [Towards AI Academy](https://academy.towardsai.net/?utm_source=taiwordpress&utm_medium=banner). Cohorts still open - use COHORT10 for 10% off.\n\n## Publication\n\n- [Home](https://towardsai.net/)\n- [Publication](https://towardsai.net/p)\n- [Data Analysis](https://towardsai.net/ai/data-analysis)\n- Titanic Survival Prediction\u200a\u2014\u200aI\n\n[Data Analysis](https://towardsai.net/ai/data-analysis)\n\n# Titanic Survival Prediction\u200a\u2014\u200aI\n\n[Towards AI Team](https://towardsai.net/p/author/team)\n\n[66 likes](https://towardsai.net/wp-admin/admin-ajax.php?action=process_simple_like&post_id=8094&nonce=bb2217e73f&is_comment=0&disabled=true)\n\nFebruary 8, 2021\n\nShare this post\n\nLast Updated on January 6, 2023 by [Editorial Team](https://towardsai.net/p/author/editorial-team)\n\n#### **Author(s): [Hira Akram](https://medium.com/towards-artificial-intelligence/titanic-survival-prediction-i-bf5a04afff46?source=rss----98111c9905da---4)**\n\n#### [Data Analysis](https://towardsai.net/p/category/data-analysis)\n\n### Titanic Survival Prediction\u200a\u2014\u200aI\n\n#### Exploratory Data Analysis and Feature Engineering on the Titanic\u00a0dataset\n\n**_Artist_** _: Graham\u00a0Coton_\n\n[_Titanic Kaggle competition_](https://www.kaggle.com/c/titanic) is a great place to understand the [machine learning](https://towardsai.net/ai/machine-learning) pipeline. In this article, we will discuss the preliminary steps involved in building a predictive model. Let\u2019s\u00a0begin!\n\n**_Competition Summary:_**\n\nOur aim is to determine from a set of features whether a passenger would have survived the wreck. In other words, we will build a binary classification model that outputs \u20181\u2019 stating that the passenger had survived the Titanic sinking and \u20180\u2019 for not surviving.\n\nBelow is a visualization of the survival percentage in the provided [training data](https://towardsai.net/p/machine-learning/best-datasets-for-machine-learning-and-data-science-d80e9f030279) set. Based on this given percentage of ground truth; we will train **five** typical classification models and compare their performances:\n\n**_Exploratory Data Analysis:_**\n\nFirst and foremost step for building a [machine learning](https://towardsai.net/ai/machine-learning) model is EDA. This involves the following:\n\n- Comprehend the underlying information like numbers of rows and columns, data types, look at a few samples from the data, general figures like mean median mode\u00a0values.\n- Indicate the proportion of [missing values](https://towardsai.net/p/data-science/handling-missing-values-in-pandas-f87cec928937) and decide how to address\u00a0them.\n- Data visualization.\n\nSo let\u2019s explore the training set and establish an understanding of our\u00a0data:\n\nWe gather the following information from the above\u00a0code:\n\n- Training data comprises a total of 891 examples and 12 features; including the \u2018 _Survived_\u2019 flag.\n- We have a mix of discrete and continuous features.\n- There are three columns with [missing values](https://towardsai.net/p/data-science/handling-missing-values-in-pandas-f87cec928937) (Age, Cabin, Embarked)\n\nMoving on, now let\u2019s discuss each feature in detail to get a better intuition.\n\n1. **_Pclass_**\n\nOur first feature to consider is **Pclass.** This is a categorical feature and it represents the socio-economic status of the passengers. We have plotted a treemap which clearly illustrates that the majority of the passengers belonged to the _upper_ socio-economic status.\n\n2\\. **_Name_**\n\nSince the passenger names can not be used as is; therefore we have extracted the titles within the data to make more sense of this information.\n\n3\\. **_Embarked_**\n\nThere were two missing values in this columns. We can either remove these rows or replace them with the mode value. Deleting rows would ultimately lead to date loss and given dataset is already very small; therefore we will go with option two. We also noticed that a high number of passengers embarked on _Southampton._\n\n4\\. **_Cabin_**\n\nAlthough a huge number of va...",
      "url": "https://towardsai.net/p/data-analysis/titanic-survival-prediction-i-bf5a04afff46"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Feature Engineering Titanic.dbc",
      "text": "[Skip to content](https://gist.github.com/gist.github.com#start-of-content)\n\nSearch Gists\nSearch Gists\n\n[Sign\u00a0in](https://gist.github.com/auth/github?return_to=https%3A%2F%2Fgist.github.com%2Faero-girl%2Ffdad9933f0d650796fe91e3bff9ac9f6) [Sign\u00a0up](https://gist.github.com/join?return_to=https%3A%2F%2Fgist.github.com%2Faero-girl%2Ffdad9933f0d650796fe91e3bff9ac9f6&source=header-gist)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\nInstantly share code, notes, and snippets.\n\n# [aero-girl](https://gist.github.com/aero-girl)/ **[Feature Engineering Titanic.dbc](https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6)** Secret\n\nLast active\nJanuary 31, 2023 14:40\n\nShow Gist options\n\n- [Download ZIP](https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6/archive/31d1a849db5df6d5c4a0b7d3d8db82169d2184b4.zip)\n\n- [Star0(0)](https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Faero-girl%2Ffdad9933f0d650796fe91e3bff9ac9f6) You must be signed in to star a gist\n- [Fork5(5)](https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Faero-girl%2Ffdad9933f0d650796fe91e3bff9ac9f6) You must be signed in to fork a gist\n\n- Embed\n\n\n\n\n\n- Embed\nEmbed this gist in your website.\n- Share\nCopy sharable link for this gist.\n- Clone via HTTPS\nClone using the web URL.\n- [Learn more about clone URLs](https://docs.github.com/articles/which-remote-url-should-i-use)\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6.js&quot;&gt;&lt;/script&gt;\n\n- Save aero-girl/fdad9933f0d650796fe91e3bff9ac9f6 to your computer and use it in GitHub Desktop.\n\nEmbed\n\n- Embed\nEmbed this gist in your website.\n- Share\nCopy sharable link for this gist.\n- Clone via HTTPS\nClone using the web URL.\n- [Learn more about clone URLs](https://docs.github.com/articles/which-remote-url-should-i-use)\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6.js&quot;&gt;&lt;/script&gt;\n\nSave aero-girl/fdad9933f0d650796fe91e3bff9ac9f6 to your computer and use it in GitHub Desktop.\n\n[Download ZIP](https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6/archive/31d1a849db5df6d5c4a0b7d3d8db82169d2184b4.zip)\n\n[Raw](https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6/raw/31d1a849db5df6d5c4a0b7d3d8db82169d2184b4/Feature%2520Engineering%2520Titanic.dbc)\n\n[**Feature Engineering Titanic.dbc**](https://gist.github.com/gist.github.com#file-feature-engineering-titanic-dbc)\n\n|     |\n| --- |\n| \\# Databricks notebook source |\n| \\# MAGIC %md |\n| \\# MAGIC # Titanic \ud83d\udc4b\ud83d\udef3\ufe0f Ahoy! |\n| \\# MAGIC This is the legendary Titanic ML dataset |\n| \\# MAGIC The goal is simple: utilise machine learning to develop a model that predicts which passengers survived the Titanic shipwreck. To generate the optimal model, however, the features in the dataset must go through a feature engineering procedure. |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC # Background |\n| \\# MAGIC \\* The sinking of the Titanic is one of the most infamous shipwrecks in history. |\n| \\# MAGIC \\* On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. |\n| \\# MAGIC \\* While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. |\n| \\# MAGIC \\* This is about building a predictive model that is able to predict which passengers survived using passenger data (ie name, age, gender, socio-economic class, etc). |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC # 1. Setup |\n| \\# COMMAND ---------- |\n| import numpy as np |\n| import pandas as pd |\n| from pyspark.sql.functions import \\* |\n| from databricks import feature\\_store |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC # 2. Import Dataset \u2b07 |\n| \\# MAGIC Training Dataset: \\`train\\` \ud83d\udea2 |\n| \\# COMMAND ---------- |\n| #Alter this path to point to the location of where you have uploaded your train.csv |\n| dbfs\\_path = 'dbfs:/FileStore/tables/train.csv' |\n| df\\_train = spark.read.csv(dbfs\\_path, header = \"True\", inferSchema=\"True\") |\n| \\# COMMAND ---------- |\n| \\# Display training data |\n| display(df\\_train) |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md ### Summary of Data |\n| \\# COMMAND ---------- |\n| display(df\\_train.describe()) |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC ## Checking Schema of our dataset |\n| \\# COMMAND ---------- |\n| df\\_train.printSchema() |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md # 3. Cleaning Data \ud83e\uddf9 |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md ### Renaming Columns |\n| \\# COMMAND ---------- |\n| df\\_train = (df\\_train |\n| .withColumnRenamed(\"Pclass\", \"PassengerClass\") |\n| .withColumnRenamed(\"SibSp\", \"SiblingsSpouses\") |\n| .withColumnRenamed(\"Parch\", \"ParentsChildren\")) |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md # 4. Feature Engineering \ud83d\udee0 |\n| \\# MAGIC Apply feature engineering steps |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC ### Passenger's Title |\n| \\# COMMAND ---------- |\n| df\\_train.select(\"Name\") |\n| \\# COMMAND ---------- |\n| \\# These titles provides information on social status, profession, etc. |\n| \\# Extract Title from Name, store in column \"Title\" |\n| df = df\\_train.withColumn(\"Title\",regexp\\_extract(col(\"Name\"),\"(\\[A-Za-z\\]+)\\\\.\",1)) |\n| \\# COMMAND ---------- |\n| \\# This new column 'Title' is actually a new feature for your data set! |\n| display(df) |\n| \\# COMMAND ---------- |\n| \\# Sanitise and group titles |\n| \\# 'Mlle', 'Mme', 'Ms' --> Miss |\n| \\# 'Lady', 'Dona', 'Countess' --> Mrs |\n| \\# 'Dr', 'Master', 'Major', 'Capt', 'Sir', 'Don' --> Mr |\n| \\# 'Jonkheer' , 'Col' , 'Rev' --> Other |\n| df = df.replace(\\['Mlle','Mme', 'Ms', 'Dr','Master','Major','Lady','Dona','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'\\], |\n| \\['Miss','Miss','Miss','Mr','Mr', 'Mr', 'Mrs', 'Mrs', 'Mrs', 'Other', 'Other','Other','Mr','Mr','Mr'\\]) |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC ### Passenger's Cabins |\n| \\# COMMAND ---------- |\n| \\# Did they have a Cabin? |\n| df = df.withColumn('Has\\_Cabin', df.Cabin.isNotNull()) |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC ## Family Sizes of Passengers |\n| \\# COMMAND ---------- |\n| \\# Create a new feature called \"Family\\_size\". This feature is the summation of ParentsChildren (parents/children) and SiblingsSpouses(siblings/spouses). It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers |\n| df = df.withColumn(\"Family\\_Size\", col('SiblingsSpouses') + col('ParentsChildren') + 1) |\n| \\# COMMAND ---------- |\n| \\# Let's organise the newly created features in a dataframe |\n| titanic\\_feature = df.select(\"Name\",\"Cabin\",\"Title\",\"Has\\_Cabin\",\"Family\\_Size\") |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md |\n| \\# MAGIC # 5. Use Feature Store library to create new feature tables \ud83d\udcbb |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md First, create the database where the feature tables will be stored. |\n| \\# COMMAND ---------- |\n| \\# MAGIC %sql |\n| \\# MAGIC CREATE DATABASE IF NOT EXISTS feature\\_store\\_titanic; |\n| \\# COMMAND ---------- |\n| \\# MAGIC %md ## Instantiate a Feature Store client and create table |\n| \\# COMMAND ---------- |\n| fs = feature\\_store.FeatureStoreClient() |\n| fs.create\\_table( |\n| name=\"feature\\_store\\_titanic.titanic\\_passengers\\_features\\_2\", |\n| primary\\_keys = \\[\"Name\",\"Cabin\"\\], |\n| df = titanic\\_feature, |\n| description = \"Titanic Passenger Features\") |\n| \\# COMMAND ---------- |\n| \\# Write the features DataFrame to the feature store table |\n| \\# 'merge' - upserts rows in df into the feature table. |\n| \\# 'overwrite'...",
      "url": "https://gist.github.com/aero-girl/fdad9933f0d650796fe91e3bff9ac9f6"
    }
  ]
}