{
  "query": "What are the best machine learning models for Titanic survival prediction on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most successful Kaggle Titanic solutions rely on **strong tree\u2011based learners and ensembles** rather than a single simple classifier.  Recent open\u2011source notebooks that cracked the leaderboard report that:\n\n* **Support Vector Machines (SVM)** achieved the highest average F1 score (\u2248\u202f80\u202f%) among five tested algorithms, edging out other models by a small margin\u3010https://amyrmahdy.medium.com/titanic-survival-prediction-comparing-5-machine-learning-models-d1d451a9e558\u3011.  \n* **Random Forests** consistently rank near the top, delivering the best single\u2011model accuracy in several repos (84.35\u202f% on a hold\u2011out set) and outperforming baseline logistic regression in ensemble pipelines\u3010https://github.com/alicevillar/titanic-kaggle\u3011, \u3010https://github.com/engalaagabr/Titanic\u3011.  \n* **Gradient\u2011boosting (XGBoost)** is a core component of the highest\u2011scoring pipelines, often combined with other learners in stacked ensembles that push public scores into the top\u202f10\u202f% of the competition\u3010https://github.com/ata-turhan/titanic-survival-prediction\u3011.  \n* **Stacked or voting ensembles** that blend multiple classifiers (e.g., Logistic Regression, Random Forest, XGBoost, SVM) are repeatedly cited as the key to reaching the top\u202f5\u201110\u202f% leaderboard positions\u3010https://github.com/rishbjain1/titanic-survival-prediction-top-10-kaggle-model\u3011, \u3010https://github.com/Moddy2024/Titanic-Survival-prediction\u3011.\n\n**Bottom line:** For the Titanic Kaggle challenge, the best-performing models are (1) SVM for its strong F1 performance, (2) Random Forest as a robust single learner, (3) XGBoost for its gradient\u2011boosting power, and (4) ensemble strategies\u2014especially stacked or voting ensembles\u2014that combine these learners to achieve the highest scores.",
      "url": ""
    },
    {
      "title": "GitHub - rishbjain1/Titanic-Survival-Prediction-Top-10-Kaggle-Model: A machine learning model built to predict Titanic passenger survival \u2014 achieving a Top 10 Kaggle score.",
      "text": "<div><div><article><p></p><h2>\ud83d\udea2 Titanic - Machine Learning from Disaster</h2><a href=\"#-titanic---machine-learning-from-disaster\"></a><p></p>\n<p>Predict which passengers survived the Titanic shipwreck using supervised machine learning.</p>\n<blockquote>\n<p>\ud83c\udfc6 <strong>Achieved Top 10 on Kaggle's Titanic Leaderboard</strong><br/>\nthrough optimized feature engineering and model tuning.</p>\n</blockquote>\n<hr/>\n<p></p><h2>\ud83d\udcc1 Dataset</h2><a href=\"#-dataset\"></a><p></p>\n<p>\ud83d\udd17 <a href=\"https://www.kaggle.com/competitions/titanic\">Kaggle Competition Link</a></p>\n<ul>\n<li><code>train.csv</code> \u2013 Includes features + survival outcome</li>\n<li><code>test.csv</code> \u2013 Features only; survival is to be predicted</li>\n<li><code>gender_submission.csv</code> \u2013 Sample submission format</li>\n</ul>\n<p>Key columns: <code>Pclass</code>, <code>Sex</code>, <code>Age</code>, <code>Fare</code>, <code>Embarked</code>, <code>SibSp</code>, <code>Parch</code>, <code>Cabin</code>, etc.</p>\n<hr/>\n<p></p><h2>\ud83d\udccc Tasks Performed</h2><a href=\"#-tasks-performed\"></a><p></p>\n<ul>\n<li>\u2705 Data Cleaning &amp; Missing Value Imputation</li>\n<li>\u2705 Exploratory Data Analysis (EDA) &amp; Visualization</li>\n<li>\u2705 Feature Engineering (e.g., Title extraction, Family size, Fare binning)</li>\n<li>\u2705 Model Building: Logistic Regression, Decision Trees, Random Forest, and more</li>\n<li>\u2705 Hyperparameter Tuning &amp; Cross-Validation</li>\n<li>\u2705 Final Submission to Kaggle Leaderboard</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udee0 Tools Used</h2><a href=\"#-tools-used\"></a><p></p>\n<ul>\n<li>Python</li>\n<li>Pandas, NumPy</li>\n<li>Scikit-learn</li>\n<li>Seaborn, Matplotlib</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\ude80 How to Run</h2><a href=\"#-how-to-run\"></a><p></p>\n<div><pre>pip install -r requirements.txt\njupyter notebook titanic_model.ipynb</pre></div>\n</article></div></div>",
      "url": "https://github.com/rishbjain1/titanic-survival-prediction-top-10-kaggle-model"
    },
    {
      "title": "GitHub - ata-turhan/Titanic-Survival-Prediction: A comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[ata-turhan](https://github.com/ata-turhan)/ **[Titanic-Survival-Prediction](https://github.com/ata-turhan/Titanic-Survival-Prediction)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n- [Star\\\n27](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n\n\nA comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.\n\n### License\n\n[MIT license](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE)\n\n[27\\\nstars](https://github.com/ata-turhan/Titanic-Survival-Prediction/stargazers) [0\\\nforks](https://github.com/ata-turhan/Titanic-Survival-Prediction/forks) [Branches](https://github.com/ata-turhan/Titanic-Survival-Prediction/branches) [Tags](https://github.com/ata-turhan/Titanic-Survival-Prediction/tags) [Activity](https://github.com/ata-turhan/Titanic-Survival-Prediction/activity)\n\n[Star](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n\n[Notifications](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction) You must be signed in to change notification settings\n\n# ata-turhan/Titanic-Survival-Prediction\n\nmain\n\n[Branches](https://github.com/ata-turhan/Titanic-Survival-Prediction/branches) [Tags](https://github.com/ata-turhan/Titanic-Survival-Prediction/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[11 Commits](https://github.com/ata-turhan/Titanic-Survival-Prediction/commits/main/) |\n| [.gitignore](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/.gitignore) | [.gitignore](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/.gitignore) |\n| [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) | [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) |\n| [README.md](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/README.md) | [README.md](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/README.md) |\n| [requirements.txt](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/requirements.txt) | [requirements.txt](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/requirements.txt) |\n| [titanic.ipynb](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/titanic.ipynb) | [titanic.ipynb](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/titanic.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# **Titanic Survival Prediction**\n\nThis repository contains a comprehensive solution for the Kaggle **Titanic - Machine Learning from Disaster** competition. Using machine learning techniques, we predict the survival of passengers based on key features like age, gender, class, and more. This project also includes **Explainable AI (XAI)** techniques to understand model predictions.\n\n## **Project Overview**\n\n- **Goal**: Predict whether a Titanic passenger survived or not, using features such as age, gender, ticket class, and more.\n- **Techniques**:\n  - Data Cleaning and Feature Engineering.\n  - Model Training and Hyperparameter Tuning.\n  - Model Stacking for improved performance.\n  - Explainable AI using SHAP, permutation importance, and feature importances.\n\n## **Features**\n\n- **Extensive Data Exploration**: Visualizations and insights from the dataset.\n- **Feature Engineering**: New features such as `AgeGroup`, `FareBin`, `FamilySize`, `IsAlone`, and `Title`.\n- **Model Training**: Logistic Regression, RandomForest, XGBoost, MultiLayer Perceptron, and Stacked Ensembles.\n- **Explainable AI**: Insights into model predictions with SHAP values, permutation importance, and feature coefficients.\n- **Kaggle Submission**: Prepares predictions in the required format for submission.\n\n## **Setup Instructions**\n\n### **1\\. Clone the Repository**\n\n```\ngit clone https://github.com/ata-turhan/titanic-survival-prediction.git\ncd titanic-survival-prediction\n```\n\n### **2\\. Install Dependencies**\n\nEnsure you have Python 3.8+ installed. Install the required packages using:\n\n```\npip install -r requirements.txt\n```\n\n### **3\\. Run the Notebook**\n\nLaunch Jupyter Notebook and open `notebooks/titanic_predicting_survivors.ipynb`:\n\n```\njupyter notebook\n```\n\n## **Key Results**\n\n| **Model** | **Accuracy** | **F1-Score** |\n| --- | --- | --- |\n| Logistic Regression | 80.45% | 0.80 |\n| RandomForest | 82.68% | 0.82 |\n| MultiLayer Perceptron | 79.33% | 0.79 |\n| XGBoost | TBD | TBD |\n| **Stacked Model** | **83.50%** | **0.83** |\n\n## **Explainable AI (XAI)**\n\nThis project uses several techniques to explain model predictions:\n\n1. **Logistic Regression Coefficients**:\n\n   - Visualizes the top features affecting survival.\n2. **RandomForest Feature Importances**:\n\n   - Highlights features most impactful for survival predictions.\n3. **Permutation Importance**:\n\n   - Explains predictions of the MLP model by permuting feature values.\n4. **SHAP Values**:\n\n   - Detailed explanations for RandomForest predictions with SHAP plots.\n\n## **Kaggle Submission**\n\nThe repository includes the final predictions in the `outputs/` directory:\n\n- **File**: `submission.csv`\n- **Format**:\n\n\n```\nPassengerId,Survived\n892,0\n893,1\n...\n\n```\n\n\n## **Future Improvements**\n\n- Explore advanced stacking techniques\n- Incorporate additional external datasets for richer feature engineering.\n\n## **License**\n\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) file for details.\n\n## About\n\nA comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.\n\n### Topics\n\n[python](https://github.com/topics/python) [kaggle](https://github.com/topics/kaggle) [xgboost](https://github.com/topics/xgboost) [classification](https://github.com/topics/classification) [shap](https://github.com/topics/shap)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/github.com#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/ata-turhan/Titanic-Survival-Prediction/activity)\n\n### Stars\n\n[**27**\\\nstars](https://github.com/ata-turhan/Titanic-Survival-Prediction/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/ata-turhan/Titanic-Survival-Prediction/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/ata-turhan/Titanic-Survival-Prediction/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fata-turhan%2FTitanic-Survival-Prediction&report=ata-turhan+%28user%29)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/ata-turhan/Titanic-Survival-Prediction/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/ata-turhan/titanic-survival-prediction"
    },
    {
      "title": "GitHub - Moddy2024/Titanic-Survival-prediction: The top 5% of the titanic competition in Kaggle. achieved this through ensemble of models",
      "text": "[Skip to content](https://github.com/Moddy2024/Titanic-Survival-prediction#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n{{ message }}\n\n[Moddy2024](https://github.com/Moddy2024)/ **[Titanic-Survival-prediction](https://github.com/Moddy2024/Titanic-Survival-prediction)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n- [Star\\\n0](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n\n\nThe top 5% of the titanic competition in Kaggle. achieved this through ensemble of models\n\n[0\\\nstars](https://github.com/Moddy2024/Titanic-Survival-prediction/stargazers) [0\\\nforks](https://github.com/Moddy2024/Titanic-Survival-prediction/forks) [Branches](https://github.com/Moddy2024/Titanic-Survival-prediction/branches) [Tags](https://github.com/Moddy2024/Titanic-Survival-prediction/tags) [Activity](https://github.com/Moddy2024/Titanic-Survival-prediction/activity)\n\n[Star](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n\n[Notifications](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction) You must be signed in to change notification settings\n\n# Moddy2024/Titanic-Survival-prediction\n\nmain\n\n[Branches](https://github.com/Moddy2024/Titanic-Survival-prediction/branches) [Tags](https://github.com/Moddy2024/Titanic-Survival-prediction/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[12 Commits](https://github.com/Moddy2024/Titanic-Survival-prediction/commits/main/) |\n| [files from kaggle](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/files%20from%20kaggle) | [files from kaggle](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/files%20from%20kaggle) |  |  |\n| [results](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/results) | [results](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/results) |  |  |\n| [README.md](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/README.md) | [README.md](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/README.md) |  |  |\n| [titanic-1.ipynb](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/titanic-1.ipynb) | [titanic-1.ipynb](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/titanic-1.ipynb) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Titanic top 5%\n\nThis is the most famous Kaggle [competition](https://www.kaggle.com/c/titanic) in which you have to use machine learning to create a model that predicts which passenger can survive the Titanic shipwreck or not. I have done a lot of data engineering and feature engineering to clean and to increase the accuracy of my models.\n\nI have trained 3 models :\n\n- Logistic Regression.\n- XGBClassifier.\n- Random Forest.\n\nI have also ensemble all 3 of these models together to see the results and have only used ensemble of Logistic Regression and XGBClassifier. The accuracy in all of my submission file is a minimum of 75% in Kaggle but the best one was ensemble of Logistic Regression and XGBClassifier which is the file name [hardvoting\\_withoutrf](https://github.com/Moddy2024/Titanic/blob/main/results/hardvotingwithoutrf_submission.csv) which got me in the top 5% of all the people in the Titanic Competetion in Kaggle. RandomForest seems to be overfitting because we don't have a very big dataset. When comparing each of the models separately Logistic Regression works better than XGBoost and Random Forest so after ensembling the best two the ensembled model works even well.\n\n# Dataset\n\nYou can download the dataset from Kaggle or get it in this repo which I have already downloaded from Kaggle.\n\nFor the training file go [here](https://github.com/Moddy2024/Titanic/blob/main/files%20from%20kaggle/train.csv).\n\nFor the test file go [here](https://github.com/Moddy2024/Titanic/blob/main/files%20from%20kaggle/test.csv).\n\n# Software requirements\n\n- Numpy\n- Pandas\n- Seaborn\n- Matplotlib\n- Scikitlearn\n- XGBoost\n\n# Key Files\n\n- [titanic-1.ipynb](https://github.com/Moddy2024/Titanic/blob/main/titanic-1.ipynb) \\- In this file you can see all the data engineering and the feature engineering that I have performed. After which I train the model, ensemble them and check their cross validation score.\n- [results](https://github.com/Moddy2024/Titanic/tree/main/results) \\- All the results of the different models and ensemble are present here in csv format.\n- [files from kaggle](https://github.com/Moddy2024/Titanic/tree/main/files%20from%20kaggle) \\- The files that are provided by Kaggle. There are three files here training,test and gender\\_submission.csv. We can only use the training file for training the model for the competition and predict the results using the data in the test file for submission. The gender\\_submission.csv is as an example of what a submission file should look like.\n\n## About\n\nThe top 5% of the titanic competition in Kaggle. achieved this through ensemble of models\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [kaggle](https://github.com/topics/kaggle) [titanic-kaggle](https://github.com/topics/titanic-kaggle) [classification](https://github.com/topics/classification) [data-analysis](https://github.com/topics/data-analysis) [ensemble](https://github.com/topics/ensemble) [feature-engineering](https://github.com/topics/feature-engineering) [ensemble-model](https://github.com/topics/ensemble-model) [ensemble-classifier](https://github.com/topics/ensemble-classifier) [catboost](https://github.com/topics/catboost) [ensemble-machine-learning](https://github.com/topics/ensemble-machine-learning) [xgboost-classifier](https://github.com/topics/xgboost-classifier) [catboost-classifier](https://github.com/topics/catboost-classifier)\n\n### Resources\n\n[Readme](https://github.com/Moddy2024/Titanic-Survival-prediction#readme-ov-file)\n\n[Activity](https://github.com/Moddy2024/Titanic-Survival-prediction/activity)\n\n### Stars\n\n[**0**\\\nstars](https://github.com/Moddy2024/Titanic-Survival-prediction/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/Moddy2024/Titanic-Survival-prediction/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/Moddy2024/Titanic-Survival-prediction/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FModdy2024%2FTitanic-Survival-prediction&report=Moddy2024+%28user%29)\n\n## [Releases](https://github.com/Moddy2024/Titanic-Survival-prediction/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/Moddy2024/packages?repo_name=Titanic-Survival-prediction)\n\nNo packages published\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/Moddy2024/Titanic-Survival-prediction/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/Moddy2024/Titanic-Survival-prediction"
    },
    {
      "title": "GitHub - alicevillar/titanic-kaggle: Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%",
      "text": "[Skip to content](https://github.com/alicevillar/titanic-kaggle#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/alicevillar/titanic-kaggle) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[alicevillar](https://github.com/alicevillar)/ **[titanic-kaggle](https://github.com/alicevillar/titanic-kaggle)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle) You must be signed in to change notification settings\n- [Fork\\\n3](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n- [Star\\\n6](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n\n\nTitanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%\n\n[6\\\nstars](https://github.com/alicevillar/titanic-kaggle/stargazers) [3\\\nforks](https://github.com/alicevillar/titanic-kaggle/forks) [Branches](https://github.com/alicevillar/titanic-kaggle/branches) [Tags](https://github.com/alicevillar/titanic-kaggle/tags) [Activity](https://github.com/alicevillar/titanic-kaggle/activity)\n\n[Star](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle)\n\n[Notifications](https://github.com/login?return_to=%2Falicevillar%2Ftitanic-kaggle) You must be signed in to change notification settings\n\n# alicevillar/titanic-kaggle\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/alicevillar/titanic-kaggle/branches) [Tags](https://github.com/alicevillar/titanic-kaggle/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[13 Commits](https://github.com/alicevillar/titanic-kaggle/commits/main/) |\n| ### [README.md](https://github.com/alicevillar/titanic-kaggle/blob/main/README.md) | ### [README.md](https://github.com/alicevillar/titanic-kaggle/blob/main/README.md) |  |  |\n| ### [Titanic\\_DecisionTree.ipynb](https://github.com/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) | ### [Titanic\\_DecisionTree.ipynb](https://github.com/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) |  |  |\n| ### [accuracy\\_graph\\_titanic.png](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png) | ### [accuracy\\_graph\\_titanic.png](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png) |  |  |\n| ### [titanic.csv](https://github.com/alicevillar/titanic-kaggle/blob/main/titanic.csv) | ### [titanic.csv](https://github.com/alicevillar/titanic-kaggle/blob/main/titanic.csv) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN\n\nThe objective of Kaggle's Titanic Challenge was to build a classification model that could successfully predict the survival or the death of a given passenger based on a set of variables. The purpose of this repository is to document the process I went through to create a Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN.\n\n### Quick Start:\n\n[Check out](https://nbviewer.jupyter.org/github/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb) a static version of the notebook with Jupyter NBViewer from the comfort of your web browser.\n\n### Dependencies:\n\n- [Numpy](https://numpy.org/)\n- [Pandas](https://pandas.pydata.org/)\n- [SciKit-Learn](https://scikit-learn.org/)\n- [Matplotlib](https://matplotlib.org/)\n- [Seaborn](https://seaborn.pydata.org/)\n\n### Approach\n\n- PART 1: Data Handling -> Importing Data with Pandas, cleaning data, data description.\n- PART 2: Data Analysis -> Supervised ML Techniques: Decision Tree, SVM, Logistic Regression, Random Forest and KNN\n- PART 3: Valuation of the Analysis -> Performance measurement + K-folds cross validation to evaluate results locally + Accuracy comparison graph\n\n### Kaggle Competition \\| Titanic Machine Learning from Disaster\n\nKaggle's challenge provides information about a subset of the Titanic population and asks a predictive model that tells whether or not a given passenger survived.\nWe are given basic explanatory variables, including passenger gender, age, passenger class, among others. More details about the competition can be found on Kaggle's [Competition Page](https://www.kaggle.com/c/titanic):\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. In this contest, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\n### Results\n\nIn this repository I documented the process to create a Titanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. To all these modeles I did a Valuation Analisis (Performance Measurement and K-Fold). I have found the accuracy score with Random forest (0.8435754189944135) and Decision Tree (0.8212290502793296).Finally, I created a graph to compare the accuracy of the different models.\n\n[![print](https://github.com/alicevillar/titanic-kaggle/raw/main/accuracy_graph_titanic.png)](https://github.com/alicevillar/titanic-kaggle/blob/main/accuracy_graph_titanic.png)\n\n## About\n\nTitanic rescue prediction using Decision Tree, SVM, Logistic Regression, Random Forest and KNN. The best accuracy score was from Random Forest: 84.35%\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [numpy](https://github.com/topics/numpy) [sklearn](https://github.com/topics/sklearn) [pandas](https://github.com/topics/pandas) [seaborn](https://github.com/topics/seaborn) [titanic-kaggle](https://github.com/topics/titanic-kaggle) [logistic-regression](https://github.com/topics/logistic-regression) [matplotlib](https://github.com/topics/matplotlib) [decision-trees](https://github.com/topics/decision-trees) [kfold-cross-validation](https://github.com/topics/kfold-cross-validation) [machine-learning-projects](https://github.com/topics/machine-learning-projects) [knn-algorithm](https://github.com/topics/knn-algorithm) [support-vector-classification](https://github.com/topics/support-vector-classification) [performance-measurements](https://github.com/topics/performance-measurements)\n\n### Resources\n\n[Readme](https://github.com/alicevillar/titanic-kaggle#readme-ov-file)\n\n[Activity](https://github.com/alicevillar/titanic-kaggle/activity)\n\n### Stars\n\n[**6**\\\nstars](https://github.com/alicevillar/titanic-kaggle/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/alicevillar/titanic-kaggle/watchers)\n\n### Forks\n\n[**3**\\\nforks](https://github.com/alicevillar/titanic-kaggle/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Falicev...",
      "url": "https://github.com/alicevillar/titanic-kaggle"
    },
    {
      "title": "GitHub - engalaagabr/Titanic: This repository contains my work on the classic Kaggle Titanic Machine Learning competition. Using Python and scikit-learn, I built several machine learning models (Logistic Regression, SVC, Random Forest, and an ensemble Voting Classifier) to predict which passengers survived the Titanic disaster.",
      "text": "<div><div><article><p></p><h2>Titanic Survival Prediction</h2><a href=\"#titanic-survival-prediction\"></a><p></p>\n<p>This repository documents my end-to-end approach to the <strong>Kaggle Titanic: Machine Learning from Disaster</strong> challenge.<br/>\nThe goal is to build a model that predicts whether a passenger survived the Titanic sinking based on features like age, gender, class, and ticket information.</p>\n<p>I performed <strong>data exploration</strong>, <strong>feature engineering</strong>, and <strong>model building</strong> using Python\u2019s scientific stack.<br/>\nMultiple algorithms were compared and tuned, and the best model was submitted to Kaggle, achieving a public score of <strong>0.7799</strong>.</p>\n<hr/>\n<p></p><h2>What\u2019s Inside</h2><a href=\"#whats-inside\"></a><p></p>\n<ul>\n<li>\n<p><strong>Exploratory Data Analysis (EDA):</strong></p>\n<ul>\n<li>Visualizing distributions of Age, Fare, Passenger Class, Sex, Embarked, etc. and their relationships with survival.</li>\n</ul>\n</li>\n<li>\n<p><strong>Feature Engineering:</strong></p>\n<ul>\n<li>Extracting passenger titles (Mr, Mrs, Miss\u2026) from the Name field.</li>\n<li>Combining <code>SibSp</code> and <code>Parch</code> into family size features.</li>\n<li>Creating \u201cIsAlone\u201d flags, Fare and Age bins, and encoding categorical variables.</li>\n</ul>\n</li>\n<li>\n<p><strong>Modeling:</strong></p>\n<ul>\n<li>Logistic Regression</li>\n<li>Support Vector Classifier (SVC)</li>\n<li>Random Forest</li>\n<li>Voting Classifier ensemble</li>\n</ul>\n</li>\n<li>\n<p><strong>Evaluation &amp; Submission:</strong></p>\n<ul>\n<li>Models were evaluated with accuracy, confusion matrices, and classification reports.</li>\n<li>The final notebook generates a Kaggle-ready <code>submission.csv</code> file.</li>\n</ul>\n</li>\n</ul>\n<hr/>\n<p></p><h2>Results</h2><a href=\"#results\"></a><p></p>\n<ul>\n<li><strong>Best Kaggle Public Score:</strong> 0.7799</li>\n<li>Random Forest and ensemble models performed better than baseline logistic regression.</li>\n<li>Feature engineering and model comparison helped improve accuracy from ~0.76 to ~0.78+.</li>\n</ul>\n<hr/>\n<p></p><h2>Tech Stack</h2><a href=\"#tech-stack\"></a><p></p>\n<p>Python \u2022 pandas \u2022 NumPy \u2022 Matplotlib/Seaborn \u2022 scikit-learn</p>\n<hr/>\n<p></p><h2>How to Use</h2><a href=\"#how-to-use\"></a><p></p>\n<ol>\n<li>Clone the repo:\n<div><pre>git clone https://github.com/engalaagabr/Titanic.git\n<span>cd</span> Titanic</pre></div>\n</li>\n</ol>\n</article></div></div>",
      "url": "https://github.com/engalaagabr/Titanic"
    },
    {
      "title": "Titanic Survival Prediction: Comparing 5 Machine Learning Models",
      "text": "<div><div><h2>Introduction</h2><p>The Titanic is one of the most famous shipwrecks in history, and with the release of the movie Titanic in 1997, it has become even more famous. The Titanic dataset on Kaggle is a classic machine learning problem where the task is to predict which passengers survived the tragedy. In this article, we will be comparing the performance of 5 machine learning models: Neural Network, Logistic Regression, Random Forest, Support Vector Machine, and XGBoost on this dataset. The model with the highest F1 score on the test set will be declared as the best performing model.</p><h2>Methodology:</h2><p>First, we load the Titanic dataset and perform exploratory data analysis to understand the dataset. Then, we preprocess the dataset by encoding categorical variables and filling in missing values. We then split the data into a training set and a testing set, with a 80/20 split. Next, we normalize the data using MinMaxScaler.</p><p>We then proceed to build and fit our machine learning models. We build a neural network using Keras, a logistic regression model, a random forest model, a support vector machine model, and an XGBoost model. We then calculate the F1 score on the testing set for each of the 10 random states we used for splitting the data, and take the mean of the F1 scores across all 10 states.</p><p>Finally, we compare the performance of the 5 models by creating a dataframe of the mean F1 scores for each model and sorting the dataframe in descending order.</p><h2>Results:</h2><p>Our analysis shows that Support Vector Machine was the best performing model, with a mean F1 score of 80.09%. This was followed by Random Forest with a score of 79.62%, XGBoost with a score of 79.56%, Logistic Regression with a score of 79.34%, and finally Neural Network with a score of 77.39%.</p><h2>Conclusion:</h2><p>In this article, we compared the performance of 5 machine learning models on the Titanic dataset. Our analysis shows that Support Vector Machine was the best performing model for this dataset. It is worth noting that while we used F1 score as the metric for comparing the models, there are other metrics such as accuracy, precision, and recall that could have been used instead. Additionally, while we used only 5 models in this analysis, there are many other models that could have been used as well. Nonetheless, this analysis provides a good starting point for anyone interested in working with the Titanic dataset or similar machine learning problems.</p><h2>References:</h2><ul><li>The full code for this analysis can be found on Kaggle: <a href=\"https://www.kaggle.com/code/amyrmahdy/predicting-survival-on-the-titanic\">https://www.kaggle.com/code/amyrmahdy/predicting-survival-on-the-titanic</a></li><li>The Titanic dataset is available on Kaggle: <a href=\"https://www.kaggle.com/c/titanic\">https://www.kaggle.com/c/titanic</a></li><li>The full code for this analysis also can be found on GitHub: <a href=\"https://github.com/amyrmahdy/titanic\">https://github.com/amyrmahdy/titanic</a></li></ul></div></div>",
      "url": "https://amyrmahdy.medium.com/titanic-survival-prediction-comparing-5-machine-learning-models-d1d451a9e558"
    },
    {
      "title": "GitHub - Alpha6849/-Titanic-Survival-Prediction: End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.",
      "text": "<div><div><article><p></p><h2>-Titanic-Survival-Prediction</h2><a href=\"#-titanic-survival-prediction\"></a><p></p>\n<p>End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.</p>\n<p></p><h2>\ud83d\udea2 Titanic Survival Prediction - Random Forest Based ML Pipeline</h2><a href=\"#-titanic-survival-prediction---random-forest-based-ml-pipeline\"></a><p></p>\n<p>This is a complete end-to-end machine learning project built on the famous <strong>Titanic: Machine Learning from Disaster</strong> dataset on Kaggle. The goal is to predict passenger survival using classification models, with a focus on <strong>data preprocessing</strong>, <strong>feature engineering</strong>, and <strong>model tuning</strong>.</p>\n<hr/>\n<p></p><h2>\ud83d\udccc Project Description</h2><a href=\"#-project-description\"></a><p></p>\n<p>The Titanic dataset is a widely used beginner dataset for classification problems in machine learning. In this project, we went through all key stages of the ML pipeline:</p>\n<ul>\n<li>Exploratory Data Analysis</li>\n<li>Data Cleaning &amp; Imputation</li>\n<li>Feature Engineering</li>\n<li>Model Selection and Hyperparameter Tuning</li>\n<li>Final Prediction and Kaggle Submission</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udd0d Phases Breakdown</h2><a href=\"#-phases-breakdown\"></a><p></p>\n<p></p><h3>\u2705 Phase 1: Data Cleaning</h3><a href=\"#-phase-1-data-cleaning\"></a><p></p>\n<ul>\n<li>Dropped non-informative columns: <code>Name</code>, <code>Ticket</code>, <code>Cabin</code></li>\n<li>Filled missing values in:\n<ul>\n<li><code>Age</code>: with median grouped by <code>Pclass</code> and <code>Sex</code></li>\n<li><code>Embarked</code>: with mode</li>\n<li><code>Fare</code> in test set: with median</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 2: Feature Engineering</h3><a href=\"#-phase-2-feature-engineering\"></a><p></p>\n<ul>\n<li>Created <code>FamilySize</code> as <code>SibSp + Parch + 1</code></li>\n<li>Extracted <code>Title</code> from the <code>Name</code> column before dropping it</li>\n<li>Mapped categorical columns (<code>Sex</code>, <code>Embarked</code>, <code>Title</code>) using <strong>Label Encoding</strong></li>\n</ul>\n<p></p><h3>\u2705 Phase 3: Model Training &amp; Tuning</h3><a href=\"#-phase-3-model-training--tuning\"></a><p></p>\n<ul>\n<li>Tried baseline models including Logistic Regression and XGBoost</li>\n<li>Observed XGBoost underperformed (~0.60 accuracy)</li>\n<li>Final model: <strong>Random Forest Classifier</strong>\n<ul>\n<li>Tuned using <code>GridSearchCV</code></li>\n<li>Best parameters selected based on cross-validation</li>\n<li>Trained on full training set and predicted on test set</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 4: Final Submission</h3><a href=\"#-phase-4-final-submission\"></a><p></p>\n<ul>\n<li>Predictions were exported as <code>submission_final.csv</code></li>\n<li>Submitted to Kaggle for evaluation</li>\n</ul>\n<p></p><h2>\ud83c\udfc6 Kaggle Performance</h2><a href=\"#-kaggle-performance\"></a><p></p>\n<ul>\n<li><strong>Final Kaggle Score</strong>: <code>0.76555</code></li>\n<li><strong>Kaggle Rank</strong>: ~11,000 (Top 30\u201335% range at time of submission)</li>\n<li><strong>Final Model</strong>: Random Forest Classifier with GridSearchCV tuning</li>\n</ul>\n<p></p><h2>\ud83d\udcc1 Project Structure</h2><a href=\"#-project-structure\"></a><p></p>\n<p>\u251c\u2500\u2500 data/\n\u2502 \u251c\u2500\u2500 train.csv\n\u2502 \u2514\u2500\u2500 test.csv\n\u251c\u2500\u2500 titanic_model.ipynb &lt;- All model development in Jupyter\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission_final.csv\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt</p>\n<p></p><h2>\ud83c\udf93 What I Learned</h2><a href=\"#-what-i-learned\"></a><p></p>\n<ul>\n<li>How to clean and preprocess real-world datasets with missing values</li>\n<li>Feature extraction using domain knowledge (like <code>Title</code>, <code>FamilySize</code>)</li>\n<li>Label encoding vs OneHotEncoding choices</li>\n<li>Why some models (e.g. XGBoost) may not always outperform others</li>\n<li>Hyperparameter tuning using <code>GridSearchCV</code></li>\n<li>Submitting predictions on Kaggle and interpreting scores</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcbc Future Improvements</h2><a href=\"#-future-improvements\"></a><p></p>\n<ul>\n<li>Try ensemble techniques (Voting, Stacking)</li>\n<li>Add feature importance plots for interpretability</li>\n<li>Experiment with other preprocessing pipelines</li>\n</ul>\n<hr/>\n<blockquote>\n<p>Built as part of machine learning practice. Feel free to fork, reuse, or collaborate!</p>\n</blockquote>\n</article></div></div>",
      "url": "https://github.com/Alpha6849/-Titanic-Survival-Prediction"
    },
    {
      "title": "Kaggle Titanic: Machine Learning model (top 7%) - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa4523b7c40&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Kaggle Titanic: Machine Learning model (top 7%)\n\n[![Sanjay.M](https://miro.medium.com/v2/resize:fill:88:88/1*Sn8G7odPX60PwQo--Ggl2Q@2x.jpeg)](https://msanjay-ds.medium.com/?source=post_page-----fa4523b7c40--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n[Sanjay.M](https://msanjay-ds.medium.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa85692c31f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&user=Sanjay.M&userId=a85692c31f3f&source=post_page-a85692c31f3f----fa4523b7c40---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n\u00b7\n\n6 min read\n\n\u00b7\n\nNov 5, 2018\n\n--\n\n2\n\nListen\n\nShare\n\nThis K [aggle competition](https://www.kaggle.com/c/titanic) is all about predicting the survival or the death of a given passenger based on the features given.This machine learning model is built using scikit-learn and fastai libraries (thanks to [Jeremy howard](https://www.linkedin.com/authwall?trk=gf&trkInfo=AQH1NsDUgQbJAwAAAWbiRqbIwGLhcSbGKZ7zOi_usSDEtKTqOv4iVuPDGE3g5jP79Eg3F9l5aIlaLaKsUjjhCllbKY2z1XAvTFQ9UMKh9LR9PJcCiaVsqANQ3ttHlLm60UCyUMU%3D&originalReferer=https%3A%2F%2Fwww.google.co.in%2F&sessionRedirect=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fhowardjeremy) and [Rachel Thomas](https://www.linkedin.com/in/rachel-thomas-942a7923)). Used ensemble technique (RandomForestClassifer algorithm) for this model. I have tried other algorithms like Logistic Regression, GradientBoosting Classifier with different hyper-parameters. But I got the better results using this RandomFortestClassifer (Top 7%).\n\nGithub link for the complete code is [here](https://github.com/Msanjayds/Kaggle_Titanic-Survival-Challenge/blob/master/Titanic_Final_to_git.ipynb).\n\nBelow are the features provided in the Test dataset.\n\n- Passenger Id: and id given to each traveler on the boat\n- Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n- The Name of the passenger\n- Sex\n- Age\n- SibSp: number of siblings and spouses traveling with the passenger\n- Parch: number of parents and children traveling with the passenger\n- The ticket number\n- The ticket Fare\n- The cabin number\n- The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q\n\n**Exploratory Data Analysis**: Below are my findings during the data analysis and the methods I used to handle them.\n\nFrom the below table we can see that out of 891 observations in the test dataset only 714 records have the Age populated .i.e around 177 values are missing. We need to impute this with some values, which we can see later.\n\nNumerical feature statistics \u2014 we can see the number of missing/non-missing\n\nChart below says that more male passengers are died compared to females (Gender discrimination :-)))\n\nVisualization of Survival based on the Gender\n\nAnd also we can see men who are between 20 to 40 are survived more compared to older aged men as depicted by the green histogram. Women on the other hand survived more than men, comparatively well on all the age groups.\n\nCorrelation of Age with the Survival\n\nWhen we plot the ticket fare of passengers who are survived/dead, we can see that the passengers with cheaper ticket fares are more likely to die. That is passengers with expensive tickets (could be more important social status) are seem to be rescued on priority. X-axis : Fare, Y-axis: No of Passengers.\n\nTicket Fare vs Survival rate\n\nBelow is a single chart which shows the age and fare correlation with survival. X-axis = Age ,Y-axis = Ticket\\_Fare ,Green dots = Survived, Red dots= Died\n\nSmall green dots between x=0 & x=10 : Children who were survived\n\nSmall red dots between x=10 & x=45: Adults who died (from a lower classes)\n\nLarge green dots between x=20 & x=45 : Adults with larger ticket fares who are survived.\n\n> **Now Comes the Feature Engineering:**\n\nI have combined the train and test data to apply the transformations on both. Once this is done I separated the test and train data, train the model with the test data, validate this with the validation set (small subset of training data), Evaluate and tune the parameters. And finally train the model on complete train data. Then do the predictions on test data and submit to Kaggle.\n\n**Process Family:** Created some new features(FamilySize, Singleton, Smallfamily, LargeFamily) based on the size of the family. This is under the assumption that large families are usually grouped and support together, hence they are more likely to get rescued than those were travelling alone.\n\n**Process Embarked**: Filled the missing embarked with the most frequent one in the train set which is \u2018S\u2019 and did a one-hot encoding on the emarked column using the get\\_dummies method.\n\n**Process Cabin:** Replaced the missing cabins with U (for unknown) and took the first letter of the cabin and did a dummy encoding using get\\_dummies method.\n\n**Extract the passenger Title from Name**: Created a new feature \u2018Title\u2019 by parsing the name and mapping the titles to the categories defined.\n\n**Process Age**: As we have seen earlier Age variable has 177 missing values, which is a huge number out of 891. Just by replacing with the mean/median age might not be the best solution, since the age may differ by group and categories of passengers. Title also can contribute in computing the age. First I took median age grouped by Sex, PassengerClass and Title.\n\nThen for all the records with missing age, based on their Sex,Title and Pclass we assign the age. If at all \u2018Title\u2019 is missing assign the age based on just Sex and Pclass.\n\n**Process Name**: Drop the Name column and do a dummy variable encoding on the Title column. Data will look as below after the encoding of Title column values.\n\n**Build and Train the Model:** As I mentioned earlier I did split the train set into Train and validate set (60 for validation) and used RandomForestClassifier. Initially i started with number of trees(n\\_estimators) as 20 and finally ended up at 180 and used minimum numbers of samples required to split a node as 3(min\\_...",
      "url": "https://towardsdatascience.com/kaggle-titanic-machine-learning-model-top-7-fa4523b7c40?gi=72cb27a5f221"
    }
  ]
}