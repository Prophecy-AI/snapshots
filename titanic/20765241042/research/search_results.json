{
  "query": "How do top Kaggle Titanic solutions use ticket grouping and family survival features to improve accuracy?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle Titanic notebooks improve predictive power by turning the raw **Name** and **Ticket** columns into group\u2011level features that capture family and travel\u2011party survival trends\u202f([amypeniston](https://amypeniston.com/titanic-a-simple-gender-family-group-model)):\n\n1. **Extract a family identifier** \u2013 Parse the passenger\u2019s last name (the string before the first comma in the *Name* field) and combine it with the ticket number to form a unique \u201cfamily\u2011ticket\u201d ID. Passengers sharing this ID are assumed to be traveling together.  \n\n2. **Compute family size** \u2013 Create `FamilySize = SibSp + Parch + 1`. This numeric feature quantifies how many relatives travel with a passenger and is a strong survival predictor\u202f([Alpha6849 GitHub](https://github.com/Alpha6849/-Titanic-Survival-Prediction)).  \n\n3. **Derive family\u2011survival statistics** \u2013 Using the training set, calculate the mean survival rate for each family\u2011ticket ID (e.g., `FamilySurvivalRate = mean(Survived) per ID`). Merge this rate back into both train and test rows as a new feature; unseen families receive the overall mean or a neutral placeholder.  \n\n4. **Add ticket\u2011group frequency** \u2013 Count how many passengers share the same ticket number (`TicketCount`). Larger ticket groups often correspond to families or cabins and help the model differentiate isolated travelers from groups\u202f([Alpha6849 GitHub](https://github.com/Alpha6849/-Titanic-Survival-Prediction)).  \n\n5. **Encode the group features** \u2013 Treat `FamilySurvivalRate` and `TicketCount` as numeric inputs (or bin them) and feed them to tree\u2011based models (Random Forest, Gradient Boosting, etc.). These engineered columns consistently lift validation accuracy above baseline gender\u2011only models.  \n\nBy grouping passengers through last\u2011name\u2011ticket IDs and feeding the resulting family size, ticket frequency, and family\u2011level survival rates into the model, top solutions capture hidden social\u2011structure signals that raw fields miss, leading to the higher leaderboard scores seen in recent Kaggle submissions.",
      "url": ""
    },
    {
      "title": "Titanic \u2013 A Simple Gender + Family Group Model",
      "text": "Cheers to everyone who supported my [Notion icon pack](https://www.producthunt.com/posts/minimal-notion-icons) Product Hunt launch. \ud83d\ude80 We made it to #5!\n- [Home](https://amypeniston.com/)\n- [Blog](https://amypeniston.com/blog/)\n- [Store](https://store.amypeniston.com/)\n- [Dotnotes](https://amypeniston.com/dotnotes/)\n- [Portfolio](https://amypeniston.com/portfolio/)\nProject\n# Titanic \u2013 A Simple Gender + Family Group Model\n**Published:** January 12, 2020 **Tags:** [classification](https://amypeniston.com/tag/classification/)\u2022[kaggle](https://amypeniston.com/tag/kaggle/)\u2022[machine learning](https://amypeniston.com/tag/machine-learning/)\nA few weeks ago, I started working with the Titanic dataset. Initially, I performed a thorough round of exploratory data analysis ( [view notebook](https://www.kaggle.com/amypeniston/titanic-exploratory-data-analysis)) and I thought I had a handle on what needed to be done.\nWell, cue the part where I repeatedly run up against a wall using traditional machine learning approaches. Umpteen rounds of random forest hyperparameter tuning, cobbling together different models, creative feature engineering, categorical variable binning\u2026\nAfter trying (and failing) to score higher than ~78% accuracy using numerous variations of stacked ensembles, I decided to take a step back. Increasing complexity was obviously not increasing model accuracy. Perhaps if I better understood the problem and increased my domain knowledge, I would have a better chance of moving up the leaderboard.\nI stumbled across an interesting notebook by\u00a0[Chris Deotte](https://www.kaggle.com/cdeotte)\u00a0in which he creates a relatively high-scoring model using the Name feature alone:\u00a0[Titanic using Name only](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818). Although parsing his R code was a bit of a challenge, Chris\u2019 commentary was extremely clear and logical.\nThe basic premise is that we can infer family groups based on passenger last name, which is extracted from the Name field. Survival within these so-called \u201cwoman-child-groups\u201d is almost always binary \u2013 either all members of the WCG die or all survive. Therefore, if a test case is presented for which we can determine family group status, we should predict survival based on the survival of the family group.\n![](data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20444%20132%22%3E%3C/svg%3E)A family group-based model for predicting passenger survival aboard the Titanic.\nInterestingly, I originally constructed FamilyGroup using passenger last name. However there were a handful of occasions when this resulted in individuals being left out of groups because they had a different last name. For this reason, I selected a different grouping method using Ticket, Embarked and Fare.\nAt the end of the day, using this group-based strategy in conjunction with the baseline gender-based model in which all men die and all females survive, resulted in an accuracy of 81.3%. Quite an improvement over my overly complex ensembles!\nAll credit to Chris for the great idea. I really enjoyed exploring his hypothesis and implementing a similar model in Python.\n[View Notebook](https://www.kaggle.com/amypeniston/titanic-simple-gender-family-group-model)\nCare to share?\n* * *\n[Back to Blog](https://amypeniston.com/blog)",
      "url": "https://amypeniston.com/titanic-a-simple-gender-family-group-model"
    },
    {
      "title": "GitHub - Alpha6849/-Titanic-Survival-Prediction: End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.",
      "text": "<div><div><article><p></p><h2>-Titanic-Survival-Prediction</h2><a href=\"#-titanic-survival-prediction\"></a><p></p>\n<p>End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.</p>\n<p></p><h2>\ud83d\udea2 Titanic Survival Prediction - Random Forest Based ML Pipeline</h2><a href=\"#-titanic-survival-prediction---random-forest-based-ml-pipeline\"></a><p></p>\n<p>This is a complete end-to-end machine learning project built on the famous <strong>Titanic: Machine Learning from Disaster</strong> dataset on Kaggle. The goal is to predict passenger survival using classification models, with a focus on <strong>data preprocessing</strong>, <strong>feature engineering</strong>, and <strong>model tuning</strong>.</p>\n<hr/>\n<p></p><h2>\ud83d\udccc Project Description</h2><a href=\"#-project-description\"></a><p></p>\n<p>The Titanic dataset is a widely used beginner dataset for classification problems in machine learning. In this project, we went through all key stages of the ML pipeline:</p>\n<ul>\n<li>Exploratory Data Analysis</li>\n<li>Data Cleaning &amp; Imputation</li>\n<li>Feature Engineering</li>\n<li>Model Selection and Hyperparameter Tuning</li>\n<li>Final Prediction and Kaggle Submission</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udd0d Phases Breakdown</h2><a href=\"#-phases-breakdown\"></a><p></p>\n<p></p><h3>\u2705 Phase 1: Data Cleaning</h3><a href=\"#-phase-1-data-cleaning\"></a><p></p>\n<ul>\n<li>Dropped non-informative columns: <code>Name</code>, <code>Ticket</code>, <code>Cabin</code></li>\n<li>Filled missing values in:\n<ul>\n<li><code>Age</code>: with median grouped by <code>Pclass</code> and <code>Sex</code></li>\n<li><code>Embarked</code>: with mode</li>\n<li><code>Fare</code> in test set: with median</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 2: Feature Engineering</h3><a href=\"#-phase-2-feature-engineering\"></a><p></p>\n<ul>\n<li>Created <code>FamilySize</code> as <code>SibSp + Parch + 1</code></li>\n<li>Extracted <code>Title</code> from the <code>Name</code> column before dropping it</li>\n<li>Mapped categorical columns (<code>Sex</code>, <code>Embarked</code>, <code>Title</code>) using <strong>Label Encoding</strong></li>\n</ul>\n<p></p><h3>\u2705 Phase 3: Model Training &amp; Tuning</h3><a href=\"#-phase-3-model-training--tuning\"></a><p></p>\n<ul>\n<li>Tried baseline models including Logistic Regression and XGBoost</li>\n<li>Observed XGBoost underperformed (~0.60 accuracy)</li>\n<li>Final model: <strong>Random Forest Classifier</strong>\n<ul>\n<li>Tuned using <code>GridSearchCV</code></li>\n<li>Best parameters selected based on cross-validation</li>\n<li>Trained on full training set and predicted on test set</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 4: Final Submission</h3><a href=\"#-phase-4-final-submission\"></a><p></p>\n<ul>\n<li>Predictions were exported as <code>submission_final.csv</code></li>\n<li>Submitted to Kaggle for evaluation</li>\n</ul>\n<p></p><h2>\ud83c\udfc6 Kaggle Performance</h2><a href=\"#-kaggle-performance\"></a><p></p>\n<ul>\n<li><strong>Final Kaggle Score</strong>: <code>0.76555</code></li>\n<li><strong>Kaggle Rank</strong>: ~11,000 (Top 30\u201335% range at time of submission)</li>\n<li><strong>Final Model</strong>: Random Forest Classifier with GridSearchCV tuning</li>\n</ul>\n<p></p><h2>\ud83d\udcc1 Project Structure</h2><a href=\"#-project-structure\"></a><p></p>\n<p>\u251c\u2500\u2500 data/\n\u2502 \u251c\u2500\u2500 train.csv\n\u2502 \u2514\u2500\u2500 test.csv\n\u251c\u2500\u2500 titanic_model.ipynb &lt;- All model development in Jupyter\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission_final.csv\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt</p>\n<p></p><h2>\ud83c\udf93 What I Learned</h2><a href=\"#-what-i-learned\"></a><p></p>\n<ul>\n<li>How to clean and preprocess real-world datasets with missing values</li>\n<li>Feature extraction using domain knowledge (like <code>Title</code>, <code>FamilySize</code>)</li>\n<li>Label encoding vs OneHotEncoding choices</li>\n<li>Why some models (e.g. XGBoost) may not always outperform others</li>\n<li>Hyperparameter tuning using <code>GridSearchCV</code></li>\n<li>Submitting predictions on Kaggle and interpreting scores</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcbc Future Improvements</h2><a href=\"#-future-improvements\"></a><p></p>\n<ul>\n<li>Try ensemble techniques (Voting, Stacking)</li>\n<li>Add feature importance plots for interpretability</li>\n<li>Experiment with other preprocessing pipelines</li>\n</ul>\n<hr/>\n<blockquote>\n<p>Built as part of machine learning practice. Feel free to fork, reuse, or collaborate!</p>\n</blockquote>\n</article></div></div>",
      "url": "https://github.com/Alpha6849/-Titanic-Survival-Prediction"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Titanic Solution(Comprehensive with explanation)",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=542a18f1e5d17c4a4b7d:1:11101)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/colearninglounge/titanic-solution-comprehensive-with-explanation"
    },
    {
      "title": "Kaggle Titanic: Machine Learning model (top 7%)",
      "text": "<div><div><div><a href=\"https://msanjay-ds.medium.com/@msanjay-ds?source=post_page---byline--fa4523b7c40---------------------------------------\"><div><p></p></div></a></div><figure></figure><p>This K<a href=\"https://www.kaggle.com/c/titanic\">aggle competition</a> is all about predicting the survival or the death of a given passenger based on the features given.This machine learning model is built using scikit-learn and fastai libraries (thanks to <a href=\"https://www.linkedin.com/authwall?trk=gf&amp;trkInfo=AQH1NsDUgQbJAwAAAWbiRqbIwGLhcSbGKZ7zOi_usSDEtKTqOv4iVuPDGE3g5jP79Eg3F9l5aIlaLaKsUjjhCllbKY2z1XAvTFQ9UMKh9LR9PJcCiaVsqANQ3ttHlLm60UCyUMU%3D&amp;originalReferer=https%3A%2F%2Fwww.google.co.in%2F&amp;sessionRedirect=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fhowardjeremy\">Jeremy howard</a> and <a href=\"https://www.linkedin.com/in/rachel-thomas-942a7923\">Rachel Thomas</a>). Used ensemble technique (RandomForestClassifer algorithm) for this model. I have tried other algorithms like Logistic Regression, GradientBoosting Classifier with different hyper-parameters. But I got the better results using this RandomFortestClassifer (Top 7%).</p><p>Github link for the complete code is <a href=\"https://github.com/Msanjayds/Kaggle_Titanic-Survival-Challenge/blob/master/Titanic_Final_to_git.ipynb\">here</a>.</p><p>Below are the features provided in the Test dataset.</p><ul><li>Passenger Id: and id given to each traveler on the boat</li><li>Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)</li><li>The Name of the passenger</li><li>Sex</li><li>Age</li><li>SibSp: number of siblings and spouses traveling with the passenger</li><li>Parch: number of parents and children traveling with the passenger</li><li>The ticket number</li><li>The ticket Fare</li><li>The cabin number</li><li>The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q</li></ul><p><strong>Exploratory Data Analysis</strong>: Below are my findings during the data analysis and the methods I used to handle them.</p><p>From the below table we can see that out of 891 observations in the test dataset only 714 records have the Age populated .i.e around 177 values are missing. We need to impute this with some values, which we can see later.</p><figure><figcaption>Numerical feature statistics \u2014 we can see the number of missing/non-missing</figcaption></figure><p>Chart below says that more male passengers are died compared to females (Gender discrimination :-)))</p><figure><figcaption>Visualization of Survival based on the Gender</figcaption></figure><p>And also we can see men who are between 20 to 40 are survived more compared to older aged men as depicted by the green histogram. Women on the other hand survived more than men, comparatively well on all the age groups.</p><figure><figcaption>Correlation of Age with the Survival</figcaption></figure><p>When we plot the ticket fare of passengers who are survived/dead, we can see that the passengers with cheaper ticket fares are more likely to die. That is passengers with expensive tickets (could be more important social status) are seem to be rescued on priority. X-axis : Fare, Y-axis: No of Passengers.</p><figure><figcaption>Ticket Fare vs Survival rate</figcaption></figure><p>Below is a single chart which shows the age and fare correlation with survival. X-axis = Age ,Y-axis = Ticket_Fare ,Green dots = Survived, Red dots= Died</p><p>Small green dots between x=0 &amp; x=10 : Children who were survived</p><p>Small red dots between x=10 &amp; x=45: Adults who died (from a lower classes)</p><p>Large green dots between x=20 &amp; x=45 : Adults with larger ticket fares who are survived.</p><figure></figure><blockquote><p><strong>Now Comes the Feature Engineering:</strong></p></blockquote><p>I have combined the train and test data to apply the transformations on both. Once this is done I separated the test and train data, train the model with the test data, validate this with the validation set (small subset of training data), Evaluate and tune the parameters. And finally train the model on complete train data. Then do the predictions on test data and submit to Kaggle.</p><p><strong>Process Family:</strong> Created some new features(FamilySize, Singleton, Smallfamily, LargeFamily) based on the size of the family. This is under the assumption that large families are usually grouped and support together, hence they are more likely to get rescued than those were travelling alone.</p><figure></figure><p><strong>Process Embarked</strong>: Filled the missing embarked with the most frequent one in the train set which is \u2018S\u2019 and did a one-hot encoding on the emarked column using the get_dummies method.</p><figure></figure><p><strong>Process Cabin: </strong>Replaced the missing cabins with U (for unknown) and took the first letter of the cabin and did a dummy encoding using get_dummies method.</p><figure></figure><p><strong>Extract the passenger Title from Name</strong>: Created a new feature \u2018Title\u2019 by parsing the name and mapping the titles to the categories defined.</p><figure></figure><p><strong>Process Age</strong>: As we have seen earlier Age variable has 177 missing values, which is a huge number out of 891. Just by replacing with the mean/median age might not be the best solution, since the age may differ by group and categories of passengers. Title also can contribute in computing the age. First I took median age grouped by Sex, PassengerClass and Title.</p><figure></figure><p>Then for all the records with missing age, based on their Sex,Title and Pclass we assign the age. If at all \u2018Title\u2019 is missing assign the age based on just Sex and Pclass.</p><figure></figure><p><strong>Process Name</strong>: Drop the Name column and do a dummy variable encoding on the Title column. Data will look as below after the encoding of Title column values.</p><figure></figure><p><strong>Build and Train the Model: </strong>As I mentioned earlier I did split the train set into Train and validate set (60 for validation) and used RandomForestClassifier. Initially i started with number of trees(n_estimators) as 20 and finally ended up at 180 and used minimum numbers of samples required to split a node as 3(min_samples_leaf). And also used max_features as 0.5 ( randomly consider 50% of the features when looking for the best split). With theses parameters this model achieved a score of <strong>0.933</strong> on the test data.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p><strong>Model Evaluation</strong>: Achieved around 86.67 on the validate set and below is the confusion matrix and classification report.</p><figure></figure><p><strong>Feature Importance check</strong>: Tree-based estimators can be used to compute feature importance, after which we can discard the irrelevant features. Below are the list of top ten features with high importance.</p><figure></figure><p>As we can from the table above Title, Sex, Fare, PassengerId , Age, Pclass &amp; Family size are more important features. Not sure how passenger Id is contributing to the prediction. Below is the graphical representation of the same.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p>Finally to train the model again with one last time on entire training data I have only included the features whose importance is more than 0.01. And after training i could see a slight improvement in the score, this time it is <strong>0.938</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p>Then I ran the model on the test data, extracted the predictions and submitted to the Kaggle. It achieved a score of <strong>0.8133 </strong>which is at<strong> top 7%.</strong></p><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p>Certainly this mo...",
      "url": "https://msanjay-ds.medium.com/kaggle-titanic-machine-learning-model-top-7-fa4523b7c40"
    },
    {
      "title": "Kaggle Titanic Challenge: Features Creation",
      "text": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English\n[Sitemap](https://python.plainenglish.io/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Python in Plain English\n](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\u00b7Follow publication\n[\n![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)\n](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\nFollow publication\n# Kaggle Titanic Challenge: Features Creation\n## Better Data, Better Model!\n[\n![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)\n](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n5 min read\n\u00b7Apr 7, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/python-in-plain-english/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;user=Aman+Krishna&amp;userId=bda5c8d97e1a&amp;source=---header_actions--a324ba577812---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=---header_actions--a324ba577812---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)## Overview\nHey folks,\nIn the previous section of[**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),****we looked at the distribution of different features. And their relationship with the prediction label,**Survived**, as well as among each other.\nWe noticed that some of the features like**Name**contain extra information about a passenger\u2019s**title**which can be useful for our model.\n**In this section,**we will extract useful details from different features and create new features. Let\u2019s get started!\nYou can find the complete notebook here:\n[\n## kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7AmanKrishna/kaggle\\_titanic\n### Machine Learning model build for survivor prediction for Kaggle&#x27;s titanic competition \u2026github.com\n](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\nFirst, let\u2019s import and combine our train &amp; test data to avoid redoing the same steps for both separately.\n### Code\n## **New Feature:**Family Name\nThe**Name**feature looks something like this:\nPress enter or click to view image in full size\n![]()\nFamily Names\n**Family Name**followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n### Why Family Name?\n*Hypothesis:*If some members of a family survive then chances are others will too.\n### Code\n## New Feature: Title\nEach passenger has a title in our database (**Mr./Mrs./Master**etc.). We can extract them and add them as a new feature**Title**.\nPress enter or click to view image in full size\n![]()\nTitle associated with Name\nLet\u2019s group some of these Titles together using the below dictionary:\nTitle Dictionary\nHere is the final list of titles we will be using:\n![]()\nList of Titles### Why Title?\nLooking at the below graph we can see that survival probability changes with Title\nPress enter or click to view image in full size\n![]()\nTitle vs Survival### **Code**\n## New Feature: Cabin Section\nIf we look at the**Cabin**feature we can see that each cabin starts with an alphabet. These could be**Cabin sections**similar to the ones we have on trains or flights.\nPress enter or click to view image in full size\n![]()\nCabin Starts with Alphabet\nAfter extracting and combining a few we get the following set of**Cabin Sections**\n![]()\nSet of Cabins### Code\n## New Feature: Family Size\nIn[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we discovered that**family size**played a pivotal role in determining survival.\nPress enter or click to view image in full size\n![]()\nFamily Size vs Survival Rate\nLet\u2019s create a new feature**Family\\_Size**by summing**Sibsp**&amp;**Parch**features.\n### Code\n## New Feature: Family Size Grouped\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n### Code\n### Why grouping?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Binning family sizes into groups will help our model learn better.\n## New Feature: Grouping Ticket\nDuring[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we noticed that the number of Unique**Tickets**was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of**passengers per ticket**we get the following survival rates:\nPress enter or click to view image in full size\n![]()\nTicket\\_Group\\_Size vs Survival### Why group tickets?\n*Hypothesis:*The same ticket will be shared among the family group as well as the friends group.\n### Code\n## Age\\_Bin &amp;&amp; Fare\\_Bin\nWe have observed during[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)that Survival probability varied across**Age**and**Fare**segments. Let\u2019s bin them into separate features.\nPress enter or click to view image in full size![]()\nPress enter or click to view image in full size![]()\nAge/Fare Bins vs Survival Rates\n### Why Binning?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n## GetAman Krishna\u2019s stories in\u00a0your\u00a0inbox\nJoin Medium for free to get updates from\u00a0this\u00a0writer.\nSubscribe\nSubscribe\n**Note:**The**age**feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n### Code\n## New Feature: Survival Rates\nThis feature will contain the survival probability for each**Ticket**,**Family Name**&amp;**Cabin number**.**We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_R...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812"
    },
    {
      "title": "Kaggle Titanic: Machine Learning model (top 7%) - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa4523b7c40&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Kaggle Titanic: Machine Learning model (top 7%)\n\n[![Sanjay.M](https://miro.medium.com/v2/resize:fill:88:88/1*Sn8G7odPX60PwQo--Ggl2Q@2x.jpeg)](https://msanjay-ds.medium.com/?source=post_page-----fa4523b7c40--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n[Sanjay.M](https://msanjay-ds.medium.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa85692c31f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-titanic-machine-learning-model-top-7-fa4523b7c40&user=Sanjay.M&userId=a85692c31f3f&source=post_page-a85692c31f3f----fa4523b7c40---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa4523b7c40--------------------------------)\n\n\u00b7\n\n6 min read\n\n\u00b7\n\nNov 5, 2018\n\n--\n\n2\n\nListen\n\nShare\n\nThis K [aggle competition](https://www.kaggle.com/c/titanic) is all about predicting the survival or the death of a given passenger based on the features given.This machine learning model is built using scikit-learn and fastai libraries (thanks to [Jeremy howard](https://www.linkedin.com/authwall?trk=gf&trkInfo=AQH1NsDUgQbJAwAAAWbiRqbIwGLhcSbGKZ7zOi_usSDEtKTqOv4iVuPDGE3g5jP79Eg3F9l5aIlaLaKsUjjhCllbKY2z1XAvTFQ9UMKh9LR9PJcCiaVsqANQ3ttHlLm60UCyUMU%3D&originalReferer=https%3A%2F%2Fwww.google.co.in%2F&sessionRedirect=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fhowardjeremy) and [Rachel Thomas](https://www.linkedin.com/in/rachel-thomas-942a7923)). Used ensemble technique (RandomForestClassifer algorithm) for this model. I have tried other algorithms like Logistic Regression, GradientBoosting Classifier with different hyper-parameters. But I got the better results using this RandomFortestClassifer (Top 7%).\n\nGithub link for the complete code is [here](https://github.com/Msanjayds/Kaggle_Titanic-Survival-Challenge/blob/master/Titanic_Final_to_git.ipynb).\n\nBelow are the features provided in the Test dataset.\n\n- Passenger Id: and id given to each traveler on the boat\n- Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n- The Name of the passenger\n- Sex\n- Age\n- SibSp: number of siblings and spouses traveling with the passenger\n- Parch: number of parents and children traveling with the passenger\n- The ticket number\n- The ticket Fare\n- The cabin number\n- The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q\n\n**Exploratory Data Analysis**: Below are my findings during the data analysis and the methods I used to handle them.\n\nFrom the below table we can see that out of 891 observations in the test dataset only 714 records have the Age populated .i.e around 177 values are missing. We need to impute this with some values, which we can see later.\n\nNumerical feature statistics \u2014 we can see the number of missing/non-missing\n\nChart below says that more male passengers are died compared to females (Gender discrimination :-)))\n\nVisualization of Survival based on the Gender\n\nAnd also we can see men who are between 20 to 40 are survived more compared to older aged men as depicted by the green histogram. Women on the other hand survived more than men, comparatively well on all the age groups.\n\nCorrelation of Age with the Survival\n\nWhen we plot the ticket fare of passengers who are survived/dead, we can see that the passengers with cheaper ticket fares are more likely to die. That is passengers with expensive tickets (could be more important social status) are seem to be rescued on priority. X-axis : Fare, Y-axis: No of Passengers.\n\nTicket Fare vs Survival rate\n\nBelow is a single chart which shows the age and fare correlation with survival. X-axis = Age ,Y-axis = Ticket\\_Fare ,Green dots = Survived, Red dots= Died\n\nSmall green dots between x=0 & x=10 : Children who were survived\n\nSmall red dots between x=10 & x=45: Adults who died (from a lower classes)\n\nLarge green dots between x=20 & x=45 : Adults with larger ticket fares who are survived.\n\n> **Now Comes the Feature Engineering:**\n\nI have combined the train and test data to apply the transformations on both. Once this is done I separated the test and train data, train the model with the test data, validate this with the validation set (small subset of training data), Evaluate and tune the parameters. And finally train the model on complete train data. Then do the predictions on test data and submit to Kaggle.\n\n**Process Family:** Created some new features(FamilySize, Singleton, Smallfamily, LargeFamily) based on the size of the family. This is under the assumption that large families are usually grouped and support together, hence they are more likely to get rescued than those were travelling alone.\n\n**Process Embarked**: Filled the missing embarked with the most frequent one in the train set which is \u2018S\u2019 and did a one-hot encoding on the emarked column using the get\\_dummies method.\n\n**Process Cabin:** Replaced the missing cabins with U (for unknown) and took the first letter of the cabin and did a dummy encoding using get\\_dummies method.\n\n**Extract the passenger Title from Name**: Created a new feature \u2018Title\u2019 by parsing the name and mapping the titles to the categories defined.\n\n**Process Age**: As we have seen earlier Age variable has 177 missing values, which is a huge number out of 891. Just by replacing with the mean/median age might not be the best solution, since the age may differ by group and categories of passengers. Title also can contribute in computing the age. First I took median age grouped by Sex, PassengerClass and Title.\n\nThen for all the records with missing age, based on their Sex,Title and Pclass we assign the age. If at all \u2018Title\u2019 is missing assign the age based on just Sex and Pclass.\n\n**Process Name**: Drop the Name column and do a dummy variable encoding on the Title column. Data will look as below after the encoding of Title column values.\n\n**Build and Train the Model:** As I mentioned earlier I did split the train set into Train and validate set (60 for validation) and used RandomForestClassifier. Initially i started with number of trees(n\\_estimators) as 20 and finally ended up at 180 and used minimum numbers of samples required to split a node as 3(min\\_...",
      "url": "https://towardsdatascience.com/kaggle-titanic-machine-learning-model-top-7-fa4523b7c40?gi=72cb27a5f221"
    },
    {
      "title": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa324ba577812&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Python in Plain English**](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\n\u00b7\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---publication_nav-78073def27b8-a324ba577812---------------------publication_nav------------------)\n\n[![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\n\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------post_publication_sidebar------------------)\n\n# Kaggle Titanic Challenge: Features Creation\n\n## Better Data, Better Model!\n\n[![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\nFollow\n\n5 min read\n\n\u00b7\n\nApr 7, 2022\n\n--\n\nListen\n\nShare\n\nPhoto by [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n# Overview\n\nHey folks,\n\nIn the previous section of [**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),we looked at the distribution of different features. And their relationship with the prediction label, **Survived**, as well as among each other.\n\nWe noticed that some of the features like **Name** contain extra information about a passenger\u2019s **title** which can be useful for our model.\n\n**In this section,** we will extract useful details from different features and create new features. Let\u2019s get started!\n\nYou can find the complete notebook here:\n\n[**kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7 AmanKrishna/kaggle\\_titanic** \\\n\\\n**Machine Learning model build for survivor prediction for Kaggle's titanic competition \u2026**\\\n\\\ngithub.com](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\n\nFirst, let\u2019s import and combine our train & test data to avoid redoing the same steps for both separately.\n\n## Code\n\n# **New Feature:** Family Name\n\nThe **Name** feature looks something like this:\n\nFamily Names\n\n**Family Name** followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n\n## Why Family Name?\n\n_Hypothesis:_ If some members of a family survive then chances are others will too.\n\n## Code\n\n# New Feature: Title\n\nEach passenger has a title in our database ( **Mr./Mrs./Master** etc.). We can extract them and add them as a new feature **Title**.\n\nTitle associated with Name\n\nLet\u2019s group some of these Titles together using the below dictionary:\n\nTitle Dictionary\n\nHere is the final list of titles we will be using:\n\nList of Titles\n\n## Why Title?\n\nLooking at the below graph we can see that survival probability changes with Title\n\nTitle vs Survival\n\n## **Code**\n\n# New Feature: Cabin Section\n\nIf we look at the **Cabin** feature we can see that each cabin starts with an alphabet. These could be **Cabin sections** similar to the ones we have on trains or flights.\n\nCabin Starts with Alphabet\n\nAfter extracting and combining a few we get the following set of **Cabin Sections**\n\nSet of Cabins\n\n## Code\n\n# New Feature: Family Size\n\nIn [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we discovered that **family size** played a pivotal role in determining survival.\n\nFamily Size vs Survival Rate\n\nLet\u2019s create a new feature **Family\\_Size** by summing **Sibsp**& **Parch** features.\n\n## Code\n\n# New Feature: Family Size Grouped\n\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n\n## Code\n\n## Why grouping?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Binning family sizes into groups will help our model learn better.\n\n# New Feature: Grouping Ticket\n\nDuring [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we noticed that the number of Unique **Tickets** was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of **passengers per ticket** we get the following survival rates:\n\nTicket\\_Group\\_Size vs Survival\n\n## Why group tickets?\n\n_Hypothesis:_ The same ticket will be shared among the family group as well as the friends group.\n\n## Code\n\n# Age\\_Bin & Fare\\_Bin\n\nWe have observed during [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) that Survival probability varied across **Age** and **Fare** segments. Let\u2019s bin them into separate features.\n\nAge/Fare Bins vs Survival Rates\n\n## Why Binning?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n\n**Note:** The **age** feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n\n## Code\n\n# New Feature: Survival Rates\n\nThis feature will contain the survival probability for each **Ticket**, **Family Name** & **Cabin number**. **We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_Rate & Name\\_Survival\\_Rate**\n\nFor all the passengers travelling on the titanic the **survival probability is around 0.38**. But this probability varies according to Sex, Pclass & other f...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812?gi=750ac193f43c"
    }
  ]
}