# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival patterns by Sex/Pclass/Title

## Problem Type
Binary classification (Survived: 0 or 1). Evaluation metric: Accuracy. Target: 100% accuracy.

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name
Extract title using regex: `Name.str.extract(' ([A-Za-z]+)\.')`
- Group rare titles (count < 10) into 'Rare' category
- Key titles: Mr, Miss, Mrs, Master, Rare
- 'Master' (young boys) has ~57.5% survival vs 'Mr' at ~15.7%
- Female titles (Miss, Mrs) have ~70-80% survival

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size 2-4 tends to have better survival than alone or large families

### Binning Continuous Variables
- **AgeBin**: Use pd.cut() with 5 bins for Age
- **FareBin**: Use pd.qcut() with 4 quantile bins for Fare
- Binning helps tree models and reduces noise

### Cabin Features (Optional)
- Cabin has ~77% missing values - consider dropping
- Alternative: Extract deck letter (first character) if present
- Create HasCabin binary feature

## Missing Value Handling
- **Age**: Fill with median (grouped by Pclass/Sex for better imputation)
- **Embarked**: Fill with mode ('S')
- **Fare**: Fill with median
- **Cabin**: Drop or create HasCabin indicator

## Features to Drop
- PassengerId (no predictive value)
- Name (after extracting Title)
- Ticket (high cardinality, weak signal)
- Cabin (too many missing values)

## Models

### Recommended Base Models for Stacking
1. **Random Forest**: n_estimators=500, max_depth=6, min_samples_leaf=2
2. **Extra Trees**: n_estimators=500, max_depth=8, min_samples_leaf=2
3. **Gradient Boosting**: n_estimators=500, max_depth=5, min_samples_leaf=2
4. **AdaBoost**: n_estimators=500, learning_rate=0.75
5. **SVC**: kernel='linear', C=0.025

### Meta-Learner
- **XGBoost** as second-level model on out-of-fold predictions
- Alternative: Logistic Regression as simple meta-learner

### Single Model Alternatives
- **XGBoost/LightGBM/CatBoost**: Often competitive without stacking
- Hyperparameter tuning via GridSearchCV or RandomizedSearchCV

## Stacking Ensemble Approach
1. Use K-Fold (K=5) to generate out-of-fold predictions from base models
2. Stack predictions as features for second-level model
3. This prevents overfitting on training data

## Encoding
- **Label Encoding**: For ordinal features (Pclass, Title_Code, AgeBin_Code, FareBin_Code)
- **One-Hot Encoding**: For nominal features (Sex, Embarked) - creates dummy variables
- Tree models work well with label encoding; linear models prefer one-hot

## Validation Strategy
- Use StratifiedKFold (k=5) to maintain class balance
- Cross-validation score should be primary metric
- Small dataset (891 samples) - be careful of overfitting

## Key Survival Patterns (Domain Knowledge)
- "Women and children first" - Sex is the strongest predictor
- Higher class = higher survival (Pclass 1 > 2 > 3)
- Embarked at Cherbourg (C) had higher survival
- Young children (Age < 5) had high survival
- Passengers traveling alone had lower survival

## Hyperparameter Tuning
- Use GridSearchCV with cv=5 for systematic search
- Key parameters for trees: max_depth, n_estimators, min_samples_leaf
- Use RFECV (Recursive Feature Elimination with CV) for feature selection

## Tips for Maximizing Accuracy
1. Feature engineering is more important than model selection for this dataset
2. Title extraction is crucial - captures age/gender/social status
3. Family features capture group dynamics
4. Ensemble multiple diverse models
5. Careful handling of Age missing values (use Pclass/Sex grouping)
6. Consider interaction features (Sex_Pclass, Title_Pclass)
