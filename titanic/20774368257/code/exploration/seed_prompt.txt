# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival rates by features
- Key features: Pclass, Sex, Age, SibSp, Parch, Fare, Cabin, Embarked, Name (for title extraction)
- Missing values: Age (~20%), Cabin (~77%), Embarked (2 rows), Fare (1 in test)
- Target: ~38% survived, ~62% died (binary classification)

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title using regex: `Name.str.extract(' ([A-Za-z]+)\.')`
- Group rare titles: Map Mlle/Ms -> Miss, Mme -> Mrs, rare titles -> 'Rare'
- Title is highly predictive: Master (young boys) 57.5% survival, Mr 15.7%, Miss 69.8%, Mrs 79.2%

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- **Fsize_Cat** = Categorize family size: 'Alone' (1), 'Small' (2-4), 'Big' (5+)
- Family size of 2-4 has better survival than alone or large families

### Advanced Family/Group Survival Features (Key for 81%+ accuracy)
- **Family_Friends_Surv_Rate**: Calculate survival rate for passengers sharing same surname+fare or same ticket
- This captures group survival patterns - families/friends often survived or died together
- Implementation: Group by (Surname, Fare) and by Ticket, calculate mean survival rate for each group

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class)
- **Cabin_Deck** = First letter of Cabin (A, B, C, etc.) - different decks had different survival rates
- Label encode cabin deck: A=9, B=8, C=7, D=6, E=5, F=4, G=3, T=2, U=1 (unknown)

### Age Features
- Fill missing Age with median by Title group (more accurate than overall median)
- **AgeBin** = Binned age into categories (0-16, 16-32, 32-48, 48-64, 64+)
- Children (Age < 16) have higher survival rates

### Fare Features
- Fill missing Fare with median by Pclass
- **FareBin** = Binned fare into quartiles or custom bins (<=7, 7-39, 39+)
- Higher fare correlates with survival

### Name Features
- **Name_length** = len(Name) - longer names may indicate nobility/higher status
- **Is_Married** = 1 if Title == 'Mrs', else 0

### Ticket Features
- **Ticket_Prefix** = Extract prefix from ticket number using regex
- **Ticket_Frequency** = Count of passengers sharing same ticket
- **TicketGroup** = Categorize ticket frequency (1, 2-4, 5-8, 8+)

## Advanced Encoding Techniques (For 81%+ accuracy)

### Target Encoding
Use target encoding for high-cardinality categorical features:
- Sex, Cabin, Ticket, Embarked, Title
- Use `category_encoders.TargetEncoder` to encode based on target mean

### Frequency Encoding
Encode categorical features by their frequency in the dataset

### Factor Analysis / Quantile Transformation
- Apply QuantileTransformer to numerical features for normalization
- Use Factor Analysis or PCA for dimensionality reduction

## Missing Value Imputation
1. **Age**: Fill with median grouped by Title and Pclass, or use KNNImputer
2. **Embarked**: Fill with mode ('S' - Southampton)
3. **Fare**: Fill with median by Pclass
4. **Cabin**: Create Has_Cabin indicator, extract deck letter, drop original

## Models

### Single Models (Baseline)
- **Random Forest**: n_estimators=500, max_depth=6, min_samples_leaf=2
- **Gradient Boosting**: n_estimators=500, max_depth=4, learning_rate=0.01
- **XGBoost**: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8
- **LightGBM**: Fast alternative to XGBoost
- **CatBoost**: Handles categorical features natively
- **Logistic Regression**: Good baseline, interpretable
- **SVM**: kernel='rbf', C=1.0 (requires feature scaling)

### Deep Learning (For advanced approaches)
- Keras Classifier with hyperparameter tuning via Keras Tuner
- Architecture: Dense layers with dropout, BatchNormalization
- Can achieve competitive results on tabular data

### Ensemble Methods (Best Performance)

#### Voting Classifier
Combine multiple models with soft voting:
```python
VotingClassifier([
    ('rf', RandomForestClassifier()),
    ('gb', GradientBoostingClassifier()),
    ('xgb', XGBClassifier()),
    ('svc', SVC(probability=True))
], voting='soft')
```

#### Stacking (Top Approach)
Two-level stacking architecture:
1. **First Level (Base Models)**:
   - Random Forest (n_estimators=500, max_depth=6)
   - Extra Trees (n_estimators=500, max_depth=8)
   - AdaBoost (n_estimators=300)
   - Gradient Boosting (n_estimators=500, max_depth=5)
   - SVC (kernel='linear', C=0.025)

2. **Second Level (Meta-Classifier)**:
   - XGBoost: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8, colsample_bytree=0.8

3. **Out-of-Fold Predictions**:
   - Use K-Fold (5 folds) to generate out-of-fold predictions from base models
   - These predictions become features for the meta-classifier
   - Prevents overfitting in the stacking process

## Feature Selection
- **Boruta-Shap**: Advanced feature selection method combining Boruta with SHAP values
- **Recursive Feature Elimination (RFE)** with cross-validation
- Most important features typically: Sex, Pclass, Title, Fare, Age, FamilySize, Family_Surv_Rate
- Remove highly correlated features to reduce redundancy

## Handling Class Imbalance
- **SMOTE** (Synthetic Minority Over-sampling Technique) can help with the ~38/62 class split
- Use `imblearn.over_sampling.SMOTE`
- Apply only to training data, not validation/test

## Validation Strategy
- **Stratified K-Fold** (k=5) to maintain class distribution
- Use cross_val_score for model comparison
- Monitor both train and validation accuracy to detect overfitting

## Preprocessing Pipeline
1. Combine train and test for consistent feature engineering
2. Handle missing values (Age, Embarked, Fare)
3. Create engineered features (Title, FamilySize, IsAlone, Has_Cabin, AgeBin, FareBin, Family_Surv_Rate)
4. Encode categorical variables:
   - Label encoding for ordinal features (Pclass)
   - One-hot encoding or target encoding for nominal features
5. Feature scaling (StandardScaler) for SVM and neural networks
6. Drop unnecessary columns: PassengerId, Name, Ticket, Cabin

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV
- Key parameters to tune:
  - Tree models: max_depth, n_estimators, min_samples_leaf, min_samples_split
  - XGBoost: learning_rate, max_depth, min_child_weight, gamma, subsample, colsample_bytree
  - SVM: C, gamma, kernel

## Tips for High Accuracy (81%+)
1. **Feature engineering is key** - Title extraction, Family_Surv_Rate are critical
2. **Target encoding** for categorical features can boost performance
3. **Ensemble diverse models** - Combine tree-based and linear models
4. **Proper cross-validation** - Use stratified K-fold to get reliable estimates
5. **Handle missing values intelligently** - Group-based imputation (by Title/Pclass)
6. **Feature interactions** - Consider Pclass*Sex, Age*Pclass interactions
7. **Don't overfit** - Monitor validation scores, use regularization
8. **Try SMOTE** for handling class imbalance

## Expected Performance
- Simple baseline (gender-only): ~76% accuracy
- Good feature engineering + single model: ~80-82% accuracy
- Ensemble/Stacking with advanced features: ~82-84% accuracy
- Top Kaggle scores: ~84-86% accuracy

## Key References
- See `../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/` for comprehensive feature engineering
- See `../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` for stacking implementation
- See `../research/kernels/startupsci_titanic-data-science-solutions/` for data science workflow
