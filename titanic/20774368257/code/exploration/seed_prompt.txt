# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival rates by features
- Key features: Pclass, Sex, Age, SibSp, Parch, Fare, Cabin, Embarked, Name (for title extraction)
- Missing values: Age (~20%), Cabin (~77%), Embarked (2 rows), Fare (1 in test)
- Target: ~38% survived, ~62% died (binary classification)

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title using regex: `Name.str.extract(' ([A-Za-z]+)\.')`
- Group rare titles: Map Mlle/Ms -> Miss, Mme -> Mrs, rare titles -> 'Rare'
- Title is highly predictive: Master (young boys) 57.5% survival, Mr 15.7%, Miss 69.8%, Mrs 79.2%

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size of 2-4 has better survival than alone or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class)
- **Cabin_Deck** = First letter of Cabin (A, B, C, etc.) - different decks had different survival rates

### Age Features
- Fill missing Age with median by Title group (more accurate than overall median)
- **AgeBin** = Binned age into categories (0-16, 16-32, 32-48, 48-64, 64+)
- Children (Age < 16) have higher survival rates

### Fare Features
- Fill missing Fare with median by Pclass
- **FareBin** = Binned fare into quartiles
- Higher fare correlates with survival

### Name Length
- **Name_length** = len(Name) - longer names may indicate nobility/higher status

### Ticket Features
- **Ticket_Prefix** = Extract prefix from ticket number (some prefixes correlate with survival)
- **Ticket_Len** = Length of ticket number

## Missing Value Imputation
1. **Age**: Fill with median grouped by Title and Pclass
2. **Embarked**: Fill with mode ('S' - Southampton)
3. **Fare**: Fill with median by Pclass
4. **Cabin**: Create Has_Cabin indicator, drop original Cabin column

## Models

### Single Models (Baseline)
- **Random Forest**: n_estimators=500, max_depth=6, min_samples_leaf=2
- **Gradient Boosting**: n_estimators=500, max_depth=4, learning_rate=0.01
- **XGBoost**: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8
- **Logistic Regression**: Good baseline, interpretable
- **SVM**: kernel='rbf', C=1.0 (requires feature scaling)

### Ensemble Methods (Best Performance)

#### Voting Classifier
Combine multiple models with soft voting:
```
VotingClassifier([
    ('rf', RandomForestClassifier()),
    ('gb', GradientBoostingClassifier()),
    ('xgb', XGBClassifier()),
    ('svc', SVC(probability=True))
], voting='soft')
```

#### Stacking (Top Approach)
Two-level stacking architecture:
1. **First Level (Base Models)**:
   - Random Forest (n_estimators=500, max_depth=6)
   - Extra Trees (n_estimators=500, max_depth=8)
   - AdaBoost (n_estimators=300)
   - Gradient Boosting (n_estimators=500, max_depth=5)
   - SVC (kernel='linear', C=0.025)

2. **Second Level (Meta-Classifier)**:
   - XGBoost: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8, colsample_bytree=0.8

3. **Out-of-Fold Predictions**:
   - Use K-Fold (5 folds) to generate out-of-fold predictions from base models
   - These predictions become features for the meta-classifier
   - Prevents overfitting in the stacking process

## Validation Strategy
- **Stratified K-Fold** (k=5) to maintain class distribution
- Use cross_val_score for model comparison
- Monitor both train and validation accuracy to detect overfitting

## Preprocessing Pipeline
1. Handle missing values (Age, Embarked, Fare)
2. Create engineered features (Title, FamilySize, IsAlone, Has_Cabin, AgeBin, FareBin)
3. Encode categorical variables:
   - Label encoding for ordinal features (Pclass)
   - One-hot encoding for nominal features (Sex, Embarked, Title)
4. Feature scaling (StandardScaler) for SVM and neural networks
5. Drop unnecessary columns: PassengerId, Name, Ticket, Cabin

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Most important features typically: Sex, Pclass, Title, Fare, Age, FamilySize
- Remove highly correlated features to reduce redundancy

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV
- Key parameters to tune:
  - Tree models: max_depth, n_estimators, min_samples_leaf, min_samples_split
  - XGBoost: learning_rate, max_depth, min_child_weight, gamma, subsample, colsample_bytree
  - SVM: C, gamma, kernel

## Tips for High Accuracy
1. **Feature engineering is key** - Title extraction alone can boost accuracy significantly
2. **Ensemble diverse models** - Combine tree-based and linear models
3. **Proper cross-validation** - Use stratified K-fold to get reliable estimates
4. **Handle missing values intelligently** - Group-based imputation (by Title/Pclass) is better than simple median
5. **Don't overfit** - Monitor validation scores, use regularization
6. **Feature interactions** - Consider Pclass*Sex, Age*Pclass interactions

## Expected Performance
- Simple baseline (gender-only): ~76% accuracy
- Good feature engineering + single model: ~80-82% accuracy
- Ensemble/Stacking: ~82-84% accuracy
- Top Kaggle scores: ~84-86% accuracy (may involve some leakage or manual tuning)
