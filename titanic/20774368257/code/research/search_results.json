{
  "query": "What are the best ensemble and stacking techniques to maximize accuracy on binary classification problems?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensemble learning boosts binary\u2011classification performance by combining the predictions of several diverse models, while stacking (or stacked generalization) adds a meta\u2011learner that learns how to weight those base predictions.  A recent method replaces the usual logistic\u2011regression meta\u2011model with a computational\u2011geometry\u2011based optimizer that solves a maximum\u2011weighted\u2011rectangle problem, delivering higher accuracy, better interpretability and no hyper\u2011parameter tuning for the meta\u2011model\u202f([arxiv](https://arxiv.org/html/2410.22722v1)).  Classic stacking frameworks follow the same principle: base learners are trained on the full data, their out\u2011of\u2011fold predictions are fed to a second\u2011level model, and the final prediction is produced by the meta\u2011learner\u202f([MachineLearningMastery](https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning)).\n\nIn practice, the strongest base learners for tabular binary tasks are tree\u2011based ensembles\u2014gradient\u2011boosted decision trees (GBDT or HistGradientBoosting) and random forests\u2014augmented by bagging or voting classifiers when complementary algorithms (e.g., linear models, factorization machines) are available\u202f([scikit\u2011learn](https://scikit-learn.org/stable/modules/ensemble.html)).  Scikit\u2011learn\u2019s `StackingClassifier` makes it easy to assemble such pipelines, defaulting to a logistic\u2011regression meta\u2011estimator but allowing any classifier (e.g., another boosted tree)\u202f([scikit\u2011learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)).  A proven recipe is to stack several gradient\u2011boosting implementations (XGBoost, LightGBM, CatBoost) as first\u2011layer features and train a simple logistic regression on top, as demonstrated by the `stackgbm` package\u202f([CRAN](https://cran.r-project.org/web/packages/stackgbm/vignettes/stackgbm.html)).\n\nFor maximal accuracy, combine heterogeneous base models (tree ensembles, linear/logistic models, factorization machines) and use an expressive meta\u2011learner such as non\u2011negative least squares, hill\u2011climbing optimization, or gradient boosting\u2014techniques shown to outperform single models in SAS\u2019s stacked\u2011ensemble workflow\u202f([SAS](https://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf)).  Crucially, generate out\u2011of\u2011fold predictions with proper cross\u2011validation to avoid data leakage, tune each base learner independently, and let the meta\u2011model learn optimal weights; this layered diversity and careful validation consistently yields the highest binary\u2011classification accuracy.",
      "url": ""
    },
    {
      "title": "Enhancing binary classification: A new stacking method via leveraging computational geometry",
      "text": "# Enhancing binary classification: A new stacking method via leveraging computational geometry\n\nWei\u00a0Wu\nShizuoka University\n&Liang\u00a0Tang\nDalian Maritime University\n&Zhongjie\u00a0Zhao\nDalian Maritime University\n&Chung-Piaw\u00a0Teo\nNational University of Singapore\nCo-corresponding author. email: goi@shizuoka.ac.jp.Co-corresponding author. email: tangericliang@dlmu.edu.cn.\n\n###### Abstract\n\nStacking, a potent ensemble learning method, leverages a meta-model to harness the strengths of multiple base models, thereby enhancing prediction accuracy. Traditional stacking techniques typically utilize established learning models, such as logistic regression, as the meta-model. This paper introduces a novel approach that integrates computational geometry techniques, specifically solving the maximum weighted rectangle problem, to develop a new meta-model for binary classification. Our method is evaluated on multiple open datasets, with statistical analysis showing its stability and demonstrating improvements in accuracy compared to current state-of-the-art stacking methods with out-of-fold predictions. This new stacking method also boasts two significant advantages: enhanced interpretability and the elimination of hyperparameter tuning for the meta-model, thus increasing its practicality. These merits make our method highly applicable not only in stacking ensemble learning but also in various real-world applications, such as hospital health evaluation scoring and bank credit scoring systems, offering a fresh evaluation perspective.\n\n_K_ eywords\u2002Binary classification \u00a0\u22c5\u22c5\\\\cdot\u22c5\nEnsemble learning \u00a0\u22c5\u22c5\\\\cdot\u22c5\nStacking \u00a0\u22c5\u22c5\\\\cdot\u22c5\nComputational geometry \u00a0\u22c5\u22c5\\\\cdot\u22c5\nMaximum weighted rectangle problem\n\n## 1 Introduction\n\nBinary classification is a fundamental task in machine learning and data science, with applications spanning numerous domains, including spam detection, medical diagnostics, image recognition, credit scoring. The goal is to predict a binary outcome\u2014typically labeled as 0 or 1\u2014based on a set of input features. Various machine learning algorithms, such as logistic regression (LR), k\ud835\udc58kitalic\\_k-nearest neighbors (k\ud835\udc58kitalic\\_kNN), support vector machines (SVM), and neural network (NN), are commonly employed for binary classification tasks.\nThese algorithms can be mainly divided into two categories: those with interpretability, which are convenient for analysis and control (e.g., LR); and those without interpretability but with potentially good classification performance (e.g., NN).\n\nEnsemble learning, a powerful technique in predictive modeling, has gained widespread recognition for its ability to improve model performance by combining the strengths of multiple learning algorithms\u00a0\\[ [1](https://arxiv.org/html/2410.22722v1#bib.bib1)\\]. Among ensemble methods, stacking stands out by integrating the predictions of diverse base models (different learning algorithms) through a meta-model, resulting in enhanced prediction accuracy compared to only using the best base model\u00a0\\[ [2](https://arxiv.org/html/2410.22722v1#bib.bib2)\\]. Stacking has demonstrated significant applications in classification problems such as network intrusion detection\u00a0\\[ [3](https://arxiv.org/html/2410.22722v1#bib.bib3), [4](https://arxiv.org/html/2410.22722v1#bib.bib4)\\],\ncancer type classification\u00a0\\[ [5](https://arxiv.org/html/2410.22722v1#bib.bib5)\\],\ncredit lending\u00a0\\[ [6](https://arxiv.org/html/2410.22722v1#bib.bib6)\\],\nand protein-protein binding affinity prediction\u00a0\\[ [7](https://arxiv.org/html/2410.22722v1#bib.bib7)\\].\n\nMany researchers have been working on configuring the components of stacking, selecting adaptive base models and meta-model from a set of candidates by meta-heuristics\u00a0\\[ [8](https://arxiv.org/html/2410.22722v1#bib.bib8)\\], including genetic algorithm\u00a0\\[ [9](https://arxiv.org/html/2410.22722v1#bib.bib9)\\], data envelopment analysis\u00a0\\[ [10](https://arxiv.org/html/2410.22722v1#bib.bib10)\\] and ant colony optimization\u00a0\\[ [11](https://arxiv.org/html/2410.22722v1#bib.bib11)\\]. However, the design of the meta-model itself in stacking has largely been limited to conventional machine learning algorithms.\n\nIn this paper, we propose a novel stacking ensemble learning framework that leverages the maximum weighted rectangle problem (MWRP), a well-known problem in computational geometry, to construct an effective meta-model. The essence of the MWRP-based stacking lies in transforming the one-dimensional probability outputs of multiple base models for individual sample points into a multi-dimensional space. This transformation enriches the representation of the sample points with more high-dimensional information, which in turn allows the method to identify the optimal combination of base models and corresponding thresholds on selected models. The goal is to construct a rectangle in this high-dimensional space that maximizes the expected number of correctly classified sample points within it.\n\nThe MWRP-based stacking framework not only selects base models but also provides valuable insights into the selected models, thereby maintaining high interpretability.\nThrough extensive computational experiments across various datasets, we demonstrate the efficacy of our approach in enhancing predictive accuracy compared to traditional stacking methods.\nFurthermore, we explore potential extensions of the MWRP-based technique, particularly its application as an interpretable binary classifier and its capability to effectively handle imbalanced datasets.\n\n## 2 Binary Classification by Stacking Ensemble Learning\n\nIn this section, we discuss the background of supervised binary classification and the fundamental principles of the stacking ensemble learning framework.\n\n### 2.1 Binary Classification\n\nLet \ud835\udcb3=\u211dm\ud835\udcb3superscript\u211d\ud835\udc5a\\\\mathcal{X}=\\\\mathbb{R}^{m}caligraphic\\_X = blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_m end\\_POSTSUPERSCRIPT denote the m\ud835\udc5amitalic\\_m-dimensional feature space, and \ud835\udcb4B={0,1}subscript\ud835\udcb4B01\\\\mathcal{Y}\\_{\\\\mathrm{B}}=\\\\{0,1\\\\}caligraphic\\_Y start\\_POSTSUBSCRIPT roman\\_B end\\_POSTSUBSCRIPT = { 0 , 1 } denote the label space.\nGiven a dataset D=((x1,y1),(x2,y2),\u2026,(xn,yn))\ud835\udc37subscript\ud835\udc651subscript\ud835\udc661subscript\ud835\udc652subscript\ud835\udc662\u2026subscript\ud835\udc65\ud835\udc5bsubscript\ud835\udc66\ud835\udc5bD=((x\\_{1},y\\_{1}),(x\\_{2},y\\_{2}),\\\\ldots,(x\\_{n},y\\_{n}))italic\\_D = ( ( italic\\_x start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , italic\\_y start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) , ( italic\\_x start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , italic\\_y start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT ) , \u2026 , ( italic\\_x start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT , italic\\_y start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT ) ) with pairs of training samples xi\u2208\ud835\udcb3subscript\ud835\udc65\ud835\udc56\ud835\udcb3x\\_{i}\\\\in\\\\mathcal{X}italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT \u2208 caligraphic\\_X and yi\u2208\ud835\udcb4Bsubscript\ud835\udc66\ud835\udc56subscript\ud835\udcb4By\\_{i}\\\\in\\\\mathcal{Y}\\_{\\\\mathrm{B}}italic\\_y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT \u2208 caligraphic\\_Y start\\_POSTSUBSCRIPT roman\\_B end\\_POSTSUBSCRIPT for i=1,2,\u2026,n\ud835\udc5612\u2026\ud835\udc5bi=1,2,\\\\ldots,nitalic\\_i = 1 , 2 , \u2026 , italic\\_n, binary classification aims to\npredict the output y\u2208\ud835\udcb4B\ud835\udc66subscript\ud835\udcb4By\\\\in\\\\mathcal{Y}\\_{\\\\mathrm{B}}italic\\_y \u2208 caligraphic\\_Y start\\_POSTSUBSCRIPT roman\\_B end\\_POSTSUBSCRIPT of a new input x\u2208\ud835\udcb3\ud835\udc65\ud835\udcb3x\\\\in\\\\mathcal{X}italic\\_x \u2208 caligraphic\\_X.\n\nBinary classification is a fundamental task in data science and machine learning, with applications that extend across numerous domains, including spam detection, medical diagnosis, image analysis, quality assurance, credit assessment, and marketing.\nEach of these applications presents unique challenges, such as varying data distributions, noise levels, and feature correlations, which influence the choice of the appropriate machine learning method.\n\nA variety of machine learning algorithms are employed for binary classification.\nThe selection of a particular method is typically influenced by the characteristics of the dataset and the specific requirements of the problem at hand.\nFor example, logistic regre...",
      "url": "https://arxiv.org/html/2410.22722v1"
    },
    {
      "title": "Essence of Stacking Ensembles for Machine Learning",
      "text": "Essence of Stacking Ensembles for Machine Learning - MachineLearningMastery.comEssence of Stacking Ensembles for Machine Learning - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Ensemble Learning Crash-Course]()\n# Essence of Stacking Ensembles for Machine Learning\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onApril 27, 2021in[Ensemble Learning](https://machinelearningmastery.com/category/ensemble-learning/)[**3](https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning/#comments)\nShare*Post*Share\nStacked generalization, or**stacking**, may be a less popular machine learning ensemble given that it describes a framework more than a specific model.\nPerhaps the reason it has been less popular in mainstream machine learning is that it can be tricky to train a stacking model correctly, without suffering data leakage. This has meant that the technique has mainly been used by highly skilled experts in high-stakes environments, such as machine learning competitions, and given new names like blending ensembles.\nNevertheless, modern machine learning frameworks make stacking routine to implement and evaluate for classification and regression predictive modeling problems. As such, we can review ensemble learning methods related to stacking through the lens of the stacking framework. This broader family of stacking techniques can also help to see how to tailor the configuration of the technique in the future when exploring our own predictive modeling projects.\nIn this tutorial, you will discover the essence of the stacked generalization approach to machine learning ensembles.\nAfter completing this tutorial, you will know:\n* The stacking ensemble method for machine learning uses a meta-model to combine predictions from contributing members.\n* How to distill the essential elements from the stacking method and how popular extensions like blending and the super ensemble are related.\n* How to devise new extensions to stacking by selecting new procedures for the essential elements of the method.\n**Kick-start your project**with my new book[Ensemble Learning Algorithms With Python](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/), including*step-by-step tutorials*and the*Python source code*files for all examples.\nLet\u2019s get started.\n![Essence of Stacking Ensembles for Machine Learning](https://machinelearningmastery.com/wp-content/uploads/2020/12/Essence-of-Stacking-Ensembles-for-Machine-Learning.jpg)\nEssence of Stacking Ensembles for Machine Learning\nPhoto by[Thomas](https://www.flickr.com/photos/photommo/31459357916/), some rights reserved.\n## Tutorial Overview\nThis tutorial is divided into four parts; they are:\n1. Stacked Generalization\n2. Essence of Stacking Ensembles\n3. Stacking Ensemble Family\n1. Voting Ensembles\n2. Weighted Average\n3. Blending Ensemble\n4. Super Learner Ensemble\n5. Customized Stacking Ensembles## Stacked Generalization\n[Stacked Generalization](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/), or stacking for short, is an ensemble machine learning algorithm.\nStacking involves using a machine learning model to learn how to best combine the predictions from contributing ensemble members.\nIn[voting](https://machinelearningmastery.com/voting-ensembles-with-python/), ensemble members are typically a diverse collection of model types, such as a decision tree, naive Bayes, and support vector machine. Predictions are made by averaging the predictions, such as selecting the class with the most votes (the statistical mode) or the largest summed probability.\n> &#8230; (unweighted) voting only makes sense if the learning schemes perform comparably well.\n&#8212; Page 497,[Data Mining: Practical Machine Learning Tools and Techniques](https://amzn.to/2WehhJC), 2016.\nAn extension to voting is to weigh the contribution of each ensemble member in the prediction, providing a weighted sum prediction. This allows more weight to be placed on models that perform better on average and less on those that don&#8217;t perform as well but still have some predictive skill.\nThe weight assigned to each contributing member must be learned, such as the performance of each model on the training dataset or a holdout dataset.\nStacking generalizes this approach and allows any machine learning model to be used to learn how to best combine the predictions from contributing members. The model that combines the predictions is referred to as the meta-model, whereas the ensemble members are referred to as base-models.\n> The problem with voting is that it is not clear which classifier to trust. Stacking tries to learn which classifiers are the reliable ones, using another learning algorithm\u2014the metalearner\u2014to discover how best to combine the output of the base learners.\n&#8212; Page 497,[Data Mining: Practical Machine Learning Tools and Techniques](https://amzn.to/2WehhJC), 2016.\nIn the language taken from the paper that introduced the technique, base models are referred to as level-0 learners, and the meta-model is referred to as a level-1 model.\nNaturally, the stacking of models can continue to any desired level.\n> Stacking is a general procedure where a learner is trained to combine the individual learners. Here, the individual learners are called the first-level learners, while the combiner is called the second-level learner, or meta-learner.\n&#8212; Page 83,[Ensemble Methods](https://amzn.to/2XZzrjG), 2012.\nImportantly, the way that the meta-model is trained is different to the way the base-models are trained.\nThe input to the meta-model are the predictions made by the base-models, not the raw inputs from the dataset. The target is the same expected target value. The predictions made by the base-models used to train the meta-model are for examples not used to train the base-models, meaning that they are out of sample.\nFor example, the dataset can be split into train, validation, and test datasets. Each base-model can then be fit on the training set and make predictions on the validation dataset. The predictions from the validation set are then used to train the meta-model.\nThis means that the meta-model is trained to best combine the capabilities of the base-models when they are making[out-of-sample predictions](https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/), e.g. examples not seen during training.\n> &#8230; we reserve some instances to form the training data for the level-1 learner and build level-0 classifiers from the remaining data. Once the level-0 classifiers have been built they are used to classify the instances in the holdout set, forming the level-1 training data.\n&#8212; Page 498,[Data Mining: Practical Machine Learning Tools and Techniques](https://amzn.to/2WehhJC), 2016.\nOnce the meta-model is trained, the base models can be re-trained on the combined training and validation datasets. The whole system can then be evaluated on the test set by passing examples first through the base models to collect base-level predictions, then passing those predictions through the meta-model to get final predictions. The system can be used in the same way when making predictions on new data.\nThis approach to training, evaluating, and using a stacking model can be further generalized to work with[k-fold cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/).\nTypically, base models are prepared using different algorithms, meaning that the ensembles are a heterogeneous collection of model types providing a desired level of diversity to the predictions made. However, this does not have to be the case, and different configurations of the same models can be u...",
      "url": "https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning"
    },
    {
      "title": "1.11.  Ensembles: Gradient boosting, random forests, bagging, voting, stacking #",
      "text": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)](../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 1.11.Ensembles: Gradient boosting, random forests, bagging, voting, stacking[#](#ensembles-gradient-boosting-random-forests-bagging-voting-stacking)\n**Ensemble methods**combine the predictions of several\nbase estimators built with a given learning algorithm in order to improve\ngeneralizability / robustness over a single estimator.\nTwo very famous examples of ensemble methods are[gradient-boosted trees](#gradient-boosting)and[random forests](#forest).\nMore generally, ensemble models can be applied to any base learner beyond\ntrees, in averaging methods such as[Bagging methods](#bagging),[model stacking](#stacking), or[Voting](#voting-classifier), or in\nboosting, as[AdaBoost](#adaboost).\n## 1.11.1.Gradient-boosted trees[#](#gradient-boosted-trees)\n[Gradient Tree Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)or Gradient Boosted Decision Trees (GBDT) is a generalization\nof boosting to arbitrary differentiable loss functions, see the seminal work of[[Friedman2001]](#friedman2001). GBDT is an excellent model for both regression and\nclassification, in particular for tabular data.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)vs[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\nScikit-learn provides two implementations of gradient-boosted trees:[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)vs[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)for classification, and the\ncorresponding classes for regression. The former can be**orders of\nmagnitude faster**than the latter when the number of samples is\nlarger than tens of thousands of samples.\nMissing values and categorical data are natively supported by the\nHist\u2026 version, removing the need for additional preprocessing such as\nimputation.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)might be preferred for small sample\nsizes since binning may lead to split points that are too approximate\nin this setting.\n### 1.11.1.1.Histogram-Based Gradient Boosting[#](#histogram-based-gradient-boosting)\nScikit-learn 0.21 introduced two new implementations of\ngradient boosted trees, namely[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)and[`HistGradientBoostingRegressor`](generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor), inspired by[LightGBM](https://github.com/Microsoft/LightGBM)(See[[LightGBM]](#lightgbm)).\nThese histogram-based estimators can be**orders of magnitude faster**than[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)when the number of samples is larger\nthan tens of thousands of samples.\nThey also have built-in support for missing values, which avoids the need\nfor an imputer.\nThese fast estimators first bin the input samples`X`into\ninteger-valued bins (typically 256 bins) which tremendously reduces the\nnumber of splitting points to consider, and allows the algorithm to\nleverage integer-based data structures (histograms) instead of relying on\nsorted continuous values when building the trees. The API of these\nestimators is slightly different, and some of the features from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)are not yet supported, for instance some loss functions.\nExamples\n* [Partial Dependence and Individual Conditional Expectation Plots](../auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py)\n* [Comparing Random Forests and Histogram Gradient Boosting models](../auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html#sphx-glr-auto-examples-ensemble-plot-forest-hist-grad-boosting-comparison-py)\n#### 1.11.1.1.1.Usage[#](#usage)\nMost of the parameters are unchanged from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\nOne exception is the`max\\_iter`parameter that replaces`n\\_estimators`, and\ncontrols the number of iterations of the boosting process:\n```\n&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_hastie\\_10\\_2&gt;&gt;&gt;X,y=make\\_hastie\\_10\\_2(random\\_state=0)&gt;&gt;&gt;X\\_train,X\\_test=X[:2000],X[2000:]&gt;&gt;&gt;y\\_train,y\\_test=y[:2000],y[2000:]&gt;&gt;&gt;clf=HistGradientBoostingClassifier(max\\_iter=100).fit(X\\_train,y\\_train)&gt;&gt;&gt;clf.score(X\\_test,y\\_test)0.8965\n```\nAvailable losses for**regression**are:\n* \u2018squared\\_error\u2019, which is the default loss;\n* \u2018absolute\\_error\u2019, which is less sensitive to outliers than the squared error;\n* \u2018gamma\u2019, which is well suited to model strictly positive outcomes;\n* \u2018poisson\u2019, which is well suited to model counts and frequencies;\n* \u2018quantile\u2019, which allows for estimating a conditional quantile that can later\nbe used to obtain prediction intervals.\nFor**classification**, \u2018log\\_loss\u2019 is the only option. For binary classification\nit uses the binary log loss, also known as binomial deviance or binary\ncross-entropy. For`n\\_classes&gt;=3`, it uses the multi-class log loss function,\nwith multinomial deviance and categorical cross-entropy as alternative names.\nThe appropriate loss version is selected based on[y](../glossary.html#term-y)passed to[fit](../glossary.html#term-fit).\nThe size of the trees can be controlled through the`max\\_leaf\\_nodes`,`max\\_depth`, and`min\\_samples\\_leaf`parameters.\nThe number of bins used to bin the data is controlled with the`max\\_bins`parameter. Using less bins acts as a form of regularization. It is generally\nrecommended to use as many bins as possible (255), which is the default.\nThe`l2\\_regularization`parameter acts as a regularizer for the loss function,\nand corresponds to\\\\(\\\\lambda\\\\)in the following expression (see equation (2)\nin[[XGBoost]](#xgboost)):\n\\\\[\\\\mathcal{L}(\\\\phi) = \\\\sum\\_i l(\\\\hat{y}\\_i, y\\_i) + \\\\frac12 \\\\sum\\_k \\\\lambda ||w\\_k||^2\\\\]\nDetails on l2 regularization[#](#details-on-l2-regularization)\nIt is important to notice that the loss term\\\\(l(\\\\hat{y}\\_i, y\\_i)\\\\)describes\nonly half of the actual loss function except for the pinball loss and absolute\nerror.\nThe index\\\\(k\\\\)refers to the k-th tree in the ensemble of trees. In the\ncase of regression and binary classification, gradient boosting models grow one\ntree per iteration, then\\\\(k\\\\)runs up to`max\\_iter`. In the case o...",
      "url": "https://scikit-learn.org/stable/modules/ensemble.html"
    },
    {
      "title": "StackingClassifier #",
      "text": "StackingClassifier &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# StackingClassifier[#](#stackingclassifier)\n*class*sklearn.ensemble.StackingClassifier(*estimators*,*final\\_estimator=None*,*\\**,*cv=None*,*stack\\_method='auto'*,*n\\_jobs=None*,*passthrough=False*,*verbose=0*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/ensemble/_stacking.py#L422)[#](#sklearn.ensemble.StackingClassifier)\nStack of estimators with a final classifier.\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\nNote that`estimators\\_`are fitted on the full`X`while`final\\_estimator\\_`is trained using cross-validated predictions of the base estimators using`cross\\_val\\_predict`.\nRead more in the[User Guide](../ensemble.html#stacking).\nAdded in version 0.22.\nParameters:**estimators**list of (str, estimator)\nBase estimators which will be stacked together. Each element of the\nlist is defined as a tuple of string (i.e. name) and an estimator\ninstance. An estimator can be set to \u2018drop\u2019 using`set\\_params`.\nThe type of estimator is generally expected to be a classifier.\nHowever, one can pass a regressor for some use case (e.g. ordinal\nregression).\n**final\\_estimator**estimator, default=None\nA classifier which will be used to combine the base estimators.\nThe default classifier is a[`LogisticRegression`](sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression).\n**cv**int, cross-validation generator, iterable, or \u201cprefit\u201d, default=None\nDetermines the cross-validation splitting strategy used in`cross\\_val\\_predict`to train`final\\_estimator`. Possible inputs for\ncv are:\n* None, to use the default 5-fold cross validation,\n* integer, to specify the number of folds in a (Stratified) KFold,\n* An object to be used as a cross-validation generator,\n* An iterable yielding train, test splits,\n* `&quot;prefit&quot;`, to assume the`estimators`are prefit. In this case, the\nestimators will not be refitted.\nFor integer/None inputs, if the estimator is a classifier and y is\neither binary or multiclass,[`StratifiedKFold`](sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)is used.\nIn all other cases,[`KFold`](sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)is used.\nThese splitters are instantiated with`shuffle=False`so the splits\nwill be the same across calls.\nRefer[User Guide](../cross_validation.html#cross-validation)for the various\ncross-validation strategies that can be used here.\nIf \u201cprefit\u201d is passed, it is assumed that all`estimators`have\nbeen fitted already. The`final\\_estimator\\_`is trained on the`estimators`predictions on the full training set and are**not**cross validated\npredictions. Please note that if the models have been trained on the same\ndata to train the stacking model, there is a very high risk of overfitting.\nAdded in version 1.1:The \u2018prefit\u2019 option was added in 1.1\nNote\nA larger number of split will provide no benefits if the number\nof training samples is large enough. Indeed, the training time\nwill increase.`cv`is not used for model evaluation but for\nprediction.\n**stack\\_method**{\u2018auto\u2019, \u2018predict\\_proba\u2019, \u2018decision\\_function\u2019, \u2018predict\u2019}, default=\u2019auto\u2019\nMethods called for each base estimator. It can be:\n* if \u2018auto\u2019, it will try to invoke, for each estimator,`'predict\\_proba'`,`'decision\\_function'`or`'predict'`in that\norder.\n* otherwise, one of`'predict\\_proba'`,`'decision\\_function'`or`'predict'`. If the method is not implemented by the estimator, it\nwill raise an error.\n**n\\_jobs**int, default=None\nThe number of jobs to run in parallel for`fit`of all`estimators`.`None`means 1 unless in a`joblib.parallel\\_backend`context. -1 means\nusing all processors. See[Glossary](../../glossary.html#term-n_jobs)for more details.\n**passthrough**bool, default=False\nWhen False, only the predictions of estimators will be used as\ntraining data for`final\\_estimator`. When True, the`final\\_estimator`is trained on the predictions as well as the\noriginal training data.\n**verbose**int, default=0\nVerbosity level.\nAttributes:**classes\\_**ndarray of shape (n\\_classes,) or list of ndarray if`y`is of type`&quot;multilabel-indicator&quot;`.\nClass labels.\n**estimators\\_**list of estimators\nThe elements of the`estimators`parameter, having been fitted on the\ntraining data. If an estimator has been set to`'drop'`, it\nwill not appear in`estimators\\_`. When`cv=&quot;prefit&quot;`,`estimators\\_`is set to`estimators`and is not fitted again.\n**named\\_estimators\\_**[`Bunch`](sklearn.utils.Bunch.html#sklearn.utils.Bunch)\nAttribute to access any fitted sub-estimators by name.\n`n\\_features\\_in\\_`int\nNumber of features seen during[fit](../../glossary.html#term-fit).\n**feature\\_names\\_in\\_**ndarray of shape (`n\\_features\\_in\\_`,)\nNames of features seen during[fit](../../glossary.html#term-fit). Only defined if the\nunderlying estimators expose such an attribute when fit.\nAdded in version 1.0.\n**final\\_estimator\\_**estimator\nThe classifier fit on the output of`estimators\\_`and responsible for\nfinal predictions.\n**stack\\_method\\_**list of str\nThe method used by each base estimator.\nSee also\n[`StackingRegressor`](sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor)\nStack of estimators with a final regressor.\nNotes\nWhen`predict\\_proba`is used by each estimator (i.e. most of the time for`stack\\_method='auto'`or specifically for`stack\\_method='predict\\_proba'`),\nthe first column predicted by each estimator will be dropped in the case\nof a binary classification problem. Indeed, both feature will be perfectly\ncollinear.\nIn some cases (e.g. ordinal regression), one can pass regressors as the\nfirst layer of the[`StackingClassifier`](#sklearn.ensemble.StackingClassifier). However, note that`y`will\nbe internally encoded in a numerically increasing order or lexicographic\norder. If this ordering is not adequate, one should manually numerically\nencode the classes in the desired order.\nReferences\n[1]\nWolpert, David H. \u201cStacked generalization.\u201d Neural networks 5.2\n(1992): 241-259.\nExamples\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportload\\_iris&gt;&gt;&gt;fromsklearn.ensembleimportRandomForestClassifier&gt;&gt;&gt;fromsklearn.svmimportLinearSVC&gt;&gt;&gt;fromsklearn.linear\\_modelimportLogisticRegression&gt;&gt;&gt;fromsklearn.preprocessingimportStandardScaler&gt;&gt;&gt;fromsklearn.pipelineimportmake\\_pipeline&gt;&gt;&gt;fromsklearn.ensembleimportStackingClassifier&gt;&gt;&gt;X,y=load\\_iris(return\\_X\\_y=True)&gt;&gt;&gt;estimators=[...(&#39;rf&#39;,RandomForestClassifier(n\\_estimators=10,random\\_state=42)),...(&#39;svr&#39;,make\\_pipeline(StandardScaler(),...LinearSVC(random\\_state=42)))...]&gt;&gt;&gt;clf=StackingClassifier(...estimators=estimators,final\\_estimator=LogisticRegression()...)&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,stratify=y,random\\_state=42...)&gt;&gt;&gt;clf.fit(X\\_train,y\\_train).score(X\\_test,y\\_test)0.9...\n```\ndecision\\_function(*X*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/ensemble/_stacking.py#L791)[#](#sklearn.ensemble.StackingClassifier.decision_function)\nDecision function for samples in`X`using the final estimator.\nParameters:**X**{array-like, sparse matrix} of shape (n\\_samples, n\\_features)\nTraining vectors, where`n\\_samples`is the number of samples and`n\\_features`is ...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html"
    },
    {
      "title": "",
      "text": "1 \nPaper SAS-2017 \nStacked Ensemble Models for Improved Prediction Accuracy \nFunda G\u00fcne\u015f, Russ Wolfinger, and Pei-Yi Tan \nSAS Institute Inc. \nABSTRACT \nEnsemble modeling is now a well-established means for improving prediction accuracy; it enables you to \naverage out noise from diverse models and thereby enhance the generalizable signal. Basic stacked \nensemble techniques combine predictions from multiple machine learning algorithms and use these \npredictions as inputs to second-level learning models. This paper shows how you can generate a diverse \nset of models by various methods such as forest, gradient boosted decision trees, factorization machines, \nand logistic regression and then combine them with stacked-ensemble techniques such as hill climbing, \ngradient boosting, and nonnegative least squares in SAS\u00ae Visual Data Mining and Machine Learning. The \napplication of these techniques to real-world big data problems demonstrates how using stacked \nensembles produces greater prediction accuracy and robustness than do individual models. The \napproach is powerful and compelling enough to alter your initial data mining mindset from finding the \nsingle best model to finding a collection of really good complementary models. It does involve additional \ncost due both to training a large number of models and the proper use of cross validation to avoid \noverfitting. This paper shows how to efficiently handle this computational expense in a modern SAS\u00ae\nenvironment and how to manage an ensemble workflow by using parallel computation in a distributed \nframework. \nINTRODUCTION \nEnsemble methods are commonly used to boost predictive accuracy by combining the predictions of \nmultiple machine learning models. Model stacking is an efficient ensemble method in which the \npredictions that are generated by using different learning algorithms are used as inputs in a second-level \nlearning algorithm. This second-level algorithm is trained to optimally combine the model predictions to \nform a final set of predictions (Sill et al. 2009). \nIn the last decade, model stacking has been successfully used on a wide variety of predictive modeling \nproblems to boost the models\u2019 prediction accuracy beyond the level obtained by any of the individual \nmodels. This is sometimes referred to as a \u201cwisdom of crowds\u201d approach, pulling from the age-old \nphilosophy of Aristotle. Ensemble modeling and model stacking are especially popular in data science \ncompetitions, in which a sponsor posts training and test data and issues a global challenge to produce \nthe best model for a specified performance criterion. The winning model is almost always an ensemble \nmodel. Often individual teams develop their own ensemble model in the early stages of the competition \nand then join forces in the later stages. One such popular site is Kaggle, and you are encouraged to \nexplore numerous winning solutions that are posted in the discussion forums there to get a flavor of the \nstate of the art. \nThe diversity of the models in a library plays a key role in building a powerful ensemble model. Dietterich \n(2000) emphasizes the importance of diversity by stating, \u201cA necessary and sufficient condition for an \nensemble model to be more accurate than any of its individual members is if the classifiers are accurate \nand diverse.\u201d By combining information from diverse modeling approaches, ensemble models gain more \naccuracy and robustness than a fine-tuned single model can gain. There are many parallels with \nsuccessful human teams in business, science, politics, and sports, in which each team member makes a \nsignificant contribution and individual weaknesses and biases are offset by the strengths of other \nmembers. \nOverfitting is an omnipresent concern in ensemble modeling because a model library includes so many \nmodels that predict the same target. As the number of models in a model library increases, the chances \nof building overfitting ensemble models increases greatly. A related problem is leakage, in which \n2\ninformation from the target inadvertently and sometimes surreptitiously works its way into the model\u0002checking mechanism and causes an overly optimistic assessment of generalization performance. The \nmost efficient techniques that practitioners commonly use to minimize overfitting and leakage include \ncross validation, regularization, and bagging. This paper covers applications of these techniques for\nbuilding ensemble models that can generalize well to new data.\nThis paper first provides an introduction to SAS Visual Data Mining and Machine Learning in SAS\u00ae\nViya\u2122, which is a new single, integrated, in-memory environment. The section following that discusses \nhow to generate a diverse library of machine learning models for stacking while avoiding overfitting and \nleakage, and then shows an approach to building a diverse model library for a binary classification \nproblem. A subsequent section shows how to perform model stacking by using regularized regression \nmodels, including nonnegative least squares regression. Another section demonstrates stacking with the \nscalable gradient boosting algorithm and focuses on an automatic tuning implementation that is based on \nefficient distributed and parallel paradigms for training and tuning models in the SAS Viya platform. The \npenultimate section shows how to build powerful ensemble models with the hill climbing technique. The \nlast section compares the stacked ensemble models that are built by each approach to a na\u00efve ensemble \nmodel and the single best model, and also provides a brief summary.\nOVERVIEW OF THE SAS VIYA ENVIRONMENT\nThe SAS programs used in this paper are built in the new SAS Viya environment. SAS Viya uses SAS\u00ae\nCloud Analytic Services (CAS) to perform tasks and enables you to build various model scenarios in a \nconsistent environment, resulting in improved productivity, stability, and maintainability. SAS Viya \nrepresents a major rearchitecture of core data processing and analytical components in SAS software to \nenable computations across a large distributed grid in which it is typically more efficient to move \nalgorithmic code rather than to move data. \nThe smallest unit of work for the CAS server is a CAS action. CAS actions can load data, transform data, \ncompute statistics, perform analytics, and create output. Each action is configured by specifying a set of \ninput parameters. Running a CAS action in the CAS server processes the action's parameters and the \ndata to create an action result.\nIn SAS Viya, you can run CAS actions via a variety of interfaces, including the following:\n\uf0b7 SAS session, which uses the CAS procedure. PROC CAS uses the CAS language (CASL) for \nspecifying CAS actions and their input parameters. The CAS language also supports normal program \nlogic such as conditional and looping statements and user-written functions.\n\uf0b7 Python or Lua, which use the SAS Scripting Wrapper for Analytics Transfer (SWAT) libraries\n\uf0b7 Java, which uses the CAS Client class\n\uf0b7 Representational state transfer (REST), which uses the CAS REST APIs\nCAS actions are organized into action sets, where each action set defines an application programming \ninterface (API). SAS Viya currently provides the following action sets:\n\uf0b7 Data mining and machine learning action sets support gradient boosted trees, neural networks,\nfactorization machines, support vector machines, graph and network analysis, text mining, and more.\n\uf0b7 Statistics action sets compute summary statistics and perform clustering, regression, sampling, \nprincipal component analysis, and more.\n\uf0b7 Analytics action sets provide additional numeric and text analytics. \n\uf0b7 System action sets run SAS code via the DATA step or DS2, manage CAS libraries and tables, \nmanage CAS servers and sessions, and more.\nSAS Viya also provides CAS-powered procedures, which enable you to have the familiar experience of \ncoding traditional SAS procedures. Behind each statement in these procedures is one or more CAS \n3...",
      "url": "https://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf"
    },
    {
      "title": "Model stacking for boosted trees",
      "text": "Model stacking for boosted trees\n# Model stacking for boosted trees\n* [Introduction](#introduction)\n* [Generate\ndata](#generate-data)\n* [Parameter\ntuning](#parameter-tuning)\n* [Train the stackgbm model](#train-the-stackgbm-model)\n* [Inference](#inference)\n* [Performance evaluation](#performance-evaluation)\n* [xgboost](#xgboost)\n* [lightgbm](#lightgbm)\n* [catboost](#catboost)\n* [Tabular\nsummary](#tabular-summary)\n* [ROC curves](#roc-curves)\n* [Notes on categorical\nfeatures](#notes-on-categorical-features)\n* [References](#references)\n## Introduction\nModel stacking(Wolpert 1992)is a\nmethod for ensemble learning that combines the strength of multiple base\nlearners to drive up predictive performance. It is a particularly\npopular and effective strategy used in machine learning\ncompetitions.\nstackgbm implements a two-layer stacking model: the first layer\ngenerates \u201cfeatures\u201d produced by gradient boosting trees. The boosted\ntree models are built by xgboost(Chen and\nGuestrin 2016), lightgbm(Ke et al.\n2017), and catboost(Prokhorenkova et al.\n2018). The second layer is a logistic regression that uses these\nfeatures as inputs.\n```\n`library(&quot;stackgbm&quot;)`\n```\n## Generate data\nLet\u2019s generate some data for demonstrate purposes. The simulated data\nhas a\\\\(1000 \\\\times 50\\\\)predictor\nmatrix with a binary outcome vector. 800 samples will be in the training\nset and the rest 200 will be in the (independent) test set. 25 out of\nthe 50 features will be informative and follows\\\\(N(0, 10)\\\\).\n```\n`sim\\_data &lt;&lt;- msaenet::msaenet.sim.binomial(\nn = 1000,\np = 50,\nrho = 0.6,\ncoef = rnorm(25, mean = 0, sd = 10),\nsnr = 1,\np.train = 0.8,\nseed = 42\n)\nx\\_train &lt;&lt;- sim\\_data$x.tr\nx\\_test &lt;&lt;- sim\\_data$x.te\ny\\_train &lt;&lt;- as.vector(sim\\_data$y.tr)\ny\\_test &lt;&lt;- as.vector(sim\\_data$y.te)`\n```\n## Parameter tuning\n`cv\\_xgboost()`,`cv\\_lightgbm()`and`cv\\_catboost()`provide wrappers for tuning the most\nessential hyperparameters for each type of boosted tree models with\nk-fold cross-validation. The \u201coptimal\u201d parameters will be used to fit\nthe stacking model later.\n```\n`params\\_xgboost &lt;&lt;- cv\\_xgboost(x\\_train, y\\_train)\nparams\\_lightgbm &lt;&lt;- cv\\_lightgbm(x\\_train, y\\_train)\nparams\\_catboost &lt;&lt;- cv\\_catboost(x\\_train, y\\_train)`\n```\n## Train the stackgbm model\n```\n`model\\_stackgbm &lt;&lt;- stackgbm(\nsim\\_data$x.tr,\nsim\\_data$y.tr,\nparams = list(\nparams\\_xgboost,\nparams\\_lightgbm,\nparams\\_catboost\n)\n)`\n```\n## Inference\n```\n`roc\\_stackgbm\\_train &lt;&lt;- pROC::roc(\ny\\_train,\npredict(model\\_stackgbm, x\\_train)$prob,\nquiet = TRUE\n)\nroc\\_stackgbm\\_test &lt;&lt;- pROC::roc(\ny\\_test,\npredict(model\\_stackgbm, x\\_test)$prob,\nquiet = TRUE\n)\nroc\\_stackgbm\\_train$auc\n#&gt;&gt; Area under the curve: 0.9709\nroc\\_stackgbm\\_test$auc\n#&gt;&gt; Area under the curve: 0.7965`\n```\n## Performance evaluation\nLet\u2019s compare the predictive performance between the stacking model\nand the three types of tree boosting models (base learners) fitted\nindividually. Note that the models and performance metrics should be\n(bitwise) reproducible on the same operating system but they might vary\non different platforms.\n```\n`model\\_xgboost &lt;&lt;- xgboost\\_train(\nparams = list(\nobjective = &quot;&quot;binary:logistic&quot;&quot;,\neval\\_metric = &quot;&quot;auc&quot;&quot;,\nmax\\_depth = params\\_xgboost$max\\_depth,\neta = params\\_xgboost$eta\n),\ndata = xgboost\\_dmatrix(x\\_train, label = y\\_train),\nnrounds = params\\_xgboost$nrounds\n)\nmodel\\_lightgbm &lt;&lt;- lightgbm\\_train(\ndata = x\\_train,\nlabel = y\\_train,\nparams = list(\nobjective = &quot;&quot;binary&quot;&quot;,\nlearning\\_rate = params\\_lightgbm$learning\\_rate,\nnum\\_iterations = params\\_lightgbm$num\\_iterations,\nmax\\_depth = params\\_lightgbm$max\\_depth,\nnum\\_leaves = 2^params\\_lightgbm$max\\_depth - 1\n),\nverbose = -1\n)\nmodel\\_catboost &lt;&lt;- catboost\\_train(\ncatboost\\_load\\_pool(data = x\\_train, label = y\\_train),\nNULL,\nparams = list(\nloss\\_function = &quot;&quot;Logloss&quot;&quot;,\niterations = params\\_catboost$iterations,\ndepth = params\\_catboost$depth,\nlogging\\_level = &quot;&quot;Silent&quot;&quot;\n)\n)`\n```\n### xgboost\n```\n`roc\\_xgboost\\_train &lt;&lt;- pROC::roc(\ny\\_train,\npredict(model\\_xgboost, x\\_train),\nquiet = TRUE\n)\nroc\\_xgboost\\_test &lt;&lt;- pROC::roc(\ny\\_test,\npredict(model\\_xgboost, x\\_test),\nquiet = TRUE\n)\nroc\\_xgboost\\_train$auc\n#&gt;&gt; Area under the curve: 0.9949\nroc\\_xgboost\\_test$auc\n#&gt;&gt; Area under the curve: 0.7819`\n```\n### lightgbm\n```\n`roc\\_lightgbm\\_train &lt;&lt;- pROC::roc(\ny\\_train,\npredict(model\\_lightgbm, x\\_train),\nquiet = TRUE\n)\nroc\\_lightgbm\\_test &lt;&lt;- pROC::roc(\ny\\_test,\npredict(model\\_lightgbm, x\\_test),\nquiet = TRUE\n)\nroc\\_lightgbm\\_train$auc\n#&gt;&gt; Area under the curve: 0.9956\nroc\\_lightgbm\\_test$auc\n#&gt;&gt; Area under the curve: 0.7801`\n```\n### catboost\n```\n`roc\\_catboost\\_train &lt;&lt;- pROC::roc(\ny\\_train,\ncatboost\\_predict(\nmodel\\_catboost,\ncatboost\\_load\\_pool(data = x\\_train, label = NULL)\n),\nquiet = TRUE\n)\nroc\\_catboost\\_test &lt;&lt;- pROC::roc(\ny\\_test,\ncatboost\\_predict(\nmodel\\_catboost,\ncatboost\\_load\\_pool(data = x\\_test, label = NULL)\n),\nquiet = TRUE\n)\nroc\\_catboost\\_train$auc\n#&gt;&gt; Area under the curve: 0.9366\nroc\\_catboost\\_test$auc\n#&gt;&gt; Area under the curve: 0.7739`\n```\n### Tabular summary\nWe can summarize the AUC values in a table.\nAUC values from four models on training and testing\nset||stackgbm|xgboost|lightgbm|catboost|\nTraining|0.9709|0.9949|0.9956|0.9366|\nTesting|0.7965|0.7819|0.7801|0.7739|\n### ROC curves\nPlot the ROC curves of all models on the independent test set.\n```\n`pal &lt;&lt;- c(&quot;&quot;#e15759&quot;&quot;, &quot;&quot;#f28e2c&quot;&quot;, &quot;&quot;#59a14f&quot;&quot;, &quot;&quot;#4e79a7&quot;&quot;, &quot;&quot;#76b7b2&quot;&quot;)\nplot(pROC::smooth(roc\\_stackgbm\\_test), col = pal[1], lwd = 1)\nplot(pROC::smooth(roc\\_xgboost\\_test), col = pal[2], lwd = 1, add = TRUE)\nplot(pROC::smooth(roc\\_lightgbm\\_test), col = pal[3], lwd = 1, add = TRUE)\nplot(pROC::smooth(roc\\_catboost\\_test), col = pal[4], lwd = 1, add = TRUE)\nlegend(\n&quot;&quot;bottomright&quot;&quot;,\ncol = pal,\nlwd = 2,\nlegend = c(&quot;&quot;stackgbm&quot;&quot;, &quot;&quot;xgboost&quot;&quot;, &quot;&quot;lightgbm&quot;&quot;, &quot;&quot;catboost&quot;&quot;)\n)`\n```\n## Notes on categorical features\n[xgboost](https://cran.r-project.org/package=xgboost/vignettes/discoverYourData.html#conversion-from-categorical-to-numeric-variables)and[lightgbm](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support)both prefer the categorical features to be encoded as integers. For[catboost](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic),\nthe categorical features can be encoded as character factors.\nTo avoid possible confusions, if your data has any categorical\nfeatures, we recommend converting them to integers or use one-hot\nencoding, and use a numerical matrix as the input.\n## References\nChen, Tianqi, and Carlos Guestrin. 2016.\u201cXGBoost: A\nScalable Tree Boosting System.\u201dIn*Proceedings of the 22ndACMSIGKDDInternational Conference on\nKnowledge Discovery and Data Mining*, 785\u201394. ACM.\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017.\u201cLightGBM: A Highly\nEfficient Gradient Boosting Decision Tree.\u201dIn*Advances in\nNeural Information Processing Systems*, 3146\u201354.\nProkhorenkova, Liudmila, Gleb Gusev, Aleksandr Vorobev, Anna Veronika\nDorogush, and Andrey Gulin. 2018.\u201cCatBoost: Unbiased\nBoosting with Categorical Features.\u201dIn*Advances in Neural\nInformation Processing Systems*, 6638\u201348.\nWolpert, David H. 1992.\u201cStacked Generalization.\u201d*Neural Networks*5 (2): 241\u201359.",
      "url": "https://cran.r-project.org/web/packages/stackgbm/vignettes/stackgbm.html"
    },
    {
      "title": "Combining Varied Learners for Binary Classification using Stacked Generalization",
      "text": "COMBINING VARIED LEARNERS FOR BINARY CLASSIFICATION\nUSING STACKED GENERALIZATION\nSruthi Nair\nMaster of Engineering\nVidyalankar Institute of Technology\nMumbai, India\nsruthi.rk.nair@gmail.com\nAbhishek Gupta\nResearch Engineer\nUniversity of Mumbai\nMumbai, India\nabhishekgupta@sjcem.edu.in\nRaunak Joshi\nMentor\nUniversity of Mumbai\nMumbai, India\nraunakjoshi.m@gmail.com\nDr. Vidya Chitre\nAssistant Professor\nVidyalankar Institute of Technology\nMumbai, India\nvidya.chitre@vit.edu.in\nABSTRACT\nThe Machine Learning has various learning algorithms that are better in some or the other aspect\nwhen compared with each other but a common error that all algorithms will suffer from is training\ndata with very high dimensional feature set. This usually ends up algorithms into generalization error\nthat deplete the performance. This can be solved using an Ensemble Learning method known as\nStacking commonly termed as Stacked Generalization. In this paper we perform binary classification\nusing Stacked Generalization on high dimensional Polycystic Ovary Syndrome dataset and prove\nthe point that model becomes generalized and metrics improve significantly. The various metrics\nare given in this paper that also point out a subtle transgression found with Receiver Operating\nCharacteristic Curve that was proved to be incorrect.\nKeywords Ensemble Learning \u00b7 Generalizing Error \u00b7 Stacked Generalization\n1 Introduction\nDeriving inference from a very high dimensional data for classification tasks is very difficult for Machine Learning\n[Kumari and Srivastava, 2017] techniques no matter how good they are operationally. Inference that is not able to derive\nis termed as error for machine learning models and results in loss [Yessou et al., 2020] of information. Now there are\ntechniques known as loss optimizers [Poojary and Pai, 2019] for deep learning [LeCun et al., 2015] techniques, but\nmachine learning techniques require some for generalization of errors [Jakubovitz et al., 2019]. If we consider machine\nlearning, the wide variety of algorithms are available in parametric as well as non-parametric learning methods. The\nlearning methods are about how efficiently they fit the data no matter how high are the dimensions. If we consider\nclassification, we will specifically give importance to binary classification [Bahel et al., 2020] as it is the type of\nclassification we are considering for this paper. Logistic Regression [Peng et al., 2002] is one of the most commonly\nused binary classification algorithm. The Logistic Regression is parametric and supervised learning algorithm and uses\na logit function that gives 2 separate classes effectively. Even after working with hyperparameters of the algorithm,\nthere can be many algorithms that work efficiently in different aspects of data. The extension of linear models can be\nalgorithms like K-Nearest Neighbors, Support Vector Machines and many more. Right before diving into algorithms\nand techniques for generalizing errors in machine learning, let us consider the data we have used. Our selection criteria\nfor the data was not limited as we wanted a data with very high number of dimensions with categorical variables\nand binary classification problem. The data that we considered for this paper is Polycystic Ovary Syndrome [Azziz\net al., 2016] Classification. This data is specifically binary classification problem which based on the features gives the\npresence of symptom. The data is very high dimensional in terms of features and there are many categorical features. In\narXiv:2202.08910v1 [cs.LG] 17 Feb 2022\nCombining Varied Learners for Binary Classification using Stacked Generalization\norder to harness any machine learning algorithm performance, categorical variables are challenging as the basis of it\nwhen converted to numerical depends on its purpose. The quantitative variables are very hard distinguish for algorithms\nand that is where the test of its performance starts. Conversion of such variables is necessary into numerical formats and\nit depends on the fact if they need to be converted into rank oriented or occurrence oriented. There are many facets for\nsuch kind of data and that is where we decided to work with it. If we consider polycystic ovary syndrome for machine\nlearning, the problem is tackled by using logistic regression [jiao Li et al., 2011], bagging ensemble [Kanvinde et al.,\n2022], discriminant analysis [Gupta et al., 2022], boosting ensemble [Gupta et al., 2021] methods in these papers. This\npaper primarily focuses on techniques to generalize the errors of other models and secondarily focuses on application of\nthe methodology given in this paper with polycystic ovary syndrome data.\n2 Methodology\nThis section of paper specifically gives detailed procedure of how we approached the problem. In order to work with\ngeneralization error, we will use Stacking Ensemble Method which is also known as Stacked Generalization [Wolpert,\n1992, Ting and Witten, 1997]. First we will thoroughly walk you through all the classifiers used in stack of stacked\ngeneralization and then give detailed explanation of the stacked model.\n2.1 Logistic Regression\nThis is going to be one of the algorithm that will contribute to the stack. The logistic regression is parametric supervised\nlinear learning method. The logistic regression calculates the fixed parameters and uses them to calculate the prediction\nequation which is very similar to the linear regression. The formula for prediction function for calculating one single\nfeature is given as\ny\u02c6 = \u03b30 + \u03b3i.(xi) (1)\nThe \u03b3 is just the arbitrary form of variable and any other notation can be used. This equation is for fitting a linear line\nover the data points, but since logistic regression is classification algorithm, the line needs to be converted with logit\nfunction to give distinct separation between the classes. This function for logit is given as\n\u03c3(Z) = 1\n1 + e\u2212Z\n(2)\nwhere \u03c3 is termed as the logit function also known as Sigmoid function and Z is basically linear function.\n2.2 Support Vector Machine\nThe support vector machine [Hearst et al., 1998, Evgeniou and Pontil, 2001, Cristianini and Ricci, 2008] abbreviated as\nSVM is a non-parametric supervised learning algorithm that uses the hyperplane function to estimate the points in the\ndata. The hyperplane is a line that is passed between different data points for differentiating classes separately. The\nhyperplane is a single line and requires maximal margin that to get closer to points. The points that touch the margin\nare known as Support Vectors. The points that first touch the maximal margin is the classified label. The hyperplane\ncalculation requires vector normal and an offset point. This can be represented in equation as\nw\nT\n.x + b = 0 (3)\nwhere w is the vector normal and b is the offset point. The maximal margin is later given with +/- equation and can be\nrepresented as\n\u001a\n+, wT.x + b > 0\n\u2212, wT.x + b < 0\n(4)\n2.3 Multi Layer Perceptron\nThe term of Multi Layer Perceptron [Ramchoun et al., 2016] abbreviated as MLP is coined from the field of Deep\nLearning. The field of deep learning is very widely used now and then but the foundation of it is Perceptron. The term of\nperceptron was coined for the fact that system was able to develop human brain abilities that included perception. The\n2\nCombining Varied Learners for Binary Classification using Stacked Generalization\nperceptron works with initially, initialized weights and biases that influence the learning process of the representations\nfrom the data. These weights and biases requires an activation function [Nwankpa et al., 2018] to retain the features\nand details of the data. Logit function of Logistic Regression is actually a type of activation function that is used for\nbinary classification. Similarly for learning information in a greater amount the perceptron is used in a connection\nwhich is known as Multi Layer. The activation function in initial perceptron requires to learn features and can use\ndifferent activation f...",
      "url": "https://arxiv.org/pdf/2202.08910"
    },
    {
      "title": "1.11.  Ensembles: Gradient boosting, random forests, bagging, voting, stacking #",
      "text": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking &#8212; scikit-learn 1.5.2 documentation\n[Skip to main content](#main-content)\n**Back to top\n**Ctrl+K\n[![scikit-learn homepage](../_static/scikit-learn-logo-small.png)](../index.html)\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\n# 1.11.Ensembles: Gradient boosting, random forests, bagging, voting, stacking[#](#ensembles-gradient-boosting-random-forests-bagging-voting-stacking)\n**Ensemble methods**combine the predictions of several\nbase estimators built with a given learning algorithm in order to improve\ngeneralizability / robustness over a single estimator.\nTwo very famous examples of ensemble methods are[gradient-boosted trees](#gradient-boosting)and[random forests](#forest).\nMore generally, ensemble models can be applied to any base learner beyond\ntrees, in averaging methods such as[Bagging methods](#bagging),[model stacking](#stacking), or[Voting](#voting-classifier), or in\nboosting, as[AdaBoost](#adaboost).\n## 1.11.1.Gradient-boosted trees[#](#gradient-boosted-trees)\n[Gradient Tree Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)or Gradient Boosted Decision Trees (GBDT) is a generalization\nof boosting to arbitrary differentiable loss functions, see the seminal work of[[Friedman2001]](#friedman2001). GBDT is an excellent model for both regression and\nclassification, in particular for tabular data.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)vs[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\nScikit-learn provides two implementations of gradient-boosted trees:[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)vs[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)for classification, and the\ncorresponding classes for regression. The former can be**orders of\nmagnitude faster**than the latter when the number of samples is\nlarger than tens of thousands of samples.\nMissing values and categorical data are natively supported by the\nHist\u2026 version, removing the need for additional preprocessing such as\nimputation.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor), might be preferred for small sample\nsizes since binning may lead to split points that are too approximate\nin this setting.\n### 1.11.1.1.Histogram-Based Gradient Boosting[#](#histogram-based-gradient-boosting)\nScikit-learn 0.21 introduced two new implementations of\ngradient boosted trees, namely[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)and[`HistGradientBoostingRegressor`](generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor), inspired by[LightGBM](https://github.com/Microsoft/LightGBM)(See[[LightGBM]](#lightgbm)).\nThese histogram-based estimators can be**orders of magnitude faster**than[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)when the number of samples is larger\nthan tens of thousands of samples.\nThey also have built-in support for missing values, which avoids the need\nfor an imputer.\nThese fast estimators first bin the input samples`X`into\ninteger-valued bins (typically 256 bins) which tremendously reduces the\nnumber of splitting points to consider, and allows the algorithm to\nleverage integer-based data structures (histograms) instead of relying on\nsorted continuous values when building the trees. The API of these\nestimators is slightly different, and some of the features from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)are not yet supported, for instance some loss functions.\nExamples\n* [Partial Dependence and Individual Conditional Expectation Plots](../auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py)\n* [Comparing Random Forests and Histogram Gradient Boosting models](../auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html#sphx-glr-auto-examples-ensemble-plot-forest-hist-grad-boosting-comparison-py)\n#### 1.11.1.1.1.Usage[#](#usage)\nMost of the parameters are unchanged from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\nOne exception is the`max\\_iter`parameter that replaces`n\\_estimators`, and\ncontrols the number of iterations of the boosting process:\n```\n&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_hastie\\_10\\_2&gt;&gt;&gt;X,y=make\\_hastie\\_10\\_2(random\\_state=0)&gt;&gt;&gt;X\\_train,X\\_test=X[:2000],X[2000:]&gt;&gt;&gt;y\\_train,y\\_test=y[:2000],y[2000:]&gt;&gt;&gt;clf=HistGradientBoostingClassifier(max\\_iter=100).fit(X\\_train,y\\_train)&gt;&gt;&gt;clf.score(X\\_test,y\\_test)0.8965\n```\nAvailable losses for**regression**are:\n* \u2018squared\\_error\u2019, which is the default loss;\n* \u2018absolute\\_error\u2019, which is less sensitive to outliers than the squared error;\n* \u2018gamma\u2019, which is well suited to model strictly positive outcomes;\n* \u2018poisson\u2019, which is well suited to model counts and frequencies;\n* \u2018quantile\u2019, which allows for estimating a conditional quantile that can later\nbe used to obtain prediction intervals.\nFor**classification**, \u2018log\\_loss\u2019 is the only option. For binary classification\nit uses the binary log loss, also known as binomial deviance or binary\ncross-entropy. For`n\\_classes&gt;=3`, it uses the multi-class log loss function,\nwith multinomial deviance and categorical cross-entropy as alternative names.\nThe appropriate loss version is selected based on[y](../glossary.html#term-y)passed to[fit](../glossary.html#term-fit).\nThe size of the trees can be controlled through the`max\\_leaf\\_nodes`,`max\\_depth`, and`min\\_samples\\_leaf`parameters.\nThe number of bins used to bin the data is controlled with the`max\\_bins`parameter. Using less bins acts as a form of regularization. It is generally\nrecommended to use as many bins as possible (255), which is the default.\nThe`l2\\_regularization`parameter acts as a regularizer for the loss function,\nand corresponds to\\\\(\\\\lambda\\\\)in the following expression (see equation (2)\nin[[XGBoost]](#xgboost)):\n\\\\[\\\\mathcal{L}(\\\\phi) = \\\\sum\\_i l(\\\\hat{y}\\_i, y\\_i) + \\\\frac12 \\\\sum\\_k \\\\lambda ||w\\_k||^2\\\\]\nDetails on l2 regularization[#](#details-on-l2-regularization)\nIt is important to notice that the loss term\\\\(l(\\\\hat{y}\\_i, y\\_i)\\\\)describes\nonly half of the actual loss function except for the pinball loss and absolute\nerror.\nThe index\\\\(k\\\\)refers to the k-th tree in the ensemble of trees. In the\ncase of regression and binary classification, gradient boosting models grow one\ntree per iteration, then\\\\(k\\\\)runs up to`max\\_iter`. In the case of\nmulticlass classification problems, the maximal value of the index\\\\(k\\\\)is`n\\_classes`\\\\(\\\\times\\\\)`max\\_iter`.\nIf\\\\(T\\_k\\\\)denote...",
      "url": "https://scikit-learn.org/1.5/modules/ensemble.html"
    }
  ]
}