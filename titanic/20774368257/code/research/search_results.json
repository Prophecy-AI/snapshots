{
  "query": "What feature engineering techniques achieve highest accuracy on Titanic survival prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The highest reported Titanic survival accuracy (~81\u202f%) is achieved by extensive feature engineering\u2014including extracting titles, creating\u202fFamilySize\u202fand\u202fIsAlone, binning\u202fAge, encoding cabin presence, applying target and frequency encoding, regex\u2011derived features, factor analysis/quantile transformation, Boruta\u2011Shap feature selection and SMOTE oversampling\u2014followed by powerful models such as XGBoost or deep\u2011learning classifiers ([shainis.quarto.pub](https://shainis.quarto.pub/posts/Titanic), [github.com](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708), [scatta.cc](https://scatta.cc/2020/08/15/prediction-titanic-dataset-kaggle.html)).",
      "url": ""
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "Predicting Titanic survival with feature engineering and XGBoost",
      "text": "# Predicting Titanic survival with feature engineering and XGBoost\n\n![](https://scatta.cc/assets/images/titanic_ship.png)\n\nThe classic Titanic machine learning competition is an fantastic way to apply your machine learning skills and compare your work with others.\n\nEven though the full dataset is available and you can cheat your way to perfect score, it is very satisfying to compete fairly and achieve good result.\n\nIn the notebook, I focus on the preprocessing aspect of machine learning, i.e., the cleaning and feature engineering of the dataset, which resulted in a top 4% position in the leaderboard with XGBoost.\n\nThis dataset is an excellent example to illustrate the power of understanding your dataset and making it more useful before diving into specific ML algorithms.\n\nThe notebook is divided into five parts:\n\n**Chapter 1: Missing values**\n\n**Chapter 2: Feature engineering**\n\n**Chapter 3: Assessing the features**\n\n**Chapter 4: Prepare data for training**\n\n**Chapter 5: Build a model to predict Titanic survival**\n\nThe success of the prediction boils down to judicious selection of useful features in addtion to creating new ones. Below is the correlation matrix of existing and engineered features in this dataset.\n\n![](data:image/png;base64,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...",
      "url": "https://scatta.cc/2020/08/15/prediction-titanic-dataset-kaggle.html"
    },
    {
      "title": "Titanic Top 2% with Accuracy 81.1%, My Secret Sauce",
      "text": "![](https://shainis.quarto.pub/posts/Titanic/1680417506321.jpg)\n\nScreenshot from my Kaggle profile\n\n# Overview\n\nMy secret sauce to a top score in the Titanic competition: integrating numerous new tools and items of knowledge that I have obtained recently.\n\n### The secret sauce is:\n\n1. **Extensive feature engineering,** including target encoding, frequency encoding, regular expression, and more.\n\n2. **Factor analysis and quantile transformation** of the numerical variables.\n\n3. **Feature selection with Boruta Shap.**\n\n4. **Handling the imbalanced dataset using SMOTE.**\n\n5. **Utilizing Keras Classifier and tuning it with Keras tuner.** Yep, DNN rocks on tabular data.\n\n\n```\n#Load important packages\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, FactorAnalysis\n\nimport re\n\nimport matplotlib.pyplot as plt\n```\n\n```\n#Load the data\n\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n#Correct the type of the following columns\nfor col in ['Sex', 'Cabin', 'Ticket', 'Embarked']:\n    train[col] = train[col].astype('category')\n    test[col] = test[col].astype('category')\n\n#Extract the target\ntarget = train['Survived']\n\n#Combine the train and test for easier EDA and feature eng'\ncombined = pd.concat([train, test])\n\n#Check the shapes of the dataframes\nprint('train shape:',train.shape, '\\ntest shape:', test.shape, '\\ncombined shape:', combined.shape,\n      '\\n**************************')\n\ncombined.head()\n```\n\n```\ntrain shape: (891, 12)\ntest shape: (418, 11)\ncombined shape: (1309, 12)\n**************************\n```\n\n|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0.0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1.0 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1.0 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n| 3 | 4 | 1.0 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S |\n| 4 | 5 | 0.0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S |\n\n```\n%%capture\n\n#Install category_encoders for target encoding\n!pip install category_encoders\n```\n\n```\n#Target encode the following features\n\nfrom category_encoders import TargetEncoder\nencoder = TargetEncoder()\n\ncombined['Sex_TE'] = encoder.fit_transform(combined.iloc[:891].Sex, target)\ncombined['Cabin_TE'] = encoder.fit_transform(combined.iloc[:891].Cabin, target)\ncombined['Ticket_TE'] = encoder.fit_transform(combined.iloc[:891].Ticket, target)\ncombined['Embarked_TE'] = encoder.fit_transform(combined.iloc[:891].Embarked, target)\n```\n\n```\n#Transform the Sex column to numerical\ncombined['Sex'] = combined['Sex'].map({'male': 0, 'female': 1})\ncombined['Sex'] = combined['Sex'].astype(int)\n\n#Extract Titles\ncombined['Title'] = combined['Name']\ncombined['Title'] = combined['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n#Replace rare titles\nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other',\n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other',\n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal',\n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n\ncombined.replace({'Title': mapping}, inplace=True)\n\n#Target encoding the Title\ncombined['Title_for_te'] = combined['Title'].astype('category')\ncombined['Title_TE'] = encoder.fit_transform(combined.iloc[:891].Title_for_te, target)\ncombined.drop(columns=['Title_for_te'], inplace=True)\n\n#Create new feature - is married\ncombined['Is_Married'] = 0\ncombined['Is_Married'].loc[combined['Title'] == 'Mrs'] = 1\n\n#Create a new feature of Name Length - because longer names usually are given to higher class people\ncombined[\"Name_Length\"] = combined.Name.str.replace(\"[^a-zA-Z]\", \"\").str.len()\n\n#Label encode the Title column\ntitle_dict = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Other': 4, 'Royal': 5, 'Master': 6}\ncombined['Title'] = combined['Title'].map(title_dict).astype('int')\n\n#Create a feature of Family size\ncombined['Family_Size'] = combined['Parch'] + combined['SibSp'] + 1\n\n#Create a feature of Family size category\ncombined['Fsize_Cat'] = combined['Family_Size'].map(lambda val: 'Alone' if val <= 1 else ('Small' if val < 5 else 'Big'))\nFsize_dict = {'Alone':3, 'Small':2, 'Big':1}\ncombined['Fsize_Cat'] = combined['Fsize_Cat'].map(Fsize_dict).astype('int')\n\n#Extract the Surname\ncombined['Surname'] = combined['Name'].str.extract('([A-Za-z]+.[A-Za-z]+)\\,', expand=True)\n\n#Create a family survival rate feature\nMEAN_SURVIVAL_RATE = round(np.mean(train['Survived']), 4)\n\ncombined['Family_Friends_Surv_Rate'] = MEAN_SURVIVAL_RATE\ncombined['Surv_Rate_Invalid'] = 1\n\nfor _, grp_df in combined[['Survived', 'Surname', 'Fare', 'Ticket', 'PassengerId']].groupby(['Surname', 'Fare']):\n    if (len(grp_df) > 1):\n        if(grp_df['Survived'].isnull().sum() != len(grp_df)):\n            for ind, row in grp_df.iterrows():\n                combined.loc[combined['PassengerId'] == row['PassengerId'],\n                            'Family_Friends_Surv_Rate'] = round(grp_df['Survived'].mean(), 4)\n                combined.loc[combined['PassengerId'] == row['PassengerId'],\n                            'Surv_Rate_Invalid'] = 0\n\nfor _, grp_df in combined[['Survived', 'Surname', 'Fare', 'Ticket', 'PassengerId', 'Family_Friends_Surv_Rate']].groupby('Ticket'):\n    if (len(grp_df) > 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Friends_Surv_Rate'] == 0.) | (row['Family_Friends_Surv_Rate'] == MEAN_SURVIVAL_RATE):\n                if(grp_df['Survived'].isnull().sum() != len(grp_df)):\n                    combined.loc[combined['PassengerId'] == row['PassengerId'],\n                                'Family_Friends_Surv_Rate'] = round(grp_df['Survived'].mean(), 4)\n                    combined.loc[combined['PassengerId'] == row['PassengerId'],\n                                'Surv_Rate_Invalid'] = 0\n\n\n#Clean the Cabin column\ncombined['Cabin'] = combined['Cabin'].astype('category')\ncombined['Cabin'] = combined['Cabin'].cat.add_categories('U')\ncombined['Cabin_Clean'] = combined['Cabin'].fillna('U')\ncombined['Cabin_Clean'] = combined['Cabin_Clean'].str.strip(' ').str[0]\n\n# Label Encoding of the Cabin\ncabin_dict = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\ncombined['Cabin_Clean'] = combined['Cabin_Clean'].map(cabin_dict).astype('int')\n\n#Target encoding of the Cleaned cabin column\ncombined['Cabin_for_te'] = combined['Cabin_Clean'].astype('category')\ncombined['Cabin_TE'] = encoder.fit_transform(combined.iloc[:891].Cabin_for_te, target)\ncombined.drop(columns=['Cabin_for_te'], inplace=True)\n\n#Clean the ticket column\ndef clean_ticket(each_ticket):\n    prefix = re.sub(r'[^a-zA-Z]', '', each_ticket)\n    if(prefix):\n        return prefix\n    else:\n        return \"NUM\"\n\ncombined[\"Tkt_Clean\"] = combined.Ticket.apply(clean_ticket)\n\n#Create ticket frequency column\ncombined['Ticket_Frequency'] = combined.groupby('Ticket')['Ticket'].transform('count')\n\n#Create ticket groups\nTicket_Count = dict(combined['Ticket'].value_counts())\ncombined['TicketGroup'] = combined['Ticket'].map(Ticket_Count)\n\ndef Ticket_Label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 8)) | (s == 1):\n        return 1\n    elif (s > 8):\n        return 0\n\ncombined['TicketGroup'] = combined['TicketGroup'].apply(Ticket_Label)\n\n#Create fare bins\ndef fare_cat(fare):\n    if fare <= 7.0:\n        return 1\n    elif fare <= 39 and fare > 7.0:\n        return 2\n    e...",
      "url": "https://shainis.quarto.pub/posts/Titanic"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Feature Engineering and Algorithm Accuracy for the Titanic Dataset",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5891cfa5a4ac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Feature Engineering and Algorithm Accuracy for the Titanic Dataset\n\n[![Felipe Sanchez Garz\u00f3n](https://miro.medium.com/v2/resize:fill:88:88/1*Fpa3v30c-WJi4yYDsz5xng.jpeg)](https://fesan818181.medium.com/?source=post_page-----5891cfa5a4ac--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Felipe Sanchez Garz\u00f3n](https://fesan818181.medium.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff241c993c58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&user=Felipe+Sanchez+Garz%C3%B3n&userId=ff241c993c58&source=post_page-ff241c993c58----5891cfa5a4ac---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n\u00b7\n\n4 min read\n\n\u00b7\n\nAug 15, 2019\n\n--\n\nShare\n\nOne of the most popular dataset for Machine Learning correspond to the Titanic accident\n\nHere we are playing with features within this dataset, trying to discover the effect of the choice of differente features in the accuracy of some basic ML algorithms. These features correspond to the head of the data\n\nthe head of the dataset (first 5 rows)\n\nThe dataset presents if a passanger survived (1) or not (0) and the description of the passanger in several columns such as his class (Pclass), his Age, etc. The description of the features is in the next table [(extracted from kaggle)](https://www.kaggle.com/c/titanic):\n\nThe first graphical exploratory analysis we did, shows some relationship between the features\n\nWe can close up it for some special features that are special, such as SibSp and Parch\n\n[![Felipe Sanchez Garz\u00f3n](https://miro.medium.com/v2/resize:fill:144:144/1*Fpa3v30c-WJi4yYDsz5xng.jpeg)](https://fesan818181.medium.com/?source=post_page-----5891cfa5a4ac--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff241c993c58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&user=Felipe+Sanchez+Garz%C3%B3n&userId=ff241c993c58&source=post_page-ff241c993c58----5891cfa5a4ac---------------------follow_profile-----------)\n\n[**Written by Felipe Sanchez Garz\u00f3n**](https://fesan818181.medium.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[697 Followers](https://fesan818181.medium.com/followers?source=post_page-----5891cfa5a4ac--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\nPhD, Forecasting Analyst / Data Scientist writing about data applied to business : Investments, Risk Management, Supply Chain, Agile Project Managenet.\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff241c993c58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac&user=Felipe+Sanchez+Garz%C3%B3n&userId=ff241c993c58&source=post_page-ff241c993c58----5891cfa5a4ac---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----5891cfa5a4ac--------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----5891cfa5a4ac--------------------------------)",
      "url": "https://towardsdatascience.com/feature-engineering-and-algorithm-accuracy-for-the-titanic-dataset-5891cfa5a4ac?gi=363c76b5d1b9"
    },
    {
      "title": "GitHub - Alpha6849/-Titanic-Survival-Prediction: End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.",
      "text": "<div><div><article><p></p><h2>-Titanic-Survival-Prediction</h2><a href=\"#-titanic-survival-prediction\"></a><p></p>\n<p>End-to-end machine learning solution for the Titanic dataset using data cleaning, feature engineering, model tuning, and final Kaggle submission with a public score of 0.76555.</p>\n<p></p><h2>\ud83d\udea2 Titanic Survival Prediction - Random Forest Based ML Pipeline</h2><a href=\"#-titanic-survival-prediction---random-forest-based-ml-pipeline\"></a><p></p>\n<p>This is a complete end-to-end machine learning project built on the famous <strong>Titanic: Machine Learning from Disaster</strong> dataset on Kaggle. The goal is to predict passenger survival using classification models, with a focus on <strong>data preprocessing</strong>, <strong>feature engineering</strong>, and <strong>model tuning</strong>.</p>\n<hr/>\n<p></p><h2>\ud83d\udccc Project Description</h2><a href=\"#-project-description\"></a><p></p>\n<p>The Titanic dataset is a widely used beginner dataset for classification problems in machine learning. In this project, we went through all key stages of the ML pipeline:</p>\n<ul>\n<li>Exploratory Data Analysis</li>\n<li>Data Cleaning &amp; Imputation</li>\n<li>Feature Engineering</li>\n<li>Model Selection and Hyperparameter Tuning</li>\n<li>Final Prediction and Kaggle Submission</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udd0d Phases Breakdown</h2><a href=\"#-phases-breakdown\"></a><p></p>\n<p></p><h3>\u2705 Phase 1: Data Cleaning</h3><a href=\"#-phase-1-data-cleaning\"></a><p></p>\n<ul>\n<li>Dropped non-informative columns: <code>Name</code>, <code>Ticket</code>, <code>Cabin</code></li>\n<li>Filled missing values in:\n<ul>\n<li><code>Age</code>: with median grouped by <code>Pclass</code> and <code>Sex</code></li>\n<li><code>Embarked</code>: with mode</li>\n<li><code>Fare</code> in test set: with median</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 2: Feature Engineering</h3><a href=\"#-phase-2-feature-engineering\"></a><p></p>\n<ul>\n<li>Created <code>FamilySize</code> as <code>SibSp + Parch + 1</code></li>\n<li>Extracted <code>Title</code> from the <code>Name</code> column before dropping it</li>\n<li>Mapped categorical columns (<code>Sex</code>, <code>Embarked</code>, <code>Title</code>) using <strong>Label Encoding</strong></li>\n</ul>\n<p></p><h3>\u2705 Phase 3: Model Training &amp; Tuning</h3><a href=\"#-phase-3-model-training--tuning\"></a><p></p>\n<ul>\n<li>Tried baseline models including Logistic Regression and XGBoost</li>\n<li>Observed XGBoost underperformed (~0.60 accuracy)</li>\n<li>Final model: <strong>Random Forest Classifier</strong>\n<ul>\n<li>Tuned using <code>GridSearchCV</code></li>\n<li>Best parameters selected based on cross-validation</li>\n<li>Trained on full training set and predicted on test set</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>\u2705 Phase 4: Final Submission</h3><a href=\"#-phase-4-final-submission\"></a><p></p>\n<ul>\n<li>Predictions were exported as <code>submission_final.csv</code></li>\n<li>Submitted to Kaggle for evaluation</li>\n</ul>\n<p></p><h2>\ud83c\udfc6 Kaggle Performance</h2><a href=\"#-kaggle-performance\"></a><p></p>\n<ul>\n<li><strong>Final Kaggle Score</strong>: <code>0.76555</code></li>\n<li><strong>Kaggle Rank</strong>: ~11,000 (Top 30\u201335% range at time of submission)</li>\n<li><strong>Final Model</strong>: Random Forest Classifier with GridSearchCV tuning</li>\n</ul>\n<p></p><h2>\ud83d\udcc1 Project Structure</h2><a href=\"#-project-structure\"></a><p></p>\n<p>\u251c\u2500\u2500 data/\n\u2502 \u251c\u2500\u2500 train.csv\n\u2502 \u2514\u2500\u2500 test.csv\n\u251c\u2500\u2500 titanic_model.ipynb &lt;- All model development in Jupyter\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission_final.csv\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt</p>\n<p></p><h2>\ud83c\udf93 What I Learned</h2><a href=\"#-what-i-learned\"></a><p></p>\n<ul>\n<li>How to clean and preprocess real-world datasets with missing values</li>\n<li>Feature extraction using domain knowledge (like <code>Title</code>, <code>FamilySize</code>)</li>\n<li>Label encoding vs OneHotEncoding choices</li>\n<li>Why some models (e.g. XGBoost) may not always outperform others</li>\n<li>Hyperparameter tuning using <code>GridSearchCV</code></li>\n<li>Submitting predictions on Kaggle and interpreting scores</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcbc Future Improvements</h2><a href=\"#-future-improvements\"></a><p></p>\n<ul>\n<li>Try ensemble techniques (Voting, Stacking)</li>\n<li>Add feature importance plots for interpretability</li>\n<li>Experiment with other preprocessing pipelines</li>\n</ul>\n<hr/>\n<blockquote>\n<p>Built as part of machine learning practice. Feel free to fork, reuse, or collaborate!</p>\n</blockquote>\n</article></div></div>",
      "url": "https://github.com/Alpha6849/-Titanic-Survival-Prediction"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    },
    {
      "title": "Exploratory Data Analysis and Feature Engineering Techniques",
      "text": "Hira Akram Exploratory Data Analysis and Feature Engineering Techniques https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46\nExploratory Data Analysis and Feature Engineering Techniques\nHira Akram\n2024-07-31T19:37:05Z\n# Exploratory Data Analysis and Feature Engineering Techniques\n\n## Strategies for enhanced predictive modelling\n\n[Hira Akram](https://hirakram.medium.com/?source=post_page---byline--bf5a04afff46---------------------------------------)\n\n5 min read\n\n\nJan 13, 2021\n\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n[Image Source](https://dribbble.com/ousama_alsaadi)\n\n[_Titanic Kaggle competition_](https://www.kaggle.com/c/titanic) is a great place to understand the machine learning pipeline. In this article, we will discuss the preliminary steps involved in building a predictive model. Let\u2019s begin!\n\n**_Competition Summary:_**\n\nOur aim is to determine from a set of features whether a passenger would have survived the wreck. In other words, we will build a binary classification model that outputs \u20181\u2019 stating that the passenger had survived the Titanic sinking and \u20180\u2019 for not surviving.\n\nBelow is a visualization of the survival percentage in the provided training dataset. Based on this given percentage of ground truth; we will train **five** typical classification models and compare their performances:\n\n**_Exploratory Data Analysis:_**\n\nFirst and foremost step for building a machine learning model is EDA. This involves the following:\n\n- Comprehend the underlying information like numbers of rows and columns, data types, look at a few samples from the data, general figures like mean median mode values.\n- Indicate the proportion of missing values and decide how to address them.\n- Data visualization.\n\nSo let\u2019s explore the training set and establish an understanding of our data:\n\nWe gather the following information from the above code:\n\n- Training data comprises a total of 891 examples and 12 features; including the \u2018 _Survived_\u2019 flag.\n- We have a mix of discrete and continuous features.\n- There are three columns with missing values (Age, Cabin, Embarked)\n\nMoving on, now let\u2019s discuss each feature in detail to get a better intuition.\n\n1. **_Pclass_**\n\nOur first feature to consider is **Pclass.** This is a categorical feature and it represents the socio-economic status of the passengers. We have plotted a treemap which clearly illustrates that the majority of the passengers belonged to the _upper_ socio-economic status.\n\n. **_Name_**\n\nSince the passenger names can not be used as is; therefore we have extracted the titles within the data to make more sense of this information.\n\n. **_Embarked_**\n\nThere were two missing values in this columns. We can either remove these rows or replace them with the mode value. Deleting rows would ultimately lead to date loss and given dataset is already very small; therefore we will go with option two. We also noticed that a high number of passengers embarked on _Southampton._\n\nPress enter or click to view image in full size\n\n. **_Cabin_**\n\nAlthough a huge number of values were missing from this column, but we extracted the alphabets specifying the cabin category. I will shortly discuss how we can make use of this to recreate features.\n\n. **_Age_**\n\nBelow code approximates the missing age values using _IterativeImputer()_ function _._ Later on, we will utilize this column to build more complex features.\n\n**_Feature Engineering_**\n\nThis method is used for adding complexity to the existing data; by constructing new features on top of the pre-existing columns. In order to better represent the underlying structure of the data; we can either split or merge information from different columns. This is one of the many techniques that can enhance our model performance altogether.\n\nIn the Titanic dataset we will include **six** additional columns using this method. Just like we carefully transformed all the passenger names into their titles _;_ let\u2019s look at some more columns.\n\nFirstly, we will make use of the _Parch_ and _SibSp_ columns to count the number of family members that are on board, as below:\n\nMoreover, an attribute like _Fare per Family_ can also help understand the link between these two columns:\n\n_Ticket_ column in itself doesn\u2019t seem quite useful, so we will construct a categorical column based on the nature of this data:\n\nConcatenate the above-created column with the _Cabin_ column that we transformed earlier, to gain some more value out of these features:\n\nThe following two features can also be built using the _Age_ column. For _ageBins,_ we will split the ages into four discrete equal-sized buckets using the pandas _qcut()_ function.\n\nLastly, we will multiply _Age_ with _Fare_ to make a new _numerical_ column.\n\nInitially, the training dataset contained very simple features which were insufficient for the algorithm to search for patterns. Newly constructed features are designed to establish a correspondence between the dataset which ultimately enhances the model accuracy.\n\nNow that our features are ready, we can move on to the next steps that are involved in data preparation.\n\n**_Categorical Encoding_**\n\nSince many algorithms can only interpret numerical data, therefore, encoding the categorical features is an essential step. There are three common workarounds for encoding such features:\n\n1. One Hot Encoding (binary split)\n2. Label Encoding (numerical assignment to each category)\n3. Ordinal Encoding (ordered assignment to each category)\n\nIn our case, we will implement **label encoding** using sklearn _LabelEncoder()_ function. We will label encode the below-mentioned list of columns that contain categorical information in form of words, alphabets or alphanumeric. This task can be achieved by using the following code:\n\n\n['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'familyMembers', 'ticketInfo', 'Cabin', 'Cabin_ticketInfo']\n\n\n**_Standard Scaling_**\n\nContinuous features can largely vary, resulting in very slow convergence hence impacting the model performance on the whole. The term scaling suggests, that we redistribute the feature values upon a fixed range so that the data dispersion is standardized. Once this is achieved, the model will efficiently converge to the global minima.\n\nOriginally, the Titanic data only had a single feature i.e. Fare which required scaling. While constructing new advanced features we included two more. Here\u2019s how we\u2019ll carry out standard scaling on all the continuous data:\n\nBelow is a comparison between unscaled and scaled _\u2018Fare\u2019_ values. We can see that new _x range_ has approximately shrunk to \u00b12, also data is now distributed on zero _mean:_\n\nPress enter or click to view image in full size\n\n**_Conclusion_**\n\nNow that we feel confident about our data, let\u2019s use it to train a bunch of models and pick the algorithm which suits best! In the next article, we will make survival predictions on the Titanic dataset using five binary classification algorithms.\n\nHere are a few samples from the finalized training data:\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Titanic](https://medium.com/tag/titanic?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Data Preprocessing](https://medium.com/tag/data-preprocessing?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Feature Engineering](https://medium.com/tag/feature-engineering?source=post_page-----bf5a04afff46---------------------------------------)\n\nFollow\n\n[**Published in Towards AI**](https://pub.towardsai.net/?source=post_page---post_publication_info--bf5a04afff46---------------------------------------)\n\n[86K followers](https://pub.towardsai.net/followers?source=post_page---post_publication_info--bf5a04afff46----------------------...",
      "url": "https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46"
    }
  ]
}