# Titanic Survival Prediction - Strategy Guide (Loop 4)

## Current Status
- Best CV score: 0.8372 from exp_001 (Voting Ensemble)
- Best LB score: 0.7727 from exp_001
- CV-LB gap: +0.0645 (improved from +0.0732)
- Target: 1.0 (UNREALISTIC - top Kaggle solutions achieve ~0.80-0.82)
- Realistic target: LB 0.80 requires CV ~0.8510

## Response to Evaluator
- Technical verdict: TRUSTWORTHY. Stacking implementation was correct, just didn't help.
- Evaluator's top priority: "Improve the voting ensemble, not replace it." AGREE.
- Key concerns raised:
  1. Stacking underperformed (0.8293 vs 0.8372) - ADDRESSED: OOF predictions too correlated
  2. Target of 1.0 unrealistic - ACKNOWLEDGED: Focus on incremental gains toward 0.80 LB
  3. Don't submit stacking model - AGREED: Save submissions for improvements
- Evaluator suggested: Add model diversity (KNN, NaiveBayes), feature selection, interaction features
- My synthesis: Model diversity is secondary. The BIGGEST opportunity is **Family/Ticket survival rate encoding** which achieved 0.837 LB in reference kernel.

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Basic EDA
- `exploration/evolver_loop1_analysis.ipynb` - Deck, Ticket_Frequency, Name_Length analysis
- `exploration/evolver_loop3_analysis.ipynb` - Family/Ticket survival rate analysis (NEW)

**CRITICAL NEW FINDING:**
- Family_Survival_Rate (surname-based) has **0.76 correlation** with Survived
- Ticket_Survival_Rate has **0.81 correlation** with Survived
- These are the STRONGEST predictive signals available!
- 45% of test passengers share surnames with train; 36% share tickets
- This technique achieved 0.837 LB in the advanced FE kernel

## Recommended Approaches (Priority Order)

### 1. **Family/Ticket Survival Rate Encoding** (HIGHEST PRIORITY)
**Why:** Correlation of 0.76-0.81 with target - strongest signal available
**How:**
```python
# Extract surname
df['Surname'] = df['Name'].apply(lambda x: x.split(',')[0])

# Calculate survival rates from TRAIN ONLY
family_survival = train.groupby('Surname')['Survived'].mean()
ticket_survival = train.groupby('Ticket')['Survived'].mean()

# Map to both train and test
df['Family_Survival_Rate'] = df['Surname'].map(family_survival)
df['Ticket_Survival_Rate'] = df['Ticket'].map(ticket_survival)

# For passengers without matches, use default (e.g., overall survival rate 0.384)
df['Family_Survival_Rate'] = df['Family_Survival_Rate'].fillna(0.5)
df['Ticket_Survival_Rate'] = df['Ticket_Survival_Rate'].fillna(0.5)

# Also create indicator for whether rate is known
df['Family_Survival_Rate_NA'] = df['Family_Survival_Rate'].isna().astype(int)
df['Ticket_Survival_Rate_NA'] = df['Ticket_Survival_Rate'].isna().astype(int)

# Average the two rates
df['Survival_Rate'] = (df['Family_Survival_Rate'] + df['Ticket_Survival_Rate']) / 2
```
**CAUTION:** This is a form of target encoding - must be done carefully to avoid leakage:
- Calculate rates from TRAIN only
- For CV, recalculate rates within each fold (exclude validation fold)

### 2. **Outlier Removal** (MEDIUM PRIORITY)
**Why:** Top 4% kernel removes 10-11 outliers using Tukey IQR method
**How:**
```python
def detect_outliers(df, n, features):
    outlier_indices = []
    for col in features:
        Q1, Q3 = np.percentile(df[col].dropna(), [25, 75])
        IQR = Q3 - Q1
        outlier_step = 1.5 * IQR
        outliers = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outliers)
    # Keep only rows with >n outlier features
    from collections import Counter
    counts = Counter(outlier_indices)
    return [k for k, v in counts.items() if v > n]

outliers = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])
train = train.drop(outliers).reset_index(drop=True)
```
**Outliers identified:** Mostly Sage family (8 members, all died) and Fortune family (high fare)

### 3. **Hyperparameter Tuning with GridSearchCV** (MEDIUM PRIORITY)
**Why:** Top 4% kernel uses extensive GridSearchCV for each model
**How:** Focus on best-performing models (GradientBoosting, RandomForest)
```python
from sklearn.model_selection import GridSearchCV

gb_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4, 5],
    'min_samples_leaf': [50, 100, 150]
}
gsGBC = GridSearchCV(GradientBoostingClassifier(), gb_params, cv=5, scoring='accuracy')
gsGBC.fit(X_train, y_train)
```

### 4. **Interaction Features** (LOWER PRIORITY)
**Why:** Evaluator suggested, may capture non-linear relationships
**How:**
- Sex × Pclass (female in 1st class vs male in 3rd class)
- Fare_Per_Person = Fare / Ticket_Frequency
- Age × Pclass

## What NOT to Try
- **Stacking with current model set** - Already tried, underperformed (0.8293 vs 0.8372)
- **Adding more similar ensemble models** - OOF predictions too correlated
- **Complex neural networks** - Dataset too small, will overfit

## Validation Notes
- Use 10-fold StratifiedKFold for CV
- For Family/Ticket survival rate encoding: recalculate rates within each CV fold to avoid leakage
- CV-LB calibration: LB ≈ 2.55*CV - 1.37
- To achieve LB 0.80, need CV ~0.8510 (current: 0.8372, need +0.0138)

## Submission Strategy
- 2 submissions remaining
- DO NOT submit unless CV improves significantly over 0.8372
- Family/Ticket survival rate encoding is the most promising path to improvement
- If CV reaches 0.85+, submit to verify LB improvement

## Reference Kernels
- `research/kernels/yassineghouzam_titanic-top-4-with-ensemble-modeling/` - Outlier removal, GridSearchCV
- `research/kernels/gunesevitan_titanic-advanced-feature-engineering-tutorial/` - Family/Ticket survival rate (0.837 LB)
