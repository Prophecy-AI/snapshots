{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27260e24",
   "metadata": {},
   "source": [
    "# Family/Ticket Survival Rate Encoding + Voting Ensemble\n",
    "\n",
    "**Goal:** Add the strongest predictive features identified:\n",
    "- Family_Survival_Rate (surname-based): 0.76 correlation with Survived\n",
    "- Ticket_Survival_Rate: 0.81 correlation with Survived\n",
    "\n",
    "**CRITICAL:** This is target encoding - must recalculate rates within each CV fold to avoid leakage!\n",
    "\n",
    "**Reference:** Advanced FE kernel achieved 0.837 LB with this technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecdb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(name):\n",
    "    \"\"\"Extract title from name using regex\"\"\"\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def extract_surname(name):\n",
    "    \"\"\"Extract surname from name\"\"\"\n",
    "    return name.split(',')[0]\n",
    "\n",
    "def process_base_features(df):\n",
    "    \"\"\"Process base features (non-target-encoded)\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Title extraction\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', \n",
    "                                        'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    # Surname extraction (for survival rate encoding)\n",
    "    df['Surname'] = df['Name'].apply(extract_surname)\n",
    "    \n",
    "    # Family features\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df['FamilySize_Bin'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 11], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Cabin features\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'U')\n",
    "    \n",
    "    # Name_Length\n",
    "    df['Name_Length'] = df['Name'].apply(len)\n",
    "    \n",
    "    # Embarked - fill missing with mode\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # Fare - fill missing with median by Pclass\n",
    "    if df['Fare'].isna().any():\n",
    "        df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process base features\n",
    "train_processed = process_base_features(train)\n",
    "test_processed = process_base_features(test)\n",
    "\n",
    "print(\"Surname examples:\")\n",
    "print(train_processed[['Name', 'Surname']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age imputation using median by Pclass, Sex, Title\n",
    "def impute_age(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    age_medians = train_df.groupby(['Pclass', 'Sex', 'Title'])['Age'].median()\n",
    "    \n",
    "    def get_median_age(row, medians, fallback_median):\n",
    "        if pd.isna(row['Age']):\n",
    "            try:\n",
    "                return medians.loc[(row['Pclass'], row['Sex'], row['Title'])]\n",
    "            except KeyError:\n",
    "                return fallback_median\n",
    "        return row['Age']\n",
    "    \n",
    "    fallback = train_df['Age'].median()\n",
    "    train_df['Age'] = train_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    test_df['Age'] = test_df.apply(lambda x: get_median_age(x, age_medians, fallback), axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_processed, test_processed = impute_age(train_processed, test_processed)\n",
    "\n",
    "# Age_Bin feature\n",
    "def add_age_bin(df):\n",
    "    df = df.copy()\n",
    "    df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 16, 32, 48, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "    return df\n",
    "\n",
    "train_processed = add_age_bin(train_processed)\n",
    "test_processed = add_age_bin(test_processed)\n",
    "\n",
    "print(f\"Age missing: Train={train_processed['Age'].isna().sum()}, Test={test_processed['Age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "def encode_categorical(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Sex encoding\n",
    "    train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    test_df['Sex'] = test_df['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    # Embarked encoding\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    train_df['Embarked'] = train_df['Embarked'].map(embarked_map)\n",
    "    test_df['Embarked'] = test_df['Embarked'].map(embarked_map)\n",
    "    \n",
    "    # Title encoding\n",
    "    title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "    train_df['Title'] = train_df['Title'].map(title_map)\n",
    "    test_df['Title'] = test_df['Title'].map(title_map)\n",
    "    \n",
    "    # Deck encoding\n",
    "    deck_map = {'U': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "    train_df['Deck'] = train_df['Deck'].map(deck_map)\n",
    "    test_df['Deck'] = test_df['Deck'].map(deck_map)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_encoded, test_encoded = encode_categorical(train_processed, test_processed)\n",
    "print(\"Categorical encoding complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Family/Ticket Survival Rate Encoding\n",
    "# Must be calculated within CV folds to avoid leakage!\n",
    "\n",
    "def calculate_survival_rates(train_fold_df, default_rate=0.5):\n",
    "    \"\"\"\n",
    "    Calculate family and ticket survival rates from training fold only.\n",
    "    Returns dictionaries for mapping.\n",
    "    \"\"\"\n",
    "    # Family survival rate (by surname)\n",
    "    family_survival = train_fold_df.groupby('Surname')['Survived'].mean().to_dict()\n",
    "    \n",
    "    # Ticket survival rate\n",
    "    ticket_survival = train_fold_df.groupby('Ticket')['Survived'].mean().to_dict()\n",
    "    \n",
    "    return family_survival, ticket_survival\n",
    "\n",
    "def apply_survival_rates(df, family_survival, ticket_survival, default_rate=0.5):\n",
    "    \"\"\"\n",
    "    Apply survival rate encoding to a dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Map survival rates\n",
    "    df['Family_Survival_Rate'] = df['Surname'].map(family_survival)\n",
    "    df['Ticket_Survival_Rate'] = df['Ticket'].map(ticket_survival)\n",
    "    \n",
    "    # Create NA indicators BEFORE filling\n",
    "    df['Family_Survival_NA'] = df['Family_Survival_Rate'].isna().astype(int)\n",
    "    df['Ticket_Survival_NA'] = df['Ticket_Survival_Rate'].isna().astype(int)\n",
    "    \n",
    "    # Fill missing with default rate\n",
    "    df['Family_Survival_Rate'] = df['Family_Survival_Rate'].fillna(default_rate)\n",
    "    df['Ticket_Survival_Rate'] = df['Ticket_Survival_Rate'].fillna(default_rate)\n",
    "    \n",
    "    # Combined survival rate\n",
    "    df['Survival_Rate'] = (df['Family_Survival_Rate'] + df['Ticket_Survival_Rate']) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the encoding on full train to see coverage\n",
    "family_surv, ticket_surv = calculate_survival_rates(train_processed.assign(Survived=train['Survived']))\n",
    "test_with_rates = apply_survival_rates(test_processed, family_surv, ticket_surv)\n",
    "\n",
    "print(\"Test set coverage:\")\n",
    "print(f\"  Family rate known: {(test_with_rates['Family_Survival_NA'] == 0).sum()}/{len(test_with_rates)} ({100*(test_with_rates['Family_Survival_NA'] == 0).mean():.1f}%)\")\n",
    "print(f\"  Ticket rate known: {(test_with_rates['Ticket_Survival_NA'] == 0).sum()}/{len(test_with_rates)} ({100*(test_with_rates['Ticket_Survival_NA'] == 0).mean():.1f}%)\")\n",
    "print(f\"  Either known: {((test_with_rates['Family_Survival_NA'] == 0) | (test_with_rates['Ticket_Survival_NA'] == 0)).sum()}/{len(test_with_rates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8527a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base features (without survival rate encoding - those are added per fold)\n",
    "base_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "                 'Title', 'FamilySize', 'IsAlone', 'Has_Cabin', \n",
    "                 'Deck', 'FamilySize_Bin', 'Age_Bin', 'Name_Length']\n",
    "\n",
    "# Survival rate features (added per fold)\n",
    "survival_features = ['Family_Survival_Rate', 'Ticket_Survival_Rate', \n",
    "                     'Family_Survival_NA', 'Ticket_Survival_NA', 'Survival_Rate']\n",
    "\n",
    "all_features = base_features + survival_features\n",
    "\n",
    "print(f\"Base features: {len(base_features)}\")\n",
    "print(f\"Survival rate features: {len(survival_features)}\")\n",
    "print(f\"Total features: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define voting ensemble models (same as exp_001)\n",
    "models = [\n",
    "    ('lr', LogisticRegression(C=0.1, max_iter=1000, random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_leaf=4, random_state=42)),\n",
    "    ('ada', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, \n",
    "                          use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
    "]\n",
    "\n",
    "print(f\"Models in ensemble: {len(models)}\")\n",
    "for name, model in models:\n",
    "    print(f\"  - {name}: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold Stratified CV with proper survival rate encoding per fold\n",
    "print(\"Running 10-fold CV with survival rate encoding...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "y = train['Survived'].values\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(train))\n",
    "test_predictions = np.zeros(len(test))\n",
    "\n",
    "# Track individual model scores\n",
    "individual_scores = {name: [] for name, _ in models}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_encoded, y)):\n",
    "    # Get fold data\n",
    "    train_fold = train_encoded.iloc[train_idx].copy()\n",
    "    val_fold = train_encoded.iloc[val_idx].copy()\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # CRITICAL: Calculate survival rates from TRAINING FOLD ONLY\n",
    "    train_fold_with_target = train_fold.assign(Survived=y_train_fold)\n",
    "    family_surv, ticket_surv = calculate_survival_rates(train_fold_with_target)\n",
    "    \n",
    "    # Apply survival rates to train fold, val fold, and test\n",
    "    train_fold = apply_survival_rates(train_fold, family_surv, ticket_surv)\n",
    "    val_fold = apply_survival_rates(val_fold, family_surv, ticket_surv)\n",
    "    test_fold = apply_survival_rates(test_encoded.copy(), family_surv, ticket_surv)\n",
    "    \n",
    "    # Extract features\n",
    "    X_train = train_fold[all_features].values\n",
    "    X_val = val_fold[all_features].values\n",
    "    X_test = test_fold[all_features].values\n",
    "    \n",
    "    # Scale for SVC and LR\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train models and collect predictions for soft voting\n",
    "    fold_probs = np.zeros((len(X_val), 2))\n",
    "    test_fold_probs = np.zeros((len(X_test), 2))\n",
    "    \n",
    "    for name, model_template in models:\n",
    "        model = clone(model_template)\n",
    "        \n",
    "        if name in ['lr', 'svc']:\n",
    "            model.fit(X_train_scaled, y_train_fold)\n",
    "            val_prob = model.predict_proba(X_val_scaled)\n",
    "            test_prob = model.predict_proba(X_test_scaled)\n",
    "            val_pred = model.predict(X_val_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train_fold)\n",
    "            val_prob = model.predict_proba(X_val)\n",
    "            test_prob = model.predict_proba(X_test)\n",
    "            val_pred = model.predict(X_val)\n",
    "        \n",
    "        fold_probs += val_prob\n",
    "        test_fold_probs += test_prob\n",
    "        individual_scores[name].append(accuracy_score(y_val_fold, val_pred))\n",
    "    \n",
    "    # Average probabilities (soft voting)\n",
    "    fold_probs /= len(models)\n",
    "    test_fold_probs /= len(models)\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    val_pred_ensemble = (fold_probs[:, 1] >= 0.5).astype(int)\n",
    "    oof_predictions[val_idx] = val_pred_ensemble\n",
    "    \n",
    "    # Accumulate test predictions\n",
    "    test_predictions += test_fold_probs[:, 1] / kfold.n_splits\n",
    "    \n",
    "    # Calculate fold accuracy\n",
    "    fold_acc = accuracy_score(y_val_fold, val_pred_ensemble)\n",
    "    cv_scores.append(fold_acc)\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"Overall OOF Accuracy: {accuracy_score(y, oof_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual model performance\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "print(\"=\"*50)\n",
    "for name, scores in individual_scores.items():\n",
    "    print(f\"{name:5s}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "\n",
    "print(f\"\\nEnsemble: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Baseline (XGBoost):           0.8316 (+/- 0.0324)\")\n",
    "print(f\"  Voting Ensemble (exp_001):    0.8372 (+/- 0.0239)\")\n",
    "print(f\"  + Survival Rate Encoding:     {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"\\nImprovement over voting: {np.mean(cv_scores) - 0.8372:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "test_pred_binary = (test_predictions >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Baseline:         267 died, 151 survived\")\n",
    "print(f\"  Voting Ensemble:  255 died, 163 survived\")\n",
    "print(f\"  + Survival Rate:  {(test_pred_binary == 0).sum()} died, {(test_pred_binary == 1).sum()} survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and expected LB\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFeatures: {len(all_features)} total\")\n",
    "print(f\"  Base features: {len(base_features)}\")\n",
    "print(f\"  NEW Survival rate features: {len(survival_features)}\")\n",
    "print(f\"    - Family_Survival_Rate (0.76 correlation)\")\n",
    "print(f\"    - Ticket_Survival_Rate (0.81 correlation)\")\n",
    "print(f\"    - NA indicators and combined rate\")\n",
    "print(f\"\\nCV Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"\\nExpected LB (using calibration LB = 2.55*CV - 1.37):\")\n",
    "expected_lb = 2.55 * np.mean(cv_scores) - 1.37\n",
    "print(f\"  Predicted LB: {expected_lb:.4f}\")\n",
    "print(f\"  Previous best LB: 0.7727\")\n",
    "print(f\"  Expected improvement: {expected_lb - 0.7727:+.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
