# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Key findings: 891 train samples, 418 test samples, binary classification (Survived 0/1)
- Target imbalanced: 61.6% died, 38.4% survived
- Missing values: Age (20%), Cabin (77%), Embarked (2 samples)

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name
Extract titles from passenger names using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Mrs, Miss, Master
- Rare titles (group together): Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona
- Map: Mlle → Miss, Ms → Miss, Mme → Mrs
- Title is highly predictive: Master (children) and Miss/Mrs (women) have higher survival

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size categories: Single (1), Small (2-4), Large (5+) have different survival rates
- Medium-sized families had better survival (could help each other but not too many to manage)

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class passengers)
- Cabin deck letter (first character) can indicate location on ship

### Age Handling
- Fill missing Age using median by Pclass and Sex groups (more accurate than global median)
- Alternative: Use random values within (mean - std, mean + std)
- Create **AgeBand** with 5 bins: 0-16, 16-32, 32-48, 48-64, 64+
- Children (Age <= 16) had higher survival priority

### Fare Handling
- Fill missing Fare with median
- Create **FareBand** with 4 quantile bins
- Higher fare correlates with higher class and better survival

### Name Length
- **Name_length** = len(Name) - longer names may indicate nobility/higher status

## Models to Use

### Ensemble Methods (Best Performance)
Based on top kernels achieving 0.77-0.81 accuracy:

1. **Stacking (Recommended)**
   - First level: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
   - Second level: XGBoost as meta-learner
   - Use out-of-fold predictions to avoid leakage
   - Key: Use uncorrelated base models for better ensemble

2. **Voting Classifier**
   - Hard voting: Majority vote from multiple classifiers
   - Soft voting: Average predicted probabilities
   - Combine: LogisticRegression, SVC, RandomForest, GradientBoosting, XGBoost

3. **Gradient Boosting Models**
   - XGBoost: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8
   - LightGBM: Fast training, good for small datasets
   - CatBoost: Handles categorical features natively

### Individual Models (Baseline)
- RandomForestClassifier: n_estimators=100, good baseline
- GradientBoostingClassifier: Often best single model
- SVC: Works well with proper scaling
- LogisticRegression: Good interpretability

## Preprocessing

### Feature Encoding
- Sex: female=0, male=1 (or vice versa)
- Embarked: S=0, C=1, Q=2 (or one-hot encoding)
- Title: Numerical mapping (Mr=1, Miss=2, Mrs=3, Master=4, Rare=5)

### Features to Drop
- PassengerId (identifier only)
- Name (after extracting Title)
- Ticket (complex alphanumeric, hard to use)
- Cabin (after extracting Has_Cabin and deck)

### Final Feature Set (Recommended)
- Pclass, Sex, Age (binned), Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin

## Validation Strategy

### Cross-Validation
- Use StratifiedKFold with k=5 or k=10
- Stratification important due to class imbalance
- CV score may differ from LB score due to train/test distribution differences

### Hyperparameter Tuning
- GridSearchCV or RandomizedSearchCV
- Focus on: n_estimators, max_depth, learning_rate, min_samples_split

## Key Insights from Top Kernels

1. **Feature engineering matters more than model selection** for this dataset
2. **Simple models with good features** can match complex ensembles
3. **Title extraction** is one of the most important engineered features
4. **Family size** features capture important survival dynamics
5. **Stacking with diverse base models** achieves best results (~0.808 LB)
6. **Hard voting classifier** with tuned models achieves ~0.78 LB

## Achieving High Accuracy

To beat the target score of 1.0 (100% accuracy):
- This is an extremely challenging target as even top solutions achieve ~0.80-0.82
- Focus on:
  1. Comprehensive feature engineering (all features mentioned above)
  2. Multiple model ensemble (stacking preferred)
  3. Careful hyperparameter tuning
  4. Cross-validation to avoid overfitting
  5. Consider pseudo-labeling with high-confidence predictions
  6. Try different random seeds and ensemble predictions

## Implementation Priority

1. **First**: Implement all feature engineering (Title, FamilySize, IsAlone, Has_Cabin, Age/Fare bins)
2. **Second**: Train multiple diverse models (RF, XGB, LGB, CatBoost, SVC)
3. **Third**: Create stacking ensemble with XGBoost meta-learner
4. **Fourth**: Tune hyperparameters with cross-validation
5. **Fifth**: Try voting ensemble as alternative
