## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Binary classification problem predicting survival (0/1)
- Evaluation metric: Accuracy (percentage of correct predictions)
- Target score to beat: 1.0 (100% accuracy) - extremely ambitious, typical top scores are 80-85%

## Feature Engineering (Critical for High Scores)

Feature engineering is MORE important than model selection for this dataset. Top solutions emphasize creative feature extraction.

### Title Extraction from Name (MOST IMPORTANT)
Extract title from Name field - this is one of the most predictive features:
```python
# Extract title using regex
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
```
- Common titles: Mr, Mrs, Miss, Master, Dr, Rev, etc.
- Group rare titles into 'Rare' category
- 'Master' title indicates young male children (high survival ~57%)
- Title captures both gender and social status information
- Map titles: {'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs', 'Lady':'Rare', 'Countess':'Rare', etc.}

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- **FamilySizeGroup**: Small (2-4) vs Large (>4) vs Alone (1)
- Family size of 2-4 has higher survival rates than alone or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates wealth/class)
- **Deck** = First letter of Cabin (A, B, C, D, E, F, G) - deck position matters
- Passengers with cabins had ~67% survival vs ~30% without

### Binning Continuous Variables
- **AgeBin**: Bin Age into categories (Child <16, Teen 16-32, Adult 32-48, Senior >48)
- **FareBin**: Bin Fare into quartiles or quintiles
- Binning helps tree models and reduces noise

### Other Features
- **Name_length**: Length of name (correlates with social status)
- **Ticket_prefix**: Extract alphabetic prefix from ticket number
- **Ticket_frequency**: Count of passengers sharing same ticket (group travel)
- **Fare_per_person**: Fare / (number of people on same ticket)

### Interaction Features
- **Sex_Pclass**: Combine Sex and Pclass (female_1, female_2, female_3, male_1, etc.)
- **Age_Pclass**: Age group combined with class

## Missing Value Handling
- **Age**: Fill with median grouped by (Pclass, Sex, Title) for better accuracy
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median grouped by Pclass
- **Cabin**: Create Has_Cabin feature, then can drop original

## Models for Binary Classification

### Single Models (Baseline)
- **Random Forest**: n_estimators=100-500, max_depth=6-8, good baseline ~82-84%
- **Gradient Boosting**: Often outperforms RF slightly
- **XGBoost**: n_estimators=2000, max_depth=4, learning_rate=0.01-0.1
- **LightGBM**: Fast training, good for hyperparameter search
- **CatBoost**: Handles categorical features natively
- **SVM**: With RBF kernel, requires feature scaling
- **Logistic Regression**: Good interpretability, ~80% accuracy

### Ensemble Methods (For Higher Scores)

#### Voting Classifier
Combine multiple models with soft voting:
- RandomForest + GradientBoosting + XGBoost + SVC + LogisticRegression
- Use soft voting (probability-based) for better results

#### Stacking (Recommended for Best Scores)
Two-level stacking approach from top kernels:

1. **First Level (Base Models)**:
   - Random Forest (n_estimators=500, max_depth=6, min_samples_leaf=2)
   - Extra Trees (n_estimators=500, max_depth=8, min_samples_leaf=2)
   - AdaBoost (n_estimators=500, learning_rate=0.75)
   - Gradient Boosting (n_estimators=500, max_depth=5, learning_rate=0.1)
   - SVC (kernel='rbf', probability=True, C=1.0)

2. **Second Level (Meta-Learner)**:
   - XGBoost (n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8, colsample_bytree=0.8)
   - Or Logistic Regression for simpler meta-learner
   - Use out-of-fold predictions from first level as features

3. **Out-of-Fold Prediction Process**:
   - Use K-Fold (k=5) cross-validation
   - For each fold, train on k-1 folds, predict on held-out fold
   - Concatenate predictions to form new feature set
   - This prevents data leakage

### Key Insight: Model Diversity
- Uncorrelated base models produce better ensemble results
- Mix tree-based (RF, GBM) with linear (LogReg) and kernel (SVM) methods
- Check correlation between model predictions - low correlation = better ensemble

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV with cross-validation
- Key parameters for tree models: max_depth, n_estimators, min_samples_leaf, max_features
- For XGBoost: learning_rate, max_depth, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda
- For SVM: C, gamma, kernel

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Check feature importances from tree models
- Most important features typically: Sex, Title, Pclass, Fare, Age, FamilySize
- Remove highly correlated features (keep one)

## Validation Strategy
- Use Stratified K-Fold (k=5 or k=10) to maintain class balance
- Small dataset (891 samples) - be careful of overfitting
- Cross-validation score is more reliable than single train/test split
- Trust CV score over public leaderboard score

## Domain Knowledge
- "Women and children first" - Sex and Age are critical
- Higher class passengers had better access to lifeboats
- Embarked port correlates with class/wealth (Cherbourg had more 1st class)
- Family dynamics matter - small families survived better
- Crew members mostly died (not in dataset but context helps)

## Code Patterns

### Feature Engineering Pipeline
1. Combine train and test for consistent encoding
2. Extract Title from Name
3. Create FamilySize, IsAlone, Has_Cabin, Deck
4. Fill missing values (Age by Title/Pclass/Sex groups)
5. Bin Age and Fare
6. Create interaction features
7. Encode categorical variables (LabelEncoder or OneHotEncoder)
8. Drop unused columns (Name, Ticket, Cabin, PassengerId)

### Recommended Feature Set
Final features to use:
- Pclass, Sex_Code, Age (or AgeBin), Fare (or FareBin)
- Title_Code, FamilySize, IsAlone
- Embarked_Code, Has_Cabin
- Optional: Deck, Ticket_frequency, interaction features

## Tips for Achieving High Accuracy
1. **Feature engineering is key** - Title extraction alone can boost accuracy 3-5%
2. **Ensemble diverse models** - Stacking typically adds 1-3% over single models
3. **Tune hyperparameters carefully** - Use cross-validation, not single split
4. **Don't overfit** - Small dataset is prone to overfitting; regularize models
5. **Try multiple random seeds** - Results can vary with initialization
6. **Combine original features with engineered features** in stacking
7. **Use probability predictions** for soft voting ensembles

## Expected Performance
- Basic model (RF with raw features): ~78-80%
- With feature engineering: ~82-84%
- With stacking ensemble: ~84-86%
- Top public scores: ~85-87%
- Perfect score (1.0) is extremely difficult - may require perfect feature engineering or luck
