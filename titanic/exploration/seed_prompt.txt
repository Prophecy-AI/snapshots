## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Binary classification problem predicting survival (0/1)
- Evaluation metric: Accuracy (percentage of correct predictions)
- Target score to beat: 1.0 (100% accuracy)

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title from Name field - this is one of the most predictive features:
- Common titles: Mr, Mrs, Miss, Master, Dr, Rev, etc.
- Group rare titles into 'Rare' category
- 'Master' title indicates young male children (high survival)
- Title captures both gender and social status information

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size of 2-4 has higher survival rates than alone or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates wealth/class)
- Cabin deck letter can be extracted (A, B, C, etc.) - deck position matters

### Binning Continuous Variables
- **AgeBin**: Bin Age into categories (e.g., Child, Teen, Adult, Senior)
- **FareBin**: Bin Fare into quartiles or quintiles
- Binning helps tree models and reduces noise

### Other Features
- **Name_length**: Length of name (correlates with social status)
- **Ticket prefix**: Extract alphabetic prefix from ticket number

## Missing Value Handling
- **Age**: Fill with median (grouped by Pclass and Sex for better accuracy)
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median (grouped by Pclass)
- **Cabin**: Create Has_Cabin feature, then can drop

## Models for Binary Classification

### Single Models (Baseline)
- **Random Forest**: n_estimators=100-500, max_depth=6-8, good baseline ~82-84%
- **Gradient Boosting**: Often outperforms RF slightly
- **XGBoost**: n_estimators=2000, max_depth=4, learning_rate=0.01-0.1
- **SVM**: With RBF kernel, requires feature scaling
- **Logistic Regression**: Good interpretability, ~80% accuracy

### Ensemble Methods (For Higher Scores)

#### Voting Classifier
Combine multiple models with soft voting:
- RandomForest + GradientBoosting + XGBoost + SVC + LogisticRegression

#### Stacking (Recommended for Best Scores)
Two-level stacking approach:
1. **First Level (Base Models)**:
   - Random Forest (n_estimators=500, max_depth=6)
   - Extra Trees (n_estimators=500, max_depth=8)
   - AdaBoost (n_estimators=500, learning_rate=0.75)
   - Gradient Boosting (n_estimators=500, max_depth=5)
   - SVC (with probability=True)

2. **Second Level (Meta-Learner)**:
   - XGBoost (n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8)
   - Use out-of-fold predictions from first level as features

3. **Out-of-Fold Prediction Process**:
   - Use K-Fold (k=5) cross-validation
   - For each fold, train on k-1 folds, predict on held-out fold
   - Concatenate predictions to form new feature set
   - This prevents data leakage

### Key Insight: Model Diversity
- Uncorrelated base models produce better ensemble results
- Mix tree-based (RF, GBM) with linear (LogReg) and kernel (SVM) methods

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV
- Key parameters for tree models: max_depth, n_estimators, min_samples_leaf
- For XGBoost: learning_rate, max_depth, subsample, colsample_bytree, gamma

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Check feature importances from tree models
- Most important features typically: Sex, Title, Pclass, Fare, Age

## Validation Strategy
- Use Stratified K-Fold (k=5 or k=10) to maintain class balance
- Small dataset (891 samples) - be careful of overfitting
- Cross-validation score is more reliable than single train/test split

## Domain Knowledge
- "Women and children first" - Sex and Age are critical
- Higher class passengers had better access to lifeboats
- Embarked port correlates with class/wealth
- Family dynamics matter - small families survived better

## Code Patterns

### Feature Engineering Pipeline
1. Combine train and test for consistent encoding
2. Extract Title from Name
3. Create FamilySize, IsAlone, Has_Cabin
4. Fill missing values
5. Bin Age and Fare
6. Encode categorical variables (LabelEncoder or OneHotEncoder)
7. Drop unused columns (Name, Ticket, Cabin, PassengerId)

### Stacking Implementation
```
# Pseudo-code structure:
# 1. Define base models with parameters
# 2. Generate OOF predictions for each base model
# 3. Stack OOF predictions as new features
# 4. Train meta-learner on stacked features
# 5. Generate final predictions
```

## Tips for Achieving High Accuracy
1. **Feature engineering is key** - Title extraction alone can boost accuracy significantly
2. **Ensemble diverse models** - Stacking typically adds 1-3% over single models
3. **Tune hyperparameters carefully** - Use cross-validation
4. **Don't overfit** - Small dataset is prone to overfitting
5. **Try multiple random seeds** - Results can vary with initialization
