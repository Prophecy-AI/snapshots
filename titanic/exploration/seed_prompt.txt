# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Key findings: 891 train samples, 418 test samples, binary classification (Survived 0/1)
- Target imbalanced: 61.6% died, 38.4% survived
- Missing values: Age (20%), Cabin (77%), Embarked (2 samples)

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name (MOST IMPORTANT)
Extract titles from passenger names using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Mrs, Miss, Master
- Rare titles (group together): Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona
- Map: Mlle → Miss, Ms → Miss, Mme → Mrs
- Title is highly predictive: Master (children) and Miss/Mrs (women) have higher survival

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size categories: Single (1), Small (2-4), Large (5+) have different survival rates
- Medium-sized families had better survival (could help each other but not too many to manage)

### Cabin Features (Advanced)
- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates higher class passengers)
- **Deck** = First character of Cabin (A, B, C, D, E, F, G, T), fill missing with "U" (Unknown)
- Deck survival rates vary: B (74%), D/E (~75%), U (30%)
- **Cabin_Number** = Numeric part of cabin, can be binned (0-50: 45%, 51-100: 60%)

### Age Handling (Advanced)
- **Best approach**: Use XGBoost to predict missing Age values based on other features
- **Alternative**: Fill missing Age using median by Pclass and Sex groups
- **Simple**: Use random values within (mean - std, mean + std)
- Create **AgeBand** with 5 bins: 0-16, 16-32, 32-48, 48-64, 64+
- Children (Age <= 16) had higher survival priority

### Fare Handling
- Fill missing Fare with median (or median by Pclass and Embarked)
- Create **FareBand** with 4 quantile bins
- Higher fare correlates with higher class and better survival

### Additional Features
- **Name_length** = len(Name) - longer names may indicate nobility/higher status
- **Ticket_Prefix** = Extract prefix from Ticket (letters before numbers)
- **Ticket_Len** = Length of ticket number

## Models to Use

### Ensemble Methods (Best Performance - 81-84% accuracy)
Based on top solutions and research:

1. **Stacking (Recommended - achieves ~0.808-0.84)**
   - First level: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
   - Second level: XGBoost as meta-learner
   - Use out-of-fold predictions to avoid leakage
   - Key: Use uncorrelated base models for better ensemble

2. **Soft Voting Classifier (achieves ~0.81)**
   - Average predicted probabilities from multiple classifiers
   - Combine: LogisticRegression, SVC, RandomForest, GradientBoosting, XGBoost
   - Better than hard voting for this problem

3. **Gradient Boosting Models**
   - XGBoost: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8
   - LightGBM: Fast training, good for small datasets
   - CatBoost: Handles categorical features natively

### Individual Models (Baseline ~0.77-0.80)
- RandomForestClassifier: n_estimators=100, good baseline
- GradientBoostingClassifier: Often best single model
- SVC: Works well with proper scaling
- LogisticRegression: Good interpretability

## Preprocessing

### Feature Encoding
- Sex: female=0, male=1 (or vice versa)
- Embarked: S=0, C=1, Q=2 (or one-hot encoding)
- Title: Numerical mapping (Mr=1, Miss=2, Mrs=3, Master=4, Rare=5)
- Deck: Numerical mapping or one-hot encoding

### Features to Drop
- PassengerId (identifier only)
- Name (after extracting Title and Name_length)
- Ticket (after extracting prefix if used)
- Cabin (after extracting Has_Cabin, Deck, Cabin_Number)

### Final Feature Set (Recommended)
- Pclass, Sex, Age (binned), Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin, Deck

## Validation Strategy

### Cross-Validation
- Use StratifiedKFold with k=5 or k=10 (10-fold recommended for small dataset)
- Stratification important due to class imbalance
- CV score may differ from LB score due to train/test distribution differences

### Hyperparameter Tuning
- **Bayesian Optimization** (recommended) - more efficient than grid search
- GridSearchCV or RandomizedSearchCV as alternatives
- Focus on: n_estimators, max_depth, learning_rate, min_samples_split

### Feature Selection
- Use SelectFromModel with RandomForest or XGBoost
- Recursive Feature Elimination (RFE) with cross-validation
- Feature importance analysis to identify key predictors

## Key Insights from Top Solutions

1. **Feature engineering matters more than model selection** for this dataset
2. **Simple models with good features** can match complex ensembles
3. **Title extraction** is one of the most important engineered features
4. **Family size** features capture important survival dynamics
5. **Stacking with diverse base models** achieves best results (~0.808-0.84 LB)
6. **Soft voting classifier** with tuned models achieves ~0.81 LB
7. **XGBoost for Age imputation** improves predictions vs simple median

## Realistic Expectations

- **Top public LB scores**: 0.81-0.84 (81-84% accuracy)
- **Target score of 1.0 (100%)** is extremely challenging
- Best approach: Comprehensive feature engineering + ensemble methods
- Focus on maximizing accuracy through:
  1. All feature engineering techniques mentioned
  2. Multiple diverse models in ensemble
  3. Careful hyperparameter tuning with Bayesian optimization
  4. Multiple random seeds and averaging predictions

## Implementation Priority

1. **First**: Implement all feature engineering (Title, FamilySize, IsAlone, Has_Cabin, Deck, Age/Fare bins)
2. **Second**: Train multiple diverse models (RF, XGB, LGB, CatBoost, SVC, LogReg)
3. **Third**: Create stacking ensemble with XGBoost meta-learner
4. **Fourth**: Tune hyperparameters with Bayesian optimization or GridSearchCV
5. **Fifth**: Try soft voting ensemble as alternative
6. **Sixth**: Experiment with different random seeds and average predictions
