# Titanic Survival Prediction - Evolved Strategy (Loop 1)

## Current Status
- Best CV score: 0.8271 from exp_000 (RandomForest baseline)
- Best LB score: Not yet submitted (0/10 submissions used)
- CV-LB gap: Unknown - **NEED TO SUBMIT TO ESTABLISH BASELINE**

## Response to Evaluator
- Technical verdict: TRUSTWORTHY. Execution is sound, CV methodology is correct.
- Evaluator's top priority: **Submit the current baseline immediately** to get LB feedback. I AGREE.
- Key concerns raised:
  1. **Target of 1.0 is impossible** - ACKNOWLEDGED. Top Titanic solutions achieve 81-84% accuracy. The current CV of 82.71% is already near the ceiling. We will pursue maximum possible accuracy but recognize 100% is not achievable.
  2. **No submission yet** - ADDRESSING NOW. First action should be to submit baseline.
  3. **Ensemble methods not tried** - Will be the focus after establishing LB baseline.

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Basic EDA, target distribution (61.6% died, 38.4% survived)
- `exploration/evolver_loop1_analysis.ipynb` - Hard case analysis

Key patterns to exploit:
1. **Easy cases** (high confidence): Female 1st/2nd class (94.7% survival), Male 3rd class (13.5% survival)
2. **Hard cases** (need better features): Female 3rd class (50% survival), Male 1st/2nd class (27% survival)
3. **Ticket groups**: 134 tickets have multiple passengers - potential for group survival features
4. **Deck patterns**: Decks D/E/B have ~75% survival, Deck A has 47%

## Recommended Approaches (Priority Order)

### IMMEDIATE: Submit Baseline (exp_000)
- Submit the current baseline to establish LB benchmark
- This costs 1 submission but provides crucial CV-LB calibration
- Expected LB: ~0.77-0.80 based on typical CV-LB gap for Titanic

### Priority 1: Stacking Ensemble (HIGHEST VALUE)
Based on `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python`:
- **First level models**: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC
- **Second level**: XGBoost meta-learner on out-of-fold predictions
- **Key insight**: Uncorrelated base models produce better ensemble results
- **Expected improvement**: From ~82% to ~84% (0.808 LB achieved in kernel)

Implementation details:
```python
# Use 5-fold CV for out-of-fold predictions
# Each base model generates OOF predictions
# Stack OOF predictions as features for XGBoost meta-learner
# XGBoost params: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8
```

### Priority 2: Ticket-Based Features
- Create `TicketGroupSize` = count of passengers with same ticket
- Create `TicketGroupSurvivalRate` = mean survival of ticket group (careful of leakage!)
- Passengers with same ticket are likely family/traveling companions
- Group survival patterns may help with hard cases

### Priority 3: Soft Voting Ensemble
Alternative to stacking if simpler approach needed:
- Combine: LogisticRegression, SVC, RandomForest, GradientBoosting, XGBoost
- Use `voting='soft'` to average probabilities
- Typically achieves ~0.81 LB

### Priority 4: Advanced Age Imputation
- Current: Median by Pclass/Sex
- Better: Use XGBoost to predict missing Age values
- Features for Age prediction: Pclass, Sex, SibSp, Parch, Fare, Title

### Priority 5: Hyperparameter Tuning
- Only after ensemble structure is established
- Use Bayesian optimization (optuna) for efficiency
- Focus on: n_estimators, max_depth, learning_rate, min_samples_split

## What NOT to Try
- **Chasing 100% accuracy** - Impossible for this dataset
- **Complex neural networks** - Overkill for 891 samples, tree-based models dominate
- **Excessive feature engineering without validation** - Current features are comprehensive
- **Single model optimization** - Ensembles are the path to improvement

## Validation Notes
- Use 10-fold Stratified CV (current approach is sound)
- CV and LB may differ by 2-5% on Titanic due to train/test distribution differences
- After LB submission, calibrate expectations based on CV-LB gap

## Realistic Expectations
- **Ceiling**: ~84% accuracy (based on top solutions)
- **Current**: 82.71% CV
- **Gap**: ~1.3% potential improvement
- **Path**: Stacking ensemble is the most likely route to improvement

## Implementation Order
1. **FIRST**: Submit exp_000 baseline to get LB feedback
2. **SECOND**: Implement stacking ensemble with 5 base models + XGBoost meta-learner
3. **THIRD**: Add ticket-based features if stacking shows promise
4. **FOURTH**: Try soft voting as alternative ensemble
5. **FIFTH**: Hyperparameter tuning with Bayesian optimization
