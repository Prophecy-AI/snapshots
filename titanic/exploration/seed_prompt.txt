# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival rates by feature
- Key findings: 891 train samples, 418 test samples, binary classification (Survived: 0/1)
- Target is imbalanced: 61.6% died, 38.4% survived

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name
Extract title using regex: `Name.str.extract(' ([A-Za-z]+)\.', expand=False)`
- Group rare titles: Replace Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona with 'Rare'
- Normalize: Mlle→Miss, Ms→Miss, Mme→Mrs
- Map to ordinal: Mr=1, Miss=2, Mrs=3, Master=4, Rare=5
- Title is highly predictive: Master (young boys) had high survival

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size affects survival: solo travelers and very large families had lower survival

### Cabin Features
- **Has_Cabin** = 0 if Cabin is NaN, else 1 (proxy for wealth/status)
- Cabin deck can be extracted from first letter (A, B, C, etc.)
- ~77% missing - use Has_Cabin binary flag

### Fare and Age Binning
- **FareBin**: Bin fare into quartiles or custom ranges (0-7.91, 7.91-14.45, 14.45-31, 31+)
- **AgeBin**: Bin age into ranges (0-16, 16-32, 32-48, 48-64, 64+)
- Binning helps tree models find splits

### Name Length
- **Name_length** = len(Name) - can be proxy for social status

### Ticket Features
- Ticket prefix extraction (some tickets have letter prefixes)
- Ticket frequency (shared tickets indicate traveling together)

## Missing Value Handling
- **Age**: Fill with median (by Pclass and Sex for better accuracy)
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median (by Pclass)
- **Cabin**: Create Has_Cabin flag, don't impute

## Models

### Single Models (Baseline)
For this small tabular dataset, tree-based models work well:
- **Random Forest**: n_estimators=500, max_depth=6, min_samples_leaf=2, max_features='sqrt'
- **Gradient Boosting**: n_estimators=500, max_depth=5, min_samples_leaf=2, learning_rate=0.01
- **XGBoost**: Similar parameters, use early stopping
- **Extra Trees**: n_estimators=500, max_depth=6
- **Logistic Regression**: Good baseline, use regularization

### Ensemble Approaches (Higher Accuracy)

#### Voting Classifier
Combine multiple models with soft voting:
- AdaBoost, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier
- Use soft voting (probability-based) for better results

#### Stacking (Best Approach)
Two-level stacking architecture:
1. **First Level (Base Models)**: 
   - Random Forest
   - Extra Trees
   - AdaBoost
   - Gradient Boosting
   - SVC (with probability=True)
   
2. **Second Level (Meta-Classifier)**:
   - XGBoost or Logistic Regression
   
3. **Out-of-Fold Predictions**:
   - Use K-Fold (K=5) to generate out-of-fold predictions
   - This prevents overfitting when training meta-classifier
   - Each base model generates predictions on held-out fold
   - Stack these predictions as features for meta-classifier

## Validation Strategy
- Use Stratified K-Fold Cross-Validation (K=5 or K=10)
- Small dataset (891 samples) - CV is critical to avoid overfitting
- Track both train and validation scores to detect overfitting

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV
- Key parameters to tune:
  - Tree depth (max_depth): 3-8
  - Number of estimators: 100-1000
  - Learning rate (for boosting): 0.01-0.1
  - Min samples per leaf: 1-5
- Use cross-validation during tuning

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Check feature importances from tree models
- Most important features typically: Sex, Title, Pclass, Fare, Age

## Key Insights from Top Kernels
1. Sex is the strongest predictor (females 74% survival vs males 19%)
2. Pclass strongly correlates with survival (1st class: 63%, 3rd class: 24%)
3. Title extraction captures both sex and age information effectively
4. Family size has non-linear relationship with survival
5. Ensemble methods consistently outperform single models

## Submission Strategy
- Generate predictions using best ensemble model
- Consider averaging predictions from multiple models
- Threshold at 0.5 for binary classification
- Format: PassengerId, Survived (0 or 1)
