## Current Status
- Best CV score: 0.8283 from exp_000 (Baseline RF with Feature Engineering)
- Best LB score: None (no submissions yet)
- CV-LB gap: Unknown - need to submit baseline to calibrate

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Execution is sound, can proceed with strategy.
- Evaluator's top priority: Implement stacking ensemble with diverse base models. **Agree** - this is the highest-leverage improvement based on research (stacking adds 1-3% per arthurtok kernel).
- Key concerns raised:
  1. Target score of 1.0 is unrealistic → **Acknowledged**. Top Titanic scores are 85-87%. Reframing goal to "beat 0.85 on LB" which is achievable.
  2. Missing features (Deck, Age/Fare bins, interaction) → Will add these alongside stacking.
  3. No LB submission yet → Will submit baseline first to calibrate CV-LB relationship.
- Minor leakage in age imputation noted but impact is minimal per evaluator.

## Data Understanding
**Reference notebooks:**
- `exploration/eda.ipynb` - EDA with feature distributions, missing values analysis
- `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` - Stacking implementation reference

**Key patterns to exploit:**
- Sex is most predictive (females 74.2% survival vs males 18.9%)
- Title captures both gender and social status (Master indicates young male children with ~57% survival)
- Pclass strongly correlated with survival (1st: 63%, 2nd: 47%, 3rd: 24%)
- Family size 2-4 has higher survival than alone or large families
- Has_Cabin indicates wealth/class (67% survival with cabin vs 30% without)

**Important: 100% Public Leaderboard**
Per Kaggle staff discussion, Titanic uses 100% of test data for public LB. This means:
- No private/public shake-up risk
- CV-LB correlation should be reliable
- LB score is the true final score

## Recommended Approaches

### Priority 1: Submit Baseline for LB Calibration
Before implementing stacking, submit current baseline (exp_000) to establish LB benchmark.
- This uses 1 submission but provides critical CV-LB calibration
- Expected LB: ~0.78-0.80 (CV often slightly optimistic)

### Priority 2: Implement Stacking Ensemble (HIGHEST IMPACT)
Based on arthurtok kernel, implement 2-level stacking:

**First Level (5 Base Models):**
1. RandomForest (n_estimators=500, max_depth=6, min_samples_leaf=2)
2. ExtraTrees (n_estimators=500, max_depth=8, min_samples_leaf=2)
3. AdaBoost (n_estimators=500, learning_rate=0.75)
4. GradientBoosting (n_estimators=500, max_depth=5, learning_rate=0.1)
5. SVC (kernel='rbf', probability=True, C=1.0) - requires feature scaling!

**Second Level (Meta-Learner):**
- XGBoost (n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8, colsample_bytree=0.8)
- OR Logistic Regression for simpler meta-learner

**Out-of-Fold Prediction Process (CRITICAL to prevent leakage):**
```python
# For each base model:
# 1. Use K-Fold (k=5) cross-validation
# 2. For each fold, train on k-1 folds, predict on held-out fold
# 3. Concatenate predictions to form new feature set for meta-learner
# 4. Also generate test predictions by averaging across folds
```

### Priority 3: Add Missing Features
Before or alongside stacking, add:
1. **Deck** - First letter of Cabin (A, B, C, D, E, F, G, Unknown)
2. **Age binning** - Child (<16), Teen (16-32), Adult (32-48), Senior (>48)
3. **Fare binning** - Quartiles (0-7.91, 7.91-14.45, 14.45-31, >31)
4. **Sex_Pclass interaction** - female_1, female_2, female_3, male_1, etc.
5. **Ticket_frequency** - Count of passengers sharing same ticket (group travel)

### Priority 4: Hyperparameter Tuning (Lower Priority)
Only after stacking is implemented and features are mature:
- GridSearchCV on XGBoost meta-learner
- Tune base model parameters

## What NOT to Try
- Single model hyperparameter tuning before stacking (low ROI)
- Deep learning (dataset too small, 891 samples)
- Complex neural architectures (overkill for this problem)
- Chasing 100% accuracy (impossible due to inherent noise)

## Validation Notes
- Use Stratified 5-Fold CV (maintains class balance)
- Current CV: 0.8283 ± 0.0138 (fold scores: 0.803-0.844)
- Expect LB to be slightly lower than CV (typical pattern)
- After LB submission, calculate CV-LB gap to calibrate expectations

## Expected Outcomes
- Baseline LB: ~0.78-0.80
- With stacking: ~0.80-0.82 LB (84-86% CV)
- With additional features + stacking: ~0.82-0.85 LB
- Realistic ceiling: ~0.85-0.87 LB (top public scores)

## Code Reference
See `research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/` for:
- SklearnHelper class for consistent model interface
- get_oof() function for out-of-fold predictions
- Feature engineering patterns (Age/Fare binning, Title mapping)
- XGBoost meta-learner configuration
