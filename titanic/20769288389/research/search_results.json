{
  "query": "What are the best machine learning models and ensembling strategies for Titanic survival prediction to achieve high accuracy?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Titanic survival is a classic binary\u2011classification task that rewards careful feature engineering, robust hyper\u2011parameter tuning, and the use of powerful learners such as gradient\u2011boosted trees.  In recent Kaggle\u2011style notebooks the single best performing model is an XGBoost classifier that, after extensive tuning, reaches roughly\u202f93\u202f% accuracy on the held\u2011out test set\u202f([github.com](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost)).  Comparable results are obtained with Random\u202fForest and Support\u202fVector\u202fMachines when they are tuned and combined with engineered variables (e.g., family size, title extraction, age imputation)\u202f([medium.com](https://amyrmahdy.medium.com/titanic-survival-prediction-comparing-5-machine-learning-models-d1d451a9e558)).  Logistic regression remains a strong baseline when limited to a small, well\u2011chosen subset of features (Pclass, Sex, Age, SibSp, etc.) and can be boosted with gradient\u2011boosting (GBM) to improve stability\u202f([rstudio\u2011pubs\u2011static.s3.amazonaws.com](https://rstudio-pubs-static.s3.amazonaws.com/16257_4cea608eb27c4c7a8789eb3c89a86537.html)).\n\nThe most reliable way to push performance into the top\u202f5\u202f% of the competition is to ensemble several of these learners.  Stacked ensembles\u2014where predictions from Logistic Regression, Random Forest, XGBoost, and sometimes a shallow neural network are fed into a meta\u2011learner (often another Logistic Regression or Gradient Boosting model)\u2014have been shown to yield the highest Kaggle rankings and also provide model\u2011interpretability via SHAP values\u202f([github.com](https://github.com/ata-turhan/titanic-survival-prediction)).  Simpler blending (weighted averaging) of a logistic\u2011regression model with a GBM model also works well; in practice a 75\u202f%\u202fGBM\u202f+\u202f25\u202f%\u202flogistic blend achieved near\u2011identical predictions to each component (\u224894.7\u202f% agreement) and modest gains over either alone\u202f([rstudio\u2011pubs\u2011static.s3.amazonaws.com](https://rstudio-pubs-static.s3.amazonaws.com/16257_4cea608eb27c4c7a8789eb3c89a86537.html)).  Successful pipelines therefore combine: (1) rich engineered features (titles, family size, cabin decks, fare per person), (2) strong base learners (XGBoost, Random Forest, SVM), and (3) a stacking or weighted\u2011average ensemble with a linear meta\u2011model.\n\nIn practice, a high\u2011accuracy workflow for 2026 would look like: clean and impute missing values, create categorical encodings and interaction terms, train individual models with cross\u2011validated hyper\u2011parameter searches (e.g., XGBoost\u202f\u2248\u202f500 trees, learning\u2011rate\u202f0.01; Random Forest\u202f\u2248\u202f200 trees; SVM with RBF kernel), collect out\u2011of\u2011fold predictions, and fit a Logistic Regression or LightGBM meta\u2011learner on those predictions.  This stacked approach consistently outperforms any single model and is the strategy used by the top\u2011performing public repositories\u202f([github.com](https://github.com/Moddy2024/Titanic-Survival-prediction)).",
      "url": ""
    },
    {
      "title": "GitHub - mrankitgupta/titanic-survival-prediction-93-xgboost: Titanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[mrankitgupta](https://github.com/mrankitgupta)/ **[titanic-survival-prediction-93-xgboost](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n- [Star\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### License\n\n[MIT license](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE)\n\n[1\\\nstar](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/stargazers) [1\\\nfork](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/forks) [Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags) [Activity](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/activity)\n\n[Star](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n[Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n\n# mrankitgupta/titanic-survival-prediction-93-xgboost\n\nmain\n\n[Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[6 Commits](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/commits/main/) |\n| [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) | [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) |\n| [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) | [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) |\n| [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) | [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# Titanic - Machine Learning from Disaster \\| (Accuracy: 93%) XGBoost \ud83d\udef3\ufe0f\n\n### Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### ML Models used: XGBoost, Random Forest, Logistic Regression\n\nIn this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model and Hyperparameter tunning.\n\n### **Prerequisites:**\n\n`Data Analyst Roadmap` \u231b\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### **Overview**\n\n1. **Understand the shape of the data (Histograms, box plots, etc.)**\n\n2. **Data Cleaning**\n\n3. **Data Exploration**\n\n4. **Feature Engineering**\n\n5. **Data Preprocessing for Model**\n\n6. **Basic Model Building**\n\n7. **Model Tuning**\n\n8. **Ensemble Modle Building**\n\n9. **Results**\n\n\n### **About the Project** \ud83d\udef3\ufe0f\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\nKnowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## **Workflow stages**\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\n## Technologies used \u2699\ufe0f\n\n- [Python](https://github.com/mrankitgupta/Python-Lessons)\n\n- [Statistics](https://github.com/mrankitgupta/Statistics-for-Data-Science-using-Python)\n\n- [Jupyter](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-92-74-xgboost)\n\n\n##### Python Libraries :\n\n- [Pandas](https://github.com/mrankitgupta/Kaggle-Pandas-Solved-Exercises) \\| [NumPy](https://numpy.org/) \\| [Matplotlib](https://matplotlib.org/) \\| [Seaborn](https://seaborn.pydata.org) \\|\n\n- [Scikit-Learn](https://scikit-learn.org/) [\\|](https://scikit-learn.org/) [XGBoost](https://xgboost.readthedocs.io/en/stable/)\n\n\n## Project - Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### **Kaggle Project Link:** **[Titanic Survival Prediction](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** \ud83d\udef3\ufe0f \ud83d\udd17\n\n### Datasets\n\nKaggle Titanic Datasets: `Titanic Train` & `Titanic Test`\n\n## Related Projects\u2753 \ud83d\udc68\u200d\ud83d\udcbb \ud83d\udef0\ufe0f\n\n`Spotify Data Analysis using Python` \ud83d\udcca\n\n`Data Analyst Roadmap` \u231b\n\n`Statistics for Data Science using Python` \ud83d\udcca\n\n`Sales Insights - Data Analysis using Tableau & SQL` \ud83d\udcca\n\n`Kaggle - Pandas Solved Exercises` \ud83d\udcca\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### Liked my Contributions\u2753 Follow Me\ud83d\udc49 [Kaggle](https://www.kaggle.com/MrAnkitGupta) and [GitHub](https://github.com/MrAnkitGupta)\n\n[Nominate Me for GitHub Stars](https://stars.github.com/nominate/) \u2b50 \u2728\n\n## For any queries/doubts \ud83d\udd17 \ud83d\udc47\n\n### [Ankit Gupta](https://bio.link/AnkitGupta)\n\n[aggle](https://kaggle.com/mrankitgupta)\n\n## About\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### Topics\n\n[nlp](https://github.com/topics/nlp) [data-science](https://github.com/topics/data-science) [machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [linear-regression](https://github.com/topics/linear-regression) [machine-learning-algorithms](https://github.com/topics/machine-learning-algorithms) [ml](https://github.com/topics/ml) [prediction](https://github.com/topi...",
      "url": "https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost"
    },
    {
      "title": "GitHub - ata-turhan/Titanic-Survival-Prediction: A comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[ata-turhan](https://github.com/ata-turhan)/ **[Titanic-Survival-Prediction](https://github.com/ata-turhan/Titanic-Survival-Prediction)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n- [Star\\\n27](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n\n\nA comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.\n\n### License\n\n[MIT license](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE)\n\n[27\\\nstars](https://github.com/ata-turhan/Titanic-Survival-Prediction/stargazers) [0\\\nforks](https://github.com/ata-turhan/Titanic-Survival-Prediction/forks) [Branches](https://github.com/ata-turhan/Titanic-Survival-Prediction/branches) [Tags](https://github.com/ata-turhan/Titanic-Survival-Prediction/tags) [Activity](https://github.com/ata-turhan/Titanic-Survival-Prediction/activity)\n\n[Star](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction)\n\n[Notifications](https://github.com/login?return_to=%2Fata-turhan%2FTitanic-Survival-Prediction) You must be signed in to change notification settings\n\n# ata-turhan/Titanic-Survival-Prediction\n\nmain\n\n[Branches](https://github.com/ata-turhan/Titanic-Survival-Prediction/branches) [Tags](https://github.com/ata-turhan/Titanic-Survival-Prediction/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[11 Commits](https://github.com/ata-turhan/Titanic-Survival-Prediction/commits/main/) |\n| [.gitignore](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/.gitignore) | [.gitignore](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/.gitignore) |\n| [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) | [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) |\n| [README.md](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/README.md) | [README.md](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/README.md) |\n| [requirements.txt](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/requirements.txt) | [requirements.txt](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/requirements.txt) |\n| [titanic.ipynb](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/titanic.ipynb) | [titanic.ipynb](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/titanic.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# **Titanic Survival Prediction**\n\nThis repository contains a comprehensive solution for the Kaggle **Titanic - Machine Learning from Disaster** competition. Using machine learning techniques, we predict the survival of passengers based on key features like age, gender, class, and more. This project also includes **Explainable AI (XAI)** techniques to understand model predictions.\n\n## **Project Overview**\n\n- **Goal**: Predict whether a Titanic passenger survived or not, using features such as age, gender, ticket class, and more.\n- **Techniques**:\n  - Data Cleaning and Feature Engineering.\n  - Model Training and Hyperparameter Tuning.\n  - Model Stacking for improved performance.\n  - Explainable AI using SHAP, permutation importance, and feature importances.\n\n## **Features**\n\n- **Extensive Data Exploration**: Visualizations and insights from the dataset.\n- **Feature Engineering**: New features such as `AgeGroup`, `FareBin`, `FamilySize`, `IsAlone`, and `Title`.\n- **Model Training**: Logistic Regression, RandomForest, XGBoost, MultiLayer Perceptron, and Stacked Ensembles.\n- **Explainable AI**: Insights into model predictions with SHAP values, permutation importance, and feature coefficients.\n- **Kaggle Submission**: Prepares predictions in the required format for submission.\n\n## **Setup Instructions**\n\n### **1\\. Clone the Repository**\n\n```\ngit clone https://github.com/ata-turhan/titanic-survival-prediction.git\ncd titanic-survival-prediction\n```\n\n### **2\\. Install Dependencies**\n\nEnsure you have Python 3.8+ installed. Install the required packages using:\n\n```\npip install -r requirements.txt\n```\n\n### **3\\. Run the Notebook**\n\nLaunch Jupyter Notebook and open `notebooks/titanic_predicting_survivors.ipynb`:\n\n```\njupyter notebook\n```\n\n## **Key Results**\n\n| **Model** | **Accuracy** | **F1-Score** |\n| --- | --- | --- |\n| Logistic Regression | 80.45% | 0.80 |\n| RandomForest | 82.68% | 0.82 |\n| MultiLayer Perceptron | 79.33% | 0.79 |\n| XGBoost | TBD | TBD |\n| **Stacked Model** | **83.50%** | **0.83** |\n\n## **Explainable AI (XAI)**\n\nThis project uses several techniques to explain model predictions:\n\n1. **Logistic Regression Coefficients**:\n\n   - Visualizes the top features affecting survival.\n2. **RandomForest Feature Importances**:\n\n   - Highlights features most impactful for survival predictions.\n3. **Permutation Importance**:\n\n   - Explains predictions of the MLP model by permuting feature values.\n4. **SHAP Values**:\n\n   - Detailed explanations for RandomForest predictions with SHAP plots.\n\n## **Kaggle Submission**\n\nThe repository includes the final predictions in the `outputs/` directory:\n\n- **File**: `submission.csv`\n- **Format**:\n\n\n```\nPassengerId,Survived\n892,0\n893,1\n...\n\n```\n\n\n## **Future Improvements**\n\n- Explore advanced stacking techniques\n- Incorporate additional external datasets for richer feature engineering.\n\n## **License**\n\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/ata-turhan/Titanic-Survival-Prediction/blob/main/LICENSE) file for details.\n\n## About\n\nA comprehensive solution for the Kaggle Titanic Challenge, featuring advanced data exploration, feature engineering, model training, and explainable AI techniques. Includes Logistic Regression, RandomForest, XGBoost, and Stacked Ensembles with SHAP and permutation importance for model interpretability.\n\n### Topics\n\n[python](https://github.com/topics/python) [kaggle](https://github.com/topics/kaggle) [xgboost](https://github.com/topics/xgboost) [classification](https://github.com/topics/classification) [shap](https://github.com/topics/shap)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/github.com#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/ata-turhan/Titanic-Survival-Prediction/activity)\n\n### Stars\n\n[**27**\\\nstars](https://github.com/ata-turhan/Titanic-Survival-Prediction/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/ata-turhan/Titanic-Survival-Prediction/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/ata-turhan/Titanic-Survival-Prediction/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fata-turhan%2FTitanic-Survival-Prediction&report=ata-turhan+%28user%29)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/ata-turhan/Titanic-Survival-Prediction/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/ata-turhan/titanic-survival-prediction"
    },
    {
      "title": "GitHub - Moddy2024/Titanic-Survival-prediction: The top 5% of the titanic competition in Kaggle. achieved this through ensemble of models",
      "text": "[Skip to content](https://github.com/Moddy2024/Titanic-Survival-prediction#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n{{ message }}\n\n[Moddy2024](https://github.com/Moddy2024)/ **[Titanic-Survival-prediction](https://github.com/Moddy2024/Titanic-Survival-prediction)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n- [Star\\\n0](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n\n\nThe top 5% of the titanic competition in Kaggle. achieved this through ensemble of models\n\n[0\\\nstars](https://github.com/Moddy2024/Titanic-Survival-prediction/stargazers) [0\\\nforks](https://github.com/Moddy2024/Titanic-Survival-prediction/forks) [Branches](https://github.com/Moddy2024/Titanic-Survival-prediction/branches) [Tags](https://github.com/Moddy2024/Titanic-Survival-prediction/tags) [Activity](https://github.com/Moddy2024/Titanic-Survival-prediction/activity)\n\n[Star](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction)\n\n[Notifications](https://github.com/login?return_to=%2FModdy2024%2FTitanic-Survival-prediction) You must be signed in to change notification settings\n\n# Moddy2024/Titanic-Survival-prediction\n\nmain\n\n[Branches](https://github.com/Moddy2024/Titanic-Survival-prediction/branches) [Tags](https://github.com/Moddy2024/Titanic-Survival-prediction/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[12 Commits](https://github.com/Moddy2024/Titanic-Survival-prediction/commits/main/) |\n| [files from kaggle](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/files%20from%20kaggle) | [files from kaggle](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/files%20from%20kaggle) |  |  |\n| [results](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/results) | [results](https://github.com/Moddy2024/Titanic-Survival-prediction/tree/main/results) |  |  |\n| [README.md](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/README.md) | [README.md](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/README.md) |  |  |\n| [titanic-1.ipynb](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/titanic-1.ipynb) | [titanic-1.ipynb](https://github.com/Moddy2024/Titanic-Survival-prediction/blob/main/titanic-1.ipynb) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Titanic top 5%\n\nThis is the most famous Kaggle [competition](https://www.kaggle.com/c/titanic) in which you have to use machine learning to create a model that predicts which passenger can survive the Titanic shipwreck or not. I have done a lot of data engineering and feature engineering to clean and to increase the accuracy of my models.\n\nI have trained 3 models :\n\n- Logistic Regression.\n- XGBClassifier.\n- Random Forest.\n\nI have also ensemble all 3 of these models together to see the results and have only used ensemble of Logistic Regression and XGBClassifier. The accuracy in all of my submission file is a minimum of 75% in Kaggle but the best one was ensemble of Logistic Regression and XGBClassifier which is the file name [hardvoting\\_withoutrf](https://github.com/Moddy2024/Titanic/blob/main/results/hardvotingwithoutrf_submission.csv) which got me in the top 5% of all the people in the Titanic Competetion in Kaggle. RandomForest seems to be overfitting because we don't have a very big dataset. When comparing each of the models separately Logistic Regression works better than XGBoost and Random Forest so after ensembling the best two the ensembled model works even well.\n\n# Dataset\n\nYou can download the dataset from Kaggle or get it in this repo which I have already downloaded from Kaggle.\n\nFor the training file go [here](https://github.com/Moddy2024/Titanic/blob/main/files%20from%20kaggle/train.csv).\n\nFor the test file go [here](https://github.com/Moddy2024/Titanic/blob/main/files%20from%20kaggle/test.csv).\n\n# Software requirements\n\n- Numpy\n- Pandas\n- Seaborn\n- Matplotlib\n- Scikitlearn\n- XGBoost\n\n# Key Files\n\n- [titanic-1.ipynb](https://github.com/Moddy2024/Titanic/blob/main/titanic-1.ipynb) \\- In this file you can see all the data engineering and the feature engineering that I have performed. After which I train the model, ensemble them and check their cross validation score.\n- [results](https://github.com/Moddy2024/Titanic/tree/main/results) \\- All the results of the different models and ensemble are present here in csv format.\n- [files from kaggle](https://github.com/Moddy2024/Titanic/tree/main/files%20from%20kaggle) \\- The files that are provided by Kaggle. There are three files here training,test and gender\\_submission.csv. We can only use the training file for training the model for the competition and predict the results using the data in the test file for submission. The gender\\_submission.csv is as an example of what a submission file should look like.\n\n## About\n\nThe top 5% of the titanic competition in Kaggle. achieved this through ensemble of models\n\n### Topics\n\n[machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [kaggle](https://github.com/topics/kaggle) [titanic-kaggle](https://github.com/topics/titanic-kaggle) [classification](https://github.com/topics/classification) [data-analysis](https://github.com/topics/data-analysis) [ensemble](https://github.com/topics/ensemble) [feature-engineering](https://github.com/topics/feature-engineering) [ensemble-model](https://github.com/topics/ensemble-model) [ensemble-classifier](https://github.com/topics/ensemble-classifier) [catboost](https://github.com/topics/catboost) [ensemble-machine-learning](https://github.com/topics/ensemble-machine-learning) [xgboost-classifier](https://github.com/topics/xgboost-classifier) [catboost-classifier](https://github.com/topics/catboost-classifier)\n\n### Resources\n\n[Readme](https://github.com/Moddy2024/Titanic-Survival-prediction#readme-ov-file)\n\n[Activity](https://github.com/Moddy2024/Titanic-Survival-prediction/activity)\n\n### Stars\n\n[**0**\\\nstars](https://github.com/Moddy2024/Titanic-Survival-prediction/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/Moddy2024/Titanic-Survival-prediction/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/Moddy2024/Titanic-Survival-prediction/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FModdy2024%2FTitanic-Survival-prediction&report=Moddy2024+%28user%29)\n\n## [Releases](https://github.com/Moddy2024/Titanic-Survival-prediction/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/Moddy2024/packages?repo_name=Titanic-Survival-prediction)\n\nNo packages published\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/Moddy2024/Titanic-Survival-prediction/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/Moddy2024/Titanic-Survival-prediction"
    },
    {
      "title": "Titanic Survival Prediction: Comparing 5 Machine Learning Models",
      "text": "<div><div><h2>Introduction</h2><p>The Titanic is one of the most famous shipwrecks in history, and with the release of the movie Titanic in 1997, it has become even more famous. The Titanic dataset on Kaggle is a classic machine learning problem where the task is to predict which passengers survived the tragedy. In this article, we will be comparing the performance of 5 machine learning models: Neural Network, Logistic Regression, Random Forest, Support Vector Machine, and XGBoost on this dataset. The model with the highest F1 score on the test set will be declared as the best performing model.</p><h2>Methodology:</h2><p>First, we load the Titanic dataset and perform exploratory data analysis to understand the dataset. Then, we preprocess the dataset by encoding categorical variables and filling in missing values. We then split the data into a training set and a testing set, with a 80/20 split. Next, we normalize the data using MinMaxScaler.</p><p>We then proceed to build and fit our machine learning models. We build a neural network using Keras, a logistic regression model, a random forest model, a support vector machine model, and an XGBoost model. We then calculate the F1 score on the testing set for each of the 10 random states we used for splitting the data, and take the mean of the F1 scores across all 10 states.</p><p>Finally, we compare the performance of the 5 models by creating a dataframe of the mean F1 scores for each model and sorting the dataframe in descending order.</p><h2>Results:</h2><p>Our analysis shows that Support Vector Machine was the best performing model, with a mean F1 score of 80.09%. This was followed by Random Forest with a score of 79.62%, XGBoost with a score of 79.56%, Logistic Regression with a score of 79.34%, and finally Neural Network with a score of 77.39%.</p><h2>Conclusion:</h2><p>In this article, we compared the performance of 5 machine learning models on the Titanic dataset. Our analysis shows that Support Vector Machine was the best performing model for this dataset. It is worth noting that while we used F1 score as the metric for comparing the models, there are other metrics such as accuracy, precision, and recall that could have been used instead. Additionally, while we used only 5 models in this analysis, there are many other models that could have been used as well. Nonetheless, this analysis provides a good starting point for anyone interested in working with the Titanic dataset or similar machine learning problems.</p><h2>References:</h2><ul><li>The full code for this analysis can be found on Kaggle: <a href=\"https://www.kaggle.com/code/amyrmahdy/predicting-survival-on-the-titanic\">https://www.kaggle.com/code/amyrmahdy/predicting-survival-on-the-titanic</a></li><li>The Titanic dataset is available on Kaggle: <a href=\"https://www.kaggle.com/c/titanic\">https://www.kaggle.com/c/titanic</a></li><li>The full code for this analysis also can be found on GitHub: <a href=\"https://github.com/amyrmahdy/titanic\">https://github.com/amyrmahdy/titanic</a></li></ul></div></div>",
      "url": "https://amyrmahdy.medium.com/titanic-survival-prediction-comparing-5-machine-learning-models-d1d451a9e558"
    },
    {
      "title": "Ensemble Learning for Kaggle Titanic Competition",
      "text": "# Ensemble Learning for Kaggle Titanic Competition\n\n> Idea is that since Logistic Regression with 6 variables and Boosting seems to work best, I'd like to combine these two using Ensemble techniques to see if I can get better predictions.\n\n## Logistic and Boosting Ensemble (By \u201cBlending\u201d)\n\n```\n# Logistic Regression using 6 variables and non-linear SibSp\nset.seed(200)\ntrain.glm <- glm(Survived ~ Pclass + Sex + Age + I(SibSp^3) + Child + Sex *\n    Pclass, family = binomial, data = trainData)\nglm.pred <- predict.glm(train.glm, newdata = testData, type = \"response\")\nsurvival.glm = ifelse(glm.pred > 0.5, 1, 0)\n\n# Boosting using 6 variables\nlibrary(gbm)\n\n```\n\n```\n## Loading required package: survival\n## Loading required package: splines\n## Loading required package: lattice\n## Loading required package: parallel\n## Loaded gbm 2.1\n\n```\n\n```\nset.seed(200)\nboost.titanic = gbm(Survived ~ Pclass + Sex + Age + SibSp + Fare + Child, data = trainData,\n    distribution = \"gaussian\", n.trees = 5000, shrinkage = 0.001, interaction.depth = 4)\nboost.pred = predict(boost.titanic, newdata = testData, n.trees = 5000)\nsurvival.boost = ifelse(boost.pred > 0.5, 1, 0)\n\n```\n\nNow combine the two methods in various proportions and then making predictions:\n\n```\nensemble.pred.75boost = glm.pred * 0.25 + boost.pred * 0.75\nensemble.pred.50 = glm.pred * 0.5 + boost.pred * 0.5\nensemble.pred.75glm = glm.pred * 0.75 + boost.pred * 0.25\n\nsurvival.75boost = ifelse(ensemble.pred.75boost > 0.5, 1, 0)\nsurvival.50 = ifelse(ensemble.pred.50 > 0.5, 1, 0)\nsurvival.75glm = ifelse(ensemble.pred.75glm > 0.5, 1, 0)\n\n# Check to see how many times their 'voting' coincide:\nlength(which(survival.glm == survival.boost))/length(survival.glm)\n\n```\n\n```\n## [1] 0.9474\n\n```\n\n**So 94.7% of the time, these two methods agree!** I believe this implies that the room for improvement as a result of blending the two methods will be quite limited. Let's see if this is the case.\n\n```\nkaggle.sub <- cbind(PassengerId, survival.75boost)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_ensemble_75boost.csv\",\n    row.names = FALSE)\n\nkaggle.sub <- cbind(PassengerId, survival.50)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_ensemble_50.csv\",\n    row.names = FALSE)\n\nkaggle.sub <- cbind(PassengerId, survival.75glm)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_ensemble_75glm.csv\",\n    row.names = FALSE)\n\n```\n\n**As expected, none of these techniques improved the prediction.**\n\nTry: an Ensemble of 3 top methods by \u201cMajority Voting\u201d and then \u201cBlending\u201d. This would be more \u201cinteresting\u201d.\n\n```\n# Add SVM with linear kernel to the mix\nlibrary(e1071)\n\n```\n\n```\n## Loading required package: class\n\n```\n\n```\n# Fix NA in 'Fare'\ntestData$Fare[which(is.na(testData$Fare))] = mean(testData$Fare, na.rm = TRUE)\n\n# Use tune() to do 10-fold CV\ntune.out = tune(svm, Survived ~ Pclass + Sex + Age + Fare + Child + Embarked_C +\n    Embarked_Q + Sex * Pclass + I(SibSp^3), data = trainData, kernel = \"linear\",\n    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10)))\nsummary(tune.out)\n\n```\n\n```\n##\n## Parameter tuning of 'svm':\n##\n## - sampling method: 10-fold cross validation\n##\n## - best parameters:\n##   cost\n##  0.001\n##\n## - best performance: 0.1726\n##\n## - Detailed performance results:\n##    cost  error dispersion\n## 1 1e-03 0.1726    0.02244\n## 2 1e-02 0.1959    0.04293\n## 3 1e-01 0.1940    0.04461\n## 4 1e+00 0.1850    0.04326\n## 5 5e+00 0.1849    0.04306\n## 6 1e+01 0.1849    0.04307\n\n```\n\n```\nbestmod = tune.out$best.model\nsummary(bestmod)\n\n```\n\n```\n##\n## Call:\n## best.tune(method = svm, train.x = Survived ~ Pclass + Sex + Age +\n##     Fare + Child + Embarked_C + Embarked_Q + Sex * Pclass + I(SibSp^3),\n##     data = trainData, ranges = list(cost = c(0.001, 0.01, 0.1,\n##         1, 5, 10)), kernel = \"linear\")\n##\n##\n## Parameters:\n##    SVM-Type:  eps-regression\n##  SVM-Kernel:  linear\n##        cost:  0.001\n##       gamma:  0.1111\n##     epsilon:  0.1\n##\n##\n## Number of Support Vectors:  689\n\n```\n\n```\n\nyhat.svm.linear = predict(bestmod, testData)\nsurvival.svm.linear = ifelse(yhat.svm.linear > 0.5, 1, 0)\n\nkaggle.sub <- cbind(PassengerId, survival.svm.linear)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_svm_linear2.csv\",\n    row.names = FALSE)\n\n```\n\nMajority Voting of for SVM, Logistic, Boosting:\n\n```\nsurvival.vote = ifelse(survival.glm + survival.boost + survival.svm.linear >\n    1, 1, 0)\n\nkaggle.sub <- cbind(PassengerId, survival.svm.linear)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_vote.csv\",\n    row.names = FALSE)\n\n```\n\n**Actually brought down the score!**\n\nBlending:\n\n```\nensemble.pred.3blends = (glm.pred + boost.pred + yhat.svm.linear)/3\nsurvival.blend = ifelse(ensemble.pred.3blends > 0.5, 1, 0)\n\nkaggle.sub <- cbind(PassengerId, survival.svm.linear)\ncolnames(kaggle.sub) <- c(\"PassengerId\", \"Survived\")\nwrite.csv(kaggle.sub, file = \"~/Dropbox/Data Science/Kaggle/Titanic/titanic_3blends.csv\",\n    row.names = FALSE)\n\n```\n\n**Did not help either.**",
      "url": "https://rstudio-pubs-static.s3.amazonaws.com/16257_4cea608eb27c4c7a8789eb3c89a86537.html"
    },
    {
      "title": "GitHub - hoomanm/Kaggle-Titanic: Kaggle's Titanic Competition: Machine Learning from Disaster",
      "text": "[Skip to content](https://github.com/hoomanm/Kaggle-Titanic#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/hoomanm/Kaggle-Titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/hoomanm/Kaggle-Titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/hoomanm/Kaggle-Titanic) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[hoomanm](https://github.com/hoomanm)/ **[Kaggle-Titanic](https://github.com/hoomanm/Kaggle-Titanic)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fhoomanm%2FKaggle-Titanic) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fhoomanm%2FKaggle-Titanic)\n- [Star\\\n1](https://github.com/login?return_to=%2Fhoomanm%2FKaggle-Titanic)\n\n\nKaggle's Titanic Competition: Machine Learning from Disaster\n\n[1\\\nstar](https://github.com/hoomanm/Kaggle-Titanic/stargazers) [0\\\nforks](https://github.com/hoomanm/Kaggle-Titanic/forks) [Branches](https://github.com/hoomanm/Kaggle-Titanic/branches) [Tags](https://github.com/hoomanm/Kaggle-Titanic/tags) [Activity](https://github.com/hoomanm/Kaggle-Titanic/activity)\n\n[Star](https://github.com/login?return_to=%2Fhoomanm%2FKaggle-Titanic)\n\n[Notifications](https://github.com/login?return_to=%2Fhoomanm%2FKaggle-Titanic) You must be signed in to change notification settings\n\n# hoomanm/Kaggle-Titanic\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/hoomanm/Kaggle-Titanic/branches) [Tags](https://github.com/hoomanm/Kaggle-Titanic/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[7 Commits](https://github.com/hoomanm/Kaggle-Titanic/commits/master/) |\n| ### [README.md](https://github.com/hoomanm/Kaggle-Titanic/blob/master/README.md) | ### [README.md](https://github.com/hoomanm/Kaggle-Titanic/blob/master/README.md) |  |  |\n| ### [Titanic-Classifiers.py](https://github.com/hoomanm/Kaggle-Titanic/blob/master/Titanic-Classifiers.py) | ### [Titanic-Classifiers.py](https://github.com/hoomanm/Kaggle-Titanic/blob/master/Titanic-Classifiers.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle's Titanic Competition: Machine Learning from Disaster\n\nThe aim of this project is to predict which passengers survived the Titanic tragedy given a set of labeled data as the training dataset. Our strategy is to identify an informative set of features and then try different classification techniques to attain a good accuracy in predicting the class labels. To achieve this goal, we combined different approaches explained in some of the Kaggle Kernels such as: [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) and [Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions). Our results indicate that ensemble methods perform better than other algorithms. Specifically, using the majority vote of a number of classifiers (including the ensemble models) gives us the best accuracy.\n\nWe are given a training dataset with 891 samples and a test dataset with 418 samples including 9 features. After feature engineering, which involves removing some of the features and creating a couple of new ones by combining other features, we keep 10 features. These features are the most important ones according to the feature importances produced by different trained models.\n\nOnce we have our set of features, in order to examine different classifiers, we use 5-fold cross-validation technique on the training dataset. in addition to the cross-validation, we compute the accuracy of the models using the test dataset provided by Kaggle. The below table shows the cross-validation scores and the scores achieved on the Kaggle leaderboard for our different models:\n\n| Model | Accuracy (5-fold Cross Validation) | Accuracy (Kaggle Test Dataset) |\n| --- | --- | --- |\n| Voting Classifier | 0.8272 | 0.8086 |\n| Two-Layer XGBoost | 0.8431 | 0.7847 |\n| Random Forest | 0.8070 | 0.7751 |\n| XGBoost | 0.8194 | 0.7703 |\n| Support Vector Classifier | 0.7867 | 0.7655 |\n| Decision Tree | 0.7185 | 0.6651 |\n\nIt is worth mentioning that in order to train the two-layer XGBoost model, first we train a couple of other classifiers such as: AdaBoost, Gradient Boosting, Random Forest, and Extra Trees. Then, we use the output of these classifiers as the input features for the XGBoost classifier.\n\nThe Voting Classifier uses two-layer XGBoost, Logistic Regression, K-Nearest Neighbor, Support Vector Classifier, and Random Forest all together and then predicts the class label that has the majority of the votes. We also use different weights for different classifiers based on their influence on the accuracy.\n\n## About\n\nKaggle's Titanic Competition: Machine Learning from Disaster\n\n### Resources\n\n[Readme](https://github.com/hoomanm/Kaggle-Titanic#readme-ov-file)\n\n[Activity](https://github.com/hoomanm/Kaggle-Titanic/activity)\n\n### Stars\n\n[**1**\\\nstar](https://github.com/hoomanm/Kaggle-Titanic/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/hoomanm/Kaggle-Titanic/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/hoomanm/Kaggle-Titanic/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fhoomanm%2FKaggle-Titanic&report=hoomanm+%28user%29)\n\n## [Releases](https://github.com/hoomanm/Kaggle-Titanic/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/hoomanm/packages?repo_name=Kaggle-Titanic)\n\nNo packages published\n\n## Languages\n\n- [Python100.0%](https://github.com/hoomanm/Kaggle-Titanic/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/hoomanm/Kaggle-Titanic"
    },
    {
      "title": "Titanic Survival \u2014 Titanic Survival Machine Learning",
      "text": "<div><section>\n<h2>Titanic Survival<a href=\"#titanic-survival\">#</a></h2>\n<p>Essential Machine Learning Classification Coding Tutorials</p>\n<p></p>\n<p>Could you predict which passengers would survive the Titanic?</p>\n<p>This is a classic classification problem, and we will use this problem to explore methods of machine learning, from logistic regression and Random Forests through to \u2018Deep Learning\u2019 using TensorFlow. Along the way we\u2019ll look at processing data, how to run replicates, measuring accuracy, avoiding over-fitting with regularisation, trading off false positives and false negatives, feature selection and expansion, finding out how much data you need, optimising model parameters, dealing with imbalanced data, and more!</p>\n<p>These examples will use Kaggle\u2019s Titanic Survival data to explore machine learning. The Kaggle page may be found at: <a href=\"https://www.kaggle.com/c/titanic\">https://www.kaggle.com/c/titanic</a>.</p>\n<p>The contents of this book are:</p>\n<section>\n<h2>SETTING UP AN ENVIRONMENT TO RUN THESE NOTEBOOKS<a href=\"#setting-up-an-environment-to-run-these-notebooks\">#</a></h2>\n<ul>\n<li><p><em>Setting up an environment</em>: Setting up an environment using an <code><span>environment.yml</span></code> file, or setting up manually.</p></li>\n</ul>\n</section>\n<section>\n<h2>DATA PREPROCESSING<a href=\"#data-preprocessing\">#</a></h2>\n<ul>\n<li><p><em>Kaggle Titanic survival</em>: data preprocessing: Getting our data in a form suitable for machine learning</p></li>\n</ul>\n</section>\n<section>\n<h2>BASIC METHODS<a href=\"#basic-methods\">#</a></h2>\n<p>The \u2018essentials\u2019 of machine learning, using a logistic regression model as our example model.</p>\n<ul>\n<li><p><em>Logistic regression model</em>: A basic logistic regression model to predict survival.</p></li>\n<li><p><em>Measuring model accuracy with K-fold stratification</em>: The best way to manage train/test data splits. Trains multiple models where all the data is used once, but only once, across the test sets.</p></li>\n<li><p><em>Avoiding over-fitting with regularisation</em>: Prevent a model over-fitting to training data.</p></li>\n<li><p><em>Accuracy measurements in machine learning</em>: Go beyond simply measuring the proportion of predictions that are right.</p></li>\n<li><p><em>Application of alternative accuracy measurements to a logistic regression model</em>: See how different accuracy measurements are applied to Titanic survival data.</p></li>\n<li><p><em>Learning curves - how much data do we need?</em>: Would gathering more data improve your accuracy. Do you need to use all the data you have?</p></li>\n<li><p><em>Receiver Operator Characteristic (ROC) curve</em>: Many consider the ROC the \u2018gold standard\u2019 of measuring model performance.</p></li>\n<li><p><em>Checking model calibration</em>: Check that the model output probabilities are well calibrated.</p></li>\n</ul>\n</section>\n<section>\n<h2>OPTIMISING MACHINE LEARNING MODELS<a href=\"#optimising-machine-learning-models\">#</a></h2>\n<ul>\n<li><p><em>Optimising machine learning models with Optuna</em>. A modern easy-to-use machine learning hyperparamter optimiser.</p></li>\n</ul>\n</section>\n<section>\n<h2>FEATURE SELECTION AND EXPANSION<a href=\"#feature-selection-and-expansion\">#</a></h2>\n<p>Should you use all features? Is it useful to create extra polynomial features?</p>\n<ul>\n<li><p><em>Feature selection using univariate statistical selection</em>: Select features based on how well they correlate with our target label.</p></li>\n<li><p><em>Feature selection using forward selection</em>: Select features based on progressively selecting features that increase model accuracy most.</p></li>\n<li><p><em>Feature selection using backward elimination</em>: Remove features based on progressively removing those that can be removed with the smallest loss in accuracy:</p></li>\n<li><p><em>Feature expansion</em>: Add polynomial features (interactions between features) coupled with feature selection using forward selection.</p></li>\n</ul>\n</section>\n<section>\n<h2>WORKING WITH IMBALANCED DATA SETS<a href=\"#working-with-imbalanced-data-sets\">#</a></h2>\n<p>Methods that help you manage data sets where there is not an even balance between classes.</p>\n<ul>\n<li><p><em>Dealing with imbalanced data by model weighting</em>: Manage imbalance by adjusted the weight (influence) of different classes.</p></li>\n<li><p><em>Dealing with imbalanced data by under or over sampling</em>: Sample the minority class more, or sample the majority class less.</p></li>\n<li><p><em>Dealing with imbalanced data by changing classification cut-off levels</em>: Adjust the probability threshold used to classify data.</p></li>\n<li><p><em>Dealing with imbalanced data by enhancing the minority class with synthetic data (SMOTE: Synthetic Minority Over-sampling Technique)</em>: Create synthetic examples of the minority class.</p></li>\n</ul>\n</section>\n<section>\n<h2>RANDOM FOREST MODEL<a href=\"#random-forest-model\">#</a></h2>\n<p>A popular model type for structured data.</p>\n<ul>\n<li><p><em>A Random * Forest model</em>: Setting up a Random Forest model.</p></li>\n<li><p><em>Random Forest Receiver Operator Characteristic (ROC) curve and balancing of model classification</em>: Performing a ROC analysis for Random Forest, and looking at the effect of adjusting classification threshold.</p></li>\n</ul>\n</section>\n<section>\n<h2>TENSORFLOW NEURAL NETS<a href=\"#tensorflow-neural-nets\">#</a></h2>\n<p>A popular framework for building neural networks.</p>\n<ul>\n<li><p><em>TensorFlow neural net</em>: AN example taking you through the basics of setting up a neural network in TensorFlow including regularisation of neural networks, and automatically stopping training when the model is at peak performance.</p></li>\n<li><p><em>TensorFlow api-based neural net</em>: A second, more versatile way, to build TensorFlow neural networks.</p></li>\n<li><p><em>TensorFlow Receiver Operator Characteristic (ROC) curve and balancing of model classification</em>: Performing a ROC analysis for a neural network, and looking at the effect of adjusting classification threshold.</p></li>\n<li><p><em>TensorFlow \u2018Wide and Deep\u2019 neural nets</em>: Combining \u2018shallow\u2019 and \u2018deep\u2019 learning in one neural network.</p></li>\n<li><p><em>TensorFlow Bagging</em>: Train multiple networks to improve accuracy and/or get a measure of prediction uncertainty.</p></li>\n</ul>\n</section>\n<section>\n<h2>PyTORCH NEURAL NETS<a href=\"#pytorch-neural-nets\">#</a></h2>\n<p>Another popular framework for building neural networks. PyTorch frequently requires more lines of code than TensorFlow, but is more Pythonic.</p>\n<ul>\n<li><p><em>PyTorch simple sequential neural net</em>: The simplest neural network architecture in PyTorch.</p></li>\n<li><p><em>PyTorch class-based neural net</em>: A more flexible PyTorch neural net architecture.</p></li>\n</ul>\n</section>\n<section>\n<h2>MODEL EXPLAINABILITY<a href=\"#model-explainability\">#</a></h2>\n<ul>\n<li><p><em>Explaining model predictions with Shapley values - Logistic Regression</em>: Shapley values provide the contribution of each feature to predictions (either for individual predictions or for the model as a whole). Here we provide an example using logistic regression.</p></li>\n<li><p><em>Explaining model predictions with Shapley values - Random Forest</em>: An example of using Shap with Random Forests, which requires a slightly different syntax to logistic regression.</p></li>\n<li><p><em>Explaining model predictions with Shapley values - Neural Network</em>: An example of using Shap with a TensorFlow/Keras neural network.</p></li>\n</ul>\n</section>\n</section></div>",
      "url": "https://michaelallen1966.github.io/titanic/front_page.html"
    },
    {
      "title": "GitHub - yassnemo/titanic-survival-analysis: Project for Titanic survival prediction, achieving 83.28% accuracy through advanced feature engineering, hyperparameter optimization, and ensemble methods.",
      "text": "<div><div>\n<p>\nProject for Titanic survival prediction, achieving 83.28% accuracy through advanced feature engineering, hyperparameter optimization, and ensemble methods.\n</p>\n<p>\n<a href=\"https://github.com/yassnemo/titanic-survival-analysis/stargazers\">\n<span>1</span>\nstar\n</a> <a href=\"https://github.com/yassnemo/titanic-survival-analysis/forks\">\n<span>0</span>\nforks\n</a> <a href=\"https://github.com/yassnemo/titanic-survival-analysis/branches\">\n<span>Branches</span>\n</a> <a href=\"https://github.com/yassnemo/titanic-survival-analysis/tags\">\n<span>Tags</span>\n</a> <a href=\"https://github.com/yassnemo/titanic-survival-analysis/activity\">\n<span>Activity</span>\n</a> </p>\n<div>\n<div>\n<p>\n<a href=\"https://github.com/login?return_to=%2Fyassnemo%2Ftitanic-survival-analysis\"> <span>\nStar\n</span>\n</a></p>\n</div>\n<p>\n<a href=\"https://github.com/login?return_to=%2Fyassnemo%2Ftitanic-survival-analysis\"> Notifications\n</a>\n</p>\n</div>\n</div></div>",
      "url": "https://github.com/yassnemo/titanic-survival-analysis"
    }
  ]
}