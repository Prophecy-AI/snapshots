{
  "query": "What are the best feature engineering techniques for Titanic survival prediction on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering is the most powerful lever for boosting Titanic\u2011survival scores because the raw columns (e.g.,\u202fName, Ticket, Cabin) contain a lot of hidden information that tree\u2011based or linear models cannot exploit directly.  The most widely\u2011used tricks\u2014validated by recent Kaggle notebooks such as the 2025 Random\u2011Forest repo\u2014include:  \n\n* **Missing\u2011value handling** \u2013 fill missing\u202fAge\u202fwith the median age of passengers sharing the same\u202fTitle\u202for\u202fPclass\u202fand impute missing\u202fEmbarked\u202fwith the mode; replace missing\u202fFare\u202fwith the median of the corresponding\u202fPclass\u202fand\u202fEmbarked\u202fgroup.  \n* **Categorical encoding** \u2013 convert\u202fSex,\u202fEmbarked,\u202fPclass\u202fto numeric codes or one\u2011hot vectors; extract the **Title** (Mr,\u202fMrs,\u202fMiss,\u202fMaster,\u202fRare) from the\u202fName\u202ffield and encode it as a categorical feature.  \n* **Family\u2011based features** \u2013 compute **FamilySize**\u202f=\u202fSibSp\u202f+\u202fParch\u202f+\u202f1, create a binary **IsAlone** flag (FamilySize\u202f==\u202f1), and optionally a **FamilyID** that groups passengers with the same surname and family size.  \n* **Cabin and deck information** \u2013 derive a **CabinPresence** flag (Cabin\u202f!=\u202fnull), extract the first letter as **Deck**, and map unknown decks to a separate category.  \n* **Ticket parsing** \u2013 pull the ticket prefix (e.g.,\u202f\u201cPC\u202f17599\u201d \u2192 \u201cPC\u201d) as a categorical feature and compute **TicketCount** (how many passengers share the same ticket).  \n* **Age and fare binning** \u2013 bucket\u202fAge\u202finto 5\u2011year intervals or into \u201cChild/Teen/Adult/Senior\u201d groups, and bin\u202fFare\u202finto quantiles; these bins often interact better with tree models.  \n* **Interaction terms** \u2013 combine\u202fPclass\u202f\u00d7\u202fSex\u202for\u202fTitle\u202f\u00d7\u202fDeck\u202fto capture socioeconomic effects that single columns miss.  \n\nApplying these engineered columns typically lifts a baseline ~72\u202f% accuracy to the high\u201170s or low\u201180s on the public leaderboard, as shown by the same 2025 repository that raised performance from ~72\u202f% to ~78\u202f% after adding Cabin presence, FamilySize, IsAlone, Title, and Age bins ([github.com](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)).  The R tutorial further emphasizes extracting useful substrings from\u202fName,\u202fTicket, and\u202fCabin\u202fto create the Title, TicketPrefix, and Deck features, noting that even simple models can outperform complex algorithms when these features are present ([trevorstephens.com](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering)).  Together, these techniques form the current best practice for Titanic survival prediction on Kaggle.",
      "url": ""
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708: Predict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[SamanthaZhang25](https://github.com/SamanthaZhang25)/**[Kaggle-Titanic--Machine-Learning--Random-Forest-78.708](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)**Public\n* [Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n* [Star1](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\\\\\~72% to \\\\\\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n[1star](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/stargazers)[0forks](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/forks)[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)[Activity](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/activity)\n[Star](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)\n[Notifications](https://github.com/login?return_to=/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708)You must be signed in to change notification settings\n# SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708\nmain\n[Branches](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[Tags](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/branches)[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[5 Commits](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n[](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/commits/main/)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n[README.md](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/README.md)\n|\n|\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n[rf-78-708 (5).ipynb](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/rf-78-708%20(5).ipynb)\n|\n|\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n[test.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/test.csv)\n|\n|\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n[train.csv](https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708/blob/main/train.csv)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle-Titanic---Machine-Learning---Random-Forest-78.708-\n[](#kaggle-titanic---machine-learning---random-forest-78708-)\nPredict Titanic survival using targeted feature engineering (Cabin presence, FamilySize, IsAlone, Title, Age bins) and compare popular models (RF, SVM, KNN, XGBoost, LightGBM). Feature engineering lifted accuracy from \\~72% to \\~78%. Full code, notebooks, and instructions are available in the GitHub repo.\n# Titanic Survival Prediction\n[](#titanic-survival-prediction)\nThis repository contains a full end-to-end analysis and modeling workflow for the classic Titanic Kaggle challenge. It includes:\n* **Detailed EDA**\nA Jupyter notebook that walks through exploratory data analysis:\n* Summary statistics\n* Missing-value patterns\n* Feature distributions and pairwise relationships\n* Correlation heatmaps\n* **Feature Engineering**\nCreation of high-signal features from raw inputs, including:\n* `Has\\_Cabin`(binary cabin indicator)\n* `FareBin`(quantile-based fare categories)\n* Group-median imputation for`Age`, then 10-year age bins\n* `FamilySize`and`IsAlone`derived from SibSp/Parch\n* `Title`extracted and consolidated from passenger names\n* **Model Comparison**\nTraining and evaluation of multiple classifiers on the engineered feature set:\n* Random Forest\n* Support Vector Machine\n* K-Nearest Neighbors (k=3,5,7)\n* XGBoost\n* LightGBM\nResults are compared side-by-side with a clean bar chart, and each model\u2019s strengths and weaknesses are discussed in detailed Markdown cells.\n* **Analysis &amp; Interpretation**\nInline Markdown commentary explains:\n* Why each feature matters\n* How missing data were handled\n* The rationale for model selection\n* Key takeaways and next-step recommendations\n## Repository Structure\n[](#repository-structure)\ndata/\n* train.csv # Official Kaggle training set\n* test.csv # Official Kaggle test set\nnotebooks/\n* titanic\\_analysis.ipynb\n\u2022EDA, feature engineering, model training\n\u2022Detailed Markdown analysis\n* README.md # Project overview and instructions\n## Contributing\n[](#contributing)\nFeel free to open issues, fork the repository, or submit pull requests with enhancements\u2014whether it\u2019s new feature ideas, alternative modeling approaches, or visualization improvements.\n## About\nPredict Titanic survival using t...",
      "url": "https://github.com/SamanthaZhang25/Kaggle-Titanic--Machine-Learning--Random-Forest-78.708"
    },
    {
      "title": "Titanic: Getting Started With R - Part 4: Feature Engineering",
      "text": "![Titanic: Getting Started With R - Part 4: Feature Engineering](https://trevorstephens.com/images/titanic-header.png)\n\n[![Trevor Stephens](https://trevorstephens.com/images/blue_avatar.png)](https://trevorstephens.com/)\n\n### Trevor Stephens\n\nRegular Data Scientist, Occasional Blogger.\n\nFollow\n\n- Melbourne, Australia\n- [Website](https://trevorstephens.com)\n- [Twitter](http://twitter.com/trevs)\n- [LinkedIn](http://linkedin.com/in/trevorstephens)\n- [Github](http://github.com/trevorstephens)\n\n[Tutorial index](https://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r)\n\nFeature engineering is so important to how your model performs, that even a simple model with great features can outperform a complicated algorithm with poor ones. In fact, feature engineering has been described as [easily the most important factor](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) in determining the success or failure of your predictive model. Feature engineering really boils down to the human element in machine learning. How much you understand the data, with your human intuition and creativity, can make the difference.\n\nSo what is feature engineering? It can mean many things to different problems, but in the Titanic competition it could mean chopping, and combining different attributes that we were given by the good folks at Kaggle to squeeze a little bit more value from them. In general, an engineered feature may be easier for a machine learning algorithm to digest and make rules from than the variables it was derived from.\n\nThe initial suspects for gaining more machine learning mojo from are the three text fields that we never sent into our decision trees [last time](https://trevorstephens.com/kaggle-titanic-tutorial/r-part-3-decision-trees). While the ticket number, cabin, and name were all unique to each passenger; perhaps parts of those text strings could be extracted to build a new predictive attribute. Let\u2019s start with the name field. If we take a glance at the first passenger\u2019s name we see the following:\n\n```\n>train$Name[1][1]Braund,Mr.OwenHarris891Levels:Abbing,Mr.AnthonyAbbott,Mr.RossmoreEdward...Zimmerman,Mr.Leo\n```\n\nPreviously we have only accessed passenger groups by subsetting, now we access an individual by using the row number, 1, as an index instead. Okay, no one else on the boat had that name, that\u2019s pretty much certain, but what else might they have shared? Well, I\u2019m sure there were plenty of Mr\u2019s aboard. Perhaps the persons title might give us a little more insight.\n\nIf we scroll through the dataset we see many more titles including Miss, Mrs, Master, and even the Countess! The title \u201cMaster\u201d is a bit outdated now, but back in these days, it was reserved for unmarried boys. Additionally, the nobility such as our Countess would probably act differently to the lowly proletariat too. There seems to be a fair few possibilities of patterns in this that may dig deeper than the combinations of age, gender, etc that we looked at before.\n\nIn order to extract these titles to make new variables, we\u2019ll need to perform the same actions on both the training and testing set, so that the features are available for growing our decision trees, and making predictions on the unseen testing data. An easy way to perform the same processes on both datasets at the same time is to merge them. In R we can use `rbind`, which stands for row bind, so long as both dataframes have the same columns as each other. Since we obviously lack the Survived column in our test set, let\u2019s create one full of missing values (NAs) and then row bind the two datasets together:\n\n```\n>test$Survived<-NA>combi<-rbind(train,test)\n```\n\nNow we have a new dataframe called \u201ccombi\u201d with all the same rows as the original two datasets, stacked in the order in which we specified: train first, and test second.\n\nIf you look back at the output of our inquiry on Owen, his name is still encoded as a factor. As we mentioned earlier in the tutorial series, strings are automatically imported as factors in R, even if it doesn\u2019t make sense. So we need to cast this column back into a text string. To do this we use `as.character`. Let\u2019s do this and then take another look at Owen:\n\n```\n>combi$Name<-as.character(combi$Name)>combi$Name[1][1]\"Braund, Mr. Owen Harris\"\n```\n\nExcellent, no more levels, now it\u2019s just pure text. In order to break apart a string, we need some hooks to tell the program to look for. Nicely, we see that there is a comma right after the person\u2019s last name, and a full stop after their title. We can easily use the function `strsplit`, which stands for string split, to break apart our original name over these two symbols. Let\u2019s try it out on Mr. Braund:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][1]\"Braund\"\" Mr\"\" Owen Harris\"\n```\n\nOkay, good. Here we have sent `strsplit` the cell of interest, and given it some symbols to chose from when splitting the string up, either a comma or period. Those symbols in the square brackets are called regular expressions, though this is a very simple one, and if you plan on working with a lot of text I would certainly recommend getting used to using them!\n\nWe see that the title has been broken out on its own, though there\u2019s a strange space before it begins because the comma occurred at the end of the surname. But how do we get to that title piece and clear out the rest of the stuff we don\u2019t want? An index `[[1]]` is printed before the text portions. Let\u2019s try to dig into this new type of container by appending all those square brackets to the original command:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][1]\"Braund\"\" Mr\"\" Owen Harris\"\n```\n\nGetting there! String split uses a doubly stacked matrix because it can never be sure that a given regex will have the same number of pieces. If there were more commas or periods in the name, it would create more segments, so it hides them a level deeper to maintain the rectangular types of containers that we are used to in things like spreadsheets, or now dataframes! Let\u2019s go a level deeper into the indexing mess and extract the title. It\u2019s the second item in this nested list, so let\u2019s dig in to index number 2 of this new container:\n\n```\n>strsplit(combi$Name[1],split='[,.]')[[1]][2][1]\" Mr\"\n```\n\nGreat. We have isolated the title we wanted at last. But how to _apply_ this transformation to every row of the combined train/test dataframe? Luckily, R has some extremely useful functions that apply more complicated functions one row at a time. As we had to dig into this container to get the title, simply trying to run `combi$Title <- strsplit(combi$Name, split='[,.]')[[1]][2]` over the whole name vector would result in all of our rows having the same value of Mr., so we need to work a bit harder. Unsurprisingly applying a function to a lot of cells in a dataframe or vector uses the `apply` suite of functions of R:\n\n```\n>combi$Title<-sapply(combi$Name,FUN=function(x){strsplit(x,split='[,.]')[[1]][2]})\n```\n\nR\u2019s apply functions all work in slightly different ways, but `sapply` will work great here. We feed `sapply` our vector of names and our function that we just came up with. It runs through the rows of the vector of names, and sends each name to the function. The results of all these string splits are all combined up into a vector as output from the `sapply` function, which we then store to a new column in our original dataframe, called Title.\n\nFinally, we may wish to strip off those spaces from the beginning of the titles. Here we can just substitute the first occurrence of a space with nothing. We can use `sub` for this ( `gsub` would replace all spaces, poor \u201cthe Countess\u201d would look strange then though):\n\n```\n>combi$Title<-sub(' ','',combi$Title)\n```\n\nAlright, we now have a nice new column of titles, let\u2019s have a look at it:\n\n```\n>table(combi$Title)CaptColDonDonaDrJonkheerLady1411811MajorMasterMissMlleMmeMrMrs26126021757197MsRevSirtheCountess2811\n```\n\nHmm, there are a few very rare titles in here that won\u2019t...",
      "url": "https://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Exploratory Data Analysis and Feature Engineering Techniques",
      "text": "Hira Akram Exploratory Data Analysis and Feature Engineering Techniques https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46\nExploratory Data Analysis and Feature Engineering Techniques\nHira Akram\n2024-07-31T19:37:05Z\n# Exploratory Data Analysis and Feature Engineering Techniques\n\n## Strategies for enhanced predictive modelling\n\n[Hira Akram](https://hirakram.medium.com/?source=post_page---byline--bf5a04afff46---------------------------------------)\n\n5 min read\n\n\nJan 13, 2021\n\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n[Image Source](https://dribbble.com/ousama_alsaadi)\n\n[_Titanic Kaggle competition_](https://www.kaggle.com/c/titanic) is a great place to understand the machine learning pipeline. In this article, we will discuss the preliminary steps involved in building a predictive model. Let\u2019s begin!\n\n**_Competition Summary:_**\n\nOur aim is to determine from a set of features whether a passenger would have survived the wreck. In other words, we will build a binary classification model that outputs \u20181\u2019 stating that the passenger had survived the Titanic sinking and \u20180\u2019 for not surviving.\n\nBelow is a visualization of the survival percentage in the provided training dataset. Based on this given percentage of ground truth; we will train **five** typical classification models and compare their performances:\n\n**_Exploratory Data Analysis:_**\n\nFirst and foremost step for building a machine learning model is EDA. This involves the following:\n\n- Comprehend the underlying information like numbers of rows and columns, data types, look at a few samples from the data, general figures like mean median mode values.\n- Indicate the proportion of missing values and decide how to address them.\n- Data visualization.\n\nSo let\u2019s explore the training set and establish an understanding of our data:\n\nWe gather the following information from the above code:\n\n- Training data comprises a total of 891 examples and 12 features; including the \u2018 _Survived_\u2019 flag.\n- We have a mix of discrete and continuous features.\n- There are three columns with missing values (Age, Cabin, Embarked)\n\nMoving on, now let\u2019s discuss each feature in detail to get a better intuition.\n\n1. **_Pclass_**\n\nOur first feature to consider is **Pclass.** This is a categorical feature and it represents the socio-economic status of the passengers. We have plotted a treemap which clearly illustrates that the majority of the passengers belonged to the _upper_ socio-economic status.\n\n. **_Name_**\n\nSince the passenger names can not be used as is; therefore we have extracted the titles within the data to make more sense of this information.\n\n. **_Embarked_**\n\nThere were two missing values in this columns. We can either remove these rows or replace them with the mode value. Deleting rows would ultimately lead to date loss and given dataset is already very small; therefore we will go with option two. We also noticed that a high number of passengers embarked on _Southampton._\n\nPress enter or click to view image in full size\n\n. **_Cabin_**\n\nAlthough a huge number of values were missing from this column, but we extracted the alphabets specifying the cabin category. I will shortly discuss how we can make use of this to recreate features.\n\n. **_Age_**\n\nBelow code approximates the missing age values using _IterativeImputer()_ function _._ Later on, we will utilize this column to build more complex features.\n\n**_Feature Engineering_**\n\nThis method is used for adding complexity to the existing data; by constructing new features on top of the pre-existing columns. In order to better represent the underlying structure of the data; we can either split or merge information from different columns. This is one of the many techniques that can enhance our model performance altogether.\n\nIn the Titanic dataset we will include **six** additional columns using this method. Just like we carefully transformed all the passenger names into their titles _;_ let\u2019s look at some more columns.\n\nFirstly, we will make use of the _Parch_ and _SibSp_ columns to count the number of family members that are on board, as below:\n\nMoreover, an attribute like _Fare per Family_ can also help understand the link between these two columns:\n\n_Ticket_ column in itself doesn\u2019t seem quite useful, so we will construct a categorical column based on the nature of this data:\n\nConcatenate the above-created column with the _Cabin_ column that we transformed earlier, to gain some more value out of these features:\n\nThe following two features can also be built using the _Age_ column. For _ageBins,_ we will split the ages into four discrete equal-sized buckets using the pandas _qcut()_ function.\n\nLastly, we will multiply _Age_ with _Fare_ to make a new _numerical_ column.\n\nInitially, the training dataset contained very simple features which were insufficient for the algorithm to search for patterns. Newly constructed features are designed to establish a correspondence between the dataset which ultimately enhances the model accuracy.\n\nNow that our features are ready, we can move on to the next steps that are involved in data preparation.\n\n**_Categorical Encoding_**\n\nSince many algorithms can only interpret numerical data, therefore, encoding the categorical features is an essential step. There are three common workarounds for encoding such features:\n\n1. One Hot Encoding (binary split)\n2. Label Encoding (numerical assignment to each category)\n3. Ordinal Encoding (ordered assignment to each category)\n\nIn our case, we will implement **label encoding** using sklearn _LabelEncoder()_ function. We will label encode the below-mentioned list of columns that contain categorical information in form of words, alphabets or alphanumeric. This task can be achieved by using the following code:\n\n\n['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'familyMembers', 'ticketInfo', 'Cabin', 'Cabin_ticketInfo']\n\n\n**_Standard Scaling_**\n\nContinuous features can largely vary, resulting in very slow convergence hence impacting the model performance on the whole. The term scaling suggests, that we redistribute the feature values upon a fixed range so that the data dispersion is standardized. Once this is achieved, the model will efficiently converge to the global minima.\n\nOriginally, the Titanic data only had a single feature i.e. Fare which required scaling. While constructing new advanced features we included two more. Here\u2019s how we\u2019ll carry out standard scaling on all the continuous data:\n\nBelow is a comparison between unscaled and scaled _\u2018Fare\u2019_ values. We can see that new _x range_ has approximately shrunk to \u00b12, also data is now distributed on zero _mean:_\n\nPress enter or click to view image in full size\n\n**_Conclusion_**\n\nNow that we feel confident about our data, let\u2019s use it to train a bunch of models and pick the algorithm which suits best! In the next article, we will make survival predictions on the Titanic dataset using five binary classification algorithms.\n\nHere are a few samples from the finalized training data:\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Titanic](https://medium.com/tag/titanic?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Data Preprocessing](https://medium.com/tag/data-preprocessing?source=post_page-----bf5a04afff46---------------------------------------)\n\n[Feature Engineering](https://medium.com/tag/feature-engineering?source=post_page-----bf5a04afff46---------------------------------------)\n\nFollow\n\n[**Published in Towards AI**](https://pub.towardsai.net/?source=post_page---post_publication_info--bf5a04afff46---------------------------------------)\n\n[86K followers](https://pub.towardsai.net/followers?source=post_page---post_publication_info--bf5a04afff46----------------------...",
      "url": "https://pub.towardsai.net/titanic-survival-prediction-i-bf5a04afff46"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "<div><div><article><p></p><h2>Titanic Survival Prediction \u2014 Kaggle Competition</h2><a href=\"#titanic-survival-prediction--kaggle-competition\"></a><p></p>\n<p>This project is my first Kaggle challenge as part of my Machine Learning Engineer learning path.\nThe goal of the competition is to predict whether a passenger survived the Titanic disaster using a dataset containing demographic and socio-economic information.\nYou can check the competition here: <a href=\"https://www.kaggle.com/competitions/titanic/\">https://www.kaggle.com/competitions/titanic/</a></p>\n<p>This repository includes:</p>\n<ul>\n<li>Exploratory Data Analysis (EDA)</li>\n<li>Feature Engineering</li>\n<li>Model Selection</li>\n<li>Training and Validation Pipelines</li>\n<li>A Kaggle-ready <code>submission.csv</code></li>\n<li>Notebook documentation</li>\n</ul>\n<hr/>\n<p></p><h2>Project Structure</h2><a href=\"#project-structure\"></a><p></p>\n<div><pre><code>\u251c\u2500\u2500 data/ # Training and test datasets\n\u251c\u2500\u2500 notebooks/\n\u2502 \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502 \u2514\u2500\u2500 02_feature_engineering_and_model.ipynb\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission.csv # Kaggle submission file\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre></div>\n<hr/>\n<p></p><h2>Exploratory Data Analysis (EDA)</h2><a href=\"#exploratory-data-analysis-eda\"></a><p></p>\n<p>During the analysis, I explored key aspects of the dataset, including:</p>\n<ul>\n<li>Distribution of variables such as Age, Fare, Pclass, SibSp, and Parch</li>\n<li>Survival differences by Sex and Pclass</li>\n<li>Missing values and their treatment</li>\n<li>Pearson correlation to check linear relationships</li>\n<li>Visualizations using bar charts, boxplots, and heatmaps</li>\n</ul>\n<p>A major insight:\n<strong>Most features do not have strong linear relationships with the target</strong>, which is important when choosing a model.\nThis suggested that linear models would not perform well, so tree-based models were preferred.</p>\n<hr/>\n<p></p><h2>Feature Engineering</h2><a href=\"#feature-engineering\"></a><p></p>\n<p>To improve model performance, I created several new features:</p>\n<p></p><h3>Family Size</h3><a href=\"#family-size\"></a><p></p>\n<p>Combined <code>SibSp + Parch + 1</code> to represent group sizes:</p>\n<ul>\n<li><strong>Alone</strong>: 1</li>\n<li><strong>Small family</strong>: 2\u20134</li>\n<li><strong>Large family</strong>: 5+</li>\n</ul>\n<p></p><h3>Age Groups</h3><a href=\"#age-groups\"></a><p></p>\n<p>Bucketized ages into categories:</p>\n<ul>\n<li><strong>Child</strong>: 0\u201312</li>\n<li><strong>Teen</strong>: 13\u201319</li>\n<li><strong>Adult</strong>: 20\u201364</li>\n<li><strong>Senior</strong>: 65+</li>\n</ul>\n<p></p><h3>One-Hot Encoding</h3><a href=\"#one-hot-encoding\"></a><p></p>\n<p>Applied to:</p>\n<ul>\n<li><code>Sex</code></li>\n<li><code>Pclass</code></li>\n<li><code>Embarked</code></li>\n</ul>\n<p>These transformations helped the model capture non-linear and categorical patterns more effectively.</p>\n<hr/>\n<p></p><h2>Model Selection</h2><a href=\"#model-selection\"></a><p></p>\n<p>Since the dataset does not show strong linear relationships, I chose <strong>Decision Trees</strong> for the first model.</p>\n<p>Initial model:</p>\n<ul>\n<li><strong>DecisionTreeClassifier</strong></li>\n<li>Achieved ~0.798 validation accuracy</li>\n<li>Scored ~0.753 on Kaggle (public leaderboard)</li>\n</ul>\n<p>Later, I tested <strong>XGBoost</strong>, a more powerful non-linear model, and improved the score to ~0.77 on Kaggle.</p>\n<p>More improvements are planned (hyperparameter tuning, ensembles, etc.).</p>\n<hr/>\n<p></p><h2>Results</h2><a href=\"#results\"></a><p></p>\n<p>This validates that tree-based models work significantly better for this dataset.</p>\n<hr/>\n<p></p><h2>Next Steps</h2><a href=\"#next-steps\"></a><p></p>\n<ul>\n<li>Hyperparameter tuning of XGBoost</li>\n<li>Trying Random Forest and Gradient Boosting</li>\n<li>Building ensembles</li>\n<li>Applying more advanced FE (e.g., Title extraction from names)</li>\n<li>Starting the <strong>Spaceship Titanic</strong> challenge</li>\n</ul>\n<hr/>\n<p></p><h2>Usage</h2><a href=\"#usage\"></a><p></p>\n<p>Run the notebooks in order:</p>\n<ol>\n<li><code>01_data_exploration.ipynb</code></li>\n<li><code>02_feature_engineering_and_model.ipynb</code></li>\n</ol>\n<p>Then generate your <code>submission.csv</code> and upload it to Kaggle.</p>\n<hr/>\n<p></p><h2>Author</h2><a href=\"#author\"></a><p></p>\n<p><strong>Paolo Giovanni Veliz Sulecio</strong>\nMachine Learning Engineer in progress\nPassionate about data, automation, and real-world problem solving.</p>\n<hr/>\n<p></p><h2>Support</h2><a href=\"#support\"></a><p></p>\n<p>If you found this useful, feel free to:</p>\n<ul>\n<li>Star the repository</li>\n<li>Fork it</li>\n<li>Connect with me on LinkedIn</li>\n</ul>\n</article></div></div>",
      "url": "https://github.com/PaoloVeliz/titanic-survival-prediction-ml"
    },
    {
      "title": "Feature Engineering, Data Exploration, and Classification using Titanic Dataset\u00b6",
      "text": "# Feature Engineering, Data Exploration, and Classification using Titanic Dataset [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#Feature-Engineering,-Data-Exploration,-and-Classification-using-Titanic-Dataset)\n\nIn machine learing, what perhaps is more important than defining a model or a neural network is data exploration. After we get the dataset, that is hopefully representative of the problem we are trying to solve, the first thing we do is to examine the data and see what characteristics and features are present. Are there any problems in the data? Are some categories under-represented? Are there missing values? Are there significant correlations between some features and outcome? Can we easily engineer new features from existing ones that will correlate better with the outcome? Data exploration not only helps to discover and eliminates some potential problems before model training begins, but will also makes our model more accurate by injecting some human intuition into the solution.\n\nIn this tutorial, we will use the Titanic passenger survival dataset to illustrate the concept of data exploration and feature engineering. We choose this dataset because it is simple, and thus allows us to focus on engineering the features. We will show how engineered features play an important role in prediction by being the most important feature in machine learning models. Then, we will illustrate compare several methods of binary classification that can used to predict whether a passenger survives the disaster.\n\nThe Titanic dataset can be [download from this website](https://storage.googleapis.com/kaggle-forum-message-attachments/66979/2180/titanic.csv). Prediction Titanic passenger survival is also a [Kaggle challege](https://www.kaggle.com/c/titanic). Note that because the passenger survival information is public, the Kaggle challenge leaderboard is spammed with people submitting actual real-world data as machine learning results, making it meaningless.\n\n## 1\\. Data Exploration [\u00b6](https://xiangyutang2.github.io/feature-engineering/\\#1.-Data-Exploration)\n\nWe first load the \"titanic.csv\" file into a Pandas dataframe. We will immediately print the first few lines of the file.\n\nIn\u00a0\\[8\\]:\n\n```\nimport pandas as pd\n\ndata = pd.read_csv('titanic.csv')\ndata.head(10)\n\n```\n\nOut\\[8\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked | boat | body | home.dest |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C | NaN | 22.0 | Montevideo, Uruguay |\n\nWe immediately notice that the second column \"survived\" is our prediction result. The \"name\" column contains not only the first and last name, but also the **title**, which could be an important indicator of socio-economic status, which could be a factor in the survival probability. The \"sibsp\" columns contains the number of siblings and spouses a passenger have on the ship. The \"parch\" column is the number of parent or guardian the passenger have on the ship. The two columns added together is the size of the family together on the ship. The \"ticket\" column is the ticket number, which is unique for vast majority of the passengers. Some tickets have alphabets in front. The \"embarked\" column is the port of departure. This could be another indicator of socio-economic status along with the \"fare\" column. The \"boat\" and \"body\" column is information that should not be used to predict survival rate, since it is statistics gathered after the event. These two columns should be dropped. The \"cabin\" column is very interesting, it contains the **deck** information and the room number. Which deck the passenger resides in could be an indicator of survival rate, and should be extracted. Room number on the other hand, whithout knowing the actual layout of the ship, is not very useful. Finally, the \"home.dest\" column shows the home and destination of the passenger, which could be another indicator of socio-economic status.\n\nWe also immdediatly see that some data is missing. We need to examine in detail which data is missing and determine the best way to fill in these missing data without skewing the prediction results.\n\nWe can use the folloing command to list the categories in the dataset.\n\nIn\u00a0\\[9\\]:\n\n```\nprint(data.columns.values)\n\n```\n\n```\n['pclass' 'survived' 'name' 'sex' 'age' 'sibsp' 'parch' 'ticket' 'fare'\n 'cabin' 'embarked' 'boat' 'body' 'home.dest']\n\n```\n\nFrom these categories, will will drop \"boat\", \"body\" from the dataset since they cannot be used as predictors and they are also not the outcome. We will also drop the \"home.dest\" column since we already have many other good indicators of socio-economic status. Keeping the \"home.dest\" column will not cause harm, we just want to keep our model a simpler.\n\nIn\u00a0\\[10\\]:\n\n```\ndrop_cat=['boat','body','home.dest']\ndata.drop(drop_cat, inplace=True, axis=1)\ndata.head(10)\n\n```\n\nOut\\[10\\]:\n\n|  | pclass | survived | name | sex | age | sibsp | parch | ticket | fare | cabin | embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.00 | 0 | 0 | 24160 | 211.3375 | B5 | S |\n| 1 | 1 | 1 | Allison, Master. Hudson Trevor | male | 0.92 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 2 | 1 | 0 | Allison, Miss. Helen Loraine | female | 2.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 3 | 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 4 | 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.00 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S |\n| 5 | 1 | 1 | Anderson, Mr. Harry | male | 48.00 | 0 | 0 | 19952 | 26.5500 | E12 | S |\n| 6 | 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.00 | 1 | 0 | 13502 | 77.9583 | D7 | S |\n| 7 | 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.00 | 0 | 0 | 112050 | 0.0000 | A36 | S |\n| 8 | 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.00 | 2 | 0 | 11769 | 51.4792 | C101 | S |\n| 9 | 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.00 | 0 | 0 | PC 17609 | 49.5042 | NaN | C |\n\nLet use directly goto the point and see how many passengers survived the disaster.\n\nIn\u00a0\\[12\\]:\n\n```\ndata['survived'].mean()\n\n```\n\nOut\\[12\\]:\n\n```\n0.3819709702062643\n```\n\nAbout 40% of the passengers survived. We know from historical records that women and children were given priority on the lifeboats. Was this true, and did it reflect in their survival rates? Lets take a look.\n\nIn\u00a0\\[70\\]:\n\n```\nprint('Survival Rate by Sex')\nprint(data['survived'].groupby(data['sex']).mean())\nprint('\\n\\nSex Ratio of Passengers')\nprint(data['sex...",
      "url": "https://xiangyutang2.github.io/feature-engineering"
    },
    {
      "title": "Kaggle Challenge - Titanic Survival Prediction Feature Engineering | Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd48875ccba21&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Vishal Gupta](https://medium.com/@vishal8092009?source=post_page---byline--d48875ccba21---------------------------------------)\n\n10 min read\n\n\u00b7\n\nJun 2, 2020\n\n--\n\nListen\n\nShare\n\nTITANIC SURVIVAL PREDICTION\n\nIn this tutorial, we will learn about one of the most popular datasets in data science. It will give you idea about how to analyze and relate with real conditions.\n\n## The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive compared to others.\n\nIn this challenge, we will need to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n_This post will help you start with data science and familiarize yourself with Machine Learning. The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck._\n\n## A. Variable Information here\n\n**PassengerId** is the unique id of the row (no effect on target)\n\n**Survived** is the target we are trying to predict 1 : Survived 0 : Not Survived\n\n**Pclass** (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has three unique values (1, 2 or 3)\n\n**Name**, **Sex** and **Age** are self-explanatory\n\n**SibSp** is the total number of the passengers\u2019 siblings and spouse\n\n**Parch** is the total number of the passengers\u2019 parents and children\n\n**Ticket** is the ticket number of the passenger\n\n**Fare** is the passenger fare\n\n**Cabin** is the cabin number of the passenger\n\n**Embarked** is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n\n```\nC = CherbourgQ = QueenstownS = Southampton\n```\n\n## Variable Notes\n\n**Pclass:** 1st = Upper 2nd = Middle 3rd = Lower\n\n**SibSp:** The dataset defines relationship as, Sibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**Parch:** The dataset defines relationship as, Parent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n## Below are the few questions we would like to answer after our analysis\n\n- Is there any relation between given info of passengers and their survival?\n- What is the survival rate for different age groups?\n- Was preference given to women and children for saving?\n- Did having higher social status help people improve survival chances?\n- What are the effects of being alone or with family?\n- Was the survival rate affected by passenger class\n- Can we predict if a passenger survived from the disaster with using machine-learning techniques?\n\n## Let\u2019s Get Started\n\n```\n# importing basic librariesimport warningswarnings.filterwarnings(\u2018ignore\u2019)import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport plotly as plyfrom scipy import statsimport math# reading the titanic train and test datatrain = pd.read_csv(\u2018https://bit.ly/kaggletrain')test = pd.read_csv(\u2018https://bit.ly/kaggletest')train.shape # (891,12)test.shape # (418,11)\n```\n\nWe can check the data head to see what kind of variables we have. Mainly we have two kinds of variables i.e. continuous and discrete.\n\nHere Survived is our target variable, so we save it in the variable y and save the _PassengerID_ for sake of result submission on _kaggle_.\n\n```\ny , testpassenger = train[\u2018Survived\u2019], test[\u2018PassengerID\u2019]\n```\n\nWe also drop the _passenger id_, as it is unique for each record and doesn\u2019t contribute anything for survival prediction. Also concatenate the train and test into one for the ease of performing EDA and missing value operations and feature engineering.\n\n```\ntrain = train.drop(\u2018PassengerId\u2019, axis = 1)test = test.drop(\u2018PassengerId\u2019, axis = 1)total = pd.concat([train,test],axis = 0, sort=False)total = total.reset_index(drop=True)# Saving variables into different listscat_var = [\u2018Pclass\u2019,\u2019Sex\u2019,\u2019SibSp\u2019,\u2019Parch\u2019,\u2019Embarked\u2019] # categoricaltarget = [\u2018Survived\u2019] # targetcon_var = [\u2018Age\u2019,\u2019Fare\u2019] # continuous\n```\n\n## B. Visualization\n\nTo plot categorical variables:\n\n```\ndef catplot(x, df):   fig, ax = plt.subplots(math.ceil(len(x)/3), 3, figsize = (20,10))   ax = ax.flatten()   for axes, cat in zip(ax, x):   if (cat == \u2018Survived\u2019):      sns.countplot(df[cat], ax = axes)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)   else:      sns.countplot(df[cat], hue=\u2019Survived\u2019, data = df, ax = axes)      axes.legend(title=\u2019Survived ?\u2019, labels = [\u2018No\u2019,\u2019Yes\u2019],                      loc= \u2018upper right\u2019)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)# call the plotcatplot(target + cat_var, total)\n```\n\nPress enter or click to view image in full size\n\nCategorical Variable Plots\n\nTo plot the continuous variables:\n\n```\ndef cont(x, df):   fig, ax = plt.subplots(1,3, figsize = (18,4))   sns.distplot( df[df[x].notna()][x] , fit = stats.norm,ax =ax[0],                     bins = 12)   # fit parameter in distplot fits a function of distribution   # passed in argumentstats.probplot( df[x].fillna(df[x].median()), plot=ax[1])   sns.boxplot(y = df[x], ax = ax[2])   plt.show()# call the plotcont(\u2018Age\u2019, total)\n```\n\n```\ncont(\u2018Fare\u2019, total)\n```\n\nDensity plots:\n\nFor Age and Fare variables we make density plots, to see if they have any impact on the survival\n\n```\nplot_con = [\u2018Age\u2019,\u2019Fare\u2019]for i in plot_con:   plt.figure(figsize=(12,4))   sns.distplot(train[train[\u2018Survived\u2019]==0][i], color=\u2019r\u2019, bins = 5)   sns.distplot(train[train[\u2018Survived\u2019]==1][i], color=\u2019g\u2019, bins = 5)   plt.legend(title=\u2019Survived vs \u2018+i, labels = [\u2018Dead\u2019,\u2019Alive\u2019],                    loc=\u2019upper right\u2019)   plt.show()\n```\n\n- People with lower age survived more\n- It can be concluded that higher fare leads to better survival chance, the binning of fare might improve decision boundary.\n- However that relationship might be result of other variables also.\n\n## C. Feature Engineering\n\nNow our next steps will be towards feature engineering\n\n**1\\. Feature Engineering \u2014 Name**:\n\nAfter a glimpse we can see that name itself cannot help us in any way, so we extract new variables from it i.e. Title and Last Name.\n\n```\ndef map_title(x):   if x in [\u2018Mr\u2019]:      return \u2018Mr\u2019   elif x in [\u2018Mrs\u2019, \u2018Mme\u2019]:      return \u2018Mrs\u2019   elif x in [\u2018Miss\u2019, \u2018Mlle\u2019, \u2018Ms\u2019]:      return \u2018Miss\u2019   # Master taken separate as all have children ageel...",
      "url": "https://medium.com/@vishal8092009/in-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21"
    },
    {
      "title": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa324ba577812&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Python in Plain English**](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\n\u00b7\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---publication_nav-78073def27b8-a324ba577812---------------------publication_nav------------------)\n\n[![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\n\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpython-in-plain-english&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fkaggle-titanic-challenge-create-them-features-a324ba577812&collection=Python+in+Plain+English&collectionId=78073def27b8&source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------post_publication_sidebar------------------)\n\n# Kaggle Titanic Challenge: Features Creation\n\n## Better Data, Better Model!\n\n[![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n\nFollow\n\n5 min read\n\n\u00b7\n\nApr 7, 2022\n\n--\n\nListen\n\nShare\n\nPhoto by [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n# Overview\n\nHey folks,\n\nIn the previous section of [**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),we looked at the distribution of different features. And their relationship with the prediction label, **Survived**, as well as among each other.\n\nWe noticed that some of the features like **Name** contain extra information about a passenger\u2019s **title** which can be useful for our model.\n\n**In this section,** we will extract useful details from different features and create new features. Let\u2019s get started!\n\nYou can find the complete notebook here:\n\n[**kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7 AmanKrishna/kaggle\\_titanic** \\\n\\\n**Machine Learning model build for survivor prediction for Kaggle's titanic competition \u2026**\\\n\\\ngithub.com](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\n\nFirst, let\u2019s import and combine our train & test data to avoid redoing the same steps for both separately.\n\n## Code\n\n# **New Feature:** Family Name\n\nThe **Name** feature looks something like this:\n\nFamily Names\n\n**Family Name** followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n\n## Why Family Name?\n\n_Hypothesis:_ If some members of a family survive then chances are others will too.\n\n## Code\n\n# New Feature: Title\n\nEach passenger has a title in our database ( **Mr./Mrs./Master** etc.). We can extract them and add them as a new feature **Title**.\n\nTitle associated with Name\n\nLet\u2019s group some of these Titles together using the below dictionary:\n\nTitle Dictionary\n\nHere is the final list of titles we will be using:\n\nList of Titles\n\n## Why Title?\n\nLooking at the below graph we can see that survival probability changes with Title\n\nTitle vs Survival\n\n## **Code**\n\n# New Feature: Cabin Section\n\nIf we look at the **Cabin** feature we can see that each cabin starts with an alphabet. These could be **Cabin sections** similar to the ones we have on trains or flights.\n\nCabin Starts with Alphabet\n\nAfter extracting and combining a few we get the following set of **Cabin Sections**\n\nSet of Cabins\n\n## Code\n\n# New Feature: Family Size\n\nIn [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we discovered that **family size** played a pivotal role in determining survival.\n\nFamily Size vs Survival Rate\n\nLet\u2019s create a new feature **Family\\_Size** by summing **Sibsp**& **Parch** features.\n\n## Code\n\n# New Feature: Family Size Grouped\n\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n\n## Code\n\n## Why grouping?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Binning family sizes into groups will help our model learn better.\n\n# New Feature: Grouping Ticket\n\nDuring [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) we noticed that the number of Unique **Tickets** was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of **passengers per ticket** we get the following survival rates:\n\nTicket\\_Group\\_Size vs Survival\n\n## Why group tickets?\n\n_Hypothesis:_ The same ticket will be shared among the family group as well as the friends group.\n\n## Code\n\n# Age\\_Bin & Fare\\_Bin\n\nWe have observed during [EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941) that Survival probability varied across **Age** and **Fare** segments. Let\u2019s bin them into separate features.\n\nAge/Fare Bins vs Survival Rates\n\n## Why Binning?\n\nWe will be using **Boosting and Bagging Tree-** based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n\n**Note:** The **age** feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n\n## Code\n\n# New Feature: Survival Rates\n\nThis feature will contain the survival probability for each **Ticket**, **Family Name** & **Cabin number**. **We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_Rate & Name\\_Survival\\_Rate**\n\nFor all the passengers travelling on the titanic the **survival probability is around 0.38**. But this probability varies according to Sex, Pclass & other f...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812?gi=750ac193f43c"
    }
  ]
}