{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ba1bfd",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Feature Importance & Optimization\n",
    "\n",
    "**Goal**: Understand what's driving the 0.9936 CV score and identify optimization opportunities.\n",
    "\n",
    "**Key Questions**:\n",
    "1. Which features are most important? (TF-IDF vs structural vs notebook-level)\n",
    "2. Can we reduce TF-IDF dimensions without losing performance?\n",
    "3. Are there redundant or correlated features?\n",
    "4. Does the model make intuitive sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Loading data and analyzing feature importance...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature importance data from exp_002\n",
    "# First, let's examine what features we have\n",
    "\n",
    "# Load a sample of the training data to see feature structure\n",
    "train_path = \"/home/data/train\"\n",
    "test_path = \"/home/data/test\"\n",
    "\n",
    "# Load a few training notebooks to understand feature distribution\n",
    "train_files = [f for f in os.listdir(train_path) if f.endswith('.json')][:5]\n",
    "print(f\"Found {len(train_files)} training files for inspection\")\n",
    "\n",
    "# Examine one notebook structure\n",
    "with open(os.path.join(train_path, train_files[0]), 'r') as f:\n",
    "    sample_nb = json.load(f)\n",
    "    \n",
    "print(\"\\nSample notebook structure:\")\n",
    "print(f\"Number of cells: {len(sample_nb['cell_type'])}\")\n",
    "print(f\"Cell types: {set(sample_nb['cell_type'].values())}\")\n",
    "print(f\"Sample cell source: {list(sample_nb['source'].values())[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reconstruct the feature engineering from exp_002 to understand features better\n",
    "# We'll load the experiment notebook to see what was done\n",
    "\n",
    "exp_002_path = \"/home/code/experiments/002_tfidf_features\"\n",
    "print(f\"Experiment folder: {exp_002_path}\")\n",
    "\n",
    "# Check if we can find the notebook\n",
    "import glob\n",
    "notebook_files = glob.glob(f\"{exp_002_path}/*.ipynb\")\n",
    "print(f\"Found notebooks: {notebook_files}\")\n",
    "\n",
    "# Let's also check the submission to verify it was generated\n",
    "submission_path = \"/home/submission/submission_002.csv\"\n",
    "if os.path.exists(submission_path):\n",
    "    submission_df = pd.read_csv(submission_path)\n",
    "    print(f\"\\nSubmission file exists: {submission_path}\")\n",
    "    print(f\"Shape: {submission_df.shape}\")\n",
    "    print(f\"Columns: {submission_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(submission_df.head())\n",
    "else:\n",
    "    print(f\"Submission file not found: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d878ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from the executor's output\n",
    "# Based on the executor's output, we have these top features:\n",
    "\n",
    "feature_importance_data = {\n",
    "    'feature': [\n",
    "        'relative_position', 'notebook_size', 'source_length_mean', 'word_count_std', \n",
    "        'source_length_std', 'word_count_mean', 'cell_type_code_mean', 'tfidf_730',\n",
    "        'tfidf_242', 'likely_first_cell', 'heading_model', 'tfidf_171', 'heading_test',\n",
    "        'tfidf_3', 'tfidf_213', 'tfidf_907', 'source_length', 'tfidf_324', 'char_count', 'word_count'\n",
    "    ],\n",
    "    'importance': [\n",
    "        2690, 2395, 755, 634, 507, 449, 398, 139, 90, 75, 65, 60, 51, 35, 35, 34, 32, 30, 27, 26\n",
    "    ]\n",
    "}\n",
    "\n",
    "importance_df = pd.DataFrame(feature_importance_data)\n",
    "print(\"Top 20 Features by Importance:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Categorize features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def categorize_feature(feature_name):\n",
    "    if 'tfidf_' in feature_name:\n",
    "        return 'TF-IDF'\n",
    "    elif 'heading_' in feature_name:\n",
    "        return 'Heading'\n",
    "    elif feature_name in ['relative_position', 'notebook_size', 'likely_first_cell']:\n",
    "        return 'Notebook-level'\n",
    "    elif any(x in feature_name for x in ['mean', 'std']):\n",
    "        return 'Statistical'\n",
    "    elif feature_name in ['source_length', 'char_count', 'word_count', 'line_count']:\n",
    "        return 'Basic length'\n",
    "    elif 'cell_type' in feature_name:\n",
    "        return 'Cell type'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(categorize_feature)\n",
    "\n",
    "# Group by category and sum importance\n",
    "category_importance = importance_df.groupby('category')['importance'].sum().sort_values(ascending=False)\n",
    "print(\"\\nImportance by Category:\")\n",
    "for cat, imp in category_importance.items():\n",
    "    print(f\"  {cat:15s}: {imp:4d}\")\n",
    "\n",
    "# Calculate percentages\n",
    "total_importance = category_importance.sum()\n",
    "print(f\"\\nPercentage by Category:\")\n",
    "for cat, imp in category_importance.items():\n",
    "    pct = imp / total_importance * 100\n",
    "    print(f\"  {cat:15s}: {pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top 15 individual features\n",
    "top_15 = importance_df.head(15)\n",
    "bars1 = ax1.barh(range(len(top_15)), top_15['importance'])\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15['feature'])\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_title('Top 15 Most Important Features')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Color by category\n",
    "color_map = {\n",
    "    'TF-IDF': '#ff9999',\n",
    "    'Notebook-level': '#66b3ff',\n",
    "    'Statistical': '#99ff99',\n",
    "    'Heading': '#ffcc99',\n",
    "    'Basic length': '#c2c2f0',\n",
    "    'Cell type': '#ffb3e6'\n",
    "}\n",
    "for i, (idx, row) in enumerate(top_15.iterrows()):\n",
    "    bars1[i].set_color(color_map.get(row['category'], 'gray'))\n",
    "\n",
    "# Category importance pie chart\n",
    "categories = category_importance.index\n",
    "values = category_importance.values\n",
    "colors = [color_map.get(cat, 'gray') for cat in categories]\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Feature Importance Distribution by Category')\n",
    "\n",
    "# Make percentage text white for better readability\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"1. Notebook-level features account for {category_importance.get('Notebook-level', 0)/total_importance*100:.1f}% of importance\")\n",
    "print(f\"2. TF-IDF features account for {category_importance.get('TF-IDF', 0)/total_importance*100:.1f}% of importance\")\n",
    "print(f\"3. Statistical features account for {category_importance.get('Statistical', 0)/total_importance*100:.1f}% of importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TF-IDF feature importance in detail\n",
    "print(\"=\"*60)\n",
    "print(\"TF-IDF FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract TF-IDF features from importance_df\n",
    "tfidf_features = importance_df[importance_df['category'] == 'TF-IDF'].copy()\n",
    "tfidf_features = tfidf_features.sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"Number of TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"Total TF-IDF importance: {tfidf_features['importance'].sum()}\")\n",
    "print(f\"Average TF-IDF importance: {tfidf_features['importance'].mean():.1f}\")\n",
    "print(f\"Top TF-IDF feature importance: {tfidf_features.iloc[0]['importance']}\")\n",
    "print(f\"Median TF-IDF importance: {tfidf_features['importance'].median():.1f}\")\n",
    "\n",
    "# Distribution of TF-IDF importance\n",
    "print(f\"\\nTF-IDF Importance Distribution:\")\n",
    "print(f\"  Features with importance > 100: {(tfidf_features['importance'] > 100).sum()}\")\n",
    "print(f\"  Features with importance > 50: {(tfidf_features['importance'] > 50).sum()}\")\n",
    "print(f\"  Features with importance > 10: {(tfidf_features['importance'] > 10).sum()}\")\n",
    "print(f\"  Features with importance <= 1: {(tfidf_features['importance'] <= 1).sum()}\")\n",
    "\n",
    "# Top TF-IDF features\n",
    "top_tfidf = tfidf_features.head(10)\n",
    "print(f\"\\nTop 10 TF-IDF features:\")\n",
    "for idx, row in top_tfidf.iterrows():\n",
    "    print(f\"  {row['feature']:12s}: {row['importance']:3d}\")\n",
    "\n",
    "# Plot TF-IDF importance distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(tfidf_features['importance'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(tfidf_features['importance'].mean(), color='red', linestyle='--', label=f'Mean: {tfidf_features[\"importance\"].mean():.1f}')\n",
    "plt.axvline(tfidf_features['importance'].median(), color='orange', linestyle='--', label=f'Median: {tfidf_features[\"importance\"].median():.1f}')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Number of TF-IDF Features')\n",
    "plt.title('Distribution of TF-IDF Feature Importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConclusion: {(tfidf_features['importance'] <= 1).sum()} out of {len(tfidf_features)} TF-IDF features ({(tfidf_features['importance'] <= 1).sum()/len(tfidf_features)*100:.1f}%) have very low importance and could potentially be removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the most important features in detail\n",
    "print(\"=\"*60)\n",
    "print(\"TOP FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features = importance_df.head(10)\n",
    "print(\"Top 10 features and their likely meaning:\")\n",
    "print()\n",
    "\n",
    "feature_meanings = {\n",
    "    'relative_position': 'Relative position within notebook (0-1) - likely the target variable or a derived feature',\n",
    "    'notebook_size': 'Total number of cells in the notebook',\n",
    "    'source_length_mean': 'Average source length across the notebook',\n",
    "    'word_count_std': 'Standard deviation of word count across notebook',\n",
    "    'source_length_std': 'Standard deviation of source length across notebook',\n",
    "    'word_count_mean': 'Average word count across the notebook',\n",
    "    'cell_type_code_mean': 'Proportion of code cells in notebook',\n",
    "    'tfidf_730': 'TF-IDF term (unknown which word)',\n",
    "    'tfidf_242': 'TF-IDF term (unknown which word)',\n",
    "    'likely_first_cell': 'Binary flag indicating if cell is likely first'\n",
    "}\n",
    "\n",
    "for idx, row in top_features.iterrows():\n",
    "    feature = row['feature']\n",
    "    importance = row['importance']\n",
    "    category = row['category']\n",
    "    meaning = feature_meanings.get(feature, 'Unknown meaning')\n",
    "    print(f\"{feature:25s} | Importance: {importance:4d} | Category: {category:15s}\")\n",
    "    print(f\"  Meaning: {meaning}\")\n",
    "    print()\n",
    "\n",
    "# Key insight: relative_position is the most important feature\n",
    "print(\"⚠️  CRITICAL OBSERVATION:\")\n",
    "print(f\"   'relative_position' is the #1 feature with importance {top_features.iloc[0]['importance']}\")\n",
    "print(\"   This suggests the model is using relative position as a strong predictor,\")\n",
    "print(\"   which might indicate data leakage or that we're predicting a derived feature.\")\n",
    "print()\n",
    "print(\"   Need to verify: Is 'relative_position' the actual target or a feature?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e93445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we can load actual training data to understand the target\n",
    "print(\"=\"*60)\n",
    "print(\"TARGET VARIABLE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load a few training notebooks to understand the target variable\n",
    "train_files = [f for f in os.listdir(train_path) if f.endswith('.json')][:3]\n",
    "\n",
    "for i, file in enumerate(train_files):\n",
    "    with open(os.path.join(train_path, file), 'r') as f:\n",
    "        nb = json.load(f)\n",
    "    \n",
    "    print(f\"\\nNotebook {i+1}: {file}\")\n",
    "    print(f\"  Number of cells: {len(nb['cell_type'])}\")\n",
    "    print(f\"  Cell types: {Counter(nb['cell_type'].values())}\")\n",
    "    \n",
    "    # Show first few cells\n",
    "    cell_ids = list(nb['source'].keys())[:5]\n",
    "    print(f\"  First 5 cells:\")\n",
    "    for cid in cell_ids:\n",
    "        cell_type = nb['cell_type'][cid]\n",
    "        source_preview = nb['source'][cid][:80].replace('\\n', ' ')\n",
    "        print(f\"    {cid}: {cell_type:7s} | {source_preview}...\")\n",
    "    \n",
    "    # Check if we have order information\n",
    "    if 'cell_order' in nb:\n",
    "        order = nb['cell_order']\n",
    "        print(f\"  Cell order (first 10): {order[:10]}\")\n",
    "        print(f\"  Position of first cell: {order.index(cell_ids[0]) if cell_ids[0] in order else 'Not found'}\")\n",
    "    else:\n",
    "        print(\"  No cell_order found - this might be test data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Based on the feature importance analysis:\")\n",
    "print(\"1. Notebook-level features (relative_position, notebook_size) dominate importance\")\n",
    "print(\"2. TF-IDF features contribute but many are low importance\")\n",
    "print(\"3. We can likely reduce TF-IDF dimensions significantly\")\n",
    "print(\"4. Need to verify if 'relative_position' is a feature or the target\")\n",
    "print()\n",
    "print(\"Recommendations:\")\n",
    "print(\"- Test TF-IDF dimensionality reduction (1000 → 500 → 250 → 100)\")\n",
    "print(\"- Verify model predictions make intuitive sense\")\n",
    "print(\"- Consider if we can simplify the feature set\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
