{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6470eeb",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "## Goal: Understand the data and establish proper validation\n",
    "\n",
    "This notebook will:\n",
    "1. Load and examine the training data structure\n",
    "2. Implement proper cross-validation (GroupKFold by notebook_id)\n",
    "3. Analyze the target distribution and patterns\n",
    "4. Explore feature engineering opportunities\n",
    "5. Establish baseline CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths\n",
    "train_path = Path('/home/data/train')\n",
    "test_path = Path('/home/data/test')\n",
    "train_orders_path = Path('/home/data/train_orders.csv')\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_orders = pd.read_csv(train_orders_path)\n",
    "print(f\"Train orders shape: {train_orders.shape}\")\n",
    "print(f\"Sample:\\n{train_orders.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377220ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a few notebooks to understand structure\n",
    "def load_notebook(notebook_path):\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load first few notebooks\n",
    "sample_notebooks = []\n",
    "for i, nb_file in enumerate(sorted(train_path.glob('*.json'))[:3]):\n",
    "    nb = load_notebook(nb_file)\n",
    "    sample_notebooks.append((nb_file.stem, nb))\n",
    "    print(f\"\\nNotebook {nb_file.stem}:\")\n",
    "    print(f\"  Cells: {len(nb['source'])}\")\n",
    "    print(f\"  Cell types: {set(nb['cell_type'].values())}\")\n",
    "    \n",
    "    # Show first few cells\n",
    "    cell_ids = list(nb['source'].keys())[:5]\n",
    "    for cid in cell_ids:\n",
    "        print(f\"  Cell {cid}: {nb['cell_type'][cid]} - {nb['source'][cid][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full training dataframe\n",
    "def build_training_data():\n",
    "    all_data = []\n",
    "    \n",
    "    for nb_file in train_path.glob('*.json'):\n",
    "        notebook_id = nb_file.stem\n",
    "        with open(nb_file, 'r') as f:\n",
    "            nb = json.load(f)\n",
    "        \n",
    "        # Get cell order from train_orders\n",
    "        cell_order = train_orders[train_orders['id'] == notebook_id]['cell_order'].iloc[0]\n",
    "        cell_order_list = cell_order.split()\n",
    "        \n",
    "        # Create position mapping\n",
    "        position_map = {cell_id: pos for pos, cell_id in enumerate(cell_order_list)}\n",
    "        \n",
    "        # Extract features for each cell\n",
    "        for cell_id, source in nb['source'].items():\n",
    "            cell_type = nb['cell_type'][cell_id]\n",
    "            \n",
    "            # Basic features\n",
    "            source_length = len(source)\n",
    "            lines = source.split('\\n')\n",
    "            line_count = len(lines)\n",
    "            word_count = len(source.split())\n",
    "            char_count = len(source.replace('\\n', ''))\n",
    "            \n",
    "            # Binary flags\n",
    "            has_import = int('import ' in source)\n",
    "            has_comment = int('#' in source)\n",
    "            has_heading = int(any(line.strip().startswith('#') for line in lines if line.strip()))\n",
    "            has_code_block = int('```' in source)\n",
    "            has_link = int('http' in source or 'www.' in source)\n",
    "            \n",
    "            all_data.append({\n",
    "                'notebook_id': notebook_id,\n",
    "                'cell_id': cell_id,\n",
    "                'position': position_map[cell_id],\n",
    "                'cell_type': cell_type,\n",
    "                'source_length': source_length,\n",
    "                'line_count': line_count,\n",
    "                'word_count': word_count,\n",
    "                'char_count': char_count,\n",
    "                'has_import': has_import,\n",
    "                'has_comment': has_comment,\n",
    "                'has_heading': has_heading,\n",
    "                'has_code_block': has_code_block,\n",
    "                'has_link': has_link,\n",
    "                'source': source  # Keep source for text analysis\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "train_df = build_training_data()\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8964afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "print(\"Target (position) distribution:\")\n",
    "print(train_df['position'].describe())\n",
    "\n",
    "# Analyze by notebook\n",
    "notebook_stats = train_df.groupby('notebook_id').agg({\n",
    "    'cell_id': 'count',\n",
    "    'position': ['min', 'max']\n",
    "}).round(2)\n",
    "notebook_stats.columns = ['num_cells', 'min_pos', 'max_pos']\n",
    "notebook_stats['pos_range'] = notebook_stats['max_pos'] - notebook_stats['min_pos']\n",
    "\n",
    "print(f\"\\nNotebook statistics:\")\n",
    "print(notebook_stats.describe())\n",
    "\n",
    "# Plot distribution of notebook sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "notebook_stats['num_cells'].hist(bins=50)\n",
    "plt.title('Distribution of Notebook Sizes (Number of Cells)')\n",
    "plt.xlabel('Number of Cells')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNotebooks with < 5 cells: {(notebook_stats['num_cells'] < 5).sum()}\")\n",
    "print(f\"Notebooks with > 50 cells: {(notebook_stats['num_cells'] > 50).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement proper cross-validation\n",
    "def kendall_tau_per_notebook(y_true, y_pred, groups):\n",
    "    \"\"\"Calculate Kendall tau correlation per notebook, then average\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for notebook_id in np.unique(groups):\n",
    "        mask = groups == notebook_id\n",
    "        y_true_nb = y_true[mask]\n",
    "        y_pred_nb = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_nb) > 1:  # Need at least 2 cells for correlation\n",
    "            tau = kendalltau(y_true_nb, y_pred_nb).correlation\n",
    "            if not np.isnan(tau):\n",
    "                results.append(tau)\n",
    "    \n",
    "    return np.mean(results) if results else 0.0\n",
    "\n",
    "# Create GroupKFold\n",
    "n_splits = 5\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "groups = train_df['notebook_id']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = ['source_length', 'line_count', 'word_count', 'char_count', \n",
    "                'has_import', 'has_comment', 'has_heading', \n",
    "                'has_code_block', 'has_link', 'cell_type_code']\n",
    "\n",
    "# Prepare data\n",
    "train_df['cell_type_code'] = (train_df['cell_type'] == 'code').astype(int)\n",
    "\n",
    "print(f\"Running {n_splits}-fold GroupKFold cross-validation...\")\n",
    "print(f\"Groups (notebooks): {len(np.unique(groups))}\")\n",
    "print(f\"Total samples: {len(train_df)}\")\n",
    "\n",
    "fold_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups)):\n",
    "    X_train = train_df.iloc[train_idx][feature_cols]\n",
    "    y_train = train_df.iloc[train_idx]['position']\n",
    "    X_val = train_df.iloc[val_idx][feature_cols]\n",
    "    y_val = train_df.iloc[val_idx]['position']\n",
    "    groups_val = train_df.iloc[val_idx]['notebook_id']\n",
    "    \n",
    "    # Train LightGBM\n",
    "    import lightgbm as lgb\n",
    "    model = lgb.LGBMRegressor(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate Kendall tau per notebook\n",
    "    score = kendall_tau_per_notebook(y_val.values, y_pred, groups_val.values)\n",
    "    fold_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Score: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "# Train on full data to get feature importance\n",
    "import lightgbm as lgb\n",
    "\n",
    "X_full = train_df[feature_cols]\n",
    "y_full = train_df['position']\n",
    "\n",
    "model_full = lgb.LGBMRegressor(random_state=42, n_estimators=100)\n",
    "model_full.fit(X_full, y_full)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model_full.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text content patterns\n",
    "# Look at common words in different positions\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_words(text):\n",
    "    \"\"\"Extract words from text\"\"\"\n",
    "    return re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "\n",
    "# Sample analysis: look at first vs last cells\n",
    "first_cells = train_df[train_df['position'] == 0]['source'].head(100).tolist()\n",
    "last_cells = train_df.groupby('notebook_id').apply(\n",
    "    lambda x: x[x['position'] == x['position'].max()]['source'].iloc[0]\n",
    ").head(100).tolist()\n",
    "\n",
    "# Count words in first cells\n",
    "first_words = []\n",
    "for text in first_cells:\n",
    "    first_words.extend(extract_words(text))\n",
    "\n",
    "# Count words in last cells  \n",
    "last_words = []\n",
    "for text in last_cells:\n",
    "    last_words.extend(extract_words(text))\n",
    "\n",
    "first_counter = Counter(first_words)\n",
    "last_counter = Counter(last_words)\n",
    "\n",
    "print(\"Most common words in FIRST cells:\")\n",
    "for word, count in first_counter.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nMost common words in LAST cells:\")\n",
    "for word, count in last_counter.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Find words that are more common in first vs last\n",
    "first_unique = set(first_counter.keys()) - set(last_counter.keys())\n",
    "last_unique = set(last_counter.keys()) - set(first_counter.keys())\n",
    "\n",
    "print(f\"\\nWords only in first cells: {list(first_unique)[:10]}\")\n",
    "print(f\"Words only in last cells: {list(last_unique)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d31e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze markdown heading patterns\n",
    "markdown_df = train_df[train_df['cell_type'] == 'markdown'].copy()\n",
    "\n",
    "def extract_headings(text):\n",
    "    \"\"\"Extract markdown headings\"\"\"\n",
    "    headings = []\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('#'):\n",
    "            # Count heading level\n",
    "            level = len(line) - len(line.lstrip('#'))\n",
    "            if 1 <= level <= 6:\n",
    "                headings.append((level, line.lstrip('#').strip()))\n",
    "    return headings\n",
    "\n",
    "# Analyze heading patterns\n",
    "heading_data = []\n",
    "for _, row in markdown_df.iterrows():\n",
    "    headings = extract_headings(row['source'])\n",
    "    for level, text in headings:\n",
    "        heading_data.append({\n",
    "            'notebook_id': row['notebook_id'],\n",
    "            'cell_id': row['cell_id'],\n",
    "            'position': row['position'],\n",
    "            'heading_level': level,\n",
    "            'heading_text': text\n",
    "        })\n",
    "\n",
    "if heading_data:\n",
    "    heading_df = pd.DataFrame(heading_data)\n",
    "    print(f\"Found {len(heading_df)} headings in {len(heading_df['notebook_id'].unique())} notebooks\")\n",
    "    \n",
    "    # Analyze heading level by position\n",
    "    print(\"\\nAverage position by heading level:\")\n",
    "    print(heading_df.groupby('heading_level')['position'].mean().round(2))\n",
    "    \n",
    "    # Most common heading texts\n",
    "    print(\"\\nMost common heading texts:\")\n",
    "    print(heading_df['heading_text'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"No headings found in sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0687177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS FROM ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. DATASET SIZE:\")\n",
    "print(f\"   - Total training samples: {len(train_df):,}\")\n",
    "print(f\"   - Total notebooks: {train_df['notebook_id'].nunique():,}\")\n",
    "print(f\"   - Average cells per notebook: {len(train_df) / train_df['notebook_id'].nunique():.1f}\")\n",
    "\n",
    "print(f\"\\n2. CROSS-VALIDATION SCORE:\")\n",
    "print(f\"   - Mean Kendall tau: {np.mean(fold_scores):.4f}\")\n",
    "print(f\"   - Std dev: {np.std(fold_scores):.4f}\")\n",
    "print(f\"   - Individual folds: {[f'{s:.4f}' for s in fold_scores]}\")\n",
    "\n",
    "print(f\"\\n3. FEATURE IMPORTANCE:\")\n",
    "for _, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.0f}\")\n",
    "\n",
    "print(f\"\\n4. KEY PATTERNS:\")\n",
    "print(f\"   - Word count is most important feature\")\n",
    "print(f\"   - Basic length features dominate importance\")\n",
    "print(f\"   - Binary flags have lower importance\")\n",
    "print(f\"   - Cell type (code/markdown) has moderate importance\")\n",
    "\n",
    "print(f\"\\n5. NEXT STEPS:\")\n",
    "print(f\"   - Add TF-IDF features for semantic content\")\n",
    "print(f\"   - Extract markdown heading features\")\n",
    "print(f\"   - Try pairwise ranking formulation\")\n",
    "print(f\"   - Use more training data (not just 30%)\")\n",
    "print(f\"   - Experiment with text embeddings\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
