{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10514f4",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: Understanding the Cell Ordering Problem\n",
    "\n",
    "**Objective**: After discovering data leakage in exp_002, we now have a true baseline of 0.4014 CV. We need to fundamentally rethink our approach to close the 0.5 point gap to gold (0.9006).\n",
    "\n",
    "**Key Questions**:\n",
    "1. What makes cell ordering predictable from a human perspective?\n",
    "2. Why did TF-IDF fail (only 5% importance)?\n",
    "3. What features would capture logical flow and semantic relationships?\n",
    "4. Should we reformulate as a ranking problem instead of regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da80be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths\n",
    "TRAIN_PATH = Path('/home/data/train')\n",
    "ORDERS_PATH = Path('/home/data/train_orders.csv')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "orders_df = pd.read_csv(ORDERS_PATH)\n",
    "\n",
    "# Sample notebooks for analysis\n",
    "np.random.seed(42)\n",
    "sample_notebooks = np.random.choice(orders_df['id'].unique(), size=100, replace=False)\n",
    "\n",
    "print(f\"Loaded {len(orders_df)} cell orderings across {orders_df['id'].nunique()} notebooks\")\n",
    "print(f\"Sampling {len(sample_notebooks)} notebooks for detailed analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cell ordering patterns in sample notebooks\n",
    "print(\"Analyzing cell ordering patterns...\")\n",
    "\n",
    "# Load a few notebooks to understand structure\n",
    "notebook_analysis = []\n",
    "\n",
    "for notebook_id in sample_notebooks[:10]:  # Look at first 10 in detail\n",
    "    notebook_path = TRAIN_PATH / f\"{notebook_id}.json\"\n",
    "    if notebook_path.exists():\n",
    "        with open(notebook_path, 'r') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        cells = notebook['source']\n",
    "        \n",
    "        # Get cell order from orders_df\n",
    "        cell_order = orders_df[orders_df['id'] == notebook_id]['cell_order'].iloc[0]\n",
    "        ordered_indices = [int(x) for x in cell_order.split()]\n",
    "        \n",
    "        # Analyze cell types and content by position\n",
    "        for pos, cell_idx in enumerate(ordered_indices):\n",
    "            cell = cells[cell_idx]\n",
    "            cell_type = cell['cell_type']\n",
    "            source = cell['source']\n",
    "            \n",
    "            # Extract first line for analysis\n",
    "            first_line = source.split('\\n')[0][:100] if source else ''\n",
    "            \n",
    "            notebook_analysis.append({\n",
    "                'notebook_id': notebook_id,\n",
    "                'position': pos,\n",
    "                'cell_type': cell_type,\n",
    "                'source_preview': first_line,\n",
    "                'source_length': len(source) if source else 0\n",
    "            })\n",
    "\n",
    "df_analysis = pd.DataFrame(notebook_analysis)\n",
    "print(f\"\\nAnalyzed {len(df_analysis)} cells from {len(df_analysis['notebook_id'].unique())} notebooks\")\n",
    "print(\"\\nFirst few cells:\")\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cell type patterns by position\n",
    "print(\"Cell type distribution by position:\")\n",
    "position_analysis = df_analysis.groupby('position')['cell_type'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(position_analysis.head(10))\n",
    "\n",
    "# Look at first 5 positions in detail\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST 5 POSITIONS - Content Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "first_cells = df_analysis[df_analysis['position'] < 5].copy()\n",
    "print(f\"\\nTotal cells in first 5 positions: {len(first_cells)}\")\n",
    "print(\"\\nCell type distribution:\")\n",
    "print(first_cells['cell_type'].value_counts())\n",
    "\n",
    "print(\"\\nSample content from first positions:\")\n",
    "for pos in range(5):\n",
    "    print(f\"\\n--- Position {pos} ---\")\n",
    "    sample_cells = first_cells[first_cells['position'] == pos]\n",
    "    for _, cell in sample_cells.head(3).iterrows():\n",
    "        print(f\"  {cell['cell_type']}: {cell['source_preview'][:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAST 5 POSITIONS - Content Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get max position per notebook\n",
    "max_positions = df_analysis.groupby('notebook_id')['position'].max()\n",
    "last_cells = []\n",
    "for notebook_id, max_pos in max_positions.items():\n",
    "    if max_pos >= 5:  # Only notebooks with at least 6 cells\n",
    "        last_positions = df_analysis[\n",
    "            (df_analysis['notebook_id'] == notebook_id) & \n",
    "            (df_analysis['position'] >= max_pos - 4)\n",
    "        ]\n",
    "        last_cells.extend(last_positions.to_dict('records'))\n",
    "\n",
    "last_df = pd.DataFrame(last_cells)\n",
    "print(f\"\\nTotal cells in last 5 positions: {len(last_df)}\")\n",
    "print(\"\\nCell type distribution:\")\n",
    "print(last_df['cell_type'].value_counts())\n",
    "\n",
    "print(\"\\nSample content from last positions:\")\n",
    "for notebook_id in last_df['notebook_id'].unique()[:3]:\n",
    "    notebook_cells = last_df[last_df['notebook_id'] == notebook_id]\n",
    "    max_pos = notebook_cells['position'].max()\n",
    "    print(f\"\\n--- Notebook {notebook_id} (max pos: {max_pos}) ---\")\n",
    "    for _, cell in notebook_cells.iterrows():\n",
    "        rel_pos = max_pos - cell['position']\n",
    "        print(f\"  Position {cell['position']} (last-{rel_pos}): {cell['cell_type']}: {cell['source_preview'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze semantic patterns that indicate position\n",
    "print(\"=\"*80)\n",
    "print(\"SEMANTIC PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Look for common terms in different positions\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract words from first and last positions\n",
    "first_position_words = []\n",
    "last_position_words = []\n",
    "middle_position_words = []\n",
    "\n",
    "for _, cell in df_analysis.iterrows():\n",
    "    if cell['source']:\n",
    "        # Simple word extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', cell['source'].lower())\n",
    "        \n",
    "        if cell['position'] == 0:\n",
    "            first_position_words.extend(words)\n",
    "        elif cell['position'] >= 5:  # Last positions (notebooks have varying lengths)\n",
    "            last_position_words.extend(words)\n",
    "        else:\n",
    "            middle_position_words.extend(words)\n",
    "\n",
    "# Count most common words\n",
    "first_counts = Counter(first_position_words)\n",
    "last_counts = Counter(last_position_words)\n",
    "middle_counts = Counter(middle_position_words)\n",
    "\n",
    "print(\"\\nTop words in FIRST position:\")\n",
    "for word, count in first_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop words in MIDDLE positions:\")\n",
    "for word, count in middle_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop words in LAST positions:\")\n",
    "for word, count in last_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Calculate word importance (frequency difference)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORDS STRONGLY ASSOCIATED WITH POSITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Words much more common in first vs last\n",
    "first_bias = {}\n",
    "for word in set(list(first_counts.keys()) + list(last_counts.keys())):\n",
    "    first_freq = first_counts.get(word, 0) / max(len(first_position_words), 1)\n",
    "    last_freq = last_counts.get(word, 0) / max(len(last_position_words), 1)\n",
    "    if first_freq > 0 and last_freq > 0:\n",
    "        ratio = first_freq / last_freq\n",
    "        if ratio > 3:  # 3x more common in first\n",
    "            first_bias[word] = ratio\n",
    "\n",
    "print(\"\\nWords 3x more common in FIRST position:\")\n",
    "for word, ratio in sorted(first_bias.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(f\"  {word}: {ratio:.1f}x more common\")\n",
    "\n",
    "# Words much more common in last vs first\n",
    "last_bias = {}\n",
    "for word in set(list(first_counts.keys()) + list(last_counts.keys())):\n",
    "    first_freq = first_counts.get(word, 0) / max(len(first_position_words), 1)\n",
    "    last_freq = last_counts.get(word, 0) / max(len(last_position_words), 1)\n",
    "    if first_freq > 0 and last_freq > 0:\n",
    "        ratio = last_freq / first_freq\n",
    "        if ratio > 3:  # 3x more common in last\n",
    "            last_bias[word] = ratio\n",
    "\n",
    "print(\"\\nWords 3x more common in LAST position:\")\n",
    "for word, ratio in sorted(last_bias.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(f\"  {word}: {ratio:.1f}x more common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze markdown structure patterns\n",
    "print(\"=\"*80)\n",
    "print(\"MARKDOWN STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "markdown_cells = df_analysis[df_analysis['cell_type'] == 'markdown'].copy()\n",
    "\n",
    "# Look for heading patterns\n",
    "heading_patterns = []\n",
    "for _, cell in markdown_cells.iterrows():\n",
    "    source = cell['source']\n",
    "    if source:\n",
    "        lines = source.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                heading_level = len(line) - len(line.lstrip('#'))\n",
    "                heading_text = line.strip('# ').lower()\n",
    "                heading_patterns.append({\n",
    "                    'position': cell['position'],\n",
    "                    'heading_level': heading_level,\n",
    "                    'heading_text': heading_text[:50]\n",
    "                })\n",
    "\n",
    "if heading_patterns:\n",
    "    headings_df = pd.DataFrame(heading_patterns)\n",
    "    print(f\"\\nFound {len(headings_df)} headings in sample\")\n",
    "    \n",
    "    print(\"\\nHeading level distribution:\")\n",
    "    print(headings_df['heading_level'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nAverage position by heading level:\")\n",
    "    avg_pos = headings_df.groupby('heading_level')['position'].mean()\n",
    "    for level, pos in avg_pos.items():\n",
    "        print(f\"  Level {level}: {pos:.1f}\")\n",
    "    \n",
    "    # Most common heading texts\n",
    "    print(\"\\nMost common heading texts:\")\n",
    "    print(headings_df['heading_text'].value_counts().head(10))\n",
    "\n",
    "# Analyze code cell patterns\n",
    "code_cells = df_analysis[df_analysis['cell_type'] == 'code'].copy()\n",
    "print(f\"\\n\\nCODE CELL ANALYSIS\")\n",
    "print(f\"Total code cells: {len(code_cells)}\")\n",
    "\n",
    "# Look for import patterns\n",
    "import_cells = code_cells[code_cells['source_preview'].str.contains('import', case=False, na=False)]\n",
    "print(f\"Code cells with 'import': {len(import_cells)}\")\n",
    "if len(import_cells) > 0:\n",
    "    print(f\"Average position of import cells: {import_cells['position'].mean():.1f}\")\n",
    "\n",
    "# Look for function definitions\n",
    "function_cells = code_cells[code_cells['source_preview'].str.contains('def ', na=False)]\n",
    "print(f\"Code cells with function definitions: {len(function_cells)}\")\n",
    "if len(function_cells) > 0:\n",
    "    print(f\"Average position of function cells: {function_cells['position'].mean():.1f}\")\n",
    "\n",
    "# Look for print/display patterns\n",
    "print_cells = code_cells[code_cells['source_preview'].str.contains('print|display|show', case=False, na=False)]\n",
    "print(f\"Code cells with print/display: {len(print_cells)}\")\n",
    "if len(print_cells) > 0:\n",
    "    print(f\"Average position of print cells: {print_cells['position'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a66295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cell type transitions\n",
    "print(\"=\"*80)\n",
    "print(\"CELL TYPE TRANSITION PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For each notebook, analyze transitions between cell types\n",
    "transitions = []\n",
    "for notebook_id in df_analysis['notebook_id'].unique():\n",
    "    notebook_cells = df_analysis[df_analysis['notebook_id'] == notebook_id].sort_values('position')\n",
    "    \n",
    "    for i in range(len(notebook_cells) - 1):\n",
    "        current_type = notebook_cells.iloc[i]['cell_type']\n",
    "        next_type = notebook_cells.iloc[i + 1]['cell_type']\n",
    "        current_pos = notebook_cells.iloc[i]['position']\n",
    "        \n",
    "        transitions.append({\n",
    "            'from_type': current_type,\n",
    "            'to_type': next_type,\n",
    "            'position': current_pos\n",
    "        })\n",
    "\n",
    "transitions_df = pd.DataFrame(transitions)\n",
    "print(f\"Total transitions analyzed: {len(transitions_df)}\")\n",
    "\n",
    "print(\"\\nTransition patterns (from → to):\")\n",
    "transition_counts = transitions_df.groupby(['from_type', 'to_type']).size().unstack(fill_value=0)\n",
    "print(transition_counts)\n",
    "\n",
    "# Normalize to percentages\n",
    "transition_pct = transition_counts.div(transition_counts.sum(axis=1), axis=0) * 100\n",
    "print(\"\\nTransition percentages:\")\n",
    "print(transition_pct.round(1))\n",
    "\n",
    "# Analyze position-specific transitions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POSITION-SPECIFIC TRANSITIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First position transitions (position 0 → 1)\n",
    "first_trans = transitions_df[transitions_df['position'] == 0]\n",
    "if len(first_trans) > 0:\n",
    "    print(\"\\nFirst position transitions (pos 0 → 1):\")\n",
    "    print(first_trans['from_type'].value_counts())\n",
    "\n",
    "# Look for common patterns\n",
    "print(\"\\nCommon patterns:\")\n",
    "markdown_to_code = len(transitions_df[(transitions_df['from_type'] == 'markdown') & (transitions_df['to_type'] == 'code')])\n",
    "code_to_markdown = len(transitions_df[(transitions_df['from_type'] == 'code') & (transitions_df['to_type'] == 'markdown')])\n",
    "\n",
    "print(f\"Markdown → Code transitions: {markdown_to_code} ({markdown_to_code/len(transitions_df)*100:.1f}%)\")\n",
    "print(f\"Code → Markdown transitions: {code_to_markdown} ({code_to_markdown/len(transitions_df)*100:.1f}%)\")\n",
    "\n",
    "# Consecutive same-type transitions\n",
    "same_type = len(transitions_df[transitions_df['from_type'] == transitions_df['to_type']])\n",
    "print(f\"Same type transitions: {same_type} ({same_type/len(transitions_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights summary\n",
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "insights = [\n",
    "    \"1. LOGICAL FLOW PATTERNS:\",\n",
    "    \"   - First positions: 'import', 'load', 'read', 'data' (setup/loading)\",\n",
    "    \"   - Middle positions: 'analysis', 'explore', 'visualize', 'plot' (processing)\",\n",
    "    \"   - Last positions: 'result', 'conclusion', 'summary', 'print', 'show' (output)\",\n",
    "    \"   \",\n",
    "    \"2. CELL TYPE TRANSITIONS:\",\n",
    "    \"   - Markdown → Code is common (explanation followed by implementation)\",\n",
    "    \"   - Code → Markdown also occurs (results followed by interpretation)\",\n",
    "    \"   - Pattern suggests narrative structure: explain → code → interpret → repeat\",\n",
    "    \"   \",\n",
    "    \"3. HEADING HIERARCHY:\",\n",
    "    \"   - Headings show position patterns (Level 1 earlier, Level 2/3 later)\",\n",
    "    \"   - Common headings: 'Introduction', 'EDA', 'Model', 'Results', 'Conclusion'\",\n",
    "    \"   - These are STRONG ordering signals that TF-IDF doesn't capture well\",\n",
    "    \"   \",\n",
    "    \"4. WHY TF-IDF FAILED:\",\n",
    "    \"   - TF-IDF treats 'introduction' and 'conclusion' as just words\",\n",
    "    \"   - Doesn't capture semantic meaning or structural role\",\n",
    "    \"   - Can't understand that 'import' belongs at start, 'conclusion' at end\",\n",
    "    \"   - 1000 dimensions but only 5% importance confirms this limitation\",\n",
    "    \"   \",\n",
    "    \"5. WHAT WE NEED:\",\n",
    "    \"   - Semantic embeddings that understand meaning (BERT, Sentence-T5)\",\n",
    "    \"   - Structural features that capture markdown hierarchy\",\n",
    "    \"   - Content-type modeling (code vs markdown have different patterns)\",\n",
    "    \"   - Learning-to-rank formulation (this is a ranking problem, not regression)\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION: RADICAL FEATURE ENGINEERING OVERHAUL\")\n",
    "print(\"=\"*80)\n",
    "print(\"Current approach (TF-IDF + stats) = 0.4014 CV\")\n",
    "print(\"Need: ~0.5 points improvement to reach 0.9006 gold\")\n",
    "print(\"\\nThis requires breakthrough features, not incremental improvements.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
