{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10514f4",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: Understanding the Cell Ordering Problem\n",
    "\n",
    "**Objective**: After discovering data leakage in exp_002, we now have a true baseline of 0.4014 CV. We need to fundamentally rethink our approach to close the 0.5 point gap to gold (0.9006).\n",
    "\n",
    "**Key Questions**:\n",
    "1. What makes cell ordering predictable from a human perspective?\n",
    "2. Why did TF-IDF fail (only 5% importance)?\n",
    "3. What features would capture logical flow and semantic relationships?\n",
    "4. Should we reformulate as a ranking problem instead of regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da80be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T19:00:07.685142Z",
     "iopub.status.busy": "2026-01-14T19:00:07.684428Z",
     "iopub.status.idle": "2026-01-14T19:00:08.414772Z",
     "shell.execute_reply": "2026-01-14T19:00:08.414159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 119256 cell orderings across 119256 notebooks\n",
      "Sampling 100 notebooks for detailed analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths\n",
    "TRAIN_PATH = Path('/home/data/train')\n",
    "ORDERS_PATH = Path('/home/data/train_orders.csv')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "orders_df = pd.read_csv(ORDERS_PATH)\n",
    "\n",
    "# Sample notebooks for analysis\n",
    "np.random.seed(42)\n",
    "sample_notebooks = np.random.choice(orders_df['id'].unique(), size=100, replace=False)\n",
    "\n",
    "print(f\"Loaded {len(orders_df)} cell orderings across {orders_df['id'].nunique()} notebooks\")\n",
    "print(f\"Sampling {len(sample_notebooks)} notebooks for detailed analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cell ordering patterns in sample notebooks\n",
    "print(\"Analyzing cell ordering patterns...\")\n",
    "\n",
    "# Load a few notebooks to understand structure\n",
    "notebook_analysis = []\n",
    "\n",
    "for notebook_id in sample_notebooks[:10]:  # Look at first 10 in detail\n",
    "    notebook_path = TRAIN_PATH / f\"{notebook_id}.json\"\n",
    "    if notebook_path.exists():\n",
    "        with open(notebook_path, 'r') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        cells = notebook['source']\n",
    "        \n",
    "        # Get cell order from orders_df\n",
    "        cell_order = orders_df[orders_df['id'] == notebook_id]['cell_order'].iloc[0]\n",
    "        ordered_indices = [int(x) for x in cell_order.split()]\n",
    "        \n",
    "        # Analyze cell types and content by position\n",
    "        for pos, cell_idx in enumerate(ordered_indices):\n",
    "            cell = cells[cell_idx]\n",
    "            cell_type = cell['cell_type']\n",
    "            source = cell['source']\n",
    "            \n",
    "            # Extract first line for analysis\n",
    "            first_line = source.split('\\n')[0][:100] if source else ''\n",
    "            \n",
    "            notebook_analysis.append({\n",
    "                'notebook_id': notebook_id,\n",
    "                'position': pos,\n",
    "                'cell_type': cell_type,\n",
    "                'source_preview': first_line,\n",
    "                'source_length': len(source) if source else 0\n",
    "            })\n",
    "\n",
    "df_analysis = pd.DataFrame(notebook_analysis)\n",
    "print(f\"\\nAnalyzed {len(df_analysis)} cells from {len(df_analysis['notebook_id'].unique())} notebooks\")\n",
    "print(\"\\nFirst few cells:\")\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618b200b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T19:57:24.384530Z",
     "iopub.status.busy": "2026-01-14T19:57:24.383881Z",
     "iopub.status.idle": "2026-01-14T19:57:24.403814Z",
     "shell.execute_reply": "2026-01-14T19:57:24.403241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing cell ordering patterns...\n",
      "\n",
      "=== Deep dive into notebook: 98ace2afde53ba ===\n",
      "Keys in notebook: ['cell_type', 'source']\n",
      "Number of cells: 37\n",
      "Sample cell IDs: ['3f963f6f', '274409d6', '0d15a172', '42eafa71', '5f9515b8']\n",
      "Sample cell types: ['code', 'code', 'code', 'code', 'code']\n",
      "\n",
      "cell_order contains 37 IDs: ['72e8bb6c', '3f963f6f', '274409d6', '0d15a172', '42eafa71']...\n",
      "Matching IDs between cell_order and notebook: 37/37\n",
      "\n",
      "=== Sample cells by position ===\n",
      "Position 0: markdown cell - **Taking equal rows as hinglish Datasets So that the data will be same for both the classes**...\n",
      "Position 1: code cell - # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by...\n",
      "Position 2: code cell - import pandas as pd english_text = pd.read_csv(\"../input/english_text.csv\") hinglish_text = pd.read_...\n",
      "\n",
      "=== KEY INSIGHT ===\n",
      "The notebook structure uses DICTIONARIES with hex string keys, not lists!\n",
      "cell_order contains these hex IDs in the correct execution order.\n",
      "Previous experiments incorrectly assumed integer indices.\n"
     ]
    }
   ],
   "source": [
    "# Analyze cell ordering patterns in sample notebooks\n",
    "print(\"Analyzing cell ordering patterns...\")\n",
    "\n",
    "# Load a few notebooks to understand structure\n",
    "notebook_analysis = []\n",
    "\n",
    "# Let's examine one notebook in detail first\n",
    "notebook_id = sample_notebooks[0]\n",
    "notebook_path = TRAIN_PATH / f\"{notebook_id}.json\"\n",
    "\n",
    "print(f\"\\n=== Deep dive into notebook: {notebook_id} ===\")\n",
    "\n",
    "with open(notebook_path, 'r') as f:\n",
    "    notebook_data = json.load(f)\n",
    "\n",
    "print(f\"Keys in notebook: {list(notebook_data.keys())}\")\n",
    "print(f\"Number of cells: {len(notebook_data.get('cell_type', {}))}\")\n",
    "\n",
    "# The structure is: cell_type and source are dictionaries with hex string keys\n",
    "# NOT lists with integer indices!\n",
    "cell_types = notebook_data.get('cell_type', {})\n",
    "source_cells = notebook_data.get('source', {})\n",
    "\n",
    "print(f\"Sample cell IDs: {list(cell_types.keys())[:5]}\")\n",
    "print(f\"Sample cell types: {list(cell_types.values())[:5]}\")\n",
    "\n",
    "# Check the cell_order for this notebook\n",
    "cell_order_row = orders_df[orders_df['id'] == notebook_id]\n",
    "if not cell_order_row.empty:\n",
    "    cell_order_str = cell_order_row['cell_order'].iloc[0]\n",
    "    cell_order_ids = cell_order_str.split()\n",
    "    print(f\"\\ncell_order contains {len(cell_order_ids)} IDs: {cell_order_ids[:5]}...\")\n",
    "    \n",
    "    # Verify that cell_order IDs match the keys in the notebook\n",
    "    matching_ids = set(cell_order_ids) & set(cell_types.keys())\n",
    "    print(f\"Matching IDs between cell_order and notebook: {len(matching_ids)}/{len(cell_order_ids)}\")\n",
    "    \n",
    "    # Show a few examples of cell content by position\n",
    "    print(\"\\n=== Sample cells by position ===\")\n",
    "    for i, cell_id in enumerate(cell_order_ids[:3]):\n",
    "        if cell_id in source_cells:\n",
    "            cell_type = cell_types[cell_id]\n",
    "            content_preview = source_cells[cell_id][:100].replace('\\n', ' ')\n",
    "            print(f\"Position {i}: {cell_type} cell - {content_preview}...\")\n",
    "else:\n",
    "    print(f\"No data found for notebook {notebook_id}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHT ===\")\n",
    "print(\"The notebook structure uses DICTIONARIES with hex string keys, not lists!\")\n",
    "print(\"cell_order contains these hex IDs in the correct execution order.\")\n",
    "print(\"Previous experiments incorrectly assumed integer indices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze semantic patterns that indicate position\n",
    "print(\"=\"*80)\n",
    "print(\"SEMANTIC PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Look for common terms in different positions\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract words from first and last positions\n",
    "first_position_words = []\n",
    "last_position_words = []\n",
    "middle_position_words = []\n",
    "\n",
    "for _, cell in df_analysis.iterrows():\n",
    "    if cell['source']:\n",
    "        # Simple word extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', cell['source'].lower())\n",
    "        \n",
    "        if cell['position'] == 0:\n",
    "            first_position_words.extend(words)\n",
    "        elif cell['position'] >= 5:  # Last positions (notebooks have varying lengths)\n",
    "            last_position_words.extend(words)\n",
    "        else:\n",
    "            middle_position_words.extend(words)\n",
    "\n",
    "# Count most common words\n",
    "first_counts = Counter(first_position_words)\n",
    "last_counts = Counter(last_position_words)\n",
    "middle_counts = Counter(middle_position_words)\n",
    "\n",
    "print(\"\\nTop words in FIRST position:\")\n",
    "for word, count in first_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop words in MIDDLE positions:\")\n",
    "for word, count in middle_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop words in LAST positions:\")\n",
    "for word, count in last_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Calculate word importance (frequency difference)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORDS STRONGLY ASSOCIATED WITH POSITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Words much more common in first vs last\n",
    "first_bias = {}\n",
    "for word in set(list(first_counts.keys()) + list(last_counts.keys())):\n",
    "    first_freq = first_counts.get(word, 0) / max(len(first_position_words), 1)\n",
    "    last_freq = last_counts.get(word, 0) / max(len(last_position_words), 1)\n",
    "    if first_freq > 0 and last_freq > 0:\n",
    "        ratio = first_freq / last_freq\n",
    "        if ratio > 3:  # 3x more common in first\n",
    "            first_bias[word] = ratio\n",
    "\n",
    "print(\"\\nWords 3x more common in FIRST position:\")\n",
    "for word, ratio in sorted(first_bias.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(f\"  {word}: {ratio:.1f}x more common\")\n",
    "\n",
    "# Words much more common in last vs first\n",
    "last_bias = {}\n",
    "for word in set(list(first_counts.keys()) + list(last_counts.keys())):\n",
    "    first_freq = first_counts.get(word, 0) / max(len(first_position_words), 1)\n",
    "    last_freq = last_counts.get(word, 0) / max(len(last_position_words), 1)\n",
    "    if first_freq > 0 and last_freq > 0:\n",
    "        ratio = last_freq / first_freq\n",
    "        if ratio > 3:  # 3x more common in last\n",
    "            last_bias[word] = ratio\n",
    "\n",
    "print(\"\\nWords 3x more common in LAST position:\")\n",
    "for word, ratio in sorted(last_bias.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(f\"  {word}: {ratio:.1f}x more common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze markdown structure patterns\n",
    "print(\"=\"*80)\n",
    "print(\"MARKDOWN STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "markdown_cells = df_analysis[df_analysis['cell_type'] == 'markdown'].copy()\n",
    "\n",
    "# Look for heading patterns\n",
    "heading_patterns = []\n",
    "for _, cell in markdown_cells.iterrows():\n",
    "    source = cell['source']\n",
    "    if source:\n",
    "        lines = source.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                heading_level = len(line) - len(line.lstrip('#'))\n",
    "                heading_text = line.strip('# ').lower()\n",
    "                heading_patterns.append({\n",
    "                    'position': cell['position'],\n",
    "                    'heading_level': heading_level,\n",
    "                    'heading_text': heading_text[:50]\n",
    "                })\n",
    "\n",
    "if heading_patterns:\n",
    "    headings_df = pd.DataFrame(heading_patterns)\n",
    "    print(f\"\\nFound {len(headings_df)} headings in sample\")\n",
    "    \n",
    "    print(\"\\nHeading level distribution:\")\n",
    "    print(headings_df['heading_level'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nAverage position by heading level:\")\n",
    "    avg_pos = headings_df.groupby('heading_level')['position'].mean()\n",
    "    for level, pos in avg_pos.items():\n",
    "        print(f\"  Level {level}: {pos:.1f}\")\n",
    "    \n",
    "    # Most common heading texts\n",
    "    print(\"\\nMost common heading texts:\")\n",
    "    print(headings_df['heading_text'].value_counts().head(10))\n",
    "\n",
    "# Analyze code cell patterns\n",
    "code_cells = df_analysis[df_analysis['cell_type'] == 'code'].copy()\n",
    "print(f\"\\n\\nCODE CELL ANALYSIS\")\n",
    "print(f\"Total code cells: {len(code_cells)}\")\n",
    "\n",
    "# Look for import patterns\n",
    "import_cells = code_cells[code_cells['source_preview'].str.contains('import', case=False, na=False)]\n",
    "print(f\"Code cells with 'import': {len(import_cells)}\")\n",
    "if len(import_cells) > 0:\n",
    "    print(f\"Average position of import cells: {import_cells['position'].mean():.1f}\")\n",
    "\n",
    "# Look for function definitions\n",
    "function_cells = code_cells[code_cells['source_preview'].str.contains('def ', na=False)]\n",
    "print(f\"Code cells with function definitions: {len(function_cells)}\")\n",
    "if len(function_cells) > 0:\n",
    "    print(f\"Average position of function cells: {function_cells['position'].mean():.1f}\")\n",
    "\n",
    "# Look for print/display patterns\n",
    "print_cells = code_cells[code_cells['source_preview'].str.contains('print|display|show', case=False, na=False)]\n",
    "print(f\"Code cells with print/display: {len(print_cells)}\")\n",
    "if len(print_cells) > 0:\n",
    "    print(f\"Average position of print cells: {print_cells['position'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a66295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cell type transitions\n",
    "print(\"=\"*80)\n",
    "print(\"CELL TYPE TRANSITION PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For each notebook, analyze transitions between cell types\n",
    "transitions = []\n",
    "for notebook_id in df_analysis['notebook_id'].unique():\n",
    "    notebook_cells = df_analysis[df_analysis['notebook_id'] == notebook_id].sort_values('position')\n",
    "    \n",
    "    for i in range(len(notebook_cells) - 1):\n",
    "        current_type = notebook_cells.iloc[i]['cell_type']\n",
    "        next_type = notebook_cells.iloc[i + 1]['cell_type']\n",
    "        current_pos = notebook_cells.iloc[i]['position']\n",
    "        \n",
    "        transitions.append({\n",
    "            'from_type': current_type,\n",
    "            'to_type': next_type,\n",
    "            'position': current_pos\n",
    "        })\n",
    "\n",
    "transitions_df = pd.DataFrame(transitions)\n",
    "print(f\"Total transitions analyzed: {len(transitions_df)}\")\n",
    "\n",
    "print(\"\\nTransition patterns (from → to):\")\n",
    "transition_counts = transitions_df.groupby(['from_type', 'to_type']).size().unstack(fill_value=0)\n",
    "print(transition_counts)\n",
    "\n",
    "# Normalize to percentages\n",
    "transition_pct = transition_counts.div(transition_counts.sum(axis=1), axis=0) * 100\n",
    "print(\"\\nTransition percentages:\")\n",
    "print(transition_pct.round(1))\n",
    "\n",
    "# Analyze position-specific transitions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POSITION-SPECIFIC TRANSITIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First position transitions (position 0 → 1)\n",
    "first_trans = transitions_df[transitions_df['position'] == 0]\n",
    "if len(first_trans) > 0:\n",
    "    print(\"\\nFirst position transitions (pos 0 → 1):\")\n",
    "    print(first_trans['from_type'].value_counts())\n",
    "\n",
    "# Look for common patterns\n",
    "print(\"\\nCommon patterns:\")\n",
    "markdown_to_code = len(transitions_df[(transitions_df['from_type'] == 'markdown') & (transitions_df['to_type'] == 'code')])\n",
    "code_to_markdown = len(transitions_df[(transitions_df['from_type'] == 'code') & (transitions_df['to_type'] == 'markdown')])\n",
    "\n",
    "print(f\"Markdown → Code transitions: {markdown_to_code} ({markdown_to_code/len(transitions_df)*100:.1f}%)\")\n",
    "print(f\"Code → Markdown transitions: {code_to_markdown} ({code_to_markdown/len(transitions_df)*100:.1f}%)\")\n",
    "\n",
    "# Consecutive same-type transitions\n",
    "same_type = len(transitions_df[transitions_df['from_type'] == transitions_df['to_type']])\n",
    "print(f\"Same type transitions: {same_type} ({same_type/len(transitions_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights summary\n",
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "insights = [\n",
    "    \"1. LOGICAL FLOW PATTERNS:\",\n",
    "    \"   - First positions: 'import', 'load', 'read', 'data' (setup/loading)\",\n",
    "    \"   - Middle positions: 'analysis', 'explore', 'visualize', 'plot' (processing)\",\n",
    "    \"   - Last positions: 'result', 'conclusion', 'summary', 'print', 'show' (output)\",\n",
    "    \"   \",\n",
    "    \"2. CELL TYPE TRANSITIONS:\",\n",
    "    \"   - Markdown → Code is common (explanation followed by implementation)\",\n",
    "    \"   - Code → Markdown also occurs (results followed by interpretation)\",\n",
    "    \"   - Pattern suggests narrative structure: explain → code → interpret → repeat\",\n",
    "    \"   \",\n",
    "    \"3. HEADING HIERARCHY:\",\n",
    "    \"   - Headings show position patterns (Level 1 earlier, Level 2/3 later)\",\n",
    "    \"   - Common headings: 'Introduction', 'EDA', 'Model', 'Results', 'Conclusion'\",\n",
    "    \"   - These are STRONG ordering signals that TF-IDF doesn't capture well\",\n",
    "    \"   \",\n",
    "    \"4. WHY TF-IDF FAILED:\",\n",
    "    \"   - TF-IDF treats 'introduction' and 'conclusion' as just words\",\n",
    "    \"   - Doesn't capture semantic meaning or structural role\",\n",
    "    \"   - Can't understand that 'import' belongs at start, 'conclusion' at end\",\n",
    "    \"   - 1000 dimensions but only 5% importance confirms this limitation\",\n",
    "    \"   \",\n",
    "    \"5. WHAT WE NEED:\",\n",
    "    \"   - Semantic embeddings that understand meaning (BERT, Sentence-T5)\",\n",
    "    \"   - Structural features that capture markdown hierarchy\",\n",
    "    \"   - Content-type modeling (code vs markdown have different patterns)\",\n",
    "    \"   - Learning-to-rank formulation (this is a ranking problem, not regression)\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION: RADICAL FEATURE ENGINEERING OVERHAUL\")\n",
    "print(\"=\"*80)\n",
    "print(\"Current approach (TF-IDF + stats) = 0.4014 CV\")\n",
    "print(\"Need: ~0.5 points improvement to reach 0.9006 gold\")\n",
    "print(\"\\nThis requires breakthrough features, not incremental improvements.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
