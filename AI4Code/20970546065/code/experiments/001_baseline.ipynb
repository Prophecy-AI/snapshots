{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f818b5",
   "metadata": {},
   "source": [
    "# AI4Code Baseline Model\n",
    "\n",
    "This notebook implements a baseline solution for the AI4Code competition.\n",
    "\n",
    "## Approach\n",
    "1. Extract basic features from cells (length, type, etc.)\n",
    "2. Train a model to predict cell ordering\n",
    "3. Use Kendall tau correlation as evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths\n",
    "DATA_PATH = Path('/home/data')\n",
    "TRAIN_PATH = DATA_PATH / 'train'\n",
    "TEST_PATH = DATA_PATH / 'test'\n",
    "TRAIN_ORDERS_PATH = DATA_PATH / 'train_orders.csv'\n",
    "\n",
    "print(\"Loading data paths...\")\n",
    "print(f\"Train path: {TRAIN_PATH}\")\n",
    "print(f\"Test path: {TEST_PATH}\")\n",
    "print(f\"Train orders: {TRAIN_ORDERS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train orders\n",
    "train_orders = pd.read_csv(TRAIN_ORDERS_PATH)\n",
    "print(f\"Train orders shape: {train_orders.shape}\")\n",
    "print(f\"First few rows:\")\n",
    "print(train_orders.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample notebook to understand structure\n",
    "def load_notebook(notebook_id, path):\n",
    "    \"\"\"Load a notebook from JSON file\"\"\"\n",
    "    with open(path / f\"{notebook_id}.json\", 'r') as f:\n",
    "        notebook = json.load(f)\n",
    "    return notebook\n",
    "\n",
    "# Load sample notebook\n",
    "sample_nb = load_notebook('0002115f48f982', TRAIN_PATH)\n",
    "print(\"Notebook keys:\", list(sample_nb.keys()))\n",
    "print(\"\\nCell types:\", list(sample_nb['cell_type'].items())[:5])\n",
    "print(\"\\nSample code cell:\", list(sample_nb['source'].items())[0])\n",
    "print(\"\\nSample markdown cell:\", list(sample_nb['source'].items())[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for a notebook\n",
    "def extract_features(notebook_id, path):\n",
    "    \"\"\"Extract features from a notebook\"\"\"\n",
    "    notebook = load_notebook(notebook_id, path)\n",
    "    \n",
    "    features = []\n",
    "    cell_ids = []\n",
    "    \n",
    "    for cell_id, cell_type in notebook['cell_type'].items():\n",
    "        source = notebook['source'][cell_id]\n",
    "        \n",
    "        # Basic features\n",
    "        feature_dict = {\n",
    "            'notebook_id': notebook_id,\n",
    "            'cell_id': cell_id,\n",
    "            'cell_type': cell_type,\n",
    "            'source_length': len(source),\n",
    "            'line_count': source.count('\\n') + 1,\n",
    "            'word_count': len(source.split()),\n",
    "            'char_count': len(source.replace('\\n', '').replace(' ', '')),\n",
    "            'has_import': int('import ' in source or 'from ' in source) if cell_type == 'code' else 0,\n",
    "            'has_comment': int('#' in source) if cell_type == 'code' else 0,\n",
    "            'has_heading': int(any(heading in source for heading in ['# ', '## ', '### '])) if cell_type == 'markdown' else 0,\n",
    "            'has_code_block': int('```' in source) if cell_type == 'markdown' else 0,\n",
    "            'has_link': int('http' in source or 'www.' in source) if cell_type == 'markdown' else 0,\n",
    "        }\n",
    "        \n",
    "        features.append(feature_dict)\n",
    "        cell_ids.append(cell_id)\n",
    "    \n",
    "    return pd.DataFrame(features), cell_ids\n",
    "\n",
    "# Test feature extraction\n",
    "sample_features, sample_cell_ids = extract_features('0002115f48f982', TRAIN_PATH)\n",
    "print(\"Sample features shape:\", sample_features.shape)\n",
    "print(sample_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "def create_training_data(notebook_ids, path, orders_df):\n",
    "    \"\"\"Create training dataset with features and target positions\"\"\"\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for notebook_id in tqdm(notebook_ids, desc=\"Processing notebooks\"):\n",
    "        # Extract features\n",
    "        features, cell_ids = extract_features(notebook_id, path)\n",
    "        \n",
    "        # Get correct order\n",
    "        correct_order = orders_df[orders_df['id'] == notebook_id]['cell_order'].iloc[0].split()\n",
    "        \n",
    "        # Create position mapping\n",
    "        position_map = {cell_id: pos for pos, cell_id in enumerate(correct_order)}\n",
    "        \n",
    "        # Add target position\n",
    "        features['position'] = features['cell_id'].map(position_map)\n",
    "        \n",
    "        all_features.append(features)\n",
    "    \n",
    "    return pd.concat(all_features, ignore_index=True)\n",
    "\n",
    "# Load a subset for initial training\n",
    "sample_notebooks = train_orders['id'].head(1000).tolist()\n",
    "train_df = create_training_data(sample_notebooks, TRAIN_PATH, train_orders)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f83ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "feature_cols = [col for col in train_df.columns if col not in ['notebook_id', 'cell_id', 'position', 'cell_type']]\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Add cell type as categorical feature\n",
    "train_df['cell_type_code'] = (train_df['cell_type'] == 'code').astype(int)\n",
    "feature_cols.append('cell_type_code')\n",
    "\n",
    "print(f\"Final feature columns: {feature_cols}\")\n",
    "print(f\"Training data shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kendall tau metric for evaluation\n",
    "def kendall_tau_score(y_true, y_pred):\n",
    "    \"\"\"Calculate Kendall tau correlation\"\"\"\n",
    "    from scipy.stats import kendalltau\n",
    "    return kendalltau(y_true, y_pred).correlation\n",
    "\n",
    "kendall_scorer = make_scorer(kendall_tau_score, greater_is_better=True)\n",
    "\n",
    "# Train model\n",
    "print(\"Training LightGBM model...\")\n",
    "\n",
    "# Use a subset for faster training\n",
    "sample_df = train_df.sample(frac=0.3, random_state=42)\n",
    "\n",
    "X = sample_df[feature_cols]\n",
    "y = sample_df['position']\n",
    "\n",
    "# Train LightGBM model\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Feature importances: {dict(zip(feature_cols, model.feature_importances_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaacbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "def predict_notebook_order(notebook_id, path, model, feature_cols):\n",
    "    \"\"\"Predict cell order for a notebook\"\"\"\n",
    "    features, cell_ids = extract_features(notebook_id, path)\n",
    "    \n",
    "    # Add cell type code\n",
    "    features['cell_type_code'] = (features['cell_type'] == 'code').astype(int)\n",
    "    \n",
    "    # Predict positions\n",
    "    X = features[feature_cols]\n",
    "    predicted_positions = model.predict(X)\n",
    "    \n",
    "    # Sort by predicted position\n",
    "    order_df = pd.DataFrame({\n",
    "        'cell_id': cell_ids,\n",
    "        'predicted_position': predicted_positions\n",
    "    })\n",
    "    \n",
    "    # Sort and get ordered cell IDs\n",
    "    ordered_cells = order_df.sort_values('predicted_position')['cell_id'].tolist()\n",
    "    \n",
    "    return ' '.join(ordered_cells)\n",
    "\n",
    "# Test on a few notebooks\n",
    "test_notebooks = [f.stem for f in TEST_PATH.glob('*.json')][:5]\n",
    "print(\"Testing prediction on sample notebooks:\")\n",
    "for nb_id in test_notebooks:\n",
    "    try:\n",
    "        predicted_order = predict_notebook_order(nb_id, TEST_PATH, model, feature_cols)\n",
    "        print(f\"{nb_id}: {predicted_order[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {nb_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9181fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission for all test notebooks\n",
    "print(\"Generating submission for all test notebooks...\")\n",
    "\n",
    "test_notebooks = [f.stem for f in TEST_PATH.glob('*.json')]\n",
    "submission_data = []\n",
    "\n",
    "for notebook_id in tqdm(test_notebooks, desc=\"Predicting notebooks\"):\n",
    "    try:\n",
    "        predicted_order = predict_notebook_order(notebook_id, TEST_PATH, model, feature_cols)\n",
    "        submission_data.append({\n",
    "            'id': notebook_id,\n",
    "            'cell_order': predicted_order\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {notebook_id}: {e}\")\n",
    "        # Use default order (code cells first, then markdown cells in random order)\n",
    "        submission_data.append({\n",
    "            'id': notebook_id,\n",
    "            'cell_order': ''\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "# Verify format\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "print(f\"\\nSample submission format:\")\n",
    "print(sample_sub.head())\n",
    "print(f\"\\nOur submission format:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(f\"\\nColumns match: {list(submission_df.columns) == list(sample_sub.columns)}\")\n",
    "print(f\"Number of rows match: {len(submission_df) == len(sample_sub)}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
