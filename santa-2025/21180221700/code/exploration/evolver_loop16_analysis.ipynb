{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fb3642",
   "metadata": {},
   "source": [
    "# Loop 16 Analysis: Iterative Mixing Strategy\n",
    "\n",
    "## Key Insights from Research\n",
    "\n",
    "1. **jiweiliu kernel**: Shows iterative mixing achieves continuous improvements:\n",
    "   - 71.65 → 71.46 (SA workflow)\n",
    "   - 71.46 → 71.45 (guided refinement)\n",
    "   - 71.45 → 71.36 (SA workflow again)\n",
    "\n",
    "2. **sacuscreed kernel**: Guided refinement = small perturbations to squeeze improvements\n",
    "\n",
    "3. **chistyakov kernel**: Corner-based reconstruction from large N to small N\n",
    "\n",
    "## Current Status\n",
    "- Best score: 70.630478 (saspav_best ensemble)\n",
    "- Target: 68.919154\n",
    "- Gap: 1.711 points (2.42%)\n",
    "- 7 consecutive experiments with NO improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2558f568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T23:17:34.043909Z",
     "iopub.status.busy": "2026-01-20T23:17:34.043405Z",
     "iopub.status.idle": "2026-01-20T23:17:34.051058Z",
     "shell.execute_reply": "2026-01-20T23:17:34.050665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, getcontext\n",
    "from shapely import affinity\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "getcontext().prec = 25\n",
    "scale_factor = Decimal(\"1e15\")\n",
    "\n",
    "class ChristmasTree:\n",
    "    def __init__(self, center_x='0', center_y='0', angle='0'):\n",
    "        self.center_x = Decimal(center_x)\n",
    "        self.center_y = Decimal(center_y)\n",
    "        self.angle = Decimal(angle)\n",
    "        trunk_w = Decimal('0.15')\n",
    "        trunk_h = Decimal('0.2')\n",
    "        base_w = Decimal('0.7')\n",
    "        mid_w = Decimal('0.4')\n",
    "        top_w = Decimal('0.25')\n",
    "        tip_y = Decimal('0.8')\n",
    "        tier_1_y = Decimal('0.5')\n",
    "        tier_2_y = Decimal('0.25')\n",
    "        base_y = Decimal('0.0')\n",
    "        trunk_bottom_y = -trunk_h\n",
    "        initial_polygon = Polygon([\n",
    "            (Decimal('0.0') * scale_factor, tip_y * scale_factor),\n",
    "            (top_w / Decimal('2') * scale_factor, tier_1_y * scale_factor),\n",
    "            (top_w / Decimal('4') * scale_factor, tier_1_y * scale_factor),\n",
    "            (mid_w / Decimal('2') * scale_factor, tier_2_y * scale_factor),\n",
    "            (mid_w / Decimal('4') * scale_factor, tier_2_y * scale_factor),\n",
    "            (base_w / Decimal('2') * scale_factor, base_y * scale_factor),\n",
    "            (trunk_w / Decimal('2') * scale_factor, base_y * scale_factor),\n",
    "            (trunk_w / Decimal('2') * scale_factor, trunk_bottom_y * scale_factor),\n",
    "            (-(trunk_w / Decimal('2')) * scale_factor, trunk_bottom_y * scale_factor),\n",
    "            (-(trunk_w / Decimal('2')) * scale_factor, base_y * scale_factor),\n",
    "            (-(base_w / Decimal('2')) * scale_factor, base_y * scale_factor),\n",
    "            (-(mid_w / Decimal('4')) * scale_factor, tier_2_y * scale_factor),\n",
    "            (-(mid_w / Decimal('2')) * scale_factor, tier_2_y * scale_factor),\n",
    "            (-(top_w / Decimal('4')) * scale_factor, tier_1_y * scale_factor),\n",
    "            (-(top_w / Decimal('2')) * scale_factor, tier_1_y * scale_factor),\n",
    "        ])\n",
    "        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))\n",
    "        self.polygon = affinity.translate(rotated, xoff=float(self.center_x * scale_factor), yoff=float(self.center_y * scale_factor))\n",
    "    def clone(self):\n",
    "        return ChristmasTree(str(self.center_x), str(self.center_y), str(self.angle))\n",
    "\n",
    "def load_trees(n, df):\n",
    "    group_data = df[df[\"id\"].str.startswith(f\"{n:03d}_\")]\n",
    "    trees = []\n",
    "    for _, row in group_data.iterrows():\n",
    "        x = str(row[\"x\"]).lstrip('s')\n",
    "        y = str(row[\"y\"]).lstrip('s')\n",
    "        deg = str(row[\"deg\"]).lstrip('s')\n",
    "        trees.append(ChristmasTree(x, y, deg))\n",
    "    return trees\n",
    "\n",
    "def calculate_score(trees):\n",
    "    xys = np.concatenate([np.asarray(t.polygon.exterior.xy).T / 1e15 for t in trees])\n",
    "    min_x, min_y = xys.min(axis=0)\n",
    "    max_x, max_y = xys.max(axis=0)\n",
    "    return max(max_x - min_x, max_y - min_y) ** 2 / len(trees)\n",
    "\n",
    "print(\"Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fff78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T23:18:06.876183Z",
     "iopub.status.busy": "2026-01-20T23:18:06.875692Z",
     "iopub.status.idle": "2026-01-20T23:18:09.695981Z",
     "shell.execute_reply": "2026-01-20T23:18:09.695554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current total score: 70.630478\n",
      "Target: 68.919154\n",
      "Gap: 1.711324 (2.48%)\n",
      "\n",
      "Worst efficiency N values:\n",
      "  N=1: 38.2% efficiency, score=0.661250\n",
      "  N=2: 56.0% efficiency, score=0.450779\n",
      "  N=3: 58.1% efficiency, score=0.434745\n",
      "  N=5: 60.6% efficiency, score=0.416850\n",
      "  N=4: 60.6% efficiency, score=0.416545\n",
      "  N=7: 63.1% efficiency, score=0.399897\n",
      "  N=6: 63.2% efficiency, score=0.399610\n",
      "  N=9: 65.2% efficiency, score=0.387415\n",
      "  N=8: 65.5% efficiency, score=0.385407\n",
      "  N=15: 67.0% efficiency, score=0.376978\n"
     ]
    }
   ],
   "source": [
    "# Load current best\n",
    "import os\n",
    "os.chdir('/home/code/exploration')\n",
    "current_best_df = pd.read_csv('datasets/saspav_best.csv')\n",
    "\n",
    "# Calculate per-N scores\n",
    "scores = {}\n",
    "for n in range(1, 201):\n",
    "    trees = load_trees(n, current_best_df)\n",
    "    scores[n] = calculate_score(trees)\n",
    "\n",
    "total_score = sum(scores.values())\n",
    "print(f\"Current total score: {total_score:.6f}\")\n",
    "print(f\"Target: 68.919154\")\n",
    "print(f\"Gap: {total_score - 68.919154:.6f} ({(total_score - 68.919154)/68.919154*100:.2f}%)\")\n",
    "\n",
    "# Identify worst efficiency N values\n",
    "efficiencies = {}\n",
    "for n, score in scores.items():\n",
    "    tree_area = 0.2525\n",
    "    bbox_area = score * n\n",
    "    efficiency = (tree_area * n) / bbox_area * 100\n",
    "    efficiencies[n] = efficiency\n",
    "\n",
    "print(\"\\nWorst efficiency N values:\")\n",
    "worst = sorted(efficiencies.items(), key=lambda x: x[1])[:10]\n",
    "for n, eff in worst:\n",
    "    print(f\"  N={n}: {eff:.1f}% efficiency, score={scores[n]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed91be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T23:18:09.697135Z",
     "iopub.status.busy": "2026-01-20T23:18:09.697035Z",
     "iopub.status.idle": "2026-01-20T23:18:09.700851Z",
     "shell.execute_reply": "2026-01-20T23:18:09.700521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score contribution by range:\n",
      "  Small N (1-20): 8.0532 (11.4%)\n",
      "  Medium N (21-50): 10.9809 (15.5%)\n",
      "  Large N (51-100): 17.6170 (24.9%)\n",
      "  Very Large N (101-200): 33.9794 (48.1%)\n",
      "\n",
      "Gap to close: 1.711324\n",
      "\n",
      "If we improve each range by X%:\n",
      "  Small N: need 21.3% improvement to close entire gap\n",
      "  Medium N: need 15.6% improvement to close entire gap\n",
      "  Large N: need 9.7% improvement to close entire gap\n",
      "  Very Large N: need 5.0% improvement to close entire gap\n"
     ]
    }
   ],
   "source": [
    "# Analyze score contribution by N range\n",
    "ranges = [\n",
    "    (1, 20, \"Small N\"),\n",
    "    (21, 50, \"Medium N\"),\n",
    "    (51, 100, \"Large N\"),\n",
    "    (101, 200, \"Very Large N\")\n",
    "]\n",
    "\n",
    "print(\"Score contribution by range:\")\n",
    "for start, end, name in ranges:\n",
    "    range_score = sum(scores[n] for n in range(start, end+1))\n",
    "    pct = range_score / total_score * 100\n",
    "    print(f\"  {name} ({start}-{end}): {range_score:.4f} ({pct:.1f}%)\")\n",
    "\n",
    "# Calculate how much improvement needed from each range to close gap\n",
    "gap = total_score - 68.919154\n",
    "print(f\"\\nGap to close: {gap:.6f}\")\n",
    "print(\"\\nIf we improve each range by X%:\")\n",
    "for start, end, name in ranges:\n",
    "    range_score = sum(scores[n] for n in range(start, end+1))\n",
    "    improvement_needed = gap / range_score * 100\n",
    "    print(f\"  {name}: need {improvement_needed:.1f}% improvement to close entire gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d6f54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T23:18:09.701852Z",
     "iopub.status.busy": "2026-01-20T23:18:09.701749Z",
     "iopub.status.idle": "2026-01-20T23:18:10.216118Z",
     "shell.execute_reply": "2026-01-20T23:18:10.215709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing corner-based reconstruction from N=200...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvements found from corner reconstruction.\n"
     ]
    }
   ],
   "source": [
    "# Corner-based reconstruction analysis\n",
    "# For each large N, check if corner subsets beat current best for smaller N\n",
    "\n",
    "def get_corner_subsets(trees, corner_idx):\n",
    "    \"\"\"Get trees sorted by distance from a corner.\"\"\"\n",
    "    all_polygons = [t.polygon for t in trees]\n",
    "    bounds = unary_union(all_polygons).bounds\n",
    "    \n",
    "    corners = [\n",
    "        (bounds[0], bounds[1]),  # bottom-left\n",
    "        (bounds[0], bounds[3]),  # top-left\n",
    "        (bounds[2], bounds[1]),  # bottom-right\n",
    "        (bounds[2], bounds[3]),  # top-right\n",
    "    ]\n",
    "    corner_x, corner_y = corners[corner_idx]\n",
    "    \n",
    "    # Calculate max distance from corner for each tree\n",
    "    distances = []\n",
    "    for tree in trees:\n",
    "        b = tree.polygon.bounds\n",
    "        dist = max(\n",
    "            abs(b[0] - corner_x),\n",
    "            abs(b[2] - corner_x),\n",
    "            abs(b[1] - corner_y),\n",
    "            abs(b[3] - corner_y)\n",
    "        )\n",
    "        distances.append((dist, tree))\n",
    "    \n",
    "    # Sort by distance\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    return [t for _, t in distances]\n",
    "\n",
    "# Test corner-based reconstruction for N=200\n",
    "print(\"Testing corner-based reconstruction from N=200...\")\n",
    "large_trees = load_trees(200, current_best_df)\n",
    "\n",
    "improvements = []\n",
    "for corner_idx in range(4):\n",
    "    corner_names = ['bottom-left', 'top-left', 'bottom-right', 'top-right']\n",
    "    sorted_trees = get_corner_subsets(large_trees, corner_idx)\n",
    "    \n",
    "    for target_n in range(2, 50):\n",
    "        subset = [t.clone() for t in sorted_trees[:target_n]]\n",
    "        subset_score = calculate_score(subset)\n",
    "        current_score = scores[target_n]\n",
    "        \n",
    "        if subset_score < current_score:\n",
    "            improvement = current_score - subset_score\n",
    "            improvements.append((target_n, corner_names[corner_idx], subset_score, current_score, improvement))\n",
    "\n",
    "if improvements:\n",
    "    print(f\"\\nFound {len(improvements)} improvements from corner reconstruction!\")\n",
    "    for n, corner, new_score, old_score, imp in improvements[:10]:\n",
    "        print(f\"  N={n} ({corner}): {old_score:.6f} -> {new_score:.6f} (improvement: {imp:.6f})\")\n",
    "else:\n",
    "    print(\"\\nNo improvements found from corner reconstruction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0ba9d",
   "metadata": {},
   "source": [
    "## Key Finding: Need Iterative Mixing Approach\n",
    "\n",
    "The jiweiliu kernel shows that continuous improvements come from ITERATING between:\n",
    "1. SA optimization (generates new configurations)\n",
    "2. Guided refinement (small perturbations)\n",
    "3. Deletion cascade (propagates improvements from large N to small N)\n",
    "\n",
    "We have NOT tried this iterative mixing approach yet. All our experiments have been single-pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what kernels we haven't fully implemented\n",
    "print(\"Kernel Implementation Status:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. jiweiliu (super-fast-simulated-annealing-with-translations):\")\n",
    "print(\"   - Numba-accelerated SA: NOT IMPLEMENTED\")\n",
    "print(\"   - Grid configuration exploration: NOT IMPLEMENTED\")\n",
    "print(\"   - Deletion cascade: NOT IMPLEMENTED\")\n",
    "print(\"   - Iterative mixing with guided refinement: NOT IMPLEMENTED\")\n",
    "\n",
    "print(\"\\n2. sacuscreed (guided-refinement):\")\n",
    "print(\"   - Small perturbation refinement: NOT IMPLEMENTED\")\n",
    "\n",
    "print(\"\\n3. chistyakov (corner-based reconstruction):\")\n",
    "print(\"   - Corner-based subset extraction: PARTIALLY TESTED (no improvements found)\")\n",
    "\n",
    "print(\"\\n4. saspav (santa-submission):\")\n",
    "print(\"   - Best ensemble: IMPLEMENTED (current best 70.630478)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Implement jiweiliu's full workflow with Numba SA + deletion cascade\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
