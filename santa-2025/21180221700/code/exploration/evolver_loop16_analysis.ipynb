{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fb3642",
   "metadata": {},
   "source": [
    "# Loop 16 Analysis: Iterative Mixing Strategy\n",
    "\n",
    "## Key Insights from Research\n",
    "\n",
    "1. **jiweiliu kernel**: Shows iterative mixing achieves continuous improvements:\n",
    "   - 71.65 → 71.46 (SA workflow)\n",
    "   - 71.46 → 71.45 (guided refinement)\n",
    "   - 71.45 → 71.36 (SA workflow again)\n",
    "\n",
    "2. **sacuscreed kernel**: Guided refinement = small perturbations to squeeze improvements\n",
    "\n",
    "3. **chistyakov kernel**: Corner-based reconstruction from large N to small N\n",
    "\n",
    "## Current Status\n",
    "- Best score: 70.630478 (saspav_best ensemble)\n",
    "- Target: 68.919154\n",
    "- Gap: 1.711 points (2.42%)\n",
    "- 7 consecutive experiments with NO improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, getcontext\n",
    "from shapely import affinity\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "getcontext().prec = 25\n",
    "scale_factor = Decimal(\"1e15\")\n",
    "\n",
    "class ChristmasTree:\n",
    "    def __init__(self, center_x='0', center_y='0', angle='0'):\n",
    "        self.center_x = Decimal(center_x)\n",
    "        self.center_y = Decimal(center_y)\n",
    "        self.angle = Decimal(angle)\n",
    "        trunk_w = Decimal('0.15')\n",
    "        trunk_h = Decimal('0.2')\n",
    "        base_w = Decimal('0.7')\n",
    "        mid_w = Decimal('0.4')\n",
    "        top_w = Decimal('0.25')\n",
    "        tip_y = Decimal('0.8')\n",
    "        tier_1_y = Decimal('0.5')\n",
    "        tier_2_y = Decimal('0.25')\n",
    "        base_y = Decimal('0.0')\n",
    "        trunk_bottom_y = -trunk_h\n",
    "        initial_polygon = Polygon([\n",
    "            (Decimal('0.0') * scale_factor, tip_y * scale_factor),\n",
    "            (top_w / Decimal('2') * scale_factor, tier_1_y * scale_factor),\n",
    "            (top_w / Decimal('4') * scale_factor, tier_1_y * scale_factor),\n",
    "            (mid_w / Decimal('2') * scale_factor, tier_2_y * scale_factor),\n",
    "            (mid_w / Decimal('4') * scale_factor, tier_2_y * scale_factor),\n",
    "            (base_w / Decimal('2') * scale_factor, base_y * scale_factor),\n",
    "            (trunk_w / Decimal('2') * scale_factor, base_y * scale_factor),\n",
    "            (trunk_w / Decimal('2') * scale_factor, trunk_bottom_y * scale_factor),\n",
    "            (-(trunk_w / Decimal('2')) * scale_factor, trunk_bottom_y * scale_factor),\n",
    "            (-(trunk_w / Decimal('2')) * scale_factor, base_y * scale_factor),\n",
    "            (-(base_w / Decimal('2')) * scale_factor, base_y * scale_factor),\n",
    "            (-(mid_w / Decimal('4')) * scale_factor, tier_2_y * scale_factor),\n",
    "            (-(mid_w / Decimal('2')) * scale_factor, tier_2_y * scale_factor),\n",
    "            (-(top_w / Decimal('4')) * scale_factor, tier_1_y * scale_factor),\n",
    "            (-(top_w / Decimal('2')) * scale_factor, tier_1_y * scale_factor),\n",
    "        ])\n",
    "        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))\n",
    "        self.polygon = affinity.translate(rotated, xoff=float(self.center_x * scale_factor), yoff=float(self.center_y * scale_factor))\n",
    "    def clone(self):\n",
    "        return ChristmasTree(str(self.center_x), str(self.center_y), str(self.angle))\n",
    "\n",
    "def load_trees(n, df):\n",
    "    group_data = df[df[\"id\"].str.startswith(f\"{n:03d}_\")]\n",
    "    trees = []\n",
    "    for _, row in group_data.iterrows():\n",
    "        x = str(row[\"x\"]).lstrip('s')\n",
    "        y = str(row[\"y\"]).lstrip('s')\n",
    "        deg = str(row[\"deg\"]).lstrip('s')\n",
    "        trees.append(ChristmasTree(x, y, deg))\n",
    "    return trees\n",
    "\n",
    "def calculate_score(trees):\n",
    "    xys = np.concatenate([np.asarray(t.polygon.exterior.xy).T / 1e15 for t in trees])\n",
    "    min_x, min_y = xys.min(axis=0)\n",
    "    max_x, max_y = xys.max(axis=0)\n",
    "    return max(max_x - min_x, max_y - min_y) ** 2 / len(trees)\n",
    "\n",
    "print(\"Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fff78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current best\\nimport os\\nos.chdir('/home/code/exploration')\\ncurrent_best_df = pd.read_csv('datasets/saspav_best.csv')\\n\\n# Calculate per-N scores\\nscores = {}\\nfor n in range(1, 201):\\n    trees = load_trees(n, current_best_df)\\n    scores[n] = calculate_score(trees)\\n\\ntotal_score = sum(scores.values())\\nprint(f\\\"Current total score: {total_score:.6f}\\\")\\nprint(f\\\"Target: 68.919154\\\")\\nprint(f\\\"Gap: {total_score - 68.919154:.6f} ({(total_score - 68.919154)/68.919154*100:.2f}%)\\\")\\n\\n# Identify worst efficiency N values\\nefficiencies = {}\\nfor n, score in scores.items():\\n    # Efficiency = tree_area * n / bounding_box_area\\n    tree_area = 0.2525  # approximate tree area\\n    bbox_area = score * n  # score = side^2 / n, so side^2 = score * n\\n    efficiency = (tree_area * n) / bbox_area * 100\\n    efficiencies[n] = efficiency\\n\\nprint(\\\"\\\\nWorst efficiency N values:\\\")\\nworst = sorted(efficiencies.items(), key=lambda x: x[1])[:10]\\nfor n, eff in worst:\\n    print(f\\\"  N={n}: {eff:.1f}% efficiency, score={scores[n]:.6f}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed91be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze score contribution by N range\n",
    "ranges = [\n",
    "    (1, 20, \"Small N\"),\n",
    "    (21, 50, \"Medium N\"),\n",
    "    (51, 100, \"Large N\"),\n",
    "    (101, 200, \"Very Large N\")\n",
    "]\n",
    "\n",
    "print(\"Score contribution by range:\")\n",
    "for start, end, name in ranges:\n",
    "    range_score = sum(scores[n] for n in range(start, end+1))\n",
    "    pct = range_score / total_score * 100\n",
    "    print(f\"  {name} ({start}-{end}): {range_score:.4f} ({pct:.1f}%)\")\n",
    "\n",
    "# Calculate how much improvement needed from each range to close gap\n",
    "gap = total_score - 68.919154\n",
    "print(f\"\\nGap to close: {gap:.6f}\")\n",
    "print(\"\\nIf we improve each range by X%:\")\n",
    "for start, end, name in ranges:\n",
    "    range_score = sum(scores[n] for n in range(start, end+1))\n",
    "    improvement_needed = gap / range_score * 100\n",
    "    print(f\"  {name}: need {improvement_needed:.1f}% improvement to close entire gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner-based reconstruction analysis\n",
    "# For each large N, check if corner subsets beat current best for smaller N\n",
    "\n",
    "def get_corner_subsets(trees, corner_idx):\n",
    "    \"\"\"Get trees sorted by distance from a corner.\"\"\"\n",
    "    all_polygons = [t.polygon for t in trees]\n",
    "    bounds = unary_union(all_polygons).bounds\n",
    "    \n",
    "    corners = [\n",
    "        (bounds[0], bounds[1]),  # bottom-left\n",
    "        (bounds[0], bounds[3]),  # top-left\n",
    "        (bounds[2], bounds[1]),  # bottom-right\n",
    "        (bounds[2], bounds[3]),  # top-right\n",
    "    ]\n",
    "    corner_x, corner_y = corners[corner_idx]\n",
    "    \n",
    "    # Calculate max distance from corner for each tree\n",
    "    distances = []\n",
    "    for tree in trees:\n",
    "        b = tree.polygon.bounds\n",
    "        dist = max(\n",
    "            abs(b[0] - corner_x),\n",
    "            abs(b[2] - corner_x),\n",
    "            abs(b[1] - corner_y),\n",
    "            abs(b[3] - corner_y)\n",
    "        )\n",
    "        distances.append((dist, tree))\n",
    "    \n",
    "    # Sort by distance\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    return [t for _, t in distances]\n",
    "\n",
    "# Test corner-based reconstruction for N=200\n",
    "print(\"Testing corner-based reconstruction from N=200...\")\n",
    "large_trees = load_trees(200, current_best_df)\n",
    "\n",
    "improvements = []\n",
    "for corner_idx in range(4):\n",
    "    corner_names = ['bottom-left', 'top-left', 'bottom-right', 'top-right']\n",
    "    sorted_trees = get_corner_subsets(large_trees, corner_idx)\n",
    "    \n",
    "    for target_n in range(2, 50):\n",
    "        subset = [t.clone() for t in sorted_trees[:target_n]]\n",
    "        subset_score = calculate_score(subset)\n",
    "        current_score = scores[target_n]\n",
    "        \n",
    "        if subset_score < current_score:\n",
    "            improvement = current_score - subset_score\n",
    "            improvements.append((target_n, corner_names[corner_idx], subset_score, current_score, improvement))\n",
    "\n",
    "if improvements:\n",
    "    print(f\"\\nFound {len(improvements)} improvements from corner reconstruction!\")\n",
    "    for n, corner, new_score, old_score, imp in improvements[:10]:\n",
    "        print(f\"  N={n} ({corner}): {old_score:.6f} -> {new_score:.6f} (improvement: {imp:.6f})\")\n",
    "else:\n",
    "    print(\"\\nNo improvements found from corner reconstruction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0ba9d",
   "metadata": {},
   "source": [
    "## Key Finding: Need Iterative Mixing Approach\n",
    "\n",
    "The jiweiliu kernel shows that continuous improvements come from ITERATING between:\n",
    "1. SA optimization (generates new configurations)\n",
    "2. Guided refinement (small perturbations)\n",
    "3. Deletion cascade (propagates improvements from large N to small N)\n",
    "\n",
    "We have NOT tried this iterative mixing approach yet. All our experiments have been single-pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what kernels we haven't fully implemented\n",
    "print(\"Kernel Implementation Status:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. jiweiliu (super-fast-simulated-annealing-with-translations):\")\n",
    "print(\"   - Numba-accelerated SA: NOT IMPLEMENTED\")\n",
    "print(\"   - Grid configuration exploration: NOT IMPLEMENTED\")\n",
    "print(\"   - Deletion cascade: NOT IMPLEMENTED\")\n",
    "print(\"   - Iterative mixing with guided refinement: NOT IMPLEMENTED\")\n",
    "\n",
    "print(\"\\n2. sacuscreed (guided-refinement):\")\n",
    "print(\"   - Small perturbation refinement: NOT IMPLEMENTED\")\n",
    "\n",
    "print(\"\\n3. chistyakov (corner-based reconstruction):\")\n",
    "print(\"   - Corner-based subset extraction: PARTIALLY TESTED (no improvements found)\")\n",
    "\n",
    "print(\"\\n4. saspav (santa-submission):\")\n",
    "print(\"   - Best ensemble: IMPLEMENTED (current best 70.630478)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Implement jiweiliu's full workflow with Numba SA + deletion cascade\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
