{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a404ea",
   "metadata": {},
   "source": [
    "# Loop 37 Analysis: Critical Assessment\n",
    "\n",
    "## Key Questions:\n",
    "1. What is the actual gap to target?\n",
    "2. What approaches have been tried vs what's left?\n",
    "3. What do top solutions do differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Analyze experiments\n",
    "experiments = state['experiments']\n",
    "print(f\"Total experiments: {len(experiments)}\")\n",
    "print(f\"\\nScore progression:\")\n",
    "for exp in experiments[-15:]:\n",
    "    print(f\"  {exp['name']}: CV={exp['cv_score']:.6f}, LB={exp.get('lb_score', 'N/A')}\")\n",
    "\n",
    "# Best scores\n",
    "cv_scores = [e['cv_score'] for e in experiments]\n",
    "best_cv = min(cv_scores)\n",
    "print(f\"\\nBest CV: {best_cv:.6f}\")\n",
    "print(f\"Target: 68.866853\")\n",
    "print(f\"Gap: {best_cv - 68.866853:.6f} ({(best_cv - 68.866853)/68.866853*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize experiments by approach type\n",
    "approach_types = {\n",
    "    'baseline': [],\n",
    "    'local_search': [],  # SA, exhaustive, NFP\n",
    "    'ensemble': [],\n",
    "    'constructive': [],  # lattice, BLF, interlock\n",
    "    'population': [],  # GA\n",
    "    'extended_opt': []  # long bbox3 runs\n",
    "}\n",
    "\n",
    "for exp in experiments:\n",
    "    name = exp['name'].lower()\n",
    "    notes = exp.get('notes', '').lower()\n",
    "    \n",
    "    if 'baseline' in name:\n",
    "        approach_types['baseline'].append(exp)\n",
    "    elif any(x in name for x in ['sa', 'annealing', 'exhaustive', 'nfp', 'shake', 'numba']):\n",
    "        approach_types['local_search'].append(exp)\n",
    "    elif 'ensemble' in name:\n",
    "        approach_types['ensemble'].append(exp)\n",
    "    elif any(x in name for x in ['lattice', 'blf', 'interlock', 'jostle', 'constructive']):\n",
    "        approach_types['constructive'].append(exp)\n",
    "    elif 'genetic' in name or 'ga' in name:\n",
    "        approach_types['population'].append(exp)\n",
    "    elif '8hr' in name or 'extended' in name:\n",
    "        approach_types['extended_opt'].append(exp)\n",
    "    else:\n",
    "        approach_types['local_search'].append(exp)  # default\n",
    "\n",
    "print(\"Experiments by approach type:\")\n",
    "for approach, exps in approach_types.items():\n",
    "    if exps:\n",
    "        best = min(e['cv_score'] for e in exps)\n",
    "        print(f\"  {approach}: {len(exps)} experiments, best={best:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-N scores from best submission\n",
    "baseline_path = '/home/code/experiments/029_final_ensemble_v2/submission.csv'\n",
    "df = pd.read_csv(baseline_path)\n",
    "\n",
    "# Parse coordinates\n",
    "def parse_coord(val):\n",
    "    if isinstance(val, str) and val.startswith('s'):\n",
    "        return float(val[1:])\n",
    "    return float(val)\n",
    "\n",
    "df['n'] = df['id'].apply(lambda x: int(str(x).split('_')[0]))\n",
    "df['i'] = df['id'].apply(lambda x: int(str(x).split('_')[1]))\n",
    "for col in ['x', 'y', 'deg']:\n",
    "    df[col] = df[col].apply(parse_coord)\n",
    "\n",
    "# Calculate per-N scores\n",
    "per_n_scores = {}\n",
    "for n in range(1, 201):\n",
    "    n_df = df[df['n'] == n]\n",
    "    if len(n_df) > 0:\n",
    "        min_x = n_df['x'].min()\n",
    "        max_x = n_df['x'].max()\n",
    "        min_y = n_df['y'].min()\n",
    "        max_y = n_df['y'].max()\n",
    "        # This is approximate - need to account for tree geometry\n",
    "        side = max(max_x - min_x, max_y - min_y) + 1.0  # rough tree size\n",
    "        per_n_scores[n] = side**2 / n\n",
    "\n",
    "print(\"Top 10 highest score contributors (worst N values):\")\n",
    "sorted_scores = sorted(per_n_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for n, score in sorted_scores[:10]:\n",
    "    print(f\"  N={n}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTotal score:\", sum(per_n_scores.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863110cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the theoretical minimum?\n",
    "# For N trees, the minimum bounding box is limited by the tree geometry\n",
    "# Tree dimensions: width ~0.7, height ~1.0\n",
    "\n",
    "# Theoretical analysis:\n",
    "# - N=1: Single tree, min side = max(0.7, 1.0) = 1.0, score = 1.0\n",
    "# - But with rotation, we can get smaller bounding box\n",
    "# - Optimal N=1 rotation gives ~0.813 (from baseline)\n",
    "\n",
    "# The gap to target is 1.45 points (2.1%)\n",
    "# This is distributed across all 200 N values\n",
    "# Average improvement needed per N: 1.45/200 = 0.00725\n",
    "\n",
    "print(\"Gap analysis:\")\n",
    "print(f\"  Current best: 70.315537\")\n",
    "print(f\"  Target: 68.866853\")\n",
    "print(f\"  Gap: 1.448684 points\")\n",
    "print(f\"  Average improvement needed per N: {1.448684/200:.6f}\")\n",
    "print(f\"\")\n",
    "print(\"  If we improve 50 N values by 0.03 each: 1.5 points\")\n",
    "print(\"  If we improve 100 N values by 0.015 each: 1.5 points\")\n",
    "print(\"  If we improve 200 N values by 0.0075 each: 1.5 points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what external data sources have been tried\n",
    "print(\"External data sources mentioned in experiments:\")\n",
    "for exp in experiments:\n",
    "    notes = exp.get('notes', '')\n",
    "    if 'external' in notes.lower() or 'snapshot' in notes.lower() or 'csv' in notes.lower():\n",
    "        print(f\"  {exp['name']}: {notes[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight from research:\n",
    "# Top teams run bbox3 for 24-72 HOURS with 24+ CPUs\n",
    "# Our longest run was 53 minutes on 1 CPU\n",
    "# That's 1/648th to 1/1944th of top competitor compute\n",
    "\n",
    "print(\"COMPUTE TIME ANALYSIS:\")\n",
    "print(\"\")\n",
    "print(\"Top competitors:\")\n",
    "print(\"  - Run time: 24-72 hours\")\n",
    "print(\"  - CPUs: 24+\")\n",
    "print(\"  - Total compute: 576-1728 CPU-hours\")\n",
    "print(\"\")\n",
    "print(\"Our best attempt:\")\n",
    "print(\"  - Run time: 53 minutes\")\n",
    "print(\"  - CPUs: 1\")\n",
    "print(\"  - Total compute: 0.88 CPU-hours\")\n",
    "print(\"\")\n",
    "print(\"Ratio: Our compute is 1/655 to 1/1964 of top competitors\")\n",
    "print(\"\")\n",
    "print(\"CONCLUSION: We have NOT tried extended optimization at scale.\")\n",
    "print(\"This is the ONLY approach that top teams use that we haven't tried.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
