{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eeb31ae",
   "metadata": {},
   "source": [
    "# Precision-Preserving Ensemble\n",
    "\n",
    "exp_002 failed because of precision loss when parsing floats.\n",
    "This experiment preserves original string precision when combining solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b04442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# Tree polygon vertices for score calculation\n",
    "TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]\n",
    "TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]\n",
    "\n",
    "def create_tree_polygon(x, y, angle):\n",
    "    \"\"\"Create a tree polygon at position (x, y) with rotation angle (degrees).\"\"\"\n",
    "    poly = Polygon(zip(TX, TY))\n",
    "    poly = affinity.rotate(poly, angle, origin=(0, 0))\n",
    "    poly = affinity.translate(poly, x, y)\n",
    "    return poly\n",
    "\n",
    "def calculate_side(trees):\n",
    "    \"\"\"Calculate the bounding box side length for a set of trees.\"\"\"\n",
    "    polys = [create_tree_polygon(*t) for t in trees]\n",
    "    union = unary_union(polys)\n",
    "    bounds = union.bounds\n",
    "    return max(bounds[2] - bounds[0], bounds[3] - bounds[1])\n",
    "\n",
    "def calculate_score_for_n(trees, n):\n",
    "    \"\"\"Calculate score contribution for N trees.\"\"\"\n",
    "    side = calculate_side(trees)\n",
    "    return (side ** 2) / n\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e58a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snapshot_raw(path):\n",
    "    \"\"\"Load snapshot preserving original string precision.\"\"\"\n",
    "    rows_by_n = {}\n",
    "    with open(path, 'r') as f:\n",
    "        next(f)  # Skip header\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            id_val = parts[0]\n",
    "            n = int(id_val.split('_')[0])\n",
    "            if n not in rows_by_n:\n",
    "                rows_by_n[n] = []\n",
    "            rows_by_n[n].append(parts)  # Keep as strings!\n",
    "    return rows_by_n\n",
    "\n",
    "def parse_row_to_tuple(row):\n",
    "    \"\"\"Parse a row to (x, y, angle) tuple for score calculation.\"\"\"\n",
    "    x_str = row[1]\n",
    "    y_str = row[2]\n",
    "    deg_str = row[3]\n",
    "    x = float(x_str[1:] if x_str.startswith('s') else x_str)\n",
    "    y = float(y_str[1:] if y_str.startswith('s') else y_str)\n",
    "    angle = float(deg_str[1:] if deg_str.startswith('s') else deg_str)\n",
    "    return (x, y, angle)\n",
    "\n",
    "def calculate_score_from_rows(rows, n):\n",
    "    \"\"\"Calculate score from raw string rows (parses floats for comparison only).\"\"\"\n",
    "    trees = [parse_row_to_tuple(row) for row in rows]\n",
    "    return calculate_score_for_n(trees, n)\n",
    "\n",
    "print(\"Raw loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline (exp_001 which passed Kaggle) as raw strings\n",
    "baseline_path = '/home/nonroot/snapshots/santa-2025/21145966992/submission/submission.csv'\n",
    "baseline_raw = load_snapshot_raw(baseline_path)\n",
    "\n",
    "# Calculate baseline scores\n",
    "baseline_scores = {n: calculate_score_from_rows(baseline_raw[n], n) for n in range(1, 201)}\n",
    "baseline_total = sum(baseline_scores.values())\n",
    "print(f\"Baseline total score: {baseline_total:.6f}\")\n",
    "\n",
    "# Verify precision is preserved\n",
    "print(f\"\\nBaseline N=2 raw rows:\")\n",
    "for row in baseline_raw[2]:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best_per_n with baseline\n",
    "best_per_n = {\n",
    "    n: {\n",
    "        'score': baseline_scores[n],\n",
    "        'rows': baseline_raw[n],\n",
    "        'source': 'baseline'\n",
    "    } for n in range(1, 201)\n",
    "}\n",
    "\n",
    "print(\"Initialized with baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190039f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all snapshots and find improvements\n",
    "snapshot_base = '/home/nonroot/snapshots/santa-2025/'\n",
    "snapshot_dirs = sorted(os.listdir(snapshot_base))\n",
    "print(f\"Found {len(snapshot_dirs)} snapshot directories\")\n",
    "\n",
    "improvements_found = 0\n",
    "snapshots_processed = 0\n",
    "\n",
    "for snap_dir in snapshot_dirs:\n",
    "    sub_path = os.path.join(snapshot_base, snap_dir, 'submission', 'submission.csv')\n",
    "    if not os.path.exists(sub_path):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        snap_raw = load_snapshot_raw(sub_path)\n",
    "        \n",
    "        # Check each N\n",
    "        for n in range(1, 201):\n",
    "            if n not in snap_raw:\n",
    "                continue\n",
    "            \n",
    "            rows = snap_raw[n]\n",
    "            score = calculate_score_from_rows(rows, n)\n",
    "            \n",
    "            # Only accept if better score\n",
    "            if score < best_per_n[n]['score'] - 1e-10:\n",
    "                best_per_n[n]['score'] = score\n",
    "                best_per_n[n]['rows'] = rows  # Keep original string rows!\n",
    "                best_per_n[n]['source'] = snap_dir\n",
    "                improvements_found += 1\n",
    "        \n",
    "        snapshots_processed += 1\n",
    "        if snapshots_processed % 20 == 0:\n",
    "            print(f\"Processed {snapshots_processed} snapshots, {improvements_found} improvements...\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nTotal: {snapshots_processed} snapshots processed\")\n",
    "print(f\"Improvements found: {improvements_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new total score\n",
    "new_total = sum(best_per_n[n]['score'] for n in range(1, 201))\n",
    "print(f\"New total score: {new_total:.6f}\")\n",
    "print(f\"Baseline total: {baseline_total:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - new_total:.6f}\")\n",
    "\n",
    "# Count unique sources\n",
    "sources = set(best_per_n[n]['source'] for n in range(1, 201))\n",
    "print(f\"\\nUnique sources used: {len(sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top improvements\n",
    "print(\"\\nTop 20 improvements:\")\n",
    "improved_n = []\n",
    "for n in range(1, 201):\n",
    "    if best_per_n[n]['source'] != 'baseline':\n",
    "        old_score = baseline_scores[n]\n",
    "        new_score = best_per_n[n]['score']\n",
    "        improved_n.append((n, old_score - new_score, best_per_n[n]['source']))\n",
    "\n",
    "for n, improvement, source in sorted(improved_n, key=lambda x: -x[1])[:20]:\n",
    "    print(f\"  N={n}: improved by {improvement:.6f} from {source}\")\n",
    "\n",
    "print(f\"\\nTotal N values improved: {len(improved_n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfff6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write ensemble preserving original string precision\n",
    "def write_ensemble(best_per_n, output_path):\n",
    "    \"\"\"Write ensemble using original string rows (preserves precision).\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write('id,x,y,deg\\n')\n",
    "        for n in range(1, 201):\n",
    "            for row in best_per_n[n]['rows']:\n",
    "                f.write(','.join(row) + '\\n')\n",
    "\n",
    "write_ensemble(best_per_n, '/home/submission/submission.csv')\n",
    "print(\"Saved ensemble to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify precision is preserved\n",
    "print(\"\\nVerifying precision preservation...\")\n",
    "with open('/home/submission/submission.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "\n",
    "# Check N=2 (the one that failed in exp_002)\n",
    "print(\"\\nN=2 rows in output:\")\n",
    "for line in lines:\n",
    "    if line.startswith('002_'):\n",
    "        print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43941490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the score by re-loading\n",
    "verify_raw = load_snapshot_raw('/home/submission/submission.csv')\n",
    "verify_total = sum(calculate_score_from_rows(verify_raw[n], n) for n in range(1, 201))\n",
    "print(f\"Verified score: {verify_total:.6f}\")\n",
    "print(f\"Expected score: {new_total:.6f}\")\n",
    "print(f\"Match: {abs(verify_total - new_total) < 1e-10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71338f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': new_total,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - new_total,\n",
    "    'improvements_found': improvements_found,\n",
    "    'unique_sources': len(sources),\n",
    "    'n_values_improved': len(improved_n),\n",
    "    'notes': 'Precision-preserving ensemble. Uses original string rows instead of re-serialized floats.'\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/004_precision_ensemble/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Metrics saved.\")\n",
    "print(f\"CV Score: {new_total:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
