{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20e51a7",
   "metadata": {},
   "source": [
    "# Evolver Loop 12 Analysis\n",
    "\n",
    "## Key Findings from Research\n",
    "\n",
    "### 1. Two Novel Techniques Discovered (NOT TRIED YET!)\n",
    "\n",
    "**A. \"Rebuild from Corners\" (chistyakov kernel)**\n",
    "- For each large N layout, check all 4 corners\n",
    "- Sort trees by distance from corner\n",
    "- Reconstruct smaller layouts by taking closest trees\n",
    "- If subset beats existing solution for smaller N, use it\n",
    "- **Pure Python, no optimizer needed!**\n",
    "\n",
    "**B. \"BackPacking\" (crodoc kernel)**\n",
    "- Start from N=200, iterate backward to N=1\n",
    "- Track best configuration at each step\n",
    "- When smaller N has worse score, copy from best larger config and drop extras\n",
    "- Propagates good packing patterns from large N to small N\n",
    "- **Pure Python, no optimizer needed!**\n",
    "\n",
    "### 2. Current State\n",
    "- All 115 local snapshots converge to same local optimum (70.615107)\n",
    "- Ensemble found only 1.76e-7 valid improvement (numerical noise)\n",
    "- The \"improvements\" from exp_002/004/006 came from INVALID snapshot 21145966992\n",
    "- Python SA, fractional translation, random restart all found ZERO improvements\n",
    "\n",
    "### 3. Path Forward\n",
    "The two novel techniques above have NOT been tried yet. They don't require:\n",
    "- Running any binary optimizer\n",
    "- C++ compilation\n",
    "- External data sources\n",
    "\n",
    "They work by EXTRACTING better solutions from existing layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3299da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:26:03.880365Z",
     "iopub.status.busy": "2026-01-26T00:26:03.879737Z",
     "iopub.status.idle": "2026-01-26T00:26:04.255549Z",
     "shell.execute_reply": "2026-01-26T00:26:04.255106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded baseline with 20100 rows\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the baseline and understand the per-N score distribution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, getcontext\n",
    "from shapely import affinity\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "getcontext().prec = 25\n",
    "scale_factor = Decimal('1e18')\n",
    "\n",
    "class ChristmasTree:\n",
    "    def __init__(self, center_x='0', center_y='0', angle='0'):\n",
    "        self.center_x = Decimal(center_x)\n",
    "        self.center_y = Decimal(center_y)\n",
    "        self.angle = Decimal(angle)\n",
    "        \n",
    "        trunk_w = Decimal('0.15')\n",
    "        trunk_h = Decimal('0.2')\n",
    "        base_w = Decimal('0.7')\n",
    "        mid_w = Decimal('0.4')\n",
    "        top_w = Decimal('0.25')\n",
    "        tip_y = Decimal('0.8')\n",
    "        tier_1_y = Decimal('0.5')\n",
    "        tier_2_y = Decimal('0.25')\n",
    "        base_y = Decimal('0.0')\n",
    "        trunk_bottom_y = -trunk_h\n",
    "        \n",
    "        initial_polygon = Polygon([\n",
    "            (float(Decimal('0.0') * scale_factor), float(tip_y * scale_factor)),\n",
    "            (float(top_w / Decimal('2') * scale_factor), float(tier_1_y * scale_factor)),\n",
    "            (float(top_w / Decimal('4') * scale_factor), float(tier_1_y * scale_factor)),\n",
    "            (float(mid_w / Decimal('2') * scale_factor), float(tier_2_y * scale_factor)),\n",
    "            (float(mid_w / Decimal('4') * scale_factor), float(tier_2_y * scale_factor)),\n",
    "            (float(base_w / Decimal('2') * scale_factor), float(base_y * scale_factor)),\n",
    "            (float(trunk_w / Decimal('2') * scale_factor), float(base_y * scale_factor)),\n",
    "            (float(trunk_w / Decimal('2') * scale_factor), float(trunk_bottom_y * scale_factor)),\n",
    "            (float(-(trunk_w / Decimal('2')) * scale_factor), float(trunk_bottom_y * scale_factor)),\n",
    "            (float(-(trunk_w / Decimal('2')) * scale_factor), float(base_y * scale_factor)),\n",
    "            (float(-(base_w / Decimal('2')) * scale_factor), float(base_y * scale_factor)),\n",
    "            (float(-(mid_w / Decimal('4')) * scale_factor), float(tier_2_y * scale_factor)),\n",
    "            (float(-(mid_w / Decimal('2')) * scale_factor), float(tier_2_y * scale_factor)),\n",
    "            (float(-(top_w / Decimal('4')) * scale_factor), float(tier_1_y * scale_factor)),\n",
    "            (float(-(top_w / Decimal('2')) * scale_factor), float(tier_1_y * scale_factor)),\n",
    "        ])\n",
    "        \n",
    "        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))\n",
    "        self.polygon = affinity.translate(\n",
    "            rotated,\n",
    "            xoff=float(self.center_x * scale_factor),\n",
    "            yoff=float(self.center_y * scale_factor)\n",
    "        )\n",
    "\n",
    "def get_side_length(trees):\n",
    "    all_polygons = [t.polygon for t in trees]\n",
    "    bounds = unary_union(all_polygons).bounds\n",
    "    return max(bounds[2] - bounds[0], bounds[3] - bounds[1]) / float(scale_factor)\n",
    "\n",
    "# Load baseline\n",
    "baseline_path = \"/home/nonroot/snapshots/santa-2025/21337353543/submission/submission.csv\"\n",
    "df = pd.read_csv(baseline_path)\n",
    "df['x'] = df['x'].str.strip('s')\n",
    "df['y'] = df['y'].str.strip('s')\n",
    "df['deg'] = df['deg'].str.strip('s')\n",
    "\n",
    "print(f\"Loaded baseline with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d058a281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:26:04.256839Z",
     "iopub.status.busy": "2026-01-26T00:26:04.256722Z",
     "iopub.status.idle": "2026-01-26T00:26:10.050246Z",
     "shell.execute_reply": "2026-01-26T00:26:10.049801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline total score: 70.615107\n",
      "\n",
      "Top 10 highest per-N scores (most room for improvement):\n",
      "  N=1: 0.661250\n",
      "  N=2: 0.450779\n",
      "  N=3: 0.434745\n",
      "  N=5: 0.416850\n",
      "  N=4: 0.416545\n",
      "  N=7: 0.399897\n",
      "  N=6: 0.399610\n",
      "  N=9: 0.387415\n",
      "  N=8: 0.385407\n",
      "  N=15: 0.376949\n"
     ]
    }
   ],
   "source": [
    "# Calculate per-N scores for baseline\n",
    "baseline_scores = {}\n",
    "for n in range(1, 201):\n",
    "    group_data = df[df['id'].str.startswith(f'{n:03d}_')]\n",
    "    trees = [ChristmasTree(row['x'], row['y'], row['deg']) for _, row in group_data.iterrows()]\n",
    "    side = get_side_length(trees)\n",
    "    score = (side ** 2) / n\n",
    "    baseline_scores[n] = score\n",
    "\n",
    "total_score = sum(baseline_scores.values())\n",
    "print(f\"Baseline total score: {total_score:.6f}\")\n",
    "print(f\"\\nTop 10 highest per-N scores (most room for improvement):\")\n",
    "sorted_scores = sorted(baseline_scores.items(), key=lambda x: -x[1])\n",
    "for n, score in sorted_scores[:10]:\n",
    "    print(f\"  N={n}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fe0c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:26:10.051672Z",
     "iopub.status.busy": "2026-01-26T00:26:10.051551Z",
     "iopub.status.idle": "2026-01-26T00:26:10.240304Z",
     "shell.execute_reply": "2026-01-26T00:26:10.239881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 'rebuild from corners' on N=111:\n",
      "  N=10: baseline=0.376630, extracted=0.512733, diff=-0.13610265 ❌ same/worse\n",
      "  N=20: baseline=0.376057, extracted=0.411852, diff=-0.03579479 ❌ same/worse\n",
      "  N=30: baseline=0.360883, extracted=0.387273, diff=-0.02638951 ❌ same/worse\n",
      "  N=50: baseline=0.360753, extracted=0.395547, diff=-0.03479428 ❌ same/worse\n",
      "  N=100: baseline=0.343395, extracted=0.380929, diff=-0.03753383 ❌ same/worse\n"
     ]
    }
   ],
   "source": [
    "# Test the \"rebuild from corners\" technique on a sample large N\n",
    "# This extracts smaller N solutions from larger N layouts\n",
    "\n",
    "def rebuild_from_corners(large_n_trees, target_n):\n",
    "    \"\"\"Extract target_n trees from large_n layout using corner-based selection.\"\"\"\n",
    "    if len(large_n_trees) < target_n:\n",
    "        return None\n",
    "    \n",
    "    all_polygons = [t.polygon for t in large_n_trees]\n",
    "    bounds = unary_union(all_polygons).bounds\n",
    "    \n",
    "    best_subset = None\n",
    "    best_side = float('inf')\n",
    "    \n",
    "    # Try all 4 corners\n",
    "    for corner_x, corner_y in [(bounds[0], bounds[1]), (bounds[0], bounds[3]), \n",
    "                                (bounds[2], bounds[1]), (bounds[2], bounds[3])]:\n",
    "        # Calculate distance from corner for each tree\n",
    "        distances = []\n",
    "        for i, tree in enumerate(large_n_trees):\n",
    "            tree_bounds = tree.polygon.bounds\n",
    "            dist = max(\n",
    "                abs(tree_bounds[0] - corner_x),\n",
    "                abs(tree_bounds[2] - corner_x),\n",
    "                abs(tree_bounds[1] - corner_y),\n",
    "                abs(tree_bounds[3] - corner_y)\n",
    "            )\n",
    "            distances.append((dist, i))\n",
    "        \n",
    "        # Sort by distance and take closest target_n trees\n",
    "        distances.sort()\n",
    "        subset_indices = [idx for _, idx in distances[:target_n]]\n",
    "        subset = [large_n_trees[i] for i in subset_indices]\n",
    "        \n",
    "        # Calculate side length\n",
    "        side = get_side_length(subset)\n",
    "        if side < best_side:\n",
    "            best_side = side\n",
    "            best_subset = subset\n",
    "    \n",
    "    return best_subset, best_side\n",
    "\n",
    "# Test on N=111 -> extract N=10, N=20, N=30\n",
    "test_large_n = 111\n",
    "group_data = df[df['id'].str.startswith(f'{test_large_n:03d}_')]\n",
    "large_trees = [ChristmasTree(row['x'], row['y'], row['deg']) for _, row in group_data.iterrows()]\n",
    "\n",
    "print(f\"Testing 'rebuild from corners' on N={test_large_n}:\")\n",
    "for target_n in [10, 20, 30, 50, 100]:\n",
    "    subset, new_side = rebuild_from_corners(large_trees, target_n)\n",
    "    new_score = (new_side ** 2) / target_n\n",
    "    baseline_score = baseline_scores[target_n]\n",
    "    improvement = baseline_score - new_score\n",
    "    status = \"✅ BETTER\" if improvement > 1e-8 else \"❌ same/worse\"\n",
    "    print(f\"  N={target_n}: baseline={baseline_score:.6f}, extracted={new_score:.6f}, diff={improvement:.8f} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816b0209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:26:10.241500Z",
     "iopub.status.busy": "2026-01-26T00:26:10.241371Z",
     "iopub.status.idle": "2026-01-26T00:41:54.486074Z",
     "shell.execute_reply": "2026-01-26T00:41:54.485630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for improvements using 'rebuild from corners'...\n",
      "(Testing all large N from 50-200 to extract smaller N solutions)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 improvements!\n",
      "\n",
      "Top 20 improvements:\n",
      " target_n  source_n  baseline_score  new_score  improvement\n",
      "      135       136        0.346709   0.346708 4.627376e-07\n",
      "\n",
      "Total potential improvement: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Test on ALL large N values to find improvements\n",
    "print(\"Searching for improvements using 'rebuild from corners'...\")\n",
    "print(\"(Testing all large N from 50-200 to extract smaller N solutions)\\n\")\n",
    "\n",
    "improvements = []\n",
    "\n",
    "for large_n in range(50, 201):\n",
    "    group_data = df[df['id'].str.startswith(f'{large_n:03d}_')]\n",
    "    large_trees = [ChristmasTree(row['x'], row['y'], row['deg']) for _, row in group_data.iterrows()]\n",
    "    \n",
    "    for target_n in range(1, large_n):\n",
    "        subset, new_side = rebuild_from_corners(large_trees, target_n)\n",
    "        new_score = (new_side ** 2) / target_n\n",
    "        baseline_score = baseline_scores[target_n]\n",
    "        improvement = baseline_score - new_score\n",
    "        \n",
    "        if improvement > 1e-8:  # Significant improvement\n",
    "            improvements.append({\n",
    "                'target_n': target_n,\n",
    "                'source_n': large_n,\n",
    "                'baseline_score': baseline_score,\n",
    "                'new_score': new_score,\n",
    "                'improvement': improvement\n",
    "            })\n",
    "\n",
    "if improvements:\n",
    "    print(f\"Found {len(improvements)} improvements!\")\n",
    "    improvements_df = pd.DataFrame(improvements)\n",
    "    improvements_df = improvements_df.sort_values('improvement', ascending=False)\n",
    "    print(\"\\nTop 20 improvements:\")\n",
    "    print(improvements_df.head(20).to_string(index=False))\n",
    "    print(f\"\\nTotal potential improvement: {improvements_df['improvement'].sum():.6f}\")\n",
    "else:\n",
    "    print(\"No improvements found with 'rebuild from corners' technique.\")\n",
    "    print(\"The baseline layouts are already well-optimized for this approach.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
