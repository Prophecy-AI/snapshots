## Current Status
- Best CV score: 70.522682 from exp_002/004/006 (ensemble)
- Best LB score: 70.615107 from exp_001 (baseline)
- Target: 68.884199 | Gap to target: 1.73 points (2.45% improvement needed)

## ⛔⛔⛔ CRITICAL: STOP REPEATING FAILED APPROACHES ⛔⛔⛔

The last 4 experiments (007-010) ALL produced the SAME score (70.615107).
This means they ALL just returned the baseline - ZERO improvements found.

**APPROACHES THAT DO NOT WORK (PROVEN BY DATA):**
- ❌ Fractional translation (exp_007) - ZERO improvements
- ❌ Python simulated annealing (exp_008) - ZERO improvements  
- ❌ Random restart + lattice (exp_009) - 24-138% WORSE
- ❌ Constructive algorithms (exp_010) - 28-92% WORSE

**DO NOT TRY THESE AGAIN. They are EXHAUSTED.**

## ✅ THE ONLY PATH FORWARD: FIX ENSEMBLE VALIDATION

The ensemble approach (exp_002, exp_004, exp_006) found CV=70.522682 - a 0.09 improvement!
But ALL 3 attempts FAILED Kaggle validation with "Overlapping trees" errors.

**THE PROBLEM:** Our local Shapely validation passes, but Kaggle's validation fails.
**THE SOLUTION:** Use Kaggle's EXACT validation method from the chistyakov kernel.

## EXPERIMENT 011: KAGGLE-VALIDATED ENSEMBLE

**MANDATORY IMPLEMENTATION:**

### Step 1: Implement Kaggle's Exact Validation

```python
from decimal import Decimal, getcontext
from shapely import affinity
from shapely.geometry import Polygon

getcontext().prec = 25
scale_factor = Decimal('1e18')  # CRITICAL: Use 1e18, not 1e15

class ChristmasTree:
    """Kaggle's exact tree implementation with integer-scaled coordinates."""
    
    def __init__(self, center_x='0', center_y='0', angle='0'):
        self.center_x = Decimal(center_x)
        self.center_y = Decimal(center_y)
        self.angle = Decimal(angle)
        
        # Tree dimensions
        trunk_w = Decimal('0.15')
        trunk_h = Decimal('0.2')
        base_w = Decimal('0.7')
        mid_w = Decimal('0.4')
        top_w = Decimal('0.25')
        tip_y = Decimal('0.8')
        tier_1_y = Decimal('0.5')
        tier_2_y = Decimal('0.25')
        base_y = Decimal('0.0')
        trunk_bottom_y = -trunk_h
        
        # Create polygon with integer-scaled coordinates
        initial_polygon = Polygon([
            (Decimal('0.0') * scale_factor, tip_y * scale_factor),
            (top_w / Decimal('2') * scale_factor, tier_1_y * scale_factor),
            (top_w / Decimal('4') * scale_factor, tier_1_y * scale_factor),
            (mid_w / Decimal('2') * scale_factor, tier_2_y * scale_factor),
            (mid_w / Decimal('4') * scale_factor, tier_2_y * scale_factor),
            (base_w / Decimal('2') * scale_factor, base_y * scale_factor),
            (trunk_w / Decimal('2') * scale_factor, base_y * scale_factor),
            (trunk_w / Decimal('2') * scale_factor, trunk_bottom_y * scale_factor),
            (-(trunk_w / Decimal('2')) * scale_factor, trunk_bottom_y * scale_factor),
            (-(trunk_w / Decimal('2')) * scale_factor, base_y * scale_factor),
            (-(base_w / Decimal('2')) * scale_factor, base_y * scale_factor),
            (-(mid_w / Decimal('4')) * scale_factor, tier_2_y * scale_factor),
            (-(mid_w / Decimal('2')) * scale_factor, tier_2_y * scale_factor),
            (-(top_w / Decimal('4')) * scale_factor, tier_1_y * scale_factor),
            (-(top_w / Decimal('2')) * scale_factor, tier_1_y * scale_factor),
        ])
        
        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))
        self.polygon = affinity.translate(
            rotated,
            xoff=float(self.center_x * scale_factor),
            yoff=float(self.center_y * scale_factor)
        )

def kaggle_has_overlap(trees):
    """Check for overlaps using Kaggle's exact method."""
    from shapely.strtree import STRtree
    
    if len(trees) <= 1:
        return False
    
    polygons = [t.polygon for t in trees]
    tree_index = STRtree(polygons)
    
    for i, poly in enumerate(polygons):
        indices = tree_index.query(poly)
        for idx in indices:
            if idx == i:
                continue
            if poly.intersects(polygons[idx]) and not poly.touches(polygons[idx]):
                return True
    return False

def validate_n_kaggle(n, df):
    """Validate a single N value using Kaggle's exact method."""
    group_data = df[df['id'].str.startswith(f'{n:03d}_')]
    trees = []
    for _, row in group_data.iterrows():
        x = str(row['x']).replace('s', '')
        y = str(row['y']).replace('s', '')
        deg = str(row['deg']).replace('s', '')
        trees.append(ChristmasTree(x, y, deg))
    return not kaggle_has_overlap(trees)
```

### Step 2: Build Ensemble with Per-N Validation

```python
def build_validated_ensemble():
    """Build ensemble, validating EACH N value before including."""
    
    # Load baseline (known to pass Kaggle validation)
    baseline_path = "/home/nonroot/snapshots/santa-2025/21337353543/submission/submission.csv"
    baseline_df = pd.read_csv(baseline_path)
    
    # Load all snapshots
    snapshot_dir = "/home/nonroot/snapshots/santa-2025/"
    bad_snapshots = {'21145963314', '21145965159', '21336527339', '21337107511'}
    
    best_per_n = {}  # n -> (score, df_rows, source)
    
    # Initialize with baseline
    for n in range(1, 201):
        group_data = baseline_df[baseline_df['id'].str.startswith(f'{n:03d}_')]
        score = compute_score(group_data)
        best_per_n[n] = (score, group_data.copy(), 'baseline')
    
    # Try each snapshot
    for snapshot_id in os.listdir(snapshot_dir):
        if snapshot_id in bad_snapshots:
            continue
        
        csv_path = f"{snapshot_dir}/{snapshot_id}/submission/submission.csv"
        if not os.path.exists(csv_path):
            continue
        
        try:
            df = pd.read_csv(csv_path)
        except:
            continue
        
        for n in range(1, 201):
            group_data = df[df['id'].str.startswith(f'{n:03d}_')]
            if len(group_data) != n:
                continue
            
            score = compute_score(group_data)
            
            # Only consider if better than current best
            if score >= best_per_n[n][0]:
                continue
            
            # CRITICAL: Validate using Kaggle's exact method
            if not validate_n_kaggle(n, df):
                print(f"  N={n}: Skipping {snapshot_id} - fails Kaggle validation")
                continue
            
            # Improvement found and validated!
            improvement = best_per_n[n][0] - score
            print(f"  N={n}: IMPROVED by {improvement:.8f} from {snapshot_id}")
            best_per_n[n] = (score, group_data.copy(), snapshot_id)
    
    # Build final submission
    rows = []
    for n in range(1, 201):
        rows.append(best_per_n[n][1])
    
    final_df = pd.concat(rows, ignore_index=True)
    
    # Final validation of entire submission
    for n in range(1, 201):
        if not validate_n_kaggle(n, final_df):
            raise ValueError(f"Final validation failed for N={n}")
    
    return final_df
```

### Step 3: Save and Submit

```python
# Build validated ensemble
final_df = build_validated_ensemble()

# Save
final_df.to_csv('/home/submission/submission.csv', index=False)

# Calculate final score
total_score = sum(compute_score(final_df[final_df['id'].str.startswith(f'{n:03d}_')]) for n in range(1, 201))
print(f"Final CV score: {total_score:.6f}")
```

## EXPECTED OUTCOME

- If validation works: CV should be ~70.52-70.57 (some improvements may be rejected)
- If validation still fails: We need to investigate WHY Kaggle's validation differs

## ⚠️ IF THIS STILL FAILS

If the Kaggle-validated ensemble STILL fails:

1. **Check the baseline itself**: Does the baseline (21337353543) actually pass Kaggle validation?
   - If YES: The problem is in how we combine solutions
   - If NO: We need a different baseline

2. **Try single-N improvements**: Instead of full ensemble, try improving just ONE N value
   - Pick N=57 (had 0.0045 improvement in exp_002)
   - Validate that single N change
   - Submit to see if it passes

3. **Investigate precision loss**: Check if combining CSVs loses precision
   - Load baseline as strings, not floats
   - Preserve original string format when writing

## SUBMISSION STRATEGY

- Remaining submissions: 95
- **SUBMIT THIS EXPERIMENT** - We need LB feedback on the validated ensemble
- Even if it fails, the error message will tell us what's wrong

## ⛔ FORBIDDEN

- ❌ Running bbox3, sa_fast, eazy_optimizer, or any binary
- ❌ Trying fractional translation again (already failed)
- ❌ Trying Python SA again (already failed)
- ❌ Trying constructive algorithms again (already failed)
- ❌ Any approach that doesn't address the validation problem

## SUCCESS CRITERIA

- ✅ Submission passes Kaggle validation (no "Overlapping trees" error)
- ✅ LB score is better than 70.615107 (current best)
- ✅ If LB score is ~70.52, we've proven the ensemble approach works

## Response to Evaluator

The evaluator correctly identified that:
1. The last 4 experiments (007-010) all returned baseline with ZERO improvements
2. The ensemble approach is the most promising path but validation is broken
3. The C++ optimizer parameters were wrong (1000 iterations vs 15000+)

I agree with the evaluator's assessment. The priority is to FIX THE ENSEMBLE VALIDATION.
The evaluator's suggestion to use Kaggle's exact validation method (1e18 integer scaling with Decimal precision) is correct.

However, I disagree with the suggestion to fix C++ optimizer parameters - we don't have access to compile and run C++ code efficiently, and even with correct parameters, we'd need millions of iterations which would take too long in Python.

The focus should be 100% on getting the ensemble validation working. If we can get the 0.09 improvement from the ensemble to pass Kaggle validation, that's 5% of the gap closed. Then we can iterate on finding more improvements.