# Santa 2025 Christmas Tree Packing - Seed Prompt (Loop 3)

## Current Status
- Best CV score: 70.734327 from exp_001 (baseline)
- Best LB score: 70.734327 (verified)
- Target: 68.931058 | Gap to target: 1.803269 (2.6%)

## Public Kernel Status (CRITICAL!)
- Have we implemented the best kernel yet? **PARTIALLY** - We have the baseline but it's already optimal for available public data
- Top kernels identified: jonathanchan ensemble (combines many sources), egortrushin SA with translations
- Kernels we've implemented: baseline santa-2025.csv, tried SA with collision constraints
- **CRITICAL FINDING**: All external datasets (bucket-of-chump, santa25-public, telegram-public) score WORSE than or equal to baseline!

## Key Findings from Analysis

### Finding 1: External Datasets Don't Help
- Analyzed 22 CSV files from external datasets
- Best valid score from all sources = 70.734327 (same as baseline)
- The baseline (santa-2025.csv) is already the best publicly available valid solution

### Finding 2: The cpp_parallel_sa Configs Are Broken
- Ensemble from cpp_parallel_sa scores 67.77 (below target!) but has overlaps in 30 N values
- These configs were optimized WITHOUT collision constraints
- Simple nudge repair doesn't work - configurations are too densely packed
- ALL 2.96 points of improvement is locked behind overlaps

### Finding 3: SA from Baseline Doesn't Improve
- Ran SA with strict collision constraints on N=1-30
- Result: NO improvement - baseline is at a local optimum
- Backward propagation also showed no improvement

### Finding 4: Top Solutions Use Crystalline Packing
- From Medium article: "N < 58: Use SA for unstructured packings. N >= 58: Switch to Crystalline Packing"
- The egortrushin kernel uses "translations" - grid-like patterns of trees
- This creates collision-free packings by construction

## Response to Evaluator
The evaluator correctly identified that:
1. SA from baseline is stuck at a local optimum
2. The cpp_parallel_sa configs are fundamentally broken (not just slightly overlapping)
3. We need a different approach - structured packing patterns

I agree with the evaluator's recommendation to try structured packing. However, I also discovered that:
- ALL external datasets score worse than or equal to baseline
- The improvement potential is locked behind overlaps that can't be easily repaired
- We need to construct better packings from scratch, not repair existing ones

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Implement Structured Grid Packing (egortrushin approach)
Based on the egortrushin kernel, try constructing packings using grid translations:
- Use 2 base trees with optimal rotation angles
- Translate them in a grid pattern (nx × ny where nx*ny >= N)
- Optimize the translation distances (dx, dy) to minimize bounding box
- This creates collision-free packings BY CONSTRUCTION

**Implementation:**
```python
def structured_packing(n, tx, ty):
    """Create N trees using grid translation pattern."""
    best_score = float('inf')
    best_config = None
    
    # Try different grid dimensions
    for nx in range(1, n+1):
        ny = (n + nx - 1) // nx
        if nx * ny < n:
            continue
        
        # Try different angle combinations
        for angle1 in [0, 30, 45, 60, 90, 120, 150]:
            for angle2 in [0, 30, 45, 60, 90, 120, 150]:
                # Optimize translation distances
                for dx in np.linspace(0.4, 1.2, 30):
                    for dy in np.linspace(0.4, 1.2, 30):
                        trees = []
                        for i in range(nx):
                            for j in range(ny):
                                if len(trees) >= n:
                                    break
                                x = i * dx
                                y = j * dy
                                angle = angle1 if (i + j) % 2 == 0 else angle2
                                trees.append((x, y, angle))
                        
                        if not has_overlap(trees) and len(trees) == n:
                            score = compute_score(trees)
                            if score < best_score:
                                best_score = score
                                best_config = trees
    
    return best_config, best_score
```

Focus on N values where baseline might be suboptimal:
- Large N (>100) where crystalline packing should dominate
- N values that are perfect squares or near-perfect rectangles

### 2. **[HIGH PRIORITY]** Aggressive SA with Much Longer Runs
The previous SA used only 3000 iterations with 5 restarts. Try:
- 50,000+ iterations per restart
- 50+ restarts
- Larger initial step sizes (0.5+)
- Higher initial temperature
- Focus on small N (1-20) which contribute most to score

### 3. **[MEDIUM PRIORITY]** Hybrid Approach
For each N, compare:
- Baseline configuration
- Structured grid packing
- Long-run SA from random initialization
Keep the best valid configuration for each N

### 4. **[LOWER PRIORITY]** Per-N Targeted Optimization
Identify N values with worst efficiency (highest s²/n ratio relative to theoretical minimum)
Focus optimization effort on these specific N values

## What NOT to Try
- Simple nudge repair of cpp_parallel_sa configs (already proven to fail)
- Short SA runs from baseline (already proven to not improve)
- Ensembling from external datasets (they're all worse than baseline)

## SUBMISSION STRATEGY
- Remaining submissions: 95
- Submit after this experiment? **YES** - we have abundant submissions and need LB feedback
- Even if score doesn't improve, we need to verify our approach

## Validation Notes
- CV scheme: Direct score calculation (sum of s²/n for all N)
- Overlap detection: Use Shapely with STRtree for efficiency
- Always validate no overlaps before saving submission

## Technical Notes
- The bbox3 binary has GLIBC 2.34 compatibility issues (system has GLIBC 2.31)
- Use Python-based optimization instead
- Consider implementing the optimization in C++ with OpenMP for speed