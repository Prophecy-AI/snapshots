# Santa 2025 - Evolved Strategy (Loop 2)

## Current Status
- Best CV score: 70.615745 from exp_001 (valid baseline)
- Best LB score: PENDING (exp_001 not yet submitted!)
- Target: 68.887744 | Gap to target: 1.73 points (2.4%)
- Submissions used: 1/100 (94 remaining)

## ⚠️ CRITICAL: SUBMIT THE VALID BASELINE FIRST!

**BEFORE ANY OTHER WORK:**
1. Submit exp_001 (valid baseline) to Kaggle immediately
2. This establishes our LB reference point
3. We have 94 submissions remaining - this is worth it!

## Response to Evaluator

The evaluator correctly identified:
1. ✅ Valid baseline established with 0 overlapping trees
2. ✅ Per-N score tracking is in place
3. ⚠️ Valid baseline NOT YET SUBMITTED - must do this first!
4. ⚠️ No optimization attempted yet - time to start!

**Key insight from my analysis:** N=1 is ALREADY OPTIMAL at 45° (score 0.661250). The evaluator suggested N=1 optimization, but it's already at the theoretical minimum. Focus should be on N=2+ instead.

## Key Findings from Analysis

### N=1: Already Optimal
- Current angle: 45°, side: 0.813173, score: 0.661250
- Exhaustive search confirms this IS the optimal angle
- **No improvement possible for N=1**

### N=2: Can Be Improved!
- Current score: 0.450779
- Found better angles (144°, 324°) at same positions: score 0.436826
- **Improvement: 0.014 points just from angle optimization**
- Further improvement possible with position optimization

### Small N Potential
- N=2-10 optimization could close ~10.6% of gap (0.18 points)
- This is meaningful but not sufficient alone
- Need to optimize ALL N values

## ⛔ FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() with binaries - FORBIDDEN
- Running ANY pre-compiled binary - FORBIDDEN
- "Optimizing" existing CSV files with binary tools - FORBIDDEN

## ✅ MANDATORY EXPERIMENT SEQUENCE

### STEP 1: Submit Valid Baseline (IMMEDIATE)
```python
# The submission file is already at /home/submission/submission.csv
# Submit exp_001 to get LB feedback
```

### STEP 2: Implement Small N Optimizer (N=2-20)
Write Python code that:
1. For each N from 2 to 20:
   - Load current configuration from baseline
   - Try multiple angle combinations (grid search)
   - Try position perturbations with scipy.optimize
   - Check for overlaps before accepting
   - Save if improved

```python
from scipy.optimize import minimize
from itertools import product

def optimize_n(n, baseline_trees):
    """Optimize configuration for N trees."""
    best_side = calculate_bounding_box_side(baseline_trees)
    best_config = baseline_trees
    
    # Grid search over angles (coarse)
    angles = np.arange(0, 360, 5)  # 5-degree increments
    for angle_combo in product(angles, repeat=n):
        config = [(t[0], t[1], a) for t, a in zip(baseline_trees, angle_combo)]
        if not has_overlap(config):
            side = calculate_bounding_box_side(config)
            if side < best_side:
                best_side = side
                best_config = config
    
    # Local optimization with scipy
    def objective(params):
        # params = [x1, y1, a1, x2, y2, a2, ...]
        config = [(params[i*3], params[i*3+1], params[i*3+2]) for i in range(n)]
        if has_overlap(config):
            return 1000  # penalty
        return calculate_bounding_box_side(config)
    
    x0 = [v for t in best_config for v in t]
    result = minimize(objective, x0, method='Nelder-Mead')
    
    return result.x, result.fun
```

### STEP 3: Implement Simulated Annealing (Pure Python)
For N > 20, implement SA from scratch:

```python
def simulated_annealing(trees, T_start=1.0, T_end=0.0001, iterations=10000):
    """Pure Python SA for tree packing."""
    current = trees.copy()
    best = trees.copy()
    best_score = calculate_bounding_box_side(best)
    T = T_start
    
    for i in range(iterations):
        # Random move: translate or rotate one tree
        tree_idx = np.random.randint(len(current))
        move_type = np.random.choice(['translate', 'rotate'])
        
        new_config = current.copy()
        if move_type == 'translate':
            dx = np.random.uniform(-0.01, 0.01)
            dy = np.random.uniform(-0.01, 0.01)
            new_config[tree_idx] = (current[tree_idx][0] + dx, 
                                    current[tree_idx][1] + dy,
                                    current[tree_idx][2])
        else:
            da = np.random.uniform(-5, 5)
            new_config[tree_idx] = (current[tree_idx][0],
                                    current[tree_idx][1],
                                    (current[tree_idx][2] + da) % 360)
        
        if not has_overlap(new_config):
            new_score = calculate_bounding_box_side(new_config)
            delta = new_score - calculate_bounding_box_side(current)
            
            if delta < 0 or np.random.random() < np.exp(-delta / T):
                current = new_config
                if new_score < best_score:
                    best = new_config
                    best_score = new_score
        
        # Cool down
        T = T_start * (T_end / T_start) ** (i / iterations)
    
    return best, best_score
```

## ✅ REQUIRED: Per-N Tracking and Ensemble

After each optimization:
1. Compare per-N scores to baseline
2. Keep ONLY N values where you improved
3. Create ensemble submission with best per-N from all sources

```python
def create_ensemble(baseline_per_n, optimized_per_n, baseline_trees, optimized_trees):
    """Create ensemble using best per-N from each source."""
    ensemble = {}
    for n in range(1, 201):
        if optimized_per_n.get(n, float('inf')) < baseline_per_n[n]:
            ensemble[n] = optimized_trees[n]
            print(f"N={n}: Using optimized (improved by {baseline_per_n[n] - optimized_per_n[n]:.6f})")
        else:
            ensemble[n] = baseline_trees[n]
    return ensemble
```

## Recommended Experiment: exp_002_small_n_optimizer

**Goal:** Optimize N=2-20 using pure Python (no binaries)

**Steps:**
1. Submit exp_001 first (valid baseline)
2. Load baseline per-N scores
3. For N=2-20, implement grid search + local optimization
4. Track improvements per-N
5. Create ensemble submission
6. Validate no overlaps
7. Submit to Kaggle

**Expected improvement:** 0.1-0.3 points from small N optimization

## What NOT to Try
- Running bbox3 or any binary optimizer (produces same ~70.6 score)
- N=1 optimization (already optimal at 45°)
- Simple ensemble of existing CSVs (one source dominates)

## Validation Notes
- Use Shapely for overlap checking
- Consider integer-scaled coordinates for strict validation
- Validate ALL N values before submission
- Check that submission has exactly 20100 rows

## Success Criteria
- Score < 70.5 = meaningful progress
- Score < 70.0 = significant progress  
- Score < 69.5 = approaching target
- Score < 68.89 = TARGET ACHIEVED!

## SUBMISSION STRATEGY
- Remaining submissions: 94
- **Submit after EVERY experiment** - we have abundant submissions!
- LB feedback is free information - USE IT!
