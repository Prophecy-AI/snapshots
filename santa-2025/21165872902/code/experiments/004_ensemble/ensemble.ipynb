{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07645dac",
   "metadata": {},
   "source": [
    "# Experiment 004: Ensemble from Multiple Pre-optimized Files\n",
    "\n",
    "Take the best N from each pre-optimized file to create an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Tree geometry\n",
    "TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]\n",
    "TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]\n",
    "BASE_TREE = Polygon(zip(TX, TY))\n",
    "\n",
    "def parse_value(s):\n",
    "    if isinstance(s, str) and s.startswith('s'):\n",
    "        return float(s[1:])\n",
    "    return float(s)\n",
    "\n",
    "def create_tree_polygon(x, y, deg):\n",
    "    tree = affinity.rotate(BASE_TREE, deg, origin=(0, 0))\n",
    "    tree = affinity.translate(tree, x, y)\n",
    "    return tree\n",
    "\n",
    "def get_score_for_n(df, n):\n",
    "    \"\"\"Get the bounding box side and score for a specific N\"\"\"\n",
    "    prefix = f\"{n:03d}_\"\n",
    "    n_trees = df[df['id'].str.startswith(prefix)]\n",
    "    \n",
    "    if len(n_trees) != n:\n",
    "        return None, None\n",
    "    \n",
    "    all_coords = []\n",
    "    for _, row in n_trees.iterrows():\n",
    "        x = parse_value(row['x'])\n",
    "        y = parse_value(row['y'])\n",
    "        deg = parse_value(row['deg'])\n",
    "        poly = create_tree_polygon(x, y, deg)\n",
    "        coords = np.array(poly.exterior.coords)\n",
    "        all_coords.append(coords)\n",
    "    \n",
    "    all_coords = np.vstack(all_coords)\n",
    "    x_range = all_coords[:, 0].max() - all_coords[:, 0].min()\n",
    "    y_range = all_coords[:, 1].max() - all_coords[:, 1].min()\n",
    "    side = max(x_range, y_range)\n",
    "    score = side**2 / n\n",
    "    \n",
    "    return side, score\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pre-optimized files\n",
    "base_path = '/home/nonroot/snapshots/santa-2025/21116303805/code/preoptimized'\n",
    "files = [\n",
    "    f'{base_path}/santa-2025.csv',\n",
    "    f'{base_path}/best_ensemble.csv',\n",
    "    f'{base_path}/ensemble.csv',\n",
    "] + glob(f'{base_path}/santa25-public/*.csv')\n",
    "\n",
    "print(f\"Found {len(files)} files\")\n",
    "for f in files:\n",
    "    print(f\"  {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ed434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataframes and calculate scores for each N\n",
    "all_data = {}\n",
    "for filepath in files:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if len(df) != 20100:\n",
    "            print(f\"Skipping {os.path.basename(filepath)}: {len(df)} rows\")\n",
    "            continue\n",
    "        \n",
    "        scores = {}\n",
    "        for n in range(1, 201):\n",
    "            side, score = get_score_for_n(df, n)\n",
    "            if score is not None:\n",
    "                scores[n] = {'side': side, 'score': score, 'df': df}\n",
    "        \n",
    "        total = sum(s['score'] for s in scores.values())\n",
    "        all_data[filepath] = {'scores': scores, 'total': total}\n",
    "        print(f\"{os.path.basename(filepath)}: total={total:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best source for each N\n",
    "best_for_n = {}\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_source = None\n",
    "    \n",
    "    for filepath, data in all_data.items():\n",
    "        if n in data['scores']:\n",
    "            score = data['scores'][n]['score']\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_source = filepath\n",
    "    \n",
    "    if best_source:\n",
    "        best_for_n[n] = {'source': best_source, 'score': best_score}\n",
    "\n",
    "# Calculate ensemble total\n",
    "ensemble_total = sum(d['score'] for d in best_for_n.values())\n",
    "print(f\"\\nEnsemble total score: {ensemble_total:.6f}\")\n",
    "\n",
    "# Compare with best single file\n",
    "best_single = min(all_data.items(), key=lambda x: x[1]['total'])\n",
    "print(f\"Best single file: {os.path.basename(best_single[0])} = {best_single[1]['total']:.6f}\")\n",
    "print(f\"Improvement from ensemble: {best_single[1]['total'] - ensemble_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c246d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which source is best for each N\n",
    "from collections import Counter\n",
    "source_counts = Counter(d['source'] for d in best_for_n.values())\n",
    "print(\"\\nSource distribution:\")\n",
    "for source, count in source_counts.most_common():\n",
    "    print(f\"  {os.path.basename(source)}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ensemble submission\n",
    "ensemble_rows = []\n",
    "for n in range(1, 201):\n",
    "    source = best_for_n[n]['source']\n",
    "    df = all_data[source]['scores'][n]['df']\n",
    "    prefix = f\"{n:03d}_\"\n",
    "    n_trees = df[df['id'].str.startswith(prefix)]\n",
    "    ensemble_rows.append(n_trees)\n",
    "\n",
    "ensemble_df = pd.concat(ensemble_rows, ignore_index=True)\n",
    "print(f\"Ensemble has {len(ensemble_df)} rows\")\n",
    "print(ensemble_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83060808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ensemble score\n",
    "def calculate_total_score(df):\n",
    "    total = 0\n",
    "    for n in range(1, 201):\n",
    "        side, score = get_score_for_n(df, n)\n",
    "        if score:\n",
    "            total += score\n",
    "    return total\n",
    "\n",
    "verified_score = calculate_total_score(ensemble_df)\n",
    "print(f\"Verified ensemble score: {verified_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble\n",
    "os.makedirs('/home/code/experiments/004_ensemble', exist_ok=True)\n",
    "ensemble_df.to_csv('/home/code/experiments/004_ensemble/ensemble.csv', index=False)\n",
    "print(\"Saved ensemble to experiments/004_ensemble/ensemble.csv\")\n",
    "\n",
    "# Copy to submission\n",
    "import shutil\n",
    "shutil.copy('/home/code/experiments/004_ensemble/ensemble.csv', '/home/submission/submission.csv')\n",
    "print(\"Copied to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {'cv_score': verified_score}\n",
    "with open('/home/code/experiments/004_ensemble/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)\n",
    "print(f\"Saved metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
