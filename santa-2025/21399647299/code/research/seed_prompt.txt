# Santa 2025 - Christmas Tree Packing Optimization

## Current Status
- **Best CV score**: 70.315653 from exp_022 (extended C++ optimization)
- **Best LB score**: 70.316492 (verified on Kaggle)
- **Target**: 68.872047 (lower is better)
- **Gap to target**: 1.44 points (2.10% improvement needed)
- **Submissions used**: 13/100 (87 remaining)

## ⚠️ CRITICAL SITUATION: 32 EXPERIMENTS, STUCK AT LOCAL OPTIMUM

After 32 experiments, we are STUCK at ~70.316:
- **12 consecutive experiments (exp_020-031)** found ZERO improvement
- All novel algorithms (GA, B&B, lattice, CP, gradient, tessellation) FAILED
- The baseline is at an EXTREMELY strong local optimum
- SA with 576 seconds, 50K iterations, 80 restarts found improvement of 0.000000319

**THE CURRENT APPROACH IS NOT WORKING. WE NEED A FUNDAMENTALLY DIFFERENT STRATEGY.**

## Response to Evaluator

The evaluator correctly identified:
1. **Best score (70.26573) from exp_007 was NEVER successfully submitted** - it failed with "Evaluation metric raised an unexpected error"
2. **Submission budget is underutilized** - only 13/100 used
3. **12 consecutive experiments found ZERO improvement** - algorithm exhaustion
4. **External data may not be fully exploited** - 279 CSV files in external folder

**Key insight**: The ONLY approach that worked was the ensemble approach (exp_007-010), which improved score from 70.615 to 70.265 (0.35 points). But these ensembles keep failing Kaggle validation due to overlaps.

## ⛔ FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() with binaries - FORBIDDEN
- More SA/local search variations - PROVEN NOT TO WORK
- Tessellation/lattice approaches - PROVEN NOT TO WORK (12 experiments failed)
- Gradient/physics-based optimization - PROVEN NOT TO WORK

## ✅ MANDATORY NEXT EXPERIMENT: ROBUST ENSEMBLE WITH STRICT VALIDATION

The ensemble approach is the ONLY thing that worked. The problem is overlap validation.

### Step 1: Create Ultra-Conservative Ensemble

```python
# experiments/032_robust_ensemble/robust_ensemble.py

import pandas as pd
import numpy as np
from shapely import Polygon
from decimal import Decimal, getcontext
getcontext().prec = 50

# CRITICAL: Use integer arithmetic for overlap checking
SCALE = 10**18

def validate_no_overlap_strict(trees):
    """
    Ultra-strict overlap validation using integer arithmetic.
    This MUST match Kaggle's validation exactly.
    """
    polygons = []
    for x, y, deg in trees:
        # Get tree vertices
        vertices = get_tree_vertices(x, y, deg)
        # Scale to integers
        int_vertices = [(int(Decimal(str(vx)) * SCALE), 
                         int(Decimal(str(vy)) * SCALE)) 
                        for vx, vy in vertices]
        polygons.append(Polygon(int_vertices))
    
    for i in range(len(polygons)):
        for j in range(i+1, len(polygons)):
            if polygons[i].intersects(polygons[j]):
                if not polygons[i].touches(polygons[j]):
                    intersection = polygons[i].intersection(polygons[j])
                    if intersection.area > 0:  # ANY overlap is bad
                        return False, f"Trees {i} and {j} overlap"
    return True, "OK"

# For each N, only keep improvements that pass STRICT validation
MIN_IMPROVEMENT = 0.01  # Much higher threshold to avoid precision issues
```

### Step 2: Systematic External Data Audit

```python
# List ALL external CSV files
import os
external_dir = "/home/code/data/external"
all_csvs = []
for root, dirs, files in os.walk(external_dir):
    for f in files:
        if f.endswith('.csv'):
            all_csvs.append(os.path.join(root, f))

print(f"Found {len(all_csvs)} CSV files")

# For each file, extract best per-N solutions
best_per_n = {}  # n -> (score, source_file, trees)

for csv_path in all_csvs:
    try:
        df = pd.read_csv(csv_path)
        for n in range(1, 201):
            trees = extract_trees_for_n(df, n)
            if trees is None:
                continue
            
            # Validate NO overlaps
            ok, msg = validate_no_overlap_strict(trees)
            if not ok:
                continue
            
            score = compute_score(trees, n)
            if n not in best_per_n or score < best_per_n[n][0]:
                best_per_n[n] = (score, csv_path, trees)
    except:
        continue

# Compare to current baseline
improvements = []
for n in range(1, 201):
    if n in best_per_n:
        new_score = best_per_n[n][0]
        baseline_score = get_baseline_score(n)
        if new_score < baseline_score - MIN_IMPROVEMENT:
            improvements.append((n, baseline_score - new_score, best_per_n[n][1]))

print(f"Found {len(improvements)} improvements with MIN_IMPROVEMENT={MIN_IMPROVEMENT}")
```

### Step 3: Create Ensemble with ONLY Validated Improvements

```python
# Start with current best submission
final_solution = load_current_best()

# Apply ONLY improvements that:
# 1. Pass strict overlap validation
# 2. Improve by at least MIN_IMPROVEMENT
# 3. Have been verified on multiple sources

for n, improvement, source in improvements:
    trees = best_per_n[n][2]
    
    # Double-check validation
    ok, msg = validate_no_overlap_strict(trees)
    if not ok:
        print(f"N={n}: SKIPPED - failed validation")
        continue
    
    # Apply improvement
    final_solution[n] = trees
    print(f"N={n}: Applied improvement of {improvement:.6f} from {source}")

# Final validation of entire solution
for n in range(1, 201):
    ok, msg = validate_no_overlap_strict(final_solution[n])
    if not ok:
        print(f"CRITICAL: N={n} failed final validation!")
        # Fall back to baseline for this N
        final_solution[n] = baseline[n]
```

## ✅ ALTERNATIVE: FOCUS ON SMALL N OPTIMIZATION

Small N values contribute disproportionately to score:
- N=1-10: 4.32 points (6.2% of total)
- N=1 alone: 0.661 points

### N=1 Optimization (CRITICAL)
N=1 is currently at 0.661250 (45° rotation, side=0.8132).
This is the THEORETICAL MINIMUM for a single tree.
**N=1 is already optimal - cannot be improved.**

### N=2-10 Optimization
For N=2-10, try:
1. Exhaustive angle search (0.1° increments)
2. NFP-based optimal placement
3. Branch-and-bound with pruning

```python
def optimize_small_n(n, angle_step=0.1):
    """Exhaustive search for small N."""
    best_score = float('inf')
    best_config = None
    
    angles = np.arange(0, 360, angle_step)
    
    for angle_combo in itertools.product(angles, repeat=n):
        # Place trees using NFP-based placement
        config = place_trees_nfp(n, angle_combo)
        if config is None:
            continue
        
        # Validate
        ok, _ = validate_no_overlap_strict(config)
        if not ok:
            continue
        
        score = compute_score(config, n)
        if score < best_score:
            best_score = score
            best_config = config
    
    return best_config, best_score
```

## Per-N Score Analysis

Current per-N scores (top contributors):
- N=1: 0.661250 (optimal)
- N=2: 0.450779
- N=3: 0.434745
- N=4: 0.416545
- N=5: 0.416850
- N=6: 0.399610
- N=7: 0.399842
- N=8: 0.385407
- N=9: 0.383047
- N=10: 0.376630

**To close the 1.44 point gap, we need ~2% improvement across ALL N values.**

## Submission Strategy

With 87 submissions remaining, SUBMIT EVERY VALID EXPERIMENT:
1. Create robust ensemble with strict validation
2. SUBMIT to verify it passes Kaggle validation
3. If it fails, identify which N values cause overlaps
4. Fall back to baseline for those N values
5. Resubmit

## What We've Learned (from 32 experiments)

### What WORKS:
- Ensemble approach (combining best per-N from multiple sources)
- Strict overlap validation with integer arithmetic
- Conservative improvement thresholds (MIN_IMPROVEMENT >= 0.001)

### What DOESN'T WORK:
- Local search (SA, hill climbing) - baseline is at strong local optimum
- Novel algorithms (GA, B&B, lattice, CP, gradient, tessellation) - all failed
- Extended C++ optimization - 576 seconds found 0.000000319 improvement
- Asymmetric tessellation - found ZERO improvement

### Key Insight:
The baseline is at an EXTREMELY strong local optimum. The ONLY way to improve is:
1. Find better solutions from external sources (ensemble approach)
2. Ensure those solutions pass Kaggle's strict overlap validation

## SUBMIT AFTER THIS EXPERIMENT

After creating the robust ensemble:
1. Validate ALL N values with strict integer arithmetic
2. SUBMIT to get LB feedback
3. If any N fails, identify and fix
4. Resubmit until we have a valid submission

**TARGET**: Get a valid submission with score < 70.316 (current best LB)