## Current Status
- Best CV score: 70.365091 from exp_010 (safe_ensemble)
- Best LB score: 70.365091 (exp_010 - PASSED Kaggle validation)
- Target: 68.878195 | Gap to target: 1.49 points (2.11%)

## ⚠️ CRITICAL: exp_013 FAILED - DO NOT REPEAT THE MISTAKE

**exp_013 (selective_threshold) failed with "Overlapping trees in group 089"**

Root cause: Used lower threshold (0.0001) for "safe" N values, but N=89 had a small improvement (0.000330) that passed local validation but failed Kaggle's stricter validation.

**LESSON LEARNED:** Kaggle's validation is STRICTER than our local validation. Any improvement < 0.001 is RISKY and may cause overlap failures.

## Submission Log
| Exp | Approach | CV | LB | Notes |
|-----|----------|----|----|-------|
| 001 | valid_baseline | 70.615 | 70.615 | Baseline from snapshots |
| 002 | backward_propagation | 70.615 | 70.615 | No improvement |
| 007 | ensemble_fractional | 70.266 | pending | N=24 contributed 0.348 improvement |
| 010 | safe_ensemble | 70.365 | 70.365 | ✅ PASSED - MIN_IMPROVEMENT=0.001 |
| 013 | selective_threshold | 70.342 | FAILED | ❌ Overlap in N=89 |

## What We've Learned
1. **Kaggle validation is stricter** - improvements < 0.001 are risky
2. **exp_010 is our best valid submission** - 70.365091
3. **The baseline is at a strong local optimum** - SA, exhaustive search, NFP all failed to improve
4. **External data (santa-2025.csv) has better solutions** - but small improvements cause overlap failures
5. **C++ optimizer found only 0.00003 improvement** - the baseline is truly optimal for local search

## Response to Evaluator

The evaluator correctly identified that:
1. The selective threshold approach was too aggressive
2. N=89 failed because its improvement (0.000330) was below the safe threshold
3. We need fundamentally different optimization, not incremental improvements

**I agree with the evaluator's assessment.** The current approach (ensemble + local optimization) has hit a ceiling at ~70.36. To reach 68.88, we need:
- 1.49 points of improvement (2.11% reduction)
- This cannot come from small per-N improvements (they cause overlap failures)
- We need LARGE improvements that are clearly valid

## ⛔ FORBIDDEN (WILL BE REJECTED)
- Running bbox3, sa_fast, eazy_optimizer, tree_packer binaries
- subprocess.run() or os.system() to run binaries
- Selective thresholds < 0.001 (causes overlap failures)
- Any approach that gave < 0.01 improvement in previous experiments

## ✅ NEXT EXPERIMENT: Fix N=89 and Submit

**IMMEDIATE ACTION:** Create exp_014 that:
1. Starts from exp_010 (our last valid submission)
2. Does NOT use any improvements with threshold < 0.001
3. Validates ALL N values with strict overlap checking
4. Submits to verify we're back to a valid baseline

```python
# exp_014: Return to safe baseline
# Simply copy exp_010 and verify it passes validation
import shutil
shutil.copy('/home/code/experiments/010_safe_ensemble/submission.csv', 
            '/home/submission/submission.csv')
```

## ✅ AFTER THAT: Try Fundamentally Different Approaches

The gap is 1.49 points. We need LARGE improvements, not small ones.

### Option 1: Multi-Restart Optimization from Scratch
Instead of optimizing the baseline, generate NEW solutions from scratch:
```python
# For each N, generate 100 random valid configurations
# Keep the best one
# This might find configurations that are DIFFERENT from the baseline
for n in range(1, 201):
    best_score = float('inf')
    for restart in range(100):
        config = generate_random_valid_config(n)
        score = calculate_score(config)
        if score < best_score:
            best_score = score
            best_config = config
```

### Option 2: Tessellation Patterns for Large N
For N > 50, try tessellation-based placement:
- Hexagonal packing
- Square grid with rotation
- Diagonal patterns

### Option 3: Branch-and-Bound for Small N
For N=2-10, implement exact branch-and-bound:
- Enumerate all possible placements
- Prune branches that can't improve
- Guarantee optimal solutions

### Option 4: Download More External Data
The jonathanchan kernel uses 15+ external data sources. We should download:
- https://www.kaggle.com/datasets/jazivxt/bucket-of-chump
- https://www.kaggle.com/datasets/asalhi/telegram-public-shared-solution-for-santa-2025

## ✅ REQUIRED: Per-N Tracking

Track best solution for EACH N separately:
```python
# After each experiment, compare per-N scores
for n in range(1, 201):
    my_score = calculate_score(my_config[n])
    baseline_score = calculate_score(baseline_config[n])
    improvement = baseline_score - my_score
    
    # ONLY keep if improvement >= 0.001 (safe threshold)
    if improvement >= 0.001:
        # Validate no overlaps
        if validate_no_overlap_strict(my_config[n]):
            print(f"✅ N={n}: IMPROVED by {improvement:.6f}")
```

## Key Insight: The Gap Analysis

- Current: 70.365091
- Target: 68.878195
- Gap: 1.486896 (2.11%)

- Top 20 N values contribute 11.4% of score
- Bottom 180 N values contribute 88.6% of score
- Large N (101-200) contributes 33.9 points

**To close the gap, we need:**
- Either 2.11% improvement across ALL N values
- Or larger improvements on specific N values

**The problem:** Small improvements (< 0.001) cause overlap failures on Kaggle.
**The solution:** Find LARGE improvements (> 0.01) that are clearly valid.

## What NOT to Try
- Selective thresholds < 0.001 (causes overlap failures)
- Running C++ optimizers (they only find 0.00003 improvement)
- Backward propagation (no improvement found)
- Exhaustive search on small N (baseline is already optimal)

## SUBMIT EVERY EXPERIMENT
We have 96 submissions remaining. Submit after EVERY experiment to get LB feedback.
