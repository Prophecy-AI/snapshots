# Santa 2025 - Evolved Strategy (Loop 21)

## Current Status
- **Best CV score**: 70.316579 (exp_020_optimal_ensemble_final)
- **Best LB score**: 70.343408 (exp_019) - but we have BETTER CV not yet submitted!
- **Target**: 68.876781
- **Gap to target**: 1.44 points (2.09%)
- **Submissions remaining**: 90/100

## ‚ö†Ô∏è CRITICAL: SUBMIT exp_020 IMMEDIATELY!

The current submission in `/home/submission/submission.csv` has score **70.316579** which is:
- 0.027 BETTER than the best LB (70.343408)
- Passes local overlap validation
- MUST be submitted to get LB feedback

**FIRST ACTION: Submit exp_020 to Kaggle!**

## Submission Log
| Exp | Approach | CV | LB | Notes |
|-----|----------|----|----|-------|
| 001 | valid_baseline | 70.615 | 70.615 | PASSED |
| 002 | backward_prop | 70.615 | 70.615 | No improvement |
| 010 | safe_ensemble | 70.365 | 70.365 | PASSED |
| 016 | ensemble | 70.354 | 70.354 | PASSED |
| 019 | ensemble | 70.343 | 70.343 | PASSED |
| 020 | optimal_ensemble | 70.317 | **PENDING** | SUBMIT NOW! |

## Response to Evaluator

The evaluator correctly identified:
1. **Gap is still large (1.44 points)** - Current ensemble approach may be near ceiling
2. **2517 overlap rejections** - Significant potential improvements lost
3. **Need more external data sources** - bucket-of-chump, telegram solutions
4. **CV-LB match is perfect** - This is deterministic, not distribution shift

**HOWEVER**, the evaluator's suggestion to "run C++ optimizer for extended periods" is BLOCKED by the rules. We must implement algorithms in Python.

## ‚õî FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() with ANY binary - FORBIDDEN
- Running ANY pre-compiled binary or executable - FORBIDDEN
- "Optimizing" existing CSV files with C++ tools - FORBIDDEN

## üéØ THE REAL PROBLEM

**We are at the PUBLIC KERNEL CEILING (~70.3x)**

All public kernels converge to the same score range:
- base-model-ensemble-santa: ~70.3x
- team-optimization-blend: ~70.3x
- why-not: 70.332
- Our best: 70.316

**To reach 68.88, we need 1.44 points improvement - this is NOT achievable by:**
- More ensemble combinations
- Different thresholds
- More external data sources (they all have ~70.3x too)

**We need a FUNDAMENTALLY DIFFERENT approach.**

## ‚úÖ MANDATORY NEXT STEPS

### Step 1: SUBMIT exp_020 (IMMEDIATE)
The current submission has CV=70.316579. Submit it NOW to get LB feedback.

### Step 2: IMPLEMENT NOVEL ALGORITHM

Since all ensemble/optimization approaches converge to ~70.3x, we need to implement something new.

**Option A: Extended Local Search with Restarts**
```python
# For each N, run 100+ random restarts of simulated annealing
# Keep the best valid solution across all restarts
# This is different from running bbox3 - we implement SA ourselves

def multi_restart_sa(n, num_restarts=100):
    best_score = float('inf')
    best_config = None
    
    for restart in range(num_restarts):
        # Random initial configuration
        config = random_valid_config(n)
        
        # Run SA from this starting point
        config = simulated_annealing(config, iterations=10000)
        
        # Validate and score
        if validate_no_overlap(config):
            score = calculate_score(config)
            if score < best_score:
                best_score = score
                best_config = config
    
    return best_config, best_score
```

**Option B: Tessellation/Lattice for Large N**
```python
# For N >= 50, use periodic lattice placement
# Optimize the lattice parameters instead of individual trees

def tessellation_packing(n):
    # Define 2-tree unit cell
    # Optimize: angle1, angle2, dx, dy, offset
    # Generate N trees by tiling the unit cell
    pass
```

**Option C: Branch-and-Bound for Small N**
```python
# For N=2-10, exhaustively search all configurations
# Use pruning to avoid exploring bad branches

def branch_and_bound(n, angle_step=0.5):
    # For N=2: 720 * 720 = 518,400 combinations
    # For N=3: 720^3 = 373M combinations (need pruning)
    pass
```

### Step 3: FOCUS ON SPECIFIC N VALUES

The top 20 N values by score contribution:
- N=1: 0.661 (already optimal)
- N=2: 0.451
- N=3: 0.435
- N=4: 0.417
- N=5: 0.417
- N=6-10: 0.38-0.40 each

**If we can improve N=2-10 by just 0.01 each, that's 0.09 points!**

### Step 4: PER-N TRACKING (MANDATORY)

```python
# After EVERY experiment, compare per-N scores
baseline_scores = load_baseline_per_n_scores()

for n in range(1, 201):
    new_score = calculate_score_for_n(new_config, n)
    if new_score < baseline_scores[n] - 0.0001:
        print(f"‚úÖ N={n}: IMPROVED by {baseline_scores[n] - new_score:.6f}")
        save_improved_solution(n, new_config[n])

# Even if total score is worse, individual N improvements are valuable!
```

## üî¨ EXPERIMENT PLAN

### Experiment 021: Submit exp_020 + Analyze Per-N Opportunities

1. **Submit exp_020** to get LB feedback
2. **Analyze per-N scores** to find where we're weakest
3. **Identify N values** where external sources have better solutions but overlaps prevented use
4. **Plan targeted optimization** for specific N values

### Experiment 022: Multi-Restart SA for Small N

1. **Implement SA from scratch** (not using bbox3)
2. **Run 100 restarts** for N=2-20
3. **Compare to baseline** per-N
4. **Keep any improvements**

### Experiment 023: Tessellation for Large N

1. **Implement tessellation** for N >= 50
2. **Optimize lattice parameters** using gradient-free optimization
3. **Compare to baseline** per-N
4. **Keep any improvements**

## What NOT to Try

1. ‚ùå More ensemble combinations (we're at the ceiling)
2. ‚ùå Different MIN_IMPROVEMENT thresholds (already tried 1e-10)
3. ‚ùå Running bbox3/sa_fast binaries (FORBIDDEN)
4. ‚ùå Loading more external data (they all have ~70.3x)

## Expected Outcome

- exp_020 submission: LB ~70.316 (if validation passes)
- Multi-restart SA: Potential 0.01-0.05 improvement on small N
- Tessellation: Potential 0.1-0.5 improvement on large N

**Total potential improvement: 0.1-0.5 points**

This would bring us to ~69.8-70.2, still 0.9-1.3 points from target.

**REALITY CHECK**: The target of 68.88 may require:
1. Running optimization for DAYS (not hours)
2. 900+ submissions to accumulate improvements
3. Techniques not publicly shared

But we MUST try. The target IS reachable - top competitors achieved it.
