## Current Status
- Best CV score: 70.353516 from exp_016
- Best LB score: 70.353516 (verified, perfect CV-LB match)
- Target: 68.877877 | Gap to target: 1.476 points (2.1%)

## ⚠️ CRITICAL FINDING: exp_007 DATA CORRUPTION
The evaluator discovered that exp_007 (CV=70.2657) has CORRUPTED DATA:
- N=24 has NaN values for ALL x-coordinates
- The "0.348 improvement" was NEVER REAL - it was a scoring bug on corrupted data
- DO NOT use exp_007 as a baseline or reference

## Submission Log
| Exp | Approach | CV | LB | Notes |
|-----|----------|----|----|-------|
| 001 | valid_baseline | 70.6151 | 70.6151 | First valid submission |
| 010 | safe_ensemble | 70.3651 | 70.3651 | Ensemble with MIN_IMPROVEMENT=0.001 |
| 016 | mega_ensemble_external | 70.3535 | 70.3535 | Best so far, uses external data |
| 018 | genetic_algorithm | 70.3535 | N/A | GA found 0 improvements, fell back to baseline |

## What We've Learned
1. **All Python optimization approaches FAILED** (0 improvements found):
   - Simulated Annealing (SA)
   - Genetic Algorithm (GA)
   - Exhaustive search for N=2
   - No-Fit Polygon (NFP) placement
   - Backward propagation (N to N-1)
   - Multi-start random initialization
   - Fractional translation

2. **What WORKED**:
   - Ensemble from external data sources (+0.25 points)
   - MIN_IMPROVEMENT=0.001 threshold (prevents overlap failures)

3. **We're missing external data sources**:
   - Top kernels (jonathanchan) use 17+ sources
   - We only have 4 sources: bucket-of-chump, saspav, telegram, chistyakov
   - MISSING: santa25-public, santa-2025-try3, why-not, santa-claude, etc.

## Response to Evaluator
The evaluator correctly identified:
1. ✅ exp_007 has corrupted data (NaN in N=24) - this explains the fake improvement
2. ✅ GA experiment executed correctly but found 0 improvements
3. ✅ We need MORE external data sources (17 vs our 4)
4. ✅ The path forward is external data, not more optimization algorithms

I AGREE with the evaluator's assessment. We've exhausted local optimization approaches.
The ONLY path forward is:
1. Download more external datasets
2. Compile and run bbox3/C++ optimizers from source
3. Build comprehensive ensemble from all sources

## ⛔ FORBIDDEN (DO NOT DO)
- ❌ More Python optimization algorithms (SA, GA, etc.) - ALL HAVE FAILED
- ❌ Using exp_007 as baseline (it has corrupted data)
- ❌ Running bbox3/sa_fast binaries (GLIBC incompatible)
- ❌ Any approach that gave < 0.01 improvement

## ✅ NEXT EXPERIMENT: DOWNLOAD ALL EXTERNAL DATA AND BUILD MEGA-ENSEMBLE

### Step 1: Download Missing Datasets
```bash
# Create external data directory
mkdir -p /home/code/data/external

# Download ALL datasets used by top kernels
kaggle datasets download -d jazivxt/bucket-of-chump -p /home/code/data/external/ --unzip
kaggle datasets download -d jonathanchan/santa25-public -p /home/code/data/external/ --unzip
kaggle datasets download -d seowoohyeon/santa-2025-try3 -p /home/code/data/external/ --unzip
kaggle datasets download -d asalhi/telegram-public-shared-solution-for-santa-2025 -p /home/code/data/external/ --unzip
kaggle datasets download -d saspav/santa-2025-csv -p /home/code/data/external/ --unzip
kaggle datasets download -d chistyakov/santa2025-packed-version-of-current-best-public -p /home/code/data/external/ --unzip

# Also download from GitHub
wget -q https://raw.githubusercontent.com/SmartManoj/Santa-Scoreboard/main/submission.csv -O /home/code/data/external/smartmanoj.csv
```

### Step 2: Download Top Kernel Outputs
```bash
# These kernels have good solutions embedded
kaggle kernels output jazivxt/why-not -p /home/code/data/external/why-not/
kaggle kernels output saspav/santa-submission -p /home/code/data/external/santa-submission/
kaggle kernels output datafad/decent-starting-solution -p /home/code/data/external/decent-starting-solution/
kaggle kernels output smartmanoj/santa-claude -p /home/code/data/external/santa-claude/
```

### Step 3: Build Comprehensive Ensemble
```python
import pandas as pd
import numpy as np
import glob
from numba import njit
import math

# Load ALL CSV files from:
# 1. /home/code/data/external/
# 2. /home/nonroot/snapshots/santa-2025/*/code/**/*.csv
# 3. Current best (exp_016)

MIN_IMPROVEMENT = 0.001  # Safety threshold

@njit
def make_polygon_template():
    tw=0.15; th=0.2; bw=0.7; mw=0.4; ow=0.25
    tip=0.8; t1=0.5; t2=0.25; base=0.0; tbot=-th
    x=np.array([0,ow/2,ow/4,mw/2,mw/4,bw/2,tw/2,tw/2,-tw/2,-tw/2,-bw/2,-mw/4,-mw/2,-ow/4,-ow/2],np.float64)
    y=np.array([tip,t1,t1,t2,t2,base,base,tbot,tbot,base,base,t2,t2,t1,t1],np.float64)
    return x,y

@njit
def score_group(xs,ys,degs,tx,ty):
    n=xs.size; V=tx.size
    mnx=1e300; mny=1e300; mxx=-1e300; mxy=-1e300
    for i in range(n):
        r=degs[i]*math.pi/180.0
        c=math.cos(r); s=math.sin(r)
        xi=xs[i]; yi=ys[i]
        for j in range(V):
            X=c*tx[j]-s*ty[j]+xi
            Y=s*tx[j]+c*ty[j]+yi
            if X<mnx: mnx=X
            if X>mxx: mxx=X
            if Y<mny: mny=Y
            if Y>mxy: mxy=Y
    side=max(mxx-mnx,mxy-mny)
    return side*side/n

def strip(a):
    return np.array([float(str(v).replace("s","")) for v in a],np.float64)

# Collect all CSV files
all_files = []
all_files += glob.glob('/home/code/data/external/**/*.csv', recursive=True)
all_files += glob.glob('/home/nonroot/snapshots/santa-2025/*/code/**/*.csv', recursive=True)
all_files.append('/home/code/experiments/016_mega_ensemble_external/submission.csv')

tx, ty = make_polygon_template()
best = {n: {"score": 1e300, "data": None, "src": None} for n in range(1, 201)}

# Load baseline first
baseline_df = pd.read_csv('/home/code/experiments/016_mega_ensemble_external/submission.csv')
baseline_df['N'] = baseline_df['id'].str.split('_').str[0].astype(int)
for n, g in baseline_df.groupby('N'):
    xs = strip(g['x'].to_numpy())
    ys = strip(g['y'].to_numpy())
    ds = strip(g['deg'].to_numpy())
    # Check for NaN
    if np.isnan(xs).any() or np.isnan(ys).any() or np.isnan(ds).any():
        continue
    sc = score_group(xs, ys, ds, tx, ty)
    best[n] = {"score": float(sc), "data": g.drop(columns=['N']).copy(), "src": "baseline"}

# Scan all files for better solutions
for fp in all_files:
    try:
        df = pd.read_csv(fp)
    except:
        continue
    if not {'id', 'x', 'y', 'deg'}.issubset(df.columns):
        continue
    df['N'] = df['id'].str.split('_').str[0].astype(int)
    for n, g in df.groupby('N'):
        if n < 1 or n > 200:
            continue
        xs = strip(g['x'].to_numpy())
        ys = strip(g['y'].to_numpy())
        ds = strip(g['deg'].to_numpy())
        # Check for NaN
        if np.isnan(xs).any() or np.isnan(ys).any() or np.isnan(ds).any():
            continue
        sc = score_group(xs, ys, ds, tx, ty)
        improvement = best[n]['score'] - sc
        if improvement >= MIN_IMPROVEMENT:
            best[n] = {"score": float(sc), "data": g.drop(columns=['N']).copy(), "src": fp}

# Build final submission
rows = []
for n in range(1, 201):
    if best[n]['data'] is not None:
        rows.append(best[n]['data'])

out = pd.concat(rows, ignore_index=True)
out['sn'] = out['id'].str.split('_').str[0].astype(int)
out['si'] = out['id'].str.split('_').str[1].astype(int)
out = out.sort_values(['sn', 'si']).drop(columns=['sn', 'si'])
out = out[['id', 'x', 'y', 'deg']]
out.to_csv('submission.csv', index=False)

# Calculate total score
total = sum(best[n]['score'] for n in range(1, 201))
print(f"Total score: {total:.6f}")
```

### Step 4: Validate and Submit
- Validate NO overlaps using integer arithmetic (SCALE=10^18)
- Submit to Kaggle
- Track which N values improved and from which source

## Expected Outcome
- With 10+ new external sources: Could find improvements for 10-20 N values
- Each improvement averaging 0.05-0.10 could close the gap significantly
- Target of 68.878 IS reachable with comprehensive external data

## What NOT to Try
- ❌ More Python optimization algorithms (SA, GA, NFP, etc.)
- ❌ Variations on the same optimizer with different parameters
- ❌ Any approach that doesn't involve NEW DATA or NEW C++ OPTIMIZER

## SUBMIT THIS EXPERIMENT
YES - we need LB feedback on the comprehensive ensemble.
Even if CV doesn't improve much, LB might be different.