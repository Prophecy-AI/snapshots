## Current Status
- Best CV score: 70.365091 from exp_010 (safe_ensemble)
- Best LB score: 70.365091 (exp_010 - PASSED Kaggle validation)
- Target: 68.878195 | Gap to target: 1.49 points (2.11%)

## ⚠️ CRITICAL: exp_013 FAILED - DO NOT REPEAT THE MISTAKE

**exp_013 (selective_threshold) failed with "Overlapping trees in group 089"**

Root cause: Used lower threshold (0.0001) for "safe" N values, but N=89 had a small improvement (0.000330) that passed local validation but failed Kaggle's stricter validation.

**LESSON LEARNED:** Kaggle's validation is STRICTER than our local validation. Any improvement < 0.001 is RISKY and may cause overlap failures.

## Submission Log
| Exp | Approach | CV | LB | Notes |
|-----|----------|----|----|-------|
| 001 | valid_baseline | 70.615 | 70.615 | Baseline from snapshots |
| 002 | backward_propagation | 70.615 | 70.615 | No improvement |
| 007 | ensemble_fractional | 70.266 | pending | N=24 contributed 0.348 improvement |
| 010 | safe_ensemble | 70.365 | 70.365 | ✅ PASSED - MIN_IMPROVEMENT=0.001 |
| 013 | selective_threshold | 70.342 | FAILED | ❌ Overlap in N=89 |

## What We've Learned
1. **Kaggle validation is stricter** - improvements < 0.001 are risky
2. **exp_010 is our best valid submission** - 70.365091
3. **The baseline is at a strong local optimum** - SA, exhaustive search, NFP all failed to improve
4. **External data (santa-2025.csv) has better solutions** - but small improvements cause overlap failures
5. **C++ optimizer found only 0.00003 improvement** - the baseline is truly optimal for local search

## ⛔ FORBIDDEN (WILL BE REJECTED)
- Running bbox3, sa_fast, eazy_optimizer, tree_packer binaries
- subprocess.run() or os.system() to run binaries
- Selective thresholds < 0.001 (causes overlap failures)
- Any approach that gave < 0.01 improvement in previous experiments

## ✅ NEXT EXPERIMENT: Try Larger Threshold Ensemble

**IMMEDIATE ACTION:** Create exp_014 that:
1. Starts from exp_010 (our last valid submission)
2. Uses MIN_IMPROVEMENT=0.005 (very conservative)
3. Only keeps improvements that are LARGE and clearly valid
4. Validates ALL N values with strict overlap checking
5. Submits to get LB feedback

The goal is to capture ONLY the large improvements from external data that are safe.

## ✅ REQUIRED: Per-N Tracking with Safe Threshold

```python
MIN_IMPROVEMENT = 0.005  # Very conservative - only keep large improvements

for n in range(1, 201):
    my_score = calculate_score(my_config[n])
    baseline_score = calculate_score(exp010_config[n])
    improvement = baseline_score - my_score
    
    # ONLY keep if improvement >= 0.005 (very safe threshold)
    if improvement >= MIN_IMPROVEMENT:
        # Validate no overlaps
        if validate_no_overlap_strict(my_config[n]):
            print(f"✅ N={n}: IMPROVED by {improvement:.6f}")
            best_per_n[n] = my_config[n]
        else:
            best_per_n[n] = exp010_config[n]
    else:
        best_per_n[n] = exp010_config[n]
```

## Key Insight: The Gap Analysis

- Current: 70.365091
- Target: 68.878195
- Gap: 1.486896 (2.11%)

**The problem:** Small improvements (< 0.001) cause overlap failures on Kaggle.
**The solution:** Find LARGE improvements (> 0.005) that are clearly valid.

## Alternative Approaches to Try After This

If the conservative ensemble doesn't help, try:

### Option 1: Download More External Data
The jonathanchan kernel uses 15+ external data sources. Download:
- https://www.kaggle.com/datasets/jazivxt/bucket-of-chump
- https://www.kaggle.com/datasets/asalhi/telegram-public-shared-solution-for-santa-2025

### Option 2: Multi-Restart Random Generation
Generate NEW solutions from scratch instead of optimizing baseline:
```python
for n in range(1, 201):
    for restart in range(100):
        config = generate_random_valid_config(n)
        if calculate_score(config) < best_score:
            best_config = config
```

### Option 3: Tessellation Patterns for Large N
For N > 50, try tessellation-based placement (hexagonal, square grid, diagonal).

## SUBMIT EVERY EXPERIMENT
We have 96 submissions remaining. Submit after EVERY experiment to get LB feedback.