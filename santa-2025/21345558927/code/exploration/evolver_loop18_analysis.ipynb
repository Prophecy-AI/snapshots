{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf0a4bc",
   "metadata": {},
   "source": [
    "# Loop 18 Strategic Analysis\n",
    "\n",
    "## Current Situation\n",
    "- **Best LB**: 70.3535 (exp_016)\n",
    "- **Target**: 68.877877\n",
    "- **Gap**: 1.48 points (2.1%)\n",
    "\n",
    "## Key Insight from Evaluator\n",
    "exp_017 confirmed we've EXHAUSTED the ensemble approach with current data:\n",
    "- 0 improvements found over exp_016\n",
    "- 17,543 improvements rejected as too small (< 0.001)\n",
    "- 2,410 rejected for overlaps\n",
    "\n",
    "## What Top Kernels Do Differently\n",
    "1. **3-hour bbox3 runs** with n=1000-2000, r=30-90\n",
    "2. **fix_direction()** - rotates entire configuration to minimize bbox\n",
    "3. **17+ external data sources** (we have ~10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze what we're missing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Count our external sources\n",
    "external_files = glob.glob('/home/code/external_data/**/*.csv', recursive=True)\n",
    "print(f\"External CSV files: {len(external_files)}\")\n",
    "for f in external_files:\n",
    "    print(f\"  {f.replace('/home/code/external_data/', '')}\")\n",
    "\n",
    "# Count unique sources (dedupe by content hash)\n",
    "import hashlib\n",
    "unique_hashes = set()\n",
    "for f in external_files:\n",
    "    try:\n",
    "        with open(f, 'rb') as file:\n",
    "            h = hashlib.md5(file.read()).hexdigest()\n",
    "            unique_hashes.add(h)\n",
    "    except:\n",
    "        pass\n",
    "print(f\"\\nUnique external sources: {len(unique_hashes)}\")\n",
    "print(f\"Top kernels use: 17-19 sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2296ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-N scores to find where we're weakest\n",
    "import sys\n",
    "sys.path.insert(0, '/home/code')\n",
    "from code.tree_geometry import calculate_score\n",
    "from code.utils import parse_submission\n",
    "\n",
    "# Load our best submission\n",
    "df = pd.read_csv('/home/code/experiments/016_mega_ensemble_external/submission.csv')\n",
    "configs = parse_submission(df)\n",
    "\n",
    "# Calculate per-N scores\n",
    "per_n_scores = {}\n",
    "for n in range(1, 201):\n",
    "    per_n_scores[n] = calculate_score(configs[n])\n",
    "\n",
    "# Find top contributors to total score\n",
    "sorted_by_score = sorted(per_n_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 20 N values by score contribution:\")\n",
    "for n, score in sorted_by_score[:20]:\n",
    "    print(f\"  N={n}: {score:.6f}\")\n",
    "\n",
    "print(f\"\\nTotal score: {sum(per_n_scores.values()):.6f}\")\n",
    "print(f\"Top 20 contribute: {sum(s for n,s in sorted_by_score[:20]):.6f}\")\n",
    "print(f\"Percentage: {sum(s for n,s in sorted_by_score[:20])/sum(per_n_scores.values())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the 17,543 rejected improvements\n",
    "# These are improvements < 0.001 that we can't safely use\n",
    "# But what if we could use them for SOME N values?\n",
    "\n",
    "# Let's see which N values have NEVER caused Kaggle failures\n",
    "# From session_state: failures were in N=2, 89, 123\n",
    "failed_n_values = {2, 89, 123}  # From exp_000, exp_009, exp_013\n",
    "\n",
    "print(\"N values that have caused Kaggle failures:\")\n",
    "print(failed_n_values)\n",
    "\n",
    "print(\"\\nN values that have NEVER failed (potential safe zones):\")\n",
    "safe_n = set(range(1, 201)) - failed_n_values\n",
    "print(f\"Count: {len(safe_n)} out of 200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: fix_direction() can improve scores by rotating the ENTIRE configuration\n",
    "# This is different from rotating individual trees\n",
    "# Let's implement this!\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def calculate_bbox_side_at_angle(angle_deg, points):\n",
    "    \"\"\"Calculate bbox side length when configuration is rotated by angle_deg\"\"\"\n",
    "    angle_rad = np.radians(angle_deg)\n",
    "    c, s = np.cos(angle_rad), np.sin(angle_rad)\n",
    "    rot_matrix_T = np.array([[c, s], [-s, c]])\n",
    "    rotated_points = points.dot(rot_matrix_T)\n",
    "    min_xy = np.min(rotated_points, axis=0)\n",
    "    max_xy = np.max(rotated_points, axis=0)\n",
    "    return max(max_xy[0] - min_xy[0], max_xy[1] - min_xy[1])\n",
    "\n",
    "def optimize_rotation(trees):\n",
    "    \"\"\"Find optimal rotation angle for entire configuration\"\"\"\n",
    "    from code.tree_geometry import get_tree_vertices_numba\n",
    "    \n",
    "    # Get all vertices\n",
    "    all_points = []\n",
    "    for x, y, angle in trees:\n",
    "        rx, ry = get_tree_vertices_numba(x, y, angle)\n",
    "        for xi, yi in zip(rx, ry):\n",
    "            all_points.append([xi, yi])\n",
    "    points_np = np.array(all_points)\n",
    "    \n",
    "    # Use convex hull for efficiency\n",
    "    hull_points = points_np[ConvexHull(points_np).vertices]\n",
    "    \n",
    "    # Find optimal rotation\n",
    "    initial_side = calculate_bbox_side_at_angle(0, hull_points)\n",
    "    \n",
    "    res = minimize_scalar(\n",
    "        lambda a: calculate_bbox_side_at_angle(a, hull_points),\n",
    "        bounds=(0.001, 89.999),\n",
    "        method=\"bounded\",\n",
    "    )\n",
    "    \n",
    "    found_angle = float(res.x)\n",
    "    found_side = float(res.fun)\n",
    "    \n",
    "    improvement = initial_side - found_side\n",
    "    return initial_side, found_side, found_angle, improvement\n",
    "\n",
    "# Test on a few N values\n",
    "print(\"Testing fix_direction optimization:\")\n",
    "for n in [10, 50, 100, 150, 200]:\n",
    "    trees = configs[n]\n",
    "    initial, optimized, angle, improvement = optimize_rotation(trees)\n",
    "    if improvement > 0.0001:\n",
    "        print(f\"  N={n}: {initial:.6f} -> {optimized:.6f} (angle={angle:.2f}Â°, improvement={improvement:.6f})\")\n",
    "    else:\n",
    "        print(f\"  N={n}: No improvement from rotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c11fa",
   "metadata": {},
   "source": [
    "## Strategic Options\n",
    "\n",
    "### Option 1: Run bbox3 for 3 hours (like top kernels)\n",
    "- Use phased approach: Phase A (2min), Phase B (10min), Phase C (20min)\n",
    "- Parameters: n=1000-2000, r=30-90\n",
    "- Expected gain: Unknown, but top kernels use this\n",
    "\n",
    "### Option 2: Implement fix_direction for all N\n",
    "- Rotate entire configuration to minimize bbox\n",
    "- This is a POST-PROCESSING step that can improve any solution\n",
    "- Expected gain: 0.01-0.1 points\n",
    "\n",
    "### Option 3: Lower threshold for \"safe\" N values\n",
    "- Only N=2, 89, 123 have failed Kaggle\n",
    "- Could use threshold=0.0001 for other N values\n",
    "- Risk: Other N values might fail too\n",
    "\n",
    "### Option 4: Generate NEW solutions from scratch\n",
    "- Current solutions are all from same sources\n",
    "- Need fundamentally different configurations\n",
    "- Requires implementing new algorithms (NFP, genetic, etc.)\n",
    "\n",
    "## Recommendation\n",
    "**Option 2 (fix_direction) is the safest and most promising.**\n",
    "- It's a pure post-processing step\n",
    "- Top kernels use it\n",
    "- Can be applied to our existing best solution\n",
    "- No risk of overlap failures (doesn't change relative positions)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
