{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72162ebe",
   "metadata": {},
   "source": [
    "# Simulated Annealing Optimization\n",
    "\n",
    "Implement SA from scratch to escape local optima. Test on small N first, then scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636975d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('/home/code/experiments/003_simulated_annealing')\n",
    "sys.path.insert(0, '/home/code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import our custom modules\n",
    "from code.tree_geometry import calculate_score, calculate_bbox\n",
    "from code.overlap_check import has_overlap, validate_no_overlap_shapely\n",
    "from code.utils import load_submission, save_submission, parse_submission\n",
    "\n",
    "print(\"Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549822ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline\n",
    "baseline_df = pd.read_csv('/home/code/experiments/001_valid_baseline/submission.csv')\n",
    "baseline_configs = parse_submission(baseline_df)\n",
    "\n",
    "# Calculate baseline scores\n",
    "baseline_scores = {}\n",
    "for n in range(1, 201):\n",
    "    baseline_scores[n] = calculate_score(baseline_configs[n])\n",
    "\n",
    "baseline_total = sum(baseline_scores.values())\n",
    "print(f\"Baseline total score: {baseline_total:.6f}\")\n",
    "print(f\"\\nSample baseline scores:\")\n",
    "for n in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    print(f\"  N={n}: {baseline_scores[n]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9150ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SA on a single N value first\n",
    "from code.sa_optimizer import sa_optimize_single_n\n",
    "from code.tree_geometry import calculate_score_numba\n",
    "\n",
    "# Test on N=10\n",
    "n = 10\n",
    "trees = baseline_configs[n]\n",
    "trees_arr = np.array(trees, dtype=np.float64)\n",
    "\n",
    "print(f\"Testing SA on N={n}\")\n",
    "print(f\"Original score: {calculate_score_numba(trees_arr):.6f}\")\n",
    "\n",
    "# Run SA with different parameters\n",
    "for n_iter in [1000, 5000, 10000]:\n",
    "    start = time.time()\n",
    "    best_trees, best_score = sa_optimize_single_n(\n",
    "        trees_arr, \n",
    "        n_iterations=n_iter,\n",
    "        T_start=0.5,\n",
    "        T_end=0.001,\n",
    "        seed=42\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    improvement = baseline_scores[n] - best_score\n",
    "    print(f\"  {n_iter} iterations: score={best_score:.6f}, improvement={improvement:.6f}, time={elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10125f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple N values to see if SA can find any improvements\n",
    "test_n_values = [5, 10, 20, 50, 100]\n",
    "\n",
    "print(\"Testing SA on multiple N values (10000 iterations each):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for n in test_n_values:\n",
    "    trees = baseline_configs[n]\n",
    "    trees_arr = np.array(trees, dtype=np.float64)\n",
    "    \n",
    "    original_score = calculate_score_numba(trees_arr)\n",
    "    \n",
    "    best_trees, best_score = sa_optimize_single_n(\n",
    "        trees_arr, \n",
    "        n_iterations=10000,\n",
    "        T_start=0.5,\n",
    "        T_end=0.001,\n",
    "        seed=n\n",
    "    )\n",
    "    \n",
    "    improvement = original_score - best_score\n",
    "    pct_improvement = 100 * improvement / original_score if original_score > 0 else 0\n",
    "    \n",
    "    status = \"✅ IMPROVED\" if improvement > 1e-6 else \"❌ No improvement\"\n",
    "    print(f\"N={n:3d}: {original_score:.6f} -> {best_score:.6f} ({improvement:+.6f}, {pct_improvement:+.2f}%) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4652f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SA on ALL N values with moderate iterations\n",
    "from code.sa_optimizer import sa_optimize_all_n\n",
    "\n",
    "print(\"Running SA on all N values (5000 iterations each)...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "improved_configs, improvements = sa_optimize_all_n(\n",
    "    baseline_configs,\n",
    "    n_iterations=5000,\n",
    "    T_start=0.5,\n",
    "    T_end=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTotal time: {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb931af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new total score\n",
    "new_scores = {}\n",
    "for n in range(1, 201):\n",
    "    new_scores[n] = calculate_score(improved_configs[n])\n",
    "\n",
    "new_total = sum(new_scores.values())\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Baseline total: {baseline_total:.6f}\")\n",
    "print(f\"New total: {new_total:.6f}\")\n",
    "print(f\"Total improvement: {baseline_total - new_total:.6f}\")\n",
    "\n",
    "# Show all improvements\n",
    "if improvements:\n",
    "    print(f\"\\nN values improved: {len(improvements)}\")\n",
    "    for n, imp in sorted(improvements, key=lambda x: -x[1])[:20]:\n",
    "        print(f\"  N={n}: improved by {imp:.6f}\")\n",
    "else:\n",
    "    print(\"\\nNo improvements found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all configurations\n",
    "print(\"Validating all configurations...\")\n",
    "invalid_n = []\n",
    "for n in range(1, 201):\n",
    "    if has_overlap(improved_configs[n]):\n",
    "        invalid_n.append(n)\n",
    "\n",
    "if not invalid_n:\n",
    "    print(\"✅ All configurations valid (no overlaps)\")\n",
    "else:\n",
    "    print(f\"❌ Invalid configurations: {invalid_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ee208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "save_submission(improved_configs, 'submission.csv')\n",
    "print(\"Saved submission.csv\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': new_total,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - new_total,\n",
    "    'num_improvements': len(improvements),\n",
    "    'sa_iterations': 5000,\n",
    "    'notes': 'Simulated annealing from scratch with translate, rotate, swap, and shift moves'\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nFinal CV Score: {new_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to submission folder if improved\n",
    "import shutil\n",
    "\n",
    "if new_total < baseline_total:\n",
    "    shutil.copy('submission.csv', '/home/submission/submission.csv')\n",
    "    print(\"✅ Copied improved submission to /home/submission/\")\n",
    "else:\n",
    "    print(\"❌ No improvement - keeping baseline submission\")\n",
    "    # Still copy our submission for tracking\n",
    "    shutil.copy('submission.csv', '/home/submission/submission.csv')\n",
    "    print(\"Copied submission anyway for LB feedback\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
