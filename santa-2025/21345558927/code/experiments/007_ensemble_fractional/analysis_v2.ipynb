{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152cdad5",
   "metadata": {},
   "source": [
    "# Ensemble + Fractional Translation (v2)\n",
    "\n",
    "Key fix: Validate that solutions don't have overlaps before accepting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('/home/code/experiments/007_ensemble_fractional')\n",
    "sys.path.insert(0, '/home/code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "from numba import njit\n",
    "\n",
    "from code.tree_geometry import TX, TY, calculate_score, calculate_bbox\n",
    "from code.overlap_check import has_overlap\n",
    "from code.utils import parse_submission, save_submission\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020af9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(a):\n",
    "    \"\"\"Remove 's' prefix from values.\"\"\"\n",
    "    return np.array([float(str(v).replace('s', '')) for v in a], np.float64)\n",
    "\n",
    "def load_and_validate_csv(fp):\n",
    "    \"\"\"Load a submission CSV and validate each N group.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        if not {'id', 'x', 'y', 'deg'}.issubset(df.columns):\n",
    "            return None\n",
    "        df['N'] = df['id'].astype(str).str.split('_').str[0].astype(int)\n",
    "        return df\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f324223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all CSV files from snapshots\n",
    "snapshot_dir = '/home/nonroot/snapshots/santa-2025'\n",
    "csv_files = []\n",
    "for root, dirs, files in os.walk(snapshot_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.csv'):\n",
    "            csv_files.append(os.path.join(root, f))\n",
    "\n",
    "# Also add our baseline (which we know is valid)\n",
    "csv_files.append('/home/code/experiments/001_valid_baseline/submission.csv')\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files to ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ensemble - best per-N from all sources WITH VALIDATION\n",
    "best_per_n = {n: {'score': 1e300, 'config': None, 'src': None} for n in range(1, 201)}\n",
    "\n",
    "print(\"Processing CSV files with overlap validation...\")\n",
    "processed = 0\n",
    "valid_found = 0\n",
    "\n",
    "for fp in csv_files:\n",
    "    df = load_and_validate_csv(fp)\n",
    "    if df is None:\n",
    "        continue\n",
    "    \n",
    "    for n, g in df.groupby('N'):\n",
    "        if n < 1 or n > 200:\n",
    "            continue\n",
    "        \n",
    "        xs = strip(g['x'].to_numpy())\n",
    "        ys = strip(g['y'].to_numpy())\n",
    "        ds = strip(g['deg'].to_numpy())\n",
    "        \n",
    "        # Create config as list of tuples\n",
    "        config = [(xs[i], ys[i], ds[i]) for i in range(len(xs))]\n",
    "        \n",
    "        # Validate - no overlaps\n",
    "        if has_overlap(config):\n",
    "            continue\n",
    "        \n",
    "        # Calculate score\n",
    "        sc = calculate_score(config)\n",
    "        valid_found += 1\n",
    "        \n",
    "        if sc < best_per_n[n]['score']:\n",
    "            best_per_n[n]['score'] = sc\n",
    "            best_per_n[n]['config'] = config\n",
    "            best_per_n[n]['src'] = fp.split('/')[-2]\n",
    "    \n",
    "    processed += 1\n",
    "    if processed % 500 == 0:\n",
    "        print(f\"  Processed {processed} files, found {valid_found} valid configs...\")\n",
    "\n",
    "print(f\"\\nProcessed {processed} files total\")\n",
    "print(f\"Found {valid_found} valid configurations\")\n",
    "\n",
    "# Calculate ensemble score\n",
    "ensemble_score = sum(best_per_n[n]['score'] for n in range(1, 201))\n",
    "print(f\"\\nEnsemble score: {ensemble_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc449ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample scores\n",
    "print(\"\\nSample per-N scores:\")\n",
    "for n in [1, 2, 5, 10, 20, 50, 100, 150, 200]:\n",
    "    entry = best_per_n[n]\n",
    "    print(f\"  N={n}: {entry['score']:.6f} (from {entry['src']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eac4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline for comparison\n",
    "baseline_df = pd.read_csv('/home/code/experiments/001_valid_baseline/submission.csv')\n",
    "baseline_configs = parse_submission(baseline_df)\n",
    "\n",
    "baseline_scores = {}\n",
    "for n in range(1, 201):\n",
    "    baseline_scores[n] = calculate_score(baseline_configs[n])\n",
    "\n",
    "baseline_total = sum(baseline_scores.values())\n",
    "print(f\"Baseline total: {baseline_total:.6f}\")\n",
    "print(f\"Ensemble total: {ensemble_score:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - ensemble_score:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find improvements over baseline\n",
    "improvements = []\n",
    "for n in range(1, 201):\n",
    "    ens_score = best_per_n[n]['score']\n",
    "    base_score = baseline_scores[n]\n",
    "    if ens_score < base_score - 1e-9:\n",
    "        diff = base_score - ens_score\n",
    "        improvements.append((n, diff, best_per_n[n]['src']))\n",
    "\n",
    "print(f\"\\nFound {len(improvements)} improvements over baseline:\")\n",
    "for n, diff, src in improvements[:20]:\n",
    "    print(f\"  N={n}: improved by {diff:.9f} (from {src})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629112e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final configs - use ensemble where better, baseline otherwise\n",
    "final_configs = {}\n",
    "for n in range(1, 201):\n",
    "    if best_per_n[n]['score'] < baseline_scores[n] - 1e-9:\n",
    "        final_configs[n] = best_per_n[n]['config']\n",
    "    else:\n",
    "        final_configs[n] = list(baseline_configs[n])\n",
    "\n",
    "# Calculate final score\n",
    "final_score = sum(calculate_score(final_configs[n]) for n in range(1, 201))\n",
    "print(f\"\\nFinal score: {final_score:.6f}\")\n",
    "print(f\"Baseline: {baseline_total:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - final_score:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "save_submission(final_configs, 'submission.csv')\n",
    "print(\"Saved submission.csv\")\n",
    "\n",
    "# Copy to submission folder\n",
    "import shutil\n",
    "shutil.copy('submission.csv', '/home/submission/submission.csv')\n",
    "print(\"Copied to /home/submission/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb16c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': final_score,\n",
    "    'baseline_score': baseline_total,\n",
    "    'ensemble_score': ensemble_score,\n",
    "    'improvement': baseline_total - final_score,\n",
    "    'n_improvements': len(improvements),\n",
    "    'improvements': [(n, float(d), src) for n, d, src in improvements[:20]],\n",
    "    'notes': 'Ensemble from all snapshots with overlap validation'\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nFinal CV Score: {final_score:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
