{
  "query": "How do Kaggle optimization competition winners achieve scores better than public kernels?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the NVIDIA Grandmasters Playbook, DataCamp\u2019s competition course, and practical tips from Kaggle veterans, winners of optimization competitions typically out\u2011perform the public kernels by following a disciplined, data\u2011driven workflow\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data); [campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12); [learnbymarketing.com](https://learnbymarketing.com/848/what-tools-do-kaggle-winners-use); [yanirseroussi.com](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions)):\n\n1. **Build a trustworthy validation scheme** \u2013 create custom folds that reflect the private test distribution (e.g., stratified, time\u2011based, or leakage\u2011aware splits) and keep the same folds for every experiment. This guards against over\u2011fitting to the public leaderboard\u202f([learnbymarketing.com](https://learnbymarketing.com/848/what-tools-do-kaggle-winners-use)).\n\n2. **Perform deep exploratory analysis and leak detection** \u2013 scrutinize the data for hidden patterns, target leakage, or unrealistic correlations, and engineer features that capture genuine signal rather than artifacts\u202f([learnbymarketing.com](https://learnbymarketing.com/848/what-tools-do-kaggle-winners-use)).\n\n3. **Use state\u2011of\u2011the\u2011art models and GPU acceleration** \u2013 train high\u2011performance algorithms such as XGBoost, LightGBM, CatBoost, or deep\u2011learning nets (ResNet, EfficientNet, etc.) on GPUs when appropriate, as recommended by the Grandmasters Playbook\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\n4. **Log every run and save intermediate artifacts** \u2013 persist fold assignments, model checkpoints, validation scores, and both validation and test predictions. Version each experiment (e.g., via Git commits) so you can reproduce or revert later\u202f([campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12)).\n\n5. **Build ensembles from diverse models** \u2013 blend or stack saved predictions (e.g., weighted averages, meta\u2011learners) to reduce variance and capture complementary strengths. Because predictions are already stored, ensembling can be done quickly near the competition\u2019s end\u202f([campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12)).\n\n6. **Optimize directly for the competition metric** \u2013 understand the evaluation measure (MAE, AUC, LogLoss, etc.) and, if possible, use a custom loss or post\u2011processing step that aligns model training with that metric\u202f([yanirseroussi.com](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions)).\n\n7. **Leverage community kernels but go beyond them** \u2013 study public notebooks for ideas, then modify architectures, hyper\u2011parameters, or feature sets to create a distinct solution that the public kernels don\u2019t capture\u202f([campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12)).\n\nFollowing these steps lets top competitors systematically improve over the baseline public kernels and achieve higher private\u2011leaderboard scores.",
      "url": ""
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Final tips | Python",
      "text": "Final tips | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12\nFinal tips | Python\nNone\n2022-06-13T00:00:00Z\n# Final tips\n####. Final tips\nAll right, we're almost done. In this lesson, we'll just cover some tips that haven't been mentioned throughout the course.\n####. Save information\nThe first tip is saving all the information we can.\nTo begin, save folds distribution to files. Our goal is to track the validation score during the competition. And of course, this validation score should always be calculated on the same folds.\nAnother data that we'd like to save is model runs. It will allow us to reproduce our experiments or go back if needed. One of the possibilities could be to create a separate git commit for each model run or submission.\nIt is also a good idea to save model predictions as well. If we start saving validation and test predictions from the very beginning of the competition, it will allow us to simply build model ensembles near the end. Because we store predictions for the models blending as well as features for the models stacking.\nFinally, we should keep a log of models' results to track the performance progress. It could be done as comments to the git commits or as notes in a separate document.\n####. Kaggle forum and kernels\nNow let's speak about the Kaggle forum and kernels. It's one of the strongest sources of knowledge on Kaggle.\n####. Kaggle forum and kernels\nEach competition has an open forum where all the participants can start topics sharing their thoughts and ideas, asking questions and so on.\n####. Kaggle forum and kernels\nKaggle kernels is another source of knowledge. It represents scripts and notebooks that participants are sharing during the competition. So, we have an opportunity not only to discuss the competition, but also to look at the code.\nMoreover, kernels represent a computational environment where we have access to an interactive session running in a Docker container with pre-installed packages, the ability to mount data sources, use GPU resources, and more.\n####. Forum and kernels usage\nForum and kernels could bring us lots of benefits during the different competition stages.\nSuppose we decided to join some of the current Kaggle competitions. First of all, it is useful to find similar past competitions on Kaggle. Usually, top teams are sharing their approach on the forum once the competition has finished. It allows us to read through the best performing solutions and get to know what could work for the similar problem types.\nDuring the rest of the competition, we should precisely follow all the topics in the forum and the most popular kernels. It allows us to be up-to-date during the competition and learn lots of new ideas and approaches.\nFinally, even after the end of the competition, it's time to learn from the top participants. Usually, winners share their solutions a couple of days after the competition finish. It's very valuable information that we should utilize to determine what we could have done better during the competition.\n####. Select final submissions\nThe last few words are devoted to the final submissions. Kaggle competitions have different durations, but generally, it's about 2 or 3 months. As we already know, every day we have a limited number of submissions to the Leaderboard.\nSo, if we have a 2-month competition with 5 submissions per day, we could make up to 300 submissions to the Public Leaderboard.\n####. Select final submissions\nHowever, for the final evaluation on the Private Leaderboard, we have to choose only 2 submissions. We mark them in the list of all submissions made.\n####. Select final submissions\nAnd only these are used for the final standings. Our result is the best score out of these two final submissions.\n####. Final submissions\nThe suggested strategy that works pretty well is to select one submission that is the best on the local validation,\nand another submission that is the best on the Public Leaderboard.\n####. Let's practice!\nLet's now review these final tips before saying good-bye!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12"
    },
    {
      "title": "",
      "text": "**Summary**: Kaggle competitors spend their time exploring the data, building training set samples to build their models on representative data, explore data leaks, and use tools like Python, R, XGBoost, and Multi-Level Models.\n\n![Past Kaggle Competitions](https://www.learnbymarketing.com/wp-content/uploads/2016/09/kaggle-competition-logos.png)\n\nI recently competed in my first Kaggle competition and definitely did not win. A clear lesson in humility for me. I look at the experience as an opportunity to learn about the serious world of applied data mining.\n\nThe Kaggle community is full of people who are willing to share their expertise. It\u2019s almost a tradition for users to share their solutions after the end of the competition. Even the top three winners! In aggregate, there are some useful patterns that the everyday analyst can use at work or in competition. I wrote a [more statistical analysis of tools used by Kaggle winners](https://www.learnbymarketing.com/950/winning-a-kaggle-competition-analysis/). I plan on periodically updating that post with new data.\n\n## Set up a good validation set\n\nKaggle compares your predicted results with a 30% sample of a test set. This lets you see of your model is improving on completely unseen data and is an important step that every analyst should take when developing a model.\n\nHowever, some of the top Kaggle competitors have called out that the test set \u201cleaderboard\u201d can be misleading and lead to overfitting (in fact one team [perfectly overfit the sample test set](https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission)).\n\nInstead of relying on many checks against that test set sample, the Kagglers will craft controlled samples that are more representative of the data set\u2019s attributes and not rely on simple random samples.\n\n## Deep Exploratory Analysis\n\n![Kaggler's spend a lot of time exploring their data.](https://www.learnbymarketing.com/wp-content/uploads/2016/05/dtree-r-vis01-tut.png)\n\nKaggle allows users to share \u201cKernels\u201d (i.e. code snippets / notebooks/ reports) that mix code and explanations in one document. You\u2019ll see Kernels that go deep on just a handful of variables in order to find just a few more points of predictive power.\n\nIt\u2019s this exploration that seems to lead to major wins in the beginning of a competition. Then comes feature engineering with a good understanding of the attributes you are working with.\n\nThis can also take another direction. One that the Kaggle competition organizers don\u2019t always intend\u2026\n\n## Data Leakage\n\n![Kaggle Data Leak Example](https://www.learnbymarketing.com/wp-content/uploads/2016/09/kaggle-data-leak.png)\n\nThe purpose of a Kaggle competition is to derive some new model for the sponsor. However, whenever there is a cash prize involved, someone is always looking to game the system.\n\nA data leak can include:\n\n- The order of the training and test set.\n- Attributes of the ID columns.\n\nAs part of the exploration, a Kaggle-competitor might spend time looking at the order of the training an testing sets, looking for data duplication, or building features on top of the different parts of the ID.\n\nThis still takes a level of skill but definitely detracts from the business goal of finding a new model that could be applied.\n\n## Where are the Tools?\n\nI see mainly **Python and some R** being used to create models for Kaggle competitions but there\u2019s an occasional deep neural network. Especially from the competitors who are in PhD programs at this time.\n\nThere are some consistent algorithms being used across competitions.\n\n### XGBOOST\n\n[eXtreme Gradient BOOSTing](http://xgboost.readthedocs.io/en/latest/model.html) is a defacto tool that top Kaggle competitors use in their models.\n\nIt is an ensemble technique that combines many [decision trees](https://www.learnbymarketing.com/methods/classification-and-regression-decision-trees-explained/) to build a more accurate model. The XGBoost algorithm is available in both Python and R.\n\n### Multi-Level Models\n\n[![Multi Level Model Example](https://www.learnbymarketing.com/wp-content/uploads/2016/09/multi-level-model-example.png)](https://www.learnbymarketing.com/wp-content/uploads/2016/09/multi-level-model-example.png)\n\nAnother very common modeling method is to train many models(dozens) and then use the predictions as input to another model.\n\nThink about it in terms of a neural network. You have your input layer, an activation layer, and an output layer. You might extend that to have a second activation layer that continues to find new combinations.\n\nThe benefit to these sorts of models is that you can use many different models and blend their strengths and weaknesses.\n\nFrom several of the winners, I have seen XGBoost used both as an input and as an activation / combination layer model.\n\n## Further Reading\n\nIf you\u2019d like to read the results for yourself, here is a small collection of solution discussions.\n\n- [Product Classification](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov)\n- [Online Product Sales](https://www.kaggle.com/c/online-sales/forums/t/2135/congrats-to-the-winners)\n- [Expedia Hotel Recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)\n- [Restaurant Revenue Prediction](https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/14060/please-share-your-codes)\n- [Store Sales](https://www.kaggle.com/c/rossmann-store-sales/forums/t/18024/model-documentation-1st-place)\n- [Search Relevance](https://www.kaggle.com/c/home-depot-product-search-relevance/forums/t/20427/congrats-to-the-winners)\n- [Coupon Purchase Prediction](https://www.kaggle.com/c/coupon-purchase-prediction/forums/t/16736/what-s-your-guess-about-private-lb)\n- [Marketing Response](https://www.kaggle.com/c/springleaf-marketing-response/forums/t/17081/solution-sharing)\n\n* * *",
      "url": "https://learnbymarketing.com/848/what-tools-do-kaggle-winners-use"
    },
    {
      "title": "How to (almost) win Kaggle competitions",
      "text": "Last week, I gave a talk at the [Data Science Sydney Meetup group](http://www.meetup.com/Data-Science-Sydney/) about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are [here](http://yanirs.github.io/talks/data-science-sydney-winning-kaggle/)). Some of these tips were covered in my [beginner tips post](https://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/) from a few months ago. Similar advice was also [recently published on the Kaggle blog](https://blog.kaggle.com/2014/08/01/learning-from-the-best/) \u2013 it\u2019s great to see that my tips are in line with the thoughts of other prolific kagglers.\n\n### Tip 1: RTFM [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-1-rtfm)\n\nIt\u2019s surprising to see how many people miss out on important details, such as remembering the final date to make the first submission. Before jumping into building models, it\u2019s important to understand the competition timeline, be able to reproduce benchmarks, generate the correct submission format, etc.\n\n### Tip 2: Know your measure [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-2-know-your-measure)\n\nA key part of doing well in a competition is understanding how the measure works. It\u2019s often easy to obtain significant improvements in your score by using an optimisation approach that is suitable to the measure. A classic example is optimising the mean absolute error (MAE) versus the mean square error (MSE). It\u2019s easy to show that given no other data for a set of numbers, the predictor that minimises the MAE is the median, while the predictor that minimises the MSE is the mean. Indeed, in the [EMC Data Science Hackathon](https://www.kaggle.com/c/dsg-hackathon/forums/t/1821/general-approaches-to-partitioning-the-models/10631#post10631) we fell back to the median rather than the mean when there wasn\u2019t enough data, and that ended up working pretty well.\n\n### Tip 3: Know your data [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-3-know-your-data)\n\nIn Kaggle competitions, overspecialisation (without overfitting) is a good thing. This is unlike academic machine learning papers, where researchers often test their proposed method on many different datasets. This is also unlike more applied work, where you may care about data drifting and whether what you predict actually makes sense. Examples include the [Hackathon](https://www.kaggle.com/c/dsg-hackathon/forums/t/1821/general-approaches-to-partitioning-the-models/10631#post10631), where the measures of pollutants in the air were repeated for consecutive hours (i.e., they weren\u2019t really measured); the [multi-label Greek article competition](https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/), where I found connected components of labels (doesn\u2019t generalise well to other datasets); and the [Arabic writers competition](https://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/), where I used histogram kernels to deal with the features that we were given. The general lesson is that custom solutions win, and that\u2019s why the world needs data scientists (at least [until we are replaced by robots](http://www.datarobot.com/)).\n\n### Tip 4: What before how [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-4-what-before-how)\n\nIt\u2019s important to know _what_ you want to model before figuring out _how_ to model it. It seems like many beginners tend to worry too much about which tool to use (Python or R? Logistic regression or SVMs?), when they should be worrying about understanding the data and what useful patterns they want to capture. For example, when we worked on the [Yandex search personalisation competition](https://www.kaggle.com/c/yandex-personalized-web-search-challenge/forums/t/6811/share-your-approach/37306#post37306), we spent a lot of time looking at the data and thinking what makes sense for users to be doing. In that case it was easy to come up with ideas, because we all use search engines. But the main message is that to be effective, you have to become one with the data.\n\n### Tip 5: Do local validation [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-5-do-local-validation)\n\nThis is a point I covered in my [Kaggle beginner tips post](https://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/#validation). Having a local validation environment allows you to move faster and produce more reliable results than when relying on the leaderboard. The main scenarios when you should skip local validation is when the data is too small (a problem I had in the [Arabic writers competition](https://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/)), or when you run out of time (towards the end of the competition).\n\n### Tip 6: Make fewer submissions [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-6-make-fewer-submissions)\n\nIn addition to making you look good, making few submissions reduces the likelihood of overfitting the leaderboard, which is a real problem. If your local validation is set up well and is consistent with the leaderboard (which you need to test by making one or two submissions), there\u2019s really no need to make many submissions. Further, if you\u2019re doing well, making submissions erodes your competitive advantage by showing your competitors what scores are obtainable and motivating them to work harder. Just resist the urge to submit, unless you have a really good reason.\n\n### Tip 7: Do your research [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-7-do-your-research)\n\nFor any given problem, it\u2019s likely that there are people dedicating their lives to its solution. These people (often academics) have probably published papers, benchmarks and code, which you can learn from. Unlike actually winning, which is not only dependent on you, gaining deeper knowledge and understanding is the only sure reward of a competition. This has worked well for me, as I\u2019ve learned something new and applied it successfully in [nearly every competition I\u2019ve worked on](https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/).\n\n### Tip 8: Apply the basics rigorously [\\#](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/\\#tip-8-apply-the-basics-rigorously)\n\nWhile playing with obscure methods can be a lot of fun, it\u2019s often the case that the basics will get you very far. Common algorithms have good implementations in most major languages, so there\u2019s really no reason not to try them. However, note that when you do try any methods, you _must_ do some minimal tuning of the main parameters (e.g., number of trees in a random forest or the regularisation of a linear model). **Running a method without minimal tuning is worse than not running it at all**, because you may get a false negative \u2013 giving up on a method that actually works very well.\n\nAn example of applying the basics rigorously is in the classic paper [In defense of one-vs-all classification](http://jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf), where the authors showed that the simple one-vs-all (OVA) approach to multiclass classification is at least as good as approaches that are much more sophisticated. In their words: \u201cWhat we find is that although a wide array of more sophisticated methods for multiclass classification exist, experimental evidence of the superiority of these methods over a simple OVA scheme is either lacking or improperly controlled or measured\u201d. If such a failure to perform proper experiments can happen to serious machine learning researchers, it can definitely happen to the average kaggler. Don\u2019t let it happen to you.\n\n### Tip 9: The forum is your frien...",
      "url": "https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions"
    },
    {
      "title": "",
      "text": "# Public vs Private leaderboard\n\n[iframe](https://projector.datacamp.com/?auto_play=pause&projector_key=course_16839_c984e76fc4deaf87cb5a401e2bdc1693&video_hls=https%3A%2F%2Fdatacamp-projector-video-recorder-uploads-prod.s3.amazonaws.com%2Fmp4%2F2aa53c56-c2dc-4c15-b2d0-29657d386eb6.mp4%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Credential%3DASIAUMJDGTMH5JGXZGRT%252F20250430%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250430T043145Z%26X-Amz-Expires%3D1200%26X-Amz-Security-Token%3DIQoJb3JpZ2luX2VjEAQaCXVzLWVhc3QtMSJGMEQCIDHXZuQZFvmjagd%252F2aiCRvP%252FVaLTXBhPlii88OWP8a0hAiBmoRhBpIgTTNJx9MJAjOZoZGRCp7mOxkg1BHen%252F8nxsSqWBQid%252F%252F%252F%252F%252F%252F%252F%252F%252F%252F8BEAQaDDMwMTI1ODQxNDg2MyIMhkyThuV7JhSeni0pKuoE8%252B0BOfNC6eRJ8L%252FDep%252B07bdUpRCrWAE689C%252FqGxYm867bp%252FwpURGdgJOttJKPAvJHs%252FduuDgqgWlcAotRmVP5T37xrdVu%252FB838NjKUFD9YZJzl4Ynsvb%252BH3Vfh9xI8eUNH3S5EjMqAPLLgVw829GjbcBzzoX%252FgdVk2zRgR%252BD1Xe4X8F1Yu49%252BKf5FPKlHzAF%252FMZiotrXhB%252B3LPBzxaD4qE17L2KVPbJZhOr%252BWKOY2zKWEhhPuSzYWwS%252BbK0HYmwvUWANQKNGCz5o8GoFl91JyZ7ROzlWoIirZFWq%252Fh18m9vdsrkMY4CsS3H7YIyQp4gd%252FviGEnAE633w1uxZQR6YdSHT2i8KZ%252F07dsLHiWn7wrPMogdRqeLY3GZQbjqNOthBv8KnacE5lItOdwnubwmfH1dO3BWEwUWqgLz6%252FYfr1dPUmh52UzUlXrRetSiIx2iGl9aizadnwq5%252BDvXUxYP2Any7mb%252FZC%252Fxk9qyyNA5Nz53n2auF8GHszVvJgwjp2cNCMkQElQizND45y1xL2pflSayFVebzcY5%252BGd4HCGOBrdEMSvn9LnbVhZz0hI0L50IvkoP3i04ca0R%252FqJl3CmqD32xoGpzJMK8H5upU%252BuD5sHMmt24wyxFz%252FO9aheY3pXKLX%252B6jn%252B42DJq408gAJl9KQxwjIwyXEim0qU%252Bs9kkb5FvEreBh%252BxoEOjc7npvxR9VQY3zdIFMI34HpnKxj24OHzHNeZD0mFF0c6n4KtxjSyNzYwMO0ehPm27G1p79GMAXqXhtCKY4tVi6PVncFfjchA%252BK7CLBQZezoPQHREijrG6C8LLDFTLoPskyQMJq%252BxsAGOpwByoP%252B3B3Z3lYe0uBhCt0GaOaXmnEj3AdlW8OQVl9NrjksC1xp7anjtDpPFdn2wl%252Ft%252Birx%252BAzc5H2y2M%252FO%252FvVl6ATwWfiZ3bYEbjaCw%252FldGuzSU1xdnCg3d5e8oaxYy0LnvGzy5jcEfyJyFr8eKd0R0rdrXcAlJNL%252FsKul8eBWh5UX%252FXoRqDQc5lJqTvMnoP1We0Y%252BHX7chQzPaxa6%26X-Amz-Signature%3D54c5c2444b7b1d348aa04272719f43215b9d0e547d83fcb1a5a075c91504f9c9%26X-Amz-SignedHeaders%3Dhost)\n\n#### 1\\. Public vs Private leaderboard\n\nIn the previous lesson, we prepared and saved our first submission to a .csv file. Now, we will talk about how Kaggle processes the submissions.\n\n#### 2\\. Competition metric\n\nEach competition specifies a single metric that is used to rank the participants. The better metric value our model achieves, the better position we obtain. So, our goal in the competition is to build a model that optimizes the metric given.\nHere is a list of metrics most frequently used in the competitions and type of problems they appear in.\n\n#### 3\\. Test split\n\nWhile preparing the submission, we have to make predictions for all the observations in the test set. However, Kaggle internally splits test data into two subsets: Public and Private.\nThis split is unknown and it is the same for all the participants. During the competition, we could see the results and standings on the Public test data. The Private test data is only used to determine the final standings at the end of the competition.\n\n#### 4\\. Leaderboards\n\nIn the previous lesson we prepared the submission file and wrote it to the disk. Now, we can go to the competition website and upload our submission.\nUsually, competitions have a limit of about 5 submissions available per day.\nOnce we submit our file, Kaggle internally calculates the competition metric on the whole test set, but shows the result only on the Public part. So, we see the standings on so-called 'Public Leaderboard' (denoted as LB). On the other hand, Private Leaderboard score is unknown until the competition deadline.\nFor example, if we've submitted a file called submission\\_1.csv and the competition metric is Mean Squared Error, then we will know the result only on the Public Leaderboard.\n\n#### 5\\. Overfitting\n\nAs long as we could track the results only on the Public Leaderboard, we could potentially overfit to it.\nSo, what is overfitting? Suppose we're developing a Machine Learning model and measure the error rate on both train and test data. While increasing the model complexity, the train error generally goes down. It happens because the model learns the train data so well, that it performs great on it with very little error. However, test error at some point could go up.\n\n#### 6\\. Overfitting\n\nIt's exactly the starting point of the overfitting. From this moment the model finds some very specific dependencies in the train data (lowering its error), that are unable to generalize well (increasing the test error).\n\n#### 7\\. Overfitting\n\nThe same could happen with Public and Private Leaderboards. If we only look at the results on the Public Leaderboard, we could potentially overfit to it. Thus, our Private Leaderboard score will be considerably worse together with our final place in the competition.\nTo beat the overfitting in both real-life projects and competitions, we need to use a good validation strategy. We will talk about it in the next chapter.\n\n#### 8\\. Public vs Private leaderboard shake-up\n\nThe difference between Public and Private leaderboards standings is called a 'shake-up'. The size of the shake-up is highly different from one competition to another.\nOn the left image, we see a competition example with a small shake-up. The movements in Private Leaderboard were about 2-3 places up and down compared to the Public Leaderboard.\nWhile the image on the right represents a competition with a huge shake-up. The winner of the competition had only 1485th place on the Public Leaderboard.\n\n#### 9\\. Let's practice!\n\nNow you're aware of overfitting and the difference between Public and Private Leaderboards in Kaggle competitions. Let's explore the overfitting on practice!\n\nShow Transcripts",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/kaggle-competitions-process?ex=8"
    },
    {
      "title": "5 Simple Tips To Improve Your Kaggle Models",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# 5 Simple Tips To Improve Your Kaggle Models\n\nHow To Get High Performing Models In Competitions\n\n[Louise Ferbach](https://towardsdatascience.com/author/lsferbach/)\n\nOct 2, 2020\n\n8 min read\n\nShare\n\nPhoto by [\u00f0\u009f\u0087\u00a8\u00f0\u009f\u0087\u00ad Claudio Schwarz \\| @purzlbaum](https://u%5Bnsplash%5D(https://unsplash.com?utm_source=medium&utm_medium=referral).com/@purzlbaum?utm_source=medium&utm_medium=referral) on Unsplash\n\nIf you recently got started on Kaggle, or if you are an old regular of the platform, you probably wonder how to easily improve the performance of your model. Here are some practical tips I\u2019ve accumulated through [my Kaggle journey](https://www.kaggle.com/louise2001). So, either build your own model or just start from a baseline public kernel, and try implementing these suggestions !\n\n## 1\\. Always review past competitions\n\nAlthough Kaggle\u2019s policy is to never feature twice an identical competition, there are often remakes of very similar problems. For example, some hosts propose a regular challenge on the same theme yearly (NFL\u2019s Big Data Bowl for example), with only small variations, or in some fields (like medical imaging for example) there are a lot of competitions with different targets but very similar spirit.\n\nReviewing winners\u2019 solutions (always made public after competition ends thanks to the incredible Kaggle community) can therefore be a great plus, as it gives you ideas to get started, and a winning strategy. If you have time to review a lot of them, you will also soon find out that, even in very different competitions, some popular baseline models seem to always do the job well enough :\n\n- Convolutional Neural Networks or the more complex ResNet or EfficientNet in **computer vision challenges**,\n- WaveNet in **audio processing challenges** (that can also very well be treated by image recognition models, if you just use a Mel Spectrogram),\n- BERT and its derivatives (RoBERTa, etc) in **natural language processing challenges**,\n- Light Gradient Boosting Method (or other Gradient Boosting or trees strategies) on **tabular data**\u2026\n\nYou can either look for similar competitions on the Kaggle platform directly, or take a look at [this great summary by Sundalai Rajkumar](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions).\n\nReviewing past competitions can also help you get hints on all the other steps explained in the following. For example, getting tips and tricks on preprocessing for similar problems, how people choose their hyperparameters, what additional tools they have implemented in their models to have them win the game, or if they focused on bagging only similar versions of their best models or rather ensembled a melting pot of all available public kernels.\n\n## 2\\. You never spend enough time on data preparation\n\nThis is far from being the most thrilling part of the job. However, the importance of this step cannot be overemphasized.\n\n- **Clean the data** : never assume the hosts worked on providing you with the cleanest possible data. Most of the time, it is wrong. Fill NaNs, remove outliers, split the data into categories of homogeneous observations\u2026\n- Do some easy **exploratory data analysis**, to get an overview of what you\u2019re working on (this will help you get insights and ideas). **This is the most important step at this stage**. Without proper knowledge of how your data is structured, what information you have, what general behavior features tend to have individually or collectively with respect to the target, you will walk blind and have no intuition of how to build your model. Draw plots, histograms, correlation matrices.\n- **Augment your data** : this is probably one of the best things to improve performance. However be careful not to make it so huge that your model won\u2019t be able to process it anymore. You can either find some additional datasets on the Internet (be very careful about rights, or you could suffer the same fate as [the winners of the $1M Deepfake Detection Challenge](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/157983)), or on the Kaggle platform (in similar past competitions !), or just work on the data you\u2019re being provided : flip and crop images, overlay audio recordings, back-translate or replace synonyms in texts\u2026\n\nPreprocessing is also the step where you have to carefully think about what **cross-validation method** you will rely on. Kaggle\u2019s motto could basically be : _Trust Your CV_. Working on your data will help you know how to split it : stratify on target values or on sample categories ? Is your data unbalanced ? If you have a clever CV strategy, and rely solely on it and not on leaderboard score (though it may be very tempting), then you\u2019re very likely to get good surprises on private final scores.\n\n## 3\\. Try hyperparameter searching\n\nHyperparameter searching helps you find the optimal parameters (learning rate, temperature of softmax, \u2026) your model should have in order to get the best possible performance, without having to run a thousand boring experiments by hand.\n\nThe most common hyperparameter searching strategies include :\n\n- **Grid Search** (please never do that) : the worst performing method to my sense since you can completely miss a pattern or a very local peak in performance for some values, it consists or testing hyperparameter values equally distributed on an interval of possible values you have defined ;\n- **Random Search** (and its Monte-Carlo derivatives) : you try random values of your parameters. The main issue with it lies in the fact that it is a parallel method and can quickly become very costly the more parameters you are testing. However, it has the advantage of enabling you to include prior knowledge in your testing : if you want to find the best learning rate between 1e-4 and 1e-1, but you suppose it must be around 1e-3, you can draw samples from a log-normal distribution centered on 1e-3.\n- **Bayesian Search** : basically the random search but improved in so far as it is iterative and therefore much less costly. It iteratively evaluates a promising hyperparameter configuration based on the current model, and then updates it. It is the best performing of the three.\n- Other methods including **gradient-based search** or **evolutionary optimization** are more hazardous and do not generally apply. They can be recommended in some special cases.\n\nThere are many **AutoML** tools that can do the job very well for you. Just take a look at the excellent Medium & TowardsDataScience ressources on this topic :\n\n- [https://towardsdatascience.com/how-to-beat-automl-hyperparameter-optimisation-with-flair-3b2f5092d9f5](https://towardsdatascience.com/how-to-beat-automl-hyperparameter-optimisation-with-flair-3b2f5092d9f5)\n- [https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)\n- [https://medium.com/@martsalz/automl-hyperparameter-tuning-with-nni-and-keras-ffbef61206cf](https://medium.com/@martsalz/automl-hyperparameter-tuning-with-nni-and-keras-ffbef61206cf)\n\nHowever, you have to be careful and keep a solid intuition of what hyperparameters values mean. If you don\u2019t have a solid validation set and homogeneous data, hyperparameter optimization pushed too far can lead into the overfitting-lion\u2019s den. Always prefer some rationally explainable parameter choice to a millidecimal accuracy win on training data.\n\n## 4\\. Simple practices can change the game\n\nI have found that there are some model wrappers you can use to get better results. They work on different levels :\n\n- In the optimization process, never forget to add a **Learning Rate Scheduler** that help get a more precise training (starting small, progressively increasing when your model is learning well, reducing the step on plateau for example).\n- Still in the optimization process...",
      "url": "https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418"
    },
    {
      "title": "What do you think are the limitations to Kaggle kernels? | Kaggle",
      "text": "<div><div><p>The main limitation of Kaggle is the 9 hour runtime limit on commit, and that Juypter notebooks don't provide access to all the tools of a localhost IDE (debugger, linter, profiler, git).</p>\n<p>The following line can be used to determine your current runtime environment</p>\n<pre><code>os<span>.environ</span><span>.get</span>(<span>'KAGGLE_KERNEL_RUN_TYPE'</span>, <span>'Localhost'</span>) <span>in</span> <span>[<span>'Batch'</span>, <span>'Interactive'</span>]</span>\n</code></pre>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/kaggle-environment-variables-os-environ\">https://www.kaggle.com/jamesmcguigan/kaggle-environment-variables-os-environ</a></li>\n</ul>\n<p>My original solution was <code>kaggle_compile.py</code>, which allowed me to concatenate my localhost IDE files into a single copy/paste bundle for upload </p>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/kaggle-compile-py-python-ide-to-kaggle-compiler\">https://www.kaggle.com/jamesmcguigan/kaggle-compile-py-python-ide-to-kaggle-compiler</a></li>\n</ul>\n<p>I since managed to figure out a better workflow for developing in a localhost IDE, version control in github, whilst retaining the ability to easily publish my research on Kaggle.</p>\n<p>Also as a workaround solution to the 9 hour runtme limit, it is possible to create a dataset reimport loop, by attaching your notebook as a dataset for itself. There may be a little bit of filepath/filesystem juggling involved, but this means you can commit for 9 hours, save to file, then look for an existing saved file in the input directory, merge/copy and then rerun for another 9 hours. </p>\n<p>It is also possible to setup a poor man's version of cluster compute to take advantage of the fact that Kaggle permits you 10 simultaneous commit sessions of 9 hours each. Host your code on github and then create 10 forks of your notebook, each importing the datasets from all the other forks. Use a modulo loop to subdivide your dataset <code>if id % 10 == N:</code> and then figure out a merge strategy to recombine multiple output files back into a single input file. For a submission.csv file, this can be done with <code>find | xargs cat | sort -nr | uniq | awk -F',' '!a[$1]++' | sort -n | sponge &gt; ./submission.csv</code></p>\n<p>I have a working example of this here:</p>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/game-of-life-z3-constraint-satisfaction\">https://www.kaggle.com/jamesmcguigan/game-of-life-z3-constraint-satisfaction</a></li>\n</ul></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/184059"
    },
    {
      "title": "Efficient GPU Usage Tips and Tricks",
      "text": "<div><div><p>Kaggle provides notebook editors with free access to NVIDIA TESLA P100 <a href=\"https://www.kaggle.com/product-feedback/83643\">GPUs.</a></p><p> These GPUs are useful for training deep learning models, though they do not accelerate most other workflows (i.e. libraries like pandas and scikit-learn do not benefit from access to GPUs).\nYou can use up to 30 hours per week of GPU, and individual sessions can run up to 9 hours.\nHere are some tips and tricks to get the most of your GPU usage on Kaggle:\n</p>\n<p><strong>Only turn on the GPU if you plan on using the GPU\n</strong> </p>\n<ul>\n<li>GPUs are only helpful if you are using code that takes advantage of GPU-accelerated libraries (e.g. TensorFlow, PyTorch, etc). But most applications don't benefit from a GPU.\n</li></ul>\n<p><strong>Actively monitor and manage your GPU usage\n</strong> </p>\n<ul>\n<li>Kaggle has tools for monitoring GPU usage in the settings menu of the notebook editor, at the top of the page at kaggle.com/notebooks, <a href=\"http://kaggle.com/me/account\">on your profile page</a>, and in the session management window.\n</li></ul>\n<p><strong>Use the \"Quick Save\" option when saving a new version\n</strong> </p>\n<ul>\n<li>When you want to save or checkpoint your work, you can choose \"Quick Save\" in the \"Save Version\" dialog. This saves your notebook as it is currently shown and unlike \"Save &amp; Run All\" doesn't run your code top to bottom, using up GPU resources.\n</li></ul>\n<p><strong>Only use a GPU in a batch (\"Save &amp; Run All\") session\n</strong> </p>\n<ul>\n<li>As you write code interactively in the editor, you can keep the GPU toggled off if you're not using it. Then, when you're ready to create a new version using \"Save &amp; Run All\", navigate to the \"Advanced Options\" and choose \"Run with GPU for this session\".\n</li></ul>\n<p><strong>Cancel unnecessary batch sessions\n</strong> </p>\n<ul>\n<li>The same notebook can have multiple concurrent batch sessions if you press the commit button prior to completing the first commit. If your latest code has been updated as compared to your previous code, it is likely better for you to cancel that first commit and leave only the 2nd commit running.\n</li></ul>\n<p><strong>Stop interactive sessions prior to closing the window\n</strong> </p>\n<ul>\n<li>Interactive sessions remain active until they reach the 60 minute idle timeout limit. If you stop the session prior to closing your window it can save you up to 60 minutes of compute.\n</li></ul>\n<p><strong>Consider using the <a href=\"https://github.com/Kaggle/kaggle-api\">Kaggle-API</a> to avoid interactive sessions entirely\n</strong> </p>\n<ul>\n<li>With the Kaggle API you can push a new version of your notebook without ever opening up an interactive session in the notebook editor.\n</li></ul>\n<p><strong>By following these guidelines we hope that you will be able to receive maximum benefit from our free GPU compute. Happy Kaggling!\n</strong></p>\n</div></div>",
      "url": "https://www.kaggle.com/page/GPU-tips-and-tricks"
    }
  ]
}