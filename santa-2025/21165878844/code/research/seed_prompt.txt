# Santa 2025 - Christmas Tree Packing Optimization

## Current Status
- Best CV score: 71.81106 from exp_002 (micro_separation)
- Best LB score: 71.8128 (exp_001 - valid submission)
- Target: 68.919154 | Gap to target: 2.89 points (4.2%)

## CRITICAL PROBLEM IDENTIFIED
**Kaggle requires distance > 0 between all trees.** The pre-optimized solutions have trees TOUCHING (distance = 0), which Kaggle rejects as "overlapping". 

The gap breakdown:
- Touching ensemble: 70.65 (rejected by Kaggle)
- Valid ensemble: 71.81 (accepted)
- Gap: 1.17 points lost due to non-touching requirement
- Target: 68.92 (still 1.73 points below touching ensemble)

## Response to Evaluator
The evaluator correctly identified that:
1. Micro-separation approach has limited effectiveness (only 5/200 N values separated)
2. The touching configurations are too tightly packed - any movement causes new overlaps
3. **The key insight**: We need to GENERATE valid solutions from scratch with gap constraints, not convert touching solutions

I fully agree with the evaluator's recommendation to modify the C++ optimizer to maintain a minimum gap during optimization. This is the highest-leverage change.

## STRATEGY PIVOT: Generate Valid Solutions with Gap Constraints

### APPROACH 1: Modify C++ Optimizer (HIGHEST PRIORITY)
Modify `sa_v1_parallel.cpp` to maintain minimum gap:

```cpp
// In overlap() function, add distance check:
double polygon_distance(const Poly& a, const Poly& b) {
    // Calculate minimum distance between two polygons
    // (Implementation needed)
}

bool overlap(const Poly& a, const Poly& b) {
    const double MIN_GAP = 0.001;
    if (a.x1 + MIN_GAP < b.x0 || b.x1 + MIN_GAP < a.x0 || 
        a.y1 + MIN_GAP < b.y0 || b.y1 + MIN_GAP < a.y0) return false;
    // ... rest of overlap check with MIN_GAP buffer
}
```

This would enable the optimizer to find solutions that are:
1. Valid for Kaggle (distance > 0)
2. Near-optimal (only slightly worse than touching solutions)

### APPROACH 2: Tessellation/Translation-Based Approach (HIGH PRIORITY)
From egortrushin kernel - for large N (>50):
1. Start with 2 base trees in optimal configuration
2. Translate them in x and y directions to create a grid pattern
3. Parameters: nt = [nx, ny] where nx*ny >= N
4. Optimize the base configuration and translation distances

This is fundamentally different from random SA and could achieve tighter bounds.

### APPROACH 3: Python-Based Gap-Constrained Optimization (MEDIUM PRIORITY)
If C++ modification is too complex, implement in Python:
1. Use shapely's `distance()` function to check gaps
2. Reject moves that bring trees closer than MIN_GAP
3. Slower but easier to implement and debug

### APPROACH 4: Hybrid Approach (MEDIUM PRIORITY)
1. Run existing C++ optimizer to get touching solutions
2. Apply systematic expansion: scale all positions by 1.001 from centroid
3. Re-optimize with gap constraints
4. This might recover some of the 1.17 point gap

## Recommended Experiments (Priority Order)

1. **[HIGHEST]** Implement gap-constrained C++ optimizer
   - Modify sa_v1_parallel.cpp to add MIN_GAP buffer
   - Run optimization from scratch for all N
   - Expected: Score between 70.65 (touching) and 71.81 (valid)

2. **[HIGH]** Implement tessellation approach for large N
   - Use egortrushin's approach for N >= 50
   - Optimize base configuration and translation distances
   - Expected: Better scores for large N configurations

3. **[MEDIUM]** Python gap-constrained SA
   - Implement simpler SA in Python with gap constraints
   - Focus on small N (1-20) where search space is manageable
   - Expected: Modest improvements for small N

## What NOT to Try
- ❌ More micro-separation attempts (proven ineffective)
- ❌ Finding better valid configs from existing CSVs (already exhausted)
- ❌ Running existing optimizers longer (they optimize for touching)

## Validation Notes
- CV = LB exactly (perfect calibration confirmed)
- Scoring: score = Σ(s_n² / n) for n=1 to 200
- Lower is better

## SUBMISSION STRATEGY
- Remaining submissions: 90
- Submit after EVERY experiment - LB feedback is free!
- Even if worse than baseline, we learn what doesn't work
