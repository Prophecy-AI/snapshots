{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd1b56a",
   "metadata": {},
   "source": [
    "# Snapshot Ensemble Experiment\n",
    "\n",
    "Verify scores of best_snapshot.csv and saspav_best.csv, then create an ensemble to achieve 70.626088."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Tree vertices\n",
    "TX = np.array([0,0.125,0.0625,0.2,0.1,0.35,0.075,0.075,-0.075,-0.075,-0.35,-0.1,-0.2,-0.0625,-0.125])\n",
    "TY = np.array([0.8,0.5,0.5,0.25,0.25,0,0,-0.2,-0.2,0,0,0.25,0.25,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    \"\"\"Calculate score for a single N configuration\"\"\"\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c = math.cos(r)\n",
    "        s = math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx = min(mnx, X)\n",
    "            mxx = max(mxx, X)\n",
    "            mny = min(mny, Y)\n",
    "            mxy = max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8298499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_submission(filepath):\n",
    "    \"\"\"Parse submission CSV and return dict of N -> (xs, ys, degs)\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Parse values (remove 's' prefix)\n",
    "    df['x_val'] = df['x'].str.replace('s', '').astype(float)\n",
    "    df['y_val'] = df['y'].str.replace('s', '').astype(float)\n",
    "    df['deg_val'] = df['deg'].str.replace('s', '').astype(float)\n",
    "    \n",
    "    # Extract N from id (format: NNN_idx)\n",
    "    df['N'] = df['id'].str.split('_').str[0].astype(int)\n",
    "    \n",
    "    configs = {}\n",
    "    for n, group in df.groupby('N'):\n",
    "        xs = group['x_val'].values\n",
    "        ys = group['y_val'].values\n",
    "        degs = group['deg_val'].values\n",
    "        configs[n] = (xs, ys, degs)\n",
    "    \n",
    "    return configs, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e478e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_score(configs, tx, ty):\n",
    "    \"\"\"Calculate total score across all N values\"\"\"\n",
    "    total = 0.0\n",
    "    scores_by_n = {}\n",
    "    for n in range(1, 201):\n",
    "        if n in configs:\n",
    "            xs, ys, degs = configs[n]\n",
    "            score = score_group(xs, ys, degs, tx, ty)\n",
    "            scores_by_n[n] = score\n",
    "            total += score\n",
    "    return total, scores_by_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all available submissions\n",
    "files = {\n",
    "    'baseline': '/home/code/preoptimized/submission.csv',\n",
    "    'best_snapshot': '/home/code/preoptimized/best_snapshot.csv',\n",
    "    'saspav_best': '/home/code/preoptimized/saspav_best.csv',\n",
    "    'smartmanoj': '/home/code/preoptimized/smartmanoj_submission.csv',\n",
    "    'ensemble': '/home/code/preoptimized/ensemble.csv',\n",
    "}\n",
    "\n",
    "all_configs = {}\n",
    "all_scores = {}\n",
    "all_scores_by_n = {}\n",
    "\n",
    "for name, path in files.items():\n",
    "    if os.path.exists(path):\n",
    "        configs, df = parse_submission(path)\n",
    "        total, scores_by_n = calculate_total_score(configs, TX, TY)\n",
    "        all_configs[name] = configs\n",
    "        all_scores[name] = total\n",
    "        all_scores_by_n[name] = scores_by_n\n",
    "        print(f\"{name}: {total:.6f}\")\n",
    "    else:\n",
    "        print(f\"{name}: FILE NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63547e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check the snapshot submission\n",
    "snapshot_path = '/home/nonroot/snapshots/santa-2025/21198927060/submission/submission.csv'\n",
    "if os.path.exists(snapshot_path):\n",
    "    configs, df = parse_submission(snapshot_path)\n",
    "    total, scores_by_n = calculate_total_score(configs, TX, TY)\n",
    "    all_configs['snapshot_submission'] = configs\n",
    "    all_scores['snapshot_submission'] = total\n",
    "    all_scores_by_n['snapshot_submission'] = scores_by_n\n",
    "    print(f\"snapshot_submission: {total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aefb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which N values are better in each submission compared to baseline\n",
    "baseline_scores = all_scores_by_n['baseline']\n",
    "\n",
    "print(\"\\nN values where other submissions beat baseline:\")\n",
    "for name in all_scores_by_n:\n",
    "    if name == 'baseline':\n",
    "        continue\n",
    "    better_n = []\n",
    "    for n in range(1, 201):\n",
    "        if n in all_scores_by_n[name] and n in baseline_scores:\n",
    "            if all_scores_by_n[name][n] < baseline_scores[n] - 1e-9:\n",
    "                improvement = baseline_scores[n] - all_scores_by_n[name][n]\n",
    "                better_n.append((n, improvement))\n",
    "    if better_n:\n",
    "        print(f\"\\n{name}: {len(better_n)} N values better than baseline\")\n",
    "        for n, imp in sorted(better_n, key=lambda x: -x[1])[:10]:\n",
    "            print(f\"  N={n}: improvement {imp:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimal ensemble - for each N, pick the best configuration\n",
    "print(\"\\nCreating optimal ensemble...\")\n",
    "best_configs = {}\n",
    "best_scores_by_n = {}\n",
    "\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_source = None\n",
    "    best_config = None\n",
    "    \n",
    "    for name, scores_by_n in all_scores_by_n.items():\n",
    "        if n in scores_by_n and scores_by_n[n] < best_score:\n",
    "            best_score = scores_by_n[n]\n",
    "            best_source = name\n",
    "            best_config = all_configs[name][n]\n",
    "    \n",
    "    best_configs[n] = best_config\n",
    "    best_scores_by_n[n] = best_score\n",
    "\n",
    "ensemble_total = sum(best_scores_by_n.values())\n",
    "print(f\"\\nOptimal ensemble score: {ensemble_total:.6f}\")\n",
    "print(f\"Baseline score: {all_scores['baseline']:.6f}\")\n",
    "print(f\"Improvement: {all_scores['baseline'] - ensemble_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb478ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which N values come from which source\n",
    "print(\"\\nSource breakdown for optimal ensemble:\")\n",
    "source_counts = {}\n",
    "for n in range(1, 201):\n",
    "    best_source = None\n",
    "    best_score = float('inf')\n",
    "    for name, scores_by_n in all_scores_by_n.items():\n",
    "        if n in scores_by_n and scores_by_n[n] < best_score:\n",
    "            best_score = scores_by_n[n]\n",
    "            best_source = name\n",
    "    source_counts[best_source] = source_counts.get(best_source, 0) + 1\n",
    "\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {source}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission CSV from optimal ensemble\n",
    "def generate_submission(configs, output_path):\n",
    "    rows = []\n",
    "    for n in range(1, 201):\n",
    "        xs, ys, degs = configs[n]\n",
    "        for i in range(len(xs)):\n",
    "            row = {\n",
    "                'id': f'{n:03d}_{i}',\n",
    "                'x': f's{xs[i]}',\n",
    "                'y': f's{ys[i]}',\n",
    "                'deg': f's{degs[i]}'\n",
    "            }\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "# Save ensemble submission\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission_df = generate_submission(best_configs, '/home/submission/submission.csv')\n",
    "print(f\"Saved ensemble submission to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f106ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved submission\n",
    "verify_configs, _ = parse_submission('/home/submission/submission.csv')\n",
    "verify_total, _ = calculate_total_score(verify_configs, TX, TY)\n",
    "print(f\"\\nVerification of saved submission: {verify_total:.6f}\")\n",
    "print(f\"Expected: {ensemble_total:.6f}\")\n",
    "print(f\"Match: {abs(verify_total - ensemble_total) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb90b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': verify_total,\n",
    "    'baseline_score': all_scores['baseline'],\n",
    "    'improvement': all_scores['baseline'] - verify_total,\n",
    "    'source_counts': source_counts\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/002_snapshot_ensemble/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved metrics to experiments/002_snapshot_ensemble/metrics.json\")\n",
    "print(f\"CV Score: {verify_total:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
