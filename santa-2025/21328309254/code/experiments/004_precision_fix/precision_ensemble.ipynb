{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b669c45",
   "metadata": {},
   "source": [
    "# Precision-Preserving Ensemble\n",
    "\n",
    "Fix the precision truncation bug by preserving original string values from source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2e46c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.315256Z",
     "iopub.status.busy": "2026-01-25T07:30:19.314721Z",
     "iopub.status.idle": "2026-01-25T07:30:19.689734Z",
     "shell.execute_reply": "2026-01-25T07:30:19.689358Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "import json\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tree vertices\n",
    "TX = np.array([0,0.125,0.0625,0.2,0.1,0.35,0.075,0.075,-0.075,-0.075,-0.35,-0.1,-0.2,-0.0625,-0.125])\n",
    "TY = np.array([0.8,0.5,0.5,0.25,0.25,0,0,-0.2,-0.2,0,0,0.25,0.25,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1567439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.691541Z",
     "iopub.status.busy": "2026-01-25T07:30:19.691397Z",
     "iopub.status.idle": "2026-01-25T07:30:19.699309Z",
     "shell.execute_reply": "2026-01-25T07:30:19.698980Z"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    \"\"\"Calculate score for a single N configuration\"\"\"\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c = math.cos(r)\n",
    "        s = math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx = min(mnx, X)\n",
    "            mxx = max(mxx, X)\n",
    "            mny = min(mny, Y)\n",
    "            mxy = max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f589cc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.700107Z",
     "iopub.status.busy": "2026-01-25T07:30:19.700019Z",
     "iopub.status.idle": "2026-01-25T07:30:19.703058Z",
     "shell.execute_reply": "2026-01-25T07:30:19.702699Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_submission_with_strings(filepath):\n",
    "    \"\"\"Parse submission CSV and return dict of N -> DataFrame with original strings\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Parse float values for scoring (but keep original strings)\n",
    "    df['x_val'] = df['x'].str.replace('s', '').astype(float)\n",
    "    df['y_val'] = df['y'].str.replace('s', '').astype(float)\n",
    "    df['deg_val'] = df['deg'].str.replace('s', '').astype(float)\n",
    "    \n",
    "    # Extract N and tree_idx from id (format: NNN_idx)\n",
    "    df['N'] = df['id'].str.split('_').str[0].astype(int)\n",
    "    df['tree_idx'] = df['id'].str.split('_').str[1].astype(int)\n",
    "    \n",
    "    # Return dict of N -> DataFrame (with original strings preserved)\n",
    "    configs = {}\n",
    "    for n, group in df.groupby('N'):\n",
    "        configs[n] = group.copy()\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8682a068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.703888Z",
     "iopub.status.busy": "2026-01-25T07:30:19.703797Z",
     "iopub.status.idle": "2026-01-25T07:30:19.706102Z",
     "shell.execute_reply": "2026-01-25T07:30:19.705751Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score_from_df(df, tx, ty):\n",
    "    \"\"\"Calculate score from DataFrame\"\"\"\n",
    "    xs = df['x_val'].values\n",
    "    ys = df['y_val'].values\n",
    "    degs = df['deg_val'].values\n",
    "    return score_group(xs, ys, degs, tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6a408f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.706970Z",
     "iopub.status.busy": "2026-01-25T07:30:19.706879Z",
     "iopub.status.idle": "2026-01-25T07:30:19.710422Z",
     "shell.execute_reply": "2026-01-25T07:30:19.710063Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_overlaps_strict(df, buffer_dist=1e-9):\n",
    "    \"\"\"Check for overlaps with strict tolerance\"\"\"\n",
    "    n = len(df)\n",
    "    if n <= 1:\n",
    "        return True, []\n",
    "    \n",
    "    xs = df['x_val'].values\n",
    "    ys = df['y_val'].values\n",
    "    degs = df['deg_val'].values\n",
    "    \n",
    "    # Create polygons\n",
    "    polygons = []\n",
    "    for i in range(n):\n",
    "        p = Polygon(zip(TX, TY))\n",
    "        p = affinity.rotate(p, degs[i], origin=(0,0))\n",
    "        p = affinity.translate(p, xs[i], ys[i])\n",
    "        polygons.append(p)\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if polygons[i].intersects(polygons[j]):\n",
    "                intersection = polygons[i].intersection(polygons[j])\n",
    "                if intersection.area > 1e-12:  # Non-trivial overlap\n",
    "                    overlaps.append((i, j, intersection.area))\n",
    "    \n",
    "    return len(overlaps) == 0, overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c95edf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:19.711275Z",
     "iopub.status.busy": "2026-01-25T07:30:19.711166Z",
     "iopub.status.idle": "2026-01-25T07:30:20.161543Z",
     "shell.execute_reply": "2026-01-25T07:30:20.161153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 70.647327\n",
      "\n",
      "Baseline N=2 x values:\n",
      "  s0.1540970696213643\n",
      "  s-0.1540970696213643\n"
     ]
    }
   ],
   "source": [
    "# Load baseline (known valid)\n",
    "baseline_path = '/home/code/preoptimized/submission.csv'\n",
    "baseline_configs = parse_submission_with_strings(baseline_path)\n",
    "\n",
    "# Calculate baseline scores\n",
    "baseline_scores = {}\n",
    "baseline_total = 0\n",
    "for n in range(1, 201):\n",
    "    score = calculate_score_from_df(baseline_configs[n], TX, TY)\n",
    "    baseline_scores[n] = score\n",
    "    baseline_total += score\n",
    "\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "\n",
    "# Verify baseline N=2 precision\n",
    "print(f\"\\nBaseline N=2 x values:\")\n",
    "for _, row in baseline_configs[2].iterrows():\n",
    "    print(f\"  {row['x']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91f7d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:30.715622Z",
     "iopub.status.busy": "2026-01-25T07:30:30.714978Z",
     "iopub.status.idle": "2026-01-25T07:30:37.927202Z",
     "shell.execute_reply": "2026-01-25T07:30:37.926795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 100 snapshot directories...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 sources\n"
     ]
    }
   ],
   "source": [
    "# Scan all snapshot directories for submission files\n",
    "snapshot_base = '/home/nonroot/snapshots/santa-2025/'\n",
    "snapshot_dirs = os.listdir(snapshot_base)\n",
    "\n",
    "all_sources = {}\n",
    "all_sources['baseline'] = baseline_configs\n",
    "\n",
    "print(f\"Scanning {len(snapshot_dirs)} snapshot directories...\")\n",
    "\n",
    "for snap_dir in sorted(snapshot_dirs):\n",
    "    snap_path = os.path.join(snapshot_base, snap_dir)\n",
    "    \n",
    "    # Check for submission.csv in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(snap_path, 'submission', 'submission.csv'),\n",
    "        os.path.join(snap_path, 'code', 'submission.csv'),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                configs = parse_submission_with_strings(path)\n",
    "                if len(configs) == 200:  # Valid submission has 200 N values\n",
    "                    all_sources[f'snap_{snap_dir}'] = configs\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "print(f\"Loaded {len(all_sources)} sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075d87d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:30:37.928306Z",
     "iopub.status.busy": "2026-01-25T07:30:37.928190Z",
     "iopub.status.idle": "2026-01-25T07:30:50.073551Z",
     "shell.execute_reply": "2026-01-25T07:30:50.073147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding best valid configuration for each N (preserving precision)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation failures (better score but overlapping): 57\n"
     ]
    }
   ],
   "source": [
    "# For each N, find the best configuration that passes strict overlap validation\n",
    "print(\"\\nFinding best valid configuration for each N (preserving precision)...\")\n",
    "\n",
    "best_configs = {}  # N -> DataFrame with original strings\n",
    "best_scores_by_n = {}\n",
    "source_used = {}\n",
    "validation_failures = []\n",
    "\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_df = None\n",
    "    best_source = None\n",
    "    \n",
    "    # Collect all candidates for this N\n",
    "    candidates = []\n",
    "    for source_name, configs in all_sources.items():\n",
    "        if n in configs:\n",
    "            df = configs[n]\n",
    "            score = calculate_score_from_df(df, TX, TY)\n",
    "            candidates.append((score, source_name, df))\n",
    "    \n",
    "    # Sort by score (best first)\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Try each candidate until we find one that passes validation\n",
    "    for score, source_name, df in candidates:\n",
    "        valid, overlaps = check_overlaps_strict(df)\n",
    "        \n",
    "        if valid:\n",
    "            best_score = score\n",
    "            best_df = df\n",
    "            best_source = source_name\n",
    "            break\n",
    "        else:\n",
    "            if source_name != 'baseline':\n",
    "                validation_failures.append((n, source_name, score, len(overlaps)))\n",
    "    \n",
    "    # If no valid config found, use baseline (should always be valid)\n",
    "    if best_df is None:\n",
    "        best_df = baseline_configs[n]\n",
    "        best_score = baseline_scores[n]\n",
    "        best_source = 'baseline_fallback'\n",
    "    \n",
    "    best_configs[n] = best_df\n",
    "    best_scores_by_n[n] = best_score\n",
    "    source_used[n] = best_source\n",
    "\n",
    "print(f\"\\nValidation failures (better score but overlapping): {len(validation_failures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4983a9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:00.546446Z",
     "iopub.status.busy": "2026-01-25T07:31:00.545919Z",
     "iopub.status.idle": "2026-01-25T07:31:00.549745Z",
     "shell.execute_reply": "2026-01-25T07:31:00.549384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ensemble score: 70.615745\n",
      "Baseline score: 70.647327\n",
      "Improvement: 0.031582\n",
      "\n",
      "Source breakdown:\n",
      "  snap_21191209482: 90 N values\n",
      "  snap_21322576827: 60 N values\n",
      "  snap_21165872902: 22 N values\n",
      "  snap_21322576451: 12 N values\n",
      "  snap_21322577324: 11 N values\n",
      "  snap_21121943993: 2 N values\n",
      "  snap_21104669204: 1 N values\n",
      "  snap_21165874980: 1 N values\n",
      "  baseline: 1 N values\n"
     ]
    }
   ],
   "source": [
    "# Calculate final score\n",
    "final_score = sum(best_scores_by_n.values())\n",
    "print(f\"\\nFinal ensemble score: {final_score:.6f}\")\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - final_score:.6f}\")\n",
    "\n",
    "# Source breakdown\n",
    "source_counts = defaultdict(int)\n",
    "for n, source in source_used.items():\n",
    "    source_counts[source] += 1\n",
    "\n",
    "print(f\"\\nSource breakdown:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {source}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff025630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:00.550648Z",
     "iopub.status.busy": "2026-01-25T07:31:00.550554Z",
     "iopub.status.idle": "2026-01-25T07:31:01.029416Z",
     "shell.execute_reply": "2026-01-25T07:31:01.029043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to /home/submission/submission.csv\n",
      "Total rows: 20100\n",
      "\n",
      "Verifying N=2 precision in saved submission:\n",
      "  002_0: x=s0.15409700000000001174\n",
      "  002_1: x=s-0.15409700000000001174\n"
     ]
    }
   ],
   "source": [
    "# Generate submission CSV PRESERVING ORIGINAL PRECISION\n",
    "def generate_submission_preserve_precision(configs_by_n, output_path):\n",
    "    \"\"\"Generate submission preserving original string precision\"\"\"\n",
    "    rows = []\n",
    "    for n in range(1, 201):\n",
    "        df = configs_by_n[n]\n",
    "        for _, row in df.iterrows():\n",
    "            rows.append({\n",
    "                'id': f'{n:03d}_{row[\"tree_idx\"]}',\n",
    "                'x': row['x'],  # Keep original string!\n",
    "                'y': row['y'],  # Keep original string!\n",
    "                'deg': row['deg']  # Keep original string!\n",
    "            })\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "# Save submission\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission_df = generate_submission_preserve_precision(best_configs, '/home/submission/submission.csv')\n",
    "print(f\"Saved submission to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission_df)}\")\n",
    "\n",
    "# Verify precision is preserved\n",
    "print(f\"\\nVerifying N=2 precision in saved submission:\")\n",
    "for _, row in submission_df[submission_df['id'].str.startswith('002_')].iterrows():\n",
    "    print(f\"  {row['id']}: x={row['x']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc247dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:01.030389Z",
     "iopub.status.busy": "2026-01-25T07:31:01.030295Z",
     "iopub.status.idle": "2026-01-25T07:31:01.113378Z",
     "shell.execute_reply": "2026-01-25T07:31:01.113006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification of saved submission: 70.615745\n",
      "Expected: 70.615745\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved submission score\n",
    "verify_configs = parse_submission_with_strings('/home/submission/submission.csv')\n",
    "verify_total = 0\n",
    "for n in range(1, 201):\n",
    "    score = calculate_score_from_df(verify_configs[n], TX, TY)\n",
    "    verify_total += score\n",
    "\n",
    "print(f\"\\nVerification of saved submission: {verify_total:.6f}\")\n",
    "print(f\"Expected: {final_score:.6f}\")\n",
    "print(f\"Match: {abs(verify_total - final_score) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2e1e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:01.114400Z",
     "iopub.status.busy": "2026-01-25T07:31:01.114317Z",
     "iopub.status.idle": "2026-01-25T07:31:01.117457Z",
     "shell.execute_reply": "2026-01-25T07:31:01.117115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved metrics to experiments/004_precision_fix/metrics.json\n",
      "CV Score: 70.615745\n"
     ]
    }
   ],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': verify_total,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - verify_total,\n",
    "    'validation_failures': len(validation_failures)\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/004_precision_fix/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved metrics to experiments/004_precision_fix/metrics.json\")\n",
    "print(f\"CV Score: {verify_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6a9b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:24.318077Z",
     "iopub.status.busy": "2026-01-25T07:31:24.317586Z",
     "iopub.status.idle": "2026-01-25T07:31:24.322577Z",
     "shell.execute_reply": "2026-01-25T07:31:24.322168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source for N=2: snap_21322576451\n",
      "\n",
      "Original source N=2 x values:\n",
      "  x string: s0.15409700000000001174\n",
      "  x_val float: 0.154097\n",
      "  x string: s-0.15409700000000001174\n",
      "  x_val float: -0.154097\n",
      "\n",
      "Baseline N=2 x values:\n",
      "  x string: s0.1540970696213643\n",
      "  x_val float: 0.1540970696213643\n",
      "  x string: s-0.1540970696213643\n",
      "  x_val float: -0.1540970696213643\n"
     ]
    }
   ],
   "source": [
    "# Check what's happening with N=2 precision\n",
    "print(\"Source for N=2:\", source_used[2])\n",
    "print(\"\\nOriginal source N=2 x values:\")\n",
    "source_df = best_configs[2]\n",
    "for _, row in source_df.iterrows():\n",
    "    print(f\"  x string: {row['x']}\")\n",
    "    print(f\"  x_val float: {row['x_val']}\")\n",
    "\n",
    "# Check baseline N=2\n",
    "print(\"\\nBaseline N=2 x values:\")\n",
    "for _, row in baseline_configs[2].iterrows():\n",
    "    print(f\"  x string: {row['x']}\")\n",
    "    print(f\"  x_val float: {row['x_val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7c3fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:31:46.916525Z",
     "iopub.status.busy": "2026-01-25T07:31:46.916010Z",
     "iopub.status.idle": "2026-01-25T07:31:46.918559Z",
     "shell.execute_reply": "2026-01-25T07:31:46.918216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare scores for N=2\\nbaseline_n2_score = calculate_score_from_df(baseline_configs[2], TX, TY)\\nsnapshot_n2_score = calculate_score_from_df(best_configs[2], TX, TY)\\n\\nprint(f\\\"Baseline N=2 score: {baseline_n2_score:.6f}\\\")\\nprint(f\\\"Snapshot N=2 score: {snapshot_n2_score:.6f}\\\")\\nprint(f\\\"Difference: {baseline_n2_score - snapshot_n2_score:.6f}\\\")\\n\\n# The snapshot has TRUNCATED precision which will cause overlaps!\\n# We need to use baseline for N=2 to ensure validity\\nprint(\\\"\\\\nThe snapshot N=2 has truncated precision - will cause overlaps!\\\")\\nprint(\\\"Must use baseline for N=2 to ensure validity.\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533a126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:32:10.855872Z",
     "iopub.status.busy": "2026-01-25T07:32:10.855365Z",
     "iopub.status.idle": "2026-01-25T07:32:10.857964Z",
     "shell.execute_reply": "2026-01-25T07:32:10.857629Z"
    }
   },
   "outputs": [],
   "source": [
    "# The issue is that some snapshots have truncated precision in their source files\\n# We need to detect this and fall back to baseline\\n\\n# Check which N values have truncated precision in their source\\nprint(\\\"Checking for truncated precision in selected sources...\\\")\\n\\ntruncated_n_values = []\\nfor n in range(1, 201):\\n    df = best_configs[n]\\n    source = source_used[n]\\n    \\n    # Check if any x/y/deg values have truncated precision\\n    for _, row in df.iterrows():\\n        x_str = row['x'].replace('s', '')\\n        y_str = row['y'].replace('s', '')\\n        deg_str = row['deg'].replace('s', '')\\n        \\n        # Check if precision is less than baseline (baseline has ~16 decimal places)\\n        # Truncated values often have patterns like \\\"0.15409700000000001174\\\"\\n        if '00000000' in x_str or '00000000' in y_str or '00000000' in deg_str:\\n            truncated_n_values.append((n, source))\\n            break\\n\\nprint(f\\\"\\\\nN values with truncated precision: {len(truncated_n_values)}\\\")\\nfor n, source in truncated_n_values[:20]:\\n    print(f\\\"  N={n}: {source}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b51c54f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T07:32:31.242387Z",
     "iopub.status.busy": "2026-01-25T07:32:31.241853Z",
     "iopub.status.idle": "2026-01-25T07:32:31.244420Z",
     "shell.execute_reply": "2026-01-25T07:32:31.244076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let me check the actual precision patterns more carefully\\nprint(\\\"Checking precision patterns...\\\")\\n\\n# Check baseline precision\\nfor _, row in baseline_configs[2].iterrows():\\n    x_str = row['x'].replace('s', '')\\n    print(f\\\"Baseline N=2: x={x_str}, len={len(x_str)}\\\")\\n\\n# Check snapshot precision  \\nfor _, row in best_configs[2].iterrows():\\n    x_str = row['x'].replace('s', '')\\n    print(f\\\"Snapshot N=2: x={x_str}, len={len(x_str)}\\\")\\n\\n# The issue is clear - snapshot has different precision format\\n# Let's just use baseline for ALL N values to ensure validity\\nprint(\\\"\\\\n=== SOLUTION: Use baseline for all N values ===\\\")\\nprint(\\\"This ensures full precision and validity.\\\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
