{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b669c45",
   "metadata": {},
   "source": [
    "# Precision-Preserving Ensemble\n",
    "\n",
    "Fix the precision truncation bug by preserving original string values from source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "import json\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tree vertices\n",
    "TX = np.array([0,0.125,0.0625,0.2,0.1,0.35,0.075,0.075,-0.075,-0.075,-0.35,-0.1,-0.2,-0.0625,-0.125])\n",
    "TY = np.array([0.8,0.5,0.5,0.25,0.25,0,0,-0.2,-0.2,0,0,0.25,0.25,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1567439",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    \"\"\"Calculate score for a single N configuration\"\"\"\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c = math.cos(r)\n",
    "        s = math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx = min(mnx, X)\n",
    "            mxx = max(mxx, X)\n",
    "            mny = min(mny, Y)\n",
    "            mxy = max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f589cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_submission_with_strings(filepath):\n",
    "    \"\"\"Parse submission CSV and return dict of N -> DataFrame with original strings\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Parse float values for scoring (but keep original strings)\n",
    "    df['x_val'] = df['x'].str.replace('s', '').astype(float)\n",
    "    df['y_val'] = df['y'].str.replace('s', '').astype(float)\n",
    "    df['deg_val'] = df['deg'].str.replace('s', '').astype(float)\n",
    "    \n",
    "    # Extract N and tree_idx from id (format: NNN_idx)\n",
    "    df['N'] = df['id'].str.split('_').str[0].astype(int)\n",
    "    df['tree_idx'] = df['id'].str.split('_').str[1].astype(int)\n",
    "    \n",
    "    # Return dict of N -> DataFrame (with original strings preserved)\n",
    "    configs = {}\n",
    "    for n, group in df.groupby('N'):\n",
    "        configs[n] = group.copy()\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_from_df(df, tx, ty):\n",
    "    \"\"\"Calculate score from DataFrame\"\"\"\n",
    "    xs = df['x_val'].values\n",
    "    ys = df['y_val'].values\n",
    "    degs = df['deg_val'].values\n",
    "    return score_group(xs, ys, degs, tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlaps_strict(df, buffer_dist=1e-9):\n",
    "    \"\"\"Check for overlaps with strict tolerance\"\"\"\n",
    "    n = len(df)\n",
    "    if n <= 1:\n",
    "        return True, []\n",
    "    \n",
    "    xs = df['x_val'].values\n",
    "    ys = df['y_val'].values\n",
    "    degs = df['deg_val'].values\n",
    "    \n",
    "    # Create polygons\n",
    "    polygons = []\n",
    "    for i in range(n):\n",
    "        p = Polygon(zip(TX, TY))\n",
    "        p = affinity.rotate(p, degs[i], origin=(0,0))\n",
    "        p = affinity.translate(p, xs[i], ys[i])\n",
    "        polygons.append(p)\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if polygons[i].intersects(polygons[j]):\n",
    "                intersection = polygons[i].intersection(polygons[j])\n",
    "                if intersection.area > 1e-12:  # Non-trivial overlap\n",
    "                    overlaps.append((i, j, intersection.area))\n",
    "    \n",
    "    return len(overlaps) == 0, overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline (known valid)\n",
    "baseline_path = '/home/code/preoptimized/submission.csv'\n",
    "baseline_configs = parse_submission_with_strings(baseline_path)\n",
    "\n",
    "# Calculate baseline scores\n",
    "baseline_scores = {}\n",
    "baseline_total = 0\n",
    "for n in range(1, 201):\n",
    "    score = calculate_score_from_df(baseline_configs[n], TX, TY)\n",
    "    baseline_scores[n] = score\n",
    "    baseline_total += score\n",
    "\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "\n",
    "# Verify baseline N=2 precision\n",
    "print(f\"\\nBaseline N=2 x values:\")\n",
    "for _, row in baseline_configs[2].iterrows():\n",
    "    print(f\"  {row['x']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all snapshot directories for submission files\n",
    "snapshot_base = '/home/nonroot/snapshots/santa-2025/'\n",
    "snapshot_dirs = os.listdir(snapshot_base)\n",
    "\n",
    "all_sources = {}\n",
    "all_sources['baseline'] = baseline_configs\n",
    "\n",
    "print(f\"Scanning {len(snapshot_dirs)} snapshot directories...\")\n",
    "\n",
    "for snap_dir in sorted(snapshot_dirs):\n",
    "    snap_path = os.path.join(snapshot_base, snap_dir)\n",
    "    \n",
    "    # Check for submission.csv in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(snap_path, 'submission', 'submission.csv'),\n",
    "        os.path.join(snap_path, 'code', 'submission.csv'),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                configs = parse_submission_with_strings(path)\n",
    "                if len(configs) == 200:  # Valid submission has 200 N values\n",
    "                    all_sources[f'snap_{snap_dir}'] = configs\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "print(f\"Loaded {len(all_sources)} sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N, find the best configuration that passes strict overlap validation\n",
    "print(\"\\nFinding best valid configuration for each N (preserving precision)...\")\n",
    "\n",
    "best_configs = {}  # N -> DataFrame with original strings\n",
    "best_scores_by_n = {}\n",
    "source_used = {}\n",
    "validation_failures = []\n",
    "\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_df = None\n",
    "    best_source = None\n",
    "    \n",
    "    # Collect all candidates for this N\n",
    "    candidates = []\n",
    "    for source_name, configs in all_sources.items():\n",
    "        if n in configs:\n",
    "            df = configs[n]\n",
    "            score = calculate_score_from_df(df, TX, TY)\n",
    "            candidates.append((score, source_name, df))\n",
    "    \n",
    "    # Sort by score (best first)\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Try each candidate until we find one that passes validation\n",
    "    for score, source_name, df in candidates:\n",
    "        valid, overlaps = check_overlaps_strict(df)\n",
    "        \n",
    "        if valid:\n",
    "            best_score = score\n",
    "            best_df = df\n",
    "            best_source = source_name\n",
    "            break\n",
    "        else:\n",
    "            if source_name != 'baseline':\n",
    "                validation_failures.append((n, source_name, score, len(overlaps)))\n",
    "    \n",
    "    # If no valid config found, use baseline (should always be valid)\n",
    "    if best_df is None:\n",
    "        best_df = baseline_configs[n]\n",
    "        best_score = baseline_scores[n]\n",
    "        best_source = 'baseline_fallback'\n",
    "    \n",
    "    best_configs[n] = best_df\n",
    "    best_scores_by_n[n] = best_score\n",
    "    source_used[n] = best_source\n",
    "\n",
    "print(f\"\\nValidation failures (better score but overlapping): {len(validation_failures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final score\n",
    "final_score = sum(best_scores_by_n.values())\n",
    "print(f\"\\nFinal ensemble score: {final_score:.6f}\")\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - final_score:.6f}\")\n",
    "\n",
    "# Source breakdown\n",
    "source_counts = defaultdict(int)\n",
    "for n, source in source_used.items():\n",
    "    source_counts[source] += 1\n",
    "\n",
    "print(f\"\\nSource breakdown:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {source}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff025630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission CSV PRESERVING ORIGINAL PRECISION\n",
    "def generate_submission_preserve_precision(configs_by_n, output_path):\n",
    "    \"\"\"Generate submission preserving original string precision\"\"\"\n",
    "    rows = []\n",
    "    for n in range(1, 201):\n",
    "        df = configs_by_n[n]\n",
    "        for _, row in df.iterrows():\n",
    "            rows.append({\n",
    "                'id': f'{n:03d}_{row[\"tree_idx\"]}',\n",
    "                'x': row['x'],  # Keep original string!\n",
    "                'y': row['y'],  # Keep original string!\n",
    "                'deg': row['deg']  # Keep original string!\n",
    "            })\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "# Save submission\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission_df = generate_submission_preserve_precision(best_configs, '/home/submission/submission.csv')\n",
    "print(f\"Saved submission to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission_df)}\")\n",
    "\n",
    "# Verify precision is preserved\n",
    "print(f\"\\nVerifying N=2 precision in saved submission:\")\n",
    "for _, row in submission_df[submission_df['id'].str.startswith('002_')].iterrows():\n",
    "    print(f\"  {row['id']}: x={row['x']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc247dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved submission score\n",
    "verify_configs = parse_submission_with_strings('/home/submission/submission.csv')\n",
    "verify_total = 0\n",
    "for n in range(1, 201):\n",
    "    score = calculate_score_from_df(verify_configs[n], TX, TY)\n",
    "    verify_total += score\n",
    "\n",
    "print(f\"\\nVerification of saved submission: {verify_total:.6f}\")\n",
    "print(f\"Expected: {final_score:.6f}\")\n",
    "print(f\"Match: {abs(verify_total - final_score) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': verify_total,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - verify_total,\n",
    "    'validation_failures': len(validation_failures)\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/004_precision_fix/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved metrics to experiments/004_precision_fix/metrics.json\")\n",
    "print(f\"CV Score: {verify_total:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
