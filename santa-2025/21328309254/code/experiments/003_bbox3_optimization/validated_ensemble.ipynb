{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775c4fac",
   "metadata": {},
   "source": [
    "# Validated Ensemble with Strict Overlap Checking\n",
    "\n",
    "Scan all snapshots for better per-N configurations, validate strictly for overlaps, and fall back to baseline for any failing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddf5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "import json\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tree vertices\n",
    "TX = np.array([0,0.125,0.0625,0.2,0.1,0.35,0.075,0.075,-0.075,-0.075,-0.35,-0.1,-0.2,-0.0625,-0.125])\n",
    "TY = np.array([0.8,0.5,0.5,0.25,0.25,0,0,-0.2,-0.2,0,0,0.25,0.25,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    \"\"\"Calculate score for a single N configuration\"\"\"\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c = math.cos(r)\n",
    "        s = math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx = min(mnx, X)\n",
    "            mxx = max(mxx, X)\n",
    "            mny = min(mny, Y)\n",
    "            mxy = max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree_polygon(x, y, deg):\n",
    "    \"\"\"Create a Shapely polygon for a tree at given position and rotation\"\"\"\n",
    "    p = Polygon(zip(TX, TY))\n",
    "    p = affinity.rotate(p, deg, origin=(0,0))\n",
    "    p = affinity.translate(p, x, y)\n",
    "    return p\n",
    "\n",
    "def check_overlaps_strict(xs, ys, degs, buffer_dist=1e-6):\n",
    "    \"\"\"Check for overlaps with strict tolerance (buffer to catch near-overlaps)\"\"\"\n",
    "    n = len(xs)\n",
    "    if n <= 1:\n",
    "        return True, []\n",
    "    \n",
    "    # Create polygons with small buffer to catch near-overlaps\n",
    "    polygons = []\n",
    "    for i in range(n):\n",
    "        p = make_tree_polygon(xs[i], ys[i], degs[i])\n",
    "        # Apply negative buffer to be stricter (polygons must not touch)\n",
    "        polygons.append(p)\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            # Check if polygons overlap (not just touch)\n",
    "            if polygons[i].intersects(polygons[j]):\n",
    "                # Calculate minimum distance\n",
    "                dist = polygons[i].distance(polygons[j])\n",
    "                if dist < buffer_dist:\n",
    "                    # Check if they actually overlap (not just touch)\n",
    "                    intersection = polygons[i].intersection(polygons[j])\n",
    "                    if intersection.area > 1e-12:  # Non-trivial overlap\n",
    "                        overlaps.append((i, j, intersection.area))\n",
    "    \n",
    "    return len(overlaps) == 0, overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_submission(filepath):\n",
    "    \"\"\"Parse submission CSV and return dict of N -> (xs, ys, degs)\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Parse values (remove 's' prefix)\n",
    "    df['x_val'] = df['x'].str.replace('s', '').astype(float)\n",
    "    df['y_val'] = df['y'].str.replace('s', '').astype(float)\n",
    "    df['deg_val'] = df['deg'].str.replace('s', '').astype(float)\n",
    "    \n",
    "    # Extract N from id (format: NNN_idx)\n",
    "    df['N'] = df['id'].str.split('_').str[0].astype(int)\n",
    "    \n",
    "    configs = {}\n",
    "    for n, group in df.groupby('N'):\n",
    "        xs = group['x_val'].values\n",
    "        ys = group['y_val'].values\n",
    "        degs = group['deg_val'].values\n",
    "        configs[n] = (xs, ys, degs)\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155592d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_score(configs, tx, ty):\n",
    "    \"\"\"Calculate total score across all N values\"\"\"\n",
    "    total = 0.0\n",
    "    scores_by_n = {}\n",
    "    for n in range(1, 201):\n",
    "        if n in configs:\n",
    "            xs, ys, degs = configs[n]\n",
    "            score = score_group(xs, ys, degs, tx, ty)\n",
    "            scores_by_n[n] = score\n",
    "            total += score\n",
    "    return total, scores_by_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline (known valid)\n",
    "baseline_path = '/home/code/preoptimized/submission.csv'\n",
    "baseline_configs = parse_submission(baseline_path)\n",
    "baseline_total, baseline_scores = calculate_total_score(baseline_configs, TX, TY)\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "\n",
    "# Verify baseline passes strict overlap check for N=40 (the failing group)\n",
    "xs, ys, degs = baseline_configs[40]\n",
    "valid, overlaps = check_overlaps_strict(xs, ys, degs)\n",
    "print(f\"Baseline N=40 valid: {valid}, overlaps: {len(overlaps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all snapshot directories for submission files\n",
    "snapshot_base = '/home/nonroot/snapshots/santa-2025/'\n",
    "snapshot_dirs = os.listdir(snapshot_base)\n",
    "\n",
    "all_sources = {}\n",
    "all_sources['baseline'] = baseline_configs\n",
    "\n",
    "print(f\"Scanning {len(snapshot_dirs)} snapshot directories...\")\n",
    "\n",
    "for snap_dir in sorted(snapshot_dirs):\n",
    "    snap_path = os.path.join(snapshot_base, snap_dir)\n",
    "    \n",
    "    # Check for submission.csv in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(snap_path, 'submission', 'submission.csv'),\n",
    "        os.path.join(snap_path, 'code', 'submission.csv'),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                configs = parse_submission(path)\n",
    "                if len(configs) == 200:  # Valid submission has 200 N values\n",
    "                    total, _ = calculate_total_score(configs, TX, TY)\n",
    "                    all_sources[f'snap_{snap_dir}'] = configs\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "print(f\"Loaded {len(all_sources)} sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load preoptimized files\n",
    "preopt_files = [\n",
    "    '/home/code/preoptimized/smartmanoj_submission.csv',\n",
    "    '/home/code/preoptimized/saspav_best.csv',\n",
    "]\n",
    "\n",
    "for path in preopt_files:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            configs = parse_submission(path)\n",
    "            if len(configs) == 200:\n",
    "                name = os.path.basename(path).replace('.csv', '')\n",
    "                all_sources[name] = configs\n",
    "                total, _ = calculate_total_score(configs, TX, TY)\n",
    "                print(f\"Loaded {name}: {total:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2292bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N, find the best configuration that passes strict overlap validation\n",
    "print(\"\\nFinding best valid configuration for each N...\")\n",
    "\n",
    "best_configs = {}\n",
    "best_scores_by_n = {}\n",
    "source_used = {}\n",
    "validation_failures = []\n",
    "\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_config = None\n",
    "    best_source = None\n",
    "    \n",
    "    # Collect all candidates for this N\n",
    "    candidates = []\n",
    "    for source_name, configs in all_sources.items():\n",
    "        if n in configs:\n",
    "            xs, ys, degs = configs[n]\n",
    "            score = score_group(xs, ys, degs, TX, TY)\n",
    "            candidates.append((score, source_name, (xs, ys, degs)))\n",
    "    \n",
    "    # Sort by score (best first)\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Try each candidate until we find one that passes validation\n",
    "    for score, source_name, config in candidates:\n",
    "        xs, ys, degs = config\n",
    "        valid, overlaps = check_overlaps_strict(xs, ys, degs)\n",
    "        \n",
    "        if valid:\n",
    "            best_score = score\n",
    "            best_config = config\n",
    "            best_source = source_name\n",
    "            break\n",
    "        else:\n",
    "            if source_name != 'baseline':\n",
    "                validation_failures.append((n, source_name, score, len(overlaps)))\n",
    "    \n",
    "    # If no valid config found, use baseline (should always be valid)\n",
    "    if best_config is None:\n",
    "        xs, ys, degs = baseline_configs[n]\n",
    "        best_score = baseline_scores[n]\n",
    "        best_config = (xs, ys, degs)\n",
    "        best_source = 'baseline_fallback'\n",
    "    \n",
    "    best_configs[n] = best_config\n",
    "    best_scores_by_n[n] = best_score\n",
    "    source_used[n] = best_source\n",
    "\n",
    "print(f\"\\nValidation failures (better score but overlapping): {len(validation_failures)}\")\n",
    "for n, source, score, num_overlaps in validation_failures[:10]:\n",
    "    print(f\"  N={n}: {source} score={score:.6f} overlaps={num_overlaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final score\n",
    "final_score = sum(best_scores_by_n.values())\n",
    "print(f\"\\nFinal ensemble score: {final_score:.6f}\")\n",
    "print(f\"Baseline score: {baseline_total:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - final_score:.6f}\")\n",
    "\n",
    "# Source breakdown\n",
    "source_counts = defaultdict(int)\n",
    "for n, source in source_used.items():\n",
    "    source_counts[source] += 1\n",
    "\n",
    "print(f\"\\nSource breakdown:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {source}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which N values improved\n",
    "print(\"\\nN values with improvement over baseline:\")\n",
    "improvements = []\n",
    "for n in range(1, 201):\n",
    "    if best_scores_by_n[n] < baseline_scores[n] - 1e-9:\n",
    "        imp = baseline_scores[n] - best_scores_by_n[n]\n",
    "        improvements.append((n, imp, source_used[n]))\n",
    "\n",
    "improvements.sort(key=lambda x: -x[1])\n",
    "for n, imp, source in improvements[:20]:\n",
    "    print(f\"  N={n}: +{imp:.6f} from {source}\")\n",
    "\n",
    "print(f\"\\nTotal improvements: {len(improvements)} N values\")\n",
    "print(f\"Total improvement: {sum(x[1] for x in improvements):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edb465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission CSV\n",
    "def generate_submission(configs, output_path):\n",
    "    rows = []\n",
    "    for n in range(1, 201):\n",
    "        xs, ys, degs = configs[n]\n",
    "        for i in range(len(xs)):\n",
    "            row = {\n",
    "                'id': f'{n:03d}_{i}',\n",
    "                'x': f's{xs[i]}',\n",
    "                'y': f's{ys[i]}',\n",
    "                'deg': f's{degs[i]}'\n",
    "            }\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "# Save submission\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission_df = generate_submission(best_configs, '/home/submission/submission.csv')\n",
    "print(f\"Saved submission to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857eb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved submission\n",
    "verify_configs = parse_submission('/home/submission/submission.csv')\n",
    "verify_total, verify_scores = calculate_total_score(verify_configs, TX, TY)\n",
    "print(f\"\\nVerification of saved submission: {verify_total:.6f}\")\n",
    "print(f\"Expected: {final_score:.6f}\")\n",
    "print(f\"Match: {abs(verify_total - final_score) < 1e-6}\")\n",
    "\n",
    "# Double-check N=40 passes validation\n",
    "xs, ys, degs = verify_configs[40]\n",
    "valid, overlaps = check_overlaps_strict(xs, ys, degs)\n",
    "print(f\"\\nN=40 validation: valid={valid}, overlaps={len(overlaps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e92545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': verify_total,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - verify_total,\n",
    "    'num_improvements': len(improvements),\n",
    "    'validation_failures': len(validation_failures)\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/003_bbox3_optimization/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved metrics to experiments/003_bbox3_optimization/metrics.json\")\n",
    "print(f\"CV Score: {verify_total:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
