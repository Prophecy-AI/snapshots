# Santa 2025 - Christmas Tree Packing Optimization
## Evolved Strategy - Loop 3

## Current Status
- Best CV score: 70.659944 (exp_001 - but REJECTED due to overlaps)
- Best VALID CV score: 70.676102 (exp_000, exp_002)
- Best LB score: 70.676102 (exp_000)
- Target: 68.919154 | Gap to target: 1.756948 (2.5%)
- Submissions used: 2/100 (98 remaining - ABUNDANT!)

## Public Kernel Status (CRITICAL!)
- Have we implemented the best kernel yet? **NO**
- Top kernels identified:
  1. jonathanchan/santa25-ensemble-sa-fractional-translation (~70.5)
  2. yongsukprasertsuk/santa-2025-best-keeping-bbox3-runner
  3. crodoc/74-75-backpacking-christmas-trees (backward propagation)
- Kernels we've implemented: Basic bbox3, tree_packer_v21 (short runs only)
- **CRITICAL**: Fractional translation NOT implemented yet!

## Response to Evaluator

The evaluator correctly identified several key gaps:

1. **Fractional Translation NOT Applied** - AGREED. This is the #1 priority.
   The jonathanchan kernel achieves ~70.5 using fractional translation post-processing.
   We're at 70.676 - this technique alone could give us 0.17+ points.

2. **Ensemble Needs Diverse Sources** - AGREED. We have 728 CSV files in snapshots
   but haven't properly ensembled them. Need to take best N from each source.

3. **Perturbed Restarts Need Larger Perturbations** - AGREED. The 0.05-0.2 perturbations
   converged to worse optima. Need 0.5-1.0 or completely different starting points.

4. **Symmetric Search Was Too Limited** - AGREED. Only tried N=1,2,4 with coarse grids.
   Need finer search for N=2-20.

## Key Insight: Target is VERY Aggressive

The target of 68.919 is ~1.58 points BETTER than the best public kernel (~70.5).
This means we need to:
1. FIRST match the best public kernel (fractional translation → ~70.5)
2. THEN find 1.58 MORE points through innovation

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Implement Fractional Translation
Extract from jonathanchan's C++ code and apply to baseline:
- Steps: [0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]
- Directions: 8 (up, down, left, right, 4 diagonals)
- For each tree, try tiny movements, keep if bbox improves without overlap
- This is LOW-RISK and could give 0.17+ points improvement

### 2. **[HIGH PRIORITY]** Build Proper Ensemble from All Snapshots
- Scan all 728 CSV files in snapshots
- For each N (1-200), find the configuration with smallest bounding box
- Validate no overlaps in the ensemble
- Apply fractional translation to the ensemble result

### 3. **[MEDIUM PRIORITY]** Run MUCH Longer Optimization
The top solutions run for HOURS:
- 15000-20000 iterations (not 3000)
- 80+ rounds (not 30)
- Population-based (keep top 3 candidates)
Run bbox3 for 30+ minutes on the ensemble result.

### 4. **[MEDIUM PRIORITY]** Backward Propagation (bp.cpp)
Systematically improve smaller N by removing trees from larger N:
- Start from N=200, work down to N=2
- For each config, try removing boundary-touching trees
- Keep if (N-1) config improves

### 5. **[LOWER PRIORITY]** Lattice-Based Packing for Large N
For N > 100, try grid-based placements (nx × ny grids).
This is fundamentally different from random optimization.

## What NOT to Try
- Short optimization runs (already proven ineffective)
- Small perturbations (0.05-0.2) - converge to worse optima
- Simple ensemble without overlap validation (caused rejection)
- Symmetric packing with coarse grids (already tried, no improvement)

## Validation Notes
- MUST use strict overlap detection (scale_factor=1e15)
- Check intersection.area > 0 (not > 1e-10)
- Validate ALL 200 groups before submission
- The previous submission was REJECTED due to overlaps in group 069

## SUBMISSION STRATEGY
- Remaining submissions: 98 (ABUNDANT!)
- Submit after EVERY valid experiment
- LB feedback is FREE information - use it!
- Even if score doesn't improve, we learn about CV-LB relationship

## Implementation Notes

### Fractional Translation (Python implementation):
```python
def fractional_translation(trees, n, max_iter=200):
    """Apply fractional translation to squeeze out improvements"""
    frac_steps = [0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]
    dx = [0, 0, 1, -1, 1, 1, -1, -1]
    dy = [1, -1, 0, 0, 1, -1, 1, -1]
    
    best_side = get_bounding_box_side(trees)
    
    for iteration in range(max_iter):
        improved = False
        for i in range(n):
            for step in frac_steps:
                for d in range(8):
                    # Try moving tree i by (dx[d]*step, dy[d]*step)
                    old_x, old_y = trees[i].x, trees[i].y
                    trees[i].x += dx[d] * step
                    trees[i].y += dy[d] * step
                    trees[i].update_polygon()
                    
                    if not has_overlap(trees, i):
                        new_side = get_bounding_box_side(trees)
                        if new_side < best_side - 1e-12:
                            best_side = new_side
                            improved = True
                        else:
                            # Revert
                            trees[i].x, trees[i].y = old_x, old_y
                            trees[i].update_polygon()
                    else:
                        # Revert
                        trees[i].x, trees[i].y = old_x, old_y
                        trees[i].update_polygon()
        
        if not improved:
            break
    
    return trees, best_side
```

### Ensemble Building:
```python
def build_ensemble(csv_files):
    """Build ensemble by taking best N from each source"""
    best = {n: {'score': float('inf'), 'data': None} for n in range(1, 201)}
    
    for csv_path in csv_files:
        df = pd.read_csv(csv_path)
        for n in range(1, 201):
            trees = load_trees_for_n(df, n)
            if not has_overlap(trees):
                score = get_score(trees, n)
                if score < best[n]['score']:
                    best[n]['score'] = score
                    best[n]['data'] = trees
    
    return best
```

## Expected Outcome
1. Fractional translation: 70.676 → ~70.5 (0.17 points)
2. Proper ensemble: ~70.5 → ~70.3 (0.2 points)
3. Longer optimization: ~70.3 → ~70.0 (0.3 points)
4. Innovation needed: ~70.0 → 68.919 (1.08 points)

The last 1.08 points will require techniques beyond public kernels.