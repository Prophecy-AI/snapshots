{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9f79c3",
   "metadata": {},
   "source": [
    "# Experiment 006: Find Better Complete Snapshot\n",
    "\n",
    "Strategy: The ensemble approach is broken (5 failed submissions). We need to find a COMPLETE snapshot that:\n",
    "1. Has a better score than baseline (70.676102)\n",
    "2. Is used AS-IS without mixing configurations\n",
    "3. Should pass Kaggle validation since it's from a known-good source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/code')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from utils import load_submission, score_submission, is_valid_submission\n",
    "import json\n",
    "\n",
    "print(\"Utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d31575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files in snapshots\n",
    "snapshot_dir = '/home/nonroot/snapshots/santa-2025/'\n",
    "all_csvs = glob.glob(f'{snapshot_dir}/**/*.csv', recursive=True)\n",
    "print(f\"Found {len(all_csvs)} CSV files in snapshots\")\n",
    "\n",
    "# Filter to valid submissions only\n",
    "valid_submissions = []\n",
    "for csv_path in all_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if is_valid_submission(df):\n",
    "            valid_submissions.append(csv_path)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f\"Found {len(valid_submissions)} valid submission files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score each complete submission and find the best ones\n",
    "scores = []\n",
    "\n",
    "for i, csv_path in enumerate(valid_submissions):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        score, _, _ = score_submission(df, check_overlaps=False)\n",
    "        scores.append((score, csv_path))\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(valid_submissions)} submissions...\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nScored {len(scores)} submissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by score (lower is better)\n",
    "scores.sort(key=lambda x: x[0])\n",
    "\n",
    "# Show top 20 best scores\n",
    "print(\"Top 20 best complete submissions:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (score, path) in enumerate(scores[:20]):\n",
    "    short_path = '/'.join(path.split('/')[-3:])\n",
    "    print(f\"{i+1:2d}. Score: {score:.6f} - {short_path}\")\n",
    "\n",
    "print(f\"\\nBaseline score: 70.676102\")\n",
    "print(f\"Best found: {scores[0][0]:.6f}\")\n",
    "print(f\"Improvement over baseline: {70.676102 - scores[0][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the best submission is different from our baseline\n",
    "best_score, best_path = scores[0]\n",
    "print(f\"\\nBest submission: {best_path}\")\n",
    "print(f\"Score: {best_score:.6f}\")\n",
    "\n",
    "# Load and compare with baseline\n",
    "baseline_path = '/home/code/experiments/000_baseline/submission.csv'\n",
    "baseline_df = pd.read_csv(baseline_path)\n",
    "best_df = pd.read_csv(best_path)\n",
    "\n",
    "# Check if they're the same\n",
    "if baseline_df.equals(best_df):\n",
    "    print(\"\\nBest submission is IDENTICAL to baseline.\")\n",
    "else:\n",
    "    print(\"\\nBest submission is DIFFERENT from baseline!\")\n",
    "    # Check how many rows differ\n",
    "    diff_count = (baseline_df != best_df).sum().sum()\n",
    "    print(f\"Number of differing values: {diff_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d911d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If best is different and better, use it\n",
    "if best_score < 70.676102 - 0.0001:  # At least 0.0001 improvement\n",
    "    print(f\"\\nFound better submission!\")\n",
    "    print(f\"Score: {best_score:.6f} (improvement: {70.676102 - best_score:.6f})\")\n",
    "    \n",
    "    # Copy to experiment folder and submission folder\n",
    "    import shutil\n",
    "    work_dir = '/home/code/experiments/006_find_better_snapshot'\n",
    "    \n",
    "    shutil.copy(best_path, f'{work_dir}/submission.csv')\n",
    "    shutil.copy(best_path, '/home/submission/submission.csv')\n",
    "    print(f\"Copied to {work_dir}/submission.csv and /home/submission/submission.csv\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'cv_score': best_score,\n",
    "        'baseline_score': 70.676102,\n",
    "        'improvement': 70.676102 - best_score,\n",
    "        'source_path': best_path\n",
    "    }\n",
    "    with open(f'{work_dir}/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Metrics saved: {metrics}\")\n",
    "else:\n",
    "    print(f\"\\nNo better complete submission found.\")\n",
    "    print(f\"Best score: {best_score:.6f}\")\n",
    "    print(f\"Baseline score: 70.676102\")\n",
    "    \n",
    "    # Use baseline as fallback\n",
    "    import shutil\n",
    "    work_dir = '/home/code/experiments/006_find_better_snapshot'\n",
    "    shutil.copy(baseline_path, f'{work_dir}/submission.csv')\n",
    "    shutil.copy(baseline_path, '/home/submission/submission.csv')\n",
    "    \n",
    "    metrics = {\n",
    "        'cv_score': 70.676102,\n",
    "        'baseline_score': 70.676102,\n",
    "        'improvement': 0.0,\n",
    "        'notes': 'No better complete snapshot found, using baseline'\n",
    "    }\n",
    "    with open(f'{work_dir}/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Using baseline as fallback.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 006: FIND BETTER SNAPSHOT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total submissions scanned: {len(valid_submissions)}\")\n",
    "print(f\"Baseline score: 70.676102\")\n",
    "print(f\"Best complete snapshot: {best_score:.6f}\")\n",
    "print(f\"Improvement: {70.676102 - best_score:.6f}\")\n",
    "print(f\"\\nTarget: 68.888293\")\n",
    "print(f\"Gap to target: {best_score - 68.888293:.6f} ({(best_score - 68.888293) / 68.888293 * 100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
