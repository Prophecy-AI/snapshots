# Santa 2025 - Christmas Tree Packing Optimization

## Current Status
- Best CV score: 70.675672 from exp_003 (004_eazy_long_run)
- Best LB score: 70.676059 (from exp_002)
- Target: 68.891380 | Gap to target: 1.78 points (2.59%)

## ⚠️ CRITICAL: WE ARE STUCK AT A LOCAL OPTIMUM ⚠️

**Evidence:**
- 4 experiments have produced only 0.000430 total improvement
- At this rate, it would take 6,224 HOURS to reach target
- All external CSV sources (telegram, santa25-public, bucket-of-chump) are WORSE than baseline
- Local optimization (SA, bbox3, eazy) CANNOT close this gap

**The baseline is at a GLOBAL optimum for its configuration TYPE.**
**To beat it, we need a DIFFERENT configuration type.**

## Public Kernel Status
- Have we implemented the best kernel yet? YES - we've tried bbox3, eazy, SA
- Top kernels identified: saspav, jonathanchan, jazivxt, smartmanoj
- **KEY INSIGHT**: All public kernels converge to the SAME local optimum (~70.67)
- The target score (68.89) requires approaches NOT in public kernels

## Response to Evaluator

The evaluator correctly identified that:
1. Local optimization cannot close the 1.78-point gap (CONFIRMED)
2. We need fundamentally different configurations (AGREED)
3. Running optimizers longer is not viable (CONFIRMED - 6,224 hours needed)

**Where I disagree with evaluator:**
- Running sa_v1_parallel for hours won't help - it's still local optimization
- External datasets won't help - they're ALL worse than baseline (verified)

**What we need instead:**
- RANDOM RESTARTS with completely different initial configurations
- GENETIC ALGORITHM with population diversity
- CONSTRUCTIVE HEURISTICS that build solutions from scratch

## CV-LB Relationship Analysis
- CV = LB exactly (within floating point precision)
- This is expected for deterministic optimization
- Any valid CV improvement will directly translate to LB improvement

## Recommended Approaches (Priority Order)

### 1. [HIGHEST PRIORITY] Random Restarts with Different Initial Configurations
The current solution uses ~63° and ~243° (blue/pink pattern).
Try completely different starting points:
- All trees at 0° (upright)
- All trees at 90° (sideways)
- Random angles for each tree
- Different tessellation patterns (hexagonal, diagonal)

```python
# For each N, generate 10-100 random initial configurations
# Run SA on each for 5-10 seconds
# Keep the best result
# This explores different basins of attraction
for n in range(1, 201):
    best_score = current_score[n]
    for trial in range(100):
        config = generate_random_config(n)
        optimized = run_sa(config, time=5)
        if optimized.score < best_score:
            best_score = optimized.score
            save_config(n, optimized)
```

### 2. [HIGH PRIORITY] Genetic Algorithm with Population Diversity
Maintain diverse population of solutions:
- Crossover: swap partial configurations between solutions
- Mutation: random perturbations to angles/positions
- Selection: keep best + diverse solutions (not just best)

Key: Diversity is more important than local optimization!

### 3. [MEDIUM PRIORITY] Constructive Heuristics
Build solutions tree-by-tree from scratch:
- Bottom-left placement with rotation search
- Greedy with backtracking
- Integer programming for small N (exact solutions)

### 4. [MEDIUM PRIORITY] Small N Exact Solutions
For N=2-10, try exhaustive search:
- Fine angle grid (0.1° steps)
- All possible relative positions
- Find provably optimal solutions

## What NOT to Try (Proven Ineffective)
- ❌ More SA/bbox3/eazy optimization (converges to same optimum)
- ❌ Longer optimization runs (6,224 hours not viable)
- ❌ External CSV ensemble (all worse than baseline)
- ❌ Parameter tuning on existing optimizers
- ❌ Backward propagation (already tried, no improvement)
- ❌ Tessellation with standard blue/pink pattern (already optimal for that pattern)

## Validation Notes
- CV = LB exactly for this deterministic optimization problem
- MUST verify no overlaps before submission using Shapely
- Use area > 1e-10 threshold for overlap detection
- Submission format: x, y, deg with 's' prefix

## SUBMISSION STRATEGY
- Remaining submissions: 96
- Submit after EVERY experiment that produces valid submission
- LB feedback is free information - USE IT!
- Even if score is worse, we learn what DOESN'T work

## Key Questions to Answer
1. Can random restarts find different basins of attraction?
2. What do optimal small-N solutions look like? (N=2, 3, 4, 5)
3. Are there fundamentally different packing patterns that work better?
4. Can genetic algorithms maintain diversity while improving?

## First Experiment Recommendation

**Implement Random Restarts for Small N (N=2-20)**

1. For each N from 2 to 20:
   - Generate 100 random initial configurations
   - Each config: random positions within bounding box, random angles
   - Run SA for 5 seconds on each
   - Keep best result

2. Compare with baseline:
   - If ANY N improves, we've found a different basin!
   - If no improvement, the baseline is truly optimal for small N

3. If small N improves:
   - Scale up to all N
   - Use improved small N as seeds for larger N

This tests whether the local optimum is actually global, or if there are better configurations we haven't found.

## Code Resources
- Experiments: `/home/code/experiments/`
- Kernels: `/home/code/research/kernels/`
- Snapshots: `/home/nonroot/snapshots/santa-2025/`
- Current best: `/home/code/experiments/003_long_sa/submission_eazy_longer.csv`
