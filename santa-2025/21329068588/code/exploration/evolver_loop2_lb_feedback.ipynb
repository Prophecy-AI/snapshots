{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5073bd7e",
   "metadata": {},
   "source": [
    "# Loop 2 LB Feedback Analysis\n",
    "\n",
    "## Key Findings:\n",
    "- LB score 70.627582 confirmed (CV-LB gap: 0.0000)\n",
    "- Better validated submission exists at 70.626088\n",
    "- Target: 68.892432 | Gap: 1.73 points (2.5%)\n",
    "\n",
    "## Strategy:\n",
    "1. Find best validated submission\n",
    "2. Ensemble best N values from multiple validated snapshots\n",
    "3. Test bbox3 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4275b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from numba import njit\n",
    "from glob import glob\n",
    "\n",
    "# Tree geometry\n",
    "TX = np.array([0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125])\n",
    "TY = np.array([0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5])\n",
    "\n",
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c, s = math.cos(r), math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx, mxx = min(mnx, X), max(mxx, X)\n",
    "            mny, mxy = min(mny, Y), max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n\n",
    "\n",
    "print('Scoring functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_scores(df):\n",
    "    \"\"\"Get score for each N value\"\"\"\n",
    "    scores = {}\n",
    "    for n in range(1, 201):\n",
    "        mask = df['id'].str.startswith(f'{n:03d}_')\n",
    "        group = df[mask]\n",
    "        if len(group) != n:\n",
    "            continue\n",
    "        xs = group['x'].str[1:].astype(float).values\n",
    "        ys = group['y'].str[1:].astype(float).values\n",
    "        degs = group['deg'].str[1:].astype(float).values\n",
    "        scores[n] = score_group(xs, ys, degs, TX, TY)\n",
    "    return scores\n",
    "\n",
    "def total_score(scores_dict):\n",
    "    return sum(scores_dict.values())\n",
    "\n",
    "print('Helper functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all validated snapshots with LB scores\n",
    "snapshot_base = '/home/nonroot/snapshots/santa-2025'\n",
    "validated_snapshots = []\n",
    "\n",
    "for snap_dir in sorted(os.listdir(snapshot_base)):\n",
    "    session_path = os.path.join(snapshot_base, snap_dir, 'code', 'session_state.json')\n",
    "    submission_path = os.path.join(snapshot_base, snap_dir, 'code', 'submission.csv')\n",
    "    \n",
    "    if os.path.exists(session_path) and os.path.exists(submission_path):\n",
    "        try:\n",
    "            with open(session_path) as f:\n",
    "                state = json.load(f)\n",
    "            \n",
    "            # Find successful submissions\n",
    "            for sub in state.get('submissions', []):\n",
    "                lb = sub.get('lb_score')\n",
    "                if lb and lb != '' and not sub.get('error'):\n",
    "                    validated_snapshots.append({\n",
    "                        'dir': snap_dir,\n",
    "                        'lb_score': float(lb),\n",
    "                        'submission_path': submission_path\n",
    "                    })\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f'Found {len(validated_snapshots)} validated snapshots')\n",
    "validated_snapshots = sorted(validated_snapshots, key=lambda x: x['lb_score'])\n",
    "print('\\nTop 10 validated snapshots:')\n",
    "for v in validated_snapshots[:10]:\n",
    "    print(f\"  LB: {v['lb_score']:.12f} - {v['dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best validated submission\n",
    "best_snap = validated_snapshots[0]\n",
    "print(f\"Best validated: {best_snap['lb_score']:.12f}\")\n",
    "\n",
    "df_best = pd.read_csv(best_snap['submission_path'])\n",
    "scores_best = get_n_scores(df_best)\n",
    "print(f\"Total score: {total_score(scores_best):.6f}\")\n",
    "print(f\"\\nTop 10 N values by contribution:\")\n",
    "for n, s in sorted(scores_best.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  N={n:3d}: {s:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to ensemble best N values from multiple validated snapshots\n",
    "# Load top 5 validated submissions\n",
    "top_submissions = []\n",
    "for v in validated_snapshots[:5]:\n",
    "    df = pd.read_csv(v['submission_path'])\n",
    "    scores = get_n_scores(df)\n",
    "    top_submissions.append({\n",
    "        'dir': v['dir'],\n",
    "        'lb_score': v['lb_score'],\n",
    "        'df': df,\n",
    "        'scores': scores\n",
    "    })\n",
    "    print(f\"Loaded {v['dir']}: total={total_score(scores):.6f}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(top_submissions)} submissions for ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best N from each submission\n",
    "best_n_scores = {}\n",
    "best_n_source = {}\n",
    "\n",
    "for n in range(1, 201):\n",
    "    best_score = float('inf')\n",
    "    best_src = None\n",
    "    \n",
    "    for sub in top_submissions:\n",
    "        if n in sub['scores'] and sub['scores'][n] < best_score:\n",
    "            best_score = sub['scores'][n]\n",
    "            best_src = sub['dir']\n",
    "    \n",
    "    if best_src:\n",
    "        best_n_scores[n] = best_score\n",
    "        best_n_source[n] = best_src\n",
    "\n",
    "ensemble_total = sum(best_n_scores.values())\n",
    "print(f\"\\nEnsemble total score: {ensemble_total:.6f}\")\n",
    "print(f\"Best single submission: {top_submissions[0]['lb_score']:.6f}\")\n",
    "print(f\"Improvement: {top_submissions[0]['lb_score'] - ensemble_total:.6f}\")\n",
    "\n",
    "# Count how many N values come from each source\n",
    "from collections import Counter\n",
    "source_counts = Counter(best_n_source.values())\n",
    "print(f\"\\nN values per source:\")\n",
    "for src, count in source_counts.most_common():\n",
    "    print(f\"  {src}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40571964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ensemble is actually different from best single\n",
    "if ensemble_total < top_submissions[0]['lb_score'] - 1e-6:\n",
    "    print(\"Ensemble is better! Creating ensemble submission...\")\n",
    "    \n",
    "    # Build ensemble submission\n",
    "    ensemble_rows = []\n",
    "    for n in range(1, 201):\n",
    "        best_src = best_n_source[n]\n",
    "        for sub in top_submissions:\n",
    "            if sub['dir'] == best_src:\n",
    "                mask = sub['df']['id'].str.startswith(f'{n:03d}_')\n",
    "                group = sub['df'][mask]\n",
    "                for _, row in group.iterrows():\n",
    "                    ensemble_rows.append(row)\n",
    "                break\n",
    "    \n",
    "    ensemble_df = pd.DataFrame(ensemble_rows)\n",
    "    \n",
    "    # Verify score\n",
    "    ensemble_scores = get_n_scores(ensemble_df)\n",
    "    print(f\"Ensemble verified score: {total_score(ensemble_scores):.6f}\")\n",
    "else:\n",
    "    print(\"Ensemble is NOT better than best single submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582924e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bbox3 is available and test it\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Copy bbox3 from snapshot\n",
    "bbox3_src = '/home/nonroot/snapshots/santa-2025/21198927060/code/bbox3'\n",
    "bbox3_dst = '/home/code/bbox3'\n",
    "\n",
    "if os.path.exists(bbox3_src):\n",
    "    shutil.copy(bbox3_src, bbox3_dst)\n",
    "    os.chmod(bbox3_dst, 0o755)\n",
    "    print(f\"Copied bbox3 to {bbox3_dst}\")\n",
    "    \n",
    "    # Test bbox3\n",
    "    result = subprocess.run([bbox3_dst, '-h'], capture_output=True, text=True, timeout=5)\n",
    "    print(f\"\\nbbox3 help:\")\n",
    "    print(result.stdout[:500] if result.stdout else result.stderr[:500])\n",
    "else:\n",
    "    print(f\"bbox3 not found at {bbox3_src}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60155cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best validated submission to /home/submission\n",
    "import shutil\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "\n",
    "# Use the best validated submission (70.626088)\n",
    "best_submission_path = validated_snapshots[0]['submission_path']\n",
    "shutil.copy(best_submission_path, '/home/submission/submission.csv')\n",
    "\n",
    "print(f\"Copied best validated submission to /home/submission/submission.csv\")\n",
    "print(f\"Expected LB score: {validated_snapshots[0]['lb_score']:.12f}\")\n",
    "\n",
    "# Verify\n",
    "df_verify = pd.read_csv('/home/submission/submission.csv')\n",
    "scores_verify = get_n_scores(df_verify)\n",
    "print(f\"Verified score: {total_score(scores_verify):.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
