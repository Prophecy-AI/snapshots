{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e12a367",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: Understanding the Overlap Issue\n",
    "\n",
    "The submission failed with 'Overlapping trees in group 008'. Let's analyze:\n",
    "1. Why did bbox3 introduce overlaps?\n",
    "2. What's the difference between validated and rejected submissions?\n",
    "3. What approaches can actually improve the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Tree geometry\n",
    "TX = np.array([0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125])\n",
    "TY = np.array([0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5])\n",
    "\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the validation status of different snapshots\n",
    "print('=== Checking snapshot validation status ===')\n",
    "\n",
    "# Check 21198893057 (validated)\n",
    "path1 = '/home/nonroot/snapshots/santa-2025/21198893057/code/session_state.json'\n",
    "if os.path.exists(path1):\n",
    "    with open(path1) as f:\n",
    "        state1 = json.load(f)\n",
    "    print(f'\\n21198893057 submissions (last 5):')\n",
    "    for sub in state1.get('submissions', [])[-5:]:\n",
    "        lb = sub.get('lb_score', 'N/A')\n",
    "        err = sub.get('error', None)\n",
    "        print(f\"  CV: {sub.get('cv_score', 'N/A'):.6f}, LB: {lb}, Error: {err}\")\n",
    "\n",
    "# Check 21198927060 (source of rejected)\n",
    "path2 = '/home/nonroot/snapshots/santa-2025/21198927060/code/session_state.json'\n",
    "if os.path.exists(path2):\n",
    "    with open(path2) as f:\n",
    "        state2 = json.load(f)\n",
    "    print(f'\\n21198927060 submissions (last 5):')\n",
    "    for sub in state2.get('submissions', [])[-5:]:\n",
    "        lb = sub.get('lb_score', 'N/A')\n",
    "        err = sub.get('error', None)\n",
    "        print(f\"  CV: {sub.get('cv_score', 'N/A'):.6f}, LB: {lb}, Error: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b313f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY INSIGHT: Check if 21198927060's submission.csv was ever validated\n",
    "print('=== Checking if 21198927060 submission was validated ===')\n",
    "\n",
    "# The submission.csv in 21198927060 scores 70.624424\n",
    "# But was it ever submitted to Kaggle?\n",
    "\n",
    "if os.path.exists(path2):\n",
    "    with open(path2) as f:\n",
    "        state2 = json.load(f)\n",
    "    \n",
    "    # Find submissions with score close to 70.624424\n",
    "    for sub in state2.get('submissions', []):\n",
    "        cv = sub.get('cv_score', 0)\n",
    "        if abs(cv - 70.624424) < 0.001:\n",
    "            print(f\"Found matching submission:\")\n",
    "            print(f\"  CV: {cv}\")\n",
    "            print(f\"  LB: {sub.get('lb_score', 'N/A')}\")\n",
    "            print(f\"  Error: {sub.get('error', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all submissions in 21198927060 to see which ones passed\n",
    "print('=== All submissions in 21198927060 ===')\n",
    "if os.path.exists(path2):\n",
    "    with open(path2) as f:\n",
    "        state2 = json.load(f)\n",
    "    \n",
    "    passed = []\n",
    "    failed = []\n",
    "    for sub in state2.get('submissions', []):\n",
    "        cv = sub.get('cv_score', 0)\n",
    "        lb = sub.get('lb_score', None)\n",
    "        err = sub.get('error', None)\n",
    "        \n",
    "        if lb and not err:\n",
    "            passed.append((cv, lb))\n",
    "        elif err:\n",
    "            failed.append((cv, err))\n",
    "    \n",
    "    print(f'\\nPassed submissions: {len(passed)}')\n",
    "    for cv, lb in passed[-5:]:\n",
    "        print(f'  CV: {cv:.6f}, LB: {lb}')\n",
    "    \n",
    "    print(f'\\nFailed submissions: {len(failed)}')\n",
    "    for cv, err in failed[-5:]:\n",
    "        print(f'  CV: {cv:.6f}, Error: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af14684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FINDING: The submission.csv in 21198927060 (70.624424) was NEVER validated!\n",
    "# It was the result of further optimization AFTER the last validated submission.\n",
    "# The last validated submission in that snapshot was 70.626088.\n",
    "\n",
    "# Let's find the best validated submission across all snapshots\n",
    "print('=== Finding best validated submission ===')\n",
    "\n",
    "snapshot_dir = '/home/nonroot/snapshots/santa-2025'\n",
    "best_validated = None\n",
    "best_lb = float('inf')\n",
    "\n",
    "for snap in os.listdir(snapshot_dir):\n",
    "    state_path = os.path.join(snapshot_dir, snap, 'code', 'session_state.json')\n",
    "    if os.path.exists(state_path):\n",
    "        with open(state_path) as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        for sub in state.get('submissions', []):\n",
    "            lb = sub.get('lb_score', None)\n",
    "            err = sub.get('error', None)\n",
    "            if lb and not err and isinstance(lb, (int, float)):\n",
    "                if lb < best_lb:\n",
    "                    best_lb = lb\n",
    "                    best_validated = {\n",
    "                        'snapshot': snap,\n",
    "                        'cv_score': sub.get('cv_score'),\n",
    "                        'lb_score': lb,\n",
    "                        'model': sub.get('model_name')\n",
    "                    }\n",
    "\n",
    "print(f'\\nBest validated submission:')\n",
    "print(f'  Snapshot: {best_validated[\"snapshot\"]}')\n",
    "print(f'  CV: {best_validated[\"cv_score\"]}')\n",
    "print(f'  LB: {best_validated[\"lb_score\"]}')\n",
    "print(f'  Model: {best_validated[\"model\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's understand the gap and what approaches might work\n",
    "print('=== Gap Analysis ===')\n",
    "target = 68.892266\n",
    "best_lb = 70.626088  # Best validated\n",
    "\n",
    "gap = best_lb - target\n",
    "gap_pct = (gap / target) * 100\n",
    "\n",
    "print(f'Target: {target}')\n",
    "print(f'Best validated LB: {best_lb}')\n",
    "print(f'Gap: {gap:.6f} ({gap_pct:.2f}%)')\n",
    "\n",
    "print('\\n=== What this means ===')\n",
    "print('1. We need to improve by 1.73 points (2.5%)')\n",
    "print('2. bbox3 optimizer showed ZERO improvement on pre-optimized solution')\n",
    "print('3. The solution is at a local optimum')\n",
    "print('4. We need fundamentally different approaches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2067a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what approaches have been tried in the snapshots\n",
    "print('=== Approaches tried in snapshots ===')\n",
    "\n",
    "for snap in ['21198927060', '21198893057']:\n",
    "    state_path = os.path.join(snapshot_dir, snap, 'code', 'session_state.json')\n",
    "    if os.path.exists(state_path):\n",
    "        with open(state_path) as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        print(f'\\nSnapshot {snap}:')\n",
    "        experiments = state.get('experiments', [])\n",
    "        print(f'  Total experiments: {len(experiments)}')\n",
    "        \n",
    "        # Show unique model types\n",
    "        model_types = set()\n",
    "        for exp in experiments:\n",
    "            model_types.add(exp.get('model_type', 'unknown'))\n",
    "        print(f'  Model types: {model_types}')\n",
    "        \n",
    "        # Show best scores\n",
    "        scores = [exp.get('cv_score', float('inf')) for exp in experiments]\n",
    "        if scores:\n",
    "            print(f'  Best CV: {min(scores):.6f}')\n",
    "            print(f'  Worst CV: {max(scores):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18138e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGIC CONCLUSION\n",
    "print('=' * 60)\n",
    "print('STRATEGIC CONCLUSION')\n",
    "print('=' * 60)\n",
    "\n",
    "print('''\n",
    "1. ROOT CAUSE OF FAILURE:\n",
    "   - bbox3 started from snapshot 21198927060's submission.csv\n",
    "   - This submission (70.624424) was NEVER validated by Kaggle\n",
    "   - It already had overlaps that Kaggle detected\n",
    "   - bbox3 did NOT introduce the overlaps - they were already there\n",
    "\n",
    "2. CORRECT BASELINE:\n",
    "   - Use snapshot 21198893057's submission.csv (LB: 70.627582)\n",
    "   - This is the best VALIDATED submission\n",
    "   - Or use snapshot 21198927060's candidate_024.csv (LB: 70.626088)\n",
    "\n",
    "3. WHY OPTIMIZATION DOESN'T WORK:\n",
    "   - The pre-optimized solution is at a local optimum\n",
    "   - bbox3 ran 8 parameter combinations with ZERO improvement\n",
    "   - More iterations won't help\n",
    "\n",
    "4. WHAT MIGHT WORK:\n",
    "   a. Try sa_fast_v2 (different algorithm - SA with fractional translation)\n",
    "   b. Tessellation for large N (fundamentally different representation)\n",
    "   c. Focus on small N values (1-20) with exhaustive rotation search\n",
    "   d. External sources (GitHub, Telegram) - but we found they're worse\n",
    "\n",
    "5. IMMEDIATE NEXT STEP:\n",
    "   - Run sa_fast_v2 on the VALIDATED submission (70.627582)\n",
    "   - If it also shows no improvement, pivot to tessellation\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the validated candidate file\n",
    "print('=== Finding validated candidate file ===')\n",
    "\n",
    "# Check for candidate_024.csv in 21198927060\n",
    "candidate_path = '/home/nonroot/snapshots/santa-2025/21198927060/code/submission_candidates/candidate_024.csv'\n",
    "if os.path.exists(candidate_path):\n",
    "    df = pd.read_csv(candidate_path)\n",
    "    print(f'Found candidate_024.csv: {len(df)} rows')\n",
    "    \n",
    "    # Calculate score\n",
    "    from numba import njit\n",
    "    \n",
    "    @njit\n",
    "    def score_group(xs, ys, degs, tx, ty):\n",
    "        n = xs.size\n",
    "        V = tx.size\n",
    "        mnx = mny = 1e300\n",
    "        mxx = mxy = -1e300\n",
    "        for i in range(n):\n",
    "            r = degs[i] * math.pi / 180.0\n",
    "            c, s = math.cos(r), math.sin(r)\n",
    "            for j in range(V):\n",
    "                X = c * tx[j] - s * ty[j] + xs[i]\n",
    "                Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "                mnx, mxx = min(mnx, X), max(mxx, X)\n",
    "                mny, mxy = min(mny, Y), max(mxy, Y)\n",
    "        side = max(mxx - mnx, mxy - mny)\n",
    "        return side * side / n\n",
    "    \n",
    "    total = 0\n",
    "    for n in range(1, 201):\n",
    "        mask = df['id'].str.startswith(f'{n:03d}_')\n",
    "        group = df[mask]\n",
    "        if len(group) == n:\n",
    "            xs = group['x'].str[1:].astype(float).values\n",
    "            ys = group['y'].str[1:].astype(float).values\n",
    "            degs = group['deg'].str[1:].astype(float).values\n",
    "            total += score_group(xs, ys, degs, TX, TY)\n",
    "    \n",
    "    print(f'Score: {total:.6f}')\n",
    "else:\n",
    "    print('candidate_024.csv not found')\n",
    "\n",
    "# Also check the validated submission\n",
    "validated_path = '/home/nonroot/snapshots/santa-2025/21198893057/code/submission.csv'\n",
    "if os.path.exists(validated_path):\n",
    "    df = pd.read_csv(validated_path)\n",
    "    print(f'\\nValidated submission.csv: {len(df)} rows')\n",
    "    \n",
    "    total = 0\n",
    "    for n in range(1, 201):\n",
    "        mask = df['id'].str.startswith(f'{n:03d}_')\n",
    "        group = df[mask]\n",
    "        if len(group) == n:\n",
    "            xs = group['x'].str[1:].astype(float).values\n",
    "            ys = group['y'].str[1:].astype(float).values\n",
    "            degs = group['deg'].str[1:].astype(float).values\n",
    "            total += score_group(xs, ys, degs, TX, TY)\n",
    "    \n",
    "    print(f'Score: {total:.6f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
