{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9471b78",
   "metadata": {},
   "source": [
    "# Experiment 007: Safe Optimization\n",
    "\n",
    "Start from the ONLY validated submission (candidate_003.csv, LB=70.622435) and run sa_fast_v2.\n",
    "This ensures we don't introduce overlaps from external sources.\n",
    "\n",
    "**Key insight**: Previous failures were caused by replacing N values with \"better\" solutions that had overlaps Kaggle detected but we didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b10a74e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T10:55:28.364908Z",
     "iopub.status.busy": "2026-01-25T10:55:28.364364Z",
     "iopub.status.idle": "2026-01-25T10:55:28.757379Z",
     "shell.execute_reply": "2026-01-25T10:55:28.756940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Tree geometry\n",
    "TX = np.array([0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125])\n",
    "TY = np.array([0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5])\n",
    "\n",
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = mny = 1e300\n",
    "    mxx = mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c, s = math.cos(r), math.sin(r)\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xs[i]\n",
    "            Y = s * tx[j] + c * ty[j] + ys[i]\n",
    "            mnx, mxx = min(mnx, X), max(mxx, X)\n",
    "            mny, mxy = min(mny, Y), max(mxy, Y)\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n\n",
    "\n",
    "def calculate_total_score(df):\n",
    "    total_score = 0.0\n",
    "    for n in range(1, 201):\n",
    "        mask = df['id'].str.startswith(f'{n:03d}_')\n",
    "        group = df[mask]\n",
    "        if len(group) != n:\n",
    "            continue\n",
    "        xs = group['x'].str[1:].astype(float).values\n",
    "        ys = group['y'].str[1:].astype(float).values\n",
    "        degs = group['deg'].str[1:].astype(float).values\n",
    "        score = score_group(xs, ys, degs, TX, TY)\n",
    "        total_score += score\n",
    "    return total_score\n",
    "\n",
    "print(\"Scoring functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059e992f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T10:55:28.758627Z",
     "iopub.status.busy": "2026-01-25T10:55:28.758469Z",
     "iopub.status.idle": "2026-01-25T10:55:29.674617Z",
     "shell.execute_reply": "2026-01-25T10:55:29.674140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Score: 70.622435\n",
      "Expected (validated LB): 70.622435\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify starting point is the validated submission\n",
    "os.chdir('/home/code/experiments/007_safe_optimization')\n",
    "df_initial = pd.read_csv('submission.csv')\n",
    "initial_score = calculate_total_score(df_initial)\n",
    "print(f\"Initial Score: {initial_score:.6f}\")\n",
    "print(f\"Expected (validated LB): 70.622435\")\n",
    "print(f\"Match: {abs(initial_score - 70.622435) < 0.0001}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf5ad0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T10:55:29.675850Z",
     "iopub.status.busy": "2026-01-25T10:55:29.675680Z",
     "iopub.status.idle": "2026-01-25T10:55:29.680398Z",
     "shell.execute_reply": "2026-01-25T10:55:29.680005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer defined\n"
     ]
    }
   ],
   "source": [
    "# Run sa_fast_v2 optimization\n",
    "# sa_fast_v2 expects submission1.csv as input and outputs to submission2.csv\n",
    "\n",
    "def run_sa_fast_v2(n_iter, rounds, timeout=600):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Copy current to submission1.csv\n",
    "    shutil.copy('submission.csv', 'submission1.csv')\n",
    "    df_before = pd.read_csv('submission1.csv')\n",
    "    score_before = calculate_total_score(df_before)\n",
    "    \n",
    "    cmd = f'./sa_fast_v2 -n {n_iter} -r {rounds}'\n",
    "    print(f\"Running: {cmd}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=timeout)\n",
    "        print(f\"stdout: {result.stdout[:300]}...\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Timeout after {timeout}s\")\n",
    "    \n",
    "    if os.path.exists('submission2.csv'):\n",
    "        df_after = pd.read_csv('submission2.csv')\n",
    "        score_after = calculate_total_score(df_after)\n",
    "        shutil.copy('submission2.csv', 'submission.csv')\n",
    "        shutil.copy('submission2.csv', 'submission1.csv')\n",
    "    else:\n",
    "        score_after = score_before\n",
    "        print(\"No output!\")\n",
    "    \n",
    "    improvement = score_before - score_after\n",
    "    print(f\"Score: {score_before:.6f} -> {score_after:.6f} (improvement: {improvement:.6f})\")\n",
    "    print(f\"Time: {time.time() - start_time:.1f}s\")\n",
    "    return score_after, improvement\n",
    "\n",
    "print(\"Optimizer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"Running sa_fast_v2 on VALIDATED baseline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "best_score = initial_score\n",
    "\n",
    "# Run multiple rounds\n",
    "for n_iter in [5000, 10000, 15000]:\n",
    "    for rounds in [50, 80]:\n",
    "        score, improvement = run_sa_fast_v2(n_iter, rounds, timeout=300)\n",
    "        results.append({'n': n_iter, 'r': rounds, 'score': score, 'improvement': improvement})\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Initial score (validated LB): {initial_score:.6f}\")\n",
    "print(f\"Final score: {best_score:.6f}\")\n",
    "print(f\"Total improvement: {initial_score - best_score:.6f}\")\n",
    "print(f\"\\nAll runs:\")\n",
    "for r in results:\n",
    "    print(f\"  n={r['n']:5d}, r={r['r']:3d}: score={r['score']:.6f}, improvement={r['improvement']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6703071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final submission\n",
    "df_final = pd.read_csv('submission.csv')\n",
    "final_score = calculate_total_score(df_final)\n",
    "print(f\"\\nFinal verified score: {final_score:.6f}\")\n",
    "\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy('submission.csv', '/home/submission/submission.csv')\n",
    "print(\"Copied to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': final_score,\n",
    "    'initial_score': initial_score,\n",
    "    'improvement': initial_score - final_score,\n",
    "    'optimization_runs': results,\n",
    "    'source': 'candidate_003.csv (validated LB: 70.622435)',\n",
    "    'note': 'Safe optimization - started from ONLY validated submission to avoid overlap issues'\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved. Final CV Score: {final_score:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
