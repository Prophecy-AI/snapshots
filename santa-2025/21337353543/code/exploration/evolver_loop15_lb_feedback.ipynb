{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d412262c",
   "metadata": {},
   "source": [
    "# Evolver Loop 15 - LB Feedback Analysis\n",
    "\n",
    "**LB Score: 70.3167** (matches CV exactly - perfect calibration!)\n",
    "\n",
    "**Current Status:**\n",
    "- CV: 70.3167\n",
    "- LB: 70.3167\n",
    "- Target: 68.8768\n",
    "- Gap: 1.44 points (2.05%)\n",
    "\n",
    "**Key Insight:** CV-LB gap is 0.0000 - our validation is perfectly calibrated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd90317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load session state to analyze experiments\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "print(f\"Total experiments: {len(state['experiments'])}\")\n",
    "print(f\"\\nSubmission history:\")\n",
    "for i, sub in enumerate(state.get('submissions', [])):\n",
    "    print(f\"  #{i+1}: {sub.get('experiment_id', 'N/A')} | CV: {sub.get('cv_score', 'N/A'):.4f} | LB: {sub.get('lb_score', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nRemaining submissions: {state.get('remaining_submissions', 'N/A')}\")\n",
    "print(f\"Max submissions: {state.get('max_submissions', 'N/A')}\")\n",
    "print(f\"Used: {state.get('max_submissions', 100) - state.get('remaining_submissions', 98)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze score progression\n",
    "print(\"Score Progression:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = []\n",
    "for exp in state['experiments']:\n",
    "    scores.append({\n",
    "        'name': exp['name'],\n",
    "        'cv': exp.get('cv_score', exp.get('score', 0)),\n",
    "        'lb': exp.get('lb_score', None)\n",
    "    })\n",
    "\n",
    "for s in scores:\n",
    "    lb_str = f\"{s['lb']:.4f}\" if s['lb'] else 'pending'\n",
    "    print(f\"{s['name']:40s} | CV: {s['cv']:.4f} | LB: {lb_str}\")\n",
    "\n",
    "print(f\"\\nBest CV: {min(s['cv'] for s in scores):.4f}\")\n",
    "lb_scores = [s['lb'] for s in scores if s['lb']]\n",
    "if lb_scores:\n",
    "    print(f\"Best LB: {min(lb_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze improvement trajectory\n",
    "print(\"\\nImprovement Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline = 70.6158  # First baseline\n",
    "current = 70.3167   # Current best\n",
    "target = 68.8768\n",
    "\n",
    "print(f\"Starting score: {baseline:.4f}\")\n",
    "print(f\"Current score:  {current:.4f}\")\n",
    "print(f\"Target score:   {target:.4f}\")\n",
    "print(f\"\\nProgress made:  {baseline - current:.4f} points ({(baseline - current) / (baseline - target) * 100:.1f}% of gap)\")\n",
    "print(f\"Remaining gap:  {current - target:.4f} points ({(current - target) / (baseline - target) * 100:.1f}% of gap)\")\n",
    "\n",
    "print(f\"\\n⚠️ At current rate, need {(current - target) / 0.01:.0f} more experiments with 0.01 improvement each!\")\n",
    "print(f\"   Or {(current - target) / 0.1:.0f} experiments with 0.1 improvement each\")\n",
    "print(f\"   Or {(current - target) / 0.3:.0f} experiments with 0.3 improvement each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available Kaggle sources\n",
    "import os\n",
    "\n",
    "kaggle_dir = '/home/code/kaggle_datasets/'\n",
    "sources = []\n",
    "\n",
    "for item in os.listdir(kaggle_dir):\n",
    "    item_path = os.path.join(kaggle_dir, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        for f in os.listdir(item_path):\n",
    "            if f.endswith('.csv'):\n",
    "                sources.append(os.path.join(item_path, f))\n",
    "    elif item.endswith('.csv'):\n",
    "        sources.append(item_path)\n",
    "\n",
    "print(f\"Total CSV sources available: {len(sources)}\")\n",
    "for s in sorted(sources):\n",
    "    print(f\"  {s.replace(kaggle_dir, '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df23c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which sources have been used in exp_018\n",
    "used_in_018 = [\n",
    "    'egortrushin_sa_translations/submission.csv',\n",
    "    'datafad_boxes_shrunk/submission.csv',\n",
    "    'hvanphucs_ensemble/submission.csv',\n",
    "    'hvanphucs_ensemble/submission_ensemble.csv',\n",
    "    'aikhmelnytskyy_sa/submission.csv',\n",
    "    'jazivxt_team_blend/submission.csv',\n",
    "    'jazivxt_team_blend/submission_ensemble.csv',\n",
    "]\n",
    "\n",
    "print(\"Sources used in exp_018:\")\n",
    "for s in used_in_018:\n",
    "    print(f\"  ✓ {s}\")\n",
    "\n",
    "print(\"\\nSources NOT yet used:\")\n",
    "for s in sorted(sources):\n",
    "    short = s.replace(kaggle_dir, '')\n",
    "    if short not in used_in_018 and 'submission' in short.lower():\n",
    "        print(f\"  ✗ {short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: What's the theoretical minimum?\n",
    "# Each N contributes score = (bbox_side^2) / N\n",
    "# Total score = sum over N=1 to 200\n",
    "\n",
    "# For N=1, minimum bbox is when tree is at 45 degrees\n",
    "# Tree dimensions: width ~0.7, height ~1.0\n",
    "# At 45 degrees, bbox side = sqrt(0.7^2 + 1.0^2) / sqrt(2) ≈ 0.86\n",
    "# Score contribution = 0.86^2 / 1 = 0.74\n",
    "\n",
    "# Current N=1 score is 0.66 - already very good!\n",
    "\n",
    "print(\"Score breakdown analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"Current score breakdown (from session_state):\")\n",
    "print(\"  N=1:      ~0.66 (0.9% of total)\")\n",
    "print(\"  N=2-5:    ~1.72 (2.4% of total)\")\n",
    "print(\"  N=6-10:   ~1.94 (2.8% of total)\")\n",
    "print(\"  N=11-50:  ~14.63 (20.8% of total)\")\n",
    "print(\"  N=51-100: ~17.48 (24.9% of total)\")\n",
    "print(\"  N=101-200: ~33.89 (48.2% of total)\")\n",
    "print(\"\")\n",
    "print(\"Key insight: N=101-200 contributes 48% of total score!\")\n",
    "print(\"To close 1.44 point gap, need ~0.69 points from N=101-200 alone\")\n",
    "print(\"That's ~0.007 improvement per N value on average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9317d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches have been tried?\n",
    "print(\"Approaches tried:\")\n",
    "print(\"=\"*60)\n",
    "approaches = {\n",
    "    'Ensemble (multi-source)': 'WORKING - main source of improvements',\n",
    "    'Simulated Annealing (Python)': 'FAILED - 0 improvements',\n",
    "    'Fractional Translation': 'MARGINAL - 0.000033 improvement',\n",
    "    'Rotation Optimization': 'MARGINAL - 0 improvements',\n",
    "    'Backward Propagation': 'MARGINAL - 1 improvement',\n",
    "    'NFP Local Search': 'FAILED - 0 improvements',\n",
    "    'Rebuild from Corners': 'BUG - never properly tested',\n",
    "    'Constructive Heuristic': 'FAILED - much worse than baseline',\n",
    "}\n",
    "\n",
    "for approach, result in approaches.items():\n",
    "    status = '✓' if 'WORKING' in result else '✗' if 'FAILED' in result else '⚠️'\n",
    "    print(f\"  {status} {approach}: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION: Only ensemble approach works consistently.\")\n",
    "print(\"Need to either:\")\n",
    "print(\"  1. Find MORE diverse sources (new Kaggle kernels)\")\n",
    "print(\"  2. Fix rebuild from corners bug and test properly\")\n",
    "print(\"  3. Implement a fundamentally different algorithm\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
