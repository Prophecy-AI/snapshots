## Current Status
- Best CV score: 70.316589 from exp_019 (full kaggle ensemble)
- Best LB score: 70.3167 (from exp_014)
- Target: 68.876781 | Gap to target: 1.44 points (2.05%)

## ⚠️ CRITICAL: REBUILD FROM CORNERS BUG MUST BE FIXED ⚠️

The evaluator has identified a CRITICAL BUG that was NEVER fixed:

**exp_017 (rebuild from corners) used WRONG distance calculation:**
```python
# WRONG (exp_017 implementation)
x, y = float(t['x']), float(t['y'])  # tree CENTER
dist = max(abs(x - corner_x), abs(y - corner_y))
```

**CORRECT (chistyakov kernel implementation):**
```python
# CORRECT - uses POLYGON BOUNDS
candidates = {
    max(
        abs(tree.polygon.bounds[0] - corner_x),  # polygon minx
        abs(tree.polygon.bounds[2] - corner_x),  # polygon maxx
        abs(tree.polygon.bounds[1] - corner_y),  # polygon miny
        abs(tree.polygon.bounds[3] - corner_y),  # polygon maxy
    ):tree for tree in layout}
```

**WHY THIS MATTERS:**
- A tree at center (0,0) with rotation can have polygon bounds extending to ±0.8
- Using tree center ignores rotation and shape
- This causes the algorithm to select the WRONG trees for each subset
- exp_017 found 0 improvements because the technique was NEVER properly tested!

## Response to Evaluator

The evaluator is CORRECT. I have verified the bug by reading both:
1. exp_017 notebook: Uses `x, y = float(t['x']), float(t['y'])` (tree center)
2. chistyakov kernel: Uses `tree.polygon.bounds[0-3]` (polygon bounds)

The folder "018_rebuild_corners_fixed" is MISLEADING - it contains a new sources ensemble experiment, NOT a bug fix. The rebuild from corners technique has NEVER been properly tested.

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY] FIX REBUILD FROM CORNERS BUG**

Create: `experiments/020_rebuild_corners_fixed/`

**EXACT CODE TO USE:**
```python
def rebuild_from_corners_FIXED(large_layout, target_n, current_best_score):
    """Extract subset of trees closest to each corner using POLYGON BOUNDS."""
    if len(large_layout) <= target_n:
        return None
    
    # Get layout bounds
    polygons = [create_tree_polygon(t['x'], t['y'], t['deg']) for t in large_layout]
    union = unary_union(polygons)
    bounds = union.bounds  # (minx, miny, maxx, maxy)
    
    corners = [
        (bounds[0], bounds[1]),  # bottom-left
        (bounds[0], bounds[3]),  # top-left
        (bounds[2], bounds[1]),  # bottom-right
        (bounds[2], bounds[3]),  # top-right
    ]
    
    best_subset = None
    best_score = current_best_score
    
    for corner_x, corner_y in corners:
        # Sort trees by max POLYGON BOUNDS distance from corner
        trees_with_dist = []
        for t in large_layout:
            # CREATE POLYGON AND USE ITS BOUNDS - THIS IS THE FIX!
            poly = create_tree_polygon(t['x'], t['y'], t['deg'])
            b = poly.bounds  # (minx, miny, maxx, maxy)
            dist = max(
                abs(b[0] - corner_x),  # polygon minx
                abs(b[2] - corner_x),  # polygon maxx
                abs(b[1] - corner_y),  # polygon miny
                abs(b[3] - corner_y),  # polygon maxy
            )
            trees_with_dist.append((dist, t))
        
        trees_with_dist.sort(key=lambda x: x[0])
        subset = [t for _, t in trees_with_dist[:target_n]]
        
        score = get_score(subset, target_n)
        if score < best_score - 1e-9:
            if validate_no_overlap_strict(subset):
                best_score = score
                best_subset = subset
    
    return (best_subset, best_score) if best_subset else None
```

**APPLY TO:**
1. ALL large layouts from current best (N=50 to N=200)
2. ALL large layouts from kaggle_datasets sources (not just our current best!)
3. Try extracting subsets for ALL smaller N values (not just a few)

### 2. **[HIGH PRIORITY] SUBMIT CURRENT BEST TO VERIFY LB**

We have 98 submissions remaining and haven't verified LB since CV=70.615.
Current CV is 70.3167. Submit to verify CV-LB alignment still holds.

### 3. **[MEDIUM PRIORITY] FIND NEW EXTERNAL SOURCES**

The kaggle_datasets have been thoroughly mined (397 files, only 0.000119 improvement).
Look for NEW sources:
- Check Kaggle discussions for shared CSVs
- Check GitHub for Santa 2025 solutions
- Look for team merge announcements with shared solutions

## What NOT to Try
- ❌ Re-processing kaggle_datasets (already exhausted - 0.000119 improvement from 397 files)
- ❌ Local search (SA, fractional translation) - proven ineffective
- ❌ Running bbox3/sa_fast binaries - already at local optimum

## Score Breakdown (Where to Focus)
| Range | Score | % of Total | Notes |
|-------|-------|------------|-------|
| N=1-1 | 0.66 | 0.9% | Optimal (45° rotation) |
| N=2-5 | 1.72 | 2.4% | Small improvements possible |
| N=6-10 | 1.94 | 2.8% | Small improvements possible |
| N=11-50 | 14.63 | 20.8% | Medium potential |
| N=51-100 | 17.48 | 24.9% | Medium potential |
| N=101-200 | 33.89 | 48.2% | **LARGEST CONTRIBUTION** |

**Key insight**: N=101-200 contributes 48.2% of total score. The rebuild from corners technique specifically targets extracting good subsets from larger layouts - this is exactly where improvements are most needed!

## Validation Notes
- Use high-precision overlap detection (1e18 scaling)
- All N values must pass strict validation before submission
- CV-LB gap has been 0.0000 (perfect calibration) in recent experiments

## SUBMISSION STRATEGY
- Remaining submissions: 98
- Submit after this experiment? **YES** - we have abundant submissions
- Even if score doesn't improve, LB feedback is valuable

## ⛔ FORBIDDEN
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() - FORBIDDEN
- Running ANY binary or executable - FORBIDDEN
- "Optimizing" existing CSV files with binaries - FORBIDDEN
