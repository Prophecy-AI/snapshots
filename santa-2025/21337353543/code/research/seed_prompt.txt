## Current Status
- Best CV score: 70.3295 from exp_016 (jazivxt_why_not ensemble)
- Best LB score: 70.3410 (exp_010)
- Target: 68.876781 | Gap to target: 1.45 points (2.1%)

## Response to Evaluator

The evaluator correctly identified that:
1. **NFP local search found ZERO improvements** - confirms the baseline is at a VERY tight local optimum
2. **Local search approaches are EXHAUSTED** - SA, fractional translation, rotation optimization, NFP all found minimal/no improvements
3. **Ensemble strategy has reached its limit** - all available sources have been mined
4. **14 missing datasets** - evaluator suggests downloading more Kaggle datasets

I AGREE with the evaluator's assessment. However, I want to add:
- The "rebuild from corners" technique from chistyakov kernel is a NOVEL approach we haven't tried
- This technique extracts subsets from larger layouts WITHOUT running optimization
- It could find improvements for smaller N values by leveraging structure of larger layouts

## ⛔ FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() - FORBIDDEN
- Running ANY binary or executable - FORBIDDEN
- More local search variants (SA, NFP, fractional translation) - EXHAUSTED, DO NOT TRY

## ✅ MANDATORY EXPERIMENT: REBUILD FROM CORNERS

The chistyakov kernel reveals a NOVEL technique that we haven't implemented:

**Algorithm:**
1. For each large layout (N=50, 100, 150, 200):
   - Get the 4 corners of the bounding box
   - For each corner, sort trees by distance from that corner
   - Extract subsets of size 1, 2, 3, ... N-1 (trees closest to corner)
   - If subset has smaller bounding box than current best for that size, USE IT

**Why this works:**
- Large optimized layouts have good local structure
- Subsets near corners often form compact arrangements
- This finds improvements WITHOUT running optimization
- "Most problems are always with the corner" - chistyakov

**Implementation:**
```python
def rebuild_from_corners(large_layout, current_best_scores):
    improvements = []
    bounds = get_bounds(large_layout)
    corners = [(bounds[0], bounds[1]), (bounds[0], bounds[3]), 
               (bounds[2], bounds[1]), (bounds[2], bounds[3])]
    
    for corner_x, corner_y in corners:
        # Sort trees by max distance from corner
        trees_by_dist = sorted(large_layout, 
            key=lambda t: max(abs(t.x - corner_x), abs(t.y - corner_y)))
        
        # Extract subsets
        for subset_size in range(1, len(large_layout)):
            subset = trees_by_dist[:subset_size]
            subset_score = compute_score(subset, subset_size)
            
            if subset_score < current_best_scores[subset_size]:
                improvements.append((subset_size, subset, subset_score))
    
    return improvements
```

## ✅ REQUIRED: Apply to ALL large layouts

Apply rebuild_from_corners to:
- N=50, 60, 70, 80, 90, 100
- N=110, 120, 130, 140, 150
- N=160, 170, 180, 190, 200

For each large layout, check if any corner-based subset improves smaller N values.

## ✅ REQUIRED: Strict validation

All improvements MUST pass strict 1e18 overlap validation before inclusion.

## ✅ REQUIRED: Per-N tracking

Track which N values improved and by how much:
```python
for n, improvement in improvements:
    print(f"N={n}: IMPROVED by {improvement:.6f}")
```

## Recommended Approaches (Priority Order)

1. **[HIGHEST PRIORITY]** Implement "rebuild from corners" technique
   - Novel approach we haven't tried
   - No optimization needed - just subset extraction
   - Expected: 0.01-0.1 points improvement

2. **[HIGH PRIORITY]** Try different large layouts as sources
   - Use N=200 layout to extract subsets for N=1-199
   - Use N=150 layout to extract subsets for N=1-149
   - etc.

3. **[MEDIUM PRIORITY]** Combine with existing ensemble
   - After finding corner-based improvements, ensemble with current best
   - Keep best per-N from all sources

## What NOT to Try
- More local search (SA, NFP, fractional translation) - EXHAUSTED
- Running bbox3 or any binary - FORBIDDEN
- Downloading more datasets - already mined all available sources

## Validation Notes
- Use strict 1e18 overlap validation
- CV = LB exactly (verified across 7 submissions)
- All improvements must pass validation before inclusion

## SUBMISSION STRATEGY
- Remaining submissions: 90
- Submit after this experiment? YES - we have abundant submissions
- LB feedback is free - USE IT

## Expected Outcome
The "rebuild from corners" technique is NOVEL and could find improvements that local search cannot. Even small improvements (0.01-0.05 points) are valuable when accumulated across many N values.

Gap: 1.45 points. Target IS reachable with novel approaches.