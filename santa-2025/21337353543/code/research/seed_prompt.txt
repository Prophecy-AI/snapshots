# Santa 2025 - Evolved Seed Prompt (Loop 4)

## Current Status
- Best CV score: 70.622435 from exp_001 (fix_overlaps)
- Best LB score: 70.622435 (confirmed)
- Target: 68.887744 | Gap to target: **1.734 points (~2.5%)**

## ⚠️ CRITICAL: STUCK AT LOCAL OPTIMUM

**Experiments exp_002 and exp_003 produced IDENTICAL scores (70.622435).**

This proves:
1. The baseline is at a VERY TIGHT local optimum
2. Local search (fractional translation) cannot improve it
3. Simulated annealing cannot improve it
4. **We need a FUNDAMENTALLY DIFFERENT approach**

## ⛔ FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() - FORBIDDEN
- Running ANY binary or executable - FORBIDDEN
- "Optimizing" existing CSV files - FORBIDDEN
- Running "more iterations" on any optimizer - FORBIDDEN
- **Any approach that produces ~70.6 score - ALREADY TRIED**

## ✅ MANDATORY NEXT EXPERIMENT: ENSEMBLE + CONSTRUCTIVE

The evaluator identified the key missing piece: **ENSEMBLE approach**.

### STEP 1: Implement Zaburo Constructive Heuristic

The zaburo kernel shows a constructive approach that builds solutions from scratch:

```python
def find_best_trees_constructive(n: int):
    """Build solution using alternating rows of 0° and 180° trees."""
    best_score, best_trees = float("inf"), None
    
    for n_even in range(1, n + 1):
        for n_odd in [n_even, n_even - 1]:
            all_trees = []
            rest = n
            r = 0
            while rest > 0:
                m = min(rest, n_even if r % 2 == 0 else n_odd)
                rest -= m
                
                angle = 0 if r % 2 == 0 else 180
                x_offset = 0 if r % 2 == 0 else 0.35  # Half tree width
                y = r // 2 * 1.0 if r % 2 == 0 else (0.8 + (r - 1) // 2 * 1.0)
                
                for i in range(m):
                    all_trees.append({
                        'x': str(0.7 * i + x_offset),
                        'y': str(y),
                        'deg': str(angle)
                    })
                r += 1
            
            # Calculate score
            score = get_bbox_side(all_trees) ** 2
            if score < best_score:
                best_score = score
                best_trees = all_trees
    
    return best_score, best_trees
```

This achieves ~88.33 initially but provides a DIFFERENT starting point.

### STEP 2: Apply Local Search to Constructive Solution

After building the constructive solution, apply:
1. Fractional translation (tiny moves in 8 directions)
2. Rotation optimization (try different angles)
3. Compaction (squeeze toward center)

### STEP 3: ENSEMBLE - Take Best Per-N

This is the KEY technique from top kernels:

```python
def ensemble_best_per_n(sources):
    """For each N, take the best solution across all sources."""
    best_per_n = {}
    
    for source_name, trees_by_n in sources.items():
        for n in range(1, 201):
            score = get_score(trees_by_n[n], n)
            if n not in best_per_n or score < best_per_n[n]['score']:
                best_per_n[n] = {
                    'score': score,
                    'trees': trees_by_n[n],
                    'source': source_name
                }
    
    # Create final submission
    final_trees_by_n = {n: best_per_n[n]['trees'] for n in range(1, 201)}
    return final_trees_by_n
```

### STEP 4: Generate Multiple Sources

Create diverse starting points:
1. **Baseline** (current optimized solution)
2. **Constructive** (zaburo-style alternating rows)
3. **Random restart** (random initial configs + local search)
4. **Different rotation patterns** (all 45°, all 0°, mixed)

### Expected Outcome

Even if each individual source is worse than baseline, the ensemble can find improvements by taking the best per-N across all sources.

**The gap is 1.734 points. If we can improve just 1% of N values by 0.01 each, that's 0.2 points. We need to find ~17 such improvements.**

## ✅ REQUIRED: PER-N TRACKING

Track best solution for EACH N value separately:

```python
# After each experiment, compare per-N scores
improvements = []
for n in range(1, 201):
    my_score = get_score(my_trees[n], n)
    baseline_score = get_score(baseline_trees[n], n)
    diff = baseline_score - my_score  # positive = better
    if diff > 0.0001:
        improvements.append((n, diff))
        print(f"✅ N={n}: IMPROVED by {diff:.6f}")

# SAVE any N where we improved
# Even if total score is worse, individual N improvements are valuable!
```

## ✅ VALIDATION REQUIREMENTS

Use strict precision validation (1e15 scaling):

```python
from decimal import Decimal, getcontext
getcontext().prec = 25
scale_factor = Decimal('1e15')

def validate_no_overlap(trees):
    # Scale coordinates to integers for strict precision
    polygons = []
    for tree in trees:
        coords = [(int(Decimal(str(x)) * scale_factor), 
                   int(Decimal(str(y)) * scale_factor)) 
                  for x, y in get_tree_vertices(tree)]
        polygons.append(Polygon(coords))
    
    # Check all pairs
    for i in range(len(polygons)):
        for j in range(i+1, len(polygons)):
            if polygons[i].intersects(polygons[j]) and not polygons[i].touches(polygons[j]):
                return False, f"Trees {i} and {j} overlap"
    return True, "OK"
```

## What NOT to Try
- ❌ More iterations on SA (already tried, no improvement)
- ❌ Different SA parameters (local optimum is too tight)
- ❌ Fractional translation on baseline (already tried)
- ❌ Backward propagation on baseline (already tried)
- ❌ Any approach that just "optimizes" existing CSV

## Key Insight from Research

The top teams achieved 68.89 through:
1. **900+ submissions** - accumulating best per-N over time
2. **Multiple diverse sources** - different optimizers find different local optima
3. **Ensemble** - taking best per-N across all sources
4. **Novel algorithms** - not just running binaries

We have 92 submissions remaining. Use them to test diverse approaches and accumulate best per-N solutions.

## Experiment Structure

```
experiments/004_ensemble_constructive/
├── ensemble_optimization.ipynb
├── submission.csv
└── metrics.json
```

The notebook should:
1. Load baseline solution
2. Generate constructive solution (zaburo-style)
3. Apply local search to constructive solution
4. Ensemble: take best per-N from baseline vs constructive
5. Track which N values improved
6. Create submission

## Success Criteria

- ✅ Score < 70.622435 (any improvement)
- ✅ At least 1 N value improved
- ✅ No overlaps (strict validation)
- ✅ Submission passes Kaggle validation
