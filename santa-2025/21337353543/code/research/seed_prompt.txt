# Santa 2025 - Christmas Tree Packing Challenge Seed Prompt

## Current Status
- Best CV score: 70.615744 from exp_006 (validated_ensemble)
- Best LB score: 70.615744 (PERFECT CV-LB alignment!)
- Target: 68.887226 | Gap to target: 1.728 points (2.45%)
- Submissions used: 5/100 (91 remaining - ABUNDANT!)

## ‚úÖ WHAT'S WORKING
1. High-precision validation (1e18 scaling) - CV = LB exactly
2. Ensemble approach - combining best per-N from multiple sources
3. Overlap detection - all submissions now pass Kaggle validation

## ‚ùå WHAT'S NOT WORKING (PROVEN BY EXPERIMENTS)
1. Local search (fractional translation, backward propagation) - 0 improvements found
2. Simulated annealing - could NOT improve any N values
3. Constructive heuristics (zaburo-style) - score 110.18 (much worse)
4. Ensemble from 88 snapshots - limited by overlap validation (only 8 sources valid)

## üéØ CRITICAL INSIGHT: WE NEED MORE DIVERSE SOURCES

The jonathanchan kernel uses 15+ DIFFERENT sources:
- bucket-of-chump dataset
- SmartManoj GitHub repo
- telegram-public-shared-solution dataset
- Multiple different notebooks

We only have 88 snapshot submissions, and most have overlaps!

## ‚õî FORBIDDEN (WILL BE REJECTED)
- bbox3, sa_fast, eazy_optimizer, tree_packer - FORBIDDEN
- subprocess.run() or os.system() - FORBIDDEN
- Running ANY binary or executable - FORBIDDEN
- "Optimizing" existing CSV files with binaries - FORBIDDEN

## ‚úÖ REQUIRED: EXPERIMENT 007 - FIND MORE SOURCES

**STEP 1: Search for additional Kaggle datasets**
```python
# Look for datasets that might contain pre-optimized solutions
# Check if there are public datasets with santa-2025 solutions
```

**STEP 2: Check if any GitHub repos have solutions**
The jonathanchan kernel mentions:
- SmartManoj GitHub repo
- Other public repositories

**STEP 3: Implement aggressive backward propagation**
The current backward propagation found 0 improvements because the baseline is at a local optimum.
BUT: If we start from DIFFERENT configurations, we might find improvements.

```python
# For each N from 200 down to 3:
# 1. Load ALL available solutions for N (not just baseline)
# 2. For each solution, try removing boundary-touching trees
# 3. Check if resulting (N-1) solution beats current best for N-1
# 4. This generates NEW solutions, not just selects from existing
```

**STEP 4: Implement rotation optimization for entire groups**
The saspav kernel shows: optimize_rotation() - rotate entire group to minimize bbox
```python
from scipy.optimize import minimize_scalar
from scipy.spatial import ConvexHull

def optimize_rotation(trees):
    # Get all polygon points
    all_points = []
    for tree in trees:
        all_points.extend(list(tree.polygon.exterior.coords))
    points_np = np.array(all_points)
    
    # Get convex hull
    hull_points = points_np[ConvexHull(points_np).vertices]
    
    # Find optimal rotation angle
    res = minimize_scalar(
        lambda a: calculate_bbox_side_at_angle(a, hull_points),
        bounds=(0.001, 89.999), method='bounded'
    )
    return res.x, res.fun
```

## ‚úÖ REQUIRED: PER-N TRACKING

Track best solution for EACH N separately:
```python
best_per_n = {}
for n in range(1, 201):
    best_per_n[n] = {
        'score': baseline_scores[n],
        'trees': baseline_trees[n],
        'source': 'baseline'
    }

# After each experiment, update only N values that improved
for n in improved_n_values:
    if new_score[n] < best_per_n[n]['score']:
        best_per_n[n] = {'score': new_score[n], 'trees': new_trees[n], 'source': 'exp_007'}
```

## ‚úÖ REQUIRED: HIGH-PRECISION VALIDATION

ALWAYS validate before including in ensemble:
```python
from decimal import Decimal, getcontext
getcontext().prec = 30
SCALE_FACTOR = Decimal('1e18')

def validate_no_overlap_strict(trees_data):
    # Create polygons with 1e18 scaling
    # Check all pairs for intersection (not just touching)
    # Return (is_valid, error_message)
```

## üìä IMPROVEMENT TARGETS BY N RANGE

To close the 1.73 point gap:
- N=1: 0.6612 ‚Üí 0.6450 (improve by 0.0162) - ALREADY OPTIMAL
- N=2-5: 1.7189 ‚Üí 1.6768 (improve by 0.0421)
- N=6-10: 1.9490 ‚Üí 1.9013 (improve by 0.0477)
- N=11-50: 14.7036 ‚Üí 14.3437 (improve by 0.3599)
- N=51-100: 17.6063 ‚Üí 17.1753 (improve by 0.4310)
- N=101-200: 33.9768 ‚Üí 33.1451 (improve by 0.8317)

Focus on N=101-200 - it contributes 48% of total score!

## üöÄ SUBMISSION STRATEGY

With 91 submissions remaining, SUBMIT AFTER EVERY EXPERIMENT!
- LB feedback is FREE information
- Even if score is worse, we learn what doesn't work
- Perfect CV-LB alignment means we can trust our validation

## EXPERIMENT SEQUENCE

### exp_007: Rotation Optimization + More Sources
1. Implement rotation optimization for each N
2. Search for additional sources (Kaggle datasets, GitHub)
3. Apply backward propagation from MULTIPLE starting points
4. Validate all solutions with strict 1e18 precision
5. Ensemble best per-N from all sources

### exp_008+: Novel Algorithms
If exp_007 doesn't close the gap significantly:
1. Implement No-Fit Polygon (NFP) for O(1) collision checks
2. Branch-and-bound for N=2-20 (guarantee optimal)
3. Genetic algorithm with custom crossover operators

## NEVER GIVE UP

The target IS reachable. The top teams got there with:
1. 900+ submissions over weeks
2. Diverse sources (not just one optimizer)
3. Novel algorithms (not just running binaries)
4. Per-N specialization

We have 91 submissions remaining. Keep pushing!
