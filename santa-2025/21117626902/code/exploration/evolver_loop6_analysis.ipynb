{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ed6f9",
   "metadata": {},
   "source": [
    "# Loop 6 Analysis: Submission Failure Investigation\n",
    "\n",
    "The corner extraction experiment (006) failed with 'Overlapping trees in group 040'. Analysis shows the issue was precision truncation during the corner extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc54ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T23:34:40.667880Z",
     "iopub.status.busy": "2026-01-18T23:34:40.667301Z",
     "iopub.status.idle": "2026-01-18T23:34:41.074572Z",
     "shell.execute_reply": "2026-01-18T23:34:41.074130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous N=40 first tree:\n",
      "  x: s-1.689632103422090692\n",
      "  y: s-0.247894431182562769\n",
      "  deg: s252.060472972624125987\n",
      "\n",
      "Current N=40 first tree:\n",
      "  x: s-1.6896321034220907\n",
      "  y: s-0.24789443118256277\n",
      "  deg: s252.06047297262413\n",
      "\n",
      "Precision difference:\n",
      "  x: 22 -> 20 chars\n",
      "  y: 22 -> 21 chars\n",
      "  deg: 23 -> 19 chars\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Compare precision between candidate_004 and candidate_005\n",
    "prev = pd.read_csv('/home/code/submission_candidates/candidate_004.csv')\n",
    "curr = pd.read_csv('/home/code/submission_candidates/candidate_005.csv')\n",
    "\n",
    "# Check N=40 precision\n",
    "prev_40 = prev[prev['id'].str.startswith('040_')]\n",
    "curr_40 = curr[curr['id'].str.startswith('040_')]\n",
    "\n",
    "print('Previous N=40 first tree:')\n",
    "print(f\"  x: {prev_40.iloc[0]['x']}\")\n",
    "print(f\"  y: {prev_40.iloc[0]['y']}\")\n",
    "print(f\"  deg: {prev_40.iloc[0]['deg']}\")\n",
    "\n",
    "print('\\nCurrent N=40 first tree:')\n",
    "print(f\"  x: {curr_40.iloc[0]['x']}\")\n",
    "print(f\"  y: {curr_40.iloc[0]['y']}\")\n",
    "print(f\"  deg: {curr_40.iloc[0]['deg']}\")\n",
    "\n",
    "print('\\nPrecision difference:')\n",
    "print(f\"  x: {len(prev_40.iloc[0]['x'])} -> {len(curr_40.iloc[0]['x'])} chars\")\n",
    "print(f\"  y: {len(prev_40.iloc[0]['y'])} -> {len(curr_40.iloc[0]['y'])} chars\")\n",
    "print(f\"  deg: {len(prev_40.iloc[0]['deg'])} -> {len(curr_40.iloc[0]['deg'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439d37a",
   "metadata": {},
   "source": [
    "## Key Finding\n",
    "\n",
    "The corner extraction code used numpy float64 which truncated precision from ~25 decimal places to ~16 decimal places. This precision loss caused trees that were previously just touching to now overlap.\n",
    "\n",
    "## jiweiliu Kernel Analysis\n",
    "\n",
    "The jiweiliu kernel has two key techniques:\n",
    "1. **Grid-based initialization with SA** - Generate configurations from grid patterns\n",
    "2. **Deletion cascade** - Propagate good large configs to smaller sizes by iteratively removing trees\n",
    "\n",
    "The deletion cascade is the most promising technique for our current situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8802c49b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T23:34:41.075700Z",
     "iopub.status.busy": "2026-01-18T23:34:41.075588Z",
     "iopub.status.idle": "2026-01-18T23:34:41.079604Z",
     "shell.execute_reply": "2026-01-18T23:34:41.079185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found deletion cascade code:\n",
      "@njit(cache=True)\n",
      "def deletion_cascade_numba(all_xs, all_ys, all_degs, group_sizes):\n",
      "    \"\"\"\n",
      "    Apply tree deletion cascade using numba.\n",
      "    \"\"\"\n",
      "    # Build index mapping\n",
      "    group_start = np.zeros(201, dtype=np.int64)\n",
      "    for n in range(1, 201):\n",
      "        group_start[n] = group_start[n-1] + (n - 1) if n > 1 else 0\n",
      "\n",
      "    # Copy arrays\n",
      "    new_xs = all_xs.copy()\n",
      "    new_ys = all_ys.copy()\n",
      "    new_degs = all_degs.copy()\n",
      "\n",
      "    # Calculate initial side lengths\n",
      "    side_lengths = np.zeros(201, dtype=np.float64)\n",
      "    for n in range(1, 201):\n",
      "        start = group_start[n]\n",
      "        end = start + n\n",
      "        vertices = [get_tree_vertices(new_xs[i], new_ys[i], new_degs[i]) for i in range(start, end)]\n",
      "        side_lengths[n] = get_side_length(vertices)\n",
      "\n",
      "    # Cascade from n=200 down to n=2\n",
      "    for n in range(200, 1, -1):\n",
      "        start_n = group_start[n]\n",
      "        end_n = start_n + n\n",
      "        start_prev = group_start[n - 1]\n",
      "\n",
      "        best_side = side_lengths[n - 1]\n",
      "        best_delete_idx = -1\n",
      "\n",
      "        for del_idx in range(n):\n",
      "            vertices = []\n",
      "            for i in range(n):\n",
      "                if i != del_idx:\n",
      "                    idx = start_n + i\n",
      "                    vertices.append(get_tree_vertices(new_xs[idx], new_ys[idx], new_degs[idx]))\n",
      "\n",
      "            candidate_side = get_side_length(vertices)\n",
      "            if candidate_side < best_side:\n",
      "                best_side = candidate_side\n",
      "                best_delete_idx = del_idx\n",
      "\n",
      "        if best_delete_idx >= 0:\n",
      "            out_idx = start_prev\n",
      "            for i in range(n):\n",
      "                if i != best_delete_idx:\n",
      "                    in_idx = start_n + i\n",
      "                    new_xs[out_idx] = new_xs[in_idx]\n",
      "                    new_ys[out_idx] = new_ys[in_idx]\n",
      "                    new_degs[out_idx] = new_degs[in_idx]\n",
      "                    out_idx += 1\n",
      "            side_lengths[n - 1] = best_side\n",
      "\n",
      "    return new_xs, new_ys, new_degs, side_lengths\n"
     ]
    }
   ],
   "source": [
    "# Read the jiweiliu kernel to understand deletion cascade\n",
    "with open('/home/code/research/kernels/jiweiliu_super-fast-simulated-annealing-with-translations/super-fast-simulated-annealing-with-translations.ipynb', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Find the deletion cascade function\n",
    "import json\n",
    "notebook = json.loads(content)\n",
    "for cell in notebook['cells']:\n",
    "    if cell['cell_type'] == 'code':\n",
    "        source = ''.join(cell['source']) if isinstance(cell['source'], list) else cell['source']\n",
    "        if 'deletion_cascade' in source:\n",
    "            print('Found deletion cascade code:')\n",
    "            print(source[:2000])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327f6b9",
   "metadata": {},
   "source": [
    "## Strategy for Next Experiment\n",
    "\n",
    "1. **Implement jiweiliu deletion cascade** - This technique iteratively removes trees from large configurations to generate smaller ones\n",
    "2. **Preserve precision** - Use string operations to preserve the original precision\n",
    "3. **Grid-based SA** - Generate new configurations from grid patterns\n",
    "\n",
    "The deletion cascade is particularly promising because:\n",
    "- It generates NOVEL configurations (different from existing ones)\n",
    "- It's fast (~3 seconds according to the kernel)\n",
    "- It claims ~0.15 improvement"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
