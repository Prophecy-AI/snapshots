{
  "query": "What techniques do top Kaggle competitors use to escape local optima in combinatorial optimization problems?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle competitors employ a variety of techniques to escape local optima in combinatorial optimization problems, often drawing inspiration from evolutionary algorithms, local search strategies, and machine learning approaches. One common method is the use of **genetic algorithms**, which incorporate mechanisms such as mutation, crossover, and random restarts to maintain diversity in the solution pool and avoid premature convergence to local optima ([Kaggle](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm), [Kaggle](https://www.kaggle.com/code/khusheekapoor/genetic-algorithm/code)). Genetic algorithms mimic natural selection, allowing populations of solutions to evolve over generations, thereby exploring different regions of the solution space.\n\nAnother technique involves **random starts** in local search algorithms, where multiple initial solutions are generated randomly to diversify the search process. This approach helps in exploring different local basins and increases the chance of finding a global or near-global optimum ([GitHub](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb)). Additionally, advanced methods like **learning-based controllers**\u2014such as neural networks trained to adaptively select search moves\u2014are gaining popularity. For example, neural network models like NeuroLS and X2GNN are designed to balance exploration and exploitation, guiding the search process away from local optima by generating diverse solutions and refining promising candidates ([arXiv](https://arxiv.org/abs/2206.13181), [OpenReview](https://openreview.net/notes/edits/attachment?id=Ly9XkbE640&name=pdf)).\n\nIn summary, top Kaggle competitors leverage a combination of evolutionary strategies, random restarts, and machine learning techniques to enhance exploration capabilities and escape local optima in complex combinatorial problems ([Kaggle](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm), [arXiv](https://arxiv.org/abs/2206.13181)).",
      "url": ""
    },
    {
      "title": "Tutorial of Genetic Algorithm - Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Fzzettrkalpakbal%2Ftutorial-of-genetic-algorithm)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Fzzettrkalpakbal%2Ftutorial-of-genetic-algorithm)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\nIzzet Turkalp Akbasli \u00b7 3y ago \u00b7 19,313 views\n\narrow\\_drop\\_up64\n\n[Copy & Edit](https://www.kaggle.com/kernels/fork-version/101086193)189\n\n![silver medal](https://www.kaggle.com/static/images/medals/notebooks/silverl@1x.png)\n\nmore\\_vert\n\n# Tutorial of Genetic Algorithm\n\n[Copied from Izzet Turkalp Akbasli (+779,-438)](https://www.kaggle.com/code/zzettrkalpakbal/finding-best-categorization-method-with-pycaret)\n\n## Tutorial of Genetic Algorithm\n\n[Notebook](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm/notebook) [Input](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm/input) [Output](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm/output) [Logs](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm/log) [Comments (18)](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm/comments)\n\nhistoryVersion 9 of 9chevron\\_right\n\n## Runtime\n\nplay\\_arrow\n\n1m 5s\n\n## Input\n\nDATASETS\n\n![](https://storage.googleapis.com/kaggle-datasets-images/2343381/3947870/431bbbeefb4f1f5b9e55cef685538482/dataset-thumbnail.jpeg?t=2022-07-15-21-38-04)\n\nfull-filled-brain-stroke-dataset\n\n## Tags\n\n[Genetics](https://www.kaggle.com/code?tagIds=4406-Genetics) [Medicine](https://www.kaggle.com/code?tagIds=12028-Medicine) [Optimization](https://www.kaggle.com/code?tagIds=15002-Optimization) [Python](https://www.kaggle.com/code?tagIds=16639-Python)\n\n## Language\n\nPython\n\n## Table of Contents\n\n[TUTORIAL of GENETIC ALGORITHM](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#TUTORIAL-of-GENETIC-ALGORITHM) [Genetic Algorithm](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Genetic-Algorithm) [Optimization](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Optimization) [Evolutionary Algorithms (EAs)](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Evolutionary-Algorithms-(EAs)) [Here is the description of how the GA works:](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Here-is-the-description-of-how-the-GA-works:) [Parent Selection](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Parent-Selection) [Random selection:](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Random-selection:) [Crossover](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Crossover) [Mutation (now Genetic Algorithm)](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Mutation-(now-Genetic-Algorithm)) [Evaluating the Offsprings](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Evaluating-the-Offsprings) [Merge Offsprings with the Main Population and Sort](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Merge-Offsprings-with-the-Main-Population-and-Sort) [Acknowledge](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#Acknowledge) [The biggest of the praise is to, who provided all my inspiration and motivation on the subject, Prof. Dr. Asim Egemen Y\u0131lmaz.](https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm#The-biggest-of-the-praise-is-to,-who-provided-all-my-inspiration-and-motivation-on-the-subject,-Prof.-Dr.-Asim-Egemen-Y%C4%B1lmaz.)\n\n[iframe](https://www.kaggleusercontent.com/kf/101086193/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lmytDSd_jrGqB_fEJ5K4sA.4y2AthlDzFqEkKi7kyt0ie_eTDyqk5oaXZoncRxAxdKqeUbTP6xrywE5Kmzkzr19hA-YYGpfIhZViOfiB6OBwns7WZjQEbjD4SDk2ORVCQ3GO8H2P-a7oeI7dBl6dIwpYLjRtAsvB0NPcNVoUJMZRAdkJyvVH0s1p7sM9Cs6VnEvu9G_Eo_ojAizjHQc7a3coiMbMNEY5HXaqFfOGy9QbyIf7IiFP3tm0z5rqNfrSA2ALzF-zmD6LbKjs4L2LCbWbXzzc491kAn-O8gVrwNeT-MZPszX8eSZ5XjF6PO8OMiCbfBGA-e_oDj-XuYnEo06rPpPrCb3QPFzJrcdNh-jgxErDCx9Z4gtF6xeKh6pMy4GTjWIdFT-8d8yAYRrsPHa7T_BBxyfWCARnPwBwCeAy3OsEQfOOImS6zZAY6Sa5xJx0jEcmPkAQD4x9renz27HaLWaaYwS-o-pddqwRthlK2yHslUYLnEO8Y2P7wfX1qv2THTrc5qmt9EXrSiX66coqwMcfR2vf9QnZkYarUbNc86cjxb9fs3JO7-wLoj4w258N79ZBdEC3A7OCkqzBnpSDUmLFOphHeWddrTcbfpJEI1CuY5haCNjgZk-2OU-YHIjMCMSSmN_SC8vto3P_c42DCokFM7YlT4_tvPb_fK9aFBefX1l_Mhsu6aApzW4M_g.3xTTIfUmFdXzVY6ukmISQA/__results__.html?sharingControls=true)\n\n## License\n\nThis Notebook has been released under the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) open source license.\n\n## Continue exploring\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/input_light.svg)\n\n\n\n\n\n\n\nInput\n\n1 file\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/output_light.svg)\n\n\n\n\n\n\n\nOutput\n\n0 files\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/logs_light.svg)\n\n\n\n\n\n\n\nLogs\n\n65.2 second run - successful\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/comments_light.svg)\n\n\n\n\n\n\n\nComments\n\n18 comments\n\n\n\n\narrow\\_right\\_alt",
      "url": "https://www.kaggle.com/code/zzettrkalpakbal/tutorial-of-genetic-algorithm"
    },
    {
      "title": "Genetic Algorithm - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/khusheekapoor/genetic-algorithm/code"
    },
    {
      "title": "Bios8366/notebooks/Section1_2-Combinatorial-Optimization.ipynb ...",
      "text": "[Skip to content](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[fonnesbeck](https://github.com/fonnesbeck)/ **[Bios8366](https://github.com/fonnesbeck/Bios8366)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Ffonnesbeck%2FBios8366) You must be signed in to change notification settings\n- [Fork\\\n355](https://github.com/login?return_to=%2Ffonnesbeck%2FBios8366)\n- [Star\\\n538](https://github.com/login?return_to=%2Ffonnesbeck%2FBios8366)\n\n\n## Files\n\nmaster\n\n/\n\n# Section1\\_2-Combinatorial-Optimization.ipynb\n\nCopy path\n\nBlame\n\nBlame\n\n## Latest commit\n\n## History\n\n[History](https://github.com/fonnesbeck/Bios8366/commits/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb)\n\n764 lines (764 loc) \u00b7 31.5 KB\n\nmaster\n\n/\n\n# Section1\\_2-Combinatorial-Optimization.ipynb\n\nTop\n\n## File metadata and controls\n\n- Preview\n\n- Code\n\n- Blame\n\n\n764 lines (764 loc) \u00b7 31.5 KB\n\n[Raw](https://github.com/fonnesbeck/Bios8366/raw/refs/heads/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb)\n\nLoading[iframe](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&bypass_fastly=true&color_mode=auto&commit=b50cd42ca4258a08368a63b97140e3868aeb459c&device=unknown_device&docs_host=https%3A%2F%2Fdocs.github.com&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f666f6e6e65736265636b2f42696f73383336362f623530636434326361343235386130383336386136336239373134306533383638616562343539632f6e6f7465626f6f6b732f53656374696f6e315f322d436f6d62696e61746f7269616c2d4f7074696d697a6174696f6e2e6970796e62&logged_in=false&nwo=fonnesbeck%2FBios8366&path=notebooks%2FSection1_2-Combinatorial-Optimization.ipynb&platform=mac&repository_id=8252520&repository_type=Repository&version=129#b5b25371-5099-4070-ae79-1010c8221b46)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section1_2-Combinatorial-Optimization.ipynb"
    },
    {
      "title": "",
      "text": "000\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053\nUnder review as a conference paper at ICLR 2025\nLEARNING TO EXPLORE AND EXPLOIT WITH GNNS\nFOR UNSUPERVISED COMBINATORIAL OPTIMIZATION\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nCombinatorial optimization (CO) problems are pervasive across various domains,\nbut their NP-hard nature often necessitates problem-specific heuristic algorithms.\nRecent advancements in deep learning have led to the development of learning\u0002based heuristics, yet these approaches often struggle with limited search capabilities.\nWe introduce Explore-and-Exploit GNN (X2GNN, pronounced x-squared GNN),\na novel unsupervised neural framework that combines exploration and exploitation\nfor combinatorial search optimization: i) Exploration - X2GNN generates multiple\nsolutions simultaneously, promoting diversity in the search space; (ii) Exploitation\n- X2GNN employs neural stochastic iterative refinement, where sampled partial\nsolutions guide the search toward promising regions and help escape local optima.\nX2GNN employs neural stochastic iterative refinement to exploit partial existing\nsolutions, guiding the search toward promising regions and helping escape local\noptima. By balancing exploration and exploitation X2GNN achieves superior\nperformance and generalization on several graph CO problems including Max Cut,\nMax Independent Set, and Max Clique. Notably, for large Max Clique problems,\nX2GNN consistently generates solutions within 1.2% of optimality, while other\nstate-of-the-art learning-based approaches struggle to reach within 22% of optimal.\nMoreover, X2GNN consistently generates better solutions than Gurobi on large\ngraphs for all three problems under reasonable time budgets. Furthermore, X2GNN\nexhibits exceptional generalization capabilities. For the Maximum Independent\nSet problem, X2GNN outperforms state-of-the-art methods even when trained\non smaller or out-of-distribution graphs compared to the test set. Our framework\noffers a more effective and flexible approach to neural combinatorial optimization,\naddressing a key challenge in the field and providing a promising direction for\nfuture research in learning-based heuristics for combinatorial optimization.\n1 INTRODUCTION\nCombinatorial optimization (CO) problems aim to find a discrete solution that optimizes an objec\u0002tive function from a discrete set of feasible solutions constrained by specific problem parameters.\nThese optimization problems frequently emerge in commercial, governmental, and scientific con\u0002texts, prompting extensive study in fields such as mathematics, computer science, and operations\nresearch. Many combinatorial optimization problems are NP-hard, indicating no polynomial-time\nexact algorithm exists unless P = NP.\nExact algorithms, which achieve optimality by implicitly or explicitly considering all possible\nsolutions, are typically only tractable for small instances due to their exponential worst-case time\ncomplexity. Consequently, another body of work focuses on heuristic algorithms that quickly attain\nhigh-quality solutions without optimality guarantees. Work here seeks to balance computational\ntime and solution quality, often exploring the search space through multiple different solutions, and\nexploiting promising ones (Eiben and Schippers, 1998).\nHeuristics are often hand-crafted for a specific problem, exploring problem intricacies to achieve peak\nperformance for a given problem distribution, requiring time and domain expertise. The rise of deep\nlearning has enabled new learning-based heuristics that can be automatically tuned for performance\nusing data. However, current approaches are often limited in their search capabilities, often only\n1\n054\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\nUnder review as a conference paper at ICLR 2025\niteratively improving a single solution, or naively restarting and forgetting previously generated\nsolutions.\nIn this paper, we introduce Explore-and-Exploit GNN (X2GNN, pronounced X-squared GNN),\na novel unsupervised neural combinatorial optimization framework. X2GNN combines effective\nexploration of the solution space with intelligent exploitation of partial solutions.\nOur Main Contributions are:\n1. We propose X2GNN, which combines exploration and exploitation for combinatorial\nsearch optimization: (i) Exploration - X2GNN simultaneously generates multiple coupled\nsolutions, promoting diversity in the search space; (ii) Exploitation - X2GNN employs\nneural stochastic iterative refinement to exploit partial existing solutions, guiding the search\ntoward promising regions and helping escape local optima.\n2. State-of-the-art performance: X2GNN outperforms existing learning-based approaches on\nbenchmark datasets for the maximum cut, maximum independent set, and maximum clique\nproblems. Additionally, X2GNN is competitive with general OR approaches like Gurobi,\nand problem specific heuristics like KaMIS, offering improved or comparable solution\nquality at similar time cutoffs.\n3. Strong generalization capabilities: X2GNN generalizes graphs that are out-of-distribution\nor up to 4 times larger than those seen during training, while still significantly outperforming\nother learning-based methods trained on the same distribution as the test set.\n4. Rigorous Evaluation: We enhance existing benchmark datasets commonly used in the ML\nfor combinatorial optimization community by including strong traditional baselines and\nevaluating solvers at comparable runtimes. We additionally allow solvers a 30-minute\ntimelimit, which is at least 24 times longer than our longest-running model.\n2 RELATED WORK\nThe broad intersection between machine learning (ML) and combinatorial optimization (CO) has seen\nmuch work with different facets explored in various surveys (Bengio et al., 2021; Kotary et al., 2021;\nCappart et al., 2023). State-of-the-art learning-based primal heuristics specifically can be broadly\ncategorized by their training supervision and solution construction methods. Supervised learning\napproaches use training data composed of problem instances and corresponding solutions derived\nfrom existing solvers (Khalil et al., 2016; Selsam et al., 2019; Nair et al., 2020; Sun and Yang, 2023).\nHowever, these may face challenges such as the unavailability of high-quality solvers for all problems\nand poor generalization capabilities across different problem instances (Yehuda et al., 2020). Despite\nthese challenges, recent studies have shown that diffusion-based training can enhance generalization\nin supervised learning (Sun and Yang (2023)).\nUnsupervised learning approaches have also been explored, differing primarily in whether solutions\nare constructed autoregressively or not. Earlier non-autoregressive models generate a \u2018soft\u2019 solution\nin a single step, which is then decoded into a final solution using methods ranging from simple\ngreedy (Karalias and Loukas, 2020) decoding to more sophisticated techniques (Min et al., 2022). As\nSanokowski et al. (2024) noted, these approaches can be classified as single-step diffusion methods.\nThese models are notably faster and more scalable than their autoregressive counterparts. Sanokowski\net al. (2023) suggest that non-autoregressive solution construction may fail to capture essential\ndependencies among problem variables and they refer to these types of methods as mean-field\napproximations.\nThe earlier single-step non-autoregressive methods are outperformed by autoregressive construction\ngoverned by MDPs (Sanokowski et al., 2023; Zhang et al., 2023). However, these models are trained\nusing reinforcement learning (RL) and face high computational needs and poor generaliza...",
      "url": "https://openreview.net/notes/edits/attachment?id=Ly9XkbE640&name=pdf"
    },
    {
      "title": "Combinatorial Optimization \u2013 Notes and Study Guides - Fiveable",
      "text": "study content for ap studentsNEWAMSCO guided notestools for ap teachersNEWexport quizzes to google forms\n[upgrade](https://fiveable.me/pricing)\nmaking it Fiveable...\n# \ud83e\uddeeCombinatorial Optimization study guides\n[## Unit 1 \u2013Combinatorial Optimization Foundations\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-1)\n[\n1.1\nOptimization problems and objectives\n](https://fiveable.me/combinatorial-optimization/unit-1/optimization-problems-objectives/study-guide/Oii7tgZQvbYbUlCV)\n[\n1.2\nCombinatorial structures\n](https://fiveable.me/combinatorial-optimization/unit-1/combinatorial-structures/study-guide/LD9HFz8sOuknvfTm)\n[\n1.3\nComputational complexity basics\n](https://fiveable.me/combinatorial-optimization/unit-1/computational-complexity-basics/study-guide/3xATmGFelPwpvIG9)\n[\n1.4\nNP-completeness\n](https://fiveable.me/combinatorial-optimization/unit-1/np-completeness/study-guide/RnfanflM0tn4cnDZ)\n[\n1.5\nExact algorithms\n](https://fiveable.me/combinatorial-optimization/unit-1/exact-algorithms/study-guide/ODz4iuGe8nVSpZV2)\n[\n1.6\nHeuristics and approximation algorithms\n](https://fiveable.me/combinatorial-optimization/unit-1/heuristics-approximation-algorithms/study-guide/xaCHi0n9pOy4vGjd)\n[## Unit 2 \u2013Graph Theory and Algorithms\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-2)\n[\n2.1\nGraph representations\n](https://fiveable.me/combinatorial-optimization/unit-2/graph-representations/study-guide/JqyExgwPDBvFcV5n)\n[\n2.2\nGraph traversal algorithms\n](https://fiveable.me/combinatorial-optimization/unit-2/graph-traversal-algorithms/study-guide/i9xUOJSRjLCECc5x)\n[\n2.3\nShortest path problems\n](https://fiveable.me/combinatorial-optimization/unit-2/shortest-path-problems/study-guide/9jRxSFnmjgebqIIQ)\n[\n2.4\nMinimum spanning trees\n](https://fiveable.me/combinatorial-optimization/unit-2/minimum-spanning-trees/study-guide/Wq286tYM0hC6caX9)\n[\n2.5\nMaximum flow problems\n](https://fiveable.me/combinatorial-optimization/unit-2/maximum-flow-problems/study-guide/RaR6rGM0zAJkq55u)\n[\n2.6\nMatching problems\n](https://fiveable.me/combinatorial-optimization/unit-2/matching-problems/study-guide/zvoer62r3O1Xhk86)\n[\n2.7\nGraph coloring\n](https://fiveable.me/combinatorial-optimization/unit-2/graph-coloring/study-guide/6YSKqJXazpSd1jJj)\n[## Unit 3 \u2013Linear Programming and Duality\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-3)\n[\n3.1\nLinear programming formulation\n](https://fiveable.me/combinatorial-optimization/unit-3/linear-programming-formulation/study-guide/EhnJSdVbqytkGsqa)\n[\n3.2\nSimplex method\n](https://fiveable.me/combinatorial-optimization/unit-3/simplex-method/study-guide/TIZaB2147pGFM85n)\n[\n3.3\nDuality theory\n](https://fiveable.me/combinatorial-optimization/unit-3/duality-theory/study-guide/gAry8jacZtehxMya)\n[\n3.4\nSensitivity analysis\n](https://fiveable.me/combinatorial-optimization/unit-3/sensitivity-analysis/study-guide/Beq2Q39Myl3xvuGU)\n[\n3.5\nInterior point methods\n](https://fiveable.me/combinatorial-optimization/unit-3/interior-point-methods/study-guide/TeYD5zgv9SlKbm6c)\n[## Unit 4 \u2013Integer programming\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-4)\n[\n4.1\nInteger linear programming formulation\n](https://fiveable.me/combinatorial-optimization/unit-4/integer-linear-programming-formulation/study-guide/0yczFt5n6RmubAlP)\n[\n4.2\nBranch and bound\n](https://fiveable.me/combinatorial-optimization/unit-4/branch-bound/study-guide/6p8d0TSzwgBuWvhJ)\n[\n4.3\nCutting plane methods\n](https://fiveable.me/combinatorial-optimization/unit-4/cutting-plane-methods/study-guide/Ce5Hu7KAAYotUXYZ)\n[\n4.4\nBranch and cut\n](https://fiveable.me/combinatorial-optimization/unit-4/branch-cut/study-guide/GtgDOvO1eUKRqRyo)\n[\n4.5\nColumn generation\n](https://fiveable.me/combinatorial-optimization/unit-4/column-generation/study-guide/HpSWXwu7Ir1SZg2b)\n[## Unit 5 \u2013Network Flows and Matchings\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-5)\n[\n5.1\nMaximum flow algorithms\n](https://fiveable.me/combinatorial-optimization/unit-5/maximum-flow-algorithms/study-guide/PMSQGaKdPpGiF9g8)\n[\n5.2\nMinimum cost flow problems\n](https://fiveable.me/combinatorial-optimization/unit-5/minimum-cost-flow-problems/study-guide/wosSyIlQwLpGzgw5)\n[\n5.3\nBipartite matching\n](https://fiveable.me/combinatorial-optimization/unit-5/bipartite-matching/study-guide/h3oTvYl9PiRnm6Y1)\n[\n5.4\nWeighted bipartite matching\n](https://fiveable.me/combinatorial-optimization/unit-5/weighted-bipartite-matching/study-guide/Y1H15kDlYWlbrYAU)\n[\n5.5\nNon-bipartite matching\n](https://fiveable.me/combinatorial-optimization/unit-5/non-bipartite-matching/study-guide/o9UuyYAu5b1et6GC)\n[## Unit 6 \u2013Matroids and Greedy Algorithms\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-6)\n[\n6.1\nMatroid theory\n](https://fiveable.me/combinatorial-optimization/unit-6/matroid-theory/study-guide/6p2uaYlAdPxvEvYn)\n[\n6.2\nGreedy algorithm for matroids\n](https://fiveable.me/combinatorial-optimization/unit-6/greedy-algorithm-matroids/study-guide/MtfN5qb8vaAAtlt3)\n[\n6.3\nMatroid intersection\n](https://fiveable.me/combinatorial-optimization/unit-6/matroid-intersection/study-guide/0C0ICT0EJw8h951J)\n[\n6.4\nSubmodular functions\n](https://fiveable.me/combinatorial-optimization/unit-6/submodular-functions/study-guide/c2gIvs1lqDRrjwqX)\n[\n6.5\nApplications of matroids\n](https://fiveable.me/combinatorial-optimization/unit-6/applications-matroids/study-guide/mfknWuktpdfPsQFU)\n[## Unit 7 \u2013Dynamic programming\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-7)\n[\n7.1\nPrinciples of dynamic programming\n](https://fiveable.me/combinatorial-optimization/unit-7/principles-dynamic-programming/study-guide/XVLcAgxyRWrBLSwe)\n[\n7.2\nOptimal substructure\n](https://fiveable.me/combinatorial-optimization/unit-7/optimal-substructure/study-guide/QVpx7Ac8qdaWZX2s)\n[\n7.3\nOverlapping subproblems\n](https://fiveable.me/combinatorial-optimization/unit-7/overlapping-subproblems/study-guide/aAcqwH2ea19tfi5W)\n[\n7.4\nMemoization\n](https://fiveable.me/combinatorial-optimization/unit-7/memoization/study-guide/WoOZFVyWNwbSLw31)\n[\n7.5\nApplications of dynamic programming\n](https://fiveable.me/combinatorial-optimization/unit-7/applications-dynamic-programming/study-guide/Y86YgjRduClbeN0X)\n[## Unit 8 \u2013Approximation Algorithms in Optimization\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-8)\n[\n8.1\nApproximation ratio\n](https://fiveable.me/combinatorial-optimization/unit-8/approximation-ratio/study-guide/k3zocu96uoFr06J2)\n[\n8.2\nGreedy approximation algorithms\n](https://fiveable.me/combinatorial-optimization/unit-8/greedy-approximation-algorithms/study-guide/LGB3wBj4i0e6Q8Xt)\n[\n8.3\nLinear programming relaxation\n](https://fiveable.me/combinatorial-optimization/unit-8/linear-programming-relaxation/study-guide/8nopEOc0fXHy5LEu)\n[\n8.4\nRandomized approximation algorithms\n](https://fiveable.me/combinatorial-optimization/unit-8/randomized-approximation-algorithms/study-guide/N1UnorEeSuyp8cqm)\n[\n8.5\nPolynomial-time approximation schemes (PTAS)\n](https://fiveable.me/combinatorial-optimization/unit-8/polynomial-time-approximation-schemes-ptas/study-guide/JDnPT98Y6PEeHaHT)\n[## Unit 9 \u2013Metaheuristics &amp; Local Search Techniques\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-9)\n[\n9.1\nLocal search techniques\n](https://fiveable.me/combinatorial-optimization/unit-9/local-search-techniques/study-guide/GD5MR90nqSAulFOR)\n[\n9.2\nSimulated annealing\n](https://fiveable.me/combinatorial-optimization/unit-9/simulated-annealing/study-guide/JqfiEngDhPIlSeu6)\n[\n9.3\nTabu search\n](https://fiveable.me/combinatorial-optimization/unit-9/tabu-search/study-guide/grDBu3lBmHF5V9Ya)\n[\n9.4\nGenetic algorithms\n](https://fiveable.me/combinatorial-optimization/unit-9/genetic-algorithms/study-guide/bFaKWs8avgpiXlfd)\n[\n9.5\nAnt colony optimization\n](https://fiveable.me/combinatorial-optimization/unit-9/ant-colony-optimization/study-guide/2iTqA4XBZ40YnP9z)\n[## Unit 10 \u2013Constraint Programming in Optimization\u00a0\u2192\n](https://fiveable.me/combinatorial-optimization/unit-10)\n[\n10.1\nConstraint satisfaction problems\n](https://fiveable.me/...",
      "url": "https://fiveable.me/combinatorial-optimization"
    },
    {
      "title": "Optimisation In The Real World | Stephen J. Maher",
      "text": "Optimisation In The Real World | Stephen J. Maher\n[Optimisation in the Real World](https://www.optimisationintherealworld.co.uk)\n[****](#sidewidgetarea)\n# Optimisation in the Real World\nOptimisation exists everywhere in this world. The operation of airlines, the rostering of staff, the scheduling of sporting competitions and the layout of shelves in a supermarket are all examples of optimisation in the real world. Our lives are affected by optimisation, either by our own doing or through the products and services we use. Below you will find real world examples of optimisation in practice.\n[](#band-tour)**\n# The Band Manager Problem\nHow does a band manager schedule locations for an international tour? A band tour manager has many things that they need to consider when organising the locations for gigs. These include: the size and availability of venues, the countries and cities to visit, maximum length of the tour.\nA band manager, whether they know this or not, will need to solve an optimisation problem. In an optimisation problem, these requirements or restrictions called constraints. The restrictions of the problem tell the band manager what tour options are possible.\n[](#band-manager)**\nThe band manager knows the locations for the concerts and the distance between each of the locations. The band manager may then ask the question \u201cWhat is the shortest route that visits every tour location and returns the band home?\u201d\nThe answer to this question come for solving the**Traveling Salesman Problem**. Read on for more information about this classical, and very difficult, problem.\n[](#traveling-salesman)**\n# The Traveling Salesman Problem\nThe travelling salesman problem is a classical optimisation problem that arises as part of many real world applications. These applications include sports scheduling, mail delivery and more fun activities such as planning tours for vacations. It is also a key component of many other problems in mathematics and optimisation.\nThe travelling salesman problem has been the focus of much research for 60 years. This research has developed mathematical techniques that has lead to advances in our ability to solve real-world problems.\n[](#grocery-deliveries)**\n# The Grocery Delivery Problem\nOnline grocery shopping is very convenient for the consumer, but it makes the supermarket business much more complex. Instead of just stocking a store, a supermarket must also ensure the customer receives their goods at a specified time. These times are typically given as windows, say 8pm to 9pm. But, how does the supermarket decide the actual time that the delivery will be made to the customer?\nThe supermarkets are solving a problem called the Vehicle routing problem. In fact they are solving a more complex variant called the Vehicle routing problem with time windows. Scroll down to read more.\n[](#vehicle-routing)**\n# The Vehicle Routing Problem\nThe basic vehicle routing problem involves the design of a collection of delivery routes that ensure every customer is visited while minimising transportation costs, such as fuel. This can get more complex, such as in the case of grocery delivery, where there are additional constraints. The restrictions and conditions that the supermarket must take into account include the capacity of the vehicle, the locations that must be visited and the time windows when they can arrive.\nMany methods have been developed to solve the vehicle routing problem. A very successful algorithm for solving this problem, which is still the focus of much research, is called column generation.\n[Read More](https://www.optimisationintherealworld.co.uk/vehicle-routing-problem/)\n[](#bike-sharing)**\n# The Bike Sharing Problem\nHow are the bike sharing docking locations picked?\nMany bike sharing systems around the world consist of two main components: the bikes and the stations to dock the bikes. A bike sharing system will only be useful if the docking stations are located where people want to collect bikes and where they want to drop them off. An interesting, and very commonly used optimisation problem, is the problem of determining the best locations for the docking stations.\nThe problem that is solved to identify the docking stations is the called the Facility location problem. This problem finds the best docking station locations that minimises the investment and achieves a minimum level of demand. There is more on this classical problem below.\n[Read More](https://www.optimisationintherealworld.co.uk/bike-sharing/)\n[](#Facility-location)**\n# The Facility Location Problem\nThe core of the facility location problem is to identify the locations where to open facilities in order to satisfy the demand of all customers at a minimum cost. This is typically used to identify the locations to open factories or warehouses. The costs for these problems come from the opening of the facility and the transportation of the goods to each of the customers.\nThe facility location problem is a classical optimisation problem that can be solved using a technique called Benders\u2019 decomposition. Similar to the Column Generation approach, Benders\u2019 decomposition considers only a reduced version of the problem.\n[Read More](https://www.optimisationintherealworld.co.uk/facility-location/)\n&copy; 2026 Optimisation in the Real World. All Rights Reserved.[Web Design and Graphic Design](http://www.urban-attic.co.uk/)by Urban Attic.![Urban Attic Logo](https://www.optimisationintherealworld.co.uk/wp-content/uploads/2018/01/urban-attic-copyright-icon.png)\n[](#)\n* [Home](https://www.optimisationintherealworld.co.uk/)\n* [Blog](https://www.optimisationintherealworld.co.uk/blog/)\n* [About](https://www.optimisationintherealworld.co.uk/about/)\n* [Contact](https://www.optimisationintherealworld.co.uk/contact/)\n**",
      "url": "https://www.optimisationintherealworld.co.uk"
    },
    {
      "title": "Machine Learning Kaggle Competition: Part Three Optimization",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdb04ea415507&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-db04ea415507---------------------------------------)\n\n\u00b7\n\n[![TDS Archive](https://miro.medium.com/v2/resize:fill:76:76/1*JEuS4KBdakUcjg9sC7Wo4A.png)](https://medium.com/data-science?source=post_page---post_publication_sidebar-7f60cf5620c9-db04ea415507---------------------------------------)\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n# Machine Learning Kaggle Competition: Part Three Optimization\n\n## Getting the most out of a machine learning model\n\n[![Will Koehrsen](https://miro.medium.com/v2/resize:fill:64:64/1*SckxdIFfjlR-cWXkL5ya-g.jpeg)](https://medium.com/@williamkoehrsen?source=post_page---byline--db04ea415507---------------------------------------)\n\n[Will Koehrsen](https://medium.com/@williamkoehrsen?source=post_page---byline--db04ea415507---------------------------------------)\n\nFollow\n\n15 min read\n\n\u00b7\n\nJul 20, 2018\n\n--\n\n3\n\nListen\n\nShare\n\nHow best to describe a Kaggle contest? It\u2019s a machine learning education disguised as a competition! Although there are [valid criticisms](https://www.quora.com/Why-did-Kaggle-start-having-a-bad-reputation) of [Kaggle](https://www.kaggle.com/), overall, it\u2019s a great community that provides [interesting problems](https://www.kaggle.com/competitions), thousands of data scientists willing to share their knowledge, and an ideal environment for [exploring new ideas](https://www.kaggle.com/learn/overview). As evidence of this, I never would have learned about the Gradient Boosting Machine, or, one of the topics of this article, automated model optimization, were it not for the [Kaggle Home Credit contest](https://www.kaggle.com/c/home-credit-default-risk/).\n\nIn this article, part three of a series ( [Part One: Getting Started](https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426) and [Part Two: Improving)](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8) documenting my work for this contest, we will focus on a crucial aspect of the machine learning pipeline: model optimization through hyperparameter tuning. In the second article, we decided on the Gradient Boosting Machine as our model of choice, and now we have to get the most out of it through optimization. We\u2019ll do this primarily with two methods: random search and automated tuning with Bayesian optimization.\n\nAll the work presented here is available to run on Kaggle in the following notebooks. The article itself will highlight the key ideas but the code details are all in the notebooks (which are free to run with nothing to install!)\n\n1. [Random and Grid Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)\n2. [Automated Hyperparameter tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)\n3. [Tuning Results](https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt/notebook)\n\nFor this article we will skip the background, so if at any time you feel lost, I encourage you to go to the previous articles and notebooks. All of the notebooks can be run on Kaggle without the need to download anything, so I highly recommend checking them out or competing!\n\n## Recap\n\nIn the [first part of this series](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8), we got familiar with the dataset, performed exploratory data analysis, tried our hand at feature engineering, and built a few baseline models. Our public leaderboard score from this round was **0.678**(which at the moment would place us lower than 4000 on the leaderboard).\n\n[Part two](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8) involved in-depth manual feature engineering, followed by feature selection and more modeling. Using the expanded (and then contracted) set of features, we ended up with a score of **0.779**, quite an improvement from the baseline, but not quite in the top 50% of competitors. In this article, we will once again better our score and move up 1000 places on the leaderboard.\n\n# Machine Learning Optimization\n\nOptimization in the context of machine learning means finding the set of model hyperparameter values that yield the highest cross validation score for a given dataset. Model hyperparameters, in contrast to model parameters that are learned during training, are set by the data scientist before training. The number of layers in a deep neural network is a model hyperparameter while the splits in a decision tree are model parameters. I like to think of model hyperparameters as settings that we need to tune for a dataset: **the ideal combination of values is different for every problem**!\n\nA machine learning model has to be tuned like a radio \u2014 if anyone remembers what a radio was!\n\nThere are a handful of ways to tune a machine learning model:\n\n1. **Manual**: select hyperparameters with intuition/experience/guessing, train the model with the values, and find the validation score Repeat process until you run out of patience or are satisfied with the results.\n2. **Grid Search**: set up a hyperparameter grid and for each combination of values, train a model and find the validation score. In this approach, every combination of hyperparameter values is tried which is very inefficient!\n3. **Random search**: set up a hyperparameter grid and select _random_ combinations of values to train the model and find the validation score. The number of search iterations is based on time/resources.\n4. **Automated Hyperparameter Tuning**: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters. These methods use the previous results to choose the next hyperparameter values in an _informed search_ compared to random or grid which are _uninformed_ methods.\n\nThese are presented in order of increasing efficiency, with manual search taking the most time (and often yielding the poorest results) and automated methods converging on the best values the quickest, although, as with many topics in machine learning, this is not always the case! [As shown in this great paper,](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) random search does surprisingly well (which we\u2019ll see shortly).\n\n(There are also other hyperparameter tuning methods such as [evolutionary](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization) and [gradient-based](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Gradient-based_optimization). There are constantly better methods being developed, so make sure to stay up to date on the current best practice!)\n\nIn the second part of ...",
      "url": "https://medium.com/data-science/machine-learning-kaggle-competition-part-three-optimization-db04ea415507"
    },
    {
      "title": "Regularized Langevin Dynamics for Combinatorial Optimization",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/html/2502.00277v3"
    }
  ]
}