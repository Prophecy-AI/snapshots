## Current Status
- Best CV score: 70.316492 (exp_022, verified LB)
- Best LB score: 70.316492 (CV = LB exactly for this problem)
- Target: 68.871657 | Gap: 1.44 points (2.10%)
- Submissions used: 13/100 (87 remaining)
- Consecutive experiments with no improvement: 11

## ⚠️ CRITICAL SITUATION - PUBLIC SOLUTION CEILING REACHED

After 30 experiments with 11 consecutive failures, ALL optimization approaches have been exhausted:
- SA, bbox3, outer_chain, chaos_optimizer, genetic algorithm, lattice packing, NFP, branch-and-bound, jostle, BLF, constructive interlock - ALL FAILED
- External data fully mined (283+ CSV files)
- Public kernel ceiling confirmed at ~70.3

## ⛔ FORBIDDEN (DO NOT REPEAT THESE FAILURES)

- ❌ bbox3/SA/tree_packer with "more iterations" - SAME SCORE
- ❌ outer_chain, chaos_optimizer - ZERO IMPROVEMENTS
- ❌ Ensemble from existing sources - ALREADY EXHAUSTED
- ❌ Any discrete optimization approach

## ✅ REQUIRED: GRADIENT-BASED CONTINUOUS OPTIMIZATION

Since ALL discrete optimizers are stuck, try gradient-based continuous optimization:

### Create `experiments/031_gradient_optimization/`

```python
import numpy as np
from scipy.optimize import minimize
from shapely.geometry import Polygon
from shapely import affinity

# Tree geometry
TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]
TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]

def get_tree_polygon(x, y, deg):
    poly = Polygon(zip(TX, TY))
    poly = affinity.rotate(poly, deg, origin=(0, 0))
    poly = affinity.translate(poly, x, y)
    return poly

def compute_bbox_score(params, n):
    """Compute bounding box score from parameters [x0,y0,deg0,x1,y1,deg1,...]"""
    trees = []
    for i in range(n):
        x, y, deg = params[3*i], params[3*i+1], params[3*i+2]
        trees.append(get_tree_polygon(x, y, deg))
    
    all_bounds = [t.bounds for t in trees]
    minx = min(b[0] for b in all_bounds)
    miny = min(b[1] for b in all_bounds)
    maxx = max(b[2] for b in all_bounds)
    maxy = max(b[3] for b in all_bounds)
    
    side = max(maxx - minx, maxy - miny)
    return side**2 / n

def compute_overlap_penalty(params, n, penalty_weight=1000):
    """Penalty for overlapping trees"""
    trees = []
    for i in range(n):
        x, y, deg = params[3*i], params[3*i+1], params[3*i+2]
        trees.append(get_tree_polygon(x, y, deg))
    
    penalty = 0
    for i in range(n):
        for j in range(i+1, n):
            if trees[i].intersects(trees[j]) and not trees[i].touches(trees[j]):
                overlap_area = trees[i].intersection(trees[j]).area
                penalty += overlap_area * penalty_weight
    return penalty

def objective(params, n):
    return compute_bbox_score(params, n) + compute_overlap_penalty(params, n)

# Test on small N first
for n in [5, 10, 15, 20]:
    baseline_params = load_baseline_params(n)  # From current submission
    result = minimize(objective, baseline_params, args=(n,), method='L-BFGS-B')
    
    baseline_score = compute_bbox_score(baseline_params, n)
    optimized_score = compute_bbox_score(result.x, n)
    
    print(f"N={n}: baseline={baseline_score:.6f}, optimized={optimized_score:.6f}")
    if optimized_score < baseline_score - 0.0001:
        print(f"  ✅ IMPROVEMENT: {baseline_score - optimized_score:.6f}")
```

### Key Points:
1. **Start from baseline** - Use current best as initial point
2. **Test small N first** - N=5, 10, 15, 20 before scaling up
3. **Track per-N improvements** - Any improvement is valuable
4. **Validate no overlaps** - Check final solution is valid

## Alternative: Numerical Gradient Approximation

If scipy's L-BFGS-B doesn't work, try numerical gradients:

```python
def numerical_gradient(f, x, eps=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus = x.copy()
        x_plus[i] += eps
        x_minus = x.copy()
        x_minus[i] -= eps
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return grad

def gradient_descent(params, n, lr=0.001, steps=1000):
    for step in range(steps):
        grad = numerical_gradient(lambda p: objective(p, n), params)
        params = params - lr * grad
        
        if step % 100 == 0:
            score = compute_bbox_score(params, n)
            print(f"Step {step}: score = {score:.6f}")
    return params
```

## Per-N Tracking (MANDATORY)

After optimization:
```python
for n in range(1, 201):
    my_score = optimized_scores[n]
    baseline_score = baseline_scores[n]
    if my_score < baseline_score - 1e-6:
        print(f"N={n}: IMPROVED by {baseline_score - my_score:.8f}")
        # Save this configuration
```

## Submission Strategy

- **SUBMIT after this experiment** - 87 submissions remaining
- Even if total score doesn't improve, track per-N improvements
- LB feedback is valuable for understanding what works

## Success Criteria

- **SUCCESS**: Find ANY N value where gradient optimization beats baseline
- **PARTIAL**: Understand why gradient optimization fails (informs next approach)
- **FAILURE**: No improvement - then try physics simulation (Pymunk)