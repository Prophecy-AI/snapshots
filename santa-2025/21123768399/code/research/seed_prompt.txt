# Santa 2025 Christmas Tree Packing - Evolved Seed Prompt (Loop 1)

## Current Status
- Best CV score: 70.676102 from exp_000 (001_baseline_preoptimized)
- Best LB score: 70.6761 (CV-LB gap: 0.0000 - perfect match!)
- Target: 68.922808 | Gap to target: 1.753 points (2.5%)

## CV-LB Relationship Analysis
- Only 1 submission so far, but CV = LB exactly
- This is an optimization problem, not ML - no distribution shift concerns
- The score is deterministic based on geometry

## Response to Evaluator
The evaluator correctly identified that:
1. **bbox3 binary has GLIBC mismatch** - Cannot run pre-compiled binary
2. **Pre-optimized solutions are at local optima** - Simple SA won't improve them
3. **Need to compile C++ optimizer from source** - Done! Created tree_packer.cpp

However, the evaluator's suggestion to "just run bbox3" won't work due to GLIBC issues.
The C++ optimizer I compiled showed 0 improvement, confirming solutions are at local optima.

**Key insight**: The saspav solution (70.676) is already the best public solution for ALL N values.
No ensemble can improve it. We need fundamentally different approaches.

## Critical Analysis: Why 70.676 → 68.922 is Hard

The gap of 1.75 points requires ~2.5% improvement across all N values.
This is NOT achievable by:
- ❌ Running more SA iterations (solutions at local optima)
- ❌ Ensembling public solutions (saspav already best for all N)
- ❌ fix_direction optimization (already applied, 0 improvement)

This IS achievable by:
- ✅ Better lattice/crystalline packing patterns for large N
- ✅ Constructive heuristics that build from scratch with different strategies
- ✅ Longer SA runs with higher temperature to escape local optima
- ✅ Different move operators (not just translate/rotate)

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Implement Advanced C++ Optimizer with Basin Hopping
The jonathanchan kernel shows a sophisticated C++ optimizer with:
- Back propagation (improve smaller N from larger N)
- Free-area & protrusion removal heuristics
- Edge-based slide compaction
- OpenMP parallelization

**Action**: Extract and compile the C++ code from the kernel, run with high iterations.

### 2. **[HIGH PRIORITY]** Simulated Annealing with Translation Patterns
The egortrushin kernel shows a technique using 2-tree translation patterns:
- Find optimal 2-tree configurations
- Tile them with fractional translations
- This can achieve better packing for large N

**Action**: Implement the translation-based SA approach.

### 3. **[MEDIUM PRIORITY]** Constructive Lattice Generation
For N > 58, crystalline packing is mathematically superior:
- Design 2-3 tree repeating units
- Optimize the unit cell dimensions
- Tile to fill the required N

**Action**: Implement lattice-based construction for large N.

### 4. **[MEDIUM PRIORITY]** Per-N Optimization Focus
The highest-scoring N values are:
- N=1: 0.661 (single tree - may be optimal)
- N=2: 0.451 (two trees)
- N=3: 0.435 (three trees)
- N=4-10: 0.37-0.42 range

Focus optimization effort on small N where improvements have highest impact.

## What NOT to Try
- ❌ Downloading more public datasets (all worse than saspav)
- ❌ Simple ensemble of existing solutions (saspav best for all N)
- ❌ Running pre-compiled bbox3 binary (GLIBC mismatch)
- ❌ Short SA runs (won't escape local optima)

## Technical Notes

### C++ Compilation
```bash
g++ -O3 -march=native -std=c++17 -fopenmp -o optimizer optimizer.cpp
```

### Tree Geometry (15 vertices)
```
TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]
TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]
```

### Score Calculation
```python
score = sum(side_n**2 / n for n in range(1, 201))
```

## SUBMISSION STRATEGY
- Remaining submissions: 98
- Submit after EVERY experiment that produces a valid submission
- LB feedback is free - USE IT!
- Current best: 70.6761 (need to beat 68.922808)

## Next Experiment: Advanced C++ Optimizer
1. Extract the full C++ optimizer from jonathanchan kernel
2. Compile with OpenMP
3. Run with high iterations (150000+) and multiple restarts
4. Apply back propagation to improve smaller N
5. Submit result to get LB feedback