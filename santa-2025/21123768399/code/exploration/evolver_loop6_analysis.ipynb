{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2cdc9d",
   "metadata": {},
   "source": [
    "# Loop 6 Analysis: Understanding the Stagnation\n",
    "\n",
    "## Key Observations:\n",
    "1. All 6 experiments have identical score: 70.676102\n",
    "2. The saspav baseline is at a strong local optimum\n",
    "3. Lattice-based SA approaches produce WORSE scores than baseline\n",
    "4. The C++ tree_packer_v18 showed 0 improvement\n",
    "5. Target is 68.922808 - a gap of 1.75 points (2.5%)\n",
    "\n",
    "## Critical Insight from Kernels:\n",
    "The seshurajup kernel (71.78) uses a sophisticated approach:\n",
    "1. **Fractional translation** - fine-tuning positions\n",
    "2. **Multi-restart SA** with population-based selection\n",
    "3. **Local search (ls_v3)** after SA\n",
    "4. **Per-N optimization** with different parameters for different N ranges\n",
    "\n",
    "The key difference: They optimize the EXISTING solution, not generate from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load baseline\n",
    "baseline = pd.read_csv('/home/code/santa-2025-csv/santa-2025.csv')\n",
    "print(f\"Baseline rows: {len(baseline)}\")\n",
    "print(baseline.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the score breakdown by N range\n",
    "import math\n",
    "from numba import njit\n",
    "from numba.typed import List as NumbaList\n",
    "\n",
    "# Tree shape constants\n",
    "TRUNK_W = 0.15\n",
    "TRUNK_H = 0.2\n",
    "BASE_W = 0.7\n",
    "MID_W = 0.4\n",
    "TOP_W = 0.25\n",
    "TIP_Y = 0.8\n",
    "TIER_1_Y = 0.5\n",
    "TIER_2_Y = 0.25\n",
    "BASE_Y = 0.0\n",
    "TRUNK_BOTTOM_Y = -TRUNK_H\n",
    "\n",
    "@njit(cache=True)\n",
    "def get_tree_vertices(cx, cy, angle_deg):\n",
    "    angle_rad = angle_deg * math.pi / 180.0\n",
    "    cos_a, sin_a = math.cos(angle_rad), math.sin(angle_rad)\n",
    "    vertices = np.empty((15, 2), dtype=np.float64)\n",
    "    pts = np.array([[0.0, TIP_Y], [TOP_W/2, TIER_1_Y], [TOP_W/4, TIER_1_Y],\n",
    "                    [MID_W/2, TIER_2_Y], [MID_W/4, TIER_2_Y], [BASE_W/2, BASE_Y],\n",
    "                    [TRUNK_W/2, BASE_Y], [TRUNK_W/2, TRUNK_BOTTOM_Y],\n",
    "                    [-TRUNK_W/2, TRUNK_BOTTOM_Y], [-TRUNK_W/2, BASE_Y],\n",
    "                    [-BASE_W/2, BASE_Y], [-MID_W/4, TIER_2_Y], [-MID_W/2, TIER_2_Y],\n",
    "                    [-TOP_W/4, TIER_1_Y], [-TOP_W/2, TIER_1_Y]], dtype=np.float64)\n",
    "    for i in range(15):\n",
    "        rx = pts[i,0] * cos_a - pts[i,1] * sin_a\n",
    "        ry = pts[i,0] * sin_a + pts[i,1] * cos_a\n",
    "        vertices[i,0], vertices[i,1] = rx + cx, ry + cy\n",
    "    return vertices\n",
    "\n",
    "@njit(cache=True)\n",
    "def polygon_bounds(vertices):\n",
    "    min_x, min_y = vertices[0,0], vertices[0,1]\n",
    "    max_x, max_y = vertices[0,0], vertices[0,1]\n",
    "    for i in range(1, vertices.shape[0]):\n",
    "        x, y = vertices[i,0], vertices[i,1]\n",
    "        if x < min_x: min_x = x\n",
    "        if x > max_x: max_x = x\n",
    "        if y < min_y: min_y = y\n",
    "        if y > max_y: max_y = y\n",
    "    return min_x, min_y, max_x, max_y\n",
    "\n",
    "@njit(cache=True)\n",
    "def get_side_length(all_vertices):\n",
    "    min_x, min_y, max_x, max_y = math.inf, math.inf, -math.inf, -math.inf\n",
    "    for verts in all_vertices:\n",
    "        x1, y1, x2, y2 = polygon_bounds(verts)\n",
    "        if x1 < min_x: min_x = x1\n",
    "        if y1 < min_y: min_y = y1\n",
    "        if x2 > max_x: max_x = x2\n",
    "        if y2 > max_y: max_y = y2\n",
    "    return max(max_x - min_x, max_y - min_y)\n",
    "\n",
    "@njit(cache=True)\n",
    "def calculate_score_numba(all_vertices):\n",
    "    side = get_side_length(all_vertices)\n",
    "    return side * side / len(all_vertices)\n",
    "\n",
    "print(\"Functions compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf182f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse baseline data\n",
    "def load_submission_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    all_xs, all_ys, all_degs = [], [], []\n",
    "    for n in range(1, 201):\n",
    "        group = df[df[\"id\"].str.startswith(f\"{n:03d}_\")].sort_values(\"id\")\n",
    "        for _, row in group.iterrows():\n",
    "            all_xs.append(float(str(row[\"x\"]).replace('s', '')))\n",
    "            all_ys.append(float(str(row[\"y\"]).replace('s', '')))\n",
    "            all_degs.append(float(str(row[\"deg\"]).replace('s', '')))\n",
    "    return np.array(all_xs), np.array(all_ys), np.array(all_degs)\n",
    "\n",
    "baseline_xs, baseline_ys, baseline_degs = load_submission_data('/home/code/santa-2025-csv/santa-2025.csv')\n",
    "print(f\"Loaded {len(baseline_xs)} trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-N scores\n",
    "scores_by_n = {}\n",
    "idx = 0\n",
    "for n in range(1, 201):\n",
    "    vertices = NumbaList()\n",
    "    for i in range(n):\n",
    "        vertices.append(get_tree_vertices(baseline_xs[idx+i], baseline_ys[idx+i], baseline_degs[idx+i]))\n",
    "    score = calculate_score_numba(vertices)\n",
    "    scores_by_n[n] = score\n",
    "    idx += n\n",
    "\n",
    "print(f\"Total score: {sum(scores_by_n.values()):.6f}\")\n",
    "print(f\"Target: 68.922808\")\n",
    "print(f\"Gap: {sum(scores_by_n.values()) - 68.922808:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find N values with highest contribution and potential for improvement\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ns = list(scores_by_n.keys())\n",
    "scores = list(scores_by_n.values())\n",
    "\n",
    "# Calculate theoretical minimum (perfect packing efficiency)\n",
    "# For a tree with area ~0.35, perfect packing would give score = area\n",
    "tree_area = 0.35  # approximate\n",
    "theoretical_min = [tree_area for _ in ns]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(ns, scores, alpha=0.7)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Score (sÂ²/n)')\n",
    "plt.title('Score by N')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "efficiency = [tree_area / s * 100 for s in scores]\n",
    "plt.plot(ns, efficiency)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Efficiency (%)')\n",
    "plt.title('Packing Efficiency by N')\n",
    "plt.axhline(y=100, color='r', linestyle='--', label='Perfect packing')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/loop6_score_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLowest efficiency N values:\")\n",
    "eff_dict = {n: tree_area / scores_by_n[n] * 100 for n in ns}\n",
    "sorted_eff = sorted(eff_dict.items(), key=lambda x: x[1])[:20]\n",
    "for n, eff in sorted_eff:\n",
    "    print(f\"  N={n}: {eff:.1f}% efficiency, score={scores_by_n[n]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73cde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how much improvement is needed per N to reach target\n",
    "target = 68.922808\n",
    "current_total = sum(scores_by_n.values())\n",
    "gap = current_total - target\n",
    "\n",
    "print(f\"Current total: {current_total:.6f}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Gap to close: {gap:.6f}\")\n",
    "print(f\"\\nIf we improve each N by the same percentage:\")\n",
    "required_improvement = gap / 200\n",
    "print(f\"  Need {required_improvement:.6f} improvement per N on average\")\n",
    "print(f\"  That's {required_improvement / (current_total/200) * 100:.2f}% improvement per N\")\n",
    "\n",
    "print(f\"\\nIf we focus on large N (101-200):\")\n",
    "large_n_total = sum(scores_by_n[n] for n in range(101, 201))\n",
    "print(f\"  Large N contributes: {large_n_total:.2f} ({large_n_total/current_total*100:.1f}%)\")\n",
    "print(f\"  Need {gap/large_n_total*100:.2f}% improvement in large N to close gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The seshurajup kernel uses a sophisticated C++ optimizer\n",
    "# that includes:\n",
    "# 1. sa_v3 - simulated annealing with temperature schedule\n",
    "# 2. ls_v3 - local search with 300 iterations\n",
    "# 3. fractional_translation - fine position adjustments\n",
    "# 4. Multi-restart with population selection\n",
    "\n",
    "# The key parameters from their code:\n",
    "print(\"Key parameters from seshurajup kernel:\")\n",
    "print(\"  SA: T0=1.0, T_min=0.000005, alpha=0.25, max_angle=70.0\")\n",
    "print(\"  Local search: 300 iterations\")\n",
    "print(\"  Fractional translation: 120-150 iterations\")\n",
    "print(\"  Restarts: 80 per N\")\n",
    "print(\"  Iterations: 20000-50000 per restart\")\n",
    "print(\"\")\n",
    "print(\"Their approach:\")\n",
    "print(\"  1. Start from existing good solution\")\n",
    "print(\"  2. Run SA with careful temperature schedule\")\n",
    "print(\"  3. Apply local search to refine\")\n",
    "print(\"  4. Apply fractional translation for micro-adjustments\")\n",
    "print(\"  5. Keep best from multiple restarts\")\n",
    "print(\"\")\n",
    "print(\"This is FUNDAMENTALLY DIFFERENT from our lattice approach:\")\n",
    "print(\"  - We tried to GENERATE new solutions from scratch\")\n",
    "print(\"  - They REFINE existing good solutions\")\n",
    "print(\"  - The saspav solution is already highly optimized\")\n",
    "print(\"  - Small refinements (fractional translation) may help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy recommendation\n",
    "print(\"=\"*60)\n",
    "print(\"STRATEGY RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"1. STOP trying to generate new solutions from scratch\")\n",
    "print(\"   - Lattice-based SA produces WORSE results\")\n",
    "print(\"   - The saspav solution is already highly optimized\")\n",
    "print(\"\")\n",
    "print(\"2. IMPLEMENT the seshurajup C++ optimizer\")\n",
    "print(\"   - It has proven to work (71.78 score)\")\n",
    "print(\"   - Uses fractional translation for micro-adjustments\")\n",
    "print(\"   - Multi-restart with population selection\")\n",
    "print(\"\")\n",
    "print(\"3. RUN LONGER optimization\")\n",
    "print(\"   - The tree_packer_v18 ran for only 85s\")\n",
    "print(\"   - The seshurajup kernel runs for 30+ minutes\")\n",
    "print(\"   - More iterations = more chances to find improvements\")\n",
    "print(\"\")\n",
    "print(\"4. FOCUS on specific N values\")\n",
    "print(\"   - Small N (1-20) may have room for improvement\")\n",
    "print(\"   - Large N (150-200) contribute most to score\")\n",
    "print(\"\")\n",
    "print(\"5. TRY different starting solutions\")\n",
    "print(\"   - The saspav solution may be at a local optimum\")\n",
    "print(\"   - Other public solutions may have different local optima\")\n",
    "print(\"   - Ensemble the best from each\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
