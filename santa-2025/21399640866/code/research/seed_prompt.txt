## Current Status
- Best CV score: 70.306694 (exp_043)
- Best LB score: 70.3067 (verified on Kaggle)
- Target: 68.866853 | Gap: 1.44 points (2.09%)
- Submissions used: 23/100 (77 remaining)

## Critical Analysis After 45 Experiments

### What We've Proven:
1. ❌ Local search (SA, bbox3) cannot improve the baseline
2. ❌ Backward/forward iteration cannot improve the baseline
3. ❌ Random restarts produce overlapping configurations
4. ❌ Constructive placement produces 40-50% WORSE configurations
5. ❌ Translation tiling produces WORSE configurations
6. ❌ All external CSV files have SAME or WORSE per-N scores
7. ❌ bbox3 binary doesn't work (GLIBC version mismatch)

### The Fundamental Problem:
The baseline is an ensemble of the best per-N solutions from thousands of optimization runs.
Top teams (68.89 score) have 900+ submissions accumulating improvements over weeks/months.
We cannot replicate this with local optimization in a single session.

## Response to Evaluator

The evaluator correctly identified:
1. 9 consecutive experiments with zero improvement = PLATEAU
2. Constructive methods produce worse results = baseline is sophisticated
3. 96 submissions remaining = underutilized resource

However, the evaluator's suggestions have been tried:
- External CSV files don't have better per-N solutions
- bbox3 binary doesn't work on this system

## Next Experiment: exp_046_long_parallel_sa

### Objective
Run simulated annealing for MUCH longer (1+ hours) on ALL N values using ALL 26 CPU cores.

### Why This Might Work
- Previous SA runs were only 10,000 iterations
- Top teams run optimizers for hours/days
- With 26 cores, we can run 26 N values in parallel
- Even small improvements on a few N values would help

### Implementation
```python
import multiprocessing
from concurrent.futures import ProcessPoolExecutor

# Configuration
ITERATIONS = 500_000  # 50x more than before
NUM_WORKERS = 26  # Use all CPU cores
TARGET_N_VALUES = list(range(1, 201))  # All N values

def optimize_single_n(n, iterations):
    """Run SA on a single N value for many iterations."""
    # Load baseline configuration for this N
    # Run SA with very slow cooling schedule
    # Return best score found
    pass

# Run in parallel
with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
    results = executor.map(optimize_single_n, TARGET_N_VALUES, [ITERATIONS]*200)
```

### Expected Outcome
- Likely: No improvement (baseline is at strong optimum)
- Possible: Small improvements on some N values (0.001-0.01 per N)
- Best case: 0.05-0.1 total improvement

## Alternative Approach: Genetic Algorithm

If long SA doesn't work, implement a genetic algorithm:

```python
# Population-based search for diversity
# Custom crossover: swap partial solutions between candidates
# Custom mutation: rotate/translate clusters

class GeneticOptimizer:
    def __init__(self, n, population_size=100):
        self.n = n
        self.population = self.initialize_population(population_size)
    
    def crossover(self, parent1, parent2):
        """Swap subsets of trees between parents."""
        pass
    
    def mutate(self, individual):
        """Small perturbations to positions/angles."""
        pass
    
    def evolve(self, generations=1000):
        """Run genetic algorithm."""
        pass
```

## What NOT to Try
- ❌ More constructive methods (proven to produce worse results)
- ❌ More ensemble variations (all external files have same/worse scores)
- ❌ bbox3 or other binaries (don't work on this system)
- ❌ Short optimization runs (already tried, no improvement)

## Submission Strategy
- SUBMIT any experiment that shows ANY improvement
- With 77 submissions remaining, we can afford to experiment
- Even 0.001 improvement is worth submitting

## Realistic Assessment

The target of 68.866853 requires 1.44 points of improvement.
This is a 2.09% reduction from our current score.

To achieve this, we would need:
- Average improvement of 0.0072 per N value, OR
- Significant improvements on specific N values

The top teams achieved this through:
1. Running custom optimizers for DAYS (not minutes)
2. Accumulating improvements over 900+ submissions
3. Using specialized hardware (24 CPUs, AVX2)

**We should try the long parallel SA run, but be prepared that it may not close the gap.**

## MANDATORY FIRST TASK

Before implementing anything new, run the long parallel SA:

```python
# experiments/046_long_parallel_sa/long_sa.py

import numpy as np
import pandas as pd
from numba import njit
import math
import time
from concurrent.futures import ProcessPoolExecutor
import multiprocessing

# Use ALL 26 CPU cores
NUM_WORKERS = 26
ITERATIONS_PER_N = 500_000  # 50x more than before
TEMPERATURE_SCHEDULE = "very_slow"  # T = T * 0.99999 instead of 0.999

# Run for 1+ hours total
# Track best solution found over time
# Save intermediate results every 10 minutes
```

⚠️ IMPORTANT: This experiment will take 1+ hours. Be patient.
