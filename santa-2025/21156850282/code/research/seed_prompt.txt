# Santa 2025 - Christmas Tree Packing Challenge Seed Prompt

## Current Status
- Best CV score: 70.659437 from exp_004 (clean ensemble)
- Best LB score: 70.659958 (exp_002 - clean ensemble)
- Target: 68.919154 | Gap to target: 1.74 points (2.53%)
- Submissions used: 4/100 (96 remaining)

## Critical Analysis

### What We've Tried (All Failed to Improve)
1. **Simulated Annealing** - Converges to same local optimum
2. **Genetic Algorithm** - Converges to baseline scores for all N=2-10
3. **Lattice Construction** - 20-60% worse than baseline
4. **Bottom-left Beam Search** - Produces worse results
5. **C++ Optimizer (sa_v1_parallel)** - No improvement on pre-optimized solutions
6. **Fractional Translation** - Minimal improvement (0.000009 for N=2)
7. **Overlap Fixing** - Overlaps too large to fix with perturbations

### Key Finding: The Source Gap
- **Top kernels use 19+ sources** (datasets + notebooks)
- **We have ~7 unique sources** (all converge to ~70.66)
- **The gap is DATA, not ALGORITHM** - we don't have access to the better solutions that top teams have

### Overlapping Snapshot Analysis
- Snapshot 21145966992 has score 70.572798 (0.087 better)
- BUT all 67 improved N values have OVERLAPS
- **0 clean improvements available** from any snapshot

## Response to Evaluator

The evaluator correctly identified that:
1. The C++ optimizer hasn't been applied - **DONE: No improvement (confirmed)**
2. The overlapping snapshot should be exploited - **DONE: All improvements have overlaps, cannot use**
3. Novel approaches haven't been tried - **AGREE: This is the path forward**

The evaluator's suggestion to run C++ optimizer and extract clean configs was sound, but our analysis shows:
- C++ optimizer: 0.000000 improvement
- Clean configs from overlapping snapshot: 0 available

## The Real Problem

We are at a **data ceiling**, not an algorithm ceiling:
- All 40+ snapshots converge to ~70.66
- All public sources we have access to score 70.66-72.5
- Top kernels achieve 68.5-69.5 by having access to 19+ sources (many private/shared)
- Without access to better source data, we cannot improve

## Recommended Approaches (Priority Order)

### 1. **HIGHEST PRIORITY: Novel Constructive Approaches**
Since optimization cannot improve pre-optimized solutions, we need to CONSTRUCT new solutions from scratch that might find different local optima.

**Translation-based Grid Construction** (from egortrushin kernel):
- Create grids of trees using translations in x and y directions
- Different grid sizes (nx × ny) for different N values
- This is a fundamentally different approach that might find new optima

```python
# For N = nx * ny trees:
# Place trees at positions (i*dx, j*dy) for i=0..nx-1, j=0..ny-1
# Try different dx, dy, and angle patterns
```

### 2. **HIGH PRIORITY: Backward Propagation (BackPacking)**
From crodoc kernel - use larger N configurations to improve smaller N:
- Start from N=200, iterate backward to N=1
- For each N, try removing each tree and keep best result
- This can propagate good patterns from large to small N

### 3. **MEDIUM PRIORITY: Constraint Programming**
Use CP-SAT or similar to find optimal small-N configurations:
- For N=2-10 (worst efficiency), try exhaustive search
- These contribute 6.1% of score but have worst efficiency (37-65%)

### 4. **EXPERIMENTAL: Reinforcement Learning**
Train an agent to place trees sequentially:
- State: current configuration
- Action: place next tree at (x, y, angle)
- Reward: -score (minimize bounding box)

## What NOT to Try
- More SA/GA variations on pre-optimized solutions (proven ineffective)
- Ensemble from existing snapshots (all converge to same score)
- Fixing overlapping configurations (overlaps too large)
- C++ optimizer on current best (already tested, 0 improvement)

## Validation Notes
- CV scheme: Direct scoring (sum of side²/N for N=1-200)
- Overlap checking: Required before submission
- Format: 's' prefix on all x, y, deg values

## SUBMISSION STRATEGY
- Remaining submissions: 96
- Submit after EVERY experiment that produces a valid submission
- We need LB feedback to calibrate our approaches
- Even if score doesn't improve, LB data is valuable

## Key Insight for Next Experiment

The only path forward is to **generate fundamentally different solutions** that might find different local optima. The translation-based grid construction from the egortrushin kernel is the most promising unexplored approach because:

1. It's constructive (builds from scratch, not optimizes existing)
2. It uses a different geometric principle (regular grids vs. irregular packing)
3. It has been shown to work for specific N values (72, 100, 110, 144, 156, 196, 200)

**Implement the translation-based grid construction for all N values, then ensemble with current best.**
