{
  "query": "What techniques do top Kaggle competitors use for 2D polygon packing optimization problems like Santa 2025?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle solutions for 2\u2011D polygon\u2011packing challenges such as **Santa\u202f2025** rely on a blend of classic combinatorial heuristics and modern learning\u2011augmented search.  A common backbone is a **two\u2011stage heuristic** that first breaks the large container into smaller sub\u2011problems (often by recursive partitioning) and then places polygons sequentially while eliminating overlaps\u202f([drops.dagstuhl.de](https://drops.dagstuhl.de/storage/00lipics/lipics-vol293-socg2024/LIPIcs.SoCG.2024.86/LIPIcs.SoCG.2024.86.pdf)).  Within each sub\u2011problem competitors apply **local\u2011search meta\u2011heuristics** such as tabu search, differential\u2011evolution or simulated\u2011annealing to improve a greedy initial layout\u202f([drops.dagstuhl.de](https://drops.dagstuhl.de/storage/00lipics/lipics-vol293-socg2024/LIPIcs.SoCG.2024.86/LIPIcs.SoCG.2024.86.pdf)).  \n\nThe **Shadoks team** demonstrated that a strong initial solution can be generated with **integer programming** (or a carefully tuned greedy order) and then refined by a fast local\u2011search loop, yielding some of the best scores in the CG:SHOP\u202f2024 challenge\u202f([drops.dagstuhl.de](https://drops.dagstuhl.de/storage/00lipics/lipics-vol293-socg2024/LIPIcs.SoCG.2024.84/LIPIcs.SoCG.2024.84.pdf)).  Earlier Kaggle winners on the \u201cSanta\u2019s Workshop Tour\u201d used a **mixed\u2011integer programming (MIP)** formulation to model the placement constraints directly, solving the relaxed model with commercial solvers and rounding to obtain feasible packings\u202f([towardsdatascience.com](https://towardsdatascience.com/helping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5)).  \n\nMore recent work explores **learning\u2011based policies**: deep neural networks are trained to predict near\u2011rectangular groups of irregular shapes and their relative orientations, turning the problem into a bin\u2011packing task that can be solved in linear time for hundreds of items\u202f([arxiv.org](https://arxiv.org/html/2309.10329)).  However, attempts to apply **reinforcement learning** directly have shown limited success for Santa\u202f2025, with many competitors abandoning RL in favor of the deterministic heuristics above\u202f([medium.com](https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97)).  \n\nIn practice, the winning pipelines combine: (1) an exact or integer\u2011programming seed, (2) aggressive partitioning and greedy placement, (3) iterative local\u2011search/meta\u2011heuristics (tabu, differential evolution), and (4) optional neural\u2011policy guidance for large\u2011scale instances.  This hybrid strategy consistently yields the smallest feasible square parcels required by the Santa\u202f2025 competition.",
      "url": ""
    },
    {
      "title": "A General Heuristic Approach for Maximum Polygon Packing",
      "text": "A General Heuristic Approach for Maximum\nPolygon Packing\nCanhui Luo #\nHuazhong University of Science and Technology, Wuhan, China\nZhouxing Su1 #\nHuazhong University of Science and Technology, Wuhan, China\nZhipeng L\u00fc #\nHuazhong University of Science and Technology, Wuhan, China\nAbstract\nThis work proposes a general heuristic packing approach to address the Maximum Polygon Packing\nProblem introduced by the CG:SHOP 2024 Challenge. Our solver primarily consists of two steps:\n(1) Partitioning the container and polygons to form a series of small-scale subproblems; (2) For each\nsubproblem, sequentially placing polygons into the container and attempting to eliminate overlaps.\n2012 ACM Subject Classification Theory of computation \u2192 Computational geometry; Computing\nmethodologies \u2192 Search methodologies\nKeywords and phrases packing, polygon, heuristic, differential evolution, local search, tabu search\nDigital Object Identifier 10.4230/LIPIcs.SoCG.2024.86\nCategory CG Challenge\nFunding This work was supported in part by the National Natural Science Foundation of China\n(NSFC) under Grant 72101094 and the Special Project for Knowledge Innovation of Hubei Province\nunder Grant 2022013301015175.\nAcknowledgements We want to thank the organizers of CG:SHOP 2024 and all other participants\nfor creating such an engaging challenge. We also want to thank Dominik Krupke for providing a\nhelpful official validator for solutions.\n1 Introduction\nThe recent CG:SHOP 2024 Challenge introduced a variant of irregular packing problems\nknown as the Maximum Polygon Packing (MPP) problem. The MPP problem involves a\nconvex polygonal container C and a polygon set P = {p1, p2, ..., pN }, where polygon piis\nassociated with a value vi. It seeks for a non-overlapping packing with the maximum total\nvalue. The challenge presents a total of 180 instances whose number of polygons ranges from\n28 to 50,000. The official document [4] gives a detailed description of the challenge.\nOur proposed algorithm employs a general process to solve these instances indiscriminately,\nand the overall framework is presented in Figure 1. We first partition a large-scale problem\ninto multiple small-scale subproblems (Section 2) and then solve each subproblem using\nupper-level polygon ordering (Section 3.1) and lower-level packing optimization techniques\n(Section 3.2). Section 4 presents our experimental results, followed by conclusions.\n1 Corresponding author: Zhouxing Su\n\u00a9 Canhui Luo, Zhouxing Su, and Zhipeng L\u00fc;\nlicensed under Creative Commons License CC-BY 4.0\n40th International Symposium on Computational Geometry (SoCG 2024).\nEditors: Wolfgang Mulzer and Jeff M. Phillips; Article No. 86; pp. 86:1\u201386:9\nLeibniz International Proceedings in Informatics\nSchloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\n86:2 A General Heuristic Approach for Maximum Polygon Packing\nInput polygon set P and container C\nUpper-level polygon ordering\nLower-level packing optimization\nFinished?\n \nPacking\nAssemble and return the complete solution\nMPP1 MPP2 MPPm\n( ) 0? Overlap Scurr== ( ) Update Sbest best\nReturn S\nPartitioning\nYes\nNo\nYes\nNo\nSelect next one\nFigure 1 The framework of our proposed algorithm.\n2 Partitioning\nIn this section, we present the decomposition of the original large-scale problem into a series\nof smaller MPP subproblems. It involves two components: partitioning the container C into\nmultiple regions and assigning polygons to each region.\n2.1 Container Partitioning\nThe container partitioning process consists of two steps, as shown in Figure 2. Initially, we\narrange two-dimensional square grids starting from the bottom-left corner of the bounding\nbox until the entire container is covered. The subregions formed by the intersection of the\ncontainer with all the grids constitute its partition C = C1 \u222a C2 \u222a ... \u222a Cm. Subsequently,\nwe merge the small subregions with adjacent grids, which are difficult to be used effectively.\nThe grid is dimensioned to keep the scale of each subproblem at approximately 300 polygons,\nmaking a trade-off between effectiveness and efficiency of lower-level packing optimization.\n2.2 Polygon Assignment\nWe adopt a simple approach of randomly assigning polygons to each subregion. Specifically, for\neach subregion Ci, we randomly select a polygon pj from P until\nP\nj\narea(pj )\narea(Ci) \u2265\nPN\ni=0\narea(pi)\narea(C)\n.\nThe advantage of random assignment lies in ensuring that the overall characteristics of each\nsubproblem align with the original problem.\nC. Luo, Z. Su, and Z. L\u00fc 86:3\nFigure 2 The partitioning process for the instance jigsaw_cf1_4fd4c46e. Step 1 (left): Cover the\ncontainer with squares; Step 2: Intersect and merge small regions (from the middle to the right).\nminimum translation\nminimum translation\nIFP\nContainer\nFigure 3 Examples of NFP between two polygons and IFP between container and polygon.\n3 Packing\n3.1 Upper-Level Polygon Ordering\nWe define a priority for each polygon. We repeatedly select one remaining polygon with the\nhighest priority (ties are broken by value) and try to insert it into the current solution. If the\ninsertion with lower-level packing optimization fails, we skip the current polygon and turn to\nthe next one. For the majority of instances, the priority is defined as the value-to-area ratio\nof a polygon (we also call it unit value). Polygons with higher unit values are prioritized\nfor putting in the container, which is called the Unit Value First (UVF) strategy. For\nsmall-scale instances (N < 100), we employ the \u03b1\u03b2-random strategy. It randomly selects\n\u03b1% and \u03b2% of the polygons and reassigns their UVF-based priority to the highest and the\nlowest, respectively. These instances are run for multiple times to ensure comprehensive\noptimization, with \u03b1 and \u03b2 set to 10 in our implementation.\n3.2 Lower-Level Packing Optimization\nThe position of a polygon can be represented by the coordinates l = (x, y) of a reference point,\nsuch as the bottom-left corner of the boundary. Then, the translation of a polygon can be\nrepresented by a vector pointing from its original position to its new position. Given a feasible\npacking S and a polygon p to be placed, it is impossible to find a non-overlapping position\nfor p without moving other polygons in most cases. This section introduces the algorithm for\neliminating overlaps for an invalid packing, which involves solving an unconstrained nonlinear\nproblem and heuristic polygon movement.\nS o C G 2 0 2 4\n86:4 A General Heuristic Approach for Maximum Polygon Packing\n3.2.1 Overlap Minimization\nTo determine the appropriate translation for the polygons, we utilized the no-fit polygon\n(NFP) and inner-fit polygon (IFP), which are fundamental in algorithmic approaches to\ngeometric design and optimization challenges. For a fixed polygon pi and a movable polygon\npj , NFP(pi, pj ) describes their non-overlapping positions with boundaries in contact precisely,\nwhich can be utilized to determine the minimum translation for pj to avoid overlap. Similarly,\nIFP(pi, pj ) is employed to determine the minimum translation to place pj inside pi. Figure 3\nillustrates the polygon translations determined using NFP (left) and IFP (right). The readers\nmay refer to Burke et al. [2] for a more detailed description.\nFor a packing S, based on NFP and IFP, we define the overlap between polygons pi\nand pj as fij (S), representing the minimum translation to separate them, and f0i(S) as the\nminimum translation for moving pi to fit into the container. Subsequently, we employ the\nseparation algorithm proposed by Imamichi et al. [7] to minimize the overlap, which involves\nsolving an unconstrained nonlinear programming problem as follows:\nmin\nS\nF(S) = X\n0\u2264i<j\u2264N\nf\n2\nij (S) (1)\nThe model relaxes the non-overlapping constraint but introduces repulsion forces between\nany two overlapped polygons. We use the classic L-BFGS (limited memory BFGS) method\nto solve this problem. It makes the packing S converge to a local optimum but strongly\ndepends on the initial layout. ...",
      "url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol293-socg2024/LIPIcs.SoCG.2024.86/LIPIcs.SoCG.2024.86.pdf"
    },
    {
      "title": "",
      "text": "Shadoks Approach to Knapsack Polygonal Packing\nGuilherme D. da Fonseca #\nLIS, Aix-Marseille Universit\u00e9, France\nYan Gerard #\nLIMOS, University Clermont Auvergne, Aubi\u00e8re, France\nAbstract\nWe describe the heuristics used by the Shadoks team in the CG:SHOP 2024 Challenge. Each instance\nconsists of a convex polygon called container and a multiset of items, where each item is a simple\npolygon and has an associated value. The goal is to pack some of the items inside the container\nusing translations, in order to maximize the sum of their values. Our strategy consists of obtaining\ngood initial solutions and improving them with local search. To obtain the initial solutions we used\ninteger programming and a carefully designed greedy approach.\n2012 ACM Subject Classification Theory of computation \u2192 Computational geometry\nKeywords and phrases Packing, polygons, heuristics, integer programming, computational geometry\nDigital Object Identifier 10.4230/LIPIcs.SoCG.2024.84\nCategory CG Challenge\nRelated Version Full Version: https://arxiv.org/abs/2403.20123\nSupplementary Material\nSoftware (Source Code): https://github.com/gfonsecabr/shadoks-CGSHOP2024\narchived at swh:1:dir:96fb9ad50c0c1307c7aef78560655cfcfabb24a6\nFunding Work supported by the French ANR PRC grant ADDS (ANR-19-CE48-0005).\nAcknowledgements We would like to thank the Challenge organizers and other competitors for their\ntime, feedback, and making this whole event possible. We would like to thank H\u00e9l\u00e8ne Toussaint,\nRapha\u00ebl Amato, Boris Lonjon, and William Guyot-L\u00e9nat from LIMOS, as well as the Qarma and\nTALEP teams and Manuel Bertrand from LIS, who continue to make the computational resources of\nthe LIMOS and LIS clusters available to our research. We would also like to thank Aldo Gonzalez\u0002Lorenzo and the undergraduate students Aymeric Beck, Houssam Boufarachan, Marine Izoulet, and\nCarla Scardigli for coding viewers for the solutions.\nFigure 1 Our best solutions to jigsaw_cf2_5db5d75a_34, random_rcf4_6e323d40_100,\natris1240, and satris1786 instances.\n\u00a9 Guilherme D. da Fonseca and Yan Gerard;\nlicensed under Creative Commons License CC-BY 4.0\n40th International Symposium on Computational Geometry (SoCG 2024).\nEditors: Wolfgang Mulzer and Jeff M. Phillips; Article No. 84; pp. 84:1\u201384:9\nLeibniz International Proceedings in Informatics\nSchloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\n84:2 Shadoks Approach to Knapsack Polygonal Packing\n1 Introduction\nCG:SHOP Challenge is an annual geometric optimization challenge. The sixth edition in\n2024 considers a 2-dimensional knapsack packing problem. The team Shadoks won first place\nwith the best solution (among the 14 participating teams) to 75 instances out of 180. In this\npaper, we describe the heuristics we used. We start by briefly describing the problem.\nAn input instance consists of a convex polygon called container and a multiset of items.\nEach item is a simple polygon with an associated integer value. The goal is to pack some of\nthe instance items inside the container using integer translations in order to maximize the\nsum of their values. In total, 180 instances have been given, ranging from 28 to 50,000 items.\nThe instances are of several different types according to the shape and values of the items.\nSome instances have mostly convex items, while other instances have many non-convex items\nsuch as polyominoes. In terms of item values, some items have only unit value, some have\nvalues proportional to the area, and other have random values, for example. Some solutions\nare presented in Figure 1 and more details about the challenge are available in the organizers\u2019\nsurvey paper [3].\nOur general strategy consists of finding a good initial solution (using integer programming\nor a greedy heuristic) and subsequently optimizing them with local search. Our strategy\nshares many common elements with the second place [5], but they did not use integer\nprogramming to obtain initial solutions and their optimization phase is more sophisticated\nthan ours. The third place [4] uses an hierarchical grid approach. The fourth place [1] used\na completely different integer programming model and a genetic algorithm.\nWe describe the algorithms in Section 2 and experimentally analyze their performance\nusing different parameters in Section 3. Our solvers were coded in Python and C++ and\nexecuted on several desktop laptop computers, as well as the LIMOS and LIS clusters.\n2 Algorithms\nWe used two different algorithms to compute initial solutions, a preprocessing phase that\ncan be executed beforehand, and a local search phase to improve the solutions.\n2.1 Integer Programming Approach\nA simple idea to solve the challenge problem is to produce a set V of random translations of\neach item inside the container and then reduce the problem to a kind of maximum weight\nindependent set problem in a graph G = (V, E). Each translated item is a vertex and there\nare two types of edges: (1) an edge between two translations that overlap and (2) translations\nof the same item i form a clique Ci. If all item have quantity one, then this is a traditional\nmaximum weight independent set problem. However, if items have non-unit quantities, then\neach clique Ciis associated with the quantity qi of item i and at most qi vertices of the\nclique are allowed in the solution.\nThis combinatorial problem can easily be modeled as integer programming with one\nbinary variable per vertex. A type-1 edge uv is modeled as u + v \u2264 1 and each clique Ci\nis modeled as P\nv\u2208Ci\nv \u2264 qi. The CPLEX solver [2] can optimally solve graphs with a few\nthousand vertices obtained from the challenge instances, which is not enough to obtain good\nsolutions using uniformly random placements.\nTo obtain better solutions, we start from a solutions S obtained with the aforementioned\nmethod and build a new graph G = (V, E) as follows. Let \u03c3 > 0 be a parameter and N be a\nset of the zero vector and random vectors where each random vector has x and y coordinates\nG. D. da Fonseca and Y. Gerard 84:3\nas Gaussian random variables of average 0 and standard deviation \u03c3. We create a translation\nin V for each item that is placed in S and for each translation vector in N if the translation\nis inside the container. We also create vertices in V using uniform random translations for\nall items. Edges and cliques are created as before, and the new combinatorial problem is\nsolved with CPLEX. We repeat this procedure multiple times using the previous solution S\nand reducing the value of \u03c3 at each step.\nThis method works well for instances with up to 200 items. To handle larger instances,\nwe partition the container using a square grid and partition the items equally among the\ncells. The partition is such that items are grouped by the slope of the longest edge, breaking\nties by the slope of the diameter. Each cell is then solved independently. The intuition to\ngroup items of similar slope together is that they can often be placed in a way that minimizes\nthe wasted space. Since the values of the items do not seem to be related to the slopes, this\napproach works well for the challenge instances.\n2.2 Greedy Heuristic\nThe greedy heuristic starts by creating an initial list L of n grid points inside the container\n(typically n = 1000). The list L is shuffled and we compute its centroid c rounded to integer\ncoordinates. The point c is inserted in the beginning of L. The input items are placed into a\nlist I ordered by decreasing utility, where the utility function is described next. Different\nutility functions may be used (more details in Section 3). The goal is that items of small\narea and high value have high utility while large items with small value have low utility.\nWe then build the packing by considering the items of the list I one by one in order. At\neach step, we have a current packing and a new item i to pack. We first try to pack i at\nthe first grid position g \u2208 L and at a number of random positions around g. If one of these\npositions is availa...",
      "url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol293-socg2024/LIPIcs.SoCG.2024.84/LIPIcs.SoCG.2024.84.pdf"
    },
    {
      "title": "Kaggle Santa 2025: Why \u201cReinforcement Learning\u201d is Failing",
      "text": "Kaggle Santa 2025: Why \u201cReinforcement Learning\u201d is Failing | by Himanshu Dhiman | Dec, 2025 | Medium\n[Sitemap](https://hmnshudhmn24.medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Kaggle Santa 2025: Why \u201cReinforcement Learning\u201d is Failing\n[\n![Himanshu Dhiman](https://miro.medium.com/v2/resize:fill:64:64/1*3jyWFYIYAox5D7cqT7gIRA.jpeg)\n](https://hmnshudhmn24.medium.com/?source=post_page---byline--087f3f04fa97---------------------------------------)\n[Himanshu Dhiman](https://hmnshudhmn24.medium.com/?source=post_page---byline--087f3f04fa97---------------------------------------)\n3 min read\n\u00b7Dec 21, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/087f3f04fa97&amp;operation=register&amp;redirect=https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97&amp;user=Himanshu+Dhiman&amp;userId=288bb1baa8d2&amp;source=---header_actions--087f3f04fa97---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/087f3f04fa97&amp;operation=register&amp;redirect=https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97&amp;source=---header_actions--087f3f04fa97---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nEvery December, Kaggle drops a \u201cSanta\u201d competition that ruins the holidays for thousands of data scientists. We expect a fun puzzle. We get a nightmare of Combinatorial Optimization.\nThis year\u2019s edition,**\u201cSanta 2025: The Christmas Tree Packing Challenge,\u201d**is no exception. The goal? Pack $N$ irregular \u201cTree Polygons\u201d into the smallest possible square bounding box for $N=1$ to $200$.\nIf you checked the leaderboard today (Dec 21), you noticed a trend:**Reinforcement Learning (RL) is getting crushed.**\nDespite the massive hype around \u201cReasoning Models\u201d and PPO agents this year, the top 100 teams aren\u2019t using neural networks. They are using 1980s physics simulations. Here is why the \u201cOld School\u201d is winning, and how the pros are using LLMs to cheat the system.\n## The Problem: Why RL Hates \u201cPacking\u201d\nOn paper, this looks like a game.*Place a tree, check for overlap, repeat.*Perfect for a DQN agent, right?\nWrong. The geometry kills the gradient.\n1. **Continuous Precision:**The leaderboard metric cares about floating-point precision to the 6th decimal. RL agents (which output discrete actions or probability distributions) struggle to make the \u201cmicro-adjustments\u201d needed to squeeze a tree into a 0.001mm gap.\n2. **The \u201cCollision\u201d Cliff:**In packing, a move is either valid (reward = 1) or it overlaps by a pixel (reward = -100). This sparse, binary feedback loop makes it nearly impossible for an agent to \u201clearn\u201d a gradient. It doesn\u2019t know*how*to fix the overlap, just that it failed.## The Solution: Simulated Annealing (on Steroids)\nThe \u201cSecret Sauce\u201d currently dominating the leaderboard is**Simulated Annealing (SA)**combined with**Lattice Generation**.\nInstead of training a model to*predict*the position, teams are writing physics engines that*shake*the box.\n* **High Temperature:**Randomly spin and throw trees into the box (Explorer Phase).\n* **Low Temperature:**Jiggle the trees by microscopic amounts to resolve collisions (Exploiter Phase).\nThe best solutions right now use a \u201cHybrid\u201d strategy:\n* **N &lt; 58:**Use Simulated Annealing to find unstructured, chaotic packings.\n* **N &gt; 58:**Switch to \u201cCrystalline Packing\u201d (regular geometric lattices) which is mathematically superior for large numbers.## The Twist: How LLMs are Actually Helping\nIf RL is dead, where is the AI?\nTop competitors aren\u2019t using LLMs to*solve*the puzzle; they are using LLMs to**write the solver**.\nPython is too slow for 10 billion annealing steps. The meta this year is to prompt Gemini 3 or Claude to:\n\u201cRewrite this Python overlap-check function in Rust/C++ with AVX2 vectorization.\u201d\nWe are seeing \u201cHybrid\u201d workflows where the**Human**defines the heuristics, the**LLM**writes the highly optimized C++ kernels, and**Simulated Annealing**does the heavy lifting.\n## The Starter Code: A Simple Annealer\nIf you want to jump in before the January deadline, don\u2019t start with PyTorch. Start here.\nHere is a conceptual snippet for a \u201cPerturbation\u201d mover \u2014the heart of any good annealing solution.\n```\nimport numpy as np\nimport math\ndef anneal(layout, max\\_temps=10000):\n# layout: Array of [x, y, rotation] for N trees\ncurrent\\_score = calculate\\_area(layout)\ntemperature = 1.0\ncooling\\_rate = 0.9995\nfor i in range(max\\_temps):\n# 1. Propose a Move (The &quot;Shake&quot;)\n# We only move ONE tree slightly to save compute\nidx = np.random.randint(0, len(layout))\noriginal\\_pos = layout[idx].copy()\n# Perturb: Move x/y by small amount, rotate slightly\nlayout[idx][0] += np.random.normal(0, temperature)\nlayout[idx][1] += np.random.normal(0, temperature)\nlayout[idx][2] += np.random.normal(0, temperature \\* 10) # Rotate\n# 2. Check Constraints (The &quot;Wall&quot;)\nif check\\_overlap(layout):\n# Revert immediately if invalid\nlayout[idx] = original\\_pos\ncontinue\n# 3. Acceptance Criteria (Metropolis-Hastings)\nnew\\_score = calculate\\_area(layout)\ndelta = new\\_score - current\\_score\n# If better, accept. If worse, accept with prob based on Temp.\nif delta &lt; 0 or math.exp(-delta / temperature) &gt; np.random.rand():\ncurrent\\_score = new\\_score\nelse:\nlayout[idx] = original\\_pos # Revert\n# Cool down\ntemperature \\*= cooling\\_rate\nreturn layout, current\\_score\n# Pro Tip: Ask an LLM to convert &#x27;&#x27;check\\_overlap&#x27;&#x27; to Cython for 100x speed.\n```\nThe competition ends**January 30, 2026**. You have 40 days. Put down the Neural Network and pick up a physics textbook.\n*(****Discussion:****Are you using a Lattice approach or pure Annealing for N=200? Let\u2019s discuss strategy in the comments!)*\n[\n![Himanshu Dhiman](https://miro.medium.com/v2/resize:fill:96:96/1*3jyWFYIYAox5D7cqT7gIRA.jpeg)\n](https://hmnshudhmn24.medium.com/?source=post_page---post_author_info--087f3f04fa97---------------------------------------)\n[\n![Himanshu Dhiman](https://miro.medium.com/v2/resize:fill:128:128/1*3jyWFYIYAox5D7cqT7gIRA.jpeg)\n](https://hmnshudhmn24.medium.com/?source=post_page---post_author_info--087f3f04fa97---------------------------------------)\n[## Written byHimanshu Dhiman\n](https://hmnshudhmn24.medium.com/?source=post_page---post_author_info--087f3f04fa97---------------------------------------)\n[20 followers](https://hmnshudhmn24.medium.com/followers?source=post_page---post_author_info--087f3f04fa97---------------------------------------)\n\u00b7[0 following](https://hmnshudhmn24.medium.com/following?source=post_page---post_author_info--087f3f04fa97---------------------------------------)\nJust another human t...",
      "url": "https://hmnshudhmn24.medium.com/kaggle-santa-2025-why-reinforcement-learning-is-failing-087f3f04fa97"
    },
    {
      "title": "Learning based 2D Irregular Shape Packing",
      "text": "# Learning based 2D Irregular Shape Packing\n\nZeshi Yang\n[0000-0001-9680-8132](https://orcid.org/0000-0001-9680-8132)[zs243@mail.ustc.edu.cn](mailto:zs243@mail.ustc.edu.cn)LightSpeed StudiosSeattleWAUSA,\u00a0Zherong Pan\n[0000-0001-9348-526X](https://orcid.org/0000-0001-9348-526X)[zrpan@global.tencent.com](mailto:zrpan@global.tencent.com)LightSpeed StudiosSeattleWAUSA,\u00a0Manyi Li\n[0000-0002-5251-0462](https://orcid.org/0000-0002-5251-0462)[manyili@sdu.edu.cn](mailto:manyili@sdu.edu.cn)Shandong UniversityJinanShandongChina,\u00a0Kui Wu\n[0000-0003-3326-7943](https://orcid.org/0000-0003-3326-7943)[kwwu@global.tencent.com](mailto:kwwu@global.tencent.com)LightSpeed StudiosLos AngelesCAUSA\u00a0and\u00a0Xifeng Gao\n[0000-0003-0829-7075](https://orcid.org/0000-0003-0829-7075)[xifgao@global.tencent.com](mailto:xifgao@global.tencent.com)LightSpeed StudiosSeattleWAUSA\n\n###### Abstract.\n\n2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patches, we train deep neural policies to predict nearly rectangular patch subsets and determine their relative poses, leading to linear time scaling with the number of patches. We demonstrate the effectiveness of our method on three datasets for UV packing, where our method achieves a higher packing ratio over several widely used baselines with competitive computational speed.\n\ngeometry processing, deep reinforcement learning, combinatorial optimization\n\n\u2020\u2020price: 15.00\u2020\u2020copyright: acmlicensed\u2020\u2020journal: TOG\u2020\u2020journalyear: 2023\u2020\u2020journalvolume: 42\u2020\u2020journalnumber: 6\u2020\u2020publicationmonth: 12\u2020\u2020price: 15.00\u2020\u2020doi: 10.1145/3618348\u2020\u2020ccs: Computing methodologies\u00a0Computer graphics\u2020\u2020ccs: Computing methodologies\u00a0Machine learning\n\n|     |     |     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/part_model.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/part_xatlas_0.698.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/part_ours_0.774.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/chinsehouse_model.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/chinsehouse_xatlas_0.662.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/chinsehouse_ours_0.764.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/tigerhead_model.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/tigerhead_xatlas_0.610.jpg) | ![Refer to caption](https://arxiv.org/html/extracted/5118135/fig/fig1/tigerhead_ours_0.729.jpg) |\n|  | XAtlas pr: 69.8% | Ours pr: 77.4% |  | XAtlas pr: 66.2% | Ours pr: 76.4% |  | XAtlas pr: 61.0% | Ours pr: 72.9% |\n\nFigure 1. Three UV-packing results generated using XAtlas and our learning-assisted method.\n\n## 1\\. Introduction\n\n2D irregular shape packing lies the theoretical foundation for a wide spectrum of applications across various industrial areas, e.g. material parts assembly\u00a0\\[Ke\net\u00a0al., [2020](https://arxiv.org/html/2309.10329#bib.bib22)\\], VLSI module placements\u00a0\\[Cheng\net\u00a0al., [2005](https://arxiv.org/html/2309.10329#bib.bib8)\\], and 2D shelf arrangements in automatic warehouses\u00a0\\[\u00d6n\u00fct et\u00a0al., [2008](https://arxiv.org/html/2309.10329#bib.bib28)\\]. The most prominent packing application in computer graphics is UV chart packing, where a set of 2D shapes corresponding to patches of 3D models are packed together into a single texture atlas for mapping various appearance details onto the 3D surface for downstream rendering. In the realm of digital games where many 3D models are rendered in the virtual world, improving the UV packing ratio could significantly save the graphical memory, improve the loading speed, and reduce rendering overhead. This problem was recognized decades ago, e.g., by\u00a0Soucy\net\u00a0al. \\[ [1996](https://arxiv.org/html/2309.10329#bib.bib39)\\] and\u00a0Sander\net\u00a0al. \\[ [2001](https://arxiv.org/html/2309.10329#bib.bib33)\\], which has recently revived as an active area of graphic research\u00a0\\[Limper\net\u00a0al., [2018](https://arxiv.org/html/2309.10329#bib.bib24); Liu\net\u00a0al., [2019](https://arxiv.org/html/2309.10329#bib.bib25); Zhang\net\u00a0al., [2020](https://arxiv.org/html/2309.10329#bib.bib47)\\].\n\nAn packing algorithm needs to search for both the position and orientation of each patch. Due to its combinatorial nature, finding optimal 2D shape packing has been well-understood as being NP-complete and APX-hard. Prior research efforts are focused on heuristic or genetic algorithms to determine the order of objects to be packed, and then search for the pose of each shape, e.g., using no-fit polygon (NFP) and the \u201cTetris\u201d algorithm. However, the sub-optimality gap of these heuristics can be substantial. On the other hand, when the problem size is small, globally optimal stochastic optimization algorithms have been proposed to search for the near-optimal packing orders. Although these techniques can scale to tens of 2D shapes, UV unwrapping approaches and commercial 3D modeling software can oftentimes generate hundreds of patches for one 3D model, which is far beyond the capabilities of the global search algorithms.\n\nWe propose a learning-assisted 2D packing pipeline for general irregular-shaped patches. The design of our approach is based on two observations: 1) Prior works generate sub-optimal solutions by assuming objects are packed sequentially. 2) Prior works try all possible poses and orientations for each to-be-packed object to find the best solution, leading to slow computation. To tackle these issues, we propose to hierarchically group patches into nearly rectangular super-patches, allowing a larger search space for patch combination and a smaller optimality gap. Instead of exhaustively trying all possible combinations, we train a high-level group selector network (HSN) to efficiently predict how likely a candidate patch subset can be grouped into a rectangular super-patch. Given an identified patch subset, we use the sequential ordering technique, similar to prior works. Specifically, we use a low-level sorter network (LSN) to determine the suitable order of packing within the subset. Next, a low-level pose network (LPN) infers the rough initial pose of each patch. The ultimate pose of these patches is determined by local numerical optimization. These two networks are trained using reinforcement learning to efficiently infer near optimal sequential packing policies.\n\nWe have evaluated our method on 3 datasets, containing highly irregular UV patches generated from both organic and man-made 3D models. We show the effectiveness of our approach by comparing against widely used algorithm baselines, including piecewise linear NFP, XAtlas\u00a0\\[Young, [2023](https://arxiv.org/html/2309.10329#bib.bib46)\\], and\u00a0\\[Sander et\u00a0al., [2003](https://arxiv.org/html/2309.10329#bib.bib34)\\], where we achieve a 5%\u221210%percent5percent105\\\\%-10\\\\%5 % - 10 % packing ratio improvement across all the tested datasets. We further highlight the generality of our learned pol...",
      "url": "https://arxiv.org/html/2309.10329"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-: In this re-defined optimization problem, help Santa fit Christmas tree toys into the smallest (2-dimension) parcel size possible so that he can efficiently mail these stocking stuffers around the globe. Santa needs the dimensions of the smallest possible square box that fits shipments of between 1-200 trees.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[adityapawar327](https://github.com/adityapawar327)/**[Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)**Public\n* [Notifications](https://github.com/login?return_to=/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)You must be signed in to change notification settings\n* [Fork1](https://github.com/login?return_to=/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\n* [Star2](https://github.com/login?return_to=/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\nIn this re-defined optimization problem, help Santa fit Christmas tree toys into the smallest (2-dimension) parcel size possible so that he can efficiently mail these stocking stuffers around the globe. Santa needs the dimensions of the smallest possible square box that fits shipments of between 1-200 trees.\n[www.kaggle.com/code/adityapawar327/santa-2025-christmas-tree-packing-challenge-v1](https://www.kaggle.com/code/adityapawar327/santa-2025-christmas-tree-packing-challenge-v1)\n[2stars](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/stargazers)[1fork](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/forks)[Branches](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/branches)[Tags](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/tags)[Activity](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/activity)\n[Star](https://github.com/login?return_to=/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)\n[Notifications](https://github.com/login?return_to=/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-)You must be signed in to change notification settings\n# adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-\nmain\n[Branches](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/branches)[Tags](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/tags)\n[](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/branches)[](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[2 Commits](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/commits/main/)\n[](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/commits/main/)\n|\n[README.md](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/blob/main/README.md)\n|\n[README.md](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/blob/main/README.md)\n|\n|\n|\n[santa-2025-christmas-tree-packing-challenge-v1.ipynb](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/blob/main/santa-2025-christmas-tree-packing-challenge-v1.ipynb)\n|\n[santa-2025-christmas-tree-packing-challenge-v1.ipynb](https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-/blob/main/santa-2025-christmas-tree-packing-challenge-v1.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Santa 2025 - Christmas Tree Packing Challenge V1\n[](#santa-2025---christmas-tree-packing-challenge-v1)\nThis repository contains my approach for the[Santa 2025 - Christmas Tree Packing Challenge](https://www.kaggle.com/competitions/santa-2025)on Kaggle.\n## Overview\n[](#overview)\nThe objective of this challenge is to optimally pack rotatable Christmas trees (polygonal shapes) into the smallest possible square, minimizing the bounding box area for each value of N (number of trees from 1 to 200). The final solution is evaluated by an ensemble score that combines all cases.\n**Notebook Link**:[Santa 2025 - Christmas Tree Packing Challenge V1](https://www.kaggle.com/code/adityapawar327/santa-2025-christmas-tree-packing-challenge-v1)\n**Public Score**: 85.92\n## Table of Contents\n[](#table-of-contents)\n* Library Imports and Environment Setup\n* Global Configuration and Precision Settings\n* ChristmasTree Class Definition\n* Utility Functions (Scoring/Collision)\n* Simulated Annealing Algorithm (N &lt; 20)\n* Grid Search Algorithm (N &gt;= 20)\n* Hybrid &amp; Ensemble Solvers\n* Main Computation Loop (N = 1 to 200)\n* Submission Formatting and Export\n## Methodology\n[](#methodology)\n* **Small N (&lt; 20):**Uses Simulated Annealing for efficient search in small configuration spaces.\n* **Large N (&gt;= 20):**Uses a tailored Grid Search for tractable solution space exploration.\n* **Hybrid/Ensemble:**Runs multiple seeds and chooses the best solution for each N.\n* **Collision Detection:**Employs Shapely geometry operations to prevent tree overlap.\n* **Performance:**Code leverages parallelization (`ProcessPoolExecutor`) and high-precision (Decimal) arithmetic.\n## Usage\n[](#usage)\n1. Install dependencies:\n* `shapely`,`numpy`,`pandas`,`matplotlib`,`tqdm`\n* Run the notebook or script sequentially to output results and save a`submission.csv`.\n* Check the score on important N cases (e.g., 1, 10, 25, 50, 100, 150, 200).\n## Output\n[](#output)\n* The notebook prints key logs during computation.\n* A`submission.csv`is generated with columns:`id`,`x`,`y`,`deg`(example row:`001\\_0, s-9.146226, s-0.12832, s-224.999647`).\n## License\n[](#license)\nThis project is released under the[Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0).\n## Author\n[](#author)\nAditya Pawar ([Kaggle Profile](https://www.kaggle.com/adityapawar327))\nFor questions or collaboration, feel free to reach out via Kaggle discussions or connect on[LinkedIn](https://www.linkedin.com/in/adityapawar327).\n## About\nIn this re-defined optimization problem, help Santa fit Christmas tree toys into the smallest (2-dimension) parcel size possible so that ...",
      "url": "https://github.com/adityapawar327/Christmas-Tree-Packing-Kaggle-Challenge-Santa-25-"
    },
    {
      "title": "Helping Santa plan with Mixed Integer Programming (MIP)",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1951386a6ba5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhelping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhelping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhelping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhelping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Helping Santa plan with Mixed Integer Programming (MIP)\n\n## A write-up on how we obtained a silver medal in the \u201cSanta\u2019s Workshop Tour 2019\u201d competition organized by Kaggle\n\n[![Gilles Vandewiele](https://miro.medium.com/v2/resize:fill:88:88/1*9uJwS_E_nZKfDjQjRMGndA.jpeg)](https://gillesvandewiele.medium.com/?source=post_page-----1951386a6ba5--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----1951386a6ba5--------------------------------)\n\n[Gilles Vandewiele](https://gillesvandewiele.medium.com/?source=post_page-----1951386a6ba5--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3e8cbc53806e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhelping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5&user=Gilles+Vandewiele&userId=3e8cbc53806e&source=post_page-3e8cbc53806e----1951386a6ba5---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1951386a6ba5--------------------------------)\n\n\u00b7\n\n10 min read\n\n\u00b7\n\nJan 15, 2020\n\n--\n\n2\n\nListen\n\nShare\n\nThis year, [Bram Steenwinckel](https://bsteenwi.github.io/) and I ensured our Christmas gifts by being the 28th team (of more than 1600 participants) to deliver Santa an optimal schedule for his annual 100-day workshop. This blog post contains a problem description and the different steps we took to achieve this optimal solution.\n\n**DISCLAIMER:** Bram and I are, by no means, experts in optimization. Nevertheless, we had a lot of fun and learned a lot.\n\n# Problem definition\n\nEach year, Santa organizes a Christmas workshop spanning 100 days, allowing 5000 families to attend it. Each of the families (consisting of a certain number of family members) can provide a list of their top 10 preferences of days that they would like to attend. As the workshop venue has only a limited capacity (there must be between 125 and 300 attendants each day), it is impossible to grant everyone their first choice. Therefore, compensations are provided to families that do not get their first pick. A family of size N, not getting their first pick would get:\n\n- $50 gift card if they get their second choice\n- $50 gift card and 25% discount (valued at $9) on Santa\u2019s buffet per family member if they get their third choice\n- $100 gift card and 25% discount (valued at $9) on Santa\u2019s buffet per family member for the fourth choice\n- $200 gift card and 25% discount (valued at $9) on Santa\u2019s buffet per family member if they get their fifth choice\n- $200 gift card and 50% discount (valued at $18) on Santa\u2019s buffet per family member corresponding to the sixth choice\n- $300 gift card and free Santa\u2019s buffet (valued at $36) per family member for the seventh choice\n- $400 gift card and free Santa\u2019s buffet (valued at $36) per family member for the eighth choice\n- $500 gift card, free Santa\u2019s buffet (valued at $36) per family member, and 50% off a North Pole Helicopter Ride (value of $199) per family member if they receive their ninth choice\n- $500 gift card, free Santa\u2019s buffet (valued at $36) per family member, and a free North Pole Helicopter Ride (value of $398) per family member otherwise\n\nOh boy, I wouldn\u2019t mind not getting a pick from my top 10 list. I can already imagine myself sitting next to Santa for a helicopter ride over the North Pole. Also, $500 and a free dinner is not too shabby.\n\nUncolored image of Stevens family getting assigned its 10th choice.\n\nBut that\u2019s not all\u2026 There\u2019s also an accounting penalty if we do not uniformly distribute the number of attendants over the 100 days:\n\nThe accounting penalty for not uniformly distributing the number of attendants. N\\_d is the number of attendants for day d and N\\_(d+1) the number of attendants on day d + 1\n\n**The ultimate goal is to create an assignment of these 5000 families to one of the 100 days that minimizes the sum of the preference and accounting costs.**\n\n# Inspecting the data & costs\n\nLet\u2019s take a closer look at the provided data of the 5000 families. First, let\u2019s check the family sizes:\n\nThe sizes of the 5000 families follow a somewhat normal distribution\n\nSo it seems that the family sizes are somewhat normally distributed with a size of 4 being the mode. Now let\u2019s look at the days provided as first choice:\n\nThe number of families that provided a certain day as first choice.\n\nClearly, day 1 is a very popular one. Actually, the days count backwards, day d means d days before Christmas. As such, day 1 is the day right before Christmas, which explains why it is so popular. Moreover, we see periodical peaks corresponding to weekends. Let\u2019s create a heatmap of the (logarithm of the) preference costs in function of the family size and assigned choice:\n\nA heatmap of the preference costs in function of the family\u2019s assigned choice and its size\n\nFinally, let\u2019s take a look at a heatmap for the log of the accounting costs:\n\nThe accounting cost is always zero when the number of attendants is equal to 125 for that day and is maximal when the number of attendants on the current day is 300 and 125 the next day.\n\n# First approaches: greedy, Hungarian method, genetic algorithms and simulated annealing\n\n## Greedy algorithm\n\nOur first attempt was a very naive one. As the number of attendants each day had to be at least 125, we kept a list of days that did not yet have 125 attendants. We then iterated over each of the families and assigned it to one of its choices that is still in the list. If none of the family\u2019s choices is within that list, we just assigned it to the best choice, ensuring the attendants on that day would not exceed 300.\n\n## Hungarian method\n\nShortly after the launch of the competition, a [blog post](https://opensourc.es/blog/kaggle-santa-2019) was published by opensourc.es which achieved a much better score than what we currently were achieving. His strategy was to ignore the accounting cost, and minimize the preference cost by using the Hungarian method. The vanilla implementation tries to uniformly assign the families (50 per day). We quickly extended that approach by assigning more families to more popular days. We actually tried to solve the following optimization problem for a while: find a 100-dimensional vector **V** of which the elements sum to 5000 (number of famil...",
      "url": "https://towardsdatascience.com/helping-santa-plan-with-mixed-integer-programming-mip-1951386a6ba5?gi=e3cdde5cb857"
    },
    {
      "title": "",
      "text": "A machine learning approach\nthat beats Rubik\u2019s cubes\nAlexander Chervov\u2217\nInstitut Curie, Universite PSL\nParis, F-75005, France\nKirill Khoruzhii\u2217\nTechnical University of Munich\nGarching, 85748, Germany\nNikita Bukhal\nNovosibirsk State University\nNovosibirsk, 630090, Russia\nJalal Naghiyev\nTechnical University of Munich\nGarching, 85748, Germany\nVladislav Zamkovoy\nIndependent Researcher\nMoscow, 119454, Russia\nIvan Koltsov\nRTU MIREA\nMoscow, 119454, Russia\nLyudmila Cheldieva\nRTU MIREA\nMoscow, 119454, Russia\nArsenii Sychev\nRTU MIREA\nMoscow, 119454, Russia\nArsenii Lenin\nRTU MIREA\nMoscow, 119454, Russia\nMark Obozov\nInnopolis University\nInnopolis, 420500, Russia\nEgor Urvanov\nRTU MIREA\nMoscow, 119454, Russia\nAlexey M. Romanov\u2020\nRTU MIREA\nMoscow, 119454, Russia\nromanov@mirea.ru\nAbstract\nThe paper proposes a novel machine learning-based approach to the pathfinding\nproblem on extremely large graphs. This method leverages diffusion distance\nestimation via a neural network and uses beam search for pathfinding. We demon\u0002strate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik\u2019s cubes with\nunprecedentedly short solution lengths, outperforming all available solvers and\nintroducing the first machine learning solver beyond the 3x3x3 case. In particular,\nit surpasses every single case of the combined best results in the Kaggle Santa\n2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik\u2019s cube, our\napproach achieves an optimality rate exceeding 98%, matching the performance\nof task-specific solvers and significantly outperforming prior solutions such as\nDeepCubeA (60.3%) and EfficientCube (69.6%). Our solution in its current im\u0002plementation is approximately 25.6 times faster in solving 3x3x3 Rubik\u2019s cubes\nwhile requiring up to 8.5 times less model training time than the most efficient\nstate-of-the-art competitor. Finally, it is demonstrated that even a single agent\ntrained using a relatively small number of examples can robustly solve a broad\nrange of puzzles represented by Cayley graphs of size up to 10145, confirming the\ngenerality of the proposed method.\n\u2217Alexander Chervov and Kirill Khoruzhii contributed equally to this work.\n\u2020Corresponding author\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n1 Introduction\nRubik\u2019s cube is one of the most famous puzzles, which is believed to be played by more than a billion\npeople in the world [1]. According to [2], it was included in the 100 most influential inventions of\nthe 20th century. Even decades after its first introduction, it is still used as a benchmark and model\ntask in various fields: artificial intelligence [3], robotics [4], graphs algorithms [5], [6], cryptography\n[7], image encryption [8], statistical physics [9],[10], group theory [11],[12], for human cognitive\nabilities [13].\nFrom a broader perspective, solving the Rubik\u2019s Cube is a specific instance of a planning problem.\nOne must plan actions to transition between the initial and solved states. The mathematical framework\nfor such problems is pathfinding on graphs (state transition graphs): all possible states are represented\nas nodes, and edges correspond to transitions between states based on actions (moves). The planning\ntask thus reduces to finding a path from a given initial node to one or more desired nodes. A specific\nclass of graphs represents the Rubik\u2019s Cube and similar puzzles\u2014Cayley-type graphs of the puzzle\u2019s\nsymmetry group. These are highly symmetric state transition graphs where the symmetry group\ncan transform any node into another. Cayley graphs are of fundamental importance in modern\nmathematics [14], [15] and have numerous applications: in bioinformatics for estimating evolutionary\ndistances [16, 17, 18, 19]; in processor interconnection networks [20, 21, 22]; in coding theory for the\nconstruction of expander graphs and related codes [23]; in cryptography for constructing specific hash\nfunctions [24, 7]; in machine learning (ML) [18]; and in quantum computing [25, 26, 27, 28, 29].\nFinding the shortest paths on generic finite Cayley graphs is an NP-hard problem [30], as it is for many\nparticular groups: the Rubik\u2019s Cube group [31] and some others [32, 18]. Brute force breadth-first\nsearch, Dijkstra\u2019s, and related methods can find the shortest paths on graphs with billions of nodes,\nthe bidirectional trick squares feasible sizes, but these methods require extremely large computational\nresources and are not practical for much larger sizes, which are of our interest. Moreover, no effective\ntools are currently available to find any (not just the shortest) paths on Cayley graphs of large finite\ngroups. For example, modern computer algebra systems like GAP [33] fail on any sufficiently large\ngroup, such as the 4x4x4 Rubik\u2019s Cube.\nThis research aims to overcome the abovementioned limits in solving large Rubik\u2019s cubes and similar\nlarge-scale pathfinding problems with a high level of optimality using machine learning. The main\ncontributions to the state of the art are the following:\n1. We propose a novel multi-agent, machine learning-based approach to find paths on Cayley\ngraphs of finite groups. It is the first machine learning approach capable of handling groups\nas large as 1074. It achieves over 98% optimality on the DeepCubeA dataset of 3x3x3\ncubes, reaching the level of task-oriented solvers based on pattern databases. It produces\nbetter results (shorter solution paths) than any known competitor for 4x4x4 and 5x5x5\nRubik\u2019s Cubes, including the aggregated best results from the 2023 Kaggle Santa Challenge,\nrepresenting the current state of the art.\n2. We demonstrate that increasing the size of the set used to train multilayer perceptrons with\nresidual blocks has a limited impact on the pathfinder\u2019s performance. At the same time,\nincreasing the beam width and number of agents robustly improves the average solution\nlength and optimality. This surprising finding helped choose the size of the train data for\neach agent and achieve best-in-class performance without wasting computational resources\non additional training.\n3. The training time and computational resources required for our approach are significantly\nsmaller than those for state-of-the-art approaches. Our solution, tested on the same hardware\nand beam width providing solution length similar to the EfficientCube (the previous leading\nML solution) solving the task approximately 25.6 faster and requiring up to 8.5 times less\nmodel training time than the competitor.\n4. We demonstrate that even a single agent trained using a relatively small number of examples\ncan robustly solve a broad range of puzzles represented by Cayley graphs with sizes up to\n10145, confirming the generality of the proposed method.\nIn recent years, machine learning has been emerging as \"a tool in theoretical science\" [34], leading\nto several noteworthy applications to mathematical problems [35, 36, 37, 38, 39, 40, 41, 42]. This\nresearch is part of the larger project, which aims to create an open-source machine learning Python\n2\nframework for analyzing Cayley graphs and contribute to the fascinating, emerging area of machine\nlearning applications in theoretical sciences.\n2 Proposed Machine Learning Approach\nThis paper presents a unified approach for finding paths on a large class of graphs, focusing on\ndemonstrating its efficiency for Rubik\u2019s cube graphs. It does not rely on any prior knowledge or\nhuman expertise about the graphs. The approach has two main components: a neural network model\nand a graph search algorithm \u2014 similar to previous works such as AlphaGo/AlphaZero [43],[44],\nDeepCube [45],[3], and EfficientCube [46], among others. The model is trained to guide what moves\nshould be done to get closer to the destination node (\"solved state\" for puzzles). The graph search\nalgorithm starts from a given node and moves to nodes closer to the destination, based on the neural\nnetwork\u2019s predictions, until the destination node is found.\nThe basic assumption on a graph is that there ...",
      "url": "https://openreview.net/attachment?id=31CaYYw1Xz&name=pdf"
    },
    {
      "title": "Kaggle: Santa's Stolen Sleigh",
      "text": "Kaggle: Santa&#39;s Stolen Sleigh\n# Kaggle: Santa&#39;s Stolen Sleigh\n[R](https://datawookie.dev/tags/r/)**2016-01-22**2025-12-02/9 min read[**](<http://twitter.com/share?url=https://datawookie.dev/blog/2016/01/kaggle-santas-stolen-sleigh/&text=Kaggle: Santa's Stolen Sleigh&via=datawookie>)[**](<mailto:?subject=Check out Kaggle: Santa's Stolen Sleigh.&body=Kaggle: Santa's Stolen Sleigh, by datawookie\nhttps://datawookie.dev/blog/2016/01/kaggle-santas-stolen-sleigh/\n>)\nThis morning I read Wendy Kan&rsquo;s interesting post on*Creating Santa&rsquo;s Stolen Sleigh*. I hadn&rsquo;t really thought too much about the process of constructing an optimisation competition, but Wendy gave some interesting insights on the considerations involved in designing a competition which was both fun and challenging but still computationally feasible without military grade hardware.\nThis seems like an opportune time to jot down some of my personal notes and also take a look at the results. I know that this sort of discussion is normally the prerogative of the winners and I trust that my ramblings won&rsquo;t be viewed as presumptuous.\n[Santa&rsquo;s Stolen Sleigh](https://www.kaggle.com/c/santas-stolen-sleigh)closed on 8 January 2016. At the top of the leaderboard:\n![Scores of best performers in the 'Santa\u2019s Stolen Sleigh' Kaggle competition.](kaggle-santas-sleigh-winners.png)\nWell done, what a phenomenal effort!\n## Problem Summary\nThe problem could be summarised as follows: 100 000 Christmas gifts have to be delivered from the North Pole to various locations across the Earth while minimising the[Weighted Reindeer Weariness](https://www.kaggle.com/c/santas-stolen-sleigh/overview/evaluation)a metric depending on weight carried and distance travelled. Distances were to be calculated using the[Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)which takes into account the curvature of the Earth&rsquo;s quasi-spherical surface. Each gift had a weight between 1 and 50 (units unspecified). In addition the sleigh itself had a weight of 10 (same unspecified and obviously rather magical units) and a weight capacity of 1000.\n> ... when the sleigh weight was too small, there's no incentive to merge any trips.\n> > Wendy Kan\n> The problem could be decomposed into two parts:\n1. partition the gifts into trips, where the total weight of gifts to be delivered on any particular trip must satisfy the weight constraint of the sleigh; and\n2. choose the order in which the gifts should be delivered on each trip.\nA solution would jointly solve both parts in such a way as to minimise Weighted Reindeer Weariness. It&rsquo;s a variation of the[Travelling Salesman Problem](https://simple.wikipedia.org/wiki/Travelling_salesman_problem). In the course of doing some preliminary research I had a look at a couple of videos ([video one](https://www.youtube.com/watch?v=SC5CX8drAtU)and[video two](https://www.youtube.com/watch?v=q6fPk0--eHY)), which were definitely worth watching for some background information and ideas.\nThe competition was sponsored by[FICO](https://www.fico.com/), who suggested a Mixed Integer Programming formulation for the problem. FICO made their Xpress Optimization Suite available to participants.\n## Exploratory Analysis\nThe first thing I did was to take a look at the distribution of gifts across the planet. I found that they were more or less evenly distributed across all the continents. Who would have guessed that so many good children lived in Antarctica? Perhaps there&rsquo;s limited scope to misbehave down there?\n> You better watch out\n> You better not cry\n> Better not pout\n> I'm telling you why\n> Santa Claus is coming to town.\n> > > Santa Claus Is Coming To Town\n> An obvious approach to the problem was to cluster the gifts according to their proximity and then deliver these clusters of gifts on a single trip. But this approach posed a problem: generally the k-means algorithm is implemented using the Euclidean distance, which makes sense on a plane but not the surface of a sphere. Of course, a distance matrix could be calculated using the Haversine formula but this would be a pretty enormous matrix, certainly too large for my hardware.\nI could, however, readily calculate the distance between each gift and its nearest neighbour, the distribution of which is displayed below. The majority of gifts are located less than 20 km from their nearest neighbour. Some are much closer, within a few hundred metres of each other. Others are significantly more distant. The summary data below indicates that the most isolated location is 2783 km from its nearest neighbour. Obviously the horizontal axis of the plot has been aggressively truncated!\n```\n`summary(gifts$dist\\_nearest)`\n```\n```\n`Min. 1st Qu. Median Mean 3rd Qu. Max. 0.1424 11.6500 18.1300 19.6200 25.9400 2783.0000`\n```\n![Histogram of distances from a gift to its nearest neighbour.](hist-distance-to-nearest.png)\nLet&rsquo;s look at the most isolated locations. The red points on the map below indicate gift locations which are at least 150 km distant from any other locations. Some of these are on extremely isolated pieces of land like Saint Helena, Coronation Island and the French Southern and Antarctic Lands. In terms of clustering, these points were going to be a little tricky and probably best delivered en route to somewhere else.\n![Locations of most isolated gifts.](map-isolated.png)## Clustering and Sorting\nMy first approach to solving the problem involved a simple clustering scheme. My gut feel was that in a tight cluster of gifts it would be important to deliver the heaviest ones first. I tried applying this simple heuristic to my clusters and the results were not particularly good. I then used a Genetic Algorithm (GA) to optimise the order of delivery within each cluster. The table below shows gifts allocated to two distinct trips (identified by TripId) with the order of delivery determined by the GA. These data confirmed that the best strategy was not simply to deliver the heaviest gifts first. Useful (and obvious in retrospect!).\n```\n`GiftId TripId Latitude Longitude Weight\n1 72834 1 52.08236 -99.84075 19.450129\n2 31544 1 51.99433 -99.66718 16.401965\n3 13514 1 51.54432 -100.80105 17.188096\n4 3711 1 51.42292 -99.61607 5.959066\n5 69035 1 51.42310 -99.21229 1.000000\n6 50071 2 52.14381 -100.39226 10.185356\n7 75708 2 52.05063 -100.09734 11.833779\n8 90582 2 51.57074 -100.11467 25.335999\n9 29591 2 50.94006 -98.51085 50.000000\n10 94799 2 51.40380 -100.10319 10.719852\n11 64255 2 51.39730 -99.99052 1.000000\n12 31418 2 50.98414 -100.38163 1.000000\n13 6254 2 51.37832 -100.42426 1.000000\n14 40247 2 51.18464 -100.77533 9.352351\n15 77396 2 51.27686 -100.77209 2.605262\n16 2600 2 50.82170 -99.46544 1.000000`\n```\nMy submission based on simple clustering with GA optimisation was fairly decent but nothing to tweet about.\n![Performance of submission 4.](kaggle-santa-submission-04.jpg)## Great Circle Solution\nThinking further about the problem it occurred to me that it might be efficient to deliver the gifts along paths that are as straight as possible, with minimal deviations. How about attempting to deliver the gifts along a Great Circle arc? For paths originating at the pole this would be equivalent to moving out and back close to a single line of longitude. It would certainly improve one aspect of Weighted Reindeer Weariness: the distance covered.\n> > The minor arc of a great circle between two points is the shortest surface-path between them.\n[> > Great Circle\n](https://en.wikipedia.org/wiki/Great_circle)> > , Wikipedia\nPutting this idea into practice and dividing the gifts up into longitudinal slices lead to an appreciable bump in my score.\n![Performance of submission 5.](kaggle-santa-submission-05.jpg)\nMy final innovation was to apply the same longitudinal slicing strategy but treat Antarctica separately. This again lead to an immediate improvement. I tried splitting off other areas of the globe, like Australia, but this didn&r...",
      "url": "https://datawookie.dev/blog/2016/01/kaggle-santas-stolen-sleigh"
    }
  ]
}