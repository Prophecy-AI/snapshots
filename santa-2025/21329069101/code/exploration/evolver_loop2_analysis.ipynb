{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29954f93",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis\n",
    "\n",
    "## Key Findings from Research:\n",
    "1. **Ensemble approach is critical** - Top kernels combine best solutions from 15+ sources per N\n",
    "2. **Lattice/crystallization patterns** - Blue Phase (0° ± 90°) + Pink Phase (180° ± 90°) interlock efficiently\n",
    "3. **Long SA runs needed** - sa_v1_parallel with -n 15000+ iterations, -r 80+ rounds\n",
    "4. **Fractional translation** - Fine-tuning with steps [0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]\n",
    "\n",
    "## Current Status:\n",
    "- Best CV: 70.673023 (exp_001)\n",
    "- Best LB: 70.676102 (exp_000)\n",
    "- Target: 68.894234\n",
    "- Gap: 1.78 points (2.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5aaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import rotate, translate\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tree geometry\n",
    "TREE_VERTICES = [\n",
    "    (0, 0.8), (-0.125, 0.5), (-0.05, 0.5), (-0.2, 0.25), (-0.1, 0.25),\n",
    "    (-0.35, 0), (-0.075, 0), (-0.075, -0.2), (0.075, -0.2), (0.075, 0),\n",
    "    (0.35, 0), (0.1, 0.25), (0.2, 0.25), (0.05, 0.5), (0.125, 0.5),\n",
    "]\n",
    "\n",
    "def create_tree_polygon(x, y, angle_deg):\n",
    "    poly = Polygon(TREE_VERTICES)\n",
    "    poly = rotate(poly, angle_deg, origin=(0, 0))\n",
    "    poly = translate(poly, x, y)\n",
    "    return poly\n",
    "\n",
    "def get_bounding_box_side(polygons):\n",
    "    if not polygons:\n",
    "        return 0\n",
    "    all_coords = []\n",
    "    for poly in polygons:\n",
    "        all_coords.extend(list(poly.exterior.coords))\n",
    "    xs = [c[0] for c in all_coords]\n",
    "    ys = [c[1] for c in all_coords]\n",
    "    width = max(xs) - min(xs)\n",
    "    height = max(ys) - min(ys)\n",
    "    return max(width, height)\n",
    "\n",
    "def parse_value(val):\n",
    "    if isinstance(val, str) and val.startswith('s'):\n",
    "        return float(val[1:])\n",
    "    return float(val)\n",
    "\n",
    "print(\"Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all available CSV files for ensemble\n",
    "csv_sources = []\n",
    "\n",
    "# Snapshots\n",
    "snapshot_csvs = glob.glob('/home/nonroot/snapshots/santa-2025/**/*.csv', recursive=True)\n",
    "csv_sources.extend(snapshot_csvs)\n",
    "\n",
    "# Local preoptimized\n",
    "local_csvs = glob.glob('/home/code/preoptimized/**/*.csv', recursive=True)\n",
    "csv_sources.extend(local_csvs)\n",
    "\n",
    "print(f\"Found {len(csv_sources)} CSV files\")\n",
    "print(\"\\nSample files:\")\n",
    "for f in csv_sources[:10]:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f37f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-N scores for all CSVs and find best per N\n",
    "def calculate_per_n_scores(csv_path):\n",
    "    \"\"\"Calculate score contribution for each N in a submission.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if not {'id', 'x', 'y', 'deg'}.issubset(df.columns):\n",
    "            return None\n",
    "        \n",
    "        df['x_val'] = df['x'].apply(parse_value)\n",
    "        df['y_val'] = df['y'].apply(parse_value)\n",
    "        df['deg_val'] = df['deg'].apply(parse_value)\n",
    "        df['n'] = df['id'].apply(lambda x: int(str(x).split('_')[0]))\n",
    "        \n",
    "        scores = {}\n",
    "        for n in range(1, 201):\n",
    "            n_data = df[df['n'] == n]\n",
    "            if len(n_data) != n:\n",
    "                continue\n",
    "            polygons = [create_tree_polygon(row['x_val'], row['y_val'], row['deg_val']) \n",
    "                       for _, row in n_data.iterrows()]\n",
    "            side = get_bounding_box_side(polygons)\n",
    "            scores[n] = side**2 / n\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Process all CSVs (this may take a while)\n",
    "all_scores = {}\n",
    "valid_csvs = []\n",
    "\n",
    "print(\"Processing CSVs...\")\n",
    "for csv_path in tqdm(csv_sources[:50]):  # Limit to first 50 for speed\n",
    "    scores = calculate_per_n_scores(csv_path)\n",
    "    if scores and len(scores) == 200:\n",
    "        name = os.path.basename(csv_path)\n",
    "        all_scores[name] = scores\n",
    "        valid_csvs.append(csv_path)\n",
    "        total = sum(scores.values())\n",
    "        print(f\"  {name}: {total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d631dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best per-N across all sources\n",
    "if all_scores:\n",
    "    best_per_n = {}\n",
    "    best_source_per_n = {}\n",
    "    \n",
    "    for n in range(1, 201):\n",
    "        best_score = float('inf')\n",
    "        best_source = None\n",
    "        for name, scores in all_scores.items():\n",
    "            if n in scores and scores[n] < best_score:\n",
    "                best_score = scores[n]\n",
    "                best_source = name\n",
    "        best_per_n[n] = best_score\n",
    "        best_source_per_n[n] = best_source\n",
    "    \n",
    "    ensemble_score = sum(best_per_n.values())\n",
    "    print(f\"\\nEnsemble score (best per N): {ensemble_score:.6f}\")\n",
    "    print(f\"Current best: 70.676102\")\n",
    "    print(f\"Improvement: {70.676102 - ensemble_score:.6f}\")\n",
    "    \n",
    "    # Show which sources contribute most\n",
    "    source_counts = {}\n",
    "    for n, source in best_source_per_n.items():\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(\"\\nSource contributions:\")\n",
    "    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {source}: {count} N values\")\n",
    "else:\n",
    "    print(\"No valid CSVs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the baseline solution to understand the lattice pattern\n",
    "baseline_path = '/home/nonroot/snapshots/santa-2025/21116303805/code/preoptimized/santa-2025.csv'\n",
    "df = pd.read_csv(baseline_path)\n",
    "df['x_val'] = df['x'].apply(parse_value)\n",
    "df['y_val'] = df['y'].apply(parse_value)\n",
    "df['deg_val'] = df['deg'].apply(parse_value)\n",
    "df['n'] = df['id'].apply(lambda x: int(str(x).split('_')[0]))\n",
    "\n",
    "# Analyze angle distribution for large N\n",
    "print(\"Angle distribution analysis for large N:\")\n",
    "for n in [50, 100, 150, 200]:\n",
    "    n_data = df[df['n'] == n]\n",
    "    angles = n_data['deg_val'].values % 360\n",
    "    \n",
    "    # Blue phase: 0° ± 90° (i.e., -90 to 90 or 270 to 360 and 0 to 90)\n",
    "    # Pink phase: 180° ± 90° (i.e., 90 to 270)\n",
    "    blue_count = sum((angles <= 90) | (angles > 270))\n",
    "    pink_count = sum((angles > 90) & (angles <= 270))\n",
    "    \n",
    "    print(f\"\\nN={n}:\")\n",
    "    print(f\"  Blue phase (up): {blue_count} ({100*blue_count/n:.1f}%)\")\n",
    "    print(f\"  Pink phase (down): {pink_count} ({100*pink_count/n:.1f}%)\")\n",
    "    print(f\"  Angle mean: {angles.mean():.1f}°, std: {angles.std():.1f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spacing patterns for large N\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for n in [100, 200]:\n",
    "    n_data = df[df['n'] == n].copy()\n",
    "    x = n_data['x_val'].values\n",
    "    y = n_data['y_val'].values\n",
    "    angles = n_data['deg_val'].values % 360\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    from scipy.spatial.distance import pdist\n",
    "    distances = pdist(np.column_stack([x, y]))\n",
    "    \n",
    "    print(f\"\\nN={n} spacing analysis:\")\n",
    "    print(f\"  Min distance: {distances.min():.4f}\")\n",
    "    print(f\"  Mean distance: {distances.mean():.4f}\")\n",
    "    print(f\"  Most common distance: {np.median(distances):.4f}\")\n",
    "    \n",
    "    # Find nearest neighbor distances\n",
    "    from scipy.spatial import cKDTree\n",
    "    tree = cKDTree(np.column_stack([x, y]))\n",
    "    nn_dists, _ = tree.query(np.column_stack([x, y]), k=2)\n",
    "    nn_dists = nn_dists[:, 1]  # Second nearest (first is self)\n",
    "    \n",
    "    print(f\"  Nearest neighbor mean: {nn_dists.mean():.4f}\")\n",
    "    print(f\"  Nearest neighbor std: {nn_dists.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are better CSVs in the bucket-of-chump dataset\n",
    "bucket_path = '/home/nonroot/snapshots/santa-2025/21116303805/code/preoptimized/bucket-of-chump'\n",
    "if os.path.exists(bucket_path):\n",
    "    bucket_csvs = glob.glob(f'{bucket_path}/**/*.csv', recursive=True)\n",
    "    print(f\"Found {len(bucket_csvs)} CSVs in bucket-of-chump\")\n",
    "    \n",
    "    for csv_path in bucket_csvs[:5]:\n",
    "        scores = calculate_per_n_scores(csv_path)\n",
    "        if scores:\n",
    "            total = sum(scores.values())\n",
    "            print(f\"  {os.path.basename(csv_path)}: {total:.6f}\")\n",
    "else:\n",
    "    print(\"bucket-of-chump not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. Current best score: 70.676102\")\n",
    "print(f\"2. Target score: 68.894234\")\n",
    "print(f\"3. Gap to close: 1.78 points (2.5%)\")\n",
    "print(f\"\\n4. Key insights:\")\n",
    "print(f\"   - Baseline is at a VERY tight local optimum\")\n",
    "print(f\"   - Standard optimization (bbox3, SA) cannot improve it\")\n",
    "print(f\"   - Need ENSEMBLE approach: combine best per-N from multiple sources\")\n",
    "print(f\"   - Need LONG optimization runs (hours, not minutes)\")\n",
    "print(f\"   - Lattice patterns: Blue (up) + Pink (down) phases interlock\")\n",
    "print(f\"\\n5. Next steps:\")\n",
    "print(f\"   - Implement ensemble from all available CSVs\")\n",
    "print(f\"   - Run sa_v1_parallel with -n 15000 -r 80 for hours\")\n",
    "print(f\"   - Try lattice-based generation for large N\")\n",
    "print(f\"   - Use fractional translation for fine-tuning\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
