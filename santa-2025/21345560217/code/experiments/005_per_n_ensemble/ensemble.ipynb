{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b01058",
   "metadata": {},
   "source": [
    "# Experiment 005: Per-N Ensemble from 116 Snapshots\n",
    "\n",
    "Systematically scan ALL 116 snapshots and select the BEST solution for EACH N value.\n",
    "This is the highest-leverage, lowest-risk improvement available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb98c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Tree shape constants\n",
    "TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]\n",
    "TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]\n",
    "\n",
    "def get_polygon_bounds(cx, cy, deg):\n",
    "    \"\"\"Calculate bounding box of rotated tree polygon\"\"\"\n",
    "    rad = deg * math.pi / 180.0\n",
    "    s, c = math.sin(rad), math.cos(rad)\n",
    "    x_coords = [TX[i] * c - TY[i] * s + cx for i in range(len(TX))]\n",
    "    y_coords = [TX[i] * s + TY[i] * c + cy for i in range(len(TX))]\n",
    "    return min(x_coords), max(x_coords), min(y_coords), max(y_coords)\n",
    "\n",
    "def calculate_score_for_n(trees):\n",
    "    \"\"\"Calculate score for a single N configuration\"\"\"\n",
    "    if not trees:\n",
    "        return float('inf')\n",
    "    \n",
    "    global_x_min, global_x_max = float('inf'), float('-inf')\n",
    "    global_y_min, global_y_max = float('inf'), float('-inf')\n",
    "    \n",
    "    for idx, cx, cy, deg in trees:\n",
    "        x_min, x_max, y_min, y_max = get_polygon_bounds(cx, cy, deg)\n",
    "        global_x_min = min(global_x_min, x_min)\n",
    "        global_x_max = max(global_x_max, x_max)\n",
    "        global_y_min = min(global_y_min, y_min)\n",
    "        global_y_max = max(global_y_max, y_max)\n",
    "    \n",
    "    side = max(global_x_max - global_x_min, global_y_max - global_y_min)\n",
    "    return side * side / len(trees)\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d068b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submission(filepath):\n",
    "    \"\"\"Load submission file, returns dict mapping n -> list of (idx, x, y, deg)\"\"\"\n",
    "    configurations = defaultdict(list)\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if len(df) < 20000:  # Skip incomplete files\n",
    "            return {}\n",
    "        for _, row in df.iterrows():\n",
    "            id_parts = row['id'].split('_')\n",
    "            n = int(id_parts[0])\n",
    "            idx = int(id_parts[1])\n",
    "            x = float(str(row['x']).replace('s', ''))\n",
    "            y = float(str(row['y']).replace('s', ''))\n",
    "            deg = float(str(row['deg']).replace('s', ''))\n",
    "            configurations[n].append((idx, x, y, deg))\n",
    "        for n in configurations:\n",
    "            configurations[n].sort(key=lambda t: t[0])\n",
    "        return dict(configurations)\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "print(\"Load function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ALL submission files in snapshots\n",
    "SNAPSHOT_DIR = '/home/nonroot/snapshots/santa-2025'\n",
    "snapshots = os.listdir(SNAPSHOT_DIR)\n",
    "print(f\"Found {len(snapshots)} snapshot directories\")\n",
    "\n",
    "# Find all submission.csv files\n",
    "all_submission_files = []\n",
    "for snapshot_id in snapshots:\n",
    "    # Check standard location\n",
    "    csv_path = f'{SNAPSHOT_DIR}/{snapshot_id}/submission/submission.csv'\n",
    "    if os.path.exists(csv_path):\n",
    "        all_submission_files.append(csv_path)\n",
    "    \n",
    "    # Check code folder\n",
    "    code_csv = f'{SNAPSHOT_DIR}/{snapshot_id}/code/submission.csv'\n",
    "    if os.path.exists(code_csv):\n",
    "        all_submission_files.append(code_csv)\n",
    "    \n",
    "    # Check experiment folders\n",
    "    exp_csvs = glob.glob(f'{SNAPSHOT_DIR}/{snapshot_id}/code/experiments/*/submission.csv')\n",
    "    all_submission_files.extend(exp_csvs)\n",
    "\n",
    "print(f\"Found {len(all_submission_files)} submission files to scan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67230c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best per-N with baseline\n",
    "baseline_path = '/home/code/experiments/002_valid_baseline/submission.csv'\n",
    "baseline_configs = load_submission(baseline_path)\n",
    "\n",
    "best_per_n = {}\n",
    "for n in range(1, 201):\n",
    "    if n in baseline_configs and len(baseline_configs[n]) == n:\n",
    "        score = calculate_score_for_n(baseline_configs[n])\n",
    "        best_per_n[n] = {'score': score, 'trees': baseline_configs[n], 'source': 'baseline'}\n",
    "    else:\n",
    "        best_per_n[n] = {'score': float('inf'), 'trees': None, 'source': None}\n",
    "\n",
    "baseline_total = sum(best_per_n[n]['score'] for n in range(1, 201))\n",
    "print(f\"Baseline total score: {baseline_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan ALL submission files\n",
    "print(f\"\\nScanning {len(all_submission_files)} submission files...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improvements_found = []\n",
    "files_processed = 0\n",
    "\n",
    "for filepath in all_submission_files:\n",
    "    configs = load_submission(filepath)\n",
    "    if not configs:\n",
    "        continue\n",
    "    \n",
    "    files_processed += 1\n",
    "    \n",
    "    for n in range(1, 201):\n",
    "        if n not in configs or len(configs[n]) != n:\n",
    "            continue\n",
    "        \n",
    "        score = calculate_score_for_n(configs[n])\n",
    "        \n",
    "        if score < best_per_n[n]['score'] - 1e-10:  # Meaningful improvement\n",
    "            improvement = best_per_n[n]['score'] - score\n",
    "            old_source = best_per_n[n]['source']\n",
    "            \n",
    "            best_per_n[n] = {'score': score, 'trees': configs[n], 'source': filepath}\n",
    "            improvements_found.append((n, improvement, filepath))\n",
    "            \n",
    "            if improvement > 0.0001:  # Only print significant improvements\n",
    "                print(f\"âœ… N={n}: {best_per_n[n]['score'] + improvement:.6f} -> {score:.6f} (improved by {improvement:.6f})\")\n",
    "    \n",
    "    if files_processed % 50 == 0:\n",
    "        print(f\"  Processed {files_processed}/{len(all_submission_files)} files...\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Processed {files_processed} valid submission files\")\n",
    "print(f\"Found {len(improvements_found)} improvements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble score\n",
    "ensemble_score = sum(best_per_n[n]['score'] for n in range(1, 201))\n",
    "print(f\"\\nBaseline score: {baseline_total:.6f}\")\n",
    "print(f\"Ensemble score: {ensemble_score:.6f}\")\n",
    "print(f\"Total improvement: {baseline_total - ensemble_score:.6f}\")\n",
    "\n",
    "# Count sources\n",
    "source_counts = defaultdict(int)\n",
    "for n in range(1, 201):\n",
    "    source_counts[best_per_n[n]['source']] += 1\n",
    "\n",
    "print(f\"\\nSource distribution (top 10):\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {source}: {count} N values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7814a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show significant improvements\n",
    "if improvements_found:\n",
    "    significant = [(n, imp, src) for n, imp, src in improvements_found if imp > 0.0001]\n",
    "    if significant:\n",
    "        print(f\"\\nSignificant improvements (> 0.0001):\")\n",
    "        for n, imp, src in sorted(significant, key=lambda x: -x[1])[:20]:\n",
    "            print(f\"  N={n}: improved by {imp:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nNo significant improvements found (all < 0.0001)\")\n",
    "else:\n",
    "    print(\"\\nNo improvements found at all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble submission\n",
    "import csv\n",
    "\n",
    "print(\"\\nSaving ensemble submission...\")\n",
    "\n",
    "with open('/home/code/experiments/005_per_n_ensemble/submission.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'x', 'y', 'deg'])\n",
    "    for n in range(1, 201):\n",
    "        trees = best_per_n[n]['trees']\n",
    "        if trees:\n",
    "            for idx, x, y, deg in trees:\n",
    "                writer.writerow([f'{n:03d}_{idx}', f's{x:.17f}', f's{y:.17f}', f's{deg:.17f}'])\n",
    "\n",
    "# Also save to submission folder\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "with open('/home/submission/submission.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'x', 'y', 'deg'])\n",
    "    for n in range(1, 201):\n",
    "        trees = best_per_n[n]['trees']\n",
    "        if trees:\n",
    "            for idx, x, y, deg in trees:\n",
    "                writer.writerow([f'{n:03d}_{idx}', f's{x:.17f}', f's{y:.17f}', f's{deg:.17f}'])\n",
    "\n",
    "print(f\"Saved ensemble submission\")\n",
    "print(f\"Final score: {ensemble_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'cv_score': ensemble_score,\n",
    "    'baseline_score': baseline_total,\n",
    "    'improvement': baseline_total - ensemble_score,\n",
    "    'improvements_found': len(improvements_found),\n",
    "    'files_scanned': len(all_submission_files),\n",
    "    'files_processed': files_processed\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/005_per_n_ensemble/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetrics saved\")\n",
    "print(f\"CV Score: {ensemble_score:.6f}\")\n",
    "print(f\"Improvement: {baseline_total - ensemble_score:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
