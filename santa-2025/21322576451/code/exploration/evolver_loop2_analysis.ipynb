{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da15b087",
   "metadata": {},
   "source": [
    "# Loop 2 Analysis: Understanding the Gap to Target\n",
    "\n",
    "## Key Questions:\n",
    "1. Where is the gap concentrated (which N values)?\n",
    "2. What would be needed per-N to reach the target?\n",
    "3. What approaches haven't been tried yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load the current best valid submission\n",
    "df = pd.read_csv('/home/submission/submission.csv')\n",
    "print(f\"Submission shape: {df.shape}\")\n",
    "\n",
    "# Parse the data\n",
    "def parse_value(v):\n",
    "    if isinstance(v, str) and v.startswith('s'):\n",
    "        return float(v[1:])\n",
    "    return float(v)\n",
    "\n",
    "df['xf'] = df['x'].apply(parse_value)\n",
    "df['yf'] = df['y'].apply(parse_value)\n",
    "df['degf'] = df['deg'].apply(parse_value)\n",
    "df['n_val'] = df['id'].apply(lambda x: int(str(x).split('_')[0]))\n",
    "\n",
    "print(f\"N values range: {df['n_val'].min()} to {df['n_val'].max()}\")\n",
    "print(f\"Unique N values: {df['n_val'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefebd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate score per N\n",
    "def calculate_score_per_n(df):\n",
    "    scores = []\n",
    "    for n in range(1, 201):\n",
    "        group = df[df['n_val'] == n]\n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "        x_min, x_max = group['xf'].min(), group['xf'].max()\n",
    "        y_min, y_max = group['yf'].min(), group['yf'].max()\n",
    "        # Need to account for tree polygon extent\n",
    "        # Tree is roughly 0.7 wide and 1.0 tall\n",
    "        # But rotation changes this - use approximate bounding\n",
    "        side = max(x_max - x_min + 0.7, y_max - y_min + 1.0)\n",
    "        score_n = (side ** 2) / n\n",
    "        scores.append({'n': n, 'side': side, 'score': score_n, 'count': len(group)})\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "scores_df = calculate_score_per_n(df)\n",
    "print(f\"Total score (approx): {scores_df['score'].sum():.6f}\")\n",
    "print(f\"\\nTop 10 N values by score contribution:\")\n",
    "print(scores_df.nlargest(10, 'score')[['n', 'side', 'score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target analysis\n",
    "TARGET = 68.897509\n",
    "CURRENT = 70.626088\n",
    "GAP = CURRENT - TARGET\n",
    "\n",
    "print(f\"Current score: {CURRENT:.6f}\")\n",
    "print(f\"Target score: {TARGET:.6f}\")\n",
    "print(f\"Gap to close: {GAP:.6f} ({GAP/TARGET*100:.2f}%)\")\n",
    "\n",
    "# If we need to close 1.73 points across 200 N values\n",
    "# Average improvement needed per N = 1.73/200 = 0.00865\n",
    "print(f\"\\nAverage improvement needed per N: {GAP/200:.6f}\")\n",
    "\n",
    "# But score = S^2/n, so improvement is weighted by 1/n\n",
    "# Small N values have much higher leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what side length reduction is needed per N to reach target\n",
    "# Current: sum(S_n^2/n) = 70.626\n",
    "# Target: sum(S_n^2/n) = 68.897\n",
    "# Need to reduce by 1.729\n",
    "\n",
    "# If we reduce ALL side lengths by factor k:\n",
    "# New score = sum((k*S_n)^2/n) = k^2 * sum(S_n^2/n) = k^2 * 70.626\n",
    "# For k^2 * 70.626 = 68.897\n",
    "# k^2 = 68.897/70.626 = 0.9755\n",
    "# k = 0.9877\n",
    "\n",
    "k_needed = np.sqrt(TARGET / CURRENT)\n",
    "print(f\"If all side lengths reduced uniformly:\")\n",
    "print(f\"  Reduction factor k = {k_needed:.6f}\")\n",
    "print(f\"  Each side needs to be {(1-k_needed)*100:.2f}% smaller\")\n",
    "print(f\"  For a side of 1.0, need to reduce by {(1-k_needed):.4f} units\")\n",
    "\n",
    "# This is a 1.23% reduction in all side lengths\n",
    "# For N=1 with side ~0.813, need to reduce by 0.01 units\n",
    "# For N=200 with side ~5.5, need to reduce by 0.068 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ce038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load snapshot experiment history to understand what's been tried\n",
    "import os\n",
    "\n",
    "snapshot_path = '/home/nonroot/snapshots/santa-2025/21222392487/code/session_state.json'\n",
    "if os.path.exists(snapshot_path):\n",
    "    with open(snapshot_path) as f:\n",
    "        snapshot_state = json.load(f)\n",
    "    \n",
    "    print(f\"Snapshot has {len(snapshot_state.get('experiments', []))} experiments\")\n",
    "    print(f\"Snapshot has {len(snapshot_state.get('submissions', []))} submissions\")\n",
    "    \n",
    "    # List experiment names and scores\n",
    "    print(\"\\nExperiment history:\")\n",
    "    for exp in snapshot_state.get('experiments', []):\n",
    "        print(f\"  {exp['name']}: {exp.get('cv_score', 'N/A'):.6f}\")\n",
    "else:\n",
    "    print(\"Snapshot not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e23186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze submissions from snapshot\n",
    "print(\"\\nSubmission history from snapshot:\")\n",
    "for sub in snapshot_state.get('submissions', []):\n",
    "    lb = sub.get('lb_score', 'N/A')\n",
    "    cv = sub.get('cv_score', 'N/A')\n",
    "    error = sub.get('error', '')\n",
    "    print(f\"  {sub.get('model_name', 'unknown')}: CV={cv}, LB={lb}, Error={error[:50] if error else 'None'}\")\n",
    "\n",
    "# Count valid vs invalid submissions\n",
    "valid_subs = [s for s in snapshot_state.get('submissions', []) if not s.get('error')]\n",
    "invalid_subs = [s for s in snapshot_state.get('submissions', []) if s.get('error')]\n",
    "print(f\"\\nValid submissions: {len(valid_subs)}\")\n",
    "print(f\"Invalid submissions (overlaps): {len(invalid_subs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: What approaches HAVEN'T been tried?\n",
    "# From the snapshot experiments:\n",
    "# - bbox3 optimization ✓\n",
    "# - SA optimization ✓\n",
    "# - Tessellation ✓\n",
    "# - Genetic algorithm ✓\n",
    "# - MIP for small N ✓\n",
    "# - Ensemble from multiple sources ✓\n",
    "# - Asymmetric perturbations ✓\n",
    "# - Deletion cascade ✓\n",
    "# - Corner rebuild ✓\n",
    "\n",
    "# What's NOT in the list:\n",
    "# 1. Reinforcement Learning (mentioned in web research)\n",
    "# 2. Mixed-Integer Programming for medium N (only small N tried)\n",
    "# 3. Constraint Programming with global constraints\n",
    "# 4. Lattice-based initialization with optimal spacing\n",
    "# 5. Cross-pollination from top LB submissions (not public)\n",
    "\n",
    "print(\"Approaches tried in snapshot:\")\n",
    "approaches_tried = [\n",
    "    \"bbox3 C++ optimizer\",\n",
    "    \"Simulated Annealing (SA)\",\n",
    "    \"Tessellation patterns\",\n",
    "    \"Genetic Algorithm\",\n",
    "    \"MIP for small N\",\n",
    "    \"Ensemble from multiple sources\",\n",
    "    \"Asymmetric perturbations\",\n",
    "    \"Deletion cascade\",\n",
    "    \"Corner rebuild\",\n",
    "    \"Fractional translation\",\n",
    "    \"Basin hopping\",\n",
    "    \"Random restart SA\",\n",
    "    \"Greedy beam search\"\n",
    "]\n",
    "for a in approaches_tried:\n",
    "    print(f\"  ✓ {a}\")\n",
    "\n",
    "print(\"\\nApproaches NOT tried:\")\n",
    "approaches_not_tried = [\n",
    "    \"Reinforcement Learning\",\n",
    "    \"MIP for medium N (10-50)\",\n",
    "    \"Constraint Programming with global constraints\",\n",
    "    \"Lattice-based initialization with optimal spacing\",\n",
    "    \"No-fit polygon (NFP) based placement\",\n",
    "    \"Jostle algorithm\",\n",
    "    \"Density gradient flow\",\n",
    "    \"Global boundary tension optimization\"\n",
    "]\n",
    "for a in approaches_not_tried:\n",
    "    print(f\"  ✗ {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the crystallization pattern from the Why Not kernel\n",
    "# The kernel shows that trees form lattice patterns with alternating orientations\n",
    "\n",
    "# Let's analyze the current solution's orientation distribution\n",
    "print(\"Orientation analysis:\")\n",
    "for n in [10, 50, 100, 150, 200]:\n",
    "    group = df[df['n_val'] == n]\n",
    "    angles = group['degf'].values % 360\n",
    "    \n",
    "    # Count trees pointing up (0-90 or 270-360) vs down (90-270)\n",
    "    up = np.sum((angles <= 90) | (angles >= 270))\n",
    "    down = np.sum((angles > 90) & (angles < 270))\n",
    "    \n",
    "    print(f\"  N={n}: Up={up}, Down={down}, Ratio={up/(up+down):.2f}\")\n",
    "\n",
    "# If the ratio is close to 0.5, trees are alternating orientations\n",
    "# This is the \"crystallization\" pattern mentioned in the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any candidate files in the snapshot that might be better\n",
    "import os\n",
    "\n",
    "candidate_dir = '/home/nonroot/snapshots/santa-2025/21222392487/code/submission_candidates'\n",
    "if os.path.exists(candidate_dir):\n",
    "    candidates = os.listdir(candidate_dir)\n",
    "    print(f\"Found {len(candidates)} candidate files in snapshot\")\n",
    "    \n",
    "    # Score a few candidates to see if any are better\n",
    "    from shapely.geometry import Polygon\n",
    "    from shapely import affinity\n",
    "    from shapely.strtree import STRtree\n",
    "    \n",
    "    def get_tree_polygon(x, y, deg):\n",
    "        TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]\n",
    "        TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]\n",
    "        p = Polygon(zip(TX, TY))\n",
    "        p = affinity.rotate(p, deg, origin=(0, 0))\n",
    "        p = affinity.translate(p, x, y)\n",
    "        return p\n",
    "    \n",
    "    def score_submission(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['xf'] = df['x'].apply(lambda v: float(str(v).lstrip('s')))\n",
    "        df['yf'] = df['y'].apply(lambda v: float(str(v).lstrip('s')))\n",
    "        df['degf'] = df['deg'].apply(lambda v: float(str(v).lstrip('s')))\n",
    "        df['n_val'] = df['id'].apply(lambda x: int(str(x).split('_')[0]))\n",
    "        \n",
    "        total_score = 0\n",
    "        for n in range(1, 201):\n",
    "            group = df[df['n_val'] == n]\n",
    "            if len(group) != n:\n",
    "                return None  # Invalid\n",
    "            \n",
    "            # Get bounding box of all tree polygons\n",
    "            all_coords = []\n",
    "            for _, row in group.iterrows():\n",
    "                poly = get_tree_polygon(row['xf'], row['yf'], row['degf'])\n",
    "                coords = np.array(poly.exterior.coords)\n",
    "                all_coords.append(coords)\n",
    "            all_coords = np.vstack(all_coords)\n",
    "            \n",
    "            x_range = all_coords[:, 0].max() - all_coords[:, 0].min()\n",
    "            y_range = all_coords[:, 1].max() - all_coords[:, 1].min()\n",
    "            side = max(x_range, y_range)\n",
    "            total_score += (side ** 2) / n\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    # Score the current submission\n",
    "    current_score = score_submission('/home/submission/submission.csv')\n",
    "    print(f\"\\nCurrent submission score: {current_score:.6f}\")\n",
    "else:\n",
    "    print(\"Candidate directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"LOOP 2 ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCurrent best valid LB score: 70.626088\")\n",
    "print(f\"Target score: 68.897509\")\n",
    "print(f\"Gap: 1.728579 (2.51%)\")\n",
    "print(f\"\\nKey findings:\")\n",
    "print(\"1. 38 experiments in snapshot all converged to ~70.624\")\n",
    "print(\"2. All standard approaches (SA, GA, bbox3, MIP, ensemble) exhausted\")\n",
    "print(\"3. Need ~1.23% reduction in ALL side lengths to reach target\")\n",
    "print(\"4. Local overlap validation is LESS strict than Kaggle's\")\n",
    "print(\"5. Trees show crystallization pattern (alternating orientations)\")\n",
    "print(f\"\\nRecommended next steps:\")\n",
    "print(\"1. Try fundamentally different initialization (lattice-based)\")\n",
    "print(\"2. Use more conservative overlap detection (add buffer)\")\n",
    "print(\"3. Focus on N=2-50 where improvements have highest leverage\")\n",
    "print(\"4. Try NFP-based placement for guaranteed non-overlap\")\n",
    "print(\"5. Research what top LB teams are doing differently\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
