# Santa 2025 Christmas Tree Packing - Evolved Seed Prompt (Loop 3)

## Current Status
- Best CV score: 70.676102 from all experiments (identical)
- Best LB score: 70.676102 (confirmed from 2 valid submissions)
- Target: 68.919154 | Gap to target: 1.756948 (2.55%)
- Remaining submissions: 93 (ABUNDANT - submit after every experiment!)

## CV-LB Relationship
- CV-LB gap is essentially ZERO (0.000000)
- Our scoring function matches Kaggle's exactly
- This is expected for an optimization problem

## Response to Evaluator

The evaluator correctly identified that:
1. **Deletion cascade found ZERO improvements** - confirming the baseline is at a very strong local optimum
2. **All 30 pre-optimized sources converge to the same local optimum** - combining them doesn't help
3. **We need fundamentally different approaches** - not modifications of existing solutions

I AGREE with the evaluator's assessment. The key insight is:
- Local search on pre-optimized solutions = WASTED EXPERIMENTS
- We must generate configurations FROM SCRATCH to explore different solution basins

The evaluator's top priority (multi-start random initialization) is correct, but I want to add:
- **Lattice-based approach for large N** (from egortrushin kernel) is equally important
- **bbox3 with fresh random starts** could help after generating new configurations

## CRITICAL: What We've Proven

After 3 experiments, we've PROVEN that:
1. ❌ Combining existing solutions doesn't help (all converge to same optimum)
2. ❌ Deletion cascade doesn't help (removing trees and recentering doesn't improve)
3. ❌ Local modifications of pre-optimized solutions are USELESS

The baseline is at a **VERY STRONG LOCAL OPTIMUM**. To improve, we MUST:
- Generate configurations FROM SCRATCH
- Explore DIFFERENT SOLUTION BASINS
- Use CONSTRUCTIVE approaches, not local search

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY] Multi-Start Random Initialization for Small N (1-20)**

For small N values, try thousands of random placements and keep the best:

```python
import numpy as np
from shapely.geometry import Polygon
from shapely import affinity

def random_initialization(n, num_restarts=10000):
    """Generate random initial configs and keep the best non-overlapping one."""
    best_config = None
    best_score = float('inf')
    
    for _ in range(num_restarts):
        # Random placement in a reasonable area
        trees = []
        for i in range(n):
            x = np.random.uniform(-2, 2)
            y = np.random.uniform(-2, 2)
            deg = np.random.uniform(0, 360)
            trees.append((x, y, deg))
        
        # Check for overlaps
        if not has_overlap(trees):
            # Recenter and calculate score
            trees = recenter_trees(trees)
            score = get_bounding_box_side(trees)**2 / n
            if score < best_score:
                best_score = score
                best_config = trees
    
    return best_config, best_score

# For each N from 1 to 20, try random initialization
for n in range(1, 21):
    baseline_score = baseline_per_n[n]
    random_config, random_score = random_initialization(n, num_restarts=10000)
    
    if random_score < baseline_score:
        print(f"N={n}: IMPROVED! {baseline_score:.6f} -> {random_score:.6f}")
```

**Why this is the right approach:**
- N=1-20 contribute ~11.4% of total score
- Small N is computationally tractable for exhaustive random search
- Explores DIFFERENT solution basins than the pre-optimized solutions

### 2. **[HIGH PRIORITY] Lattice-Based Approach for Large N**

From the egortrushin kernel, use grid-based placement for large N:

```python
# Lattice configurations from egortrushin kernel:
# N=72:  nt=[4, 9]  (4x9 grid of 2-tree cells = 72 trees)
# N=100: nt=[5, 10]
# N=110: nt=[5, 11]
# N=144: nt=[6, 12]
# N=156: nt=[6, 13]
# N=196: nt=[7, 14]
# N=200: nt=[7, 15] (take first 200 from 210)

# The approach:
# 1. Start with 2 base trees (one at 0°, one at 180°)
# 2. Translate them in x and y directions to create a grid
# 3. Use SA to optimize the translation parameters (dx, dy)
# 4. This generates crystalline/lattice packings
```

**Why this is important:**
- Large N values (100-200) contribute ~48% of total score
- Lattice packings can be tighter than random optimization for large N
- Explores a FUNDAMENTALLY DIFFERENT solution structure

### 3. **[MEDIUM PRIORITY] Use bbox3 C++ Optimizer with Fresh Starts**

After generating new configurations from random/lattice approaches:

```bash
# Compile bbox3
g++ -O3 -march=native -std=c++17 -fopenmp -o bbox3 bbox3.cpp

# Run with high iterations
./bbox3 -i new_config.csv -o optimized.csv -n 50000 -r 256
```

**Important:** Only use bbox3 AFTER generating new configurations. Running it on the pre-optimized baseline is USELESS.

### 4. **[LOWER PRIORITY] Greedy Backtracking Construction**

Build configurations from scratch using greedy placement:

```python
def greedy_construction(n):
    """Build configuration by placing trees one at a time."""
    trees = []
    for i in range(n):
        # Find best position for next tree
        best_pos = None
        best_score = float('inf')
        
        for _ in range(1000):  # Try random positions
            x = np.random.uniform(-2, 2)
            y = np.random.uniform(-2, 2)
            deg = np.random.uniform(0, 360)
            
            candidate = trees + [(x, y, deg)]
            if not has_overlap(candidate):
                score = get_bounding_box_side(candidate)**2 / len(candidate)
                if score < best_score:
                    best_score = score
                    best_pos = (x, y, deg)
        
        if best_pos:
            trees.append(best_pos)
    
    return trees
```

## What NOT to Try

1. ❌ **More local search on pre-optimized solutions** - PROVEN USELESS
2. ❌ **Combining existing solutions** - All converge to same optimum
3. ❌ **Deletion cascade variations** - Already tried, no improvement
4. ❌ **Running bbox3 on baseline** - Will not escape local optimum

## SUBMISSION STRATEGY

**SUBMIT AFTER EVERY EXPERIMENT!**
- We have 93 submissions remaining (ABUNDANT)
- LB feedback is FREE information
- Every experiment should produce a submission

## Validation Notes

- CV-LB gap is zero - our scoring matches Kaggle's
- Overlap detection is working correctly
- Precision handling: use `float_format='%.18f'` when saving CSVs

## Key Resources

1. **bbox3 C++ optimizer**: `/home/nonroot/snapshots/santa-2025/21116303805/code/bbox3`
2. **Pre-optimized baseline**: `/home/code/experiments/001_baseline/santa-2025.csv`
3. **egortrushin lattice kernel**: `/home/code/research/kernels/egortrushin_santa25-simulated-annealing-with-translations/`

## Expected Outcomes

If multi-start random initialization works:
- Small N improvements could save 0.1-0.5 points
- Combined with lattice approach for large N, could close the gap

If neither works:
- Try genetic algorithm with crossover
- Try longer optimization runs (hours, not minutes)
- Research what top teams are doing differently

## REMEMBER

The target of 68.919154 IS ACHIEVABLE. The gap of 1.76 points requires:
1. Generating configurations FROM SCRATCH
2. Exploring DIFFERENT solution basins
3. NOT relying on local search of existing solutions

DO NOT GIVE UP. The solution exists. Find it.