# Santa 2025 Christmas Tree Packing - Evolved Seed Prompt (Loop 2)

## Current Status
- Best CV score: 70.676102 from exp_000 (001_baseline)
- Best LB score: 70.676102 (confirmed from exp_000)
- Target: 68.919154 | Gap to target: 1.757 points (2.55%)

## CRITICAL: Submission Failed Due to Precision Loss
**The 002_full_ensemble submission FAILED** with error "Overlapping trees in group 040".

**Root cause:** When loading CSV files through pandas and re-saving, floating-point precision was lost:
- Original: `s0.154097069621355887` (18 decimal places)
- After pandas: `s0.1540970696213559` (16 decimal places)

This precision loss caused trees that were barely non-overlapping to now overlap.

**FIX REQUIRED:** When manipulating submission files:
1. Read the original CSV as raw text, not through pandas
2. Preserve ALL decimal places when writing
3. Or use pandas with `float_format='%.18f'` when saving

## Response to Evaluator
The evaluator correctly identified that:
1. Two experiments completed with zero improvement - we're stuck at 70.676102
2. All 30 pre-optimized sources converge to the SAME local optimum
3. The 1.76 point gap requires escaping this basin entirely
4. **Deletion cascade has been the top priority for 2 loops and hasn't been done**

I agree completely. The next experiment MUST:
1. Fix the precision issue first
2. Implement deletion cascade to generate NOVEL configurations
3. Focus on small N values (N=1-50 contribute 26.9% of total score)

## Key Insights from Research
From Medium article on Santa 2025:
- **N < 58**: Unstructured chaotic packings via SA work better
- **N > 58**: Crystalline/lattice packing is mathematically superior
- Top teams use C++ with AVX2 vectorization for speed
- bbox3 optimizer is already compiled and available

## Available Resources
1. **Pre-compiled bbox3 optimizer**: `/home/nonroot/snapshots/santa-2025/21116303805/code/bbox3`
2. **30 pre-optimized CSVs**: All at same local optimum (~70.67)
3. **Baseline submission**: `/home/code/experiments/001_baseline/submission.csv` (LB: 70.676102)

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Deletion Cascade Algorithm
Generate novel small-N configurations by removing trees from larger configs:

```python
import pandas as pd
import numpy as np
from shapely.geometry import Polygon
from shapely import affinity

# Tree geometry
TX = [0, 0.125, 0.0625, 0.2, 0.1, 0.35, 0.075, 0.075, -0.075, -0.075, -0.35, -0.1, -0.2, -0.0625, -0.125]
TY = [0.8, 0.5, 0.5, 0.25, 0.25, 0, 0, -0.2, -0.2, 0, 0, 0.25, 0.25, 0.5, 0.5]

def get_tree_polygon(x, y, deg):
    base_poly = Polygon(zip(TX, TY))
    rotated = affinity.rotate(base_poly, deg, origin=(0, 0))
    return affinity.translate(rotated, x, y)

def get_bounding_box_side(trees):
    all_x, all_y = [], []
    for x, y, deg in trees:
        poly = get_tree_polygon(x, y, deg)
        bounds = poly.bounds
        all_x.extend([bounds[0], bounds[2]])
        all_y.extend([bounds[1], bounds[3]])
    return max(max(all_x) - min(all_x), max(all_y) - min(all_y))

def recenter_trees(trees):
    """Recenter trees to minimize bounding box."""
    all_x, all_y = [], []
    for x, y, deg in trees:
        poly = get_tree_polygon(x, y, deg)
        bounds = poly.bounds
        all_x.extend([bounds[0], bounds[2]])
        all_y.extend([bounds[1], bounds[3]])
    
    center_x = (min(all_x) + max(all_x)) / 2
    center_y = (min(all_y) + max(all_y)) / 2
    
    return [(x - center_x, y - center_y, deg) for x, y, deg in trees]

def deletion_cascade(baseline_configs):
    """Generate novel configs by removing trees from larger configs."""
    best_configs = {n: list(baseline_configs[n]) for n in range(1, 201)}
    improvements = []
    
    for n in range(200, 1, -1):
        trees = best_configs[n]
        current_score = get_bounding_box_side(best_configs[n-1])**2 / (n-1)
        best_removal_score = current_score
        best_removal_config = best_configs[n-1]
        
        for i in range(n):
            # Remove tree i
            new_trees = trees[:i] + trees[i+1:]
            new_trees = recenter_trees(new_trees)
            new_score = get_bounding_box_side(new_trees)**2 / (n-1)
            
            if new_score < best_removal_score:
                best_removal_score = new_score
                best_removal_config = new_trees
        
        if best_removal_score < current_score:
            best_configs[n-1] = best_removal_config
            improvements.append((n-1, current_score - best_removal_score))
            print(f"N={n-1}: Improved by {current_score - best_removal_score:.6f}")
    
    return best_configs, improvements
```

**Expected improvement**: 0.1-0.3 points if we find better small-N configurations.

### 2. **[HIGH PRIORITY]** Exhaustive Search for N=1-10
Small N values are critical (N=1 alone contributes 0.661 to score).
For N=1-10, we can afford exhaustive search:

```python
def optimize_small_n(n, trees, angle_step=0.1):
    """Exhaustive search over rotation angles for small N."""
    best_side = get_bounding_box_side(trees)
    best_config = trees
    
    # Try rotating entire configuration
    for angle in np.arange(0, 90, angle_step):
        rotated = rotate_all_trees(trees, angle)
        side = get_bounding_box_side(rotated)
        if side < best_side:
            best_side = side
            best_config = rotated
    
    return best_config, best_side
```

### 3. **[MEDIUM PRIORITY]** Multi-Start Random Initialization
For N=1-20, try completely random placements and optimize:

```python
def random_initialization(n, num_restarts=100):
    """Generate random initial configs and optimize."""
    best_config = None
    best_score = float('inf')
    
    for _ in range(num_restarts):
        # Random placement in a reasonable area
        trees = []
        for i in range(n):
            x = np.random.uniform(-2, 2)
            y = np.random.uniform(-2, 2)
            deg = np.random.uniform(0, 360)
            trees.append((x, y, deg))
        
        # Check for overlaps and optimize
        if not has_overlap(trees):
            score = get_bounding_box_side(trees)**2 / n
            if score < best_score:
                best_score = score
                best_config = trees
    
    return best_config, best_score
```

### 4. **[LOWER PRIORITY]** Use bbox3 for Post-Processing
After generating new configurations via deletion cascade, refine with bbox3:

```bash
# Copy new submission to working directory
cp submission.csv /tmp/input.csv

# Run bbox3 optimizer
/home/nonroot/snapshots/santa-2025/21116303805/code/bbox3 \
    -i /tmp/input.csv \
    -o /tmp/output.csv \
    -n 10000 \
    -r 64
```

## What NOT to Try
1. ❌ Loading CSVs through pandas without preserving precision
2. ❌ Running more SA on pre-optimized solutions (already at local optimum)
3. ❌ Combining existing solutions (all at same optimum)
4. ❌ Hyperparameter tuning (too far from target)

## SUBMISSION STRATEGY
- Remaining submissions: 94 (ABUNDANT!)
- Submit after EVERY successful experiment
- LB feedback is free - USE IT!

## Validation Notes
- CV = sum of (side^2 / N) for N=1 to 200
- CV-LB gap is essentially zero (confirmed from exp_000)
- Overlap detection is critical - use Shapely STRtree for efficiency
- **PRESERVE FULL PRECISION** when saving submission files

## Implementation Checklist for Next Experiment
1. [ ] Load baseline submission preserving full precision
2. [ ] Implement deletion cascade from N=200 down to N=1
3. [ ] Recenter trees after each deletion
4. [ ] Ensemble: keep best of (baseline, deletion cascade) for each N
5. [ ] Verify no overlaps before saving
6. [ ] Save with full precision: `df.to_csv(..., float_format='%.18f')`
7. [ ] Calculate total score and compare to baseline
8. [ ] Submit if score improves