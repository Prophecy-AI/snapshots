{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befba8bf",
   "metadata": {},
   "source": [
    "# Loop 7 LB Feedback Analysis\n",
    "\n",
    "**Submission**: exp_006 (011_long_optimization)\n",
    "**CV Score**: 70.6600\n",
    "**LB Score**: 70.6600\n",
    "**Gap**: 0.0000 (CV = LB, as expected for this optimization problem)\n",
    "\n",
    "## Key Observations\n",
    "1. CV = LB confirms this is a pure optimization problem with no train/test split\n",
    "2. 33 minutes of C++ optimization found ZERO improvement\n",
    "3. The baseline is at an extremely tight local optimum with no slack\n",
    "4. Gap to target: 70.66 - 68.92 = 1.74 points (2.46%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import math\n",
    "\n",
    "# Load current best submission\n",
    "df = pd.read_csv('/home/code/external_data/saspav_latest/santa-2025.csv')\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function\n",
    "@njit\n",
    "def make_polygon_template():\n",
    "    tw=0.15; th=0.2; bw=0.7; mw=0.4; ow=0.25\n",
    "    tip=0.8; t1=0.5; t2=0.25; base=0.0; tbot=-th\n",
    "    x=np.array([0,ow/2,ow/4,mw/2,mw/4,bw/2,tw/2,tw/2,-tw/2,-tw/2,-bw/2,-mw/4,-mw/2,-ow/4,-ow/2],np.float64)\n",
    "    y=np.array([tip,t1,t1,t2,t2,base,base,tbot,tbot,base,base,t2,t2,t1,t1],np.float64)\n",
    "    return x,y\n",
    "\n",
    "@njit\n",
    "def score_group(xs, ys, degs, tx, ty):\n",
    "    n = xs.size\n",
    "    V = tx.size\n",
    "    mnx = 1e300; mny = 1e300; mxx = -1e300; mxy = -1e300\n",
    "    for i in range(n):\n",
    "        r = degs[i] * math.pi / 180.0\n",
    "        c = math.cos(r); s = math.sin(r)\n",
    "        xi = xs[i]; yi = ys[i]\n",
    "        for j in range(V):\n",
    "            X = c * tx[j] - s * ty[j] + xi\n",
    "            Y = s * tx[j] + c * ty[j] + yi\n",
    "            if X < mnx: mnx = X\n",
    "            if X > mxx: mxx = X\n",
    "            if Y < mny: mny = Y\n",
    "            if Y > mxy: mxy = Y\n",
    "    side = max(mxx - mnx, mxy - mny)\n",
    "    return side * side / n\n",
    "\n",
    "def strip(a):\n",
    "    return np.array([float(str(v).replace('s','')) for v in a], np.float64)\n",
    "\n",
    "tx, ty = make_polygon_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-N scores\n",
    "df['N'] = df['id'].astype(str).str.split('_').str[0].astype(int)\n",
    "\n",
    "scores = []\n",
    "for n, g in df.groupby('N'):\n",
    "    xs = strip(g['x'].to_numpy())\n",
    "    ys = strip(g['y'].to_numpy())\n",
    "    ds = strip(g['deg'].to_numpy())\n",
    "    sc = score_group(xs, ys, ds, tx, ty)\n",
    "    scores.append({'N': n, 'score': sc, 'side': np.sqrt(sc * n)})\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "print(f\"Total score: {scores_df['score'].sum():.6f}\")\n",
    "print(f\"\\nTop 10 worst-packed (highest score/N):\")\n",
    "print(scores_df.nlargest(10, 'score')[['N', 'score', 'side']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which N values have the most room for improvement\n",
    "# Theoretical minimum for N trees in a square is when they're perfectly packed\n",
    "# For this tree shape, the theoretical packing efficiency is unknown\n",
    "\n",
    "# Let's look at the efficiency metric: score / N vs N\n",
    "scores_df['efficiency'] = scores_df['score']  # Already normalized by N\n",
    "scores_df['side_per_tree'] = scores_df['side'] / np.sqrt(scores_df['N'])\n",
    "\n",
    "print(\"\\nEfficiency analysis (lower is better):\")\n",
    "print(f\"Best efficiency (lowest score): N={scores_df.loc[scores_df['score'].idxmin(), 'N']}, score={scores_df['score'].min():.6f}\")\n",
    "print(f\"Worst efficiency (highest score): N={scores_df.loc[scores_df['score'].idxmax(), 'N']}, score={scores_df['score'].max():.6f}\")\n",
    "\n",
    "# Group by N ranges\n",
    "scores_df['N_range'] = pd.cut(scores_df['N'], bins=[0, 10, 50, 100, 150, 200], labels=['1-10', '11-50', '51-100', '101-150', '151-200'])\n",
    "print(\"\\nScore contribution by N range:\")\n",
    "print(scores_df.groupby('N_range')['score'].agg(['sum', 'mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25168e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key insight from the super-fast SA kernel:\n",
    "# 1. Use 2-tree unit cells with translations\n",
    "# 2. Automatically explore all viable grid sizes\n",
    "# 3. Apply deletion cascade\n",
    "\n",
    "# Let's check what grid sizes could work for large N\n",
    "print(\"Grid configurations for large N (from egortrushin kernel):\")\n",
    "print(\"N=72: [4,9] -> 4*9*2 = 72\")\n",
    "print(\"N=100: [5,10] -> 5*10*2 = 100\")\n",
    "print(\"N=110: [5,11] -> 5*11*2 = 110\")\n",
    "print(\"N=144: [6,12] -> 6*12*2 = 144\")\n",
    "print(\"N=156: [6,13] -> 6*13*2 = 156\")\n",
    "print(\"N=196: [7,14] -> 7*14*2 = 196\")\n",
    "print(\"N=200: [7,15] -> 7*15*2 = 210, take first 200\")\n",
    "\n",
    "print(\"\\nThese are the N values where lattice approach is most effective.\")\n",
    "print(\"The super-fast SA kernel shows ~0.15 improvement in 2 minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the gap breakdown?\n",
    "target = 68.919154\n",
    "current = scores_df['score'].sum()\n",
    "gap = current - target\n",
    "\n",
    "print(f\"Current score: {current:.6f}\")\n",
    "print(f\"Target score: {target:.6f}\")\n",
    "print(f\"Gap: {gap:.6f} ({100*gap/current:.2f}%)\")\n",
    "\n",
    "# If we could improve large N by 5%, how much would that help?\n",
    "large_n_score = scores_df[scores_df['N'] > 100]['score'].sum()\n",
    "print(f\"\\nLarge N (>100) contribution: {large_n_score:.6f} ({100*large_n_score/current:.1f}%)\")\n",
    "print(f\"5% improvement on large N: {0.05 * large_n_score:.6f}\")\n",
    "print(f\"Would close {100 * 0.05 * large_n_score / gap:.1f}% of the gap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24575d60",
   "metadata": {},
   "source": [
    "## Strategy for Next Experiment\n",
    "\n",
    "### Key Insight from Research\n",
    "\n",
    "The **super-fast SA with translations** kernel (jiweiliu) shows a complete workflow that:\n",
    "1. Uses 2-tree unit cells with grid translations\n",
    "2. Automatically explores ALL viable grid sizes (not just hand-picked ones)\n",
    "3. Applies deletion cascade (backward propagation)\n",
    "4. Gets ~0.15 improvement in under 2 minutes!\n",
    "\n",
    "### The Problem with Our Current Approach\n",
    "\n",
    "We've been trying to optimize an ALREADY OPTIMIZED solution. The baseline is at a tight local optimum.\n",
    "\n",
    "### The Solution: Generate NEW Configurations from Scratch\n",
    "\n",
    "The lattice approach generates DIFFERENT configurations that may be in DIFFERENT basins:\n",
    "1. Start with 2 trees at (0,0) and (0.5, 0.5) with angles 0 and 180\n",
    "2. Optimize the 2-tree unit cell with SA\n",
    "3. Translate the unit cell in a grid pattern\n",
    "4. Run SA on the full configuration\n",
    "5. Apply deletion cascade to propagate improvements to smaller N\n",
    "\n",
    "### Next Experiment: Implement Super-Fast SA with Translations\n",
    "\n",
    "We should implement the jiweiliu kernel approach which:\n",
    "- Automatically explores all viable grid sizes\n",
    "- Uses Numba for acceleration\n",
    "- Applies deletion cascade\n",
    "- Gets ~0.15 improvement quickly\n",
    "\n",
    "This is fundamentally different from optimizing the current baseline!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
