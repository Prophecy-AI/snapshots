# Christmas Tree Packing Optimization - Evolved Seed Prompt (Loop 1)

## Current Status
- Best CV score: 135.819103 from exp_000 (001_baseline_cpp_optimizer)
- Best LB score: N/A (no submissions yet)
- Target: 68.931058 | Gap to target: 66.89 (97% worse)

## Public Kernel Status (CRITICAL!)
- Have we implemented the best kernel yet? **NO** - We only used a simplified C++ optimizer
- Top kernels identified:
  1. **74.75 BackPacking** (crodoc) - Backward iteration approach
  2. **Ensemble + SA + Fractional Translation** (jonathanchan) - Combines 15+ sources
  3. **bbox3 runner** (yongsukprasertsuk) - Multi-phase optimization with bbox3 binary
  4. **Santa Claude** (smartmanoj) - C++ optimizer with SA, local search
- Kernels we've implemented: Partial santa-claude (simplified version)
- Kernels still to implement: **BackPacking, Ensemble approach, Fractional translation**

## Ensemble Strategy (CRITICAL FOR TOP SCORES)
- Models available for ensemble: Only 1 (exp_000)
- Current ensemble score: N/A (single model)
- **PRIORITY**: Need to generate multiple diverse solutions to ensemble

## Response to Evaluator
The evaluator correctly identified several critical blind spots:
1. **bbox3 binary not used** - Agreed, but bbox3 is an external binary from Kaggle datasets. We need to work with what we have (C++ optimizer).
2. **Backward propagation not applied** - This is the KEY technique from the 74.75 kernel. MUST implement.
3. **Insufficient optimization time** - Agreed. Need multi-phase approach.
4. **No multi-run ensemble** - This is the MOST CRITICAL gap. Top solutions combine 15+ sources.
5. **Parameter exploration limited** - Agreed. Need to explore more parameter combinations.

The evaluator's top priority (multi-phase, multi-run optimization) is correct but incomplete. The REAL key is:
1. **Backward propagation** - Can improve smaller N using larger N solutions
2. **Ensemble from multiple runs** - Keep best configuration for each N across all runs
3. **Fractional translation** - Fine-grained position adjustments for final polish

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Implement Backward Propagation (BackPacking)
Based on the 74.75 kernel:
```python
# Start from N=200, work backwards
best_config = {}
best_side = {}

for n in range(200, 0, -1):
    current_trees = load_config(n)
    current_side = calculate_side(current_trees)
    
    # Check if any larger N config (with trees removed) is better
    for larger_n in range(n+1, 201):
        if larger_n in best_config:
            # Try using larger config with extra trees removed
            adapted_trees = best_config[larger_n][:n]  # Keep first n trees
            adapted_side = calculate_side(adapted_trees)
            if adapted_side < current_side:
                current_trees = adapted_trees
                current_side = adapted_side
    
    best_config[n] = current_trees
    best_side[n] = current_side
```

### 2. **[HIGH PRIORITY]** Multi-Run Ensemble
Run the C++ optimizer multiple times with different seeds and parameters:
- Seeds: 42, 123, 456, 789, 1000
- Parameters: -n {2000, 3000, 4000} × -r {32, 48, 64}
- For each N, keep the best configuration across ALL runs

### 3. **[HIGH PRIORITY]** Implement Fractional Translation
After SA optimization, apply very fine-grained position adjustments:
```python
frac_steps = [0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]
directions = [(0,1), (0,-1), (1,0), (-1,0), (1,1), (1,-1), (-1,1), (-1,-1)]

for step in frac_steps:
    for dx, dy in directions:
        # Try moving each tree by step in direction (dx, dy)
        # Keep if it reduces bounding box without overlap
```

### 4. **[MEDIUM PRIORITY]** Extended Optimization Time
- Phase A: Short runs (5 min each) with many parameter combinations
- Phase B: Medium runs (15 min each) on top 5 candidates
- Phase C: Long runs (30 min each) on best 2-3 candidates
- Total budget: 2-3 hours

### 5. **[MEDIUM PRIORITY]** Iterative Refinement Loop
```python
while True:
    old_score = current_score
    # Run optimizer
    # Apply fix_direction
    # Apply fractional_translation
    # Apply backward_propagation
    current_score = calculate_score()
    if current_score >= old_score - epsilon:
        break
```

## What NOT to Try
- Building from scratch when proven techniques exist
- Single-pass optimization (already shown to be insufficient)
- Ignoring backward propagation (key technique for reaching 74.75)
- Skipping ensemble (all top solutions use it)

## Validation Notes
- CV scheme: Calculate total score = Σ(side²/N) for N=1 to 200
- Validate no overlaps using Shapely STRtree
- Always apply fix_direction after optimization

## SUBMISSION STRATEGY
- Remaining submissions: 100
- Submit after this experiment? **YES** - We have abundant submissions and need LB feedback to calibrate
- Submit exp_000 to establish baseline LB score

## Next Experiment Plan
**Experiment 002: Backward Propagation + Multi-Run Ensemble**
1. Run C++ optimizer with 5 different seeds
2. For each N, keep best configuration across all runs
3. Apply backward propagation (BackPacking technique)
4. Apply fix_direction
5. Apply fractional translation
6. Expected improvement: 30-40% (from 135 to ~85-95)
