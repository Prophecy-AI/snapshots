# Christmas Tree Packing Optimization - Evolved Seed Prompt (Loop 2)

## Current Status
- Best CV score: 117.281454 from exp_001 (002_multiphase_optimization)
- Best LB score: 117.281454 (CV-LB gap: 0.0000 - expected for optimization problem)
- Target: 68.931058 | Gap to target: 48.35 points (70.1%)

## CRITICAL DISCOVERY: Snapshot Solutions Available!
**The best available snapshot solution scores 87.36** - this is 30 points better than our current 117.28!
- Location: `/home/nonroot/snapshots/santa-2025/21016257921/code/experiments/003_local_search/submission.csv`
- Ensemble of all snapshots: 85.65
- This means we've been optimizing from a MUCH WORSE starting point than necessary!

## Public Kernel Status
- **Best kernel technique**: Ensemble approach (combine 15+ sources, pick best per N)
- **Top kernels identified**: 
  - jonathanchan_santa25-ensemble-sa-fractional-translation (ensemble + SA + fractional translation)
  - yongsukprasertsuk_santa-2025-best-keeping-bbox3-runner (bbox3 + 3-hour budget)
  - crodoc_74-75-backpacking-christmas-trees (backward propagation)
- **Kernels we've implemented**: C++ optimizer, backward propagation, fix_direction
- **Missing**: Ensemble approach (CRITICAL), bbox3 binary (not available locally)

## Response to Evaluator
The evaluator correctly identified that we're missing the PRIMARY technique used by top kernels: the ENSEMBLE approach. Our analysis confirms this:
- Best snapshot: 87.36 (vs our 117.28 = 30 point gap!)
- Ensemble baseline: 85.65
- Target: 68.93

**The evaluator's recommendation to implement ensemble is 100% correct.** We must:
1. Use the best snapshot (87.36) as our starting point
2. Apply our optimization pipeline (C++ optimizer + backward propagation + fix_direction)
3. This should close the gap significantly

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Use Best Snapshot as Starting Point
```python
# Copy best snapshot to working directory
best_snapshot = '/home/nonroot/snapshots/santa-2025/21016257921/code/experiments/003_local_search/submission.csv'
# Score: 87.36 (vs our 117.28)
```
This alone gives us a 30-point improvement!

### 2. **[HIGH PRIORITY]** Apply Multi-Phase Optimization to Snapshot
Run our existing optimization pipeline on the 87.36 snapshot:
- C++ optimizer with higher iterations (-n 10000 -r 96)
- Backward propagation
- Fix direction rotation optimization
- Fractional translation

Expected: 87.36 → ~75-80 range

### 3. **[HIGH PRIORITY]** Build True Ensemble Baseline
For each N (1-200), pick the best configuration across ALL sources:
- All snapshot CSVs (81 files available)
- Our optimized candidates
- This creates the "best of the best" starting point

### 4. **[MEDIUM PRIORITY]** Longer Optimization Runs
Top kernels use 3-hour budgets. Our current runs are ~10 minutes.
- Increase iterations: -n 15000-20000
- Increase rounds: -r 96-128
- Multiple seeds: 5-10 seeds

### 5. **[MEDIUM PRIORITY]** Iterative Improvement Loop
```python
while time_remaining > 0:
    run_optimizer(current_best)
    apply_backward_propagation()
    apply_fix_direction()
    if new_score < best_score:
        best_score = new_score
        save_snapshot()
```

## What NOT to Try
- Building from scratch (sample_submission) - we have 87.36 snapshot!
- Short optimization runs (<5 minutes) - need longer runs
- Single-seed optimization - need multiple seeds
- Hyperparameter tuning before closing the 18-point gap to target

## Validation Notes
- CV scheme: Direct scoring (sum of side²/N for N=1 to 200)
- CV-LB gap: 0.0000 (expected for optimization problem)
- Overlap validation: Use STRtree for efficient collision detection

## Expected Outcomes
1. Using snapshot (87.36) + our optimizer → ~75-80
2. Ensemble baseline (85.65) + optimizer → ~72-78
3. With longer runs and multiple seeds → ~70-72
4. Target: 68.93 (achievable with aggressive optimization)

## SUBMISSION STRATEGY
- Remaining submissions: 98
- **Submit after EVERY experiment** - we have abundant submissions
- LB feedback is free information - USE IT!