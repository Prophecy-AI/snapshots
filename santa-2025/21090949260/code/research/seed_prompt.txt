# Christmas Tree Packing Optimization - Seed Prompt (Loop 6)

## Current Status
- Best CV score: 84.712432 from exp_004/exp_005
- Best LB score: 84.712432 (perfect CV=LB alignment)
- Target: 68.931058 | Gap to target: 15.78 points (18.6% reduction needed)

## Public Kernel Status (CRITICAL!)
- Have we implemented the best kernel yet? **PARTIALLY** - We have ensemble + SA but NOT the advanced techniques
- Top kernels identified:
  1. seshurajup/71-78-jit-parallel-sa-c-tpu-96-cores (71.78 score) - Parallel SA + fractional translation
  2. crodoc/74-75-backpacking-christmas-trees (74.75 score) - Better starting datasets
  3. blueshyy/santa-2025-ensemble-sa-greedy-backtracking - Greedy backtracking with beam search
- Kernels we've implemented: Basic ensemble, C++ bbox3 optimizer, fix_direction, backward propagation
- Kernels still to implement: **Greedy backtracking, fractional translation, per-tree rotation**

## CV-LB Relationship Analysis
- Perfect CV=LB alignment (correlation = 1.0) - expected for deterministic optimization
- No distribution shift concerns - this is a pure optimization problem
- All improvements in CV translate directly to LB improvements

## Response to Evaluator
The evaluator correctly identified that:
1. Grid initialization found ZERO improvements - our ensemble already has optimal grid-based solutions
2. We've exhausted the "combine existing solutions" approach
3. The target (68.93) is BETTER than LB #1 (71.19) - requires innovation

**I AGREE** with the evaluator's assessment. The key insight is that:
- Our ensemble has absorbed all the best publicly available configurations
- Further improvement requires IMPROVING configurations, not just selecting better ones
- The techniques that haven't been tried (per-tree rotation, greedy backtracking, fractional translation) are the path forward

**CRITICAL OBSERVATION**: The target (68.93) is BELOW the theoretical minimum (~70.0) based on tree area. This means the target requires BETTER than perfect packing efficiency. This is mathematically challenging but the top competitors have achieved it (71.19 is close). The key is likely:
1. Better optimization algorithms (not just more iterations)
2. Access to better starting solutions (datasets we don't have)
3. Novel techniques like per-tree rotation optimization

## Recommended Approaches (Priority Order)

### 1. **[HIGHEST PRIORITY]** Implement Greedy Backtracking (blueshyy approach)
The blueshyy kernel uses a beam search approach to intelligently remove trees:
- For each N, try removing different trees and see which removal minimizes the bounding box
- Use beam search with depth and lookahead to find optimal removal sequences
- This can propagate good configurations from larger N to smaller N

**Implementation:**
```python
def choose_removal_beam_lookahead(bounds_list, depth=3, beam=5, max_states=1000):
    """Find best tree to remove using beam search."""
    # Start with candidates touching the boundary
    candidates = compute_touching_candidates(bounds_list)
    
    # First layer: try removing each candidate
    first_layer = []
    for idx in candidates:
        reduced = bounds_list[:idx] + bounds_list[idx+1:]
        score = get_bounds_side(reduced)
        first_layer.append((score, reduced, idx))
    
    # Keep top beam candidates and expand
    first_layer.sort(key=lambda x: x[0])
    frontier = first_layer[:beam]
    
    # Continue to depth
    for d in range(2, depth+1):
        new_frontier = []
        for score, bds, first_idx in frontier:
            cands = compute_touching_candidates(bds)
            for j in cands:
                nb = bds[:j] + bds[j+1:]
                s = get_bounds_side(nb)
                new_frontier.append((s, nb, first_idx))
        new_frontier.sort(key=lambda x: x[0])
        frontier = new_frontier[:beam]
    
    return frontier[0][2]  # Return best first removal index
```

### 2. **[HIGH PRIORITY]** Implement Per-Tree Rotation Optimization
Current fix_direction rotates ALL trees together. Per-tree optimization:
```python
def optimize_tree_rotations(trees, n):
    """Optimize rotation of each tree individually."""
    for i in range(n):
        best_angle = trees[i].angle
        best_score = calculate_score(trees, n)
        
        for angle in np.linspace(0, 360, 72):  # 5-degree increments
            trees[i].angle = angle
            if not has_overlap(trees):
                score = calculate_score(trees, n)
                if score < best_score:
                    best_score = score
                    best_angle = angle
        
        trees[i].angle = best_angle
    return trees
```

### 3. **[HIGH PRIORITY]** Implement Fractional Translation Polish
Very fine-grained position adjustments after SA:
```python
steps = [0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]
directions = [(1,0), (-1,0), (0,1), (0,-1), (1,1), (-1,1), (1,-1), (-1,-1)]

def fractional_translation(trees, iterations=150):
    for _ in range(iterations):
        for i, tree in enumerate(trees):
            for step in steps:
                for dx, dy in directions:
                    new_x = tree.x + dx * step
                    new_y = tree.y + dy * step
                    # Try move, keep if improves score without overlap
```

### 4. **[MEDIUM PRIORITY]** Optimize N=1 Analytically
N=1 has worst efficiency (52.93%). For a single tree, we can analytically compute the optimal rotation:
- The tree has specific geometry (15 vertices)
- The optimal rotation minimizes the bounding box
- This should be a quick win: reduce score from 0.661 to ~0.35

### 5. **[MEDIUM PRIORITY]** Compile and Run Parallel SA C++ Code
The seshurajup kernel has a complete C++ implementation with:
- OpenMP parallelization
- Fractional translation
- Multiple generations
- This could be compiled and run on our current solutions

## What NOT to Try
- Grid initialization (already proven no improvement)
- More iterations of basic SA (diminishing returns - 57 min yielded only 0.19 points)
- Backward propagation (no opportunities - monotonic side lengths)
- Building from scratch when better techniques exist in kernels

## Validation Notes
- CV = LB perfectly (deterministic optimization)
- Always validate no overlaps before submission
- Use fast numpy-based scoring for quick iteration

## SUBMISSION STRATEGY
- Remaining submissions: 95
- **Submit after EVERY experiment** - we have abundant submissions
- LB feedback is free information - use it to calibrate progress

## Expected Outcomes
- Greedy backtracking: Could yield 1-3 points improvement
- Per-tree rotation: Could yield 0.5-1 points improvement
- Fractional translation: Could yield 0.1-0.5 points improvement
- Combined: Target 81-82 range (still 12-13 points from target)

## CRITICAL NOTE
The target (68.93) is BELOW theoretical minimum. This means:
1. The target may be based on solutions we don't have access to
2. We need to discover novel optimization techniques
3. The path forward is through ALGORITHMIC INNOVATION, not just more compute

The top competitors have achieved 71.19 - we need to understand HOW they did it. The techniques above are our best guesses based on public kernels, but the winning solution likely involves something we haven't discovered yet.
