{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892e02e2",
   "metadata": {},
   "source": [
    "# Baseline Model - ResNet-18\n",
    "\n",
    "This notebook implements a baseline model for the Aerial Cactus Identification competition.\n",
    "\n",
    "**Strategy:**\n",
    "- ResNet-18 architecture (well-suited for 32x32 images)\n",
    "- Stratified 5-fold cross-validation\n",
    "- Standard cross-entropy loss (baseline before trying AUC-specific losses)\n",
    "- Heavy data augmentation for small images\n",
    "- Class imbalance handling via loss weighting\n",
    "- Test-time augmentation for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257a4cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:55:47.530105Z",
     "iopub.status.busy": "2026-01-10T06:55:47.529193Z",
     "iopub.status.idle": "2026-01-10T06:55:58.165088Z",
     "shell.execute_reply": "2026-01-10T06:55:58.164395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763a839",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74db99c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:55:58.167683Z",
     "iopub.status.busy": "2026-01-10T06:55:58.166993Z",
     "iopub.status.idle": "2026-01-10T06:55:58.445396Z",
     "shell.execute_reply": "2026-01-10T06:55:58.444738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 14175\n",
      "Test samples: 3325\n",
      "\n",
      "Class distribution:\n",
      "has_cactus\n",
      "1    0.749771\n",
      "0    0.250229\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image shape: (32, 32, 3)\n",
      "Image dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df['has_cactus'].value_counts(normalize=True))\n",
    "\n",
    "# Check image dimensions\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/data/train.zip', 'r') as zip_ref:\n",
    "    first_file = zip_ref.namelist()[1]  # Skip directory\n",
    "    with zip_ref.open(first_file) as f:\n",
    "        img_data = f.read()\n",
    "        nparr = np.frombuffer(img_data, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        print(f\"\\nImage shape: {img.shape}\")\n",
    "        print(f\"Image dtype: {img.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae9fb1",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a92778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:56:10.220559Z",
     "iopub.status.busy": "2026-01-10T06:56:10.219789Z",
     "iopub.status.idle": "2026-01-10T06:56:10.227619Z",
     "shell.execute_reply": "2026-01-10T06:56:10.226833Z"
    }
   },
   "outputs": [],
   "source": [
    "class CactusDataset(Dataset):\n",
    "    def __init__(self, df, zip_path, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.zip_path = zip_path\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.zip_file = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.zip_file is None:\n",
    "            self.zip_file = zipfile.ZipFile(self.zip_path, 'r')\n",
    "            \n",
    "        img_id = self.df.iloc[idx]['id']\n",
    "        \n",
    "        # Images are in root of zip, not in subdirectories\n",
    "        img_path = img_id\n",
    "            \n",
    "        with self.zip_file.open(img_path) as f:\n",
    "            img_data = f.read()\n",
    "            \n",
    "        nparr = np.frombuffer(img_data, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.is_test:\n",
    "            return img\n",
    "        else:\n",
    "            label = self.df.iloc[idx]['has_cactus']\n",
    "            return img, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "    def __del__(self):\n",
    "        if self.zip_file is not None:\n",
    "            self.zip_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1e293",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bac336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:56:10.229539Z",
     "iopub.status.busy": "2026-01-10T06:56:10.229262Z",
     "iopub.status.idle": "2026-01-10T06:56:10.244359Z",
     "shell.execute_reply": "2026-01-10T06:56:10.243502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training transforms with heavy augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transforms (minimal)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ca771",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa44624a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:56:10.247300Z",
     "iopub.status.busy": "2026-01-10T06:56:10.246730Z",
     "iopub.status.idle": "2026-01-10T06:56:10.260641Z",
     "shell.execute_reply": "2026-01-10T06:56:10.259862Z"
    }
   },
   "outputs": [],
   "source": [
    "class CactusClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(CactusClassifier, self).__init__()\n",
    "        # Use ResNet-18 as backbone\n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first layer for 32x32 images (smaller kernel, stride)\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.backbone.maxpool = nn.Identity()  # Remove maxpool for small images\n",
    "        \n",
    "        # Get number of features from backbone\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace final layer\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=10):\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} - Train'):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            train_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} - Val'):\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train AUC: {train_auc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss/len(val_loader):.4f}, Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f'  New best validation AUC: {val_auc:.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_model_state, best_val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24585b55",
   "metadata": {},
   "source": [
    "## Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stratified k-fold\n",
    "def run_cross_validation(n_folds=5, epochs=15, batch_size=64):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    # Calculate class weights for handling imbalance\n",
    "    pos_class = (train_df['has_cactus'] == 1).sum()\n",
    "    neg_class = (train_df['has_cactus'] == 0).sum()\n",
    "    pos_weight = neg_class / pos_class\n",
    "    print(f\"Positive class weight: {pos_weight:.4f}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['has_cactus'])):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create fold datasets\n",
    "        train_fold = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_fold = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        train_dataset = CactusDataset(train_fold, '/home/data/train.zip', transform=train_transform)\n",
    "        val_dataset = CactusDataset(val_fold, '/home/data/train.zip', transform=val_transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = CactusClassifier(pretrained=True).to(device)\n",
    "        \n",
    "        # Loss with class weighting for imbalance\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        # Train model\n",
    "        best_state, best_auc = train_model(model, train_loader, val_loader, criterion, \n",
    "                                         optimizer, scheduler, device, epochs=epochs)\n",
    "        \n",
    "        fold_scores.append(best_auc)\n",
    "        fold_models.append(best_state)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Best Validation AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cross-Validation Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Mean AUC: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "    print(f\"Individual folds: {fold_scores}\")\n",
    "    \n",
    "    return fold_models, fold_scores\n",
    "\n",
    "# Run cross-validation\n",
    "fold_models, fold_scores = run_cross_validation(n_folds=5, epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518308c0",
   "metadata": {},
   "source": [
    "## Test-Time Augmentation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(models, test_loader, device, n_tta=5):\n",
    "    \"\"\"Predict with test-time augmentation\"\"\"\n",
    "    all_preds = []\n",
    "    \n",
    "    # Define TTA transforms\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.ToPILImage(), transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.ToPILImage(), transforms.RandomVerticalFlip(p=1.0), transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation(90), transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation(180), transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    ]\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        \n",
    "        for tta_idx in range(min(n_tta, len(tta_transforms))):\n",
    "            tta_pred = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(test_loader, desc=f'Model - TTA {tta_idx+1}'):\n",
    "                    images = batch.to(device)\n",
    "                    outputs = model(images).squeeze()\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    tta_pred.extend(probs)\n",
    "            \n",
    "            fold_preds.append(tta_pred)\n",
    "        \n",
    "        # Average TTA predictions for this model\n",
    "        fold_preds = np.mean(fold_preds, axis=0)\n",
    "        all_preds.append(fold_preds)\n",
    "    \n",
    "    # Average across all models (ensemble)\n",
    "    final_preds = np.mean(all_preds, axis=0)\n",
    "    return final_preds\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = CactusDataset(test_df, '/home/data/test.zip', transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load best models and predict\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_models = []\n",
    "\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    model = CactusClassifier(pretrained=False).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    best_models.append(model)\n",
    "    print(f\"Loaded model from fold {i+1}\")\n",
    "\n",
    "# Generate predictions with TTA\n",
    "print(\"\\nGenerating predictions with test-time augmentation...\")\n",
    "test_predictions = predict_with_tta(best_models, test_loader, device, n_tta=5)\n",
    "\n",
    "# Create submission\n",
    "submission_df = test_df.copy()\n",
    "submission_df['has_cactus'] = test_predictions\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
