{
  "query": "What test-time augmentation techniques work best for image classification on Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "As of\u202fFebruary\u202f10\u202f2024, the most effective test\u2011time augmentation techniques for Kaggle image\u2011classification contests are deterministic transforms\u2014horizontal/vertical flips, rotations, multi\u2011scale crops (often with modest color jitter)\u2014applied as a fixed set and averaged across predictions rather than using random augmentations ([arXiv](https://arxiv.org/html/2402.06892v1)\u202fFeb\u202f2024; [stepup.ai](https://stepup.ai/test_time_data_augmentation)\u202f2020).",
      "url": ""
    },
    {
      "title": "How to Correctly Use Test-Time Data Augmentation to Improve ...",
      "text": "[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/dufourpascal/stepupai/blob/master/tutorials/data_augmentation/test_time_data_augmentation.ipynb?ref=stepup.ai)\n\n[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/dufourpascal/stepupai/blob/master/tutorials/data_augmentation/test_time_data_augmentation.ipynb?ref=stepup.ai)\n\n[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://drive.google.com/file/d/1psZRB3HiK2HOGwvZD6nhXhzsq2YV0OA1?usp=sharing&ref=stepup.ai)\n\nTest-Time Data Augmentation (short TTA) is a technique that can boost a model's performance by applying augmentation during inference and is a popular strategy to use with deep learning models.\n\nInference is performed on multiple altered versions of the same image, and\n\nthe predictions are then aggregated to get a higher overall accuracy.\n\nUnlike [train-time data augmentation](https://stepup.ai/exploring_data_augmentation_keras), we don't need to make any changes to the model, therefore it can be applied to an already trained model!\n\nToo often have I seen it used in the wrong way, using _random_ data augmentation instead of a set of predefined _constant_ augmentation. The result is a worse performing inference pipeline and unneccessary computations, resulting in a high computational load for inference.\n\nIn this post you will learn how to apply test-time augmentation correctly in a production setting and how to find the parameters that work for _your_ model. Read on to learn how to get the most benefit out of your limited inference budget!\n\n## The Basics of Test-Time Augmentation\n\nWithout test-time data augmentation, inference is very simple: The input image is simply run through the network and we collect the results. Done.\n\nDepending on the type of model, we might have to transform the output of the model to the final format. E.g. in the case of classification, we would usually perform an `argmax`. The following is an example:\n\n![](https://stepup.ai/content/images/2020/08/sample01.png)Input: Original image\n\nLet's say we have a classifier for CIFAR-10 images that has 10 output classes:\n\n`[airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]`\n\nWhen we run the above image through a classifier, we get an output like the following:\n\n`[[0.08 0.00 0.54 0.01 0.37 0.00 0.00 0.00 0.00 0.00]]`\n\nIn this case, the image would be wrongly classified as a _bird_ and not as a _deer_.\n\nTo improve this result, we can try to run different versions of the image through the classifier and see what comes out. So let's run the original image and a horizontally flipped image through the network:\n\n![](https://stepup.ai/content/images/2020/08/sample01_flip.png)Input: Original and flipped image\n\nThe output of the network will now be two vectors:\n\n```\n[[0.08 0.00 0.54 0.01 0.37 0.00 0.00 0.00 0.00 0.00]\n [0.43 0.00 0.11 0.00 0.39 0.01 0.00 0.01 0.04 0.00]]\n\n```\n\nThe first row is for the original image (same as above) and the second row is for the flipped image. You can see that the model gives the flipped image the highest probability of being an airplane.\n\nWhen we average the two vectors, we get the vector below:\n\n`[[0.25 0.00 0.33 0.01 0.38 0.01 0.00 0.01 0.02 0.00]]`\n\nGreat! Now the highest probability is given to the class _deer_.\n\nNow, this image is _completely cherry picked_ and should just serve as an example. In reality, we cannot expect that test-time augmentation solves all our problems. But we _can_ expect that _on average_ we get an improved performance when we use multiple augmented samples instead of just a single input sample!\n\nBelow is the overall workflow to use with TTA: An input image is augmented, then the augmented **and** the original image are forwarded through the model, and finally the predictions are aggregated into a final result.\n\n![](https://stepup.ai/content/images/2020/08/test_time_augmentation_concept.png)Test-time data augmentation workflow: Altered version of an original image are all processed with the model. Then all predictions are aggregated into a single prediction that on average is more accurate than just the prediction from the original image.\n\n## Setup\n\nFor this tutorial, we will use transfer learning to very quickly train a simple CIFAR-10 deep learning classifier using tensorflow and Keras. We will then use this classifier to experiment with test-time data augmentation.\n\nLet's start with the inputs. We need just the basics: numpy, tensorflow, Keras, and some utility functions\n\n```\nimport numpy as np\nimport tensorflow as tf\nimport scipy as sp\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n```\n\nWe will also define a function to quickly visualize a dataset:\n\n```\ndef visualize_data(images, categories=None, class_names=None):\n    fig = plt.figure(figsize=(14, 6))\n    fig.patch.set_facecolor('white')\n    for i in range(min(3 * 7, len(images))):\n        plt.subplot(3, 7, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(images[i])\n        if class_names and categories is not None:\n            class_index = categories[i].argmax()\n            plt.xlabel(class_names[class_index])\n    plt.show()\n```\n\nNow it's time to load the CIFAR-10 dataset and visualize the first few images in the test set.\n\n```\nclass_names = ['airplane', 'automobile', 'bird', 'cat',\n               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nnum_classes = len(class_names)\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\nvisualize_data(x_test, y_test, class_names)\n```\n\n![](https://stepup.ai/content/images/2020/08/x_test01.jpg)\n\n### Classifier Training\n\nTo speed things up, we will not train a classifier from scratch, but use an existing model trained on _imagenet_ as a feature extractor, and just train a dense layer with those features. The purpose of this tutorial is not to beat the state-of-the-art on CIFAR-10, but rather to have a way of quickly experimenting with test-time augmentation!\n\n```\ndef create_model():\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        pooling='avg',\n        alpha=0.35,\n        input_shape=(96,96,3),\n        )\n    base_model.trainable = False\n\n    inputs = layers.Input(shape=(32, 32, 3), dtype= tf.uint8)\n    x = tf.cast(inputs, tf.float32)\n    x = preprocess_input(x)\n    x = layers.UpSampling2D(size=(3,3), interpolation='nearest')(x)\n\n    x = base_model(x)\n\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam',\n                  loss='CategoricalCrossentropy',\n                  metrics=['accuracy']\n                  )\n    return model\n```\n\nAs we want a _fast_ model, we use the smallest version of [MobileNetV2](https://arxiv.org/abs/1801.04381?ref=stepup.ai) as our feature extractor (the base model). We will not train this base model, and therefore set `base_model.trainable = False`.\n\nOn top of the feature extractor we put a batch normalization layer, a small hidden dense layer, and finally the output layer for the 10 classes.\n\nAs you can see, the model first _upsamples_ the image from `32x32` to `96x96` pixels. This is because the MobileNetV2 model was trained on imagenet with an input size of `224x224`. As a result, the features it computes are somewhat sensitive to the input size and we cannot expect good results if we j...",
      "url": "https://stepup.ai/test_time_data_augmentation"
    },
    {
      "title": "Understanding Test-Time Augmentation",
      "text": "Understanding Test-Time Augmentation\nLicense: CC BY 4.0\narXiv:2402.06892v1 [cs.LG] 10 Feb 2024\n11institutetext:Ridge-i Inc., Tokyo, Japan\n11email:mkimura@ridge-i.com\n# Understanding Test-Time Augmentation\nMasanari Kimura11[0000-0002-9953-3469](https://orcid.org/0000-0002-9953-3469)\n###### Abstract\nTest-Time Augmentation (TTA) is a very powerful heuristic that takes advantage of data augmentation during testing to produce averaged output.\nDespite the experimental effectiveness of TTA, there is insufficient discussion of its theoretical aspects.\nIn this paper, we aim to give theoretical guarantees for TTA and clarify its behavior.\n###### Keywords:\ndata augmentation, ensemble learning, machine learning\n## 1Introduction\nThe effectiveness of machine learning has been reported for a great variety of tasks> [\n[> 22\n](https://arxiv.org/html/2402.06892v1#bib.bib22)> , [> 15\n](https://arxiv.org/html/2402.06892v1#bib.bib15)> , [> 14\n](https://arxiv.org/html/2402.06892v1#bib.bib14)> , [> 3\n](https://arxiv.org/html/2402.06892v1#bib.bib3)> , [> 11\n](https://arxiv.org/html/2402.06892v1#bib.bib11)> ]\n.\nHowever, satisfactory performance during testing is often not achieved due to the lack of training data or the complexity of the model.\nOne important concept to tackle such problems is data augmentation.\nThe basic idea of data augmentation is to increase the training data by transforming the input data in some way to generate new data that resembles the original instance.\nMany data augmentations have been proposed> [\n[> 29\n](https://arxiv.org/html/2402.06892v1#bib.bib29)> , [> 37\n](https://arxiv.org/html/2402.06892v1#bib.bib37)> , [> 25\n](https://arxiv.org/html/2402.06892v1#bib.bib25)> , [> 13\n](https://arxiv.org/html/2402.06892v1#bib.bib13)> ]\n, ranging from simple ones, such as flipping input images> [\n[> 26\n](https://arxiv.org/html/2402.06892v1#bib.bib26)> , [> 20\n](https://arxiv.org/html/2402.06892v1#bib.bib20)> ]\n, to more complex ones, such as leveraging Generative Adversarial Networks (GANs) to automatically generate data> [\n[> 8\n](https://arxiv.org/html/2402.06892v1#bib.bib8)> , [> 7\n](https://arxiv.org/html/2402.06892v1#bib.bib7)> ]\n.\nIn addition, there are several studies on automatic data augmentation in the framework of AutoML> [\n[> 18\n](https://arxiv.org/html/2402.06892v1#bib.bib18)> , [> 9\n](https://arxiv.org/html/2402.06892v1#bib.bib9)> ]\n.\nAnother approach to improve the performance of machine learning models is ensemble learning> [\n[> 4\n](https://arxiv.org/html/2402.06892v1#bib.bib4)> , [> 27\n](https://arxiv.org/html/2402.06892v1#bib.bib27)> ]\n.\nEnsemble learning generates multiple models from a single training dataset and combines their outputs, hoping to outperform a single model.\nThe effectiveness of ensemble learning has also been reported in a number of domains> [\n[> 5\n](https://arxiv.org/html/2402.06892v1#bib.bib5)> , [> 6\n](https://arxiv.org/html/2402.06892v1#bib.bib6)> , [> 17\n](https://arxiv.org/html/2402.06892v1#bib.bib17)> ]\n.\nInfluenced by these approaches, a new paradigm called Test-Time Augmentation (TTA)> [\n[> 34\n](https://arxiv.org/html/2402.06892v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2402.06892v1#bib.bib35)> , [> 23\n](https://arxiv.org/html/2402.06892v1#bib.bib23)> ]\nhas been gaining attention in recent years.\nTTA is a very powerful heuristic that takes advantage of data augmentation during testing to produce averaged output.\nDespite the experimental effectiveness of TTA, there is insufficient discussion of its theoretical aspects.\nIn this paper, we aim to give theoretical guarantees for TTA and clarify its behavior.\nOur contributions are summarized as follows:\n* \u2022We prove that the expected error of the TTA is less than or equal to the average error of an original model. Furthermore, under some assumptions, the expected error of the TTA is strictly less than the average error of an original model;\n* \u2022We introduce the generalized version of the TTA, and the optimal weights of it are given by the closed-form;\n* \u2022We prove that the error of the TTA depends on the ambiguity term.\n## 2Preliminaries\nHere, we first introduce the notations and problem formulation.\n### 2.1Problem formulation\nLet\ud835\udcb3\u2208\u211dd\ud835\udcb3superscript\u211d\ud835\udc51\\\\mathcal{X}\\\\in\\\\mathbb{R}^{d}caligraphic\\_X \u2208blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d end\\_POSTSUPERSCRIPTbe thed\ud835\udc51ditalic\\_d-dimensional input space,\ud835\udcb4\u2208\u211d\ud835\udcb4\u211d\\\\mathcal{Y}\\\\in\\\\mathbb{R}caligraphic\\_Y \u2208blackboard\\_Rbe the output space, and\u210b={h\u2062(\ud835\udc99;\ud835\udf3d):\ud835\udcb3\u2192\ud835\udcb4|\ud835\udf3d\u2208\u0398}\u210bconditional-set\u210e\ud835\udc99\ud835\udf3d\u2192\ud835\udcb3conditional\ud835\udcb4\ud835\udf3d\u0398\\\\mathcal{H}=\\\\{h(\\\\bm{x};\\\\bm{\\\\theta}):\\\\mathcal{X}\\\\to\\\\mathcal{Y}\\\\ |\\\\ \\\\bm{\\\\theta}%\n\\\\in\\\\Theta\\\\}caligraphic\\_H = { italic\\_h ( bold\\_italic\\_x ; bold\\_italic\\_\u03b8 ) : caligraphic\\_X \u2192caligraphic\\_Y | bold\\_italic\\_\u03b8 \u2208roman\\_\u0398 }be a hypothesis class, where\u0398\u2282\u211dp\u0398superscript\u211d\ud835\udc5d\\\\Theta\\\\subset\\\\mathbb{R}^{p}roman\\_\u0398 \u2282blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_p end\\_POSTSUPERSCRIPTis thep\ud835\udc5dpitalic\\_p-dimensional parameter space.\nIn supervised learning, our goal is to obtainh\\*\u2208\u210b:\ud835\udcb3\u2192\ud835\udcb4:superscript\u210e\u210b\u2192\ud835\udcb3\ud835\udcb4h^{\\*}\\\\in\\\\mathcal{H}:\\\\mathcal{X}\\\\to\\\\mathcal{Y}italic\\_h start\\_POSTSUPERSCRIPT \\* end\\_POSTSUPERSCRIPT \u2208caligraphic\\_H : caligraphic\\_X \u2192caligraphic\\_Ysuch that\n|h\\*=arg\u2062minh\u2208\u210b\u211b\u2113\u2062(h)=arg\u2062minh\u2208\u210b\ud835\udd3c\u2062[\u2113\u2062(y,h\u2062(\ud835\udc99;\ud835\udf3d))],superscript\u210esubscriptargmin\u210e\u210bsuperscript\u211b\u2113\u210esubscriptargmin\u210e\u210b\ud835\udd3cdelimited-[]\u2113\ud835\udc66\u210e\ud835\udc99\ud835\udf3dh^{\\*}=\\\\mathop{\\\\rm arg\\~{}min}\\\\limits\\_{h\\\\in\\\\mathcal{H}}\\\\mathcal{R}^{\\\\ell}(h)=%\n\\\\mathop{\\\\rm arg\\~{}min}\\\\limits\\_{h\\\\in\\\\mathcal{H}}\\\\mathbb{E}\\\\Big{[}\\\\ell(y,h(\\\\bm{x%\n};\\\\bm{\\\\theta}))\\\\Big{]},italic\\_h start\\_POSTSUPERSCRIPT \\* end\\_POSTSUPERSCRIPT = start\\_BIGOP roman\\_arg roman\\_min end\\_BIGOP start\\_POSTSUBSCRIPT italic\\_h \u2208caligraphic\\_H end\\_POSTSUBSCRIPT caligraphic\\_R start\\_POSTSUPERSCRIPT roman\\_\u2113 end\\_POSTSUPERSCRIPT ( italic\\_h ) = start\\_BIGOP roman\\_arg roman\\_min end\\_BIGOP start\\_POSTSUBSCRIPT italic\\_h \u2208caligraphic\\_H end\\_POSTSUBSCRIPT blackboard\\_E [ roman\\_\u2113 ( italic\\_y , italic\\_h ( bold\\_italic\\_x ; bold\\_italic\\_\u03b8 ) ) ] ,||(1)|\nwhere\n|\u211b\u2113\u2062(h)\u2254\ud835\udd3c\u2062[\u2113\u2062(y,h\u2062(\ud835\udc99;\ud835\udf3d))]\u2254superscript\u211b\u2113\u210e\ud835\udd3cdelimited-[]\u2113\ud835\udc66\u210e\ud835\udc99\ud835\udf3d\\\\mathcal{R}^{\\\\ell}(h)\\\\coloneqq\\\\mathbb{E}\\\\Big{[}\\\\ell(y,h(\\\\bm{x};\\\\bm{\\\\theta}))%\n\\\\Big{]}caligraphic\\_R start\\_POSTSUPERSCRIPT roman\\_\u2113 end\\_POSTSUPERSCRIPT ( italic\\_h ) \u2254blackboard\\_E [ roman\\_\u2113 ( italic\\_y , italic\\_h ( bold\\_italic\\_x ; bold\\_italic\\_\u03b8 ) ) ]||(2)|\nis the expected error and\u2113:\ud835\udcb4\u00d7\ud835\udcb4\u2192\u211d+:\u2113\u2192\ud835\udcb4\ud835\udcb4subscript\u211d\\\\ell:\\\\mathcal{Y}\\\\times\\\\mathcal{Y}\\\\to\\\\mathbb{R}\\_{+}roman\\_\u2113 : caligraphic\\_Y \u00d7caligraphic\\_Y \u2192blackboard\\_R start\\_POSTSUBSCRIPT + end\\_POSTSUBSCRIPTis some loss function.\nSince we can not access\u211b\u2113\u2062(h)superscript\u211b\u2113\u210e\\\\mathcal{R}^{\\\\ell}(h)caligraphic\\_R start\\_POSTSUPERSCRIPT roman\\_\u2113 end\\_POSTSUPERSCRIPT ( italic\\_h )directly, we try to approximate\u211b\u2113\u2062(h)superscript\u211b\u2113\u210e\\\\mathcal{R}^{\\\\ell}(h)caligraphic\\_R start\\_POSTSUPERSCRIPT roman\\_\u2113 end\\_POSTSUPERSCRIPT ( italic\\_h )from the limited sampleS={(yi,\ud835\udc99i)}i=1N\ud835\udc46subscriptsuperscriptsubscript\ud835\udc66\ud835\udc56subscript\ud835\udc99\ud835\udc56\ud835\udc41\ud835\udc561S=\\\\{(y\\_{i},\\\\bm{x}\\_{i})\\\\}^{N}\\_{i=1}italic\\_S = { ( italic\\_y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ) } start\\_POSTSUPERSCRIPT italic\\_N end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPTof sizeN\u2208\u2115\ud835\udc41\u2115N\\\\in\\\\mathbb{N}italic\\_N \u2208blackboard\\_N.\nIt is the ordinal empirical risk minimization (ERM) problem, and the minimizer of the empirical error\u211b^S\u2113\u22541N\u2062\u2211i=1N\u2113\u2062(yi,h\u2062(\ud835\udc99i))\u2254subscriptsuperscript^\u211b\u2113\ud835\udc461\ud835\udc41subscriptsuperscript\ud835\udc41\ud835\udc561\u2113subscript\ud835\udc66\ud835\udc56\u210esubscript\ud835\udc99\ud835\udc56\\\\hat{\\\\mathcal{R}}^{\\\\ell}\\_{S}\\\\coloneqq\\\\frac{1}{N}\\\\sum^{N}\\_{i=1}\\\\ell(y\\_{i},h(\\\\bm%\n{x}\\_{i}))over^ start\\_ARG caligraphic\\_R end\\_ARG start\\_POSTSUPERSCRIPT roman\\_\u2113 end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT \u2254divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_N end\\_ARG \u2211start\\_POSTSUPERSCRIPT italic\\_N end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT roman\\_\u2113 ( italic\\_y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , italic\\_h ( bold\\_italic\\_x start\\_POSTSU...",
      "url": "https://arxiv.org/html/2402.06892v1"
    },
    {
      "title": "How to Use Test-Time Augmentation to Make Better Predictions",
      "text": "### [Navigation](https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onApril 3, 2020in[Deep Learning for Computer Vision](https://machinelearningmastery.com/category/deep-learning-for-computer-vision/)[12](https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/#comments)\n\nShare _Tweet_Share\n\n[Data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) is a technique often used to improve performance and reduce generalization error when training neural network models for computer vision problems.\n\nThe image data augmentation technique can also be applied when making predictions with a fit model in order to allow the model to make predictions for multiple different versions of each image in the test dataset. The predictions on the augmented images can be averaged, which can result in better predictive performance.\n\nIn this tutorial, you will discover test-time augmentation for improving the performance of models for image classification tasks.\n\nAfter completing this tutorial, you will know:\n\n- Test-time augmentation is the application of data augmentation techniques normally used during training when making predictions.\n- How to implement test-time augmentation from scratch in Keras.\n- How to use test-time augmentation to improve the performance of a convolutional neural network model on a standard image classification task.\n\n**Kick-start your project** with my new book [Deep Learning for Computer Vision](https://machinelearningmastery.com/deep-learning-for-computer-vision/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n![How to Use Test-Time Augmentation to Improve Model Performance for Image Classification](https://machinelearningmastery.com/wp-content/uploads/2019/04/How-to-Use-Test-Time-Augmentation-to-Improve-Model-Performance-for-Image-Classification.jpg)\n\nHow to Use Test-Time Augmentation to Improve Model Performance for Image Classification\n\nPhoto by [daveynin](https://www.flickr.com/photos/daveynin/7206430966/), some rights reserved.\n\n## Tutorial Overview\n\nThis tutorial is divided into five parts; they are:\n\n1. Test-Time Augmentation\n2. Test-Time Augmentation in Keras\n3. Dataset and Baseline Model\n4. Example of Test-Time Augmentation\n5. How to Tune Test-Time Augmentation Configuration\n\n## Test-Time Augmentation\n\nData augmentation is an approach typically used during the training of the model that expands the training set with modified copies of samples from the training dataset.\n\nData augmentation is often performed with image data, where copies of images in the training dataset are created with some image manipulation techniques performed, such as zooms, flips, shifts, and more.\n\nThe artificially expanded training dataset can result in a more skillful model, as often the performance of deep learning models continues to scale in concert with the size of the training dataset. In addition, the modified or augmented versions of the images in the training dataset assist the model in extracting and learning features in a way that is invariant to their position, lighting, and more.\n\nTest-time augmentation, or TTA for short, is an application of data augmentation to the test dataset.\n\nSpecifically, it involves creating multiple augmented copies of each image in the test set, having the model make a prediction for each, then returning an ensemble of those predictions.\n\nAugmentations are chosen to give the model the best opportunity for correctly classifying a given image, and the number of copies of an image for which a model must make a prediction is often small, such as less than 10 or 20.\n\nOften, a single simple test-time augmentation is performed, such as a shift, crop, or image flip.\n\nIn their 2015 paper that achieved then state-of-the-art results on the ILSVRC dataset titled \u201c [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556),\u201d the authors use horizontal flip test-time augmentation:\n\n> We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.\n\nSimilarly, in their 2015 paper on the inception architecture titled \u201c [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567),\u201d the authors at Google use cropping test-time augmentation, which they refer to as multi-crop evaluation.\n\n### Want Results with Deep Learning for Computer Vision?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nClick here to subscribe\n\n## Test-Time Augmentation in Keras\n\nTest-time augmentation is not provided natively in the Keras deep learning library but can be implemented easily.\n\nThe [ImageDataGenerator class](https://keras.io/preprocessing/image/) can be used to configure the choice of test-time augmentation. For example, the data generator below is configured for horizontal flip image data augmentation.\n\n|     |     |\n| --- | --- |\n| 1<br>2 | \\# configure image data augmentation<br>datagen=ImageDataGenerator(horizontal\\_flip=True) |\n\nThe augmentation can then be applied to each sample in the test dataset separately.\n\nFirst, the dimensions of the single image can be expanded from _\\[rows\\]\\[cols\\]\\[channels\\]_ to _\\[samples\\]\\[rows\\]\\[cols\\]\\[channels\\]_, where the number of samples is one, for the single image. This transforms the array for the image into an array of samples with one image.\n\n|     |     |\n| --- | --- |\n| 1<br>2 | \\# convert image into dataset<br>samples=expand\\_dims(image,0) |\n\nNext, an iterator can be created for the sample, and the batch size can be used to specify the number of augmented images to generate, such as 10.\n\n|     |     |\n| --- | --- |\n| 1<br>2 | \\# prepare iterator<br>it=datagen.flow(samples,batch\\_size=10) |\n\nThe iterator can then be passed to the _predict\\_generator()_ function of the model in order to make a prediction. Specifically, a batch of 10 augmented images will be generated and the model will make a prediction for each.\n\n|     |     |\n| --- | --- |\n| 1<br>2 | \\# make predictions for each augmented image<br>yhats=model.predict\\_generator(it,steps=10,verbose=0) |\n\nFinally, an ensemble prediction can be made. A prediction was made for each image, and each prediction contains a probability of the image belonging to each class, in the case of image multiclass classification.\n\nAn ensemble prediction can be made using [soft voting](https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/) where the probabilities of each class are summed across the predictions and a class prediction is made by calculating the [argmax()](https://machinelearningmastery.com/argmax-in-machine-learning/) of the summed predictions, returning the index or class number of the largest summed probability.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3<br>4 | \\# sum across predictions<br>summed=numpy.sum(yhats,axis=0)<br>\\# argmax across classes<br>returnargmax(summed) |\n\nWe can tie these elements together into a function that will take a configured data generator, fit model, and single image, and will return a class prediction (integer) using test-time augmentation.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12 | \\# make a prediction using test-time augmentation<br>def tta\\_prediction(datagen,model,image,n\\_examples):<br>\\# convert image into dataset<br>samples=expand\\_dims(image,0)<br>\\# prepare iterator<br>it=datagen.flow(samples,batch\\_size=n\\_examples)<br>\\# make predictions for each augmented image<br>yhats=model.predic...",
      "url": "https://www.machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification"
    },
    {
      "title": "Guide To Data Augmentation For Image Classification - Averroes AI",
      "text": "[Back](https://averroes.ai/blog)\n\nManufacturing quality control demands precision\u2013but precision requires data.\n\nWhen defect examples run scarce and inspection accuracy falls short, you need solutions that work with what you have.\n\nWe\u2019ve turned 63% accuracy rates into 97% without collecting a single new image. And we\u2019ll break down exactly how.\n\nFor manufacturing engineers and quality control teams seeking real solutions to data scarcity, here\u2019s your step-by-step technical guide to making every image count.\n\nNo theory\u2013just proven methods that deliver results in production environments.\n\n## Key Notes\n\n- Advanced augmentation techniques boost detection accuracy up to 97% while minimizing false positives.\n\n- Implementation requires minimal hardware changes, focusing instead on optimizing existing inspection systems.\n\n- Smart augmentation strategies reduce project timelines by 78% compared to traditional quality control methods.\n\n## What Is Data Augmentation?\n\nData augmentation serves as a method to increase training dataset size and diversity by applying systematic modifications to existing data samples.\n\nFor [image classification tasks](https://averroes.ai/features/ai-defect-classification), these modifications create variations of original images while preserving their essential characteristics and labels.\n\nThe technique operates by generating new training examples through controlled alterations, helping [machine learning models](https://averroes.ai/blog/deep-learning-in-manufacturing) learn invariant features and improve generalization capabilities.\n\nRather than collecting additional real-world data\u2014which can be expensive and time-consuming\u2014data augmentation creates synthetic variations that reflect real-world conditions the model might encounter.\n\nConsider a manufacturing quality control system: A model trained to detect defects must recognize flaws regardless of lighting conditions or part orientation. Data augmentation creates examples showing defects under various conditions, teaching the model to identify issues across different scenarios.\n\nThis approach has proven highly effective, with implementations showing dramatic improvements in inspection accuracy from [63% to 97%](https://siliconsemiconductor.net/article/120403/Beyond_AOI_An_AI-driven_revolution_in_visual_inspection) after implementing smart augmentation techniques.\n\n## Deep Learning and Data Augmentation\n\nData augmentation is particularly impactful when used with deep learning models, especially convolutional neural networks (CNNs) commonly used in image classification.\n\nThe complex architectures of these models, with their many layers and numerous parameters, make them highly susceptible to overfitting, especially when training data is limited.\n\nData augmentation mitigates this risk by:\n\n- **Increasing Data Variety**: Providing a richer variety of inputs, helping the CNN learn more generalizable features.\n- **Improving Generalization**: Enhancing the model\u2019s ability to accurately classify unseen images.\n- **Enhanced Robustness**: Making the model more resilient to real-world image variations.\n\nThe application of augmentations is highly optimized in deep learning pipelines:\n\n- **During preprocessing**: Creating augmented versions of the training dataset.\n- **Real-time during training**: Many frameworks offer built-in capabilities for on-the-fly augmentation.\n- **At inference time**: Test-time augmentation improves prediction accuracy.\n\n### Implementation Mechanics\n\nData augmentation functions through three primary mechanisms:\n\n- **Label-Preserving Transformations**: Modifications altering image appearance without changing semantic meaning.\n- **Feature Space Alterations**: Changes to the underlying data representation, crucial for deep learning architectures.\n- **Synthetic Sample Generation**: Creating new examples using advanced techniques, including Generative Adversarial Networks (GANs). GANs are particularly useful for supplementing limited datasets.\n\n## How GANs Enhance Deep Learning within Smart Augmentation\n\nSmart Augmentation tools leverage Generative Adversarial Networks (GANs), generating entirely new synthetic training images. These GANs work by using two neural networks:\n\n- **Generator**: Creates synthetic images mimicking real images.\n- **Discriminator**: Distinguishes between real and synthetic images, driving iterative improvement in the generator\u2019s output.\n\nThis process results in increasingly realistic synthetic images. These significantly boost training dataset size and diversity, improving model accuracy and reducing overfitting.\n\nThis is especially beneficial for:\n\n- **Rare Defects**: Generating synthetic examples of infrequently occurring defects.\n- **Limited Datasets**: Supplementing scarce or expensive-to-obtain real-world data.\n\nBy incorporating GAN-generated images, deep learning models become more robust and capable of generalizing to new, unseen data.\n\n### Impact on Model Performance\n\nData augmentation significantly influences model performance through multiple channels:\n\n#### Improved Generalization\n\nReduces overfitting by exposing models to varied data representations, teaching invariance to common variations in input data, and strengthening feature learning across diverse conditions.\n\n#### Enhanced Robustness\n\nModels learn to handle real-world variations more effectively, increasing resilience to noise and distortions, and resulting in better performance on edge cases and unusual inputs.\n\n#### Resource Optimization\n\nReduces data collection and labeling costs, minimizes storage requirements, and enables efficient use of computational resources during training.\n\n## Types of Data Augmentation Techniques\n\nData augmentation techniques are broadly categorized into data warping and oversampling methods.\n\nData warping techniques modify existing images while preserving their [labels](https://averroes.ai/features/labeling), whereas oversampling methods create entirely new synthetic data instances.\n\n### Geometric Transformations\n\nGeometric transformations modify an image\u2019s spatial properties without altering its content.\n\nThese transformations are particularly useful for training deep learning models to be less sensitive to variations in object position, orientation, and scale.\n\nCommon geometric transformations include:\n\n#### Rotation\n\nApplying angular rotations to images helps models learn orientation-invariant features. Rotation can range from 0\u00b0 to 360\u00b0, with smaller ranges often employed to simulate realistic variations.\n\n#### Scaling\n\nChanging image size while preserving aspect ratio teaches scale invariance, enabling models to recognize objects regardless of their size within the image.\n\n#### Flipping\n\nHorizontal flips are commonly used for natural images, while vertical flips may be applied in specialized domains such as satellite imagery.\n\nThis technique effectively doubles the potential training examples while preserving the image\u2019s semantic meaning.\n\n#### Translation\n\nShifting images along the x and y axes enhances position invariance, proving particularly useful for object detection tasks.\n\n### Color Space Transformations\n\nThese transformations modify the color characteristics of an image, enhancing a model\u2019s robustness to variations in lighting conditions and color representations.\n\nCommon color space transformations include:\n\n#### Brightness Adjustment\n\nModifying overall image luminance simulates variations in lighting conditions, enhancing model robustness to different illumination levels.\n\n#### Contrast Modification\n\nAltering the difference between light and dark regions helps models learn features across various contrast levels, improving their ability to generalize across diverse imaging conditions.\n\n#### Color Jittering\n\nIntroducing random variations in color channels (hue, saturation, and value) improves a model\u2019s robustness to color variations commonly encountered in real-world images.\n\n#### Grayscale Conversion\n\nRemoving color information forces t...",
      "url": "https://averroes.ai/blog/guide-to-data-augmentation-for-image-classification"
    },
    {
      "title": "Test Time Augmentation (Experimental) - PyTorch Tabular",
      "text": "Test Time Augmentation (Experimental) - PyTorch Tabular\n[Skip to content](#importing-the-library)\n```\n`[](#__codelineno-0-1)fromsklearn.model\\_selectionimporttrain\\_test\\_split[](#__codelineno-0-2)fromsklearn.metricsimportaccuracy\\_score,f1\\_score[](#__codelineno-0-3)importrandom[](#__codelineno-0-4)importnumpyasnp[](#__codelineno-0-5)importpandasaspd[](#__codelineno-0-6)frompytorch\\_tabular.utilsimportload\\_covertype\\_dataset[](#__codelineno-0-7)fromrichimportprint[](#__codelineno-0-8)# %load\\_ext autoreload[](#__codelineno-0-9)# %autoreload 2`\n```\n```\n`[](#__codelineno-1-1)data,cat\\_col\\_names,num\\_col\\_names,target\\_col=load\\_covertype\\_dataset()[](#__codelineno-1-2)train,test=train\\_test\\_split(data,random\\_state=42)[](#__codelineno-1-3)train,val=train\\_test\\_split(train,random\\_state=42)`\n```\n# Importing the Library[&para;](#importing-the-library)\n```\n`[](#__codelineno-2-1)frompytorch\\_tabularimportTabularModel[](#__codelineno-2-2)frompytorch\\_tabular.modelsimport([](#__codelineno-2-3)CategoryEmbeddingModelConfig[](#__codelineno-2-4))[](#__codelineno-2-5)frompytorch\\_tabular.configimportDataConfig,OptimizerConfig,TrainerConfig[](#__codelineno-2-6)frompytorch\\_tabular.models.common.headsimportLinearHeadConfig`\n```\n# Test-Time Augumentation[&para;](#test-time-augumentation)\nTest time augmentation (TTA) is a popular technique in computer vision. TTA aims at boosting the model accuracy by using data augmentation on the inference stage. The idea behind TTA is simple: for each test image, we create multiple versions that are a little different from the original (e.g., cropped or flipped). Next, we predict labels for the test images and created copies and average model predictions over multiple versions of each image. This usually helps to improve the accuracy irrespective of the underlying model.\nFor more details refer this link:[Test-Time Augmentation for Tabular Data](<https://kozodoi.me/python/structured data/test-time augmentation/2021/09/08/tta-tabular.html>)\n```\n`[](#__codelineno-3-1)results=[]`\n```\n```\n`[](#__codelineno-4-1)data\\_config=DataConfig([](#__codelineno-4-2)target=[[](#__codelineno-4-3)target\\_col[](#__codelineno-4-4)],# target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented[](#__codelineno-4-5)continuous\\_cols=num\\_col\\_names,[](#__codelineno-4-6)categorical\\_cols=cat\\_col\\_names,[](#__codelineno-4-7))[](#__codelineno-4-8)trainer\\_config=TrainerConfig([](#__codelineno-4-9)batch\\_size=1024,[](#__codelineno-4-10)max\\_epochs=100,[](#__codelineno-4-11)early\\_stopping=&quot;&quot;valid\\_loss&quot;&quot;,# Monitor valid\\_loss for early stopping[](#__codelineno-4-12)early\\_stopping\\_mode=&quot;min&quot;,# Set the mode as min because for val\\_loss, lower is better[](#__codelineno-4-13)early\\_stopping\\_patience=5,# No. of epochs of degradation training will wait before terminating[](#__codelineno-4-14)checkpoints=&quot;&quot;valid\\_loss&quot;&quot;,# Save best checkpoint monitoring val\\_loss[](#__codelineno-4-15)load\\_best=True,# After training, load the best checkpoint[](#__codelineno-4-16)# progress\\_bar=&quot;&quot;none&quot;&quot;, # Turning off Progress bar[](#__codelineno-4-17)# trainer\\_kwargs=dict([](#__codelineno-4-18)# enable\\_model\\_summary=False # Turning off model summary[](#__codelineno-4-19)# )[](#__codelineno-4-20))[](#__codelineno-4-21)optimizer\\_config=OptimizerConfig()[](#__codelineno-4-22)[](#__codelineno-4-23)head\\_config=LinearHeadConfig([](#__codelineno-4-24)layers=&quot;&quot;,dropout=0.1,initialization=&quot;kaiming&quot;# No additional layer in head, just a mapping layer to output\\_dim[](#__codelineno-4-25)).\\_\\_dict\\_\\_# Convert to dict to pass to the model config (OmegaConf doesn&#39;t accept objects)[](#__codelineno-4-26)[](#__codelineno-4-27)model1\\_config=CategoryEmbeddingModelConfig([](#__codelineno-4-28)task=&quot;classification&quot;,[](#__codelineno-4-29)layers=&quot;1024-512-512&quot;,# Number of nodes in each layer[](#__codelineno-4-30)activation=&quot;LeakyReLU&quot;,# Activation between each layers[](#__codelineno-4-31)learning\\_rate=1e-3,[](#__codelineno-4-32)head=&quot;LinearHead&quot;,# Linear Head[](#__codelineno-4-33)head\\_config=head\\_config,# Linear Head Config[](#__codelineno-4-34))[](#__codelineno-4-35)[](#__codelineno-4-36)model\\_config=CategoryEmbeddingModelConfig([](#__codelineno-4-37)task=&quot;classification&quot;,[](#__codelineno-4-38)layers=&quot;1024-512-512&quot;,# Number of nodes in each layer[](#__codelineno-4-39)activation=&quot;LeakyReLU&quot;,# Activation between each layers[](#__codelineno-4-40)learning\\_rate=1e-3,[](#__codelineno-4-41)head=&quot;LinearHead&quot;,# Linear Head[](#__codelineno-4-42)head\\_config=head\\_config,# Linear Head Config[](#__codelineno-4-43))[](#__codelineno-4-44)[](#__codelineno-4-45)tabular\\_model=TabularModel([](#__codelineno-4-46)data\\_config=data\\_config,[](#__codelineno-4-47)model\\_config=model\\_config,[](#__codelineno-4-48)optimizer\\_config=optimizer\\_config,[](#__codelineno-4-49)trainer\\_config=trainer\\_config,[](#__codelineno-4-50)verbose=False[](#__codelineno-4-51))`\n```\n```\n`[](#__codelineno-5-1)datamodule=tabular\\_model.prepare\\_dataloader(train=train,validation=val,seed=42)[](#__codelineno-5-2)model=tabular\\_model.prepare\\_model(datamodule)[](#__codelineno-5-3)tabular\\_model.train(model,datamodule)`\n```\n```\n`GPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set\\_float32\\_matmul\\_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set\\_float32\\_matmul\\_precision.html#torch.set\\_float32\\_matmul\\_precision\n/home/manujosephv/miniconda3/envs/lightning\\_upgrade/lib/python3.11/site-packages/pytorch\\_lightning/callbacks/model\\_checkpoint.py:639: Checkpoint directory saved\\_models exists and is not empty.\nLOCAL\\_RANK: 0 - CUDA\\_VISIBLE\\_DEVICES: [0]`\n```\n```\n\u250f\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\u2503\u2503Name\u2503Type\u2503Params\u2503\n\u2521\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\u25020\u2502 \\_backbone \u2502CategoryEmbeddingBackbone \u2502823 K \u2502\u25021\u2502 \\_embedding\\_layer \u2502Embedding1dLayer \u2502896 \u2502\u25022\u2502 head \u2502LinearHead \u25023.6 K \u2502\u25023\u2502 loss \u2502CrossEntropyLoss \u25020 \u2502\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518```\n```\nTrainable params: 827 KNon-trainable params: 0Total params: 827 KTotal estimated model params size (MB): 3\n```\n```\n`Output()`\n```\n```\n/home/manujosephv/miniconda3/envs/lightning\\_upgrade/lib/python3.11/site-packages/pytorch\\_lightning/trainer/connecto\nrs/data\\_connector.py:441: The 'val\\_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num\\_workers` argument` to `num\\_workers=19` in the `DataLoader` to improve performance.\n```\n```\n/home/manujosephv/miniconda3/envs/lightning\\_upgrade/lib/python3.11/site-packages/pytorch\\_lightning/trainer/connecto\nrs/data\\_connector.py:441: The 'train\\_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num\\_workers` argument` to `num\\_workers=19` in the `DataLoader` to improve performance.\n```\n```\n```\n```\n```\n```\n`&lt;&lt;pytorch\\_lightning.trainer.trainer.Trainer at 0x7f56e2806610&gt;&gt;`\n```\n```\n`[](#__codelineno-6-1)pred\\_df=tabular\\_model.predict(test)`\n```\n```\n`[](#__codelineno-7-1)tta\\_pred\\_df=tabular\\_model.predict(test,test\\_time\\_augmentation=True,num\\_tta=5,alpha\\_tta=0.005)`\n```\n```\n`[](#__codelineno-8-1)# Calculating metrics[](#__codelineno-8-2)orig\\_acc=accuracy\\_score(test[target\\_col].values,pred\\_df[&quot;prediction&quot;].values)[](#__codelineno-8-3)tta\\_acc=accuracy\\_score(test[target\\_col].values,tta\\_pred\\_df[&quot;prediction&quot;].values)[](#__codelineno-8-4)print(f&quot;Or...",
      "url": "https://pytorch-tabular.readthedocs.io/en/stable/tutorials/11-Test%20Time%20Augmentation"
    },
    {
      "title": "Test-time augmentation for deep learning-based cell segmentation ...",
      "text": "<div><div>\n \n <div><h2>Introduction</h2><div><p>Identifying objects at the single-cell level is the starting point of most microscopy-based quantitative cellular image analysis tasks. Precise segmentation of the cell\u2019s nucleus is a major challenge here. Numerous approaches have been developed, including methods based on mathematical morphology<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR1\">1</a></sup> or differential geometry<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR2\">2</a>,<a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR3\">3</a></sup>. More recently, deep learning has yielded a never-seen improvement of accuracy and robustness<sup><a href=\"#ref-CR4\">4</a>,<a href=\"#ref-CR5\">5</a>,<a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR6\">6</a></sup>. Remarkably, Kaggle\u2019s Data Science Bowl 2018 (DSB)<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR7\">7</a></sup> was dedicated to nuclei segmentation, and gave a great momentum to this field. Deep learning-based approaches have proved their effectiveness: practically all the teams used some type of a deep architecture in the first few hundred leaderboard positions. The most popular architectures included U-Net<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR4\">4</a></sup>, originally designed for medical image segmentation, and Mask R-CNN<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR8\">8</a></sup>, used for instance segmentation of natural objects.</p><p>Deep learning approaches for object segmentation require a large, and often pixel-wise annotated dataset for training. This task relies on high-quality samples and domain experts to accurately annotate images. Besides, analysing biological images is challenging because of their heterogeneity and, sometimes, poorer quality compared to natural images. In addition, ground truth masks might be imperfect due to the annotator-related bias, which introduces further uncertainty. Consequently, a plethora of annotated samples is required, making object segmentation a laborious process. One of the techniques utilized to improve the model is data augmentation<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR9\">9</a></sup> of the training set. Conventionally, a transformation (i.e. rotation, flipping, noise addition, etc.) or a series of transformations are applied on the original images. Data augmentation has become the <i>de facto</i> technique in deep learning, especially in the case of heterogeneous or small datasets, to improve the accuracy of cell-based analysis.</p><p>Another option of improving performance relies on augmenting both the training and the test datasets, then performing the prediction both on the original and on the augmented versions of the image, followed by merging the predictions. This approach is called <b><i>test-time augmentation</i></b> (Fig.\u00a0<a href=\"https://www.nature.com/articles/s41598-020-61808-3#Fig1\">1</a>). This technique was successfully used in image classification tasks<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR10\">10</a></sup>, for aleatoric uncertainty estimation<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR11\">11</a></sup>, as well as for the segmentation of MRI slices/MRI volumes<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR12\">12</a></sup>. A theoretical formulation<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR12\">12</a></sup> of test-time augmentation has recently been described by Wang <i>et al</i>. Their experiments show that TTA helps to eliminate overconfident incorrect predictions. Additionally, a framework<sup><a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR13\">13</a></sup> has also been proposed for quantifying the uncertainty of the deep neural network (DNN) model for diagnosing diabetic retinopathy based\u00a0on test-time data augmentation. Its disadvantage is increased prediction time, as it is run not only on the original image, but on all of its augmentations as well.</p><div><figure><figcaption><b>Figure 1</b></figcaption><div><div><a href=\"https://www.nature.com/articles/s41598-020-61808-3/figures/1\"></a></div><p>Principle of the proposed test-time augmentation techniques. Several augmented instances of the same test images are predicted, and the results are transformed back and merged. In the case of U-Net, pixel-wise majority voting was applied, while for Mask R-CNN a combination of object matching and majority voting was applied.</p></div><p><a href=\"https://www.nature.com/articles/s41598-020-61808-3/figures/1\"><span>Full size image</span></a></p></figure></div><p>In the current paper we assess the impact and describe cases of utilizing test-time augmentation for deep-learning models trained on microscopy datasets. We have trained deep learning models for semantic segmentation (when the network only distinguishes the foreground from the background, using the U-Net architecture) and instance segmentation (when the network assigns labels to separate objects, using the Mask R-CNN architecture) (Fig.\u00a0<a href=\"https://www.nature.com/articles/s41598-020-61808-3#Fig1\">1</a>). Test-time augmentation has outperformed single instance predictions at each test case, and could further improve the best result of the DSB, as demonstrated by the improvement of the score, changing from 0.633 to 0.644.</p></div></div><div><h2>Methods</h2><div><h3>Dataset acquisition and description</h3><p>We have used two datasets: fluorescent microscopy images (further referred to as \u2018fluorescent\u2019 dataset) and histopathology images (further referred to as \u2018tissue\u2019 dataset). Most of the images have come from the stage 1 train/test data of Data Science Bowl 2018. We also used additional sources<sup><a href=\"#ref-CR14\">14</a>,<a href=\"#ref-CR15\">15</a>,<a href=\"#ref-CR16\">16</a>,<a href=\"#ref-CR17\">17</a>,<a href=\"#ref-CR18\">18</a>,<a href=\"#ref-CR19\">19</a>,<a href=\"https://www.nature.com/articles/s41598-020-61808-3#ref-CR20\">20</a></sup> and other data published in the discussion thread \u2018Official External Data Thread\u2019 (<a href=\"https://www.kaggle.com/c/data-science-bowl-2018/discussion/47572\">https://www.kaggle.com/c/data-science-bowl-2018/discussion/47572</a>) related to DSB 2018. The images were labelled by experts using the annotation plugins of ImageJ/Fiji and Gimp. Both datasets were divided into three holdout train/test sets: approximately 5%, 15% (6 splits for each, cross-validation), and 30% (further referred to as \u20185\u2019, \u201815\u2019 and \u201830\u2019 in the dataset name, respectively) of uncropped images were held out as the test set. The test sets (\u20185\u2019, first cross-validation split of \u201815\u2019 and \u201830\u2019) did not intersect.</p><p>We used the same augmentations (horizontal and vertical flip, 90\u00b0, 180\u00b0 and 270\u00b0 rotations) for training both architectures. The images were cropped to the size of 512\u2009\u00d7\u2009512 pixels. Crops from the same image were used only in either the train or test set. Images with a resolution of less than 512\u2009\u00d7\u2009512 were resized to that particular size. Sample images are shown in Fig.\u00a0<a href=\"https://www.nature.com/articles/s41598-020-61808-3#Fig2\">2</a>.</p><div><figure><figcaption><b>Figure 2</b></figcaption><div><div><a href=\"https://www.nature.com/articles/s41598-020-61808-3/figures/2\"></a></div><p>Examples of predictions. (<b>A</b>) U-Net predictions. First column - original image, second column - predictions without TTA compared to ground truth, third column - predictions with TTA compared to ground truth. Red indicates false negative pixels, green indicates true positive pixels and blue indicates false positive pixels. Dividing lines: yellow is false positive division of pixels into objects, and cyan is false negative division of pixels into objects. Fourth column - averaged TTA predictions before thresholding, fifth column - zoomed insets from t...",
      "url": "https://www.nature.com/articles/s41598-020-61808-3"
    },
    {
      "title": "Aleatoric uncertainty estimation with test-time augmentation for ... - NIH",
      "text": "Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1016/j.neucom.2019.01.103)\n* [](pdf/EMS84271.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![UKPMC Funders Author Manuscripts logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-wtpa2.gif)\nNeurocomputing (Amst)\n. Author manuscript; available in PMC: 2019 Oct 8.\n*Published in final edited form as:*Neurocomputing (Amst). 2019 Feb 7;335:34\u201345. doi:[10.1016/j.neucom.2019.01.103](https://doi.org/10.1016/j.neucom.2019.01.103)\n# Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks\n[Guotai Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang G\"[Author]>)\n### Guotai Wang\naWellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK\nbSchool of Biomedical Engineering and Imaging Sciences, King\u2019s College London, London, UK\ncSchool of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China\nFind articles by[Guotai Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang G\"[Author]>)\na,b,c,\\*,[Wenqi Li](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Li W\"[Author]>)\n### Wenqi Li\naWellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK\nbSchool of Biomedical Engineering and Imaging Sciences, King\u2019s College London, London, UK\nFind articles by[Wenqi Li](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Li W\"[Author]>)\na,b,[Michael Aertsen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aertsen M\"[Author]>)\n### Michael Aertsen\ndDepartment of Radiology, University Hospitals Leuven, Leuven, Belgium\nFind articles by[Michael Aertsen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aertsen M\"[Author]>)\nd,[Jan Deprest](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Deprest J\"[Author]>)\n### Jan Deprest\naWellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK\ndDepartment of Radiology, University Hospitals Leuven, Leuven, Belgium\neInstitute for Women\u2019s Health, University College London, London, UK\nfDepartment of Obstetrics and Gynaecology, University Hospitals Leuven, Leuven, Belgium\nFind articles by[Jan Deprest](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Deprest J\"[Author]>)\na,d,e,f,[S\u00e9bastien Ourselin](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ourselin S\"[Author]>)\n### S\u00e9bastien Ourselin\nbSchool of Biomedical Engineering and Imaging Sciences, King\u2019s College London, London, UK\nFind articles by[S\u00e9bastien Ourselin](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ourselin S\"[Author]>)\nb,[Tom Vercauteren](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Vercauteren T\"[Author]>)\n### Tom Vercauteren\naWellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK\nbSchool of Biomedical Engineering and Imaging Sciences, King\u2019s College London, London, UK\nfDepartment of Obstetrics and Gynaecology, University Hospitals Leuven, Leuven, Belgium\nFind articles by[Tom Vercauteren](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Vercauteren T\"[Author]>)\na,b,f\n* Author information\n* Article notes\n* Copyright and License information\naWellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK\nbSchool of Biomedical Engineering and Imaging Sciences, King\u2019s College London, London, UK\ncSchool of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China\ndDepartment of Radiology, University Hospitals Leuven, Leuven, Belgium\neInstitute for Women\u2019s Health, University College London, London, UK\nfDepartment of Obstetrics and Gynaecology, University Hospitals Leuven, Leuven, Belgium\n\\*\nCorresponding author at: Wellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK.guotai.1.wang@kcl.ac.uk,guotai.wang@uestc.edu.cn(G. Wang).\nIssue date 2019 Sep 3.\nThis is an open access article under the CC BY license. ([http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/))\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC6783308\u00a0\u00a0EMSID: EMS84271\u00a0\u00a0PMID:[31595105](https://pubmed.ncbi.nlm.nih.gov/31595105/)\nThe publisher's version of this article is available at[Neurocomputing (Amst)](https://doi.org/10.1016/j.neucom.2019.01.103)\n## Abstract\nDespite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (*epistemic*) and image-based (*aleatoric*) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based*aleatoric*uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed*aleatoric*uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based*aleatoric*uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.\n**Keywords:**Uncertainty estimation, Convolutional neural networks, Medical image segmentation, Data augmentation\n## 1. Introduction\nSegmentation of medical images is an essential task for many applications such as anatomical structure modeling, tumor growth measurement, surgical planing and treatment assessment [[1](#R1)]. Despite the breadth and depth of current research, it is very challenging...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6783308"
    },
    {
      "title": "\ud83c\udfd6\ufe0f Kaggle Solutions",
      "text": "test time augmentation (tta)\n# [\ud83c\udfd6\ufe0f Kaggle Solutions](../..)\nSearch\nSearchSearch\nDark modeLight mode\n# Explorer\n* * competitions\n* [Bengali.AI Speech Recognition](../../competitions/Bengali.AI-Speech-Recognition)\n* [CAFA 5 Protein Function Prediction](../../competitions/CAFA-5-Protein-Function-Prediction)\n* [Child Mind Institute - Detect Sleep States](../../competitions/Child-Mind-Institute---Detect-Sleep-States)\n* [CommonLit - Evaluate Student Summaries](../../competitions/CommonLit---Evaluate-Student-Summaries)\n* [G-Research Crypto Forecasting](../../competitions/G-Research-Crypto-Forecasting)\n* [Google - Fast or Slow? Predict AI Model Runtime](../../competitions/Google---Fast-or-Slow-Predict-AI-Model-Runtime)\n* [Google QUEST Q&amp;A Labeling](../../competitions/Google-QUEST-Q-and-A-Labeling)\n* [ICR - Identifying Age-Related Conditions](../../competitions/ICR---Identifying-Age-Related-Conditions)\n* [LANL Earthquake Prediction](../../competitions/LANL-Earthquake-Prediction)\n* [Linking Writing Processes to Writing Quality](../../competitions/Linking-Writing-Processes-to-Writing-Quality)\n* [Mechanisms of Action (MoA) Prediction](../../competitions/Mechanisms-of-Action-(MoA)-Prediction)\n* [MLB Player Digital Engagement Forecasting](../../competitions/MLB-Player-Digital-Engagement-Forecasting)\n* [NeurIPS 2023 - Machine Unlearning](../../competitions/NeurIPS-2023---Machine-Unlearning)\n* [Novozymes Enzyme Stability Prediction](../../competitions/Novozymes-Enzyme-Stability-Prediction)\n* [Open Problems \u2013Single-Cell Perturbations](../../competitions/Open-Problems-\u2013-Single-Cell-Perturbations)\n* [RSNA 2023 Abdominal Trauma Detection](../../competitions/RSNA-2023-Abdominal-Trauma-Detection)\n* [Stanford Ribonanza RNA Folding](../../competitions/Stanford-Ribonanza-RNA-Folding)\n* [Tweet Sentiment Extraction](../../competitions/Tweet-Sentiment-Extraction)\n* [UBC Ovarian Cancer Subtype Classification and Outlier Detection (UBC-OCEAN)](../../competitions/UBC-Ovarian-Cancer-Subtype-Classification-and-Outlier-Detection-(UBC-OCEAN))\n* [Vesuvius Challenge - Ink Detection](../../competitions/Vesuvius-Challenge---Ink-Detection)\n* eval-functions\n* [Average Precision](../../eval-functions/Average-Precision)\n* [Balanced Accuracy](../../eval-functions/Balanced-Accuracy)\n* [balanced logloss](../../eval-functions/balanced-logloss)\n* [BCELoss](../../eval-functions/BCELoss)\n* [BCEWithLogitsLoss](../../eval-functions/BCEWithLogitsLoss)\n* [categorical cross entropy](../../eval-functions/categorical-cross-entropy)\n* [CosineEmbeddingLoss](../../eval-functions/CosineEmbeddingLoss)\n* [cross-entropy loss](../../eval-functions/cross-entropy-loss)\n* [DiceLoss](../../eval-functions/DiceLoss)\n* [F-score](../../eval-functions/F-score)\n* [HingeLoss](../../eval-functions/HingeLoss)\n* [Huber loss](../../eval-functions/Huber-loss)\n* [jaccard similarity (aka Intersection Over Union)](../../eval-functions/jaccard-similarity-(aka-Intersection-Over-Union))\n* [Kendall Tau correlation](../../eval-functions/Kendall-Tau-correlation)\n* [KLDivergenceLoss](../../eval-functions/KLDivergenceLoss)\n* [Levenshtein distance](../../eval-functions/Levenshtein-distance)\n* [ListMLE Loss](../../eval-functions/ListMLE-Loss)\n* [log loss](../../eval-functions/log-loss)\n* [log-likelihood](../../eval-functions/log-likelihood)\n* [LogCosh](../../eval-functions/LogCosh)\n* [Loss functions](../../eval-functions/Loss-functions)\n* [MAELoss](../../eval-functions/MAELoss)\n* [MAPE loss](../../eval-functions/MAPE-loss)\n* [marginRankingLoss](../../eval-functions/marginRankingLoss)\n* [Mean absolute error (MAE)](../../eval-functions/Mean-absolute-error-(MAE))\n* [mean column-wise mean absolute error (MCMAE)](../../eval-functions/mean-column-wise-mean-absolute-error-(MCMAE))\n* [mean reciprocal rank (MRR)](../../eval-functions/mean-reciprocal-rank-(MRR))\n* [Mean Rowwise Root Mean Squared Error](../../eval-functions/Mean-Rowwise-Root-Mean-Squared-Error)\n* [MSELoss](../../eval-functions/MSELoss)\n* [PairwiseHingeLoss](../../eval-functions/PairwiseHingeLoss)\n* [Pearson's Correlation Coefficient](../../eval-functions/Pearson's-Correlation-Coefficient)\n* [Precision](../../eval-functions/Precision)\n* [ranking loss functions](../../eval-functions/ranking-loss-functions)\n* [recall](../../eval-functions/recall)\n* [receiver operating characteristic curve (ROC)](../../eval-functions/receiver-operating-characteristic-curve-(ROC))\n* [RMSE](../../eval-functions/RMSE)\n* [Spearman's correlation Coefficient](../../eval-functions/Spearman's-correlation-Coefficient)\n* [substring segmentation](../../eval-functions/substring-segmentation)\n* [Word Error Rate](../../eval-functions/Word-Error-Rate)\n* * ml-concepts\n* activation functions\n* [activation functions](../../ml-concepts/activation-functions/activation-functions)\n* [ELU (Exponential Linear Unit)](../../ml-concepts/activation-functions/ELU-(Exponential-Linear-Unit))\n* [Gated Linear Units (GLU)](../../ml-concepts/activation-functions/Gated-Linear-Units-(GLU))\n* [Gaussian Error Linear Unit (GELU)](../../ml-concepts/activation-functions/Gaussian-Error-Linear-Unit-(GELU))\n* [sigmoid](../../ml-concepts/activation-functions/sigmoid)\n* [Sigmoid Linear Unit (Swish)](../../ml-concepts/activation-functions/Sigmoid-Linear-Unit-(Swish))\n* [Smish](../../ml-concepts/activation-functions/Smish)\n* [Softmax](../../ml-concepts/activation-functions/Softmax)\n* [SwiGLU](../../ml-concepts/activation-functions/SwiGLU)\n* augmentations\n* [cutmix](../../ml-concepts/augmentations/cutmix)\n* [cutout](../../ml-concepts/augmentations/cutout)\n* [mixup](../../ml-concepts/augmentations/mixup)\n* [test time augmentation (tta)](../../ml-concepts/augmentations/test-time-augmentation-(tta))\n* clustering\n* [cluster sampling](../../ml-concepts/clustering/cluster-sampling)\n* [DBSCAN](../../ml-concepts/clustering/DBSCAN)\n* [K-nearest neighbour (KNN)](../../ml-concepts/clustering/K-nearest-neighbour-(KNN))\n* [kmeans](../../ml-concepts/clustering/kmeans)\n* [mean-shift clustering](../../ml-concepts/clustering/mean-shift-clustering)\n* [TSNE](../../ml-concepts/clustering/TSNE)\n* [UMAP dimension reduction](../../ml-concepts/clustering/UMAP-dimension-reduction)\n* cross-validation\n* [Blocked Cross-Validation](../../ml-concepts/cross-validation/Blocked-Cross-Validation)\n* [forward chaining cross validation](../../ml-concepts/cross-validation/forward-chaining-cross-validation)\n* [GroupKFold](../../ml-concepts/cross-validation/GroupKFold)\n* [hold-out cross validation](../../ml-concepts/cross-validation/hold-out-cross-validation)\n* [kfold](../../ml-concepts/cross-validation/kfold)\n* [stratified kfold](../../ml-concepts/cross-validation/stratified-kfold)\n* input-tricks\n* [absolute positional embedding](../../ml-concepts/input-tricks/absolute-positional-embedding)\n* [adversarial validation](../../ml-concepts/input-tricks/adversarial-validation)\n* [ALiBi positional encoding](../../ml-concepts/input-tricks/ALiBi-positional-encoding)\n* [BPE tokenizer](../../ml-concepts/input-tricks/BPE-tokenizer)\n* [Online hard negative mining](../../ml-concepts/input-tricks/Online-hard-negative-mining)\n* [Sequence bucketing](../../ml-concepts/input-tricks/Sequence-bucketing)\n* [singular value decomposition (SVD)](../../ml-concepts/input-tricks/singular-value-decomposition-(SVD))\n* layers\n* [axial attention](../../ml-concepts/layers/axial-attention)\n* [channel shuffle](../../ml-concepts/layers/channel-shuffle)\n* [Convolutional Block Attention Module (CBAM)](../../ml-concepts/layers/Convolutional-Block-Attention-Module-(CBAM))\n* [GPS Layers](../../ml-concepts/layers/GPS-Layers)\n* [GRU](../../ml-concepts/layers/GRU)\n* [linformers](../../ml-concepts/layers/linformers)\n* [LSTM](../../ml-concepts/layers/LSTM)\n* [pointwise convolution](../../ml-concepts/layers/pointwise-convolution)\n* [SAGEConv](../../ml-concepts/layers/SAGEConv)\n* [Spectral Graph Convolutions](../../ml-concepts/layers/Spectral-Graph-Convolutions)\n* [Squeeze-and-Excitation layer](../../ml-concepts/layers/Squeeze-and-Exci...",
      "url": "https://kaggle.curtischong.me/ml-concepts/augmentations/test-time-augmentation-(tta)"
    }
  ]
}