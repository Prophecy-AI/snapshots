{
  "query": "How do Kaggle winners optimize for AUC metric in binary image classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent research on deep AUC maximization and a documented first\u2011place Kaggle solution, winners typically follow these core steps to squeeze the highest AUROC out of binary image\u2011classification tasks\u202f([arXiv:2111.02400](https://arxiv.org/abs/2111.02400); [arXiv:2012.03173](https://arxiv.org/abs/2012.03173); [arXiv:2402.05400v2](https://arxiv.org/html/2402.05400v2); [bjlkeng.io blog](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition)):\n\n1. **Pick an AUC\u2011specific surrogate loss**  \n   - Use a margin\u2011based min\u2011max surrogate (the \u201cAUC margin loss\u201d) or the `AUCMLoss` implementation from the LibAUC library, which is more robust than the classic AUC square loss\u202f([arXiv:2012.03173](https://arxiv.org/abs/2012.03173); [LibAUC docs](https://docs.libauc.org/examples/auroc.html)).  \n\n2. **Train with an optimizer designed for AUC loss**  \n   - Apply the PESG (Projected Stochastic Gradient) optimizer (or other stochastic algorithms proven for deep AUC maximization) to directly maximize the surrogate loss\u202f([LibAUC docs](https://docs.libauc.org/examples/auroc.html)).  \n\n3. **Employ Loss\u2011Conditional Training (LCT) over a family of losses**  \n   - Instead of a single loss, sample from a set of related AUC\u2011oriented losses during training; this reduces sensitivity to hyper\u2011parameters and yields models that generalize better on highly imbalanced data\u202f([arXiv:2402.05400v2](https://arxiv.org/html/2402.05400v2)).  \n\n4. **Use strong, pretrained vision backbones**  \n   - Fine\u2011tune high\u2011capacity CNNs such as EfficientNet (or ResNet variants) that have demonstrated top performance in Kaggle image challenges\u202f([bjlkeng.io blog](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition)).  \n\n5. **Apply aggressive data augmentation & test\u2011time augmentation (TTA)**  \n   - Mix augmentations (random flips, rotations, color jitter, CutMix, MixUp, etc.) during training and average predictions over multiple augmented views at inference to boost AUROC.  \n\n6. **Perform stratified K\u2011fold cross\u2011validation and model ensembling**  \n   - Train several folds with the AUC\u2011optimized loss, then blend their predictions (e.g., weighted average or stacking). Ensembling reduces variance and often pushes the leaderboard AUROC higher.  \n\n7. **Tune AUC\u2011centric hyper\u2011parameters**  \n   - Focus on learning\u2011rate schedules, batch size, and the margin parameter of the AUC loss; use validation AUROC as the sole metric for early stopping and hyper\u2011parameter selection.  \n\nFollowing this pipeline\u2014AUC\u2011specific loss\u202f+\u202fAUC\u2011aware optimizer\u202f+\u202floss\u2011conditional training, coupled with powerful backbones, heavy augmentation, cross\u2011validation, and ensembling\u2014has been shown to consistently deliver the top AUROC scores in Kaggle binary image classification competitions.",
      "url": ""
    },
    {
      "title": "Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2111.02400** (cs)\n\n\\[Submitted on 1 Nov 2021\\]\n\n# Title:Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities\n\nAuthors: [Tianbao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+T)\n\nView a PDF of the paper titled Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities, by Tianbao Yang\n\n[View PDF](https://arxiv.org/pdf/2111.02400)\n\n> Abstract:In this extended abstract, we will present and discuss opportunities and challenges brought about by a new deep learning method by AUC maximization (aka \\\\underline{\\\\bf D}eep \\\\underline{\\\\bf A}UC \\\\underline{\\\\bf M}aximization or {\\\\bf DAM}) for medical image classification. Since AUC (aka area under ROC curve) is a standard performance measure for medical image classification, hence directly optimizing AUC could achieve a better performance for learning a deep neural network than minimizing a traditional loss function (e.g., cross-entropy loss). Recently, there emerges a trend of using deep AUC maximization for large-scale medical image classification. In this paper, we will discuss these recent results by highlighting (i) the advancements brought by stochastic non-convex optimization algorithms for DAM; (ii) the promising results on various medical image classification problems. Then, we will discuss challenges and opportunities of DAM for medical image classification from three perspectives, feature learning, large-scale optimization, and learning trustworthy AI models.\n\n|     |     |\n| --- | --- |\n| Comments: | Medical Imaging meets NeurIPS 2021 workshop |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2111.02400](https://arxiv.org/abs/2111.02400) \\[cs.LG\\] |\n|  | (or [arXiv:2111.02400v1](https://arxiv.org/abs/2111.02400v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2111.02400](https://doi.org/10.48550/arXiv.2111.02400)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tianbao Yang \\[ [view email](https://arxiv.org/show-email/b04b836c/2111.02400)\\]\n\n**\\[v1\\]**\nMon, 1 Nov 2021 15:31:32 UTC (33 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities, by Tianbao Yang\n\n- [View PDF](https://arxiv.org/pdf/2111.02400)\n- [TeX Source](https://arxiv.org/src/2111.02400)\n- [Other Formats](https://arxiv.org/format/2111.02400)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2111.02400&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2111.02400&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-11](https://arxiv.org/list/cs.LG/2021-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2111.02400?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2111.02400?context=cs.AI)\n\n[cs.CV](https://arxiv.org/abs/2111.02400?context=cs.CV)\n\n[eess](https://arxiv.org/abs/2111.02400?context=eess)\n\n[eess.IV](https://arxiv.org/abs/2111.02400?context=eess.IV)\n\n[math](https://arxiv.org/abs/2111.02400?context=math)\n\n[math.OC](https://arxiv.org/abs/2111.02400?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2111.02400)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2111.02400)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2111.02400)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2111.html#abs-2111-02400) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2111-02400)\n\n[Tianbao Yang](https://dblp.uni-trier.de/search/author?author=Tianbao%20Yang)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2111.02400&description=Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2111.02400&title=Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2111.02400) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2111.02400"
    },
    {
      "title": "Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2012.03173** (cs)\n\n\\[Submitted on 6 Dec 2020 ( [v1](https://arxiv.org/abs/2012.03173v1)), last revised 7 Sep 2021 (this version, v2)\\]\n\n# Title:Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification\n\nAuthors: [Zhuoning Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan,+Z), [Yan Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+Y), [Milan Sonka](https://arxiv.org/search/cs?searchtype=author&query=Sonka,+M), [Tianbao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+T)\n\nView a PDF of the paper titled Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification, by Zhuoning Yuan and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2012.03173)\n\n> Abstract:Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC ( [this http URL](http://www.libauc.org)).\n\n|     |     |\n| --- | --- |\n| Comments: | Accepted by ICCV2021 |\n| Subjects: | Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2012.03173](https://arxiv.org/abs/2012.03173) \\[cs.LG\\] |\n|  | (or [arXiv:2012.03173v2](https://arxiv.org/abs/2012.03173v2) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2012.03173](https://doi.org/10.48550/arXiv.2012.03173)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | International Conference on Computer Vision (ICCV2021) |\n\n## Submission history\n\nFrom: Zhuoning Yuan \\[ [view email](https://arxiv.org/show-email/7c04838c/2012.03173)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2012.03173v1)**\nSun, 6 Dec 2020 03:41:51 UTC (12,042 KB)\n\n**\\[v2\\]**\nTue, 7 Sep 2021 19:18:12 UTC (12,301 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification, by Zhuoning Yuan and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2012.03173)\n- [TeX Source](https://arxiv.org/src/2012.03173)\n- [Other Formats](https://arxiv.org/format/2012.03173)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2012.03173&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2012.03173&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-12](https://arxiv.org/list/cs.LG/2020-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2012.03173?context=cs)\n\n[cs.CV](https://arxiv.org/abs/2012.03173?context=cs.CV)\n\n[math](https://arxiv.org/abs/2012.03173?context=math)\n\n[math.OC](https://arxiv.org/abs/2012.03173?context=math.OC)\n\n[stat](https://arxiv.org/abs/2012.03173?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2012.03173?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2012.03173)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2012.03173)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2012.03173)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2012.html#abs-2012-03173) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2012-03173)\n\n[Zhuoning Yuan](https://dblp.uni-trier.de/search/author?author=Zhuoning%20Yuan)\n\n[Yan Yan](https://dblp.uni-trier.de/search/author?author=Yan%20Yan)\n\n[Milan Sonka](https://dblp.uni-trier.de/search/author?author=Milan%20Sonka)\n\n[Tianbao Yang](https://dblp.uni-trier.de/search/author?author=Tianbao%20Yang)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2012.03173&description=Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2012.03173&title=Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender t...",
      "url": "https://arxiv.org/abs/2012.03173"
    },
    {
      "title": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions",
      "text": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\n# Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\nKelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran &amp; Carlo Tomasi\nDepartment of Computer Science\nDuke University\nDurham, NC 27708 USA\n###### Abstract\nAlthough binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code is available at[https://github.com/klieberman/roc\\_lct](https://github.com/klieberman/roc_lct).\n## 1Introduction\nConsider a classifier which takes images of skin lesions and predicts whether the lesions are melanoma> Rotemberg et\u00a0al. (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib16)> )\n. Such a system could be especially valuable in underdeveloped countries where expert resources for diagnosis are scarce> Cassidy et\u00a0al. (\n[> 2022\n](https://arxiv.org/html/2402.05400v2#bib.bib3)> )\n. Classifying melanoma from images is a problem with class imbalance since benign lesions are far more common than melanomas. Furthermore, the accuracy on the melanoma (minority) class is much more important than the accuracy on the benign (majority) class because predicting a benign lesion as melanoma would result in the cost of a biopsy while predicting a melanoma lesion as benign could result in the melanoma spreading before the patient can receive appropriate treatment.\nIn this case, overall accuracy, even on a balanced test set, is clearly an inadequate metric, as it implies that the accuracies on both classes are equally important. Instead, Receiver Operating Characteristic (ROC) curves are better suited for such problems> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. These curves plot the tradeoff between the true positive rate (TPR) on the y-axis and the false positive rate (FPR) on the x-axis over a range of classification thresholds. Unlike scalar metrics (e.g., overall accuracy on a balanced test set orF\u03b2subscript\ud835\udc39\ud835\udefdF\\_{\\\\beta}italic\\_F start\\_POSTSUBSCRIPT italic\\_\u03b2 end\\_POSTSUBSCRIPT), ROC curves show model performance over a wide range of classification thresholds. This allows practitioners to understand how the model\u2019s performance changes based on different classification thresholds and choose the best tradeoff for their needs. ROC curves can also be summarized by their Area Under the Curve (AUC)> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. Furthermore, both ROC curves and AUC have several mathematical properties which make them preferred to alternative precision-recall curves> Flach &amp; Kull (\n[> 2015\n](https://arxiv.org/html/2402.05400v2#bib.bib9)> )\n111We provide definitions and visualizations of commonly-used metrics for binary problems with imbalanced data in Appendix[A](https://arxiv.org/html/2402.05400v2#A1).\nAlthough binary problems, like melanoma classification, are often cited as the motivation for class imbalance problems and ROC curves are thede factometric of choice for such problems, the class imbalance literature largely focuses on improving performance on longtailed multi-class datasets in terms of overall accuracy on a balanced test set. We instead focus on binary problems with severe imbalance and propose a method, which adapts existing techniques for handling class imbalance, to optimize for ROC curves in these binary scenarios.\n![Refer to caption](extracted/5641151/figures/Melanoma_auc_boxplots.png)Figure 1:Distribution of Area Under the ROC Curve (AUC) values obtained by training the same model on the SIIM-ISIC Melanoma classification dataset with 48 different combinations of hyperparameters on VS Loss (hyperparameter values are given in Section[6](https://arxiv.org/html/2402.05400v2#S6)). Results are shown at three different imbalance ratios.As the imbalance becomes more severe, model performance drops and the variance in performance drastically increases. LCT addresses both of these issues by training over a family of loss functions, instead of a single loss function with one combination of hyperparameter values.\nIn particular, we adapt Vector Scaling (VS) loss, which is a general loss function for imbalanced learning with strong theoretical backing> Kini et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2402.05400v2#bib.bib11)> )\n. VS loss is a modification of Cross-entropy loss that adjusts the logits via additive and multiplicative factors. There is theory supporting the use of both of these factors: multiplicative factors are essential for the terminal phase of training, but these have negative effects early during training, so additive factors are necessary speed up convergence. Although VS loss has shown strong performance in the multi-class setting, it does have hyperparameters which require tuning (e.g., the additive and multiplicative factors on the loss function).\nWe find that, in the binary case, the effect of these hyperparameters is small and reasonable at moderate imbalance ratios\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2(where\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2is defined as the ratio of majority to minority samples); however, at large imbalance ratios, small differences in these hyperparameters lead to very wide variance in the results (Figure[1](https://arxiv.org/html/2402.05400v2#S1.F1)). This figure shows that increasing the imbalance ratio not only decreases the AUCs (as expected), but also drastically increases the variance in AUCs obtained by training models with slightly different hyperparameter values.\nIn this work, we highlight the practical effect of the theoretically-motivated VS loss on the ROC metric, especially for data problems with high imbalance ratios. We propose a method that adapts VS loss to align the training objective more closely with ROC curve optimization. Our method trains a single model on a wide range of hyperparameter values using Loss Conditional Training> Dosovitskiy &amp; Djolonga (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib5)> )\n. We find that this method not only reduces the variance in model performance caused by hyperparameter choices, but also improves performance over the best hyperparameter choices since it optimizes for many tradeoffs on the ROC curve. We provide extensive results\u2013 both on CIFAR datasets and datasets of real applications derived from Kaggle competitions\u2013 at multiple imbalance ratios and across a wide range of hyperparameter choices.\nIn summary, our contributions are as follows.\n* \u2022We identify that higher levels of imbalance are not only associated with worse model performance, but also more variance.\n* \u2022We recognize that training over a range of hyperparameter values can actually benefit classification models that are otherwise prone to overfitting to a single loss function.\n* \u2022We propose using Loss Conditional Training (LCT) to improve the training regimen for classification models trained under imbalance.\n* \u2022We show that this method consistently improves performance at high imbalance ratios.\n## 2Related work\nMany solutions have been proposed...",
      "url": "https://arxiv.org/html/2402.05400v2"
    },
    {
      "title": "Optimizing AUCMLoss on Imbalanced CIFAR10 Dataset (PESG) \u2014 libauc 1.0.0 documentation",
      "text": "Optimizing AUCMLoss on Imbalanced CIFAR10 Dataset (PESG) &mdash; libauc 1.0.0 documentation\n* [](../index.html)\n* Optimizing AUCMLoss on Imbalanced CIFAR10 Dataset (PESG)\n* [View page source](../_sources/examples/auroc.rst.txt)\n# Optimizing AUCMLoss on Imbalanced CIFAR10 Dataset (PESG)[\uf0c1](#optimizing-aucmloss-on-imbalanced-cifar10-dataset-pesg)\n[![](https://upload.wikimedia.org/wikipedia/commons/d/d0/Google_Colaboratory_SVG_Logo.svg)Run on Colab](https://colab.research.google.com/drive/1h8wpB_Z5dZECbQdVbTmuAGHQTGaRbmAo)\n[![](https://upload.wikimedia.org/wikipedia/commons/8/8d/Download_alt_font_awesome.svg)Download Notebook](https://drive.google.com/drive/folders/1WKBO-Phlrrutq3157Pzosv-jDVH62MY6?usp=sharing)\n[![](https://upload.wikimedia.org/wikipedia/commons/c/c2/GitHub_Invertocat_Logo.svg)View on Github](https://github.com/Optimization-AI/LibAUC)\n**Author**: Zhuoning Yuan, Gang Li, Tianbao Yang\n**Version**: 1.4.0\n## Introduction[\uf0c1](#introduction)\nIn this tutorial, you will learn how to quickly train a ResNet20\nmodel by optimizing**AUROC**using our novel`AUCMLoss`and`PESG`optimizer[[Ref]](https://arxiv.org/abs/2012.03173)on a binary image classification task on Cifar10. After\ncompletion of this tutorial, you should be able to use LibAUC to\ntrain your own models on your own datasets.\n**Reference**:\nIf you find this tutorial helpful in your work, please cite our[library paper](https://arxiv.org/abs/2306.03065)and the following papers:\n```\n@inproceedings{yuan2021large,\ntitle={Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification},\nauthor={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},\nbooktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\npages={3040--3049},\nyear={2021} }\n```\n## Install LibAUC[\uf0c1](#install-libauc)\nLet\u2019s start with installing our library here. In this tutorial, we will use the lastest version for LibAUC by using`pipinstall-U`.\n```\n!pipinstall-Ulibauc\n```\n## Importing LibAUC[\uf0c1](#importing-libauc)\nImport required libraries to use\n```\nfromlibauc.lossesimportAUCMLossfromlibauc.optimizersimportPESGfromlibauc.modelsimportresnet20asResNet20fromlibauc.datasetsimportCIFAR10fromlibauc.utilsimportImbalancedDataGeneratorfromlibauc.samplerimportDualSamplerfromlibauc.metricsimportauc\\_roc\\_scoreimporttorchfromPILimportImageimportnumpyasnpimporttorchvision.transformsastransformsfromtorch.utils.dataimportDatasetfromsklearn.metricsimportroc\\_auc\\_score\n```\n## Reproducibility[\uf0c1](#reproducibility)\nThe following function`set\\_all\\_seeds`limits the number of sources\nof randomness behaviors, such as model intialization, data shuffling,\netcs. However, completely reproducible results are not guaranteed\nacross PyTorch releases[[Ref]](<https://pytorch.org/docs/stable/notes/randomness.html#:~:text=Completely reproducible results are not,even when using identical seeds.>).\n```\ndefset\\_all\\_seeds(SEED):# REPRODUCIBILITYnp.random.seed(SEED)torch.manual\\_seed(SEED)torch.cuda.manual\\_seed(SEED)torch.backends.cudnn.deterministic=Truetorch.backends.cudnn.benchmark=Falseset\\_all\\_seeds(2023)\n```\n## Image Dataset[\uf0c1](#image-dataset)\nNow we define the data input pipeline such as data\naugmentations. In this tutorial, we use`RandomCrop`,`RandomHorizontalFlip`.\n```\nclassImageDataset(Dataset):def\\_\\_init\\_\\_(self,images,targets,image\\_size=32,crop\\_size=30,mode=&#39;train&#39;):self.images=images.astype(np.uint8)self.targets=targetsself.mode=modeself.transform\\_train=transforms.Compose([transforms.ToTensor(),transforms.RandomCrop((crop\\_size,crop\\_size),padding=None),transforms.RandomHorizontalFlip(),transforms.Resize((image\\_size,image\\_size)),])self.transform\\_test=transforms.Compose([transforms.ToTensor(),transforms.Resize((image\\_size,image\\_size)),])def\\_\\_len\\_\\_(self):returnlen(self.images)def\\_\\_getitem\\_\\_(self,idx):image=self.images[idx]target=self.targets[idx]image=Image.fromarray(image.astype(&#39;uint8&#39;))ifself.mode==&#39;train&#39;:image=self.transform\\_train(image)else:image=self.transform\\_test(image)returnimage,target\n```\n## Configuration[\uf0c1](#configuration)\nHyper-Parameters\n```\n# HyperParametersSEED=123BATCH\\_SIZE=128imratio=0.1# for demototal\\_epochs=100decay\\_epochs=[50,75]lr=0.1margin=1.0epoch\\_decay=0.003# refers gamma in the paperweight\\_decay=0.0001\n```\n## Loading datasets[\uf0c1](#loading-datasets)\n```\n# load data as numpy arraystrain\\_data,train\\_targets=CIFAR10(root=&#39;./data&#39;,train=True).as\\_array()test\\_data,test\\_targets=CIFAR10(root=&#39;./data&#39;,train=False).as\\_array()# generate imbalanced datagenerator=ImbalancedDataGenerator(verbose=True,random\\_seed=0)(train\\_images,train\\_labels)=generator.transform(train\\_data,train\\_targets,imratio=imratio)(test\\_images,test\\_labels)=generator.transform(test\\_data,test\\_targets,imratio=0.5)# data augmentationstrainSet=ImageDataset(train\\_images,train\\_labels)trainSet\\_eval=ImageDataset(train\\_images,train\\_labels,mode=&#39;test&#39;)testSet=ImageDataset(test\\_images,test\\_labels,mode=&#39;test&#39;)\n```\n## Pretraining (Recommended)[\uf0c1](#pretraining-recommended)\nFollowing the original[paper](https://arxiv.org/abs/2012.03173), it\u2019s recommended to start from a pretrained checkpoint with cross-entropy loss to significantly boost models\u2019 performance. It includes a pre-training step with standard cross-entropy loss, and an AUROC maximization step that maximizes an AUROC surrogate loss of the pre-trained model.\n```\nfromtorch.optimimportAdamimportwarningswarnings.filterwarnings(&#39;ignore&#39;)load\\_pretrain=Truemodel=ResNet20(pretrained=False,last\\_activation=None,num\\_classes=1)model=model.cuda()loss\\_fn=torch.nn.BCELoss()optimizer=Adam(model.parameters(),lr=1e-3,weight\\_decay=weight\\_decay)trainloader=torch.utils.data.DataLoader(trainSet,batch\\_size=BATCH\\_SIZE,shuffle=True,num\\_workers=2)testloader=torch.utils.data.DataLoader(testSet,batch\\_size=BATCH\\_SIZE,shuffle=False,num\\_workers=2)best\\_test=0forepochinrange(total\\_epochs):ifepochindecay\\_epochs:forparam\\_groupinoptimizer.param\\_groups:param\\_group[&#39;lr&#39;]=0.1\\*param\\_group[&#39;lr&#39;]model.train()foridx,(data,targets)inenumerate(trainloader):data,targets=data.cuda(),targets.cuda()y\\_pred=model(data)y\\_prob=torch.sigmoid(y\\_pred)loss=loss\\_fn(y\\_prob,targets)optimizer.zero\\_grad()loss.backward()optimizer.step()######\\*\\*\\*evaluation\\*\\*\\*##### evaluation on test setsmodel.eval()test\\_pred\\_list,test\\_true\\_list=[],[]withtorch.no\\_grad():forj,datainenumerate(testloader):test\\_data,test\\_targets=datatest\\_data=test\\_data.cuda()y\\_pred=model(test\\_data)y\\_prob=torch.sigmoid(y\\_pred)test\\_pred\\_list.append(y\\_prob.cpu().detach().numpy())test\\_true\\_list.append(test\\_targets.numpy())test\\_true=np.concatenate(test\\_true\\_list)test\\_pred=np.concatenate(test\\_pred\\_list)test\\_auc=auc\\_roc\\_score(test\\_true,test\\_pred)ifbest\\_test&lt;test\\_auc:best\\_test=test\\_auctorch.save(model.state\\_dict(),&#39;&#39;ce\\_pretrained\\_model.pth&#39;&#39;)model.train()print(&quot;epoch:%s, test\\_auc:%.4f, best\\_test\\_auc:%.4f, lr:%.4f&quot;%(epoch,test\\_auc,best\\_test,optimizer.param\\_groups[0][&#39;lr&#39;]))\n```\n```\nepoch:0,test\\_auc:0.6803,best\\_test\\_auc:0.6803,lr:0.0010epoch:1,test\\_auc:0.6997,best\\_test\\_auc:0.6997,lr:0.0010epoch:2,test\\_auc:0.7218,best\\_test\\_auc:0.7218,lr:0.0010epoch:3,test\\_auc:0.7204,best\\_test\\_auc:0.7218,lr:0.0010epoch:4,test\\_auc:0.7074,best\\_test\\_auc:0.7218,lr:0.0010epoch:5,test\\_auc:0.7808,best\\_test\\_auc:0.7808,lr:0.0010epoch:6,test\\_auc:0.8012,best\\_test\\_auc:0.8012,lr:0.0010epoch:7,test\\_auc:0.8222,best\\_test\\_auc:0.8222,lr:0.0010epoch:8,test\\_auc:0.8300,best\\_test\\_auc:0.8300,lr:0.0010epoch:9,test\\_auc:0.7981,best\\_test\\_auc:0.8300,lr:0.0010epoch:10,test\\_auc:0.8193,best\\_test\\_auc:0.8300,lr:0.0010epoch:11,test\\_auc:0.8430,best\\_test\\_auc:0.8430,lr:0.0010epoch:12,test\\_auc:0.8397,best\\_test\\_auc:0.8430,lr:0.0010epoch:13,test\\_auc:0.8459,best\\_test\\_auc:0.8459,lr:0.0010epoch:14,test\\_auc:0.8370,best\\_test\\_auc:0.8459,lr:...",
      "url": "https://docs.libauc.org/examples/auroc.html"
    },
    {
      "title": "A Look at The First Place Solution of a Dermatology Classification Kag",
      "text": "[Skip to main content](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#content)\n\nOne interesting thing I often think about is the gap between academic and real-world\nsolutions. In general academic solutions play in the realm of idealized problem\nspaces, removing themselves from needing to care about the messiness of the real-world.\n[Kaggle](https://www.kaggle.com/competitions)\ncompetitions are a (small) step in the right direction towards dealing with messiness,\nusually providing a true blind test set (vs. overused benchmarks), and opening a\nfew degrees of freedom in terms the techniques that can be used, which\nusually eschews novelty in favour of more robust methods. To this end, I\nthought it would be useful to take a look at a more realistic problem (via a\nKaggle competition) and understand the practical details that result in a\nsuperior solution.\n\nThis post will cover the [first place solution](https://arxiv.org/abs/2010.05351) \\[ [1](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#id2)\\] to the\n[SIIM-ISIC Melanoma Classification](https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview) \\[ [0](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#id1)\\] challenge.\nIn addition to using tried and true architectures (mostly EfficientNets), they\nhave some interesting tactics they use to formulate the problem, process the\ndata, and train/validate the model. I'll cover background on the\nML techniques, competition and data, architectural details, problem formulation, and\nimplementation. I've also run some experiments to better understand the\nbenefits of certain choices they made. Enjoy!\n\n## Table of Contents\n\n- [1 Background](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#background)\n\n  - [1.1 Inverted Residuals and Linear Bottlenecks (MobileNetV2)](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#inverted-residuals-and-linear-bottlenecks-mobilenetv2)\n\n  - [1.2 Squeeze and Excitation Optimization](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#squeeze-and-excitation-optimization)\n\n  - [1.3 EfficientNet](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#efficientnet)\n\n  - [1.4 Noisy Student](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#noisy-student)\n- [2 SIIM-ISIC Melanoma Classification 2020 Competition](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#siim-isic-melanoma-classification-2020-competition)\n\n- [3 Winning Solution](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#winning-solution)\n\n  - [3.1 Dataset Creation and Data Preprocessing](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#dataset-creation-and-data-preprocessing)\n\n  - [3.2 Validation Strategy](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#validation-strategy)\n\n  - [3.3 Architecture](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#architecture)\n\n  - [3.4 Augmentation](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#augmentation)\n\n  - [3.5 Prediction](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#prediction)\n\n  - [3.6 Training Details](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#training-details)\n- [4 Experiments](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#experiments)\n\n  - [4.1 Changes That Showed Improvement](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#changes-that-showed-improvement)\n\n  - [4.2 Changes That Did Not Showed Improvement](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#changes-that-did-not-showed-improvement)\n\n  - [4.3 Training Time](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#training-time)\n- [5 Miscellaneous Notes](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#miscellaneous-notes)\n\n- [6 Conclusion](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#conclusion)\n\n- [7 References](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#references)\n\n\n## [1 Background](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/\\#id8)\n\n### [1.1 Inverted Residuals and Linear Bottlenecks (MobileNetV2)](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/\\#id9)\n\nMobileNetV2 \\[ [2](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#id3)\\] introduced a new type of neural network architectural building\nblock often referred to as \"MBConv\". The two big innovations here are inverted residuals\nand linear bottlenecks.\n\nFirst to understand inverted residuals, let's take a look at the basic\nresidual block (also see my post on [ResNet](https://bjlkeng.io/posts/residual-networks/))\nshown in Listing 1. Two notable parts of the basic residual block are the\nfact that we reduce the number of channels to `squeeze` in the first\nlayer, and then grow the number of channels to `expand` in the last\nlayer. The squeeze operation is often known as a _bottleneck_ since we have\nfewer channels. The intuition here is to reduce the number of channels so that\nthe more expensive 3x3 convolution is cheaper. The other relevant part is that\nwe have the residual \"skip\" connection where we add the input to the\nresult of the transformations. Notice the residual connection connects the\nexpanded parts `x` and `m3`.\n\n```\ndef residual_block(x, squeeze=16, expand=64):\n    # x has 64 channels in this example\n    m1 = Conv2D(squeeze, (1,1), activation='relu')(x)\n    m2 = Conv2D(squeeze, (3,3), activation='relu')(m1)\n    m3 = Conv2D(expand, (1,1), activation='relu')(m2)\n    return Add()([m3, x])\n\n```\n\n**Listing 1: Example of a Basic Residual Block in Keras** (adapted from [source](https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5))\n\nNext, let's look the changes in an inverted residual block shown in Listing 2.\nHere we \"invert\" the residual connection where we are making the residual\nconnection between the bottleneck \"squeezed\" layers instead of \"expanded\"\nlayers. Recall that we'll eventually be stacking these blocks, so there will\nstill be alternations of squeezed (\"bottlenecks\") and expansion layers. The\ndifference with Listing 1 is that we'll be making residual connections between\nthe bottleneck layers instead of expansion layers.\n\n```\ndef inverted_residual_block(x, expand=64, squeeze=16):\n    # x has 16 channels in this example\n    m1 = Conv2D(expand, (1,1), activation='relu')(x)\n    m2 = DepthwiseConv2D((3,3), activation='relu')(m1)\n    m3 = Conv2D(squeeze, (1,1), activation='relu')(m2)\n    return Add()([m3, x])\n\n```\n\n**Listing 2: Example of an inverted residual block with depthwise convolution in Keras** (adapted from [source](http...",
      "url": "https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2203.14177] Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2203.14177\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2203.14177**(cs)\n[Submitted on 27 Mar 2022 ([v1](https://arxiv.org/abs/2203.14177v1)), last revised 4 Jul 2022 (this version, v3)]\n# Title:Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\nAuthors:[Dixian Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D),[Xiaodong Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X),[Tianbao Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+T)\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n[View PDF](https://arxiv.org/pdf/2203.14177)> > Abstract:\n> The area under the ROC curve (AUROC) has been vigorously applied for imbalanced classification and moreover combined with deep learning techniques. However, there is no existing work that provides sound information for peers to choose appropriate deep AUROC maximization techniques. In this work, we fill this gap from three aspects. (i) We benchmark a variety of loss functions with different algorithmic choices for deep AUROC optimization problem. We study the loss functions in two categories: pairwise loss and composite loss, which includes a total of 10 loss functions. Interestingly, we find composite loss, as an innovative loss function class, shows more competitive performance than pairwise loss from both training convergence and testing generalization perspectives. Nevertheless, data with more corrupted labels favors a pairwise symmetric loss. (ii) Moreover, we benchmark and highlight the essential algorithmic choices such as positive sampling rate, regularization, normalization/activation, and optimizers. Key findings include: higher positive sampling rate is likely to be beneficial for deep AUROC maximization; different datasets favors different weights of regularizations; appropriate normalization techniques, such as sigmoid and $\\ell_2$ score normalization, could improve model performance. (iii) For optimization aspect, we benchmark SGD-type, Momentum-type, and Adam-type optimizers for both pairwise and composite loss. Our findings show that although Adam-type method is more competitive from training perspective, but it does not outperform others from testing perspective. Comments:|32 pages|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2203.14177](https://arxiv.org/abs/2203.14177)[cs.LG]|\n|(or[arXiv:2203.14177v3](https://arxiv.org/abs/2203.14177v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2203.14177](https://doi.org/10.48550/arXiv.2203.14177)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Dixian Zhu [[view email](https://arxiv.org/show-email/be468da8/2203.14177)]\n**[[v1]](https://arxiv.org/abs/2203.14177v1)**Sun, 27 Mar 2022 00:47:00 UTC (558 KB)\n**[[v2]](https://arxiv.org/abs/2203.14177v2)**Tue, 29 Mar 2022 03:39:26 UTC (1,110 KB)\n**[v3]**Mon, 4 Jul 2022 18:49:54 UTC (1,114 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2203.14177)\n* [TeX Source](https://arxiv.org/src/2203.14177)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2203.14177&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2203.14177&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-03](https://arxiv.org/list/cs.LG/2022-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2203.14177?context=cs)\n[cs.AI](https://arxiv.org/abs/2203.14177?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.14177)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.14177)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.14177)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2203.14177&amp;description=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2203.14177&amp;title=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individu...",
      "url": "https://arxiv.org/abs/2203.14177"
    },
    {
      "title": "",
      "text": "Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and\nEmpirical Studies on Medical Image Classification\nZhuoning Yuan1, Yan Yan2, Milan Sonka1, Tianbao Yang1\nUniversity of Iowa1, Washington State University2\n{zhuoning-yuan, milan-sonka, tianbao-yang}@uiowa.edu, yan.yan1@wsu.edu\nAbstract\nDeep AUC Maximization (DAM) is a new paradigm for\nlearning a deep neural network by maximizing the AUC\nscore of the model on a dataset. Most previous works of\nAUC maximization focus on the perspective of optimiza\u0002tion by designing efficient stochastic algorithms, and stud\u0002ies on generalization performance of large-scale DAM on\ndifficult tasks are missing. In this work, we aim to make\nDAM more practical for interesting real-world applications\n(e.g., medical image classification). First, we propose a\nnew margin-based min-max surrogate loss function for\nthe AUC score (named as the AUC min-max-margin loss\nor simply AUC margin loss for short). It is more robust\nthan the commonly used AUC square loss, while enjoy\u0002ing the same advantage in terms of large-scale stochas\u0002tic optimization. Second, we conduct extensive empirical\nstudies of our DAM method on four difficult medical im\u0002age classification tasks, namely (i) classification of chest\nx-ray images for identifying many threatening diseases,\n(ii) classification of images of skin lesions for identifying\nmelanoma, (iii) classification of mammogram for breast\ncancer screening, and (iv) classification of microscopic im\u0002ages for identifying tumor tissue. Our studies demonstrate\nthat the proposed DAM method improves the performance\nof optimizing cross-entropy loss by a large margin, and\nalso achieves better performance than optimizing the ex\u0002isting AUC square loss on these medical image classifica\u0002tion tasks. Specifically, our DAM method has achieved the\n1st place on Stanford CheXpert competition on Aug. 31,\n2020. To the best of our knowledge, this is the first work that\nmakes DAM succeed on large-scale medical image datasets.\nWe also conduct extensive ablation studies to demonstrate\nthe advantages of the new AUC margin loss over the\nAUC square loss on benchmark datasets. The proposed\nmethod is implemented in our open-sourced library LibAUC\n(www.libauc.org) whose github address is https:\n//github.com/Optimization-AI/LibAUC.\nFigure 1. An illustrative example for optimizing different AUC\nlosses on a toy data for learning a two-layer neural network with\nELU activation. The top row is optimizing the AUC square loss\nand the bottom row is optimizing the new AUC margin loss. The\nfirst column depicts the initial decision boundary (dashed line) pre\u0002trained on a set of examples. In the middle column, we add some\neasy examples to the training set and retrain the model by optimiz\u0002ing the AUC loss. In the last column, we add some noisily labeled\ndata (blue circled data) to the training set and retrain the model by\noptimizing the AUC loss. The results demonstrate the new AUC\nmargin loss is more robust than the AUC square loss.\n1. Introduction\nIn the last decade, we have seen great progress in\ndeep learning (DL) techniques for medical image classi\u0002fication driven by large-scale medical datasets. For ex\u0002ample, Stanford machine learning group led by Andrew\nNg has collected and released a high-quality large-scale\nChest X-Ray dataset for detecting chest and lung diseases,\nwhich contains 224,316 high-quality X-rays images from\n65,240 patients [22]. Various deep learning methods have\nbeen designed and evaluated on this dataset by participat\u0002ing the CheXpert competition organized by Stanford ML\ngroup [22], and many of them have achieved radiologist\u0002level performance on detecting certain related diseases. Es\u0002teva et al. [10] have trained a CNN using a dataset of\n129,450 clinical images consisting of 2,032 different dis\u0002eases, and achieved dermatologist-level performance for\n3040\nclassification of skin lesions. Wu et al. [39] have trained\na deep neural network for breast cancer screening on a\nlarge-scale medical dataset, which includes 229,426 digital\nscreening mammography exams (1,001,093 images) from\n141,473 patients. Their model is as accurate as an experi\u0002enced radiologist. Despite these great efforts, an important\nquestion remains:\n\u201cCan we design a generic method that can further im\u0002prove the performance of DL on these medical datasets\nwithout relying on domain knowledge\u201d?\nIn this paper, we provide an affirmative answer to this\nquestion. Our solution is to optimize a novel loss for DL\ninstead of optimizing the standard cross-entropy loss in the\nprevious works. In particular, we choose to maximize the\nAUC score (a.k.a the area under the ROC curve) for DL.\nThere are several benefits of maximizing AUC score over\nminimizing the cross-entropy loss. First, in medical classifi\u0002cation tasks the AUC score is the default metric for evaluat\u0002ing and comparing different methods. Directly maximizing\nAUC score can potentially lead to the largest improvement\nin the model\u2019s performance. Second, the datasets in medical\nimage classification tasks are usually imbalanced (e.g., the\nnumber of malignant cases is usually much less than benign\ncases). AUC is more suitable for handling imbalanced data\ndistribution since maximizing AUC aims to rank the pred\u0002ication score of any positive data higher than any negative\ndata. However, AUC maximization is much more challeng\u0002ing than minimizing mis-classifcation error since AUC is\nmuch more sensitive to model change. A simple example\nin Appendix F shows that by changing the prediction scores\nof a few examples, the mis-classification error rate keep un\u0002changed but the AUC score drops significantly.\nAUC maximization has been studied in the community\nof machine learning [12, 41, 27, 23, 11]. However, existing\nmethods for AUC maximization are still not satisfactory for\npractical use. The foremost challenge for AUC maximiza\u0002tion is to determine a surrogate loss for the AUC score. A\nnaive way is to use a pairwise surrogate loss based on the\ndefinition of the AUC score. However, optimizing a generic\npairwise loss on training data suffers from a severe scalabil\u0002ity issue, which makes it not practical for DL on large-scale\ndatasets. Several studies have made attempts to address the\nscalability issue [23, 43, 41, 27]. One promising solution is\nto maximize the pairwise square loss for AUC by utilizing\nits special form [41, 27]. However, our study reveals that\nthe AUC square loss has adverse effect when trained with\neasy data and is sensitive to the noisy data.\nTo address these issues, we propose a new margin-based\nsurrogate loss in the min-max form for AUC (referred to\nas the AUC min-max-margin loss and the AUC margin loss\nfor short), which is inspired by addressing the two issues of\nthe AUC square loss. In particular, the AUC margin loss\nhas two features that can alleviate the two issues, making\nit more robust to noisy data and not adversely affected by\neasy data. We will explain it with more details in the tech\u0002nical section and use a toy example in Figure 1 to illustrate\nthe robustness of AUC margin loss over AUC square loss.\nMoreover, the min-max form of the AUC margin loss make\nit enjoy the same benefit as the AUC square loss in terms\nof scalability, making it more attractive than conventional\npairwise margin-based surrogate loss for AUC maximiza\u0002tion. In particular, we are able to directly employ existing\nlarge-scale optimization algorithms [15] designed for max\u0002imizing the AUC square loss to maximize our AUC margin\nloss with one line change of the code.\nTo demonstrate the effectiveness of our deep AUC maxi\u0002mization method, we conduct empirical studies on four dif\u0002ficult medical image classification tasks, namely classifica\u0002tion of X-ray images for detecting chest diseases, classi\u0002fication of images of skin lesions, classification of mam\u0002mograms for breast cancer screening and classification of\nmicroscopic images of tumor tissue. Our deep AUC maxi\u0002mization method has achieved great success on these diffi\u0002cult tasks. Spec...",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Large-Scale_Robust_Deep_AUC_Maximization_A_New_Surrogate_Loss_and_ICCV_2021_paper.pdf"
    },
    {
      "title": "ICCV 2021 Open Access Repository",
      "text": "ICCV 2021 Open Access Repository\n[![ICCV 2021](https://openaccess.thecvf.com/img/iccv2021_logo.png)](https://iccv2021.thecvf.com/)[![CVF](https://openaccess.thecvf.com/img/cropped-cvf-s.jpg)](https://www.thecvf.com/)\n[ICCV 2021](https://iccv2021.thecvf.com/)[open access](https://openaccess.thecvf.com/menu)\nThese ICCV 2021 papers are the Open Access versions, provided by the[Computer Vision Foundation.](https://www.thecvf.com/)\nExcept for the watermark, they are identical to the accepted versions;\nthe final published version of the proceedings is available on IEEE Xplore.\nThis material is presented to ensure timely dissemination of scholarly and technical work.\nCopyright and all rights therein are retained by authors or by other copyright holders.\nAll persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright.\n**Powered by:**\n![Microsoft Azure](https://openaccess.thecvf.com/img/ms-azure-logo.png)\n**Sponsored by:**\n![Amazon](https://openaccess.thecvf.com/img/amazon-logo.png)![Facebook](https://openaccess.thecvf.com/img/facebook_logo.jpg)![Google](https://openaccess.thecvf.com/img/Google_2015_logo.svg)\nLarge-Scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification\n***Zhuoning Yuan, Yan Yan, Milan Sonka, Tianbao Yang***; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 3040-3049\n**Abstract**\nDeep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as the AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org) whose github address is https://github.com/Optimization-AI/LibAUC.\n**Related Material**\n[[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Large-Scale_Robust_Deep_AUC_Maximization_A_New_Surrogate_Loss_and_ICCV_2021_paper.pdf)]\r[[supp](https://openaccess.thecvf.com/content/ICCV2021/supplemental/Yuan_Large-Scale_Robust_Deep_ICCV_2021_supplemental.pdf)]\r[[arXiv](http://arxiv.org/abs/2012.03173)]\n[bibtex]\n@InProceedings{Yuan\\_2021\\_ICCV,\nauthor = {Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},\ntitle = {Large-Scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification},\nbooktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\nmonth = {October},\nyear = {2021},\npages = {3040-3049}\n}",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Large-Scale_Robust_Deep_AUC_Maximization_A_New_Surrogate_Loss_and_ICCV_2021_paper.html"
    }
  ]
}