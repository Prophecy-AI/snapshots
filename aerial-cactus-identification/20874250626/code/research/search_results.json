{
  "query": "What are winning approaches for image classification competitions on Kaggle with small 32x32 images?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for 32\u202f\u00d7\u202f32 image tasks (e.g., CIFAR\u201110 style competitions) combine a **strong, well\u2011regularized CNN baseline** with aggressive **data\u2011augmentation, careful hyper\u2011parameter tuning, and model\u2011level ensembling**.  A common starting point is a small\u2011resolution\u2011friendly architecture such as ResNet\u201118/34, Wide\u2011ResNet, or EfficientNet\u2011B0 that is either trained from scratch or fine\u2011tuned from ImageNet weights using the \u201cbottleneck\u2011features + top\u2011layer fine\u2011tuning\u201d recipe described by Chollet\u202f(2016)\u202f([keras.io](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)).  Because 32\u202f\u00d7\u202f32 images provide very little spatial information, heavy augmentation (random crops, horizontal flips, color jitter, CutMix/MixUp) is essential to prevent over\u2011fitting and to expose the model to diverse patterns\u202f([Towards Data Science, 2019](https://towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327)).  \n\nThe next decisive factor is **systematic hyper\u2011parameter search** on a held\u2011out validation split.  Brigato et\u202fal. (2022) showed that a thoroughly tuned cross\u2011entropy baseline can match or surpass most published \u201cspecialised\u201d methods on small\u2011dataset benchmarks, highlighting that careful learning\u2011rate schedules (cosine decay with warm\u2011up), optimizer choice (AdamW or SGD with momentum), weight decay, and label\u2011smoothing often close the performance gap\u202f([arXiv\u202f2212.12478](https://arxiv.org/abs/2212.12478)).  In practice, teams run multiple seeds, use stochastic weight averaging, and blend predictions from several top\u2011performing checkpoints (test\u2011time augmentation) to squeeze out the last few points.  \n\nFinally, **engineering best practices**\u2014such as using PyTorch\u202fLightning for reproducible training loops, mixed\u2011precision to speed up epochs, and spot\u2011instance clusters for large\u2011scale sweeps\u2014have become standard in recent Kaggle wins (e.g., the Plant\u2011Pathology 2021 challenge) and translate directly to 32\u202f\u00d7\u202f32 contests\u202f([PyTorch Lightning blog, 2021](https://devblog.pytorchlightning.ai/practical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429)).  Together, these ingredients\u2014robust small\u2011CNNs, aggressive augmentation, exhaustive tuning, and ensembling\u2014form the core recipe for top\u2011ranking Kaggle image\u2011classification solutions on tiny images.",
      "url": ""
    },
    {
      "title": "Image Classification with Small Datasets: Overview and Benchmark",
      "text": "[View PDF](https://arxiv.org/pdf/2212.12478)\n\n> Abstract:Image classification with small datasets has been an active research area in the recent past. However, as research in this scope is still in its infancy, two key ingredients are missing for ensuring reliable and truthful progress: a systematic and extensive overview of the state of the art, and a common benchmark to allow for objective comparisons between published methods. This article addresses both issues. First, we systematically organize and connect past studies to consolidate a community that is currently fragmented and scattered. Second, we propose a common benchmark that allows for an objective comparison of approaches. It consists of five datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types (RGB, grayscale, multispectral). We use this benchmark to re-evaluate the standard cross-entropy baseline and ten existing methods published between 2017 and 2021 at renowned venues. Surprisingly, we find that thorough hyper-parameter tuning on held-out validation data results in a highly competitive baseline and highlights a stunted growth of performance over the years. Indeed, only a single specialized method dating back to 2019 clearly wins our benchmark and outperforms the baseline classifier.\n\n## Submission history\n\nFrom: Lorenzo Brigato \\[ [view email](https://arxiv.org/show-email/4f3ed004/2212.12478)\\]\n\n**\\[v1\\]**\nFri, 23 Dec 2022 17:11:16 UTC (3,741 KB)",
      "url": "https://arxiv.org/abs/2212.12478"
    },
    {
      "title": "Practical Lighting Tips to Rank on Kaggle Image Challenges",
      "text": "<div><div><p></p><h2>Practical Lighting Tips to Rank on Kaggle Image Challenges</h2><p></p><div><h2>This post outlines some PyTorch Lightning best practices from the recent Kaggle Plant Pathology image classification challenge that enabled us to reach the leaderboard.</h2><div><a href=\"https://medium.com/@jborovec?source=post_page---byline--242e2e533429---------------------------------------\"><div><p></p></div></a></div></div><p>We recently competed in the Kaggle <a href=\"https://www.kaggle.com/c/plant-pathology-2021-fgvc8\">Plant Pathology 2021 \u2014 FGVC8</a> challenge to detect 5 plant diseases using computer vision on images of leaves from apple trees (see the <a href=\"https://www.kaggle.com/c/plant-pathology-2021-fgvc8\">full challenge description</a>).</p><figure><figcaption>Label count histogram over the whole dataset.</figcaption></figure><p>In this post, we will walk through some <a href=\"https://github.com/PyTorchLightning/pytorch-lightning\">Lighting</a> Best Practices and show you how a few small changes to your code can significantly reduce training time and get you on the Kaggle leaderboards.</p><blockquote><p>All code snippets, notebooks and additional explanations in this post can be found in the GitHub repository below:</p></blockquote><h2>Explore the Task and Given Dataset</h2><p>There is an adage \u2014 \u201cto win a battle, you must first know your enemy\u201d. This adage applies to any machine learning and data science task.</p><p>Unfortunately, in Kaggle, proper data analysis is commonly overlooked. Novice Kagglers hungry for quick often blindly apply out-of-the-box tools on data using default configurations and are then surprised when this does not work as expected.</p><p>Even though Kaggle often provides already cleaned data with task descriptions, you should perform at least some minimal data exploration before doing any modelling.</p><blockquote><p><a href=\"https://github.com/PyTorchLightning/lightning-flash\">PyTorch Lightning Flash</a> contains easy-to-use <a href=\"https://lightning-flash.readthedocs.io/en/latest/template/data.html?highlight=visual+callback#basevisualization\">visualization callbacks</a>, which are useful to visually inspect the impact of different transforms on your data before launching a training.</p></blockquote><figure><figcaption>You can implement custom visualization by subclassing a <code><a href=\"https://lightning-flash.readthedocs.io/en/latest/api/generated/flash.core.data.base_viz.BaseVisualization.html#flash.core.data.base_viz.BaseVisualization\">BaseVisualization</a></code> callback- override any of them <code>show_{preprocess_hook_name}</code> to get the associated data and visualize it.</figcaption></figure><h2>Single vs Multi-Label Classification</h2><figure><figcaption>Showing sample images and counts per each concatenated class combination.</figcaption></figure><p>The Plant Pathology dataset provides annotated text labels in a CSV table. In each row of the table, an image name is paired with a string containing concatenated labels. The illustration (left figure) visualizes a naive data grouping by a unique string, combining multiple labels.</p><p>We noticed that several submissions used these concatenated strings as submission labels, treating the task as a single label classification problem.</p><p>Treating the challenge as a single label classification task raises a few issues:<br/>a) The classification decision space, represented as one-hot vectors, comprises 12 classes instead of 6.<br/>b) The dataset is unbalanced as many combinations are not accurately represented in the training data.<br/>c) Some label combinations do not exist in training data but may appear in testing data.</p><p>By simply splitting our labels and using a multi-label classification model with a <code>ResNet50</code> backbone from <code>torchvision</code> library, we were able to rank first with just the second submission and with a sufficient margin of about 8% in the F1 score; see leaderboard snapshot below.</p><figure><figcaption>Challenge snapshot with our first multi-label submission with base model ResNet50.</figcaption></figure><blockquote><p>Note while it wasn\u2019t available during the competition, today <a href=\"https://github.com/PyTorchLightning/lightning-flash\">Lightning Flash</a> supports Multi-Label Classification on all Timm and Torch Vision backbones out of the box.</p></blockquote><h2>Downscaling and Augmenting Images</h2><p>We used a standard <code>ResNet50</code> image classification model with simple data augmentation techniques to improve our baseline to increase robustness and prevent overfitting.</p><p>We applied simple image flipping and perspective deformation to augment our training images and extend their variability to train a more generalizable model.</p><pre><span><em># transforms to extend the dataset variability</em><br/>T.RandomRotation(degrees=30),<br/>T.RandomPerspective(distortion_scale=0.4),<br/>T.RandomHorizontalFlip(p=0.5),<br/>T.RandomVerticalFlip(p=0.5),</span><span><em># required as ResNet50 has input 224x224</em><br/>T.RandomResizedCrop(size=224),</span><span><em># standardization</em><br/>T.ToTensor(),<br/>T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),</span></pre><p>Using this naive augmentation, we struggled with the following issues:<br/> 1. The model performed initially worse than expected.<br/> 2. Training took about 2 hours per epoch on the Kaggle GPU kernels</p><p>Since image augmentation is a best practice, at first, these observations surprised us. However, upon deeper analysis, we found that most of the images in the Plant Pathology dataset were 4k resolution.</p><figure><figcaption>HIstogram of image sizes from the Plant dataset.</figcaption></figure><p>Applying a <a href=\"https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomResizedCrop\">RandomResizedCrop</a> transform on a 4k image often crops out a background image section. To address this issue, we resized all images before making the final crop.</p><p>To prevent wasting hours resizing the full dataset on each epoch, we moved the resizing to the beginning of the data pipeline as a one-time preprocessing step. In the end, we downscaled all <a href=\"https://www.kaggle.com/jirkaborovec/plant-pathology-fgvc78-640px\">datasets offline to 640px</a> and reuploaded the downscaled images to Kaggle. These changes significantly lowered our training time from 2hrs to ~7min per epoch with no performance degradation.</p><h2>Visualizing Data Abstraction Layers and Performance Boosting</h2><p>Pytorch Lightning significantly simplifies training and related data handling (e.g. loop over epochs, moving batches to proper device, etc.).</p><h2>Data Streaming and Validation</h2><p>Using Lightning requires basic data logic such as coupling images with labels (annotation) using pure PyTorch. Data handling issues are not trivial to detect, especially when the only sign of a full pipeline is that it does not train/learn as expected.</p><p>It is a best practice prefer to verify and fully control each level of data abstraction (<code>Dataset</code> \u2192 <code>dataloader</code> \u2192<code>LightningDataModule</code>). This avoids tedious and painful debugging later on. To this end, you should visualize data after defining each data wrapper in a PyTorch<code>Dataset</code> and <code>dataloader</code>.</p><p>Let's assume we inherited from the standard PyTorch <code>Dataset</code> and implement the required <code>__getitem__</code>method, then we can list first a few samples with annotation as follows:</p><pre><span>fig = plt.figure(figsize=(9, 6))<strong><br/>for</strong> i <strong>in</strong> range(9): <em># list first 9 images</em><br/> img, lb = dataset[i]<br/> ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])<br/> ax.imshow(img)<br/> ax.set_title(lb)</span></pre><figure><figcaption>Visualization of the code snapshot above shows a few first images from the dataset with labels above in binary encoding.</figcaption></figure><p...",
      "url": "https://devblog.pytorchlightning.ai/practical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429"
    },
    {
      "title": "Building powerful image classification models using very little data",
      "text": "**Note: this post was originally written in June 2016. It is now very outdated. Please see**\n**[this guide to fine-tuning](https://keras.io/guides/transfer_learning/)**\n**for an up-to-date alternative, or check out chapter 8 of my book \"Deep Learning with Python (2nd edition)\".**\n\nIn this tutorial, we will present a few simple yet effective methods that you can use to build a powerful image classifier, using only very few training examples --just a few hundred or thousand pictures from each class you want to be able to recognize.\n\nWe will go over the following options:\n\n- training a small network from scratch (as a baseline)\n- using the bottleneck features of a pre-trained network\n- fine-tuning the top layers of a pre-trained network\n\nThis will lead us to cover the following Keras features:\n\n- `fit_generator` for training Keras a model using Python data generators\n- `ImageDataGenerator` for real-time data augmentation\n- layer freezing and model fine-tuning\n- ...and more.\n\n## Our setup: only 2000 training examples (1000 per class)\n\nWe will start from the following setup:\n\n- a machine with Keras, SciPy, PIL installed. If you have a NVIDIA GPU that you can use (and cuDNN installed), that's great, but since we are working with few images that isn't strictly necessary.\n- a training data directory and validation data directory containing one subdirectory per image class, filled with .png or .jpg images:\n\n```\ndata/\n    train/\n        dogs/\n            dog001.jpg\n            dog002.jpg\n            ...\n        cats/\n            cat001.jpg\n            cat002.jpg\n            ...\n    validation/\n        dogs/\n            dog001.jpg\n            dog002.jpg\n            ...\n        cats/\n            cat001.jpg\n            cat002.jpg\n            ...\n```\n\nTo acquire a few hundreds or thousands of training images belonging to the classes you are interested in, one possibility would be to use the [Flickr API](https://www.flickr.com/services/api/) to download pictures matching a given tag, under a friendly license.\n\nIn our examples we will use two sets of pictures, which we got [from Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data): 1000 cats and 1000 dogs (although the original dataset had 12,500 cats and 12,500 dogs, we just took the first 1000 images for each class). We also use 400 additional samples from each class as validation data, to evaluate our models.\n\nThat is very few examples to learn from, for a classification problem that is far from simple. So this is a challenging machine learning problem, but it is also a realistic one: in a lot of real-world use cases, even small-scale data collection can be extremely expensive or sometimes near-impossible (e.g. in medical imaging). Being able to make the most out of very little data is a key skill of a competent data scientist.\n\nHow difficult is this problem? When Kaggle started the cats vs. dogs competition (with 25,000 training images in total), a bit over two years ago, it came with the following statement:\n\n_\"In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459._\n_The current literature suggests machine classifiers can score above 80% accuracy on this task [\\[ref\\]](http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf).\"_\n\nIn the resulting competition, top entrants were able to score over 98% accuracy by using modern deep learning techniques. In our case, because we restrict ourselves to only 8% of the dataset, the problem is much harder.\n\n## On the relevance of deep learning for small-data problems\n\nA message that I hear often is that \"deep learning is only relevant when you have a huge amount of data\". While not entirely incorrect, this is somewhat misleading. Certainly, deep learning requires the ability to learn features automatically from the data, which is generally only possible when lots of training data is available --especially for problems where the input samples are very high-dimensional, like images. However, convolutional neural networks --a pillar algorithm of deep learning-- are by design one of the best models available for most \"perceptual\" problems (such as image classification), even with very little data to learn from. Training a convnet from scratch on a small image dataset will still yield reasonable results, without the need for any custom feature engineering. Convnets are just plain good. They are the right tool for the job.\n\nBut what's more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes, as we will see in this post. Specifically in the case of computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data.\n\n## Data pre-processing and data augmentation\n\nIn order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n\nIn Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class. This class allows you to:\n\n- configure random transformations and normalization operations to be done on your image data during training\n- instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs, `fit_generator`, `evaluate_generator` and `predict_generator`.\n\nLet's look at an example right away:\n\n```\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n```\n\nThese are just a few of the options available (for more, see [the documentation](http://keras.io/preprocessing/image/)). Let's quickly go over what we just wrote:\n\n- `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures\n- `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n- `rescale` is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n- `shear_range` is for randomly applying [shearing transformations](https://en.wikipedia.org/wiki/Shear_mapping)\n- `zoom_range` is for randomly zooming inside pictures\n- `horizontal_flip` is for randomly flipping half of the images horizontally --relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n- `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n\nNow let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing --we disable rescaling in this case to keep the images displayable:\n\n```\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ndatagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n       ...",
      "url": "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
    },
    {
      "title": "Latest Winning Techniques for Kaggle Image Classification with Limited Data",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5259e7736327&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Latest Winning Techniques for Kaggle Image Classification with Limited Data\n\n## Tutorial on how to prevent your model from overfitting on a small dataset but still make accurate classifications\n\n[![Kayo Yin](https://miro.medium.com/v2/resize:fill:88:88/2*RgAwO4wXM60OTwi2_kFWCg.jpeg)](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Kayo Yin](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nOct 21, 2019\n\n--\n\n5\n\nShare\n\n[http://www.dakanarts.com/13-black-white/](http://www.dakanarts.com/13-black-white/)\n\nIn this article, I will go through the approach I used for an [in-class Kaggle challenge](https://www.kaggle.com/c/cs-ioc5008-hw1/overview). I spent about two weeks on the challenge, with a final submission score of **0.97115** which places me second on the final leaderboard. I started off by borrowing ideas from [this article](https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86), which I recommend as well.\n\n# Challenge Introduction\n\nThe proposed challenge is a natural images classification task with **13** **classes**. The first difficulty in this challenge is the **scarcity** of available data: only 3 859 images for training. The rules of the challenge was not to use external data during training as well. With little data, the model will be more prone to overfitting without learning to generalize.\n\nMoreover, because these images are in **grayscale**, they contain less information than color images such as the ImageNet dataset, so a pre-trained model on color images cannot be applied directly to this task. Upon further inspection of the dataset, many classes contain images that are visually very **similar** or containing the same elements. The model will lose accuracy when\u2026\n\n[![Kayo Yin](https://miro.medium.com/v2/resize:fill:144:144/2*RgAwO4wXM60OTwi2_kFWCg.jpeg)](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------follow_profile-----------)\n\n[**Written by Kayo Yin**](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)\n\n[575 Followers](https://medium.com/@kayo.yin/followers?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\nPhD student at UC Berkeley researching AI. Now writing at [kayoyin.github.io/blog](http://kayoyin.github.io/blog)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----5259e7736327--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----5259e7736327--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----5259e7736327--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5259e7736327--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----5259e7736327--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5259e7736327--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5259e7736327--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----5259e7736327--------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----5259e7736327--------------------------------)",
      "url": "https://towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327?gi=0580a8bafd4a"
    },
    {
      "title": "Kaggle Competition \u2014 Image Classification - Towards Data Science",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# Kaggle Competition \u2013 Image Classification\n\nTo build a model that can predict the classification of the input images\n\n[Admond Lee](https://towardsdatascience.com/author/admond1994/)\n\nNov 10, 2018\n\n7 min read\n\nShare\n\n**First misconception** \u2013 [Kaggle](https://www.kaggle.com/) is a website that hosts machine learning competitions. And I believe this misconception makes a lot of beginners in data science \u2013 including me \u2013 think that Kaggle is only for data professionals or experts with years of experience. In fact, Kaggle has much more to offer than solely competitions!\n\nThere are so many [open datasets on Kaggle](https://www.kaggle.com/datasets) that we can simply start by playing with a dataset of our choice and learn along the way. If you are a beginner with zero experience in data science and might be thinking to take more online courses before joining it, think again! Kaggle even offers you some fundamental yet practical [programming and data science courses](https://www.kaggle.com/learn/overview). Besides, you can always post your questions in the [Kaggle discussion](https://www.kaggle.com/discussion) to seek advice or clarification from the vibrant data science community for any data science problems.\n\nOne of the quotes that really enlightens me was shared by Facebook founder and CEO Mark Zuckerberg in his [commencement address at Harvard](https://news.harvard.edu/gazette/story/2017/05/mark-zuckerbergs-speech-as-written-for-harvards-class-of-2017/)\n\n> You just have to get started.\n>\n> - Mark Zuckerberg\n\nGetting started and making the very first step has always been the hardest part before doing anything, let alone making progression or improvement.\n\nThere are so many online resources to help us get started on Kaggle and I\u2019ll list down a few resources here which I think they are extremely useful:\n\n1. [Use Kaggle to start (and guide) your ML/ Data Science journey \u2013 Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35)\n2. [Machine Learning Zero-to-Hero](https://towardsdatascience.com/machine-learning-zero-to-hero-everything-you-need-in-order-to-compete-on-kaggle-for-the-first-time-18644e701cf1)\n3. [Data Science A-Z from Zero to Kaggle Kernels Master](https://towardsdatascience.com/data-science-from-zero-to-kaggle-kernels-master-f9115eadbb3)\n\nIn the following section, I hope to share with you the journey of a beginner in his first Kaggle competition (together with his team members) along with some mistakes and takeaways. You can [check out the codes here](https://www.kaggle.com/chloekexin/da-machine). The sections are distributed as below:\n\n1. The context of the Competition and Data\n2. Approach\n3. Results\n4. Final Thoughts\n\nLet\u2019s get started and I hope you\u2019ll enjoy it!\n\n## Context of the Competition and Data\n\nIn my very first post on Medium \u2013 [My Journey from Physics into Data Science](https://towardsdatascience.com/my-journey-from-physics-into-data-science-5d578d0f9aa6), I mentioned that I joined my [first Kaggle machine learning competition](https://www.kaggle.com/c/shopee-iet-machine-learning-competition) organized by [Shopee](https://shopee.sg/) and Institution of Engineering and Technology (IET) with my fellow team members \u2013 **[Low Wei Hong](https://www.linkedin.com/in/lowweihong/), [Chong Ke Xin](https://www.linkedin.com/in/kexinchong/),** and **[Ling Wei Onn](https://www.linkedin.com/in/ling-wei-onn-452957147/)**. We had a lot of fun throughout the journey and I definitely learned so much from them!!\n\nWe were given merchandise images by Shopee with 18 categories and our aim was to build a model that can predict the classification of the input images to different categories.\n\nGreat. Now that we have an understanding of the context. Let\u2019s move on to our approach for image classification prediction \u2013 which is the _**FUN (I mean hardest)**_ part!\n\n### Some Images for Classification\n\nDifferent Images for Classification\n\nAs you can see from the images, there were some noises (different background, description, or cropped words) in some images, which made the image preprocessing and model building even more harder.\n\nIn the next section I\u2019ll talk about our approach to tackle this problem until the step of building our customized CNN model.\n\n## Approach\n\n[(Source)](https://unsplash.com/photos/oVpn10bqxkc)\n\nWhenever people talk about image classification, **Convolutional Neural Networks (CNN)** will naturally come to their mind \u2013 and not surprisingly \u2013 we were no exception.\n\nWith little knowledge and experience in CNN for the first time, **Google** was my best teacher and I couldn\u2019t help but to highly recommend this concise yet comprehensive introduction to CNN written by [Adit Deshpande](https://www.linkedin.com/in/aditdeshpande/). The high level explanation broke the once formidable structure of CNN into simple terms that I could understand.\n\n### Image Preprocessing\n\nImage preprocessing can also be known as [data augmentation](https://keras.io/preprocessing/image/).\n\nGenerate batches of tensor image data with real-time data augmentation that will be looped over in batches\n\nThe data augmentation step was necessary before feeding the images to the models, particularly for the given \\_\\_ [imbalanced and limited dataset](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced). Through artificially expanding our dataset by means of different transformations, scales, and shear range on the images, we increased the number of training data.\n\n### \u2013 FIRST Mistake \u2013\n\nI believe every approach comes from multiple tries and mistakes behind. So let\u2019s talk about our first mistake before diving in to show our final approach.\n\n**We began by trying to build our CNN model from scratch** (Yes literally!) to see how the CNN model performed based on the training and testing images. Little did we know that most people rarely train a CNN model from scratch with the following reasons:\n\n1. Insufficient dataset (training images)\n2. CNN models are complex and normally take weeks \u2013 or even months \u2013 to train despite we have clusters of machines and high performance GPUs.\n3. The costs and time don\u2019t guarantee and justify the model\u2019s performance\n\nFortunately, [transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) came to our rescue.\n\n### Transfer Learning\n\nSo\u2026 What the _heck_ is transfer learning?\n\n**[Transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)** is a machine learning [method](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/) where a model developed for a task is reused as the starting point for a model on a second task. In our case, it is the method of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset previously) and \"fine-tuning\" the model with our own dataset.\n\nGreat. With so [many pre-trained models available in Keras](https://keras.io/applications/), we decided to try different pre-trained models _separately_ **(VGG16, VGG19, ResNet50, InceptionV3, DenseNet etc.)** and selected the best model.\n\n[Inception V3 Google Research](https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8)\n\nEventually we selected **[InceptionV3 model](https://keras.io/applications/#inceptionv3)**, with weights pre-trained on [ImageNet](http://image-net.org/), which had the highest accuracy.\n\nApologies for the never-ending comments as we wanted to make sure every single line was correct.\n\nAt first glance the codes might seem a bit confusing. Let\u2019s break it down this way to make things more clearer with the logic explained below:\n\n1. We first created a base model using the pre-trained InceptionV3 model imported earlier. The fully connected last layer was removed at the top of ...",
      "url": "https://towardsdatascience.com/kaggle-competition-image-classification-676dee6c0f23?gi=99076bd05cd0"
    },
    {
      "title": "Kaggle #1 Winning Approach for Image Classification Challenge",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9c1188157a86&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fneuralspace%2Fkaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fneuralspace%2Fkaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Kaggle \\#1 Winning Approach for Image Classification Challenge\n\n[![Kumar Shridhar](https://miro.medium.com/v2/resize:fill:88:88/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg)](https://medium.com/@shridhar743?source=post_page-----9c1188157a86--------------------------------)[![NeuralSpace](https://miro.medium.com/v2/resize:fill:48:48/1*tDlpVCxEvFUKN5FdYZMAJQ.png)](https://medium.com/neuralspace?source=post_page-----9c1188157a86--------------------------------)\n\n[Kumar Shridhar](https://medium.com/@shridhar743?source=post_page-----9c1188157a86--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7ad13abfbb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fneuralspace%2Fkaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86&user=Kumar+Shridhar&userId=c7ad13abfbb7&source=post_page-c7ad13abfbb7----9c1188157a86---------------------post_header-----------)\n\nPublished in\n\n[NeuralSpace](https://medium.com/neuralspace?source=post_page-----9c1188157a86--------------------------------)\n\n\u00b7\n\n11 min read\n\n\u00b7\n\nJun 20, 2018\n\n--\n\n19\n\nShare\n\nThis post is about the approach I used for the Kaggle competition: [Plant Seedlings Classification.](https://www.kaggle.com/c/plant-seedlings-classification) I was the #1 in the ranking for a couple of months and finally ending with #5 upon final evaluation. The approach is pretty generic and can be used for other Image Recognition tasks as well.\n\n> **Kaggle** is a platform for [predictive modelling](https://en.wikipedia.org/wiki/Predictive_modelling) and [analytics](https://en.wikipedia.org/wiki/Analytics) competitions in which statisticians and data miners compete to produce the best models for predicting and describing the datasets uploaded by companies and users. This [crowdsourcing](https://en.wikipedia.org/wiki/Crowdsourcing) approach relies on the fact that there are countless strategies that can be applied to any predictive modelling task and it is impossible to know beforehand which technique or analyst will be most effective.\\[1\\]\n\nAlso, check out the blog that achieves **State of the Art results** in **Intent Classification** task on NLP:\n\n[**Know your Intent: SoTA results in Intent Classification** \\\n\\\n**This blog post shows the latest state-of-the-art results obtained on the three corpora:**\\\n\\\nmedium.com](https://medium.com/@shridhar743/know-your-intent-sota-results-in-intent-classification-8e1ca47f364c?source=post_page-----9c1188157a86--------------------------------)\n\n# TASK OVERVIEW\n\n> Can you differentiate a weed from a crop seedling?\n\nThe ability to do so effectively can mean better crop yields and better stewardship of the environment.\n\nThe **Aarhus University Signal Processing** group, in collaboration with **University**\u2026\n\n[![Kumar Shridhar](https://miro.medium.com/v2/resize:fill:144:144/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg)](https://medium.com/@shridhar743?source=post_page-----9c1188157a86--------------------------------)[![NeuralSpace](https://miro.medium.com/v2/resize:fill:64:64/1*tDlpVCxEvFUKN5FdYZMAJQ.png)](https://medium.com/neuralspace?source=post_page-----9c1188157a86--------------------------------)\n\nFollow\n\n[**Written by Kumar Shridhar**](https://medium.com/@shridhar743?source=post_page-----9c1188157a86--------------------------------)\n\n[2.3K Followers](https://medium.com/@shridhar743/followers?source=post_page-----9c1188157a86--------------------------------)\n\n\u00b7Editor for\n\n[NeuralSpace](https://medium.com/neuralspace?source=post_page-----9c1188157a86--------------------------------)\n\nNLP Researcher @ { [NeuralSpace.ai](http://NeuralSpace.ai) , ETH Z\u00fcrich} \\| [kumar-shridhar.github.io](http://kumar-shridhar.github.io)\n\nFollow\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----9c1188157a86--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----9c1188157a86--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----9c1188157a86--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9c1188157a86--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----9c1188157a86--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----9c1188157a86--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9c1188157a86--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9c1188157a86--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----9c1188157a86--------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----9c1188157a86--------------------------------)",
      "url": "https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86"
    },
    {
      "title": "CIFAR-10 Image Classification",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/mu-cifar10#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/competitions/mu-cifar10)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/mu-cifar10#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcompetitions%2Fmu-cifar10)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcompetitions%2Fmu-cifar10)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\nRobbie Beane \u00b7 Community Prediction Competition \u00b7 26 years to go\n\nJoin Competitionmore\\_horiz\n\n# CIFAR-10 Image Classification\n\nImage Classification\n\n![](https://www.kaggle.com/competitions/34091/images/header)\n\n## CIFAR-10 Image Classification\n\n[Overview](https://www.kaggle.com/competitions/mu-cifar10/overview) [Data](https://www.kaggle.com/competitions/mu-cifar10/data) [Code](https://www.kaggle.com/competitions/mu-cifar10/code) [Models](https://www.kaggle.com/competitions/mu-cifar10/models) [Discussion](https://www.kaggle.com/competitions/mu-cifar10/discussion) [Leaderboard](https://www.kaggle.com/competitions/mu-cifar10/leaderboard) [Rules](https://www.kaggle.com/competitions/mu-cifar10/rules)\n\n## Overview\n\nStart\n\n3 years ago\n\n###### Close\n\n26 years to go\n\n### Description\n\nlink\n\nkeyboard\\_arrow\\_up\n\n# CIFAR-10\n\nThe [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset is a dataset commonly used to practice creating image classification models. It consists of 32x32 color images of objects and animals from the following 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset contains 60,000 images, with 50,000 images used for training and 10,000 images used for testing.\n\n![](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3649/media/cifar-10.png)\n\n### Evaluation\n\nlink\n\nkeyboard\\_arrow\\_up\n\nSubmissions in this competition will be scored based on classification accuracy.\nSubmission files should should be formatted as follows:\n\n```\nfilename,label\ntest_00001.png,0\ntest_00002.png,0\n...\ntest_10000.png,0\n\n```\n\n### Citation\n\nlink\n\nkeyboard\\_arrow\\_up\n\nRobbie Beane. CIFAR-10 Image Classification. https://kaggle.com/competitions/mu-cifar10, 2022. Kaggle.\n\nCite\n\n## Competition Host\n\nRobbie Beane\n\n## Prizes & Awards\n\nKudos\n\nDoes not award Points or Medals\n\n## Participation\n\n96 Entrants\n\n64 Participants\n\n64 Teams\n\n105 Submissions\n\n## Tags\n\nCategorization Accuracy",
      "url": "https://www.kaggle.com/competitions/mu-cifar10"
    },
    {
      "title": "Best Practices to Rank on Kaggle Competition with PyTorch Lightning and Spot Instances",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F54aa5248aa8e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fbest-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fbest-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fbest-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fbest-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nIllustration photo by [George Becker](https://www.pexels.com/@eye4dtail?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/kick-chess-piece-standing-131616/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).\n\n# Best Practices to Rank on Kaggle Competition with PyTorch Lightning and Spot Instances\n\n## Complete the data science cycle of solving the Image classification challenge with an interactive session, hyper-parameter fine-tuning on scaled spot instances, and submitting the best results/predictions.\n\n[![\u30a4\u30eb\u30ab Borovec](https://miro.medium.com/v2/resize:fill:88:88/0*Nlhuh10Ni5X6UKG9.)](https://medium.com/@jborovec?source=post_page-----54aa5248aa8e--------------------------------)[![PyTorch Lightning Developer Blog](https://miro.medium.com/v2/resize:fill:48:48/1*uSM9QqCl2DJtejQSnwkCnw.png)](https://devblog.pytorchlightning.ai/?source=post_page-----54aa5248aa8e--------------------------------)\n\n[\u30a4\u30eb\u30ab Borovec](https://medium.com/@jborovec?source=post_page-----54aa5248aa8e--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa89f75826628&operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fbest-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e&user=%E3%82%A4%E3%83%AB%E3%82%AB+Borovec&userId=a89f75826628&source=post_page-a89f75826628----54aa5248aa8e---------------------post_header-----------)\n\nPublished in\n\n[PyTorch Lightning Developer Blog](https://devblog.pytorchlightning.ai/?source=post_page-----54aa5248aa8e--------------------------------)\n\n\u00b7\n\n3 min read\n\n\u00b7\n\nSep 22, 2021\n\n--\n\nListen\n\nShare\n\nThis post aggregates a series of blog posts showing the entire data science cycle, from finding an interesting problem/challenge, exploring the data, determining the ML task, preparing an initial baseline, and performing hyper-parameter searches to get the best model. In particular, the five sub-topics go chronologically as a data scientist or enthusiast shall proceed with them.\n\nCheckout accompanying resources:\n\n\\- **Kaggle**: [Plant Pathology with Lightning \u26a1](https://www.kaggle.com/jirkaborovec/plant-pathology-with-lightning)\n\n\\- **GitHub**: [Plant Pathology 2021 \u2014 FGVC8](https://github.com/Borda/kaggle_plant-pathology)\n\n# Preparing your Kaggle Development Environment\n\nIt shares everything you need to know for preparing a Kaggle Developer Environment with [_Grid.ai_](https://docs.grid.ai/). To recap, it selects an interesting Kaggle competition, shows how to start interactive sessions and configure Kaggle credentials for any (local or cloud) environment.\n\n[**How to Prepare your Development Environment to Rank on Kaggle** \\\n\\\n**Best Practices to Rank on Kaggle with PyTorch Lightning and Grid Spot Instances (Part 1/5)**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/how-to-prepare-your-development-environment-to-rank-on-kaggle-1a0fa1032b84?source=post_page-----54aa5248aa8e--------------------------------)\n\n# Kaggle Task Explorations and Model Baselining\n\nIt guides how to screen the given dataset and what interesting aspects you should look at. It shows how to wrap a file-like dataset to the PyTorch class dataset, which is a core of data handling. Moreover, we wrote a basic image multi-label classification model within [PyTorchLightning](https://pytorch-lightning.readthedocs.io/en/stable/) based on a [TorchVision](https://pytorch.org/vision/stable/index.html) model and trained it seamlessly on GPU with mixed precision without extra code.\n\n[**Intuitive Kaggle Task Exploration and Model Baselining** \\\n\\\n**Best Practices to Rank on Kaggle with PyTorch Lightning and Grid Spot Instances (Part 2/5)**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/intuitive-kaggle-task-exploration-and-model-baselining-e5f641943d08?source=post_page-----54aa5248aa8e--------------------------------)\n\n# Converting Kaggle Notebooks to Python package\n\nIt shows how to convert scientific notebooks to a standard Python package ready to share. We have discussed a motivation to add minimal tests to facilitate any future package enhancement without accidentally breaking it. Moreover, we presented the simple transition from a plain Python script to a versatile script that exposed all the command line parameters.\n\n[**Converting Scientific Kaggle Notebooks to Friendly Python Package** \\\n\\\n**Best Practices to Rank on Kaggle with PyTorch Lightning and Grid Spot Instances (Part 3/5)**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/converting-kaggle-training-notebooks-to-sharable-code-1cc59fec2414?source=post_page-----54aa5248aa8e--------------------------------)\n\n# Hyperparameter Optimization with Grid.ai\n\nIt talks about the need for hyperparameter searchers, and we have walked through Grid Runs, which simplifies fine-tuning in the cloud. We can even monitor our training online and eventually terminate some by our choice with all default configurations.\n\n[**Hyper-Parameter Optimization with Grid.ai and No Code Change** \\\n\\\n**Best Practices to Rank on Kaggle with PyTorch Lightning and Grid Spot Instances (Part 4/5)**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/hyperparameter-optimization-with-grid-ai-and-no-code-change-b89218d4ff49?source=post_page-----54aa5248aa8e--------------------------------)\n\n# Submitting Trained Models to Kaggle\n\nIt covers preparing the submission kernel, suitable for running in a completely isolated environment. It includes a tip on freezing and setting up a mid-size Python environment.\n\n[**Submitting Model Predictions to Kaggle Competitions** \\\n\\\n**Best Practices to Rank on Kaggle with PyTorch Lightning and Grid Spot Instances (Part 5/5)**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/submitting-model-predictions-to-kaggle-competition-ccb64b17132e?source=post_page-----54aa5248aa8e--------------------------------)\n\n[**Easy Kaggle Offline Submission With Chaining Kernels** \\\n\\\n**A simple guide on facilitating your Kaggle submission for competitions with restricted internet access using Kaggle\u2026**\\\n\\\ntowardsdatascience.com](https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d?s...",
      "url": "https://devblog.pytorchlightning.ai/best-practices-to-rank-on-kaggle-competition-with-pytorch-lightning-and-grid-ai-spot-instances-54aa5248aa8e?gi=7e42e86769cd"
    }
  ]
}