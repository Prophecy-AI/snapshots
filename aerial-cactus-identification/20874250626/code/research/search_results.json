{
  "query": "How do Kaggle winners handle class imbalance in image classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the winning\u2011team write\u2011ups and recent research, Kaggle image\u2011classification champions typically handle class imbalance by fine\u2011tuning the standard training pipeline rather than adding exotic loss functions. The most common practical steps are:\n\n1. **Adjust the batch composition** \u2013 use a smaller (or larger) batch size so that minority\u2011class samples appear more frequently within each update.  \n2. **Apply strong, class\u2011aware data augmentation** \u2013 augment minority\u2011class images more aggressively (e.g., random flips, rotations, color jitter) to increase their effective sample count.  \n3. **Choose an optimizer and learning\u2011rate schedule that cope with noisy gradients** \u2013 optimizers such as AdamW or SGD with warm\u2011up help stabilize training when the data distribution is skewed.  \n4. **Add label smoothing** \u2013 smoothing the one\u2011hot targets (e.g.,\u202f0.1\u202ffor the positive class) reduces over\u2011confidence on the dominant class and improves calibration on rare classes.  \n5. **Optionally weight the loss or use sampling tricks** \u2013 if the imbalance is extreme, assign higher loss weights to minority samples or employ simple over\u2011/under\u2011sampling pipelines (e.g.,\u202fSMOTE or random undersampling) to balance each mini\u2011batch.  \n\nThese adjustments\u2014batch size, augmentation, optimizer choice, and label smoothing\u2014have been shown to achieve state\u2011of\u2011the\u2011art performance on heavily imbalanced image datasets without resorting to specialized imbalance\u2011specific losses\u202f([NeurIPS\u202f2023 paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/6ea69f8116b7c01e3c3e43b62e6868fc-Paper-Conference.pdf)). Many top Kaggle solutions also combine them with lightweight weighting or sampling strategies to further boost minority\u2011class recall\u202f([AIcrowd notebook on class imbalance](https://www.aicrowd.com/showcase/dealing-with-class-imbalance)).",
      "url": ""
    },
    {
      "title": "",
      "text": "Simplifying Neural Network Training Under\nClass Imbalance\nRavid Shwartz-Ziv\u2217\nNew York University\nravid.shwartz.ziv@nyu.edu\nMicah Goldblum\u2217\nNew York University\ngoldblum@nyu.edu\nYucen Lily Li\nNew York University\nyucenli@nyu.edu\nC. Bayan Bruss\nCapital One\nbayan.bruss@capitalone.com\nAndrew Gordon Wilson\nNew York University\nandrewgw@cims.nyu.edu\nAbstract\nReal-world datasets are often highly class-imbalanced, which can adversely impact\nthe performance of deep learning models. The majority of research on training\nneural networks under class imbalance has focused on specialized loss functions,\nsampling techniques, or two-stage training procedures. Notably, we demonstrate\nthat simply tuning existing components of standard deep learning pipelines, such\nas the batch size, data augmentation, optimizer, and label smoothing, can achieve\nstate-of-the-art performance without any such specialized class imbalance methods.\nWe also provide key prescriptions and considerations for training under class\nimbalance, and an understanding of why imbalance methods succeed or fail.\n1 Introduction\nOnly a minuscule proportion of credit card transactions are fraudulent, and most cancer screenings\ncome back negative. In reality, some events are common while others are exceedingly rare. As a\nresult, machine learning systems, often developed in class-balanced settings [e.g., 48, 46, 16], are\nroutinely trained and deployed on class-imbalanced data where relatively few samples are associated\nwith certain minority classes, while majority classes dominate the datasets. Class-imbalanced training\ndata can negatively impact performance. Consequently, a wide body of literature focuses on specially\ntailored loss functions and sampling methods for counteracting the negative effects of imbalance\n[10, 14, 25, 27, 50, 61]. In striking contrast to such approaches, we instead show that simply\ntuning existing components of standard neural network training routines can achieve state-of-the-art\nperformance on class-imbalanced image and tabular benchmarks at little implementation overhead\nand without requiring any specialized loss functions or samplers designed specifically for imbalance.\nLike Wightman et al. [69], who found that modern training routines allow ResNets to achieve\nperformance competitive with that of later architectures, we show that modern training techniques\ncause the benefits of specialized class-imbalance methods to nearly vanish.\nMoreover, our carefully tuned training routine can be combined with existing class imbalance\nmethods for additional performance boosts. Conducting evaluations on real-world datasets, we find\nthat existing methods, which performed well on web-scraped natural image benchmarks on which\nthey were designed, underperform in the real-world setting, whereas our approach is robust.\nOur investigation provides key prescriptions and considerations for training under class imbalance:\n\u2217Authors contributed equally.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n1. The impact of batch size on performance is much more pronounced in class-imbalanced\nsettings, where small batch sizes shine.\n2. Data augmentations have an amplified impact on performance under class imbalance,\nespecially on minority-class accuracy. The augmentation strategies in our experiments which\nachieve the best performance on class-balanced benchmarks yield inferior performance on\nimbalanced problems.\n3. Large architectures, which do not overfit on class-balanced training sets, strongly overfit on\nimbalanced training sets of the same size. Moreover, newer architectures which work well\non class-balanced benchmarks do not always perform well under class imbalance.\n4. Adding a self-supervised loss during training can improve feature representations, leading\nto performance boosts on class-imbalanced problems.\n5. A small modification of Sharpness-Aware Minimization (SAM) [19] pulls decision bound\u0002aries away from minority samples and significantly improves minority-group accuracy.\n6. Label smoothing [57], especially on minority class examples, helps prevent overfitting.\nTo understand why exactly such training routine improvements confer significant benefits, we\ninvestigate the role of overfitting in class-imbalanced training. Our analysis shows that naive training\nroutines overfit on minority samples, causing neural collapse [60], whereby features extracted from\nthe penultimate layer concentrate around their class-mean. Combining this analysis with decision\nboundary visualizations, we demonstrate that unsuccessful methods for class-imbalanced training\noverfit strongly, whereas successful methods regularize.\n2 Related Work\nA long line of research has been conducted on class-imbalanced classification. There are several\narchetypal approaches specially designed to address imbalance:\nResampling the data. In early ensemble learning studies, boosting and bagging algorithms were\nadjusted to take account of imbalanced data by resampling. Traditionally, resampling involves\noversampling minority class samples by simply copying them [25, 10, 27], or undersampling majority\nclasses by removing samples [17, 32, 3, 8], so that minority and majority class samples appear equally\nfrequently in the training process.\nLoss reweighting. Reweighting methods assign different weights to majority and minority class loss\nfunctions, increasing the influence of minority samples which would otherwise play little role in the\nloss function [14, 34]. For instance, one may scale the loss by inverse class frequency [28] or reweight\nit using the effective number of samples [14]. As an alternative approach, one may focus on hard\nexamples by down-weighing the loss of well-classified examples [50] or dynamically rescaling the\ncross-entropy loss based on the difficulty of classifying a sample [61]. Bertsimas et al. [6] encourage\nlarger margins for rare classes, while Goh and Sim [21] learn robust features for minority classes\nusing class-uncertainty information which approximates Bayesian methods.\nTwo-stage fine-tuning and meta-learning. Two-stage methods separate the training process into\nrepresentation learning and classifier learning [54, 59, 38, 4]. In the first stage, the data is unmodified,\nand no resampling or reweighting is used to train good representations. In the second stage, the\nclassifier is balanced by freezing the backbone and fine-tuning the last layers with resampling or by\nlearning to debias the class confidences. These methods assume that the bias towards majority classes\nexists only in the classifier layer or that tweaking the classifier layer can correct the underlying biases.\nSeveral works have also inspected representations learned under class imbalance. Kang et al. [38]\nfind that representations learned on class-imbalanced training data via supervised learning perform\nbetter when the linear head is fine-tuned on balanced samples. Yang and Xu [71] instead examine the\neffect of self- and semi-supervised training on imbalanced data and conclude that imbalanced labels\nare significantly more useful when accompanied by auxiliary data for semi-supervised learning. Kotar\net al. [44], Yang and Xu [71] make the observation that self-supervised pre-training is insensitive\nto imbalance in the upstream training data. These works study SSL pre-training for the purpose of\ntransfer learning, sometimes using linear probes to evaluate the quality of representations. Inspired\nby their observations, we find that the addition of an SSL loss function on the same class-imbalanced\ndataset, even when no upstream data is available, can significantly improve generalization.\n2\nIn summary, existing works propose countless approaches to address class imbalance during training.\nIn contrast, we show that strong performance can be achieved on class-imbalanced datasets simply\nby tuning the components of standard neural networks training routines, without specialized loss\nfunctions or sampling methods designed specifically for imbalanc...",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/6ea69f8116b7c01e3c3e43b62e6868fc-Paper-Conference.pdf"
    },
    {
      "title": "AIcrowd | Dealing with Class Imbalance | Posts",
      "text": "Loading\n\n#### [ADDI Alzheimers Detection Challenge](https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge)\n\n# Dealing with Class Imbalance\n\nLooking at different ways to address class imbalance in this dataset\n\nJohnowhitaker7 May 2021\n\n[20](https://www.aicrowd.com/participants/sign_in) [Open in Colab](https://colab.research.google.com/gist/aicrowd-bot/d9ed51a8b9be2026f0ff96c2b5cad9c2)\n\nIn this notebook I take a close look at some different ways we can address the difference in class balance between train and validation (and presumably test)\n\nWe look at changing sample weights, over\u00a0and under-sampling, SMOTE and some other tips and tricks.\n\nI hope you find it helpful :)\n\n# Introduction [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Introduction)\n\nSo you've made your first model for this challenge and it's getting a log loss of ~0.9 - a ways behind the leaders on 0.6X. You're starting to think about feature engineering, adding more models to your ensemble, maybe trying one of those tabular deep learning models the cool kids are talking about. **STOP!** Before any of that, there is one BIG issue we need to deal with: class (im)balance.\n\nThe validation set (and presumably the test set) has a different class distribution to the training data. In this notebook we will look at many different ways we can correct for this class imbalance - picking one of these will boost your score tremendously (we're taking ~0.66 with a single simple random forest model). So, let's dive in.\n\n# Setup [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Setup)\n\nImporting the libraries we'll be using, loading the data and getting ready to run our experiments.\n\nIn\u00a0\\[1\\]:\n\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score, log_loss\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n```\n\nIn\u00a0\\[2\\]:\n\n```\n#The training data\ndf = pd.read_csv('ds_shared_drive/train.csv')\nprint(df.shape)\ndf.head(2)\n```\n\n```\n(32777, 122)\n\n```\n\nOut\\[2\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | S0CIXBKIUEOUBNURP | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.52617 | 0.524975 | 0.474667 | 0 | 0 | 0 | 1 | -105.0 | 0.0 | normal |\n| 1 | IW1Z4Z3H720OPW8LL | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.00081 | 0.516212 | 0.483330 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n\n2 rows \u00d7 122 columns\n\nIn\u00a0\\[3\\]:\n\n```\n# The validation data (we merge in the labels for convenience)\nval = pd.read_csv('ds_shared_drive/validation.csv')\nval = pd.merge(val, pd.read_csv('ds_shared_drive/validation_ground_truth.csv'),\n               how='left', on='row_id')\nprint(val.shape)\nval.head()\n```\n\n```\n(362, 122)\n\n```\n\nOut\\[3\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | LA9JQ1JZMJ9D2MBZV | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.499368 | 0.553194 | 0.446447 | 0 | 0 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 1 | PSSRCWAPTAG72A1NT | 6.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.427196 | 0.496352 | 0.503273 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n| 2 | GCTODIZJB42VCBZRZ | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.505583 | 0.503047 | 0.496615 | 1 | 0 | 0 | 0 | 0.0 | 0.0 | normal |\n| 3 | 7YMVQGV1CDB1WZFNE | 3.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | ... | 0.444633 | 0.580023 | 0.419575 | 0 | 1 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 4 | PHEQC6DV3LTFJYIJU | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | ... | 0.395976 | 0.494990 | 0.504604 | 0 | 0 | 0 | 1 | 150.0 | 0.0 | normal |\n\n5 rows \u00d7 122 columns\n\nIn\u00a0\\[4\\]:\n\n```\n# We'll keep track of how different approaches perform\nresults = []\n```\n\n# Baseline \\#1 - Training on all data [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Baseline-%231---Training-on-all-data)\n\nThis is a case where we don't do any correction for the class imbalance. Some models will do better than others - tree-based models like CatBoost will be less sensitive than some other model types, but they will still over-estimate the probability that a given sample will fall into the majority class when making predicitons on the validation set (since the 'normal' class is so much more common in the training data).\n\nIn\u00a0\\[5\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False, cat_features=['intersection_pos_rel_centre'])\n\n# Evaluate on val set\nmodel.fit(X, y, eval_set = (X_val, y_val), early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'No modifications',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'No modifications', 'Log Loss': 0.6745549053231596, 'F1': 0.2848101265822785}\n\n```\n\nA log loss of 0.67 on the validation set isn't terrible. We are using the validation set for early stopping - without that in place we get a log loss of 0.78 on our validation set and 0.8X on the leaderboard. So in a way, by using the validation set for early stopping we are already starting to combat our class balance problem... but we can do much better!\n\n# Adjusting Sample Weights [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Adjusting-Sample-Weights)\n\nModels like CatBoost allow us to assign more weight to specific samples. In this case, we use this to place less weight on samples in the over-represented classes, combating the bias introduced by the imbalance:\n\nIn\u00a0\\[6\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n#Our class weights\nweights = {\n    'normal':9/74, # Chosen based on some quick mental maths comparing the distribution of train vs val\n    'post_alzheimer':0.85,\n    'pre_alzheimer':1\n}\n\n# Applying these weights as sample weights by using Pool to wrap our training data\ntrain_data = Pool(\n    data = X,\n    label = y,\n    weight = y.map(weights), # << The important bit\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\neval_data = Pool(\n    data = X_val,\n    label = y_val,\n    weight = y_val.map(lambda x: 1.0), # all validation samples get a weight of 1\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False)\n\n# Evaluate on val set\nmodel.fit(train_data, eval_set = eval_data, early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'Modifying Sample Weights',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'Modifying Sample Weights', 'Log Loss': 0.5593949556085255, 'F1': 0.4727603953181...",
      "url": "https://www.aicrowd.com/showcase/dealing-with-class-imbalance"
    },
    {
      "title": "Classification on imbalanced data \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Classification on imbalanced data | TensorFlow Core[Skip to main content](#main-content)\n[![TensorFlow](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/tensorflow/images/lockup.svg)](https://www.tensorflow.org/)\n* /\n* English\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4[GitHub](https://github.com/tensorflow)Sign in\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Learn](https://www.tensorflow.org/learn)\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n# Classification on imbalanced dataStay organized with collectionsSave and categorize content based on your preferences.\n[![](https://www.tensorflow.org/images/tf_logo_32px.png)View on TensorFlow.org](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)|[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/imbalanced_data.ipynb)|\nThis tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use[Keras](https://www.tensorflow.org/guide/keras/overview)to define the model and[class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model)to help the model learn from the imbalanced data. .\nThis tutorial contains complete code to:\n* Load a CSV file using Pandas.\n* Create train, validation, and test sets.\n* Define and train a model using Keras (including setting class weights).\n* Evaluate the model using various metrics (including precision and recall).\n* Select a threshold for a probabilistic classifier to get a deterministic classifier.\n* Try and compare with class weighted modelling and oversampling.## Setup\n```\n`importtensorflowastffromtensorflowimportkerasimportosimporttempfileimportmatplotlibasmplimportmatplotlib.pyplotaspltimportnumpyasnpimportpandasaspdimportseabornassnsimportsklearnfromsklearn.metricsimportconfusion\\_matrixfromsklearn.model\\_selectionimporttrain\\_test\\_splitfromsklearn.preprocessingimportStandardScaler`\n```\n```\n2024-08-20 01:23:52.305388: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 01:23:52.326935: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 01:23:52.333533: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n```\n`mpl.rcParams['figure.figsize']=(12,10)colors=plt.rcParams['axes.prop\\_cycle'].by\\_key()['color']`\n```\n## Data processing and exploration\n### Download the Kaggle Credit Card Fraud data set\nPandas is a Python library with many helpful utilities for loading and working with structured data. It can be used to download CSVs into a Pandas[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n**Note:**This dataset has been collected and analysed during a research collaboration of Worldline and the[Machine Learning Group](http://mlg.ulb.ac.be)of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available[here](https://www.researchgate.net/project/Fraud-detection-5)and the page of the[DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/)project\n```\n`file=tf.keras.utilsraw\\_df=pd.read\\_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')raw\\_df.head()`\n```\n```\n`raw\\_df[['Time','V1','V2','V3','V4','V5','V26','V27','V28','Amount','Class']].describe()`\n```\n### Examine the class label imbalance\nLet's look at the dataset imbalance:\n```\n`neg,pos=np.bincount(raw\\_df['Class'])total=neg+posprint('Examples:\\\\nTotal:{}\\\\nPositive:{}({:.2f}% of total)\\\\n'.format(total,pos,100\\*pos/total))`\n```\n```\nExamples:\nTotal: 284807\nPositive: 492 (0.17% of total)\n```\nThis shows the small fraction of positive samples.\n### Clean, split and normalize the data\nThe raw data has a few issues. First the`Time`and`Amount`columns are too variable to use directly. Drop the`Time`column (since it's not clear what it means) and take the log of the`Amount`column to reduce its range.\n```\n`cleaned\\_df=raw\\_df.copy()# You don't want the `Time` column.cleaned\\_df.pop('Time')# The `Amount` column covers a huge range. Convert to log-space.eps=0.001# 0 =&gt; 0.1\u00a2cleaned\\_df['Log Amount']=np.log(cleaned\\_df.pop('Amount')+eps)`\n```\nSplit the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where[overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)is a significant concern from the lack of training data.\n```\n`# Use a utility from sklearn to split and shuffle your dataset.train\\_df,test\\_df=train\\_test\\_split(cleaned\\_df,test\\_size=0.2)train\\_df,val\\_df=train\\_test\\_split(train\\_df,test\\_size=0.2)# Form np arrays of labels and features.train\\_labels=np.array(train\\_df.pop('Class')).reshape(-1,1)bool\\_train\\_labels=train\\_labels[:,0]!=0val\\_labels=np.array(val\\_df.pop('Class')).reshape(-1,1)test\\_labels=np.array(test\\_df.pop('Class')).reshape(-1,1)train\\_features=np.array(train\\_df)val\\_features=np.array(val\\_df)test\\_features=np.array(test\\_df)`\n```\nWe check whether the distribution of the classes in the three sets is about the same or not.\n```\n`print(f'Average class probability in training set:{train\\_labels.mean():.4f}')print(f'Average class probability in validation set:{val\\_labels.mean():.4f}')print(f'Average class probability in test set:{test\\_labels.mean():.4f}')`\n```\n```\nAverage class probability in training set: 0.0017\nAverage class probability in validation set: 0.0018\nAverage class probability in test set: 0.0018\n```\nGiven the small number of positive labels, this seems about right.\nNormalize the input features using the sklearn StandardScaler.\nThis will set the mean to 0 and standard deviation to 1.\n**Note:**The`StandardScaler`is only fit using the`train\\_features`to be sure the model is not peeking at the validation or test sets.\n```\n`scaler=StandardScaler()train\\_features=scaler.fit\\_transform(train\\_features)val\\_features=scaler.transform(val\\_features)test\\_features=scaler.transform(test\\_features)train\\_features=np.clip(train\\_features,-5,5)val\\_features=np.clip(val\\_features,-5,5)test\\_features=np.clip(test\\_features,-5,5)print('Training labels shape:',train\\_labels.shape)print('Validation labels shape:',...",
      "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
    },
    {
      "title": "A Gold-Winning Solution Review of Kaggle Humpback Whale Identification Challenge",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F53b0e3ba1e84&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# A Gold-Winning Solution Review of **Kaggle** Humpback Whale Identification Challenge\n\n## An extensive yet simple review of the most noticeable approaches\n\n[![Vladislav Shakhray](https://miro.medium.com/v2/resize:fill:88:88/2*tNl9AhroqJA6knyKqbVtJw.png)](https://medium.com/@vladislavshakhray_56227?source=post_page-----53b0e3ba1e84--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----53b0e3ba1e84--------------------------------)\n\n[Vladislav Shakhray](https://medium.com/@vladislavshakhray_56227?source=post_page-----53b0e3ba1e84--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6110dc0ad8d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84&user=Vladislav+Shakhray&userId=6110dc0ad8d0&source=post_page-6110dc0ad8d0----53b0e3ba1e84---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----53b0e3ba1e84--------------------------------)\n\n\u00b7\n\n10 min read\n\n\u00b7\n\nMar 4, 2019\n\n--\n\n4\n\nListen\n\nShare\n\nPhoto by [Sandra Seitamaa](https://unsplash.com/@seitamaaphotography?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nRecently, my team took part in [Humpback Whales Identification Challenge](https://www.kaggle.com/c/humpback-whale-identification/leaderboard) hosted on Kaggle. We won a gold medal and were placed at #10 (out of 2131 teams) on the leaderboard.\n\nIn this blog post, I will summarise the main ideas of our solution, as well as provide a brief overview of interesting and catchy methods used by other teams.\n\n# Problem Description\n\nThe main goal was to identify, whether the given photo of the whale fluke belongs to one of the 5004 known individuals of whales, or it is a _new\\_whale_, never observed before.\n\nExample of 9 photos of the same whale from the training data\n\nThe puzzling aspect of this competition was a huge class imbalance. For more than 2000 of classes there was **only one** training sample, which made it hard to use the classification approach out of the box. What\u2019s more, it was an important part of the competition to classify, whether the whale is _new_ or not, which turned out to be quite non-trivial.\n\nClass Imbalance, from kernel by\n\n[Tomasz Bartczak](https://www.kaggle.com/kretes)\n\nThe metric for the competition was [mAP@5](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) (mean Average Precision at 5), which allowed us to submit up to 5 predictions for each test image. Our highest result on the private test set was **0.959** mAP@5.\n\n# \u201cSanakoyeu, Pleskov, Shakhray\u201d \ud83d\udcaa\n\nThe team consisted of [Vladislav Shakhray](https://www.linkedin.com/in/vladislav-shakhray-74b98315b/) (me), [Artsiom Sanakoyeu](https://asanakoy.github.io), Ph.D. student at Heidelberg University, and [Pavel Pleskov](https://www.linkedin.com/in/ppleskov/), Kaggle Top-5 Grandmaster.\n\nWe joined our forces with Artsiom in the middle of the competition to speed-up the experiments, and Pavel joined us one week prior to the team merger\u2019s deadline.\n\n# Validation and Initial Setting\n\nA couple of months before this competition, a [playground version](https://www.kaggle.com/c/whale-categorization-playground) of the same competition was hosted on Kaggle, but, as it was [noted](https://www.kaggle.com/c/humpback-whale-identification/discussion/73208) by the competition hosts, the real (non-playground) version featured even more data and cleaner labels. We decided to utilize the knowledge and the data from the previous competition in numerous ways:\n\n1. Using the data from the previous competition, we used [image hashing](https://jenssegers.com/61/perceptual-image-hashes) to collect **over 2000 validation samples.** This proved to be crucial when we validated our ensembles later.\n2. We removed the _new\\_whale_ class from the training dataset, as it does not share any logical image features between its elements.\n3. Some images were not aligned at all. Luckily, there was a publicly available pre-trained **bounding boxes** model used in a winning solution of the playground competition.We used it to detect the precise bounding box around the whale fluke and cropped the images accordingly.\n4. Due to the different color gammas of the images, all data was converted to **grayscale** prior to training.\n\n# Approach \\#1: Siamese Networks (Vladislav)\n\nOur first architecture was a siamese network with numerous branch architectures and custom loss, which consisted of a number of convolutional and dense layers. The branch architectures that we used included:\n\n- ResNet-18, ResNet-34, Resnet-50\n- SE-ResNeXt-50\n- ResNet-like [custom branch](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563) shared publicly by Martin Piotte\n\nWe used **hard-negative** as well as **hard-positive** mining by solving Linear Assignment Problem on the matrix of scores every 4 epochs. A little of randomizations were added to the matrix to ease the training process.\n\n**Progressive learning** was used, with the resolution strategy 229x229 -> 384x384 -> 512x512. That is, we first trained our network on 229x229 images with little regularization and larger learning rate. After convergence, we reset the learning rate and increased regularization, consequently training the network again on images of higher resolution (e.g. 384x484).\n\nFurthermore, due to the nature of data, heavy augmentations were used that included random brightness, Gaussian noise, random crops, and random blur.\n\nIn addition, we pursued a smart flipping augmentation strategy that significantly helped to create more training data. Specifically, for every pair of training images belonging to the **same** whale `X, Y` , we created one more training pair `flip(X), flip(Y)`. On the other hand, for every pair of **different** whales, we created three more examples `flip(X), Y`, `Y, flip(X)` and `flip(X), flip(Y)`.\n\nAn example showing that random flips strategy does not work with a pair of same-whale photos. Notice how the lower photos become differ...",
      "url": "https://towardsdatascience.com/a-gold-winning-solution-review-of-kaggle-humpback-whale-identification-challenge-53b0e3ba1e84?gi=8fdcb2545bab"
    },
    {
      "title": "First place solution for Kaggle\u2019s SSIM-ISIC Melanoma Competition",
      "text": "[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n# First place solution for Kaggle\u2019s SSIM-ISIC Melanoma Competition\n\nCheck out how the top solution for Kaggle's 30,000$ Melanoma competition works.\n\n[Mostafa Ibrahim](https://towardsdatascience.com/author/mostafaibrahim18/)\n\nDec 28, 2020\n\n7 min read\n\nShare\n\n# 1st place solution for Kaggle\u2019s skin cancer (Melanoma) Competition\n\n![Photo by Giorgio Trovato on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/12/0fU3G2WqfgBgX75XJ-scaled.jpg)Photo by [Giorgio Trovato](https://unsplash.com/@giorgiotrovato?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)\n\n5 months ago I participated in my first official kaggle competition, [SSIM-ISIC Melanoma](https://www.kaggle.com/c/siim-isic-melanoma-classification). Here is a brief overview of what the competition was about (from Kaggle):\n\n> Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It\u2019s also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection \u2013 potentially aided by data science \u2013 can make treatment more effective.\n\nPersonally, I feel very motivated when working on something that I believe in. This feeling resonated with me throughout this competition as skin [Cancer](https://towardsdatascience.com/tag/cancer/) ruins a lot of people\u2019s lives immensely. As someone who is passionate about machine learning and especially in healthcare this was super interesting for me. Since this was my first competition, most of what I was doing was learning rather than actually coding and therefore I ended up in the top 72%.\n\nThis story will be mainly about my experience and the first place solution (which there is a lot to learn from).\n\nOne of the quite difficult challenges of this competition is that the dataset was extremely unbalanced. The domain of the problem was binary classification for images. The distribution of the classes was 98% for class 1 and only 2% for class 2 ! For the sake of brevity and not duplicating content, I found this article quite helpful to tackle this issue :\n\n> [**Methods for Dealing with Imbalanced Data**](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n\nThe rest of this story will be about tackling other challenges in this competition.\n\n## **1\\. Modelling:**\n\nIn terms on the actual model, these 3 nets were extremely popular among all solutions and thus it made a lot of sense that the top solution would be using an ensemble of those 3. I won\u2019t dive into the details of how each of them works since there are tons of resources for that, but I will give an overview.\n\nOne of the main challenges in modelling CNNs is scaling them up and determining their width and depth. These networks tackle those challenges in different ways. There are multiple issues that you will have to deal with if you just keep on increasing the number of layers (bruteforce solution!) like the [vanishing gradient problem.](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n\n- **ResNet & ResNext:**\n\nResidual networks operate on the idea of \"residual blocks\" which have identity skip connections. If you want to further understand this, check out this story:\n\n[https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n\n![Source: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035](https://towardsdatascience.com/wp-content/uploads/2018/04/1-1S42aLNjENqyJ-Gt9CbGQ.png)Source: [https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n\nResNext is just a variation of ResNet that adds a \"split-transform-merge\" step where the outputs of different paths are merged by adding them together.\n\n- **EfficientNet:**\n\n[EfficientNet](https://arxiv.org/abs/1905.11946) is to me, the most impressive one. Probably because it has only been around for about 2 years. The main idea of EfficientNet is an efficient technique to scale up the [CNN](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) so that you get higher accuracy with a much lower number of parameters.\n\n![Source: EfficientNet Paper](https://towardsdatascience.com/wp-content/uploads/2020/12/1EdUFa_WpTq8_SSctk0FPxQ.png)Source: [EfficientNet Paper](https://arxiv.org/abs/1905.11946)\n\nTo explain how this works is a bit out of the scope of this story, but you can probably get a sense of it if you are familiar with the different types of scaling:\n\n![Source: EfficientNet Paper](https://towardsdatascience.com/wp-content/uploads/2020/12/1hTnCQ15FsOfrhXfkgYLO_Q.png)Source: [EfficientNet Paper](https://arxiv.org/abs/1905.11946)\n\n- **Fully Connected Networks**\n\nOne of the lessons that I have learned from this competition is to use all of the data given, I was mostly focused on using the images and did not pay much attention to the metadata that was provided for each patient. The first place solution was using a concatenation of the CNNs outputs and the output of a fully connected network. This fully connected network was using an activation function called [Swish](https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820), which is an upgrade of the classic ReLu.\n\nFor this fully connected network, they were also using [Batch Normalisation](https://en.wikipedia.org/wiki/Batch_normalization) and [Dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5), which are also typically used in a lot of networks.\n\nThis is the final model pipeline:\n\n![Source: Kaggle](https://towardsdatascience.com/wp-content/uploads/2020/12/1PRkQSsuXA60khkf2fMXFMg.png)Source: [Kaggle](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175412)\n\nThe CNN model block represents an ensemble of the 3 CNN models discussed above. Ensembling was also a new trick that I have learned that proved to be very effective. Not only does it give you better accuracy / results, but because the final competition rankings are evaluated on a small subset of the data, ensembling gives you a much better chances of surviving a \"shake down\". A shake down is where your models overfit the training data and thus the final result is much worse so you end up with a worse rank.\n\nI also found it quite helpful to think about your [Machine Learning](https://towardsdatascience.com/tag/machine-learning/) solution as a pipeline and visualise it in this way. As a web developer, I am used to data architecture diagrams, however I never applied the same methodology of architecture in machine learning (which is a huge mistake).\n\n## **2\\. Data processing tricks and feature engineering:**\n\nOne of the main reasons that they managed to achieve the first place was that they were using a much bigger ensemble of networks and a bigger combination of data.\n\n> To have stable validation, we used 2018+2019+2020\u2019s data for both train and **validation**. We track two cv scores, `cv_all` and `cv_2020`. The former is much more stable than the latter.\n\nAlso a common trick to further increase the size of the dataset it to use data augmentation. They were using a very good mixture of simple data augmentation techniques and complex ones. As for the simple ones they were using vertical & horizontal flips, random brightness & contrast and resizing. For the complex ones, they were using Gaussian Blur / Gaussian Noise, Elastic transform and Grid distortion. There are others that you can p...",
      "url": "https://towardsdatascience.com/first-place-solution-for-kaggles-ssim-isic-melanoma-competition-48f007f62989"
    },
    {
      "title": "Latest Winning Techniques for Kaggle Image Classification with Limited Data",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5259e7736327&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Latest Winning Techniques for Kaggle Image Classification with Limited Data\n\n## Tutorial on how to prevent your model from overfitting on a small dataset but still make accurate classifications\n\n[![Kayo Yin](https://miro.medium.com/v2/resize:fill:88:88/2*RgAwO4wXM60OTwi2_kFWCg.jpeg)](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Kayo Yin](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nOct 21, 2019\n\n--\n\n5\n\nShare\n\n[http://www.dakanarts.com/13-black-white/](http://www.dakanarts.com/13-black-white/)\n\nIn this article, I will go through the approach I used for an [in-class Kaggle challenge](https://www.kaggle.com/c/cs-ioc5008-hw1/overview). I spent about two weeks on the challenge, with a final submission score of **0.97115** which places me second on the final leaderboard. I started off by borrowing ideas from [this article](https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86), which I recommend as well.\n\n# Challenge Introduction\n\nThe proposed challenge is a natural images classification task with **13** **classes**. The first difficulty in this challenge is the **scarcity** of available data: only 3 859 images for training. The rules of the challenge was not to use external data during training as well. With little data, the model will be more prone to overfitting without learning to generalize.\n\nMoreover, because these images are in **grayscale**, they contain less information than color images such as the ImageNet dataset, so a pre-trained model on color images cannot be applied directly to this task. Upon further inspection of the dataset, many classes contain images that are visually very **similar** or containing the same elements. The model will lose accuracy when\u2026\n\n[![Kayo Yin](https://miro.medium.com/v2/resize:fill:144:144/2*RgAwO4wXM60OTwi2_kFWCg.jpeg)](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------follow_profile-----------)\n\n[**Written by Kayo Yin**](https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------)\n\n[575 Followers](https://medium.com/@kayo.yin/followers?source=post_page-----5259e7736327--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------)\n\nPhD student at UC Berkeley researching AI. Now writing at [kayoyin.github.io/blog](http://kayoyin.github.io/blog)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----5259e7736327--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----5259e7736327--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----5259e7736327--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5259e7736327--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----5259e7736327--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----5259e7736327--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5259e7736327--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5259e7736327--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----5259e7736327--------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----5259e7736327--------------------------------)",
      "url": "https://towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327?gi=0580a8bafd4a"
    },
    {
      "title": "Practical Tips to Boost Kaggle Image Classification Performance with Lightning\u00a0 | by \u30a4\u30eb\u30ab Borovec | Medium | PyTorch Lightning Developer Blog",
      "text": "[Sitemap](https://devblog.pytorchlightning.ai/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F242e2e533429&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**PyTorch Lightning Developer Blog**](https://devblog.pytorchlightning.ai/?source=post_page---publication_nav-d98240dffbaf-242e2e533429---------------------------------------)\n\n\u00b7\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpytorch-lightning&operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&collection=PyTorch+Lightning+Developer+Blog&collectionId=d98240dffbaf&source=post_page---publication_nav-d98240dffbaf-242e2e533429---------------------publication_nav------------------)\n\nPyTorch Lightning is a lightweight machine learning framework that handles most of the engineering work, leaving you to focus on the science. Check it out: [pytorchlightning.ai](http://pytorchlightning.ai)\n\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fpytorch-lightning&operation=register&redirect=https%3A%2F%2Fdevblog.pytorchlightning.ai%2Fpractical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429&collection=PyTorch+Lightning+Developer+Blog&collectionId=d98240dffbaf&source=post_page---post_publication_sidebar-d98240dffbaf-242e2e533429---------------------post_publication_sidebar------------------)\n\nScreenshot from the [Kaggle competition](https://www.kaggle.com/c/plant-pathology-2021-fgvc8).\n\n# Practical Lighting Tips to Rank on Kaggle Image Challenges\n\n## This post outlines some PyTorch Lightning best practices from the recent Kaggle Plant Pathology image classification challenge that enabled us to reach the leaderboard.\n\n[\u30a4\u30eb\u30ab Borovec](https://medium.com/@jborovec?source=post_page---byline--242e2e533429---------------------------------------)\n\n7 min read\n\n\u00b7\n\nAug 3, 2021\n\n--\n\nListen\n\nShare\n\nWe recently competed in the Kaggle [Plant Pathology 2021 \u2014 FGVC8](https://www.kaggle.com/c/plant-pathology-2021-fgvc8) challenge to detect 5 plant diseases using computer vision on images of leaves from apple trees (see the [full challenge description](https://www.kaggle.com/c/plant-pathology-2021-fgvc8)).\n\nLabel count histogram over the whole dataset.\n\nIn this post, we will walk through some [Lighting](https://github.com/PyTorchLightning/pytorch-lightning) Best Practices and show you how a few small changes to your code can significantly reduce training time and get you on the Kaggle leaderboards.\n\n> All code snippets, notebooks and additional explanations in this post can be found in the GitHub repository below:\n\n[**GitHub - Borda/kaggle\\_plant-pathology: Identify the type of disease present on Appletree leafs** \\\n\\\n**Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current\u2026**\\\n\\\ngithub.com](https://github.com/Borda/kaggle_plant-pathology?source=post_page-----242e2e533429---------------------------------------)\n\n### Explore the Task and Given Dataset\n\nThere is an adage \u2014 \u201cto win a battle, you must first know your enemy\u201d. This adage applies to any machine learning and data science task.\n\nUnfortunately, in Kaggle, proper data analysis is commonly overlooked. Novice Kagglers hungry for quick often blindly apply out-of-the-box tools on data using default configurations and are then surprised when this does not work as expected.\n\nEven though Kaggle often provides already cleaned data with task descriptions, you should perform at least some minimal data exploration before doing any modelling.\n\n> [PyTorch Lightning Flash](https://github.com/PyTorchLightning/lightning-flash) contains easy-to-use [visualization callbacks](https://lightning-flash.readthedocs.io/en/latest/template/data.html?highlight=visual+callback#basevisualization), which are useful to visually inspect the impact of different transforms on your data before launching a training.\n\nYou can implement custom visualization by subclassing a `BaseVisualization` callback- override any of them `show_{preprocess_hook_name}` to get the associated data and visualize it.\n\n### Single vs Multi-Label Classification\n\nShowing sample images and counts per each concatenated class combination.\n\nThe Plant Pathology dataset provides annotated text labels in a CSV table. In each row of the table, an image name is paired with a string containing concatenated labels. The illustration (left figure) visualizes a naive data grouping by a unique string, combining multiple labels.\n\nWe noticed that several submissions used these concatenated strings as submission labels, treating the task as a single label classification problem.\n\nTreating the challenge as a single label classification task raises a few issues:a) The classification decision space, represented as one-hot vectors, comprises 12 classes instead of 6.b) The dataset is unbalanced as many combinations are not accurately represented in the training data.c) Some label combinations do not exist in training data but may appear in testing data.\n\nBy simply splitting our labels and using a multi-label classification model with a `ResNet50` backbone from `torchvision` library, we were able to rank first with just the second submission and with a sufficient margin of about 8% in the F1 score; see leaderboard snapshot below.\n\nPress enter or click to view image in full size\n\nChallenge snapshot with our first multi-label submission with base model ResNet50.\n\n> Note while it wasn\u2019t available during the competition, today [Lightning Flash](https://github.com/PyTorchLightning/lightning-flash) supports Multi-Label Classification on all Timm and Torch Vision backbones out of the box.\n\n[**Multi-label Image Classification - Flash documentation** \\\n\\\n**Edit description**\\\n\\\nlightning-flash.readthedocs.io](https://lightning-flash.readthedocs.io/en/latest/reference/image_classification_multi_label.html?source=post_page-----242e2e533429---------------------------------------)\n\n### Downscaling and Augmenting Images\n\nWe used a standard `ResNet50` image classification model with simple data augmentation techniques to improve our baseline to increase robustness and prevent overfitting.\n\nWe applied simple image flipping and perspective deformation to augment our training images and extend their variability t...",
      "url": "https://devblog.pytorchlightning.ai/practical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429?gi=db2043612248"
    },
    {
      "title": "Practical Lighting Tips to Rank on Kaggle Image Challenges",
      "text": "<div><div><p></p><h2>Practical Lighting Tips to Rank on Kaggle Image Challenges</h2><p></p><div><h2>This post outlines some PyTorch Lightning best practices from the recent Kaggle Plant Pathology image classification challenge that enabled us to reach the leaderboard.</h2><div><a href=\"https://medium.com/@jborovec?source=post_page---byline--242e2e533429---------------------------------------\"><div><p></p></div></a></div></div><p>We recently competed in the Kaggle <a href=\"https://www.kaggle.com/c/plant-pathology-2021-fgvc8\">Plant Pathology 2021 \u2014 FGVC8</a> challenge to detect 5 plant diseases using computer vision on images of leaves from apple trees (see the <a href=\"https://www.kaggle.com/c/plant-pathology-2021-fgvc8\">full challenge description</a>).</p><figure><figcaption>Label count histogram over the whole dataset.</figcaption></figure><p>In this post, we will walk through some <a href=\"https://github.com/PyTorchLightning/pytorch-lightning\">Lighting</a> Best Practices and show you how a few small changes to your code can significantly reduce training time and get you on the Kaggle leaderboards.</p><blockquote><p>All code snippets, notebooks and additional explanations in this post can be found in the GitHub repository below:</p></blockquote><h2>Explore the Task and Given Dataset</h2><p>There is an adage \u2014 \u201cto win a battle, you must first know your enemy\u201d. This adage applies to any machine learning and data science task.</p><p>Unfortunately, in Kaggle, proper data analysis is commonly overlooked. Novice Kagglers hungry for quick often blindly apply out-of-the-box tools on data using default configurations and are then surprised when this does not work as expected.</p><p>Even though Kaggle often provides already cleaned data with task descriptions, you should perform at least some minimal data exploration before doing any modelling.</p><blockquote><p><a href=\"https://github.com/PyTorchLightning/lightning-flash\">PyTorch Lightning Flash</a> contains easy-to-use <a href=\"https://lightning-flash.readthedocs.io/en/latest/template/data.html?highlight=visual+callback#basevisualization\">visualization callbacks</a>, which are useful to visually inspect the impact of different transforms on your data before launching a training.</p></blockquote><figure><figcaption>You can implement custom visualization by subclassing a <code><a href=\"https://lightning-flash.readthedocs.io/en/latest/api/generated/flash.core.data.base_viz.BaseVisualization.html#flash.core.data.base_viz.BaseVisualization\">BaseVisualization</a></code> callback- override any of them <code>show_{preprocess_hook_name}</code> to get the associated data and visualize it.</figcaption></figure><h2>Single vs Multi-Label Classification</h2><figure><figcaption>Showing sample images and counts per each concatenated class combination.</figcaption></figure><p>The Plant Pathology dataset provides annotated text labels in a CSV table. In each row of the table, an image name is paired with a string containing concatenated labels. The illustration (left figure) visualizes a naive data grouping by a unique string, combining multiple labels.</p><p>We noticed that several submissions used these concatenated strings as submission labels, treating the task as a single label classification problem.</p><p>Treating the challenge as a single label classification task raises a few issues:<br/>a) The classification decision space, represented as one-hot vectors, comprises 12 classes instead of 6.<br/>b) The dataset is unbalanced as many combinations are not accurately represented in the training data.<br/>c) Some label combinations do not exist in training data but may appear in testing data.</p><p>By simply splitting our labels and using a multi-label classification model with a <code>ResNet50</code> backbone from <code>torchvision</code> library, we were able to rank first with just the second submission and with a sufficient margin of about 8% in the F1 score; see leaderboard snapshot below.</p><figure><figcaption>Challenge snapshot with our first multi-label submission with base model ResNet50.</figcaption></figure><blockquote><p>Note while it wasn\u2019t available during the competition, today <a href=\"https://github.com/PyTorchLightning/lightning-flash\">Lightning Flash</a> supports Multi-Label Classification on all Timm and Torch Vision backbones out of the box.</p></blockquote><h2>Downscaling and Augmenting Images</h2><p>We used a standard <code>ResNet50</code> image classification model with simple data augmentation techniques to improve our baseline to increase robustness and prevent overfitting.</p><p>We applied simple image flipping and perspective deformation to augment our training images and extend their variability to train a more generalizable model.</p><pre><span><em># transforms to extend the dataset variability</em><br/>T.RandomRotation(degrees=30),<br/>T.RandomPerspective(distortion_scale=0.4),<br/>T.RandomHorizontalFlip(p=0.5),<br/>T.RandomVerticalFlip(p=0.5),</span><span><em># required as ResNet50 has input 224x224</em><br/>T.RandomResizedCrop(size=224),</span><span><em># standardization</em><br/>T.ToTensor(),<br/>T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),</span></pre><p>Using this naive augmentation, we struggled with the following issues:<br/> 1. The model performed initially worse than expected.<br/> 2. Training took about 2 hours per epoch on the Kaggle GPU kernels</p><p>Since image augmentation is a best practice, at first, these observations surprised us. However, upon deeper analysis, we found that most of the images in the Plant Pathology dataset were 4k resolution.</p><figure><figcaption>HIstogram of image sizes from the Plant dataset.</figcaption></figure><p>Applying a <a href=\"https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomResizedCrop\">RandomResizedCrop</a> transform on a 4k image often crops out a background image section. To address this issue, we resized all images before making the final crop.</p><p>To prevent wasting hours resizing the full dataset on each epoch, we moved the resizing to the beginning of the data pipeline as a one-time preprocessing step. In the end, we downscaled all <a href=\"https://www.kaggle.com/jirkaborovec/plant-pathology-fgvc78-640px\">datasets offline to 640px</a> and reuploaded the downscaled images to Kaggle. These changes significantly lowered our training time from 2hrs to ~7min per epoch with no performance degradation.</p><h2>Visualizing Data Abstraction Layers and Performance Boosting</h2><p>Pytorch Lightning significantly simplifies training and related data handling (e.g. loop over epochs, moving batches to proper device, etc.).</p><h2>Data Streaming and Validation</h2><p>Using Lightning requires basic data logic such as coupling images with labels (annotation) using pure PyTorch. Data handling issues are not trivial to detect, especially when the only sign of a full pipeline is that it does not train/learn as expected.</p><p>It is a best practice prefer to verify and fully control each level of data abstraction (<code>Dataset</code> \u2192 <code>dataloader</code> \u2192<code>LightningDataModule</code>). This avoids tedious and painful debugging later on. To this end, you should visualize data after defining each data wrapper in a PyTorch<code>Dataset</code> and <code>dataloader</code>.</p><p>Let's assume we inherited from the standard PyTorch <code>Dataset</code> and implement the required <code>__getitem__</code>method, then we can list first a few samples with annotation as follows:</p><pre><span>fig = plt.figure(figsize=(9, 6))<strong><br/>for</strong> i <strong>in</strong> range(9): <em># list first 9 images</em><br/> img, lb = dataset[i]<br/> ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])<br/> ax.imshow(img)<br/> ax.set_title(lb)</span></pre><figure><figcaption>Visualization of the code snapshot above shows a few first images from the dataset with labels above in binary encoding.</figcaption></figure><p...",
      "url": "https://devblog.pytorchlightning.ai/practical-tips-to-rank-on-kaggle-image-challenges-with-lightning-242e2e533429"
    }
  ]
}