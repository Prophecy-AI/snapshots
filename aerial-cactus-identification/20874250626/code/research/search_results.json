{
  "query": "What are winning approaches for aerial image classification and satellite imagery competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Aerial\u2011image and satellite\u2011imagery contests on Kaggle (e.g., Planet\u202fAmazon, DSTL Feature Detection, On\u202fCloud\u202fN) are typically won by pipelines that treat the data as a multispectral segmentation problem and apply modern fully\u2011convolutional networks (FCNs) or encoder\u2011decoder architectures such as U\u2011Net.  The top solutions first adapt a standard CNN backbone (ResNet, EfficientNet, or a transformer\u2011based encoder) to accept the extra spectral channels, then train it with a loss that balances pixel\u2011wise accuracy and boundary quality (e.g., a combination of cross\u2011entropy and Dice or a boundary\u2011aware term)\u202f\u30103\u2020https://arxiv.org/abs/1706.06169\u3011.  Heavy use of data\u2011augmentation\u2014random rotations, flips, scaling, and spectral\u2011index augmentations (e.g., NDVI, reflectance indices)\u2014helps the model generalise to the wide variety of lighting and seasonal conditions found in satellite data\u202f\u30103\u2020https://arxiv.org/abs/1706.06169\u3011.\n\nWinning teams also exploit the specific sensor information that the competitions provide.  In the cloud\u2011cover challenge, participants built models on the four Sentinel\u20112 bands (B02\u2011B04\u2011B08) and optimized directly for the Jaccard (IoU) metric used on the leaderboard, often adding a small post\u2011processing step to clean noisy predictions\u202f\u30109\u2020https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html\u3011.  Across recent benchmarks, the most successful approaches combine these tailored inputs with pretrained weights from large\u2011scale image\u2011classification datasets, fine\u2011tune them on the satellite task, and apply test\u2011time augmentation (multi\u2011scale and flip averaging) to squeeze out the last few points of score\u202f\u30102\u2020https://arxiv.org/abs/2207.07189\u3011.  While many top teams still use modest ensembles (2\u20114 models) to boost robustness, several prize\u2011winning solutions deliberately avoid large ensembles, focusing instead on a single well\u2011regularized network that can be deployed efficiently\u202f\u30103\u2020https://arxiv.org/abs/1706.06169\u3011.\n\nIn practice, the recipe that repeatedly appears in Kaggle\u2019s aerial\u2011image leaderboards is:\n\n1. **Multispectral\u2011aware backbone** \u2013 modify a CNN/transformer to ingest all available bands.  \n2. **Segmentation\u2011oriented loss** \u2013 combine pixel\u2011wise and boundary/Dice terms to handle class imbalance.  \n3. **Rich augmentation & spectral indices** \u2013 geometric transforms plus reflectance\u2011index features.  \n4. **Transfer learning & fine\u2011tuning** \u2013 start from ImageNet\u2011pretrained weights, then adapt to the satellite domain.  \n5. **Test\u2011time augmentation & light ensembling** \u2013 average predictions over flips/scales for the final submission.\n\nThese components, together with careful preprocessing of the raw satellite imagery, have proven to be the core of most winning Kaggle solutions for aerial image classification and related satellite\u2011imagery challenges.",
      "url": ""
    },
    {
      "title": "Computer Science > Computer Vision and Pattern Recognition",
      "text": "[2207.07189] Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2207.07189\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2207.07189**(cs)\n[Submitted on 14 Jul 2022 ([v1](https://arxiv.org/abs/2207.07189v1)), last revised 14 Jan 2023 (this version, v2)]\n# Title:Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification\nAuthors:[Ivica Dimitrovski](https://arxiv.org/search/cs?searchtype=author&amp;query=Dimitrovski,+I),[Ivan Kitanovski](https://arxiv.org/search/cs?searchtype=author&amp;query=Kitanovski,+I),[Dragi Kocev](https://arxiv.org/search/cs?searchtype=author&amp;query=Kocev,+D),[Nikola Simidjievski](https://arxiv.org/search/cs?searchtype=author&amp;query=Simidjievski,+N)\nView a PDF of the paper titled Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification, by Ivica Dimitrovski and 3 other authors\n[View PDF](https://arxiv.org/pdf/2207.07189)> > Abstract:\n> We present AiTLAS: Benchmark Arena -- an open-source benchmark suite for evaluating state-of-the-art deep learning approaches for image classification in Earth Observation (EO). To this end, we present a comprehensive comparative analysis of more than 500 models derived from ten different state-of-the-art architectures and compare them to a variety of multi-class and multi-label classification tasks from 22 datasets with different sizes and properties. In addition to models trained entirely on these datasets, we benchmark models trained in the context of transfer learning, leveraging pre-trained model variants, as it is typically performed in practice. All presented approaches are general and can be easily extended to many other remote sensing image classification tasks not considered in this study. To ensure reproducibility and facilitate better usability and further developments, all of the experimental resources including the trained models, model configurations, and processing details of the datasets (with their corresponding splits used for training and evaluating the models) are publicly available on the repository: [> this https URL\n](https://github.com/biasvariancelabs/aitlas-arena)> Subjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2207.07189](https://arxiv.org/abs/2207.07189)[cs.CV]|\n|(or[arXiv:2207.07189v2](https://arxiv.org/abs/2207.07189v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2207.07189](https://doi.org/10.48550/arXiv.2207.07189)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nRelated DOI:|[https://doi.org/10.1016/j.isprsjprs.2023.01.014](https://doi.org/10.1016/j.isprsjprs.2023.01.014)\nFocus to learn more\nDOI(s) linking to related resources\n|\n## Submission history\nFrom: Nikola Simidjievski [[view email](https://arxiv.org/show-email/a98d8213/2207.07189)]\n**[[v1]](https://arxiv.org/abs/2207.07189v1)**Thu, 14 Jul 2022 20:18:58 UTC (8,566 KB)\n**[v2]**Sat, 14 Jan 2023 16:10:58 UTC (13,320 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification, by Ivica Dimitrovski and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2207.07189)\n* [TeX Source](https://arxiv.org/src/2207.07189)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2207.07189&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2207.07189&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2022-07](https://arxiv.org/list/cs.CV/2022-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2207.07189?context=cs)\n[cs.AI](https://arxiv.org/abs/2207.07189?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2207.07189?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.07189)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.07189)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.07189)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2207.07189&amp;description=Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2207.07189&amp;title=Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share ...",
      "url": "https://arxiv.org/abs/2207.07189"
    },
    {
      "title": "Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition",
      "text": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1706.06169** (cs)\n\n\\[Submitted on 19 Jun 2017\\]\n\n# Title:Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition\n\nAuthors: [Vladimir Iglovikov](https://arxiv.org/search/cs?searchtype=author&query=Iglovikov,+V), [Sergey Mushinskiy](https://arxiv.org/search/cs?searchtype=author&query=Mushinskiy,+S), [Vladimir Osin](https://arxiv.org/search/cs?searchtype=author&query=Osin,+V)\n\nView a PDF of the paper titled Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition, by Vladimir Iglovikov and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1706.06169)\n\n> Abstract:This paper describes our approach to the DSTL Satellite Imagery Feature Detection challenge run by Kaggle. The primary goal of this challenge is accurate semantic segmentation of different classes in satellite imagery. Our approach is based on an adaptation of fully convolutional neural network for multispectral data processing. In addition, we defined several modifications to the training objective and overall training pipeline, e.g. boundary effect estimation, also we discuss usage of data augmentation strategies and reflectance indices. Our solution scored third place out of 419 entries. Its accuracy is comparable to the first two places, but unlike those solutions, it doesn't rely on complex ensembling techniques and thus can be easily scaled for deployment in production as a part of automatic feature labeling systems for satellite imagery analysis.\n\n|     |     |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:1706.06169](https://arxiv.org/abs/1706.06169) \\[cs.CV\\] |\n|  | (or [arXiv:1706.06169v1](https://arxiv.org/abs/1706.06169v1) \\[cs.CV\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.1706.06169](https://doi.org/10.48550/arXiv.1706.06169)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vladimir Iglovikov \\[ [view email](https://arxiv.org/show-email/190a3085/1706.06169)\\]\n\n**\\[v1\\]**\nMon, 19 Jun 2017 20:41:42 UTC (2,087 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition, by Vladimir Iglovikov and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1706.06169)\n- [TeX Source](https://arxiv.org/src/1706.06169)\n- [Other Formats](https://arxiv.org/format/1706.06169)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1706.06169&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1706.06169&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2017-06](https://arxiv.org/list/cs.CV/2017-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1706.06169?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.06169)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.06169)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.06169)\n\n### [1 blog link](https://arxiv.org/tb/1706.06169)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#IglovikovMO17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/IglovikovMO17)\n\n[Vladimir Iglovikov](https://dblp.uni-trier.de/search/author?author=Vladimir%20Iglovikov)\n\n[Sergey Mushinskiy](https://dblp.uni-trier.de/search/author?author=Sergey%20Mushinskiy)\n\n[Vladimir Osin](https://dblp.uni-trier.de/search/author?author=Vladimir%20Osin)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1706.06169&description=Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1706.06169&title=Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.06169) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/1706.06169"
    },
    {
      "title": "Detecting Cloud Cover Via Sentinel-2 Satellite Data - My Top-10 Percent Solution to DrivenData\u2019s On CloudN Competition",
      "text": "Benjamin Warner Detecting Cloud Cover Via Sentinel-2 Satellite Data - My Top-10 Percent Solution to DrivenData\u2019s On CloudN Competition https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html\nDetecting Cloud Cover Via Sentinel-2 Satellite Data - My Top-10 Percent Solution to DrivenData\u2019s On CloudN Competition\nBenjamin Warner\n2022-03-11T00:00:00Z\n# [](benjaminwarner.dev/2022/03/11...) Competition Summary\n\nThe goal of _[On CloudN](https://www.drivendata.org/competitions/83/cloud-cover/page/396/)_ is to create a method of labeling cloud cover from [Sentinel-2](https://sentinel.esa.int/web/sentinel/missions/sentinel-2) imagery which can beat the existing methods of thresholding, handcrafted models, or deep learning models in labeling accuracy.\n\nThe competition\u2019s scoring metric is the Jaccard index, which can be calculated as follows:\n\nJaccard(A,B)=\u2223A\u2229B\u2223\u2223A\u222aB\u2223=\u2223A\u2229B\u2223\u2223A\u2223+\u2223B\u2223\u2212\u2223A\u2229B\u2223text{Jaccard}left(A, Bright) =frac{lvert Acap Brvert}{lvert Acup Brvert} =frac{lvert Acap Brvert}{lvert Arvert+lvert Brvert-lvert Acap Brvert}Jaccard(A,B)=\u2223A\u222aB\u2223\u2223A\u2229B\u2223\u200b=\u2223A\u2223+\u2223B\u2223\u2212\u2223A\u2229B\u2223\u2223A\u2229B\u2223\u200b\n\nwhere \u2223A\u2223lvert Arvert\u2223A\u2223 is the set of true pixels and \u2223B\u2223lvert Brvert\u2223B\u2223 is the set of predicted pixels.\n\nAn example chip using DrivenData\u2019s display code. Yellowis cloud cover and purple is ground in this two-label setup.\n\nDrivenData provided twelve thousand 512 x 512 GeoTIFF chips of training data, collected between 2018 and 2020, with four image bands per chip (see Table 1), metadata, and handcrafted labels. The labels were created by a 2021 [Radiant Earth Foundation](https://www.radiant.earth/) contest, and were later refined with expert annotators from TaQadam.\n\nProvided Sentinel-2 Data Bands\n\n| Band | Description | Center wavelength |\n\n| B02 | Blue visible light | 497 nm |\n| B03 | Green visible light | 560 nm |\n| B04 | Red visible light | 665 nm |\n| B08 | Near infrared light | 835 nm |\n\nDespite these refinements, there appeared to be somewhere between five and ten percent of the chips with partially or fully incorrect labels, as figure 3 below illustrates.\n\nCloud cover (yellow) is labeled as ground (purple) in this example chip.\n\nThis sets the challenge for this competition. Train a model which is robust to incorrectly labeled data, but not too robust as to score poorly on the test set, which probably has a similar five to ten percent label error.\n\n# [](benjaminwarner.dev/2022/03/11...) Solution Summary\n\n\u2295Code to reproduce this solution is available [here](https://github.com/warner-benjamin/code_for_blog_posts/tree/main/2022/drivendata_cloudn).My solution was a customized version of XResNeXt50\u2014the fastai version of ResNeXtSaining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. 2017. Aggregated Residual Transformations for Deep Neural Networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. with architectural improvements from the Bag of TricksTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2019. Bag of Tricks for Image Classification with Convolutional Neural Networks. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 558\u2013567. DOI:10.1109/CVPR.2019.00065 paper\u2014as the backbone for a customized DeepLabV3+Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. 2018. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In _Computer Vision - ECCV 2018_, 833\u2013851. DOI:10.1007/978-3-030-01234-_49 trained on five-fold split of the entire dataset.\n\n## [](benjaminwarner.dev/2022/03/11...) Model Specification\n\nMy customized XResNeXt50 architecture replaced all ReLU activation functions with with the MishDiganta Misra. 2019. Mish: A self regularized non-monotonic neural activation function. _arXiv:1908.08681_. activation function and the pooling layers with MaxBlurPoolRichard Zhang. 2019. Making Convolutional Networks Shift-Invariant Again. _arXiv:1904.11486_.. Instead of using a standard attention module, such as Squeeze and Excitation, I found the efficient combination of channel and spatial attention from Shuffle AttentionQing-Long Zhang and Yu-Bin Yang. 2021. SA-Net: Shuffle Attention for Deep Convolutional Neural Networks. _arXiv:2102.00240_. achieved better performance while only requiring a moderate increase of compute and memory during training.\n\nDeepLabV3+ received fewer modifications. I replaced all the ReLU activation layers with Mish, but otherwise left the architecture at its defaults.\n\n## [](benjaminwarner.dev/2022/03/11...) Pretraining\n\nImagenet pretrained models work well as a base for transfer learning when the domain is similar to Imagenet. The further away from Imagenet the new domains get, the less practical use Imagenet weights have on downstream tasks. However, having a pretrained backbone is still useful for segmentation, especially with DeepLabV3+, its default settings have a lot of dropout.\n\nSentinel-2 imagery is a large enough domain shift from Imagenet that I personally observed little benefit of using an Imagenet pretrained model verses a model architecture I chose with my own pretrained weights.\n\nAfter some experimentation, I pretrained the XSA-ResNeXt50 model to predict the ordinal classificationThe ordinal labels were generated on the fly from the transformed segmentation labels via a custom transform detailed [below](https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html#new-transforms)., zero through twenty, of cloud coverage on 256-pixel random crops from the 512 image chips using BCE loss.\n\nThe XSA-ResNeXt50 backbone was trained for 15 epochs at a learning rate of 1e\u221231e^{-3}1e\u22123, on a batch size of 64, with a weight decay of 1e\u221241e^{-4}1e\u22124, with the RangerLess Wright. 2019. New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam LookAhead for the best of both. (August 2019). Retrieved from https://lessw.medium.com/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d optimizer, using cosine decay from fastai\u2019s `fit_flat_cos` starting at seventy-five percent of total training steps.\n\nIn addition to random crops, I used a small amount random zoom, warp, 45 degrees of rotation, flipping, channel dropout, and random noise. I did not use any lighting or other color shifting augmentations, as I observed a significant decrease in model performance. Augmentation details can be seen in the [DataBlock appendix](https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html#appendix-datablocks).\n\nI selected the best performing epoch via F1 Score, which was ~0.93 for all five folds, as the backbone weights for training DeepLabV3+.\n\n## [](benjaminwarner.dev/2022/03/11...) Training\n\nAfter pretraining the custom XSA-ResNeXt50 backbone, I train the lightly modified DeepLabV3+ on the segmentation labels. To fit on the GeForce RTX 3090\u2019s 24GB of RAM, I once again trained on 256 pixel random crops from the 512 pixel chips. I used a combination of label smoothing cross entropy loss and dice loss. Validation was conducted on 256-pixel four corner crops.\n\nSimilar to pretraining, the DeepLab model was trained for 80 epochs at a learning rate of 1e\u221231e^{-3}1e\u22123, on a batch size of 64, with a weight decay of 1e\u221241e^{-4}1e\u22124. Once again, I used Ranger as the optimizer with cosine decay starting at fifty percent of total training steps.\n\nLikewise, in addition to random crops, final training used a small amount random zoom, warp, 45 degrees of rotation, flipping, channel dropout, and random noise. Augmentation details can be seen in the [DataBlock appendix](https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html#appendix-datablocks).\n\nI selected the best performing epoch via Jaccard, which was between 0.894 and 0.905 across all five folds.\n\n## [](benjaminwarner.dev/2022/03/11...) Submitting Predictions\n\nPredictions on the test set were largely unchanged. The one exception: predi...",
      "url": "https://benjaminwarner.dev/2022/03/11/detecting-cloud-cover-via-satellite.html"
    },
    {
      "title": "Planet: Understanding the Amazon from Space",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space/overview/evaluation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space/overview/evaluation)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space/overview/evaluation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcompetitions%2Fplanet-understanding-the-amazon-from-space%2Foverview%2Fevaluation)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcompetitions%2Fplanet-understanding-the-amazon-from-space%2Foverview%2Fevaluation)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 4566 failed.\n(error: https://www.kaggle.com/static/assets/4566.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 4566 failed.\n(error: https://www.kaggle.com/static/assets/4566.f5129f48620a03126642.css)\n    at c.onerror.c.onload (https://www.kaggle.com/static/assets/runtime.js?v=4304d4d772e29b876ce7:1:8255)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space/overview/evaluation"
    },
    {
      "title": "Planet: Understanding the Amazon from Space, 1st Place Winner\u2019s Interview",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbf66fb444bc2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fplanet-understanding-the-amazon-from-space-1st-place-winners-interview-bf66fb444bc2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fplanet-understanding-the-amazon-from-space-1st-place-winners-interview-bf66fb444bc2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**Kaggle Blog**](https://medium.com/kaggle-blog?source=post_page---publication_nav-4b0982ce16a3-bf66fb444bc2---------------------------------------)\n\n\u00b7\n\nOfficial Kaggle Blog!\n\n# Planet: Understanding the Amazon from Space, 1st Place Winner\u2019s Interview\n\n[Kaggle Team](https://medium.com/@kaggleteam?source=post_page---byline--bf66fb444bc2---------------------------------------)\n\n9 min read\n\n\u00b7\n\nDec 2, 2019\n\n--\n\nListen\n\nShare\n\nOriginally published: 10.17.2017\n\nPress enter or click to view image in full size\n\nIn our recent [Planet: Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space) competition, [Planet](https://www.planet.com/) challenged the Kaggle community to label satellite images from the Amazon basin, in order to better track and understand causes of deforestation.\n\nThe competition contained over 40,000 training images, each of which could contain multiple labels, generally divided into the following groups:\n\n- **Atmospheric conditions:** clear, partly cloudy, cloudy, and haze\n- **Common land cover and land use types:** rainforest, agriculture, rivers, towns/cities, roads, cultivation, and bare ground\n- **Rare land cover and land use types:** slash and burn, selective logging, blooming, conventional mining, artisanal mining, and blow down\n\nWe recently talked to user [bestfitting](https://www.kaggle.com/bestfitting), the winner of the competition, to learn how he used an ensemble of 11 finely tuned convolutional nets, models of label correlation structure, and a strong focus on avoiding overfitting, to achieve 1st place.\n\nPress enter or click to view image in full size\n\n# Basics\n\n## What was your background prior to entering this challenge?\n\nI majored in computer science and have more than 10 years of experience programming in Java and working on large-scale data processing, machine learning, and deep learning.\n\n## Do you have any prior experience or domain knowledge that helped you succeed in this competition?\n\nI entered a few deep learning competitions on Kaggle this year. The experiences and the intuition I gained helped a lot.\n\n## How did you get started competing on Kaggle?\n\nI\u2019ve been reading a lot of books and papers about machine learning and deep learning since 2010, but I always found it hard to apply the algorithms I learned on the kinds of small datasets that are usually available. So I found Kaggle a great platform, with all the interesting datasets, kernels, and great discussions. I couldn\u2019t wait to try something, and entered the \u201cPredicting Red Hat Business Value\u201d competition last year.\n\n## What made you decide to enter this competition?\n\nI entered this competition for two reasons.\n\nFirst, I\u2019m interested in nature conservation. I think it\u2019s cool to use my skills to make our planet and life better. So I\u2019ve entered all the competitions of this kind that Kaggle has hosted this year. And I\u2019m especially interested in the Amazon rainforest since it appears so often in films and stories.\n\nSecond, I\u2019ve entered all kinds of deep learning competitions on Kaggle using algorithms like segmentation and detection, so I wanted a classification challenge to try something different.\n\n# Let\u2019s Get Technical\n\n## Can you introduce your solution briefly first?\n\nThis is a multi-label classification challenge, and the labels are imbalanced.\n\nIt\u2019s a hard competition, as image classification algorithms have been widely used and built upon in recent years, and there are many experienced computer vision competitors.\n\nI tried many kinds of popular classification algorithms that I thought might be helpful, and based on careful analysis of label relationships and model capabilities, I was able to build an ensemble method that won 1st place.\n\nThis was my model\u2019s architecture:\n\nPress enter or click to view image in full size\n\nIn words:\n\n- First, I **preprocessed** the dataset (by resizing the images and removing haze), and applied several standard data augmentation techniques.\n- Next, for my models, I fine-tuned **11 convolutional neural networks** (I used a variety of popular, high-performing CNNs like ResNets, DenseNets, Inception, and SimpleNet) to get a set of class label probabilities for each CNN.\n- I then passed each CNN\u2019s class label probabilities through its own ridge regression model, in order to adjust the probabilities to take advantage of **label correlations**.\n- Finally, I **ensembled** all 11 CNNs, by using another ridge regression model.\n- Also of note is that instead of using a standard log loss as my loss function, I used a **special soft F2-loss** in order to get a better score on the F2 evaluation metric.\n\n## What preprocessing and feature engineering did you do?\n\nI used several preprocessing and data augmentation steps.\n\n- First, I resized images.\n- I also added data augmentation by flipping, rotating, transposing, and elastic transforming images in my training and test sets.\n- I also used a **haze removal technique**, described in this [\u201cSingle Image Haze Removal using Dark Channel Prior\u201d](https://www.robots.ox.ac.uk/~vgg/rg/papers/hazeremoval.pdf) paper, to help my networks \u201csee\u201d the images more clearly.\n\nHere are some examples of haze removal on the dataset:\n\nPress enter or click to view image in full size\n\nAs we can see in the following chart, haze removal improved the F2 score of some labels (e.g., _water_ and _bare\\_ground_), but decreased the F2 score of others (e.g., _haze_ and _clear_). However, this was fine since ensembling can select the strongest models for each label, and the haze removal trick helped overall.\n\nPress enter or click to view image in full size\n\n## What supervised learning methods did you use?\n\nThe base of my ensemble consisted of 11 popular convolutional networks: a mixture of ResNets and DenseNets with different numbers of parameters and layers, as well an Inception and SimpleNet model. I fine-tuned all layers of these pre-trained CNNs after replacing the final output layer to meet the competition\u2019s output, and I didn\u2019t freeze any layers.\n\nThe training set consisted of 40,000+ images, so would have been large enough to even train some of these CNN architectures from scratch (e.g., resnet\\_34 and resnet\\_50), but I found that fine-tuning the weights of the pre-trained network performed a little better.\n\n## Did you use any special techniques to model the evaluation metric?\n\nSubmissions were evaluated on their F2 score, which is a way of combining precision and recall into a single score \u2014 like the F1 score, but with recall weighted higher than precision. Thus, we needed not only to train our models to predict label probabilities, but also had to select optimum thresholds to determine whether or not to select a label given its probability.\n\nAt first, like many other competitors, I used log los...",
      "url": "https://medium.com/kaggle-blog/planet-understanding-the-amazon-from-space-1st-place-winners-interview-bf66fb444bc2"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - satellite-image-deep-learning/techniques: Techniques for deep learning with satellite &amp; aerial imagery\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/satellite-image-deep-learning/techniques)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/satellite-image-deep-learning/techniques)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=satellite-image-deep-learning/techniques)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[satellite-image-deep-learning](https://github.com/satellite-image-deep-learning)/**[techniques](https://github.com/satellite-image-deep-learning/techniques)**Public\n* ### Uh oh!\nThere was an error while loading.[Please reload this page]().\n* [Notifications](https://github.com/login?return_to=/satellite-image-deep-learning/techniques)You must be signed in to change notification settings\n* [Fork1.6k](https://github.com/login?return_to=/satellite-image-deep-learning/techniques)\n* [Star9.9k](https://github.com/login?return_to=/satellite-image-deep-learning/techniques)\nTechniques for deep learning with satellite &amp; aerial imagery\n### License\n[Apache-2.0 license](https://github.com/satellite-image-deep-learning/techniques/blob/master/LICENSE)\n[9.9kstars](https://github.com/satellite-image-deep-learning/techniques/stargazers)[1.6kforks](https://github.com/satellite-image-deep-learning/techniques/forks)[Branches](https://github.com/satellite-image-deep-learning/techniques/branches)[Tags](https://github.com/satellite-image-deep-learning/techniques/tags)[Activity](https://github.com/satellite-image-deep-learning/techniques/activity)\n[Star](https://github.com/login?return_to=/satellite-image-deep-learning/techniques)\n[Notifications](https://github.com/login?return_to=/satellite-image-deep-learning/techniques)You must be signed in to change notification settings\n# satellite-image-deep-learning/techniques\nmaster\n[Branches](https://github.com/satellite-image-deep-learning/techniques/branches)[Tags](https://github.com/satellite-image-deep-learning/techniques/tags)\n[](https://github.com/satellite-image-deep-learning/techniques/branches)[](https://github.com/satellite-image-deep-learning/techniques/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[1,403 Commits](https://github.com/satellite-image-deep-learning/techniques/commits/master/)\n[](https://github.com/satellite-image-deep-learning/techniques/commits/master/)\n|\n[.github](https://github.com/satellite-image-deep-learning/techniques/tree/master/.github)\n|\n[.github](https://github.com/satellite-image-deep-learning/techniques/tree/master/.github)\n|\n|\n|\n[images](https://github.com/satellite-image-deep-learning/techniques/tree/master/images)\n|\n[images](https://github.com/satellite-image-deep-learning/techniques/tree/master/images)\n|\n|\n|\n[.gitignore](https://github.com/satellite-image-deep-learning/techniques/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/satellite-image-deep-learning/techniques/blob/master/.gitignore)\n|\n|\n|\n[.mlc\\_config.json](https://github.com/satellite-image-deep-learning/techniques/blob/master/.mlc_config.json)\n|\n[.mlc\\_config.json](https://github.com/satellite-image-deep-learning/techniques/blob/master/.mlc_config.json)\n|\n|\n|\n[LICENSE](https://github.com/satellite-image-deep-learning/techniques/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/satellite-image-deep-learning/techniques/blob/master/LICENSE)\n|\n|\n|\n[README.md](https://github.com/satellite-image-deep-learning/techniques/blob/master/README.md)\n|\n[README.md](https://github.com/satellite-image-deep-learning/techniques/blob/master/README.md)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n[![](https://github.com/satellite-image-deep-learning/techniques/raw/master/images/logo.png)](https://www.satellite-image-deep-learning.com/)\n# \ud83d\udc49[satellite-image-deep-learning.com](https://www.satellite-image-deep-learning.com/)\ud83d\udc48\n[](#-satellite-image-deep-learningcom-)\n## Introduction\n[](#introduction)\nDeep learning has revolutionized the analysis and interpretation of satellite and aerial imagery, addressing unique challenges such as vast image sizes and a wide array of object classes. This repository provides an exhaustive overview of deep learning techniques specifically tailored for satellite and aerial image processing. It covers a range of architectures, models, and algorithms suited for key tasks like classification, segmentation, and object detection.\n**How to use this repository:**use`Command + F`(Mac) or`CTRL + F`(Windows) to search this page for e.g. 'SAM'\n## Techniques\n[](#techniques)\n* [Classification](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#classification)\n* [Segmentation](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#segmentation)\n* [Object detection](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#object-detection)\n* [Regression](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#regression)\n* [Cloud detection &amp; removal](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#cloud-detection--removal)\n* [Change detection](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#change-detection)\n* [Time series](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#time-series)\n* [Crop classification](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#crop-classification)\n* [Crop yield &amp; vegetation forecasting](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#crop-yield--vegetation-forecasting)\n* [Generative networks](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#generative-networks)\n* [Autoencoders, dimensionality reduction, image embeddings &amp; similarity search](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#autoencoders-dimensionality-reduction-image-embeddings--similarity-search)\n* [Few &amp; zero shot learning](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#few--zero-shot-learning)\n* [Self-supervised, unsupervised &amp; contrastive learning](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#self-supervised-unsupervised--contrastive-learning)\n* [SAR](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#sar)\n* [Large vision &amp; language models (LLMs &amp; LVMs)](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#large-vision--language-models-llms--lvms)\n* [Foundational models](https://github.com/satellite-image-deep-learning/techniques?tab=readme-ov-file#foundational...",
      "url": "https://github.com/satellite-image-deep-learning/techniques"
    },
    {
      "title": "GitHub - drivendataorg/cloud-cover-winners: Code from the winning submissions for the On Cloud N: Cloud Cover Detection Challenge",
      "text": "[Skip to content](https://github.com/drivendataorg/cloud-cover-winners#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/drivendataorg/cloud-cover-winners) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/drivendataorg/cloud-cover-winners) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/drivendataorg/cloud-cover-winners) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[drivendataorg](https://github.com/drivendataorg)/ **[cloud-cover-winners](https://github.com/drivendataorg/cloud-cover-winners)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fdrivendataorg%2Fcloud-cover-winners) You must be signed in to change notification settings\n- [Fork\\\n4](https://github.com/login?return_to=%2Fdrivendataorg%2Fcloud-cover-winners)\n- [Star\\\n22](https://github.com/login?return_to=%2Fdrivendataorg%2Fcloud-cover-winners)\n\n\nCode from the winning submissions for the On Cloud N: Cloud Cover Detection Challenge\n\n### License\n\n[MIT license](https://github.com/drivendataorg/cloud-cover-winners/blob/main/LICENSE)\n\n[22\\\nstars](https://github.com/drivendataorg/cloud-cover-winners/stargazers) [4\\\nforks](https://github.com/drivendataorg/cloud-cover-winners/forks) [Branches](https://github.com/drivendataorg/cloud-cover-winners/branches) [Tags](https://github.com/drivendataorg/cloud-cover-winners/tags) [Activity](https://github.com/drivendataorg/cloud-cover-winners/activity)\n\n[Star](https://github.com/login?return_to=%2Fdrivendataorg%2Fcloud-cover-winners)\n\n[Notifications](https://github.com/login?return_to=%2Fdrivendataorg%2Fcloud-cover-winners) You must be signed in to change notification settings\n\n# drivendataorg/cloud-cover-winners\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/drivendataorg/cloud-cover-winners/branches) [Tags](https://github.com/drivendataorg/cloud-cover-winners/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[5 Commits](https://github.com/drivendataorg/cloud-cover-winners/commits/main/) |\n| [1st place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/1st%20place) | [1st place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/1st%20place) |  |  |\n| [2nd place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/2nd%20place) | [2nd place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/2nd%20place) |  |  |\n| [3rd place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/3rd%20place) | [3rd place](https://github.com/drivendataorg/cloud-cover-winners/tree/main/3rd%20place) |  |  |\n| [LICENSE](https://github.com/drivendataorg/cloud-cover-winners/blob/main/LICENSE) | [LICENSE](https://github.com/drivendataorg/cloud-cover-winners/blob/main/LICENSE) |  |  |\n| [README.md](https://github.com/drivendataorg/cloud-cover-winners/blob/main/README.md) | [README.md](https://github.com/drivendataorg/cloud-cover-winners/blob/main/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n[![](https://camo.githubusercontent.com/1bf4fb85a46b0244b86e9621b04aa537febad261c5d45277582bb1b62331b19f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f64726976656e646174612d7075626c69632d6173736574732f6c6f676f2d77686974652d626c75652e706e67)](https://www.drivendata.org/)\n\n[![Banner Image](https://camo.githubusercontent.com/36487f0eb9eccf44c4b4cdf4381f2d4ae7f65bff401f60675499051684c0872f/68747470733a2f2f64726976656e646174612d7075626c69632d6173736574732e73332e616d617a6f6e6177732e636f6d2f636c6f75642d636f7665722d62616e6e65722e6a7067)](https://camo.githubusercontent.com/36487f0eb9eccf44c4b4cdf4381f2d4ae7f65bff401f60675499051684c0872f/68747470733a2f2f64726976656e646174612d7075626c69632d6173736574732e73332e616d617a6f6e6177732e636f6d2f636c6f75642d636f7665722d62616e6e65722e6a7067)\n\n# On Cloud N: Cloud Cover Detection Challenge\n\n## Goal of the Competition\n\nSatellite imagery is critical for a wide variety of applications from disaster management and recovery, to agriculture, to military intelligence. Clouds present a major obstacle for all of these use cases, and usually have to be identified and removed from a dataset before satellite imagery can be used. Improving methods of identifying clouds can unlock the potential of an unlimited range of satellite imagery use cases, enabling faster, more efficient, and more accurate image-based research.\n\nIn this challenge, participants used machine learning to better detect cloud cover in satellite imagery from the [Sentinel-2](https://sentinel.esa.int/web/sentinel/missions/sentinel-2) mission. Sentinel-2 captures wide-swath, high-resolution, multi-spectral images from around the world, and is publicly available through Microsoft's [Planetary Computer](https://planetarycomputer.microsoft.com/).\n\n## What's in this Repository\n\nThis repository contains code from winning competitors in the [On Cloud N: Cloud Cover Detection](https://www.drivendata.org/competitions/83/cloud-cover/) DrivenData challenge. Code for all winning solutions are open source under the MIT License.\n\n**Winning code for other DrivenData competitions is available in the [competition-winners repository](https://github.com/drivendataorg/competition-winners).**\n\n## Winning Submissions\n\nModel scores represent a metric called [Jaccard index](https://www.drivendata.org/competitions/83/cloud-cover/page/398/#performance-metric), also known as Intersection Over Union (IoU).\n\n| Place | Team or User | Public Score | Private Score | Summary of Model |\n| --- | --- | --- | --- | --- |\n| 1 | [adityakumarsinha](https://www.drivendata.org/users/adityakumarsinha/) | 0.8988 | 0.8974 | An ensemble of two UNET model backbones was trained on significantly augmented images, including a custom augmentation for images with low cloud cover. Each model is trained separately on multiple folds of the data, generating 8 model weights whose predictions are averaged. |\n| 2 | Team IG NB: [XCZhou](https://www.drivendata.org/users/XCZhou/), [cloud\\_killer](https://www.drivendata.org/users/cloud_killer/), [Sunyueming](https://www.drivendata.org/users/Sunyueming/), [mymoon](https://www.drivendata.org/users/mymoon/), [windandcloud](https://www.drivendata.org/users/windandcloud/) | 0.8986 | 0.8972 | The data is cleaned and then randomly divided into training and validation sets five times. A base UNet++ model is trained on each random split, and the best-performing model is selected for integration. TTA is performed on the trained model with efficientnetv2\\_rw\\_s as an encoder during testing to increase the generalizability. |\n| 3 | [Victor](https://www.drivendata.org/users/Victor/) | 0.83975 | 0.8961 | The data is split into train/validation by location. 8 Unet-like models with different encoders are trained on the best single fold and then ensembled. Jaccard index is used as a metric, but calculated for each location separately and then averaged. Images are augmented before training and at test time. |\n\nAdditional solution details can be found in the `reports` folder inside the directory for each submission.\n\n**Benchmark Blog Post: [\"How to use deep learning, PyTorch lightning, and the Planetary Computer to predict cloud cover in satellite imagery\"](https://www.drivendata.co/blog/cloud-cover-benchmark/)**\n\n## About\n\nCode from the winning submissions for the On Cloud N: Cloud Cover Detection Challenge\n\n### Resources\n\n[Readme](https://github.com/drivendataorg/cloud-cover-winners#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/drivendataorg/cloud-cover-winners#MIT-1-ov-file)\n\n[Activity](https://github.com/drivendataorg/cloud-cover-winners/activity)\n\n[Custom properties](https://github.com/drivendataorg/cloud-cover-winners/custom-properties)\n\n### Stars\n\n[**22**\\\nstars](...",
      "url": "https://github.com/drivendataorg/cloud-cover-winners"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - aiir-team/satellite-image-deep-learning: Resources for deep learning with satellite &amp; aerial imagery\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/aiir-team/satellite-image-deep-learning)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/aiir-team/satellite-image-deep-learning)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=aiir-team/satellite-image-deep-learning)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[aiir-team](https://github.com/aiir-team)/**[satellite-image-deep-learning](https://github.com/aiir-team/satellite-image-deep-learning)**Public\nforked from[satellite-image-deep-learning/techniques](https://github.com/satellite-image-deep-learning/techniques)\n* [Notifications](https://github.com/login?return_to=/aiir-team/satellite-image-deep-learning)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/aiir-team/satellite-image-deep-learning)\n* [Star1](https://github.com/login?return_to=/aiir-team/satellite-image-deep-learning)\nResources for deep learning with satellite &amp; aerial imagery\n### License\n[Apache-2.0 license](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/LICENSE)\n[1star](https://github.com/aiir-team/satellite-image-deep-learning/stargazers)[1.6kforks](https://github.com/aiir-team/satellite-image-deep-learning/forks)[Branches](https://github.com/aiir-team/satellite-image-deep-learning/branches)[Tags](https://github.com/aiir-team/satellite-image-deep-learning/tags)[Activity](https://github.com/aiir-team/satellite-image-deep-learning/activity)\n[Star](https://github.com/login?return_to=/aiir-team/satellite-image-deep-learning)\n[Notifications](https://github.com/login?return_to=/aiir-team/satellite-image-deep-learning)You must be signed in to change notification settings\n# aiir-team/satellite-image-deep-learning\nmaster\n[Branches](https://github.com/aiir-team/satellite-image-deep-learning/branches)[Tags](https://github.com/aiir-team/satellite-image-deep-learning/tags)\n[](https://github.com/aiir-team/satellite-image-deep-learning/branches)[](https://github.com/aiir-team/satellite-image-deep-learning/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[964 Commits](https://github.com/aiir-team/satellite-image-deep-learning/commits/master/)\n[](https://github.com/aiir-team/satellite-image-deep-learning/commits/master/)\n|\n[.github](https://github.com/aiir-team/satellite-image-deep-learning/tree/master/.github)\n|\n[.github](https://github.com/aiir-team/satellite-image-deep-learning/tree/master/.github)\n|\n|\n|\n[.gitignore](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/.gitignore)\n|\n|\n|\n[.mlc\\_config.json](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/.mlc_config.json)\n|\n[.mlc\\_config.json](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/.mlc_config.json)\n|\n|\n|\n[CITATION.cff](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/CITATION.cff)\n|\n[CITATION.cff](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/CITATION.cff)\n|\n|\n|\n[LICENSE](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/LICENSE)\n|\n|\n|\n[README.md](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/README.md)\n|\n[README.md](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/README.md)\n|\n|\n|\n[logo.png](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/logo.png)\n|\n[logo.png](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/logo.png)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n[![](https://github.com/aiir-team/satellite-image-deep-learning/raw/master/logo.png)](https://github.com/aiir-team/satellite-image-deep-learning/blob/master/logo.png)\nThis page lists resources for performing deep learning on satellite imagery. To a lesser extent classical Machine learning (e.g. random forests) are also discussed, as are classical image processing techniques. Note there is a huge volume of academic literature published on these topics, and this repository does not seek to index them all but rather list approachable resources with published code that will benefit both the research*and*developer communities. If you find this work useful please give it a star and consider sponsoring it. You can also follow me on Twitter and LinkedIn where I aim to post frequent updates on my new discoveries, and I have created a dedicated[group on LinkedIn](https://www.linkedin.com/groups/12698393/). I have also started a blog[here](https://robmarkcole.com/)and have published a post on the history of this repository called[Dissecting the satellite-image-deep-learning repo](https://robmarkcole.com/markdown/2022/07/24/satellite-image-deep-learning.html)If you use this work in your research please cite using the citation information on the right. Thanks!\n[![Twitter Follow](https://camo.githubusercontent.com/7b2ba6f40ae5d4e3187a3fa162147db64ded7d2f99a593d8ba5c1fe7478d00a4/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f726f626d61726b636f6c653f6c6162656c3d466f6c6c6f77)](https://twitter.com/robmarkcole)[![Linkedin: robmarkcole](https://camo.githubusercontent.com/3dce4e1e942d4c492e2ebf578c0c98968d2ee5ae8b71e04476805879ff466f53/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d526f62696e253230436f6c652d626c75653f7374796c653d666c61742d737175617265266c6f676f3d4c696e6b6564696e266c6f676f436f6c6f723d7768697465266c696e6b3d68747470733a2f2f7777772e6c696e6b6564696e2e636f6d2f696e2f726f626d61726b636f6c652f)](https://www.linkedin.com/in/robmarkcole/)[![Google Scholar Badge](https://camo.githubusercontent.com/0f4b6c6bc4295adb4ef25f2ef7931f68bc156a3933c06bd0109f9e815a077f14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f476f6f676c652d5363686f6c61722d726564)](https://scholar.google.com/citations?user=oHe5ozwAAAAJ&amp;hl=en)\n# Table of contents\n[](#table-of-contents)\n* [Techniques](https://github.com/robmarkcole/satellite-image-deep-learning#techniques)\n* [Classification](https://github.com/robmarkcole/satellite-image-deep-learning#Classification)\n* [Segmentation](https://github.com/robmarkcole/satellite-image-deep-learning#Segmentation)\n* [Land use &amp; land cover](https://github.com/robmarkcole/satellite-image-deep-learning#segmentation---land-use--land-cover)\n* [Vegetation, crops &amp; crop boundaries](https://github.com/robmarkcole/satellite-image-deep-learning#segmentation---vegetation-crops--crop-boundaries)\n* [Water,...",
      "url": "https://github.com/aiir-team/satellite-image-deep-learning"
    }
  ]
}