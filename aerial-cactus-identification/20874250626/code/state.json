{
  "phase": "executor",
  "loop_count": 1,
  "last_submission": null,
  "competition_id": "aerial-cactus-identification",
  "seed_prompt": "# Seed Prompt: Aerial Cactus Identification\n\n## Problem Type\nBinary image classification (aerial imagery) with AUC evaluation metric. Images are 32x32 RGB thumbnails.\n\n**Reference notebooks for data characteristics:**\n- `eda.ipynb` - Contains full EDA: class distribution (~75% positive), image dimensions (32x32 RGB), sample visualizations\n\n## Model Architectures\n\n### Primary Approaches for Small 32x32 Images\nFor small-resolution image classification, winning Kaggle solutions typically use:\n- **ResNet-18/34**: Well-suited for small images when trained from scratch or fine-tuned\n- **Wide-ResNet**: Provides better performance on small datasets through increased width rather than depth\n- **EfficientNet-B0**: Good balance of accuracy and efficiency for small images\n- **Custom CNNs**: Simple architectures with 4-6 convolutional layers often work well for 32x32 images\n\n### Aerial/Satellite Imagery Specific\nFor aerial imagery competitions, successful approaches include:\n- **Encoder-decoder architectures**: U-Net style architectures can be effective even for classification\n- **Multiscale processing**: Process images at multiple scales to capture different levels of detail\n- **Spectral-aware models**: Since these are RGB aerial images, standard CNNs work well\n\n## AUC Optimization (Critical for This Competition)\n\n### AUC-Specific Loss Functions\nSince evaluation is AUC, consider these specialized approaches:\n- **AUC Margin Loss**: Use margin-based min-max surrogate from LibAUC library\n- **AUCMLoss**: More robust than classic AUC square loss, available in LibAUC\n- **PESG Optimizer**: Projected Stochastic Gradient optimizer designed for AUC maximization\n- **Loss-Conditional Training (LCT)**: Sample from family of AUC-oriented losses during training\n\n### Implementation\n- Use LibAUC library for AUC-specific losses and optimizers\n- Directly maximize AUC surrogate rather than using cross-entropy\n- More effective than traditional approaches for AUC metric optimization\n\n## Data Augmentation Strategies\n\n### Essential for Small Images\nHeavy augmentation is critical for 32x32 images to prevent overfitting:\n- **Geometric transforms**: Random rotations (\u00b115\u00b0), horizontal/vertical flips, random crops\n- **Color augmentation**: Color jitter, brightness/contrast adjustments, hue shifts\n- **Advanced techniques**: CutMix, MixUp, Random Erasing\n- **Test-time augmentation (TTA)**: Multi-scale and flip averaging during inference\n\n### Class-Aware Augmentation\nGiven the class imbalance (~75% positive):\n- Apply more aggressive augmentation to minority class (no cactus)\n- Use class-balanced sampling in data loaders\n- Consider oversampling minority class during training\n\n## Handling Class Imbalance\n\n### Effective Techniques\nFor the ~75%/25% class distribution:\n1. **Loss weighting**: Assign higher weights to minority class samples\n2. **Label smoothing**: Apply 0.1 smoothing to reduce overconfidence on dominant class\n3. **Batch composition**: Ensure minority class appears frequently in each batch\n4. **Focal Loss**: Consider for hard example mining\n5. **Class-balanced sampling**: Oversample minority class or undersample majority class\n\n### Validation Strategy\n- Use stratified splits to maintain class distribution\n- Monitor both AUC and per-class metrics\n- Early stopping based on validation AUC\n\n## Training Strategies\n\n### Optimization\n- **Optimizer**: AdamW or SGD with momentum (0.9)\n- **Learning rate**: Start with 1e-3 (Adam) or 1e-1 (SGD), use cosine annealing with warm-up\n- **Batch size**: 32-128 (smaller batches may help with class imbalance)\n- **Weight decay**: 1e-4 to 1e-2 for regularization\n\n### Regularization\n- **Dropout**: 0.2-0.5 in fully connected layers\n- **Weight decay**: Essential for small datasets\n- **Early stopping**: Monitor validation AUC\n- **Stochastic Weight Averaging (SWA)**: Often improves generalization\n\n## Test-Time Augmentation (TTA)\n\n### Effective TTA Strategies\n- **Deterministic transforms**: Use fixed set of augmentations rather than random\n- **Horizontal/vertical flips**: Most effective and computationally cheap\n- **Rotations**: \u00b190\u00b0 rotations work well for aerial imagery\n- **Multi-scale**: Limited benefit for 32x32 images but can help\n- **Averaging**: Average predictions across all augmented versions\n\n### Implementation\n- Apply 5-10 different augmentations during inference\n- Weight predictions equally or by validation performance\n- Typically provides 1-2% AUC improvement\n\n## Ensembling\n\n### Model Ensembling Strategies\n- **Diverse architectures**: Combine ResNet, EfficientNet, and custom CNNs\n- **Snapshot ensembling**: Use multiple checkpoints from same training run\n- **Test-time augmentation**: Average predictions across multiple augmentations\n- **Weighted averaging**: Weight models by validation performance\n\n### Recommended Ensemble Size\n- 3-5 models typically sufficient for small datasets\n- Focus on diversity rather than quantity\n- Consider both architecture diversity and training seed diversity\n\n## Preprocessing\n\n### Image Normalization\n- Standard ImageNet normalization works well\n- Consider dataset-specific normalization if time permits\n- Ensure consistent preprocessing between train and test\n\n### Data Loading\n- Use efficient data loading (PyTorch DataLoader with multiple workers)\n- Cache decoded images if memory permits\n- Consider prefetching for faster training\n\n## Validation Strategy\n\n### Cross-Validation\n- **Stratified K-Fold (k=5)**: Maintain class distribution in each fold\n- **Hold-out validation**: 80/20 split if computational resources limited\n- **Multiple seeds**: Run experiments with different random seeds\n\n### Performance Monitoring\n- **Primary metric**: AUC (as per competition)\n- **Secondary metrics**: Accuracy, precision, recall, F1\n- **Per-class performance**: Ensure good performance on minority class\n\n## Implementation Notes\n\n### Framework Recommendations\n- **PyTorch**: Flexible and widely used in Kaggle competitions\n- **PyTorch Lightning**: Provides reproducible training loops and best practices\n- **Mixed precision**: Use for faster training (FP16)\n\n### Computational Efficiency\n- Use progressive resizing if training from scratch\n- Consider knowledge distillation for model compression\n- Use gradient accumulation for large effective batch sizes\n\n## Competition-Specific Considerations\n\n### Kernels-Only Competition\n- Optimize for inference speed and memory usage\n- Consider model size limitations\n- Use efficient architectures that balance accuracy and speed\n- Pre-compute features if possible within kernel constraints\n\n### Aerial Imagery Characteristics\n- Cacti may appear at various orientations \u2192 use rotation augmentation\n- Lighting variations \u2192 use color augmentation\n- Small objects in 32x32 images \u2192 ensure model can capture fine details\n- Potential for similar backgrounds \u2192 focus on discriminative features\n\n## Key Success Factors\n1. **AUC-specific optimization**: Use specialized losses (AUC Margin Loss, AUCMLoss) and optimizers (PESG)\n2. **Strong augmentation pipeline**: Critical for small images\n3. **Class imbalance handling**: Important for optimal AUC\n4. **Test-time augmentation**: Often provides 1-2% boost\n5. **Model ensembling**: 3-5 diverse models\n6. **Careful validation**: Stratified splits, monitor AUC closely\n7. **Regularization**: Essential for small dataset",
  "evaluator_feedback": "",
  "latest_experiment": {},
  "competitive_intel": "",
  "target_score": 1.0,
  "manual_instructions": "",
  "messages": [],
  "experiments": [],
  "submissions": [],
  "candidates": [],
  "findings": [],
  "metric_lower_is_better": false,
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [],
  "web_research": [
    {
      "query": "What are winning approaches for image classification competitions on Kaggle with small 32x32 images?",
      "finding": "Winning Kaggle solutions for 32\u202f\u00d7\u202f32 image tasks (e.g., CIFAR\u201110 style competitions) combine a **strong, well\u2011regularized CNN baseline** with aggressive **data\u2011augmentation, careful hyper\u2011parameter tuning, and model\u2011level ensembling**.  A common starting point is a small\u2011resolution\u2011friendly architecture such as ResNet\u201118/34, Wide\u2011ResNet, or EfficientNet\u2011B0 that is either trained from scratch or fine\u2011tuned from ImageNet weights using the \u201cbottleneck\u2011features + top\u2011layer fine\u2011tuning\u201d recipe described by Chollet\u202f(2016)\u202f([keras.io](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)).  Because 32\u202f\u00d7\u202f32 images provide very little spatial information, heavy augmentation (random crops, horizontal flips, color jitter, CutMix/MixUp) is essential to prevent over\u2011fitting and to expose the model to diverse patterns\u202f([Towards Data Science, 2019](https://towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259",
      "agent": "explorer"
    },
    {
      "query": "What are winning approaches for aerial image classification and satellite imagery competitions on Kaggle?",
      "finding": "Aerial\u2011image and satellite\u2011imagery contests on Kaggle (e.g., Planet\u202fAmazon, DSTL Feature Detection, On\u202fCloud\u202fN) are typically won by pipelines that treat the data as a multispectral segmentation problem and apply modern fully\u2011convolutional networks (FCNs) or encoder\u2011decoder architectures such as U\u2011Net.  The top solutions first adapt a standard CNN backbone (ResNet, EfficientNet, or a transformer\u2011based encoder) to accept the extra spectral channels, then train it with a loss that balances pixel\u2011wise accuracy and boundary quality (e.g., a combination of cross\u2011entropy and Dice or a boundary\u2011aware term)\u202f\u30103\u2020https://arxiv.org/abs/1706.06169\u3011.  Heavy use of data\u2011augmentation\u2014random rotations, flips, scaling, and spectral\u2011index augmentations (e.g., NDVI, reflectance indices)\u2014helps the model generalise to the wide variety of lighting and seasonal conditions found in satellite data\u202f\u30103\u2020https://arxiv.org/abs/1706.06169\u3011.\n\nWinning teams also exploit the specific sensor information that the competit",
      "agent": "explorer"
    },
    {
      "query": "How do Kaggle winners handle class imbalance in image classification competitions?",
      "finding": "Based on the winning\u2011team write\u2011ups and recent research, Kaggle image\u2011classification champions typically handle class imbalance by fine\u2011tuning the standard training pipeline rather than adding exotic loss functions. The most common practical steps are:\n\n1. **Adjust the batch composition** \u2013 use a smaller (or larger) batch size so that minority\u2011class samples appear more frequently within each update.  \n2. **Apply strong, class\u2011aware data augmentation** \u2013 augment minority\u2011class images more aggressively (e.g., random flips, rotations, color jitter) to increase their effective sample count.  \n3. **Choose an optimizer and learning\u2011rate schedule that cope with noisy gradients** \u2013 optimizers such as AdamW or SGD with warm\u2011up help stabilize training when the data distribution is skewed.  \n4. **Add label smoothing** \u2013 smoothing the one\u2011hot targets (e.g.,\u202f0.1\u202ffor the positive class) reduces over\u2011confidence on the dominant class and improves calibration on rare classes.  \n5. **Optionally weight t",
      "agent": "explorer"
    },
    {
      "query": "How do Kaggle winners optimize for AUC metric in binary image classification competitions?",
      "finding": "Based on recent research on deep AUC maximization and a documented first\u2011place Kaggle solution, winners typically follow these core steps to squeeze the highest AUROC out of binary image\u2011classification tasks\u202f([arXiv:2111.02400](https://arxiv.org/abs/2111.02400); [arXiv:2012.03173](https://arxiv.org/abs/2012.03173); [arXiv:2402.05400v2](https://arxiv.org/html/2402.05400v2); [bjlkeng.io blog](https://bjlkeng.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition)):\n\n1. **Pick an AUC\u2011specific surrogate loss**  \n   - Use a margin\u2011based min\u2011max surrogate (the \u201cAUC margin loss\u201d) or the `AUCMLoss` implementation from the LibAUC library, which is more robust than the classic AUC square loss\u202f([arXiv:2012.03173](https://arxiv.org/abs/2012.03173); [LibAUC docs](https://docs.libauc.org/examples/auroc.html)).  \n\n2. **Train with an optimizer designed for AUC loss**  \n   - Apply the PESG (Projected Stochastic Gradient) optimizer (or other stochastic algorithms ",
      "agent": "explorer"
    },
    {
      "query": "What test-time augmentation techniques work best for image classification on Kaggle competitions?",
      "finding": "As of\u202fFebruary\u202f10\u202f2024, the most effective test\u2011time augmentation techniques for Kaggle image\u2011classification contests are deterministic transforms\u2014horizontal/vertical flips, rotations, multi\u2011scale crops (often with modest color jitter)\u2014applied as a fixed set and averaged across predictions rather than using random augmentations ([arXiv](https://arxiv.org/html/2402.06892v1)\u202fFeb\u202f2024; [stepup.ai](https://stepup.ai/test_time_data_augmentation)\u202f2020).",
      "agent": "explorer"
    }
  ],
  "prev_agent_summaries": [
    {
      "agent": "ExplorerAgent",
      "summary": "ExplorerAgent performed initial dataset reconnaissance and produced starter artifacts for the \u201cAerial Cactus Identification\u201d Kaggle-style competition.\n\n**Key actions taken**\n- Enumerated available files under `/home/data` (`description.md`, `train.csv`, `train.zip`, `test.zip`, `sample_submission.csv`).\n- Read the competition description to identify the task as binary image classification on aerial thumbnails with **AUC** as the evaluation metric.\n- Inspected `train.csv` (header + sample rows) and `sample_submission.csv` to confirm the required submission schema (`id,has_cactus`) and that predictions are probabilities.\n- Computed dataset size and label balance via shell commands (`wc -l`, `uniq -c`).\n- Created and executed an EDA notebook (`/home/code/exploration/eda.ipynb`) to load metadata and visualize sample images directly from `train.zip`.\n- Ran multiple web searches to gather common high-performing strategies for (1) small 32\u00d732 image classification, (2) aerial/satellite imagery competitions, (3) class imbalance handling, (4) optimizing for AUC, and (5) test-time augmentation (TTA).\n\n**Important findings / decisions**\n- Dataset characteristics:\n  - `train.csv` has **14176 lines including header** (\u2192 **14175 training samples**).\n  - Class distribution is **10628 positives (has_cactus=1)** and **3547 negatives (has_cactus=0)** (~75% / 25%).\n  - Images are **32\u00d732 RGB** thumbnails (confirmed in notebook via PIL).\n- Noted implications: small resolution favors relatively small CNNs; transfer learning may be less straightforward; augmentation and careful validation are important; AUC metric motivates probability calibration/robust ranking rather than accuracy alone.\n\n**Data explored / models trained**\n- No model training occurred in this run.\n- Data exploration included CSV metadata inspection, label distribution calculation, and plotting sampled positive/negative images from the training archive.\n- Discovered `train.zip` file structure: images are stored at the **zip root**, not under a `train/` subfolder.\n\n**Errors encountered**\n- Notebook image-reading initially failed with `KeyError` because the code attempted to read `train/<filename>` inside `train.zip`. The agent debugged by listing zip contents (`unzip -l`) and fixed the path to use just `<filename>`.\n- Later, while intending to update `seed_prompt.txt`, the agent mistakenly used a notebook-editing tool (`EditNotebookCell`) instead of editing the text file; it recognized the mistake and corrected it by re-writing the file properly.\n\n**Final state / outputs**\n- Created EDA notebook: **`/home/code/exploration/eda.ipynb`**, containing:\n  - Loading of `train.csv`, printing sample count and class distribution.\n  - Visualization of sample images from each class.\n  - Printout of image dimensions/mode.\n- Created and finalized **`/home/code/exploration/seed_prompt.txt`**, a structured \u201cseed prompt\u201d summarizing recommended approaches:\n  - Candidate architectures for small images (e.g., ResNet/Wide-ResNet/EfficientNet/custom CNNs).\n  - Augmentation ideas (including MixUp/CutMix/Random Erasing) and class-aware sampling.\n  - Class-imbalance tactics.\n  - AUC-focused considerations (mentions LibAUC-style surrogate losses/optimizers) and TTA guidance (deterministic flips/rotations/multi-crop averaging).\n- No submission file was generated and no training pipeline was implemented; the run ends with exploratory understanding + guidance artifacts for downstream agents.",
      "timestamp": "2026-01-10T06:50:27.331108"
    }
  ],
  "agent_rl_models": {
    "ExplorerAgent": "tinker://5cc59b0d-9415-59e8-adf9-0817ccd854eb:train:0/sampler_weights/ExplorerAgent_rl_iter1"
  },
  "remaining_submissions": 999,
  "max_submissions": 999,
  "last_reset_date_utc": "2026-01-10",
  "start_time": "2026-01-10T06:35:50.560157",
  "time_limit_minutes": 1440,
  "_saved_at": "2026-01-10T06:50:27.340805"
}