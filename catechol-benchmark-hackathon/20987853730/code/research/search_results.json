{
  "query": "How do Gaussian Processes perform for multi-output regression in chemistry with small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Multi\u2011output Gaussian processes (MOGPs) extend the standard GP framework by modelling several correlated target properties jointly, which lets the model \u201cborrow strength\u2019\u2019 from one output to improve predictions of the others.  In chemistry this is especially valuable because experimental or high\u2011level computational data are often scarce; the Bayesian nature of GPs provides calibrated uncertainty estimates that guide further measurements.  A recent review of MOGP methods highlights that knowledge transfer across related outputs can dramatically reduce the amount of data needed for accurate regression\u202f([sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123)).  \n\nConcrete chemistry examples confirm this advantage.  Griffiths\u202fet\u202fal. used a multi\u2011output GP to predict the absorption spectra of molecular photoswitches, achieving high\u2011quality predictions and reliable error bars with only a few hundred measured compounds\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h)).  Their open\u2011source GAUCHE library further streamlines the application of MOGPs to small chemical datasets and supports active\u2011learning loops that select the most informative experiments\u202f([arxiv](https://arxiv.org/abs/2212.04450)).  In a related engineering study, safe active\u2011learning with MOGPs was shown to converge quickly on tiny datasets (acceptance rate \u2248\u202f0.7 for Hamiltonian Monte\u202fCarlo), demonstrating that the approach remains computationally tractable even when data are limited\u202f([mlr.press](https://proceedings.mlr.press/v151/li22d/li22d.pdf)).  \n\nOverall, MOGPs tend to outperform single\u2011output models on small chemical datasets because they exploit inter\u2011property correlations, deliver principled uncertainty quantification, and can be combined with active\u2011learning strategies to make the most of every new measurement.  Modern software such as GPflow\u2019s multi\u2011output module provides efficient approximations that keep training times reasonable for the modest data volumes typical in chemistry\u202f([gpflow.github.io](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html)).",
      "url": ""
    },
    {
      "title": "Remarks on multi-output Gaussian process regression - ScienceDirect",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0950705117306123)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0950705117306123/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#preview-section-snippets)\n- [References (104)](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#preview-section-references)\n- [Cited by (220)](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/knowledge-based-systems)\n\n## [Knowledge-Based Systems](https://www.sciencedirect.com/journal/knowledge-based-systems)\n\n[Volume 144](https://www.sciencedirect.com/journal/knowledge-based-systems/vol/144/suppl/C), 15 March 2018, Pages 102-121\n\n[![Knowledge-Based Systems](https://ars.els-cdn.com/content/image/1-s2.0-S0950705118X00045-cov150h.gif)](https://www.sciencedirect.com/journal/knowledge-based-systems/vol/144/suppl/C)\n\n# Remarks on multi-output Gaussian process regression\n\nAuthor links open overlay panelHaitaoLiua, JianfeiCaib, Yew-SoonOngbc\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.knosys.2017.12.034](https://doi.org/10.1016/j.knosys.2017.12.034) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0950705117306123&orderBeanReset=true)\n\n## Highlights\n\n- \u2022\nState-of-the-art MOGPs were reviewed and analyzed.\n\n- \u2022\nTen representative MOGPs were investigated in different scenarios.\n\n- \u2022\nHeterotopic [training data](https://www.sciencedirect.com/topics/computer-science/training-data) enlarges information diversity for symmetric MOGPs.\n\n- \u2022\nSymmetric MOGPs favor moderate training size, while asymmetric MOGPs favor small and moderate training sizes.\n\n- \u2022\nDecomposed process helps asymmetric MOGPs perform well for complex cases.\n\n\n## Abstract\n\nMulti-output [regression problems](https://www.sciencedirect.com/topics/computer-science/regression-problem) have extensively arisen in modern engineering community. This article investigates the state-of-the-art multi-output [Gaussian processes](https://www.sciencedirect.com/topics/computer-science/gaussian-process) (MOGPs) that can transfer the knowledge across related outputs in order to improve prediction quality. We classify existing MOGPs into two main categories as (1) symmetric MOGPs that improve the predictions for all the outputs, and (2) asymmetric MOGPs, particularly the multi-fidelity MOGPs, that focus on the improvement of high fidelity output via the useful information transferred from related low fidelity outputs. We review existing symmetric/asymmetric MOGPs and analyze their characteristics, e.g., the [covariance functions](https://www.sciencedirect.com/topics/computer-science/covariance-function) (separable or non-separable), the modeling process (integrated or decomposed), the [information transfer](https://www.sciencedirect.com/topics/mathematics/information-transfer) (bidirectional or unidirectional), and the hyperparameter inference (joint or separate). Besides, we assess the performance of ten representative MOGPs thoroughly on eight examples in symmetric/asymmetric scenarios by considering, e.g., different [training data](https://www.sciencedirect.com/topics/computer-science/training-data) (heterotopic or isotopic), different training sizes (small, moderate and large), different output correlations (low or high), and different output sizes (up to four outputs). Based on the qualitative and quantitative analysis, we give some recommendations regarding the usage of MOGPs and highlight potential research directions.\n\n## Introduction\n\nComputer simulators, e.g., computational fluid dynamics (CFD) and finite element analysis (FEA), have gained popularity in many scientific fields to simulate various physical problems. For computationally expensive simulators, we usually employ surrogates to approximate the input-output relationship in order to relieve computational budget \\[1\\], \\[2\\], \\[3\\]. As a statistical surrogate model that provides not only the predictions but also the relevant uncertainty, Gaussian process (GP) has been gaining widespread applications, e.g., small- or large-scale regression \\[4\\], \\[5\\], \\[6\\], dimensionality reduction \\[7\\], Bayesian optimization \\[8\\], uncertainty quantification \\[9\\] and time-series analysis \\[10\\].\n\nTypical GPs are usually designed for single-output scenarios wherein the output is a scalar. However, the multi-output problems have arisen in various fields, e.g., environmental sensor networks \\[11\\], robot inverse dynamics \\[12\\], multivariate physiological time-series analysis \\[13\\], structural design \\[14\\], and aircraft design \\[15\\]. Suppose that we attempt to approximate _T_ outputs { _ft_}1\u202f\u2264\u202f_t_\u202f\u2264\u202f_T_, one intuitive idea is to use the single-output GP (SOGP) to approximate them individually using the associated training data Dt={Xt,yt}, see Fig.\u00a01(a). Considering that the outputs are correlated in some way, modeling them individually may result in the loss of valuable information. Hence, an increasing diversity of engineering applications are embarking on the use of multi-output GP (MOGP), which is conceptually depicted in Fig.\u00a01(b), for surrogate modeling.\n\nThe study of MOGP has a long history and is known as multivariate Kriging or Co-Kriging \\[16\\], \\[17\\], \\[18\\], \\[19\\] in the geostatistic community; it also overlaps with the broad field of multi-task learning \\[20\\], \\[21\\] and transfer learning \\[22\\], \\[23\\] of the machine learning community. The MOGP handles problems with the basic assumption that the outputs are correlated in some way. Hence, a key issue in MOGP is to _exploit the output correlations such that the outputs can leverage information from one another_ in order to provide more accurate predictions in comparison to modeling them individually.\n\nExisting MOGPs can in general be classified into two categories: (1) _symmetric_ MOGPs and (2) _asymmetric_ MOGPs. Symmetric MOGPs use a _symmetric dependency structure_ to capture the output correlations and approximate the _T_ outputs simultaneously. Therefore, these MOGPs usually have an _integrated_ modeling process, i.e., fusing all the information in an entire covariance matrix, which leads to _bidirectional_ information transfer between the outputs. Typically, the symmetric MOGPs attempt to improve the predictions of all the outputs in symmetric scenarios, where the outputs are of equal importance and have roughly equivalent training information.\n\nOn the contrary, asymmetric MOGPs, which have an _asymmetric dependency structure_ specifically designed for asymmetric scenarios, target to enhance the _primary_ output predictions by transferring useful knowledge from other related _secondary_ outputs.1 The basic assumption is that the primary output has a few training points, but the secondary outputs, also denoted as source domains in transfer learning \\[24\\], \\[25\\], \\[26\\], usually have sufficient training points. Here, we particularly restrict ourselves to a _hierarchical_ asymmetric scenario where the simulator for the physics-based problem of interests has multiple levels of fidelity. Regarding them as different outputs, the version...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123"
    },
    {
      "title": "[PDF] Safe Active Learning for Multi-Output Gaussian Processes",
      "text": "Safe Active Learning for Multi-Output Gaussian Processes\nCen-You Li Barbara Rakitsch Christoph Zimmer\nCen-You.Li@de.bosch.com Barbara.Rakitsch@de.bosch.com\nBosch Center for Artificial Intelligence\nRobert-Bosch-Campus 1, 71272 Renningen, Germany\nChristoph.Zimmer@de.bosch.com\nAbstract\nMulti-output regression problems are com\u0002monly encountered in science and engineer\u0002ing. In particular, multi-output Gaussian\nprocesses have been emerged as a prom\u0002ising tool for modeling these complex sys\u0002tems since they can exploit the inherent cor\u0002relations and provide reliable uncertainty es\u0002timates. In many applications, however, ac\u0002quiring the data is expensive and safety con\u0002cerns might arise (e.g. robotics, engineer\u0002ing). We propose a safe active learning ap\u0002proach for multi-output Gaussian process re\u0002gression. This approach queries the most in\u0002formative data or output taking the related\u0002ness between the regressors and safety con\u0002straints into account. We prove the effect\u0002iveness of our approach by providing theor\u0002etical analysis and by demonstrating empir\u0002ical results on simulated datasets and on a\nreal-world engineering dataset. On all data\u0002sets, our approach shows improved conver\u0002gence compared to its competitors.\n1 Introduction\nActive learning (AL) selects the most informative data\nsequentially according to previous measurements and\nan acquisition function (Krause et al., 2008; Houlsby\net al., 2011; Zhang et al., 2016). The objective is to\noptimize a model without labeling unnecessary data.\nThe problem setup is closely related to Bayesian op\u0002timization, i.e. BO (Brochu et al., 2010), which optim\u0002izes a black-box function with limited exploration. In\nvarious scenarios, safety concerns are also critical dur\u0002ing the exploration phase. For instance, movements\nProceedings of the 25th International Conference on Artifi\u0002cial Intelligence and Statistics (AISTATS) 2022, Valencia,\nSpain. PMLR: Volume 151. Copyright 2022 by the au\u0002thor(s).\nof a machine are not supposed to crash any objects.\nA system should avoid generating high pressure, high\ntemperature, or explosion. Safe learning addresses this\nby incorporating and learning safety constraints (Sui\net al., 2015). Schreiter et al. (2015) and Zimmer et al.\n(2018) combine safety considerations with AL so that\nthe data selection is done only in the determined safe\ndomain.\nThese works, however, rarely considered multi-output\n(MO) regression problems, despite them commonly en\u0002countered in science, engineering and medicine (Xu\net al., 2019; Zhang and Yang, 2021; Liu et al., 2018).\nIn such problems, it is possible to consider individual\ntasks or outputs independently, but the plausibly\nshared mechanisms are ignored, and the performances\nor data efficiency might be deteriorated. Zhang et al.\n(2016) dealt with AL on MO models but focused on\nefficient computation of AL with large datasets and\nsafe exploration was not addressed.\nWe consider safe AL for MO regression models that\nexploit the correlations. In particular, we focus on\nproblems in which different output components may\nnot be synchronously observed (e.g. due to differ\u0002ent measuring cost or difficulty). MO Gaussian pro\u0002cesses (GPs) are natural candidates for these prob\u0002lems (Bonilla et al., 2008; Alvarez and Lawrence, 2011; \u00b4\nAlvarez et al., 2012; van der Wilk et al., 2020), due to \u00b4\ntheir capability of capturing the correlations among\ndifferent outputs and of quantifying the uncertainty.\nIn our work, we consider as main model the Lin\u0002ear Model of Coregionalization (LMC, Journel and\nHuijbregts (1976)), in which each output is modeled\nas a weighted sum of shared latent functions. Each\nlatent function is drawn from a GP. Later on, we ex\u0002tend the theoretical analysis also to the convolution\nprocess (Higdon, 2002; Alvarez and Lawrence, 2011) \u00b4\nin which each latent function is additionally convolved\nby an output-specific smoothing kernel.\nTo the best of our knowledge, this is the first frame\u0002work about safe AL for MOGP regression. Our con-\nSafe Active Learning for Multi-Output Gaussian Processes\ntributions can be summarized as follows:\n\u2022 We formulate an acquisition function for safe act\u0002ive learning in the MOGP framework that allows\nasynchronous measurements.\n\u2022 We provide theoretical analysis of the safe AL al\u0002gorithm in our framework, particularly we derive\na convergence rate to the algorithm.\n\u2022 We demonstrate the performance and superiority\nto state-of-the-art competitors on a real-world en\u0002gineering dataset.\nThe overview of this papers is as follows. In section 2,\nwe briefly review the related works. In section 3, we\nintroduce our algorithm. We discuss the theory behind\nour algorithm in section 4, and validate empirically its\nusefulness in section 5. Finally, section 6 concludes\nour work.\n2 Related Work\nAL has been extensively investigated for classification\ntasks (Hoi et al., 2006; Joshi et al., 2009; Houlsby et al.,\n2011; Hahn et al., 2019; Shi and Yu, 2021), but less lit\u0002erature addresses AL in the regression setting (Krause\net al., 2008; Garnett et al., 2014). The problem setup is\nclosely related to BO (Brochu et al., 2010). While AL\nand BO both consider limited exploration, the goals\nare very different. AL aims to obtain a well performing\nmodel, usually with characteristic of overall precision,\nbut BO only finds an optimum, e.g. a configuration of\nbest performance or lowest cost. In a BO problem, the\nmodel quality for points far away from the optimum is\nnot important and can be really bad. For a more gen\u0002eral problem in this line of research, i.e. optimizing\nunder uncertainty, GPs, which are capable of mak\u0002ing predictions under uncertainty, are often used as\nsurrogate models (Brochu et al., 2010; Srinivas et al.,\n2012).\nIn recent years, the importance of safety considera\u0002tions has led to a novel line of research ranging from\nSafe Bayesian optimization (Sui et al., 2015; Berken\u0002kamp et al., 2016, 2020) to safe AL in a static environ\u0002ment (Schreiter et al., 2015) or dynamic systems (Zi\u0002mmer et al., 2018). None of these contributions, how\u0002ever, consider MO which is able to exploit correlations\namong outputs.\nExploiting MO correlations has been shown successful\nin various applications (Casale et al., 2017; Liu et al.,\n2018; Cheng et al., 2020). State of the art MO mod\u0002els, in particular with GPs (also refer to Alvarez et al. \u00b4\n(2012) and van der Wilk et al. (2020) for an overview\nover MOGPs), include the Linear Model of Coregion\u0002alization (LMC), a simple yet effective model (Jour\u0002nel and Huijbregts, 1976; Bonilla et al., 2008; Teh\net al., 2005), and one of its extensions, the convolution\nprocess, which further captures correlations in mul\u0002tiple outputs that vary in smoothness (Higdon, 2002;\nAlvarez and Lawrence, 2011). \u00b4\nComplexity of GPs and MOGPs scales cubically with\nthe number of observations (Rasmussen and Williams,\n2006). Existing works focus a lot on approximation\nmethods for large datasets with GPs (Titsias, 2009;\nHensman et al., 2013) and with MOGPs (Alvarez and \u00b4\nLawrence, 2011; Nguyen and Bonilla, 2014; van der\nWilk et al., 2020). In contrast to Zhang et al. (2016),\nboth our simulation and our real-world dataset can\nbe modeled with very few data points. We thus\nconsider MOGPs without any sparse approximations,\neven though these methods could be incorporated into\nour approach.\nVery few works have tried to combine MO modeling\nwith BO (Swersky et al., 2013) or AL (Zhang et al.,\n2016). Swersky et al. (2013) focused on transferring\nBO results between tasks, while Zhang et al. (2016) in\u0002vestigated efficient computation of AL for sparse MO\u0002GPs (Zhang et al., 2016). To the best of our know\u0002ledge, none of the literature addressed safe data query\nor safe AL for MOGPs.\n3 Methods\nWe first provide background on GPs and MOGPs,\nand different inference strategies. In a second step,\nwe show how safe active learning can be applied over\nmultiple outputs.\n3.1 GP Regression\nSingle-output A GP is a stochastic process where\nevery finite subset follows a multivariate normal dis\u0002tribut...",
      "url": "https://proceedings.mlr.press/v151/li22d/li22d.pdf"
    },
    {
      "title": "Data-driven discovery of molecular photoswitches with multioutput Gaussian processes",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d2sc04306h)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04135a)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04523k)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)\nThis Open Access Article is licensed under a\n\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D2SC04306H](https://doi.org/10.1039/D2SC04306H)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022, **13**, 13541-13551\n\n# Data-driven discovery of molecular photoswitches with multioutput Gaussian processes [\u2020](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h\\#fn1)\n\nRyan-Rhys\nGriffiths [\u2021](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#fn2)[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-3117-4559)\\*a,\nJake L.\nGreenfield [\u2021](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#fn2)[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-7650-5414)bc,\nAditya R.\nThawani [\u2021](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#fn2)[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0388-9055)b,\nArian R.\nJamasb\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-6727-7579)d,\nHenry B.\nMoss\ne,\nAnthony\nBourached\nf,\nPenelope\nJones\na,\nWilliam\nMcCorkindale\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6052-8833)a,\nAlexander A.\nAldrick\na,\nMatthew J.\nFuchter\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1767-7072)b and Alpha A.\nLee\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-9616-3108)\\*a\n\naThe Cavendish Laboratory, Department of Physics, University of Cambridge, Cambridge CB3 0HE, UK. E-mail: [rrg27@cam.ac.uk](mailto:rrg27@cam.ac.uk); [aal44@cam.ac.uk](mailto:aal44@cam.ac.uk)\n\nbMolecular Sciences Research Hub, Department of Chemistry, Imperial College London, London W12 0BZ, UK\n\ncCenter for Nanosystems Chemistry (CNC), Institut f\u00fcr Organische Chemie, Universit\u00e4t W\u00fcrzburg, W\u00fcrzburg 97074, Germany\n\ndThe Computer Laboratory, University of Cambridge, Cambridge CB3 0FD, UK\n\neSecondmind.ai, Cambridge CB2 1LA, UK\n\nfThe Institute of Neurology, Department of Neurology, University College London, London WC1N 3BG, UK\n\nReceived\n12th August 2022\n, Accepted 16th September 2022\n\nFirst published on 10th November 2022\n\n* * *\n\n## Abstract\n\nPhotoswitchable molecules display two or more isomeric forms that may be accessed using light. Separating the electronic absorption bands of these isomers is key to selectively addressing a specific isomer and achieving high photostationary states whilst overall red-shifting the absorption bands serves to limit material damage due to UV-exposure and increases penetration depth in photopharmacological applications. Engineering these properties into a system through synthetic design however, remains a challenge. Here, we present a data-driven discovery pipeline for molecular photoswitches underpinned by dataset curation and multitask learning with Gaussian processes. In the prediction of electronic transition wavelengths, we demonstrate that a multioutput Gaussian process (MOGP) trained using labels from four photoswitch transition wavelengths yields the strongest predictive performance relative to single-task models as well as operationally outperforming time-dependent density functional theory (TD-DFT) in terms of the wall-clock time for prediction. We validate our proposed approach experimentally by screening a library of commercially available photoswitchable molecules. Through this screen, we identified several motifs that displayed separated electronic absorption bands of their isomers, exhibited red-shifted absorptions, and are suited for information transfer and photopharmacological applications. Our curated dataset, code, as well as all models are made available at https://github.com/Ryan-Rhys/The-Photoswitch-Dataset.\n\n* * *\n\n## 1 Introduction\n\nPhotoswitches [1](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit1) are molecules that can change their structure and properties in response to light as illustrated in [Fig. 1](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#imgfig1). Photoswitches have found increasing use in molecular, [2\u20135](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit2) supramolecular, [6\u20138](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit6) and materials applications. [9\u201313](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit9) On the molecular level, the incorporation of a photoswitchable motif into a drug molecule can provide a means of turning on, or off, its activity using light. [14,15](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit14) Photoswitchable molecules have demonstrated use as the active moiety in light-responsive molecular pumps, serving to drive systems out of equilibrium. [6,16](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit6) Materials designed to transfer information, [17,18](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit17) via light, have also benefited from the incorporation of photoswitchable molecules as the responsive component. In all of these examples, the structure of the photoswitch, [1,19](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit1) and hence its photophysical properties, is a key consideration to efficient light addressability.\n\n|     |     |     |\n| --- | --- | --- |\n| [![image file: d2sc04306h-f1.tif](https://pubs.rsc.org/image/article/2022/SC/d2sc04306h/d2sc04306h-f1.gif)](https://pubs.rsc.org/image/article/2022/SC/d2sc04306h/d2sc04306h-f1_hi-res.gif) |\n|  | **Fig. 1** Photoswitchable molecules undergo reversible structural changes between multiple states upon irradiation with light. |  |\n\nAzobenzene-based photoswitches switch about their N![[double bond, length as m-dash]](https://www.rsc.org/images/entities/char_e001.gif)N bond giving rise to two isomeric forms, cis\u2013trans or E\u2013Z isomers. These photoswitches are commonly employed in applications seeking to exploit the significant change in structure, dipole moment, or conductivity of their isomeric forms. [20,21](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit20) Recently, azoheteroarenes, where one or more of the phenyl rings of azobenzene are replaced by heteroarene rings, have emerged as a promising subclass of the azobenzene photoswitch. [18](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h#cit18) Azoheteroarenes demonstrate an expansive structural\u2013property tunability of their photophysical properties. These properties include the degree of photoswitching induced by a specified wavelength, quantified by the photostationary state (PSS), and the thermal half-life of the metastable photogenerated state.\n\nFactors that can determine an azoarene's usefulness in a particular application include the thermal half-life of the metastable isomer, quantum yields of photoswitching and the steady-state distribution of a given isomer at a particular irradiation wavelength (PSS). The ideal thermal half-life is dependent on the targeted application, for example, information transfer requires photoswitches with sh...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h"
    },
    {
      "title": "Multi-output Gaussian processes in GPflow \u2014 GPflow 2.5.2 documentation",
      "text": "On this page\n\n# Multi-output Gaussian processes in GPflow [\\#](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html\\#Multi-output-Gaussian-processes-in-GPflow)\n\nThis notebook shows how to construct a multi-output GP model using GPflow, together with different interdomain inducing variables which lead to different approximation properties. GPflow provides a framework for specifying multioutput GP priors, and interdomain approximations which is - modular, by providing a consistent interface for the user of the resulting `SVGP` model, - extensible, by allowing new interdomain variables and kernels to be specified while reusing exising code where\npossible, - efficient, by allowing the most efficient custom code path to be specified where desired.\n\nGetting to grips with the maths and code can be a bit daunting, so to accompany the documentation there is an [in-depth review on arXiv](https://arxiv.org/abs/2003.01115), which provides a unified mathematical framework, together with a high-level description of software design choices in GPflow.\n\nThis notebook shows the various design choices that can be made, to show the reader the flexibility of the framework. This is done in the hope that an example is provided that can be easily adapted to the special case that the reader wants to implement.\n\nA reader who just wants to use a multioutput kernel should simply choose the most efficient set of inducing variables.\n\nTo cite this framework, please reference our [arXiv paper](https://arxiv.org/abs/2003.01115).\n\n```\n@article{GPflow2020multioutput,\n  author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, ST and\n            Artemev, Artem and Adam, Vincent and Hensman, James},\n  title = {A Framework for Interdomain and Multioutput {G}aussian Processes},\n  year = {2020},\n  journal = {arXiv:2003.01115},\n  url = {https://arxiv.org/abs/2003.01115}\n}\n\n```\n\n\\\\begin{equation}\n\\\\newcommand{\\\\GP}{\\\\mathcal{GP}}\n\\\\newcommand{\\\\NN}{\\\\mathcal{N}}\n\\\\newcommand{\\\\LL}{\\\\mathcal{L}}\n\\\\newcommand{\\\\RR}{\\\\mathbb{R}}\n\\\\newcommand{\\\\EE}{\\\\mathbb{E}}\n\\\\newcommand{\\\\valpha}{\\\\boldsymbol\\\\alpha}\n\\\\newcommand{\\\\vf}{\\\\mathbf{f}}\n\\\\newcommand{\\\\vF}{\\\\mathbf{F}}\n\\\\newcommand{\\\\vg}{\\\\mathbf{g}}\n\\\\newcommand{\\\\vW}{\\\\mathbf{W}}\n\\\\newcommand{\\\\vI}{\\\\mathbf{I}}\n\\\\newcommand{\\\\vZ}{\\\\mathbf{Z}}\n\\\\newcommand{\\\\vu}{\\\\mathbf{u}}\n\\\\newcommand{\\\\vU}{\\\\mathbf{U}}\n\\\\newcommand{\\\\vX}{\\\\mathbf{X}}\n\\\\newcommand{\\\\vY}{\\\\mathbf{Y}}\n\\\\newcommand{\\\\identity}{\\\\mathbb{I}}\n\\\\end{equation}\n\n## Task [\\#](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html\\#Task)\n\nWe will consider a regression problem for functions \\\\(f: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^P\\\\). We assume that the dataset is of the form \\\\((X, f\\_1), \\\\dots, (X, f\\_P)\\\\), that is, we observe all the outputs for a particular input location (for cases where there are **not** fully observed outputs for each input, see [A simple demonstration of coregionalization](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/coregionalisation.html)).\n\nHere we assume a model of the form: \\\\begin{equation}\nf(x) = W g(x),\n\\\\end{equation} where \\\\(g(x) \\\\in \\\\mathbb{R}^L\\\\), \\\\(f(x) \\\\in \\\\mathbb{R}^P\\\\) and \\\\(W \\\\in \\\\mathbb{R}^{P \\\\times L}\\\\). We assume that the outputs of \\\\(g\\\\) are uncorrelated, and that by _mixing_ them with \\\\(W\\\\) they become correlated. In this notebook, we show how to build this model using Sparse Variational Gaussian Process (SVGP) for \\\\(g\\\\), which scales well with the numbers of data points and outputs.\n\nHere we have two options for \\\\(g\\\\): 1\\. The output dimensions of \\\\(g\\\\) share the same kernel. 1. Each output of \\\\(g\\\\) has a separate kernel.\n\nIn addition, we have two further suboptions for the inducing inputs of \\\\(g\\\\): 1\\. The instances of \\\\(g\\\\) share the same inducing inputs. 1. Each output of \\\\(g\\\\) has its own set of inducing inputs.\n\nThe notation is as follows: - \\\\(X \\\\in \\\\mathbb{R}^{N \\\\times D}\\\\) denotes the input - \\\\(Y \\\\in \\\\RR^{N \\\\times P}\\\\) denotes the output - \\\\(k\\_{1..L}\\\\), \\\\(L\\\\) are kernels on \\\\(\\\\RR^{N \\\\times D}\\\\) \\- \\\\(g\\_{1..L}\\\\), \\\\(L\\\\) are independent \\\\(\\\\GP\\\\)s with \\\\(g\\_l \\\\sim \\\\GP(0,k\\_l)\\\\) \\- \\\\(f\\_{1..P}\\\\), \\\\(P\\\\) are correlated \\\\(\\\\GP\\\\)s with \\\\(\\\\vf = \\\\vW \\\\vg\\\\)\n\n```\n[1]:\n\n```\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gpflow as gpf\nimport tensorflow as tf\n\nfrom gpflow.utilities import print_summary\nfrom gpflow.ci_utils import ci_niter\n\ngpf.config.set_default_float(np.float64)\ngpf.config.set_default_summary_fmt(\"notebook\")\nnp.random.seed(0)\n%matplotlib inline\n\nMAXITER = ci_niter(2000)\n\n```\n\n```\n2022-05-10 10:59:37.552850: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-05-10 10:59:37.552876: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n```\n\n## Generate synthetic data [\\#](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html\\#Generate-synthetic-data)\n\nWe create a utility function to generate synthetic data. We assume that:\n\n```\n[2]:\n\n```\n\n```\nN = 100  # number of points\nD = 1  # number of input dimensions\nM = 15  # number of inducing points\nL = 2  # number of latent GPs\nP = 3  # number of observations = output dimensions\n\n```\n\n```\n[3]:\n\n```\n\n```\ndef generate_data(N=100):\n    X = np.random.rand(N)[:, None] * 10 - 5  # Inputs = N x D\n    G = np.hstack((0.5 * np.sin(3 * X) + X, 3.0 * np.cos(X) - X))  # G = N x L\n    W = np.array([[0.5, -0.3, 1.5], [-0.4, 0.43, 0.0]])  # L x P\n    F = np.matmul(G, W)  # N x P\n    Y = F + np.random.randn(*F.shape) * [0.2, 0.2, 0.2]\n\n    return X, Y\n\n```\n\n```\n[4]:\n\n```\n\n```\nX, Y = data = generate_data(N)\nZinit = np.linspace(-5, 5, M)[:, None]\n\n```\n\nWe create a utility function for plotting:\n\n```\n[5]:\n\n```\n\n```\ndef plot_model(m, lower=-8.0, upper=8.0):\n    pX = np.linspace(lower, upper, 100)[:, None]\n    pY, pYv = m.predict_y(pX)\n    if pY.ndim == 3:\n        pY = pY[:, 0, :]\n    plt.plot(X, Y, \"x\")\n    plt.gca().set_prop_cycle(None)\n    plt.plot(pX, pY)\n    for i in range(pY.shape[1]):\n        top = pY[:, i] + 2.0 * pYv[:, i] ** 0.5\n        bot = pY[:, i] - 2.0 * pYv[:, i] ** 0.5\n        plt.fill_between(pX[:, 0], top, bot, alpha=0.3)\n    plt.xlabel(\"X\")\n    plt.ylabel(\"f\")\n    plt.title(f\"ELBO: {m.elbo(data):.3}\")\n    plt.plot(Z, Z * 0.0, \"o\")\n\n```\n\n## Model the outputs of \\\\(f(x)\\\\) directly [\\#](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html\\#Model-the-outputs-of-f(x)-directly)\n\nThe three following examples show how to model the outputs of the model \\\\(f(x)\\\\) directly. Mathematically, this case is equivalent to having: \\\\begin{equation}\nf(x) = I g(x),\n\\\\end{equation} i.e.\u00a0\\\\(W = I\\\\) and \\\\(P = L\\\\).\n\n### 1\\. Shared independent multi-output kernel (MOK) and shared independent inducing variables [\\#](https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html\\#1.-Shared-independent-multi-output-kernel-(MOK)-and-shared-independent-inducing-variables)\n\nHere the priors on all outputs are constrained to have the same kernel hyperparameters. We also share the inducing inputs between all outputs. The different GPs are independent both in the prior and the approximate posterior.\n\n```\n[6]:\n\n```\n\n```\n# create multi-output kernel\nkernel = gpf.kernels.SharedIndependent(\n    gpf.kernels.SquaredExponential() + gpf.kernels.Linear(), output_dim=P\n)\n# initialization of inducing input locations (M random points from the training inputs)\nZ = Zinit.copy()\n# create multi-output inducing variables from Z\niv = gpf.inducing_variables.SharedIndependentInducingVariables(\n    gpf.inducing_variables.InducingPoints(Z)\n)\n\n```\n\n```\n2022-05-10 10:59:39.628032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: canno...",
      "url": "https://gpflow.github.io/GPflow/2.5.2/notebooks/advanced/multioutput.html"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2212.04450** (physics)\n\n\\[Submitted on 6 Dec 2022 ( [v1](https://arxiv.org/abs/2212.04450v1)), last revised 21 Feb 2023 (this version, v2)\\]\n\n# Title:GAUCHE: A Library for Gaussian Processes in Chemistry\n\nAuthors: [Ryan-Rhys Griffiths](https://arxiv.org/search/physics?searchtype=author&query=Griffiths,+R), [Leo Klarner](https://arxiv.org/search/physics?searchtype=author&query=Klarner,+L), [Henry B. Moss](https://arxiv.org/search/physics?searchtype=author&query=Moss,+H+B), [Aditya Ravuri](https://arxiv.org/search/physics?searchtype=author&query=Ravuri,+A), [Sang Truong](https://arxiv.org/search/physics?searchtype=author&query=Truong,+S), [Samuel Stanton](https://arxiv.org/search/physics?searchtype=author&query=Stanton,+S), [Gary Tom](https://arxiv.org/search/physics?searchtype=author&query=Tom,+G), [Bojana Rankovic](https://arxiv.org/search/physics?searchtype=author&query=Rankovic,+B), [Yuanqi Du](https://arxiv.org/search/physics?searchtype=author&query=Du,+Y), [Arian Jamasb](https://arxiv.org/search/physics?searchtype=author&query=Jamasb,+A), [Aryan Deshwal](https://arxiv.org/search/physics?searchtype=author&query=Deshwal,+A), [Julius Schwartz](https://arxiv.org/search/physics?searchtype=author&query=Schwartz,+J), [Austin Tripp](https://arxiv.org/search/physics?searchtype=author&query=Tripp,+A), [Gregory Kell](https://arxiv.org/search/physics?searchtype=author&query=Kell,+G), [Simon Frieder](https://arxiv.org/search/physics?searchtype=author&query=Frieder,+S), [Anthony Bourached](https://arxiv.org/search/physics?searchtype=author&query=Bourached,+A), [Alex Chan](https://arxiv.org/search/physics?searchtype=author&query=Chan,+A), [Jacob Moss](https://arxiv.org/search/physics?searchtype=author&query=Moss,+J), [Chengzhi Guo](https://arxiv.org/search/physics?searchtype=author&query=Guo,+C), [Johannes Durholt](https://arxiv.org/search/physics?searchtype=author&query=Durholt,+J), [Saudamini Chaurasia](https://arxiv.org/search/physics?searchtype=author&query=Chaurasia,+S), [Felix Strieth-Kalthoff](https://arxiv.org/search/physics?searchtype=author&query=Strieth-Kalthoff,+F), [Alpha A. Lee](https://arxiv.org/search/physics?searchtype=author&query=Lee,+A+A), [Bingqing Cheng](https://arxiv.org/search/physics?searchtype=author&query=Cheng,+B), [Al\u00e1n Aspuru-Guzik](https://arxiv.org/search/physics?searchtype=author&query=Aspuru-Guzik,+A), [Philippe Schwaller](https://arxiv.org/search/physics?searchtype=author&query=Schwaller,+P), [Jian Tang](https://arxiv.org/search/physics?searchtype=author&query=Tang,+J)\n\nView a PDF of the paper titled GAUCHE: A Library for Gaussian Processes in Chemistry, by Ryan-Rhys Griffiths and Leo Klarner and Henry B. Moss and Aditya Ravuri and Sang Truong and Samuel Stanton and Gary Tom and Bojana Rankovic and Yuanqi Du and Arian Jamasb and Aryan Deshwal and Julius Schwartz and Austin Tripp and Gregory Kell and Simon Frieder and Anthony Bourached and Alex Chan and Jacob Moss and Chengzhi Guo and Johannes Durholt and Saudamini Chaurasia and Felix Strieth-Kalthoff and Alpha A. Lee and Bingqing Cheng and Al\\\\'an Aspuru-Guzik and Philippe Schwaller and Jian Tang\n\n[View PDF](https://arxiv.org/pdf/2212.04450)\n\n> Abstract:We introduce GAUCHE, a library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to chemical representations, however, is nontrivial, necessitating kernels defined over structured inputs such as graphs, strings and bit vectors. By defining such kernels in GAUCHE, we seek to open the door to powerful tools for uncertainty quantification and Bayesian optimisation in chemistry. Motivated by scenarios frequently encountered in experimental chemistry, we showcase applications for GAUCHE in molecular discovery and chemical reaction optimisation. The codebase is made available at [this https URL](https://github.com/leojklarner/gauche)\n\n|     |     |\n| --- | --- |\n| Subjects: | Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2212.04450](https://arxiv.org/abs/2212.04450) \\[physics.chem-ph\\] |\n|  | (or [arXiv:2212.04450v2](https://arxiv.org/abs/2212.04450v2) \\[physics.chem-ph\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2212.04450](https://doi.org/10.48550/arXiv.2212.04450)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ryan-Rhys Griffiths \\[ [view email](https://arxiv.org/show-email/892178c4/2212.04450)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2212.04450v1)**\nTue, 6 Dec 2022 08:28:21 UTC (557 KB)\n\n**\\[v2\\]**\nTue, 21 Feb 2023 06:16:06 UTC (2,390 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GAUCHE: A Library for Gaussian Processes in Chemistry, by Ryan-Rhys Griffiths and Leo Klarner and Henry B. Moss and Aditya Ravuri and Sang Truong and Samuel Stanton and Gary Tom and Bojana Rankovic and Yuanqi Du and Arian Jamasb and Aryan Deshwal and Julius Schwartz and Austin Tripp and Gregory Kell and Simon Frieder and Anthony Bourached and Alex Chan and Jacob Moss and Chengzhi Guo and Johannes Durholt and Saudamini Chaurasia and Felix Strieth-Kalthoff and Alpha A. Lee and Bingqing Cheng and Al\\\\'an Aspuru-Guzik and Philippe Schwaller and Jian Tang\n\n- [View PDF](https://arxiv.org/pdf/2212.04450)\n- [TeX Source](https://arxiv.org/src/2212.04450)\n- [Other Formats](https://arxiv.org/format/2212.04450)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2212.04450&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2212.04450&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2022-12](https://arxiv.org/list/physics.chem-ph/2022-12)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2212.04450?context=cond-mat)\n\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2212.04450?context=cond-mat.mtrl-sci)\n\n[cs](https://arxiv.org/abs/2212.04450?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2212.04450?context=cs.LG)\n\n[physics](https://arxiv.org/abs/2212.04450?context=physics)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.04450)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.04450)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.04450)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2212.04450&description=GAUCHE: A Library for Gaussian Processes in Chemistry) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2212.04450&title=GAUCHE: A Library for Gaussian Processes in Chemistry)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [Wh...",
      "url": "https://arxiv.org/abs/2212.04450"
    },
    {
      "title": "[PDF] Multi-Output Robust and Conjugate Gaussian Processes - arXiv",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/pdf/2510.26401"
    },
    {
      "title": "Multi-output prediction of dose\u2013response curves enables drug ...",
      "text": "Multi-output prediction of dose\u2013response curves enables drug repositioning and biomarker discovery - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1038/s41698-024-00691-x)\n* [](pdf/41698_2024_Article_691.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![NPJ Precision Oncology logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-npjprecisoncol.jpg)\nNPJ Precis Oncol\n. 2024 Sep 20;8:209. doi:[10.1038/s41698-024-00691-x](https://doi.org/10.1038/s41698-024-00691-x)\n# Multi-output prediction of dose\u2013response curves enables drug repositioning and biomarker discovery\n[Juan-Jos\u00e9 Giraldo Gutierrez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Gutierrez JJG\"[Author]>)\n### Juan-Jos\u00e9 Giraldo Gutierrez\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\nFind articles by[Juan-Jos\u00e9 Giraldo Gutierrez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Gutierrez JJG\"[Author]>)\n1,2,\u2709,#,[Evelyn Lau](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Lau E\"[Author]>)\n### Evelyn Lau\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Evelyn Lau](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Lau E\"[Author]>)\n3,#,[Subhashini Dharmapalan](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Dharmapalan S\"[Author]>)\n### Subhashini Dharmapalan\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Subhashini Dharmapalan](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Dharmapalan S\"[Author]>)\n2,3,[Melody Parker](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Parker M\"[Author]>)\n### Melody Parker\n4Nuffield Department of Clinical Medicine, University of Oxford, John Radcliffe Hospital, Oxford, UK\n5Big Data Institute at the Li Ka Shing Centre for Health Information and Discovery, University of Oxford, Oxford, UK\nFind articles by[Melody Parker](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Parker M\"[Author]>)\n4,5,[Yurui Chen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Chen Y\"[Author]>)\n### Yurui Chen\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n6Department of Mathematics, National University of Singapore, Singapore, Republic of Singapore\nFind articles by[Yurui Chen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Chen Y\"[Author]>)\n3,6,[Mauricio A \u00c1lvarez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00c1lvarez MA\"[Author]>)\n### Mauricio A \u00c1lvarez\n7Department of Computer Science, The University of Manchester, Manchester, UK\nFind articles by[Mauricio A \u00c1lvarez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00c1lvarez MA\"[Author]>)\n7,[Dennis Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang D\"[Author]>)\n### Dennis Wang\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n8Bioinformatics Institute (BII), Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Dennis Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang D\"[Author]>)\n1,2,3,8,\u2709\n* Author information\n* Article notes\n* Copyright and License information\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n4Nuffield Department of Clinical Medicine, University of Oxford, John Radcliffe Hospital, Oxford, UK\n5Big Data Institute at the Li Ka Shing Centre for Health Information and Discovery, University of Oxford, Oxford, UK\n6Department of Mathematics, National University of Singapore, Singapore, Republic of Singapore\n7Department of Computer Science, The University of Manchester, Manchester, UK\n8Bioinformatics Institute (BII), Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n\u2709Corresponding author.\n#\nContributed equally.\nReceived 2023 Nov 10; Accepted 2024 Aug 28; Collection date 2024.\n\u00a9The Author(s) 2024\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC11415488\u00a0\u00a0PMID:[39304771](https://pubmed.ncbi.nlm.nih.gov/39304771/)\n## Abstract\nDrug response prediction is hampered by uncertainty in the measures of response and selection of doses. In this study, we propose a probabilistic multi-output model to simultaneously predict all dose\u2013responses and uncover their biomarkers. By describing the relationship between genomic features and chemical properties to every response at every dose, our multi-output Gaussian Process (MOGP) models enable assessment of drug efficacy using any dose\u2013response metric. This approach was tested across two drug screening studies and ten cancer types. Kullback-leibler divergence measured the importance of each feature and identified*EZH2*gene as a novel biomarker of BRAF inhibitor response. We demonstrate the effectiveness of our MOGP models in accurately predicting dose\u2013responses in different cancer types and when there is a limited number of drug screening experiments for training. Our findings highlight the potential of MOGP models in enhancing drug develop...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11415488"
    },
    {
      "title": "Multi-output Gaussian processes for the reconstruction of ... - EPJ N",
      "text": "Multi-output Gaussian processes for the reconstruction of homogenized cross-sections | EPJ N\n[![EDP Sciences logo](https://www.epj-n.org/templates/source/images/logos/logo_edp_105.svg)](https://www.edpsciences.org)\n[Submit your paper](https://epjn.nestor-edp.org/)\nEDPS Account\n[![EPJ N](https://www.epj-n.org/templates/template1/images/epjn/logo_epjn.png)](https://www.epj-n.org/)\nSearch\n[Menu](#menu-mobile)\n[Advanced Search](https://www.epj-n.org/component/solr/)\n[All issues](https://www.epj-n.org/component/issues/)![](https://www.epj-n.org/media/system/images/arrow.png)[Volume 11 (2025)](https://www.epj-n.org/articles/epjn/abs/2025/01/contents/contents.html)![](https://www.epj-n.org/media/system/images/arrow.png)[EPJ Nuclear Sci. Technol., 11 (2025) 61](https://www.epj-n.org/articles/epjn/abs/2025/01/epjn20250040/epjn20250040.html)![](https://www.epj-n.org/media/system/images/arrow.png)Full HTML\n[Special Issue on \u2018Overview of recent advances in HPC simulation methods for nuclear applications\u2019, edited by Andrea Zoia, Elie Saikali, Cheikh Diop and Cyrille de Saint Jean](https://www.epj-n.org/component/toc/?task=topic&amp;id=2252)\nOpen Access\n|Issue||\nEPJ Nuclear Sci. Technol.\n**Volume**11, 2025\nSpecial Issue on \u2018Overview of recent advances in HPC simulation methods for nuclear applications\u2019, edited by Andrea Zoia, Elie Saikali, Cheikh Diop and Cyrille de Saint Jean\n||\n|\nArticle Number||61|\nNumber of page(s)||24|\nDOI||[https://doi.org/10.1051/epjn/2025055](https://doi.org/10.1051/epjn/2025055)|\nPublished online||01 October 2025|\n* [Top](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#top_full)\n* [Abstract](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#abs)\n* [1. Introduction](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S1)\n* [2. Description of the model](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S4)\n* [3. Experimental results](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S10)\n* [4. Practical considerations](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S16)\n* [5. Conclusions](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S20)\n* [Glossary](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#glossary)\n* [Acknowledgments](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#ack)\n* [Funding](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S21)\n* [Conflicts of interest](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S22)\n* [Data availability statement](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S23)\n* [Author contribution statement](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#S24)\n* [References](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#ref)\n* [Appendix A](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#app)\n* [List of tables](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#tabs)\n* [List of figures](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#figs)\nEPJ Nuclear Sci. Technol.**11**, 61 (2025)\n[https://doi.org/10.1051/epjn/2025055](https://doi.org/10.1051/epjn/2025055)\nRegular Article\n## Multi-output Gaussian processes for the reconstruction of homogenized cross-sections\nOlivier Truffinet1,2\\*,[a](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html#FN2),Karim Ammar2,Jean-Philippe Argaud1,Nicolas G&#233;rard Castaing2andBertrand Bouriquet3\n1EDF R&amp;D PERICLES, 7 Boulevard Gaspard Monge, 91120 Palaiseau, France\n2Universit&#233; Paris-Saclay, CEA, Service d&#8217;&#201;tudes des R&#233;acteurs et de Math&#233;matiques Appliqu&#233;es, 91190 Gif-sur-Yvette, France\n3EDF DQI, 2 Rue Amp&#232;re, 93206 Saint-Denis Cedex, France\n\\*e-mail:[olivier.truffinet@edf.fr](mailto:olivier.truffinet@edf.fr)\nReceived:1 June 2025\nReceived in final form:6 August 2025\nAccepted:7 August 2025\nPublished online:1 October 2025\nAbstract\nDeterministic nuclear reactor neutronics codes employing the prevalent two-step scheme often generate a substantial amount of intermediate data at the interface of their two subcodes, which can impede the overall performance of the software. The bulk of this data comprises &#8220;few-groups homogenized cross-sections&#8221; or HXS, which are stored as tabulated multivariate functions and interpolated inside the core code. A number of mathematical tools have been studied for this interpolation purpose over the years, but few meet all the challenging requirements of neutronics computation chains: extreme accuracy, low memory footprint, fast predictions. We here present a new framework to tackle this task, based on multi-output Gaussian processes (MOGP). These smooth and tunable bayesian regressors are able to model several quantities at once, and to capture the correlations between them &#8211; a key asset in the modeling of HXS&#8217;s, which we show to be highly similar from one another. Several models of this family are discussed, compared, adapted to the case of very numerous HXS&#8217;s, and their possible modeling choices are experimented on. These machine learning models enable us to interpolate HXS&#8217;s with improved accuracy compared to the current multilinear standard, using only a fraction of its training data &#8211; meaning that the amount of required precomputation is reduced by a factor of several dozens. They also necessitate an even smaller fraction of its storage requirements, preserve its reconstruction speed, and unlock new functionalities such as adaptive sampling and facilitated uncertainty quantification. We demonstrate the efficiency of this approach on a rich test case reproducing the VERA benchmark, proving in particular its scalability to datasets of millions of HXS&#8217;s.\na\nCurrently employed at institution 1, but worked at institution 2 at the time of this work.\n*&#169; O. Truffinet et al., Published by EDP Sciences, 2025*\n[![Licence Creative Commons](https://i.creativecommons.org/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0)This is an Open Access article distributed under the terms of the Creative Commons Attribution License ([https://creativecommons.org/licenses/by/4.0](https://creativecommons.org/licenses/by/4.0)), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n## 1. Introduction\n### 1.1. Problem description\nMost deterministic neutronics codes make use of the so-called &#8220;two-step scheme&#8221;, and thus pass processed nuclear data at the interface of their two subcodes (lattice code and core code). This data consists mainly of**few-groups homogenized cross-sections (denoted HXS in this article)**, which quantify the interaction of neutrons with matter. For a given isotope*i*and nuclear reaction*r*, a spatial region*V*and an energy range [*E**g*&#8197;&#8722;&#8197;1;*E**g*], the corresponding HXS is defined as:\n&#963;i,rg,V(P)=&#8747;Eg&#8722;1&#8804;E&#8804;Egr&#8712;V&#963;i,r(E,T)&#934;(r,E,P)drdE&#8747;Eg&#8722;1&#8804;E&#8804;Egr&#8712;V&#934;(r,E,P)drdE![$$ \\begin{aligned} \\sigma _{i,r}^{g,V}(\\mathbf P ) = \\dfrac{\\displaystyle \\int _{\\begin{matrix} E_{g-1} \\le E \\le E_{g}\\\\ \\mathbf r \\in V \\end{matrix}} \\sigma _{i,r}(E, T) \\, \\mathrm \\Phi (\\mathbf r , E, \\mathbf P ) \\, d \\mathbf r dE}{\\displaystyle \\int _{\\begin{matrix} E_{g-1} \\le E \\le E_{g}\\\\ \\mathbf r \\in V \\end{matrix}} \\mathrm \\Phi (\\mathbf r , E, \\mathbf P ) \\, d \\mathbf r dE} \\end{aligned} $$](https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040-eq1.g...",
      "url": "https://www.epj-n.org/articles/epjn/full_html/2025/01/epjn20250040/epjn20250040.html"
    }
  ]
}