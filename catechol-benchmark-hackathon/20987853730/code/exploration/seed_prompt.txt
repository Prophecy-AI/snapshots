# Chemical Reaction Yield Prediction - Seed Prompt

## Problem Overview
This is a multi-output regression problem predicting 3 chemical reaction yields (SM, Product 2, Product 3) based on solvent conditions, temperature, and residence time. The challenge is to generalize to unseen solvents/solvent mixtures.

**Target Score:** 0.017270 (lower is better - this is very aggressive, current best public ~0.098)

**Dataset Origin:** This is the "Catechol Benchmark" dataset (arXiv 2506.07619), specifically designed for few-shot ML benchmarking in chemistry. The dataset provides the first-ever transient flow dataset for ML benchmarking.

## Data Understanding
**Reference notebooks:** `exploration/eda.ipynb`

Key characteristics:
- Full data: 1227 samples with solvent mixtures (A + B at various percentages)
- Single solvent data: 656 samples with 24 unique solvents
- Targets: SM, Product 2, Product 3 (yields in 0-1 range, some exceed 1.0 due to measurement noise)
- Features: Residence Time (2-15 min), Temperature (175-225°C), Solvent descriptors

## Cross-Validation Strategy (CRITICAL)
The competition uses a specific leave-one-out CV:
- **Task 0 (Single Solvent):** Leave-one-solvent-out (24 folds) - must generalize to completely unseen solvents
- **Task 1 (Full Data):** Leave-one-ramp-out - each ramp is a unique (SOLVENT A, SOLVENT B) pair

This means the model must learn to generalize to NEW solvents it has never seen during training.

## Solvent Featurization Options
Available lookup tables (indexed by solvent name):
1. **Spange descriptors** (13 features): dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, n, f(n), delta - MOST COMMONLY USED
2. **ACS PCA descriptors** (5 features): PCA of green chemistry properties
3. **DRFP** (2048 features): Differential reaction fingerprints
4. **Fragprints** (2133 features): Concatenation of fragments and fingerprints

For mixed solvents: Use weighted average of features: `A * (1-pct) + B * pct` where pct is SolventB%

## Physics-Informed Feature Engineering (KEY INSIGHT)

### Arrhenius Kinetics Features
Chemical reactions follow Arrhenius kinetics. Transform features accordingly:
```
temp_k = Temperature + 273.15  # Convert to Kelvin
inv_temp = 1000.0 / temp_k     # Inverse temperature (Arrhenius)
log_time = log(Residence Time) # Log of time
interaction = inv_temp * log_time  # Interaction term
```

### Additional Numeric Features
- rt^2, temp^2 (polynomial)
- log1p(rt), log1p(temp)
- rt * temp (interaction)
- Reaction_Energy = Temperature * Residence Time
- B_Conc_Temp = SolventB% * Temperature

## Chemical Symmetry (TTA for Mixed Solvents)
**Critical insight:** A mixture of "Solvent A + Solvent B" is physically identical to "Solvent B + Solvent A".

**Test Time Augmentation (TTA):**
1. Predict with input as (A, B, pct)
2. Predict with input as (B, A, 1-pct) [flipped]
3. Average the two predictions

This can also be used during training as data augmentation.

## Model Architectures

### MLP (Primary Approach - Best Public Score ~0.098)
Architecture that works well:
- BatchNorm1d(input_dim)
- Linear(input_dim, 128) → BatchNorm → ReLU → Dropout(0.2)
- Linear(128, 128) → BatchNorm → ReLU → Dropout(0.2)
- Linear(128, 64) → BatchNorm → ReLU → Dropout(0.2)
- Linear(64, 3) → Sigmoid (or no activation with clipping)

Training:
- Loss: HuberLoss (robust to outliers) or SmoothL1Loss
- Optimizer: Adam/AdamW with lr=5e-4, weight_decay=1e-5
- Scheduler: ReduceLROnPlateau or CosineAnnealingLR
- Epochs: 250-300
- Batch size: 32
- Gradient clipping: 1.0

### XGBoost Alternative
- MultiOutputRegressor with XGBRegressor
- n_estimators=1500, learning_rate=0.015, max_depth=6
- subsample=0.8, colsample_bytree=0.9

### Multi-Output Gaussian Processes (MOGP) - PROMISING FOR SMALL DATA
MOGPs exploit inter-property correlations between the 3 outputs (SM, Product 2, Product 3) to improve predictions.
- Use Linear Model of Coregionalization (LMC) 
- GAUCHE library provides chemistry-specific kernels
- Can provide uncertainty estimates for active learning
- GPflow's multi-output module for efficient implementation
- Particularly effective when outputs are correlated (yields should sum to ~1)

## Ensembling Strategies

### Bagging (Seed Ensemble)
Train 5-7 models with different random seeds, average predictions.

### Model Diversity
Combine different approaches:
- MLP with Spange descriptors
- MLP with DRFP features
- XGBoost with engineered features
- Gaussian Process (if computationally feasible)

## Post-Processing
1. **Clip predictions** to [0, 1] range
2. **Optional normalization:** Ensure SM + Product2 + Product3 ≈ 1 (chemical constraint)
   - Divide each row by its sum

## Advanced Strategies to Beat Target Score

### 1. Better Solvent Representations
- Combine multiple descriptor types (Spange + ACS + DRFP)
- Learn solvent embeddings during training
- Use molecular fingerprints directly from SMILES
- Concatenate rather than just average for mixed solvents
- GNN architectures encoding solvent as node/edge attributes

### 2. Meta-Learning for Few-Shot Generalization
Since we need to generalize to unseen solvents:
- **MetaRF:** Attention-based random forest with meta-learning (from Journal of Cheminformatics 2023)
  - Attention weights optimized to weight training examples by relevance to target
  - Specifically designed for few-shot yield prediction
- **MAML:** Model-Agnostic Meta-Learning for quick adaptation
- **Prototypical Networks:** Learn solvent prototypes

### 3. Reaction Transformers
- Fine-tune pretrained reaction transformers on this dataset
- Data augmentation of reaction SMILES
- Test-time augmentation provides uncertainty estimates
- IBM Research showed this works well in low-data regime

### 4. Hybrid Physics-ML Models
- Combine DFT-computed descriptors with ML
- Gaussian Process Regression with physics-informed kernels
- Achieved MAE of 0.77 kcal/mol in activation energy prediction (Chem Sci 2021)

### 5. Multi-Task Learning
- Jointly predict all 3 outputs with shared representations
- Use correlation between outputs (they should sum to ~1)
- Softmax output layer to enforce constraint

### 6. Transfer Learning
- Pre-train on larger reaction datasets (USPTO, ORD)
- Fine-tune on this specific dataset
- The benchmark paper specifically recommends transfer-learning approaches

### 7. Active Learning Simulation
- Use uncertainty estimates to identify hard-to-predict solvents
- Weight training samples by similarity to test solvents

## Implementation Notes

### Required Submission Format
The notebook must follow the template structure with specific last 3 cells unchanged (except model definition line).

### Key Files
- `/home/data/catechol_full_data_yields.csv` - Full data with mixtures
- `/home/data/catechol_single_solvent_yields.csv` - Single solvent data
- `/home/data/spange_descriptors_lookup.csv` - Spange features (13 dims)
- `/home/data/acs_pca_descriptors_lookup.csv` - ACS PCA features (5 dims)
- `/home/data/drfps_catechol_lookup.csv` - DRFP features (2048 dims)
- `/home/data/fragprints_lookup.csv` - Fragprints (2133 dims)
- `/home/data/utils.py` - CV split generators

### Evaluation
Custom metric (catechol_hackathon_metric) - likely MSE or MAE based on CV predictions.

## Priority Approaches (Ordered by Expected Impact)

1. **Physics-informed features (Arrhenius)** + **TTA for symmetry** - Already proven effective (~0.098)
2. **Multi-output GP** - Exploits output correlations, good for small data
3. **Ensemble of diverse models** - MLP + XGBoost + GP
4. **Meta-learning (MetaRF)** - Specifically designed for this problem type
5. **Feature combination** - Concatenate Spange + ACS + reduced DRFP
6. **Softmax output constraint** - Enforce yields sum to 1
7. **Transfer learning** - Pre-train on larger chemistry datasets

## Key Insights from Top Kernels

From `../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/`:
- Arrhenius kinetics features (1/T, ln(time)) significantly improve predictions
- TTA for chemical symmetry (A+B = B+A) reduces variance
- Bagging 7 models with HuberLoss achieves ~0.098 score
- Spange descriptors work best for linear mixing

From `../research/kernels/paritoshtripathi5_alchemy-baseline/`:
- StandardScaler normalization important
- Polynomial features (rt^2, temp^2) help
- CosineAnnealingLR scheduler
- Ensemble of 3 seeds per fold
