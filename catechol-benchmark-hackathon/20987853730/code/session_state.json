{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T08:58:26.580142",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius Kinetics approach: Use 1/Temperature (in Kelvin) and ln(Time) as features instead of raw values. Also add interaction term (inv_temp * log_time). This is physics-informed feature engineering.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Test Time Augmentation (TTA) for chemical symmetry: For mixed solvents, predict twice - once with (A, B) and once with (B, A) flipped, then average. This exploits that A+B mixture is physically identical to B+A mixture.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble of 7 MLP models with HuberLoss (robust to outliers), BatchNorm, Dropout(0.2), and ReduceLROnPlateau scheduler. Architecture: 128-128-64-3 with Sigmoid output.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors provide the best base for linear mixing of solvent features. Mixed solvent features = A * (1-pct) + B * pct where pct is SolventB%.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature engineering for numeric features: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Also use StandardScaler for normalization.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble of 3 models with different seeds (42, 77, 2025) per fold. Use CosineAnnealingLR scheduler, SmoothL1Loss (Huber), AdamW optimizer with weight decay 1e-4.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Post-processing: Clip predictions to [0,1] range. Some approaches also normalize rows so sum equals 1 (chemical constraint that yields should sum to 100%).",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "XGBoost approach: MultiOutputRegressor with XGBRegressor, n_estimators=1500, learning_rate=0.015, max_depth=6. Feature engineering: Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV procedure: Task 0 (single solvent) uses leave-one-solvent-out (24 folds). Task 1 (full data) uses leave-one-ramp-out where each ramp is a unique (SOLVENT A, SOLVENT B) pair.",
      "source": "/home/data/utils.py",
      "agent": "explorer"
    },
    {
      "finding": "Target labels are Product 2, Product 3, SM (starting material). These are yields in % / 100 (0-1 range). Some values exceed 1.0 (max SM=1.08) due to measurement noise.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available solvent descriptors: Spange (13 features), ACS PCA (5 features), DRFP (2048 features), Fragprints (2133 features). Spange descriptors are most commonly used in top solutions.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "MetaRF: attention-based random forest with meta-learning for few-shot yield prediction. Attention weights are optimized to weight training examples by relevance to target reaction.",
      "source": "WebSearch: MetaRF paper (Journal of Cheminformatics 2023)",
      "agent": "explorer"
    },
    {
      "finding": "Reaction transformers fine-tuned on small experimental sets with data augmentation of reaction SMILES and test-time augmentation provide higher accuracy and calibrated uncertainty estimates.",
      "source": "WebSearch: IBM Research 2021",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid models combining DFT transition state modeling with machine learning (Gaussian Process Regression) achieve MAE of 0.77 kcal/mol for activation energy prediction.",
      "source": "WebSearch: Chem Sci 2021 - Machine learning meets mechanistic modelling",
      "agent": "explorer"
    },
    {
      "finding": "Multi-output Gaussian Processes (MOGPs) exploit inter-property correlations to improve predictions on small datasets. GAUCHE library provides efficient implementations for chemistry applications with molecular kernels.",
      "source": "WebSearch: GAUCHE library (arXiv 2212.04450)",
      "agent": "explorer"
    },
    {
      "finding": "The Catechol Benchmark paper (arXiv 2506.07619) is the original dataset paper. It recommends: transfer-learning approaches, feature engineering with solvent fingerprints, active learning, and Gaussian processes. The dataset is specifically designed for few-shot ML benchmarking.",
      "source": "WebSearch: The Catechol Benchmark paper (arXiv)",
      "agent": "explorer"
    },
    {
      "finding": "For generalizing to unseen solvents: 1) Use rich solvent descriptors (SMILES, fingerprints, learned embeddings), 2) Active learning with uncertainty-driven sampling, 3) Transfer/few-shot fine-tuning of pretrained models, 4) GNN architectures encoding solvent as node/edge attributes.",
      "source": "WebSearch: Solvent generalization techniques",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with limited data and need to generalize to unseen molecules?",
      "finding": "Predicting reaction yields when only a handful of experiments are available requires models that can learn strong priors from large\u2011scale data and then adapt quickly to new chemistry.  Recent work frames the problem as a **low\u2011data or few\u2011shot learning task**, leveraging transfer\u2011learning, meta\u2011learning, and active\u2011learning strategies rather than plain supervised regression on a fixed fingerprint set\u202f([ACS\u202fJCIM](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.3c00577)).\n\nA particularly effective class of models are **attention\u2011based meta\u2011learners** such as MetaRF, which combine a random\u2011forest backbone with an attention mechanism that weights training examples according to their relevance to the target reaction, enabling accurate yield predictions from only a few trials\u202f([Journal\u202fof\u202fCheminformatics](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-023-00715-x.pdf)).  Complementary approaches fine\u2011tune **reaction\u2011transformer** architectures (originally trained on millions of ",
      "agent": "explorer"
    },
    {
      "query": "How do Gaussian Processes perform for multi-output regression in chemistry with small datasets?",
      "finding": "Multi\u2011output Gaussian processes (MOGPs) extend the standard GP framework by modelling several correlated target properties jointly, which lets the model \u201cborrow strength\u2019\u2019 from one output to improve predictions of the others.  In chemistry this is especially valuable because experimental or high\u2011level computational data are often scarce; the Bayesian nature of GPs provides calibrated uncertainty estimates that guide further measurements.  A recent review of MOGP methods highlights that knowledge transfer across related outputs can dramatically reduce the amount of data needed for accurate regression\u202f([sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123)).  \n\nConcrete chemistry examples confirm this advantage.  Griffiths\u202fet\u202fal. used a multi\u2011output GP to predict the absorption spectra of molecular photoswitches, achieving high\u2011quality predictions and reliable error bars with only a few hundred measured compounds\u202f([pubs.rsc.org](https://pubs.rsc.org/en/c",
      "agent": "explorer"
    },
    {
      "query": "What techniques work best for generalizing machine learning models to unseen chemical solvents in reaction prediction?",
      "finding": "Generalizing reaction\u2011prediction models to solvents that were not present in the training set requires a combination of data\u2011efficient sampling, rich solvent representations, and out\u2011of\u2011distribution (OOD) learning strategies.  Recent work shows that **active\u2011learning loops coupled with descriptor\u2011based selectors** can build compact yet chemically diverse training sets that span the relevant solvent\u2011reaction space, dramatically reducing the amount of data needed while still capturing the key solute\u2011solvent interactions that govern rates and selectivity\u3010nature.com\u3011.  In parallel, generating a large, systematic solvent database (e.g.,\u202f\u2248\u202f30\u202f000 reactions across\u202f\u2248\u202f300 solvents using COSMO\u2011RS calculations) and training models on **atom\u2011mapped reaction SMILES together with explicit solvent SMILES** yields low mean\u2011absolute errors on unseen reactions and reliable relative rate predictions, demonstrating that a simple string\u2011based encoding can be highly transferable when the underlying data are",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 0
}