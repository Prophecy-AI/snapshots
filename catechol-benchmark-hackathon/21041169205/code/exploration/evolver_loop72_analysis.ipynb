{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5e628d",
   "metadata": {},
   "source": [
    "# Loop 72 Analysis: CV-LB Gap and Strategy Assessment\n",
    "\n",
    "## Key Questions:\n",
    "1. What is the current CV-LB relationship?\n",
    "2. Is the extrapolation detection approach viable?\n",
    "3. What strategies haven't been tried yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission history with confirmed LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.011081, 'lb': 0.09816},\n",
    "    {'exp': 'exp_001', 'cv': 0.012297, 'lb': 0.10649},\n",
    "    {'exp': 'exp_003', 'cv': 0.010501, 'lb': 0.09719},\n",
    "    {'exp': 'exp_005', 'cv': 0.01043, 'lb': 0.09691},\n",
    "    {'exp': 'exp_006', 'cv': 0.009749, 'lb': 0.09457},\n",
    "    {'exp': 'exp_007', 'cv': 0.009262, 'lb': 0.09316},\n",
    "    {'exp': 'exp_009', 'cv': 0.009192, 'lb': 0.09364},\n",
    "    {'exp': 'exp_012', 'cv': 0.009004, 'lb': 0.09134},\n",
    "    {'exp': 'exp_024', 'cv': 0.008689, 'lb': 0.08929},\n",
    "    {'exp': 'exp_026', 'cv': 0.008465, 'lb': 0.08875},\n",
    "    {'exp': 'exp_030', 'cv': 0.008298, 'lb': 0.08772},\n",
    "    {'exp': 'exp_035', 'cv': 0.009825, 'lb': 0.09696},\n",
    "    {'exp': 'exp_067', 'cv': 0.008303, 'lb': 0.08774},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Confirmed submissions with LB scores:')\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression: LB = slope * CV + intercept\n",
    "X = df['cv'].values.reshape(-1, 1)\n",
    "y = df['lb'].values\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "r2 = reg.score(X, y)\n",
    "\n",
    "print(f'\\nCV-LB Relationship:')\n",
    "print(f'  LB = {slope:.4f} * CV + {intercept:.6f}')\n",
    "print(f'  R² = {r2:.4f}')\n",
    "print(f'\\nInterpretation:')\n",
    "print(f'  Intercept = {intercept:.6f} (structural gap even at CV=0)')\n",
    "print(f'  Target LB = 0.0347')\n",
    "print(f'  Required CV = (0.0347 - {intercept:.6f}) / {slope:.4f} = {(0.0347 - intercept) / slope:.6f}')\n",
    "print(f'\\nCRITICAL: Intercept ({intercept:.4f}) > Target ({0.0347:.4f})')\n",
    "print(f'  This means standard CV optimization CANNOT reach the target!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['cv'], df['lb'], s=100, alpha=0.7, label='Submissions')\n",
    "\n",
    "# Plot regression line\n",
    "cv_range = np.linspace(0, 0.015, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'r--', label=f'LB = {slope:.2f}*CV + {intercept:.4f} (R²={r2:.3f})')\n",
    "\n",
    "# Mark target\n",
    "plt.axhline(y=0.0347, color='g', linestyle=':', linewidth=2, label='Target LB = 0.0347')\n",
    "\n",
    "# Mark best submission\n",
    "best_idx = df['lb'].idxmin()\n",
    "plt.scatter(df.loc[best_idx, 'cv'], df.loc[best_idx, 'lb'], s=200, c='red', marker='*', \n",
    "            label=f'Best: {df.loc[best_idx, \"exp\"]} (LB={df.loc[best_idx, \"lb\"]:.4f})')\n",
    "\n",
    "plt.xlabel('CV Score (MSE)')\n",
    "plt.ylabel('LB Score (MSE)')\n",
    "plt.title('CV-LB Relationship Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=150)\n",
    "plt.show()\n",
    "print('\\nPlot saved to /home/code/exploration/cv_lb_relationship.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca25c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gap between best CV and best LB\n",
    "print('=== Gap Analysis ===')\n",
    "print(f'Best CV: exp_049 with CV=0.008092')\n",
    "print(f'Best LB: exp_030 with LB=0.08772 (CV=0.008298)')\n",
    "print(f'\\nGap ratio: LB/CV = {0.08772/0.008298:.2f}x')\n",
    "print(f'\\nTo reach target LB=0.0347:')\n",
    "print(f'  If gap ratio stays ~10.6x: Need CV = 0.0347/10.6 = {0.0347/10.6:.6f}')\n",
    "print(f'  Current best CV: 0.008092')\n",
    "print(f'  Gap: {0.008092 - 0.0347/10.6:.6f}')\n",
    "print(f'\\nAlternatively, to reduce the intercept:')\n",
    "print(f'  Current intercept: {intercept:.6f}')\n",
    "print(f'  Target intercept: < 0.0347 (to make target reachable)')\n",
    "print(f'  Need to reduce intercept by: {intercept - 0.03:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dac78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "print('=== Approaches Tried ===')\n",
    "approaches = [\n",
    "    ('MLP (various architectures)', 'exp_000-008', 'CV 0.009-0.011'),\n",
    "    ('LightGBM', 'exp_001', 'CV 0.012'),\n",
    "    ('DRFP features', 'exp_002-003', 'CV 0.010-0.017'),\n",
    "    ('Large ensembles (15 models)', 'exp_005', 'CV 0.010'),\n",
    "    ('Ridge/Kernel Ridge', 'exp_033-034', 'CV 0.008-0.009'),\n",
    "    ('GP + MLP + LGBM ensemble', 'exp_030', 'CV 0.008, LB 0.0877 (BEST LB)'),\n",
    "    ('CatBoost + XGBoost', 'exp_049-053', 'CV 0.008 (pending LB)'),\n",
    "    ('Extrapolation detection', 'exp_068', 'CV 0.057 (FAILED - too aggressive)'),\n",
    "]\n",
    "\n",
    "for approach, exp, result in approaches:\n",
    "    print(f'  {approach}: {result}')\n",
    "\n",
    "print('\\n=== Approaches NOT Tried ===')\n",
    "untried = [\n",
    "    'GNN (Graph Neural Network) - benchmark achieved 0.0039 MSE',\n",
    "    'Pseudo-labeling with confident test predictions',\n",
    "    'Adversarial validation to detect train/test differences',\n",
    "    'Solvent clustering with class-specific models',\n",
    "    'Uncertainty-weighted predictions with GP variance',\n",
    "    'Fixed extrapolation detection (compare to ALL solvents, not just fold)',\n",
    "]\n",
    "for approach in untried:\n",
    "    print(f'  - {approach}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca55d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the evaluator's feedback on exp_068\n",
    "print('=== Evaluator Feedback on exp_068 (Extrapolation Detection) ===')\n",
    "print('''\n",
    "CRITICAL FLAW: The extrapolation detection logic is fundamentally flawed.\n",
    "\n",
    "In leave-one-solvent-out CV, the test solvent is ALWAYS far from the training \n",
    "distribution BY DESIGN. The code computes distance to nearest training solvent,\n",
    "but since the test solvent is always held out, its distance is always maximum.\n",
    "\n",
    "Result: blend_weights = 1.0 for ALL samples, so all predictions become the mean.\n",
    "\n",
    "FIX: Compare to ALL solvents (all 24), not just the training fold solvents.\n",
    "Or use GP uncertainty instead of distance-based detection.\n",
    "''')\n",
    "\n",
    "print('\\n=== Corrected Approach ===')\n",
    "print('''\n",
    "1. Fit NearestNeighbors on ALL 24 solvents (not just training fold)\n",
    "2. Compute distance from test solvent to nearest 3 solvents\n",
    "3. Normalize by mean inter-solvent distance\n",
    "4. Only blend for TRUE outliers (distance > mean + threshold * std)\n",
    "5. Use the best base model (GP+MLP+LGBM), not a simple MLP\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The target IS reachable\n",
    "print('=== KEY INSIGHT: THE TARGET IS REACHABLE ===')\n",
    "print('''\n",
    "The benchmark paper achieved MSE 0.0039 using GNN with DRFP features.\n",
    "This is 22x better than our best LB (0.0877).\n",
    "\n",
    "The gap is NOT due to:\n",
    "- Model architecture (we've tried MLP, LGBM, XGB, CatBoost, GP)\n",
    "- Feature engineering (we've tried Spange, DRFP, ACS, combined)\n",
    "- Ensemble methods (we've tried bagging, stacking, weighted averaging)\n",
    "\n",
    "The gap IS due to:\n",
    "- Distribution shift between train and test solvents\n",
    "- The test solvents may have more extreme properties\n",
    "- Standard ML approaches optimize CV but can't fix extrapolation error\n",
    "\n",
    "SOLUTION: We need to CHANGE THE CV-LB RELATIONSHIP, not just improve CV.\n",
    "\n",
    "Options:\n",
    "1. Fix the extrapolation detection approach (compare to ALL solvents)\n",
    "2. Use GP uncertainty to make conservative predictions\n",
    "3. Implement GNN (but complex and may not work with template)\n",
    "4. Study top public kernels more carefully\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print('=== FINAL RECOMMENDATION ===')\n",
    "print('''\n",
    "PRIORITY 1: Fix the extrapolation detection approach\n",
    "- The concept is correct (make conservative predictions when extrapolating)\n",
    "- The implementation was flawed (compared to fold solvents, not all solvents)\n",
    "- Fix: Compare test solvent to ALL 24 solvents, not just training fold\n",
    "\n",
    "PRIORITY 2: Use GP uncertainty for conservative predictions\n",
    "- GP already provides uncertainty estimates\n",
    "- High uncertainty → blend toward population mean\n",
    "- This is more principled than distance-based detection\n",
    "\n",
    "PRIORITY 3: Submit best CV model (exp_049) to verify pipeline\n",
    "- We have 4 submissions left\n",
    "- exp_049 has CV=0.008092 (best CV)\n",
    "- Need to verify if it improves LB over exp_030 (LB=0.0877)\n",
    "\n",
    "PRIORITY 4: Study top public kernels\n",
    "- The ens-model kernel uses CatBoost + XGBoost\n",
    "- The mixall kernel uses MLP+XGB+RF+LGBM ensemble\n",
    "- These may have different CV-LB relationships\n",
    "''')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
