{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d22df9",
   "metadata": {},
   "source": [
    "# Loop 69 Analysis: Submission Error Investigation\n",
    "\n",
    "The submission failed with \"Evaluation metric raised an unexpected error\". Let me investigate the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36069385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Check submission history\n",
    "submissions = state.get('submissions', [])\n",
    "print(f'Total submissions: {len(submissions)}')\n",
    "print('\\nSubmission history:')\n",
    "for s in submissions:\n",
    "    print(f\"  {s.get('experiment_id', 'N/A')}: CV={s.get('cv_score', 'N/A')}, LB={s.get('lb_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship from successful submissions\n",
    "successful = [(0.0111, 0.0982), (0.0123, 0.1065), (0.0105, 0.0972), (0.0104, 0.0969),\n",
    "              (0.0097, 0.0946), (0.0093, 0.0932), (0.0092, 0.0936), (0.0090, 0.0913),\n",
    "              (0.0087, 0.0893), (0.0085, 0.0887), (0.0083, 0.0877), (0.0098, 0.0970)]\n",
    "\n",
    "cv_scores = [x[0] for x in successful]\n",
    "lb_scores = [x[1] for x in successful]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array(cv_scores).reshape(-1, 1)\n",
    "y = np.array(lb_scores)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "print(f'CV-LB Linear Fit: LB = {reg.coef_[0]:.2f} * CV + {reg.intercept_:.4f}')\n",
    "print(f'R-squared = {reg.score(X, y):.4f}')\n",
    "print(f'\\nIntercept: {reg.intercept_:.4f}')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'\\nTo reach target LB 0.0347:')\n",
    "required_cv = (0.0347 - reg.intercept_) / reg.coef_[0]\n",
    "print(f'Required CV = (0.0347 - {reg.intercept_:.4f}) / {reg.coef_[0]:.2f} = {required_cv:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f521a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the submission file format\n",
    "sub = pd.read_csv('/home/submission/submission.csv')\n",
    "print('Submission shape:', sub.shape)\n",
    "print('Columns:', sub.columns.tolist())\n",
    "print('\\nTask 0 (single solvent):')\n",
    "print(f'  Folds: {sorted(sub[sub[\"task\"]==0][\"fold\"].unique())}')\n",
    "print(f'  Total rows: {len(sub[sub[\"task\"]==0])}')\n",
    "print('\\nTask 1 (full data):')\n",
    "print(f'  Folds: {sorted(sub[sub[\"task\"]==1][\"fold\"].unique())}')\n",
    "print(f'  Total rows: {len(sub[sub[\"task\"]==1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any issues with the predictions\n",
    "print('Target value ranges:')\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f'  {col}: [{sub[col].min():.6f}, {sub[col].max():.6f}]')\n",
    "\n",
    "print('\\nAny NaN:', sub.isna().sum().sum())\n",
    "print('Any Inf:', (sub.select_dtypes(include='number').abs() == float('inf')).sum().sum())\n",
    "print('Any negative:', (sub[['target_1', 'target_2', 'target_3']] < 0).sum().sum())\n",
    "print('Any > 1:', (sub[['target_1', 'target_2', 'target_3']] > 1).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e53d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The submission format looks correct. The error might be from Kaggle's evaluation system.\n",
    "# Let me check if there's a pattern in the failed submissions.\n",
    "\n",
    "# Failed submissions: exp_049, exp_050, exp_052, exp_053, exp_054, exp_055, exp_057, exp_063, exp_064, exp_065\n",
    "# All of these had \"Evaluation metric raised an unexpected error\"\n",
    "\n",
    "# Successful submissions: exp_000 through exp_035 (except some)\n",
    "# The last successful submission was exp_035 with LB 0.0970\n",
    "\n",
    "# What changed between exp_035 and exp_049?\n",
    "# - exp_035 was a simple MLP with Spange features\n",
    "# - exp_049+ were more complex ensembles\n",
    "\n",
    "print('Analysis: The submission format is correct.')\n",
    "print('The error is likely from Kaggle\\'s evaluation system, not our submission format.')\n",
    "print('\\nPossible causes:')\n",
    "print('1. Kaggle evaluation system issue (temporary)')\n",
    "print('2. Notebook structure issue (extra cells after final cell)')\n",
    "print('3. Model definition issue (complex ensemble not compatible)')\n",
    "print('\\nRecommendation: Try a simpler model that matches the template exactly.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
