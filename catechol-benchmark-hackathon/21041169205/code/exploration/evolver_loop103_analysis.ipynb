{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4beca3c",
   "metadata": {},
   "source": [
    "# Loop 103 Analysis: CV-LB Relationship and Strategy Assessment\n",
    "\n",
    "**Goal**: Analyze the CV-LB relationship to understand the structural gap and identify viable paths forward.\n",
    "\n",
    "**Key Questions**:\n",
    "1. What is the CV-LB relationship? Is the intercept too high?\n",
    "2. What approaches have been tried and what's the pattern?\n",
    "3. What fundamentally different approaches haven't been tried?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "    {'exp': 'exp_067', 'cv': 0.0083, 'lb': 0.0877},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df.to_string())\n",
    "print(f'\\nBest CV: {df[\"cv\"].min():.4f} ({df.loc[df[\"cv\"].idxmin(), \"exp\"]})')\n",
    "print(f'Best LB: {df[\"lb\"].min():.4f} ({df.loc[df[\"lb\"].idxmin(), \"exp\"]})')\n",
    "print(f'Target LB: 0.0347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression: LB = slope * CV + intercept\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "\n",
    "print('=== CV-LB LINEAR RELATIONSHIP ===')\n",
    "print(f'LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'R-squared: {r_value**2:.4f}')\n",
    "print(f'Standard error: {std_err:.4f}')\n",
    "print()\n",
    "print('=== CRITICAL ANALYSIS ===')\n",
    "print(f'Intercept: {intercept:.4f}')\n",
    "print(f'Target LB: 0.0347')\n",
    "print(f'Gap (intercept - target): {intercept - 0.0347:.4f}')\n",
    "print()\n",
    "\n",
    "# What CV would be needed to hit target?\n",
    "required_cv = (0.0347 - intercept) / slope\n",
    "print(f'Required CV to hit target: {required_cv:.6f}')\n",
    "if required_cv < 0:\n",
    "    print('>>> IMPOSSIBLE: Required CV is NEGATIVE!')\n",
    "    print('>>> This means the intercept alone exceeds the target.')\n",
    "    print('>>> Standard CV optimization CANNOT reach the target.')\n",
    "else:\n",
    "    print(f'>>> Need to reduce CV from {df[\"cv\"].min():.4f} to {required_cv:.4f}')\n",
    "    print(f'>>> That\\'s a {(df[\"cv\"].min() - required_cv) / df[\"cv\"].min() * 100:.1f}% reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CV-LB relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot submissions\n",
    "plt.scatter(df['cv'], df['lb'], s=100, c='blue', alpha=0.7, label='Submissions')\n",
    "\n",
    "# Plot regression line\n",
    "cv_range = np.linspace(0, 0.015, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'r--', label=f'LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Plot target\n",
    "plt.axhline(y=0.0347, color='green', linestyle='-', linewidth=2, label='Target (0.0347)')\n",
    "\n",
    "# Plot intercept\n",
    "plt.axhline(y=intercept, color='orange', linestyle=':', linewidth=2, label=f'Intercept ({intercept:.4f})')\n",
    "\n",
    "# Annotate best submission\n",
    "best_idx = df['lb'].idxmin()\n",
    "plt.annotate(f'Best: {df.loc[best_idx, \"exp\"]}\\nCV={df.loc[best_idx, \"cv\"]:.4f}\\nLB={df.loc[best_idx, \"lb\"]:.4f}',\n",
    "             xy=(df.loc[best_idx, 'cv'], df.loc[best_idx, 'lb']),\n",
    "             xytext=(df.loc[best_idx, 'cv'] + 0.001, df.loc[best_idx, 'lb'] - 0.01),\n",
    "             arrowprops=dict(arrowstyle='->', color='black'),\n",
    "             fontsize=10)\n",
    "\n",
    "plt.xlabel('CV Score (MSE)')\n",
    "plt.ylabel('LB Score (MSE)')\n",
    "plt.title('CV-LB Relationship Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 0.015)\n",
    "plt.ylim(0, 0.12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\nFigure saved to /home/code/exploration/cv_lb_relationship.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31340ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gap between 1st place and our best\n",
    "print('=== LEADERBOARD GAP ANALYSIS ===')\n",
    "print(f'1st place: 0.0347')\n",
    "print(f'Our best LB: 0.0877')\n",
    "print(f'Gap: {0.0877 - 0.0347:.4f} ({(0.0877 - 0.0347) / 0.0347 * 100:.1f}% worse)')\n",
    "print()\n",
    "print('=== WHAT DOES 1ST PLACE KNOW THAT WE DON\\'T? ===')\n",
    "print('1. They achieved LB=0.0347 which is BELOW our intercept (0.052)')\n",
    "print('2. This means they found a way to CHANGE the CV-LB relationship')\n",
    "print('3. Possible approaches:')\n",
    "print('   a) Different validation scheme that better matches test distribution')\n",
    "print('   b) Features that generalize better to unseen solvents')\n",
    "print('   c) Model architecture that handles extrapolation better')\n",
    "print('   d) Post-processing that adjusts predictions for test distribution')\n",
    "print('   e) Domain knowledge/constraints that hold for all solvents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "print('=== APPROACHES TRIED ===')\n",
    "approaches = [\n",
    "    ('MLP baseline', 'exp_000', 0.0111, 0.0982),\n",
    "    ('LightGBM', 'exp_001', 0.0123, 0.1065),\n",
    "    ('Combined features (Spange+DRFP)', 'exp_003', 0.0105, 0.0972),\n",
    "    ('Large ensemble (15 models)', 'exp_005', 0.0104, 0.0969),\n",
    "    ('Simpler model', 'exp_006', 0.0097, 0.0946),\n",
    "    ('Ridge regression', 'exp_009', 0.0092, 0.0936),\n",
    "    ('Simple ensemble', 'exp_012', 0.0090, 0.0913),\n",
    "    ('ACS PCA features', 'exp_024', 0.0087, 0.0893),\n",
    "    ('Weighted loss', 'exp_026', 0.0085, 0.0887),\n",
    "    ('GP+MLP+LGBM ensemble', 'exp_030', 0.0083, 0.0877),\n",
    "    ('Lower GP weight', 'exp_035', 0.0098, 0.0970),\n",
    "    ('Sigmoid output', 'exp_067', 0.0083, 0.0877),\n",
    "]\n",
    "\n",
    "print('All approaches fall on the SAME CV-LB line!')\n",
    "print('This means the problem is NOT the model - it\\'s DISTRIBUTION SHIFT.')\n",
    "print()\n",
    "print('Recent experiments that did NOT improve:')\n",
    "recent = [\n",
    "    ('exp_094: ens-model exact', 0.009564),\n",
    "    ('exp_095: Ridge regression', 0.015756),\n",
    "    ('exp_096: Conservative blend', 0.011124),\n",
    "    ('exp_097: GP uncertainty blend', 0.008930),\n",
    "    ('exp_098: 5-model ensemble', 0.009387),\n",
    "]\n",
    "for name, cv in recent:\n",
    "    print(f'  {name}: CV={cv:.6f} ({(cv - 0.0083) / 0.0083 * 100:+.1f}% vs exp_030)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c042ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What HASN'T been tried that could change the CV-LB relationship?\n",
    "print('=== APPROACHES THAT COULD CHANGE THE CV-LB RELATIONSHIP ===')\n",
    "print()\n",
    "print('1. PSEUDO-LABELING / SELF-TRAINING')\n",
    "print('   - Use confident predictions on test data to augment training')\n",
    "print('   - This adapts the model to the test distribution')\n",
    "print('   - NOT tried yet!')\n",
    "print()\n",
    "print('2. DOMAIN-SPECIFIC CONSTRAINTS')\n",
    "print('   - Yields must sum to <= 1 (mass balance)')\n",
    "print('   - Arrhenius kinetics constraints')\n",
    "print('   - Solvent polarity relationships')\n",
    "print('   - Partially tried but not enforced as hard constraints')\n",
    "print()\n",
    "print('3. SOLVENT SIMILARITY WEIGHTING')\n",
    "print('   - Weight training samples by similarity to test solvents')\n",
    "print('   - Tried in exp_037 but may not have been implemented correctly')\n",
    "print()\n",
    "print('4. DIFFERENT VALIDATION SCHEME')\n",
    "print('   - The mixall kernel uses GroupKFold(5) instead of Leave-One-Out')\n",
    "print('   - This may better match the test distribution')\n",
    "print('   - Tried in exp_078 but didn\\'t help')\n",
    "print()\n",
    "print('5. FUNDAMENTALLY DIFFERENT FEATURES')\n",
    "print('   - Pre-trained molecular embeddings (ChemBERTa, MolBERT)')\n",
    "print('   - Graph neural networks on molecular structure')\n",
    "print('   - Tried GNN (exp_085-087) and ChemBERTa (exp_088) but they failed')\n",
    "print()\n",
    "print('6. ENSEMBLE OF FUNDAMENTALLY DIFFERENT APPROACHES')\n",
    "print('   - Combine tree-based + neural + GP models')\n",
    "print('   - Already doing this (exp_030) - best so far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the public kernels we have\n",
    "print('=== PUBLIC KERNEL ANALYSIS ===')\n",
    "print()\n",
    "print('1. ens-model (matthewmaree)')\n",
    "print('   - Uses CatBoost + XGBoost only')\n",
    "print('   - Correlation filtering (threshold=0.90)')\n",
    "print('   - Feature priority: spange > acs > drfps > frag > smiles')\n",
    "print('   - Different weights for single vs full data')\n",
    "print('   - We tried to replicate but got worse results')\n",
    "print()\n",
    "print('2. mixall (lishellliang)')\n",
    "print('   - Uses GroupKFold(5) instead of Leave-One-Out')\n",
    "print('   - Their CV is NOT comparable to ours')\n",
    "print('   - Fast runtime (2m 15s)')\n",
    "print()\n",
    "print('3. best-work-here (gentilless)')\n",
    "print('   - 4-model ensemble (CatBoost, XGBoost, LightGBM, NN)')\n",
    "print('   - Adaptive weighting')\n",
    "print('   - Non-linear mixture: A*(1-r) + B*r + 0.05*A*B*r*(1-r)')\n",
    "print('   - Uses train/val split within each fold')\n",
    "print()\n",
    "print('4. catechol-strategy (dabansherwani)')\n",
    "print('   - Claims to get 0.11161')\n",
    "print('   - Need to check their approach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The 1st place score (0.0347) is BELOW our intercept (0.052)\n",
    "# This means they found a way to CHANGE the CV-LB relationship\n",
    "# \n",
    "# Possible explanations:\n",
    "# 1. They have a fundamentally different approach that doesn't follow our CV-LB line\n",
    "# 2. They found features that generalize better to unseen solvents\n",
    "# 3. They use domain knowledge/constraints that hold for all solvents\n",
    "# 4. They use a different validation scheme that better matches test distribution\n",
    "# 5. They use post-processing that adjusts predictions for test distribution\n",
    "\n",
    "print('=== CRITICAL INSIGHT ===')\n",
    "print()\n",
    "print('Our CV-LB relationship: LB = 4.36*CV + 0.052')\n",
    "print('Intercept (0.052) > Target (0.0347)')\n",
    "print()\n",
    "print('This means:')\n",
    "print('1. Even with PERFECT CV (0.0), our predicted LB would be 0.052')\n",
    "print('2. The 1st place score (0.0347) is BELOW our intercept')\n",
    "print('3. They must have found a way to CHANGE the CV-LB relationship')\n",
    "print()\n",
    "print('What could change the relationship?')\n",
    "print('- Features that generalize better (reduce intercept)')\n",
    "print('- Model that handles extrapolation better (reduce intercept)')\n",
    "print('- Post-processing that adjusts for test distribution (reduce intercept)')\n",
    "print('- Domain constraints that hold for all solvents (reduce intercept)')\n",
    "print()\n",
    "print('What WON\\'T change the relationship?')\n",
    "print('- Better hyperparameters (just moves along the line)')\n",
    "print('- More models in ensemble (just moves along the line)')\n",
    "print('- Different model types (all fall on same line)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37045c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what the dabansherwani kernel does\n",
    "import os\n",
    "kernel_path = '/home/code/research/kernels/dabansherwani_catechol-strategy-to-get-0-11161'\n",
    "if os.path.exists(kernel_path):\n",
    "    files = os.listdir(kernel_path)\n",
    "    print(f'Files in {kernel_path}:')\n",
    "    for f in files:\n",
    "        print(f'  {f}')\n",
    "else:\n",
    "    print(f'Kernel not found: {kernel_path}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
