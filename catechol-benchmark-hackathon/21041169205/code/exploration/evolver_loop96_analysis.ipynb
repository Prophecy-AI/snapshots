{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57120b56",
   "metadata": {},
   "source": [
    "# Loop 96 Analysis: Strategic Assessment\n",
    "\n",
    "## Key Questions:\n",
    "1. What is the CV-LB relationship across all submissions?\n",
    "2. What techniques from top kernels haven't we tried?\n",
    "3. What's the path to beating 0.0347?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ca5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history with LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "    {'exp': 'exp_067', 'cv': 0.0083, 'lb': 0.0877},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-LB Relationship Analysis\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "\n",
    "print(f'\\n=== CV-LB RELATIONSHIP ===')\n",
    "print(f'Linear fit: LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nInterpretation:')\n",
    "print(f'  - Intercept: {intercept:.4f}')\n",
    "print(f'  - Target LB: 0.0347')\n",
    "print(f'  - Intercept > Target? {intercept > 0.0347}')\n",
    "\n",
    "if intercept > 0.0347:\n",
    "    print(f'\\n⚠️ CRITICAL: Intercept ({intercept:.4f}) > Target (0.0347)')\n",
    "    print(f'   Even with CV=0, expected LB would be {intercept:.4f}')\n",
    "    print(f'   This means standard CV optimization CANNOT reach the target!')\n",
    "    required_cv = (0.0347 - intercept) / slope\n",
    "    print(f'   Required CV to hit target: {required_cv:.4f} (IMPOSSIBLE - negative)')\n",
    "else:\n",
    "    required_cv = (0.0347 - intercept) / slope\n",
    "    print(f'\\n✓ Target is theoretically reachable')\n",
    "    print(f'   Required CV to hit target: {required_cv:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV-LB relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cv, lb, s=100, c='blue', alpha=0.7, label='Submissions')\n",
    "\n",
    "# Fit line\n",
    "cv_range = np.linspace(0, max(cv)*1.1, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'r--', label=f'Fit: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.0347, color='green', linestyle=':', linewidth=2, label='Target (0.0347)')\n",
    "\n",
    "# Intercept line\n",
    "plt.axhline(y=intercept, color='orange', linestyle='--', alpha=0.5, label=f'Intercept ({intercept:.4f})')\n",
    "\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship - All Submissions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nBest CV: {df.loc[df[\"cv\"].idxmin(), \"exp\"]} with CV={df[\"cv\"].min():.4f}')\n",
    "print(f'Best LB: {df.loc[df[\"lb\"].idxmin(), \"exp\"]} with LB={df[\"lb\"].min():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What techniques from top kernels haven't we fully exploited?\n",
    "\n",
    "print('=== TECHNIQUES FROM TOP KERNELS ===')\n",
    "print()\n",
    "print('1. best-work-here kernel (gentilless):')\n",
    "print('   - Non-linear mixture features: A*(1-r) + B*r + 0.05*A*B*r*(1-r)')\n",
    "print('   - SE (Squeeze-and-Excitation) attention blocks')\n",
    "print('   - Adaptive ensemble weighting based on validation performance')\n",
    "print('   - Multi-scaler approach (RobustScaler + QuantileTransformer)')\n",
    "print('   - Deep residual network with LayerNorm and GELU')\n",
    "print('   STATUS: We tried some of these but not the non-linear mixture features')\n",
    "print()\n",
    "print('2. ens-model kernel (matthewmaree):')\n",
    "print('   - CatBoost + XGBoost ensemble with optimized weights')\n",
    "print('   - All features combined: spange + acs_pca + drfps + fragprints + smiles')\n",
    "print('   - Correlation filtering with priority-based feature selection')\n",
    "print('   - Numeric feature engineering: T_x_RT, RT_log, T_inv, RT_scaled')\n",
    "print('   - Different weights for single vs full data')\n",
    "print('   STATUS: We tried this but with aggressive correlation filtering (70% dropped)')\n",
    "print()\n",
    "print('3. mixall kernel (lishellliang):')\n",
    "print('   - Uses GroupKFold (5-fold) instead of Leave-One-Out')\n",
    "print('   - MLP + XGBoost + RF + LightGBM ensemble')\n",
    "print('   - Optuna hyperparameter optimization')\n",
    "print('   STATUS: Different CV scheme - may explain different CV-LB relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The intercept problem\n",
    "\n",
    "print('=== THE INTERCEPT PROBLEM ===')\n",
    "print()\n",
    "print('Current situation:')\n",
    "print(f'  - CV-LB relationship: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'  - Intercept ({intercept:.4f}) > Target (0.0347)')\n",
    "print(f'  - This means: Even with PERFECT CV (0.0), LB would be {intercept:.4f}')\n",
    "print()\n",
    "print('What causes the intercept?')\n",
    "print('  1. DISTRIBUTION SHIFT: Test solvents are fundamentally different from training')\n",
    "print('  2. EXTRAPOLATION ERROR: Models trained on some solvents cannot predict others')\n",
    "print('  3. STRUCTURAL PROBLEM: The Leave-One-Out CV simulates this, but test set may be harder')\n",
    "print()\n",
    "print('How to REDUCE the intercept (not just improve CV)?')\n",
    "print('  1. Non-linear mixture features - capture solvent interactions')\n",
    "print('  2. Solvent similarity-based blending - conservative predictions for dissimilar solvents')\n",
    "print('  3. Physics-informed constraints - Arrhenius kinetics that generalize')\n",
    "print('  4. Domain adaptation - make model robust to distribution shift')\n",
    "print('  5. Ensemble diversity - different models may have different intercepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's our best path forward?\n",
    "\n",
    "print('=== STRATEGIC RECOMMENDATIONS ===')\n",
    "print()\n",
    "print('With 4 submissions remaining, we need to be strategic.')\n",
    "print()\n",
    "print('PRIORITY 1: Non-linear mixture features (UNTRIED)')\n",
    "print('  - From best-work-here kernel: A*(1-r) + B*r + 0.05*A*B*r*(1-r)')\n",
    "print('  - This captures solvent-solvent interactions that linear mixing misses')\n",
    "print('  - Could reduce the intercept by better modeling mixture behavior')\n",
    "print()\n",
    "print('PRIORITY 2: Less aggressive feature filtering')\n",
    "print('  - exp_090 dropped 70% of features - too aggressive')\n",
    "print('  - Try threshold=0.95 instead of 0.80')\n",
    "print('  - Keep more features, let the model decide importance')\n",
    "print()\n",
    "print('PRIORITY 3: Adaptive ensemble weighting')\n",
    "print('  - From best-work-here: weight models by inverse validation error')\n",
    "print('  - weights = (1/val_errors) ** power / sum((1/val_errors) ** power)')\n",
    "print('  - This gives more weight to better-performing models per fold')\n",
    "print()\n",
    "print('PRIORITY 4: Solvent similarity-based conservative predictions')\n",
    "print('  - Calculate similarity of test solvent to training solvents')\n",
    "print('  - When dissimilar, blend toward population mean')\n",
    "print('  - This could reduce extrapolation error (the intercept)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "print('=== LOOP 96 SUMMARY ===')\n",
    "print()\n",
    "print('Current state:')\n",
    "print(f'  - Best CV: 0.0081 (exp_049/050 CatBoost+XGBoost, but submission errors)')\n",
    "print(f'  - Best LB: 0.0877 (exp_030/067 GP+MLP+LGBM)')\n",
    "print(f'  - Target: 0.0347')\n",
    "print(f'  - Gap: 0.0530 (152.7% above target)')\n",
    "print()\n",
    "print('CV-LB relationship:')\n",
    "print(f'  - LB = {slope:.2f} * CV + {intercept:.4f} (R² = {r_value**2:.4f})')\n",
    "print(f'  - Intercept ({intercept:.4f}) > Target (0.0347) = STRUCTURAL PROBLEM')\n",
    "print()\n",
    "print('Latest experiment (exp_091):')\n",
    "print('  - Per-target heterogeneous ensemble (HGB for SM, ETR for Products)')\n",
    "print('  - CV = 0.014242 (76% WORSE than best)')\n",
    "print('  - DO NOT SUBMIT - predicted LB would be ~0.11')\n",
    "print()\n",
    "print('Next steps:')\n",
    "print('  1. Try non-linear mixture features with best CatBoost+XGBoost model')\n",
    "print('  2. Try less aggressive feature filtering (threshold=0.95)')\n",
    "print('  3. Try adaptive ensemble weighting')\n",
    "print('  4. Consider solvent similarity-based conservative predictions')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
