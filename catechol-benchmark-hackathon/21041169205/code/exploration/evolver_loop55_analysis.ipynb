{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ae96f0",
   "metadata": {},
   "source": [
    "# Loop 55 Analysis: Submission Failure Investigation\n",
    "\n",
    "**Problem:** The last 5 submissions (exp_049 through exp_054) all failed with \"Evaluation metric raised an unexpected error\"\n",
    "\n",
    "**Goal:** Understand why submissions are failing and fix the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5229fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Get successful submissions\n",
    "submissions = state.get('submissions', [])\n",
    "print('All submissions:')\n",
    "for s in submissions:\n",
    "    lb = s.get('lb_score', '')\n",
    "    cv = s.get('cv_score', '')\n",
    "    print(f\"  {s.get('experiment_id')}: CV={cv}, LB={lb}\")\n",
    "\n",
    "# Filter successful submissions (those with LB scores)\n",
    "successful = [s for s in submissions if s.get('lb_score')]\n",
    "print(f'\\nSuccessful submissions: {len(successful)}')\n",
    "print(f'Failed submissions: {len(submissions) - len(successful)}')\n",
    "\n",
    "# Failed submissions\n",
    "failed = [s for s in submissions if not s.get('lb_score')]\n",
    "print(f'\\nFailed submission IDs: {[s.get(\"experiment_id\") for s in failed]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-LB relationship analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "cv_scores = [s['cv_score'] for s in successful]\n",
    "lb_scores = [s['lb_score'] for s in successful]\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "\n",
    "print('CV-LB Relationship Analysis')\n",
    "print('=' * 50)\n",
    "print(f'Linear fit: LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'R-squared = {r_value**2:.4f}')\n",
    "print(f'Intercept = {intercept:.4f}')\n",
    "print(f'Target = 0.0347')\n",
    "print()\n",
    "print(f'CRITICAL: Intercept ({intercept:.4f}) > Target (0.0347)')\n",
    "print(f'Required CV to hit target: (0.0347 - {intercept:.4f}) / {slope:.4f} = {(0.0347 - intercept) / slope:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current submission format\n",
    "df = pd.read_csv('/home/submission/submission.csv')\n",
    "\n",
    "print('Current Submission Analysis')\n",
    "print('=' * 50)\n",
    "print(f'Total rows: {len(df)}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "print(f'\\nTask distribution:')\n",
    "print(df['task'].value_counts().sort_index())\n",
    "print(f'\\nFolds per task:')\n",
    "print(df.groupby('task')['fold'].nunique())\n",
    "print(f'\\nTarget statistics:')\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f'  {col}: min={df[col].min():.6f}, max={df[col].max():.6f}')\n",
    "    print(f'    Values > 1: {(df[col] > 1).sum()}')\n",
    "    print(f'    Values < 0: {(df[col] < 0).sum()}')\n",
    "    print(f'    NaN: {df[col].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c960bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fold/row structure more carefully\n",
    "print('Fold/Row structure analysis:')\n",
    "print('=' * 50)\n",
    "\n",
    "for task in [0, 1]:\n",
    "    task_df = df[df['task'] == task]\n",
    "    print(f'\\nTask {task}:')\n",
    "    print(f'  Total rows: {len(task_df)}')\n",
    "    print(f'  Unique folds: {task_df[\"fold\"].nunique()}')\n",
    "    print(f'  Fold range: {task_df[\"fold\"].min()} to {task_df[\"fold\"].max()}')\n",
    "    \n",
    "    # Check rows per fold\n",
    "    rows_per_fold = task_df.groupby('fold').size()\n",
    "    print(f'  Rows per fold: min={rows_per_fold.min()}, max={rows_per_fold.max()}')\n",
    "    \n",
    "    # Check row indices within each fold\n",
    "    for fold in sorted(task_df['fold'].unique())[:3]:  # Check first 3 folds\n",
    "        fold_df = task_df[task_df['fold'] == fold]\n",
    "        print(f'    Fold {fold}: rows {fold_df[\"row\"].min()} to {fold_df[\"row\"].max()}, count={len(fold_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual data to compare\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "# Load single solvent data\n",
    "single_df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "print('Single Solvent Data:')\n",
    "print(f'  Total rows: {len(single_df)}')\n",
    "print(f'  Unique solvents: {single_df[\"SOLVENT NAME\"].nunique()}')\n",
    "\n",
    "# Load full data\n",
    "full_df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "print('\\nFull Data:')\n",
    "print(f'  Total rows: {len(full_df)}')\n",
    "print(f'  Unique solvent pairs: {full_df.groupby([\"SOLVENT A NAME\", \"SOLVENT B NAME\"]).ngroups}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Summary\n",
    "print('Analysis Summary:')\n",
    "print('=' * 50)\n",
    "print()\n",
    "print('1. Submission format appears correct:')\n",
    "print('   - 1883 rows (656 single + 1227 full)')\n",
    "print('   - Correct columns: id, index, task, fold, row, target_1, target_2, target_3')\n",
    "print('   - Task 0: 24 folds, Task 1: 13 folds')\n",
    "print('   - All targets in [0, 1] range')\n",
    "print('   - No NaN or Inf values')\n",
    "print()\n",
    "print('2. CV-LB Relationship:')\n",
    "print(f'   - LB = {slope:.4f} * CV + {intercept:.4f} (R-squared = {r_value**2:.4f})')\n",
    "print(f'   - Intercept ({intercept:.4f}) > Target (0.0347)')\n",
    "print(f'   - This means even CV=0 would give LB={intercept:.4f}')\n",
    "print()\n",
    "print('3. Best CV achieved: 0.008092 (exp_050/051/053)')\n",
    "print(f'   - Predicted LB: {slope * 0.008092 + intercept:.4f}')\n",
    "print(f'   - Best actual LB: 0.0877 (exp_030)')\n",
    "print()\n",
    "print('4. Submission failures:')\n",
    "print('   - exp_049, exp_050, exp_052, exp_053, exp_054 all failed')\n",
    "print('   - Error: \"Evaluation metric raised an unexpected error\"')\n",
    "print('   - The format looks correct, so this may be a Kaggle-side issue')\n",
    "print('   - OR there is something subtle about the evaluation we are missing')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
