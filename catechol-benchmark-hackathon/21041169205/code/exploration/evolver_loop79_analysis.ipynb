{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d448cec",
   "metadata": {},
   "source": [
    "# Loop 79 Analysis: Reconciling CV-LB Relationship Discrepancy\n",
    "\n",
    "## Key Question:\n",
    "The evaluator claims R² ≈ 0 (no correlation), but previous analysis shows R² = 0.956 (strong correlation).\n",
    "Which is correct? Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea42419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Full submission history with LB scores\n",
    "submissions = [\n",
    "    ('exp_000', 0.0111, 0.0982),\n",
    "    ('exp_001', 0.0123, 0.1065),\n",
    "    ('exp_003', 0.0105, 0.0972),\n",
    "    ('exp_005', 0.0104, 0.0969),\n",
    "    ('exp_006', 0.0097, 0.0946),\n",
    "    ('exp_007', 0.0093, 0.0932),\n",
    "    ('exp_009', 0.0092, 0.0936),\n",
    "    ('exp_012', 0.0090, 0.0913),\n",
    "    ('exp_024', 0.0087, 0.0893),\n",
    "    ('exp_026', 0.0085, 0.0887),\n",
    "    ('exp_030', 0.0083, 0.0877),\n",
    "    ('exp_035', 0.0098, 0.0970),\n",
    "    ('exp_067', 0.0083, 0.0877),\n",
    "]\n",
    "\n",
    "cv_scores = np.array([s[1] for s in submissions])\n",
    "lb_scores = np.array([s[2] for s in submissions])\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "\n",
    "print(f'=== CV-LB Relationship Analysis (13 submissions) ===')\n",
    "print(f'Linear fit: LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nIntercept = {intercept:.4f}')\n",
    "print(f'Target LB = 0.0347')\n",
    "print(f'Required CV for target = ({0.0347} - {intercept:.4f}) / {slope:.4f} = {(0.0347 - intercept) / slope:.4f}')\n",
    "print(f'\\nBest CV: {min(cv_scores):.4f}')\n",
    "print(f'Best LB: {min(lb_scores):.4f}')\n",
    "print(f'Gap to target: {min(lb_scores) - 0.0347:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV vs LB\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cv_scores, lb_scores, s=100, alpha=0.7, label='Submissions')\n",
    "\n",
    "# Regression line\n",
    "cv_range = np.linspace(min(cv_scores) - 0.001, max(cv_scores) + 0.001, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'r--', label=f'Fit: LB = {slope:.2f}*CV + {intercept:.4f} (R²={r_value**2:.3f})')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.0347, color='green', linestyle='-', alpha=0.7, label='Target (0.0347)')\n",
    "\n",
    "# Intercept line\n",
    "plt.axhline(y=intercept, color='orange', linestyle=':', alpha=0.5, label=f'Intercept ({intercept:.4f})')\n",
    "\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship (13 submissions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship_loop79.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluator's claim of R² ≈ 0 is WRONG\n",
    "# The actual R² = 0.956 shows STRONG correlation\n",
    "# This means CV improvements DO translate to LB improvements, but with a high intercept\n",
    "\n",
    "print('=== CRITICAL FINDING ===')\n",
    "print(f'R² = {r_value**2:.4f} (STRONG correlation, NOT zero!)')\n",
    "print(f'\\nThe evaluator\\'s claim of R² ≈ 0 is INCORRECT.')\n",
    "print(f'The previous analysis (R² = 0.956) is CORRECT.')\n",
    "print(f'\\nThis means:')\n",
    "print(f'1. CV improvements DO translate to LB improvements')\n",
    "print(f'2. BUT the intercept ({intercept:.4f}) is higher than target (0.0347)')\n",
    "print(f'3. To reach target, we need to REDUCE THE INTERCEPT')\n",
    "print(f'\\nThe intercept represents STRUCTURAL DISTRIBUTION SHIFT')\n",
    "print(f'that cannot be fixed by just improving CV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gap\n",
    "gap = intercept - 0.0347\n",
    "print(f'=== Gap Analysis ===')\n",
    "print(f'Intercept: {intercept:.4f}')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'Gap: {gap:.4f}')\n",
    "print(f'\\nThis gap ({gap:.4f}) represents the MINIMUM LB we can achieve')\n",
    "print(f'even with perfect CV (CV=0).')\n",
    "print(f'\\nTo reach target, we need approaches that CHANGE THE RELATIONSHIP,')\n",
    "print(f'not just improve CV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf34a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches might change the relationship?\n",
    "print('=== Approaches to Change CV-LB Relationship ===')\n",
    "print('\\n1. EXTRAPOLATION DETECTION')\n",
    "print('   - Detect when test solvent is far from training distribution')\n",
    "print('   - Blend predictions toward population mean for outliers')\n",
    "print('   - This could reduce error on \"hard\" test solvents')\n",
    "\n",
    "print('\\n2. UNCERTAINTY-WEIGHTED PREDICTIONS')\n",
    "print('   - Use GP variance as uncertainty estimate')\n",
    "print('   - High uncertainty → conservative prediction')\n",
    "print('   - This could reduce variance on outlier solvents')\n",
    "\n",
    "print('\\n3. SOLVENT CLUSTERING')\n",
    "print('   - Group solvents by chemical class')\n",
    "print('   - Use class-specific models')\n",
    "print('   - This could help if test solvents are from different classes')\n",
    "\n",
    "print('\\n4. PHYSICS-INFORMED CONSTRAINTS')\n",
    "print('   - Add constraints that hold even for unseen solvents')\n",
    "print('   - E.g., Arrhenius kinetics, solvent polarity effects')\n",
    "\n",
    "print('\\n5. STUDY TOP PUBLIC KERNELS')\n",
    "print('   - The \"ens-model\" kernel uses CatBoost + XGBoost')\n",
    "print('   - The \"mixall\" kernel uses GroupKFold CV')\n",
    "print('   - These might have different CV-LB characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the best-performing kernels do\n",
    "print('=== Top Public Kernels Analysis ===')\n",
    "print('\\n1. ens-model kernel:')\n",
    "print('   - CatBoost with MultiRMSE loss (trains all targets jointly)')\n",
    "print('   - XGBoost per-target')\n",
    "print('   - Weights: 7:6 for CatBoost:XGBoost on single solvent, 1:2 on full data')\n",
    "print('   - Post-processing: clip negatives, renormalize if sum > 1')\n",
    "\n",
    "print('\\n2. mixall kernel:')\n",
    "print('   - GroupKFold (5 splits) instead of Leave-One-Out')\n",
    "print('   - Ensemble of MLP + XGBoost + RandomForest + LightGBM')\n",
    "print('   - Weights: [0.4, 0.2, 0.2, 0.2]')\n",
    "print('   - Simple Spange descriptors (13 features)')\n",
    "\n",
    "print('\\n3. best-work-here kernel:')\n",
    "print('   - Probability normalization (but we found this HURTS CV!)')\n",
    "print('   - Yields don\\'t sum to 1 in actual data (mean sum ~0.80)')\n",
    "print('   - DO NOT USE probability normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print('=== SUMMARY ===')\n",
    "print(f'Best LB: 0.0877 (exp_030, exp_067)')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'Gap: 0.0530 (153% above target)')\n",
    "print(f'\\nCV-LB Relationship: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f} (STRONG correlation)')\n",
    "print(f'Intercept ({intercept:.4f}) > Target ({0.0347})')\n",
    "print(f'\\nCRITICAL: The intercept is the bottleneck, not CV!')\n",
    "print(f'\\n=== RECOMMENDED NEXT STEPS ===')\n",
    "print('1. DO NOT submit exp_074 (prob_norm) - CV regressed 64%')\n",
    "print('2. Try extrapolation detection with conservative fallback')\n",
    "print('3. Try the ens-model kernel approach (CatBoost + XGBoost)')\n",
    "print('4. Consider uncertainty-weighted predictions using GP variance')\n",
    "print('5. Save submissions for approaches that might change the CV-LB relationship')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
