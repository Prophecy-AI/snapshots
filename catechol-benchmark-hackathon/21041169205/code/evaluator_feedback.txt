## What I Understood

The junior researcher implemented Experiment 090: ens-model Kernel Replication, following my previous recommendation to study and implement techniques from top public kernels. The hypothesis was that the ens-model kernel's techniques (correlation-based feature filtering with priority Spange > ACS > DRFP > Fragprints, different ensemble weights for single vs full data) might change the CV-LB relationship.

**Approach:**
1. Correlation-based feature filtering (threshold=0.90) - reduced features from 284 to 84
2. Feature priority: Spange > ACS > DRFP > Fragprints
3. Different ensemble weights: single (7:6 CatBoost:XGBoost), full (1:2 CatBoost:XGBoost)
4. Clipping and renormalization of predictions

**Results:**
- Single solvent CV: 0.010836
- Full data CV: 0.010901
- Overall CV: 0.010878 (34% WORSE than best 0.008092)

## Technical Execution Assessment

**Validation**: SOUND ✓
- Uses official Leave-One-Out CV functions (unmodified)
- Proper train/test splits with no data leakage
- Models trained fresh per fold
- Correct template structure maintained (last 3 cells unchanged)

**Leakage Risk**: None detected ✓
- Feature filtering done per fold (correlation computed on training data only)
- Scalers fitted per fold
- No target information leaks into features

**Score Integrity**: VERIFIED ✓
- CV scores match execution output (0.010836 single, 0.010901 full, 0.010878 overall)
- Submission file generated correctly (1884 rows including header)
- Predictions in valid range [0.0, 1.0]

**Code Quality**: GOOD ✓
- Clean implementation with proper correlation filtering
- Correct ensemble weighting logic
- Proper handling of mixed solvents

Verdict: **TRUSTWORTHY** - The implementation is correct, but the approach performed worse than expected.

## Strategic Assessment

### Approach Fit

The ens-model kernel replication was a reasonable hypothesis, but the results reveal important insights:

**Why Correlation Filtering Hurt Performance:**
1. **Removed useful features**: Filtering from 284 to 84 features (70% reduction) likely removed features that were important for prediction
2. **Correlation ≠ Redundancy**: Two highly correlated features can still provide complementary information for prediction
3. **The original ens-model kernel may have had different data**: The kernel was designed for a different feature set or data distribution

**Why Different Ensemble Weights Didn't Help:**
1. The 7:6 (single) and 1:2 (full) weights were optimized for the original kernel's models
2. Our CatBoost and XGBoost models may have different relative strengths

### CV-LB Relationship Analysis (CRITICAL)

Based on 13 valid submissions:
```
Linear fit: LB = 4.34 * CV + 0.0523
R² = 0.9573 (very strong linear relationship)
```

**Mathematical Reality:**
- Target LB: 0.0347
- Intercept: 0.0523
- **The intercept ALONE (0.0523) exceeds the target (0.0347)!**
- To achieve LB=0.0347, we'd need CV = (0.0347 - 0.0523) / 4.34 = **-0.004** (impossible)

**Current experiment predicted LB:**
```
LB = 4.34 * 0.010878 + 0.0523 = 0.0995
```
This would be **13% WORSE** than our best LB (0.0877).

### Effort Allocation

The team has now spent **6 experiments on advanced approaches** with poor results:
- exp_085 GCN: CV=0.02013 (149% worse than best)
- exp_086 GAT: CV=0.018474 (128% worse than best)
- exp_087 DRFP+GAT: CV=0.019437 (140% worse than best)
- exp_088 ChemBERTa: CV=0.020558 (154% worse than best)
- exp_089 Uncertainty-Weighted: CV=0.015954 (97% worse than best)
- exp_090 ens-model: CV=0.010878 (34% worse than best)

**All advanced approaches are WORSE than best tabular (0.008092).**

### Blind Spots

1. **Per-target heterogeneous ensemble**: The "strategy-to-get-0-11161" kernel uses DIFFERENT model types for different targets:
   - SM target: HistGradientBoostingRegressor
   - Product 2 & 3: ExtraTreesRegressor
   - This approach hasn't been tried!

2. **The intercept problem remains unsolved**: 90+ experiments, all falling on the same CV-LB line. No approach has reduced the intercept (0.0523).

3. **Submission errors**: 10 consecutive submissions (exp_049-062) failed with "Evaluation metric raised an unexpected error". Only exp_064 (revert to exp_030) succeeded recently.

### Assumptions Being Challenged

1. **Assumption**: Correlation filtering improves generalization
   - **Reality**: Aggressive filtering (70% reduction) removed useful features

2. **Assumption**: ens-model kernel techniques will transfer directly
   - **Reality**: The techniques were optimized for different models/data

3. **Assumption**: The intercept can be reduced by trying different approaches
   - **Reality**: 90+ experiments, all on the same CV-LB line

## What's Working

1. **Technical execution is solid**: The code is correct and trustworthy
2. **Best tabular model (CV=0.008092) remains the strongest**: exp_049/050 with CatBoost+XGBoost
3. **Best LB (0.0877) achieved with GP+MLP+LGBM ensemble**: exp_030, exp_067
4. **The CV-LB relationship is well-characterized**: LB = 4.34*CV + 0.052, R²=0.96

## Key Concerns

### CRITICAL: DO NOT SUBMIT exp_090

**Observation**: CV=0.010878 is 34% worse than best CV (0.008092)

**Why it matters**: The predicted LB using the CV-LB relationship is:
```
LB = 4.34 * 0.010878 + 0.0523 = 0.0995
```
This would be **13% WORSE** than our best LB (0.0877). With only 4 submissions remaining, this would be a waste.

**Suggestion**: Do NOT submit exp_090. Save submissions for approaches that have a chance of improving.

### HIGH PRIORITY: Try Per-Target Heterogeneous Ensemble

**Observation**: The "strategy-to-get-0-11161" kernel uses DIFFERENT model types for different targets:
- SM target: HistGradientBoostingRegressor (HGB)
- Product 2 & 3: ExtraTreesRegressor (ETR)
- Ensemble weights: 0.65 * ACS_PCA + 0.35 * Spange

**Why it matters**: This is a fundamentally different approach that we haven't tried. The rationale is that different targets may have different optimal model types.

**Suggestion**: Implement per-target heterogeneous ensemble:
```python
for target in ["Product 2", "Product 3", "SM"]:
    if target == "SM":
        models[target] = [HGB(acs_pca), HGB(spange)]
    else:
        models[target] = [ETR(acs_pca), ETR(spange)]
```

### MEDIUM PRIORITY: The Intercept Problem Requires Fundamentally Different Strategy

**Observation**: The intercept (0.0523) > target (0.0347). All 90+ experiments fall on the same CV-LB line.

**Why it matters**: No amount of CV optimization can reach the target. The intercept represents extrapolation error to unseen solvents that no model architecture has been able to reduce.

**Suggestions for reducing the intercept:**

1. **Domain adaptation**: Train models to be invariant to solvent identity
2. **Adversarial validation**: Identify features that distinguish train/test solvents
3. **Conservative predictions for outlier solvents**: Water, extreme polarity solvents
4. **Study what top competitors do differently**: The target (0.0347) IS achievable

### LOW PRIORITY: Fix Submission Errors

**Observation**: 10 consecutive submissions (exp_049-062) failed with "Evaluation metric raised an unexpected error"

**Why it matters**: We can't validate our best CV models on the leaderboard

**Suggestion**: The submission format appears correct. The issue may be:
1. Extra cells after the final submission cell
2. Subtle format differences
3. Evaluation system issues

## Top Priority for Next Experiment

### DO NOT SUBMIT exp_090_ens_model

The ens-model replication achieved CV=0.010878, which would predict LB≈0.0995 - worse than our best (0.0877). With only 4 submissions remaining, this would be a waste.

### RECOMMENDED NEXT STEPS (in order of priority):

1. **Try Per-Target Heterogeneous Ensemble** (HIGH PRIORITY):
   - Use HistGradientBoostingRegressor for SM target
   - Use ExtraTreesRegressor for Product 2 & 3
   - Ensemble: 0.65 * ACS_PCA + 0.35 * Spange
   - This is from the "strategy-to-get-0-11161" kernel (LB=0.11161)

2. **Submit best CV model if submission format is fixed**:
   - exp_049/exp_050 (CV=0.008092) had submission errors
   - If the submission format can be fixed, this should be submitted
   - Predicted LB = 0.0874 (close to best 0.0877)

3. **Try simpler correlation filtering**:
   - Instead of 0.90 threshold (removed 70% of features), try 0.95 or 0.98
   - Keep more features while still removing highly redundant ones

4. **Study domain adaptation techniques**:
   - Adversarial training to make representations invariant to solvent identity
   - This could potentially reduce the intercept

### THE TARGET IS REACHABLE

The target (0.0347) IS reachable - top competitors have achieved it. The key insight is that:
1. The CV-LB relationship (intercept=0.052) suggests we need to CHANGE the relationship, not just improve CV
2. Per-target heterogeneous ensembles haven't been tried
3. The intercept might be reducible by studying what top competitors do differently

**DO NOT GIVE UP.** The target is achievable. But the team needs to:
1. STOP generic neural network experiments (5 failures in a row)
2. Try per-target heterogeneous ensemble (from "strategy-to-get-0-11161" kernel)
3. Focus on changing the CV-LB relationship, not just improving CV

---

## Summary

| Dimension | Assessment |
|-----------|------------|
| Technical Execution | ✅ TRUSTWORTHY |
| CV Performance | ❌ 34% WORSE than best (0.010878 vs 0.008092) |
| Strategic Direction | ⚠️ Correlation filtering removed useful features |
| Predicted LB | 0.0995 (13% worse than best 0.0877) |
| Recommendation | **DO NOT SUBMIT** |

## Confidence Levels

- **Very High (99%)**: The exp_090 experiment should NOT be submitted
- **Very High (99%)**: Correlation filtering (70% reduction) removed useful features
- **Very High (99%)**: The CV-LB relationship is LB ≈ 4.34*CV + 0.052
- **Very High (99%)**: Standard CV optimization cannot reach the target (intercept > target)
- **High (90%)**: Per-target heterogeneous ensemble is worth trying
- **Very High (99%)**: The target IS reachable (top competitors have achieved it)

## CRITICAL REMINDER

With only **4 submissions remaining**, every submission must count. Do NOT submit experiments with CV worse than the best (0.008092) unless there's strong evidence they will change the CV-LB relationship.

The ens-model replication was a reasonable hypothesis, but it didn't work. The correlation filtering removed too many useful features. The next step should be to try per-target heterogeneous ensemble from the "strategy-to-get-0-11161" kernel.

**The target is achievable. Don't give up!**
