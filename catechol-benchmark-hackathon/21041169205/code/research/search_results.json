{
  "query": "What techniques do Kaggle competition winners use when the cross-validation to leaderboard gap is caused by structural distribution shift that cannot be fixed by model tuning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "When the public\u2011leaderboard score is much lower than the cross\u2011validation (CV) estimate, the most common cause is a **structural distribution shift** between the hidden test set and the data used for CV \u2013 a shift that cannot be remedied simply by changing hyper\u2011parameters.  Kaggle winners therefore treat the problem as a *domain\u2011adaptation* task and deliberately design their validation and modelling pipeline to mimic the unseen distribution.\n\n* **Detect and quantify the shift** \u2013 they start with *adversarial validation* (training a classifier to distinguish training from test rows) to spot features that behave differently and to guide feature\u2011engineering or data\u2011cleaning decisions\u30102\u3011.  They also inspect CV variance across many random seeds and folds; large discrepancies often signal a temporal, group\u2011 or seasonal split that the original CV did not capture\u30108\u3011.  \n\n* **Adapt the data or model to the new domain** \u2013 once the shift is identified, winners apply unsupervised or source\u2011free domain\u2011adaptation techniques (e.g., feature alignment, importance\u2011weighting, test\u2011time adaptation) that do not require labelled target data, as recommended in recent best\u2011practice studies on domain adaptation\u30101\u3011.  When possible they generate synthetic target\u2011like examples (data augmentation, time\u2011based rolling windows) to enlarge the \u201ctarget\u201d portion of the training set.\n\n* **Robust validation and ensembling** \u2013 they rebuild the CV scheme to reflect the true split (e.g., time\u2011based hold\u2011outs, group\u2011aware folds) and then train multiple models on different pseudo\u2011target splits.  The diverse models are combined via *stacking* or *blending* to reduce variance caused by the shift\u30102\u3011.  Ensembles that include both high\u2011capacity learners (e.g., deep nets) and more stable linear or tree\u2011based models tend to be less sensitive to distribution changes\u30102\u3011.  \n\n* **Iterative \u201cleaderboard\u2011aware\u201d tuning** \u2013 after each submission they use the public leaderboard as a noisy proxy for the hidden test distribution, adjusting the weighting of features or models that appear to generalise better, while limiting the number of submissions to avoid over\u2011fitting the public hold\u2011out\u30105\u3011.\n\nIn short, winners treat a CV\u2011to\u2011leaderboard gap caused by structural shift as a domain\u2011adaptation problem: they first *detect* the shift with adversarial validation and multi\u2011seed CV, then *adapt* the data or model (unsupervised DA, test\u2011time adaptation, synthetic target data), and finally *stabilise* predictions through carefully engineered validation folds and strong ensembles (stacking/blending).  These steps together bridge the gap without relying on ordinary hyper\u2011parameter tweaks.",
      "url": ""
    },
    {
      "title": "Better Practices for Domain Adaptation",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2309.03879** (cs)\n\n\\[Submitted on 7 Sep 2023\\]\n\n# Title:Better Practices for Domain Adaptation\n\nAuthors: [Linus Ericsson](https://arxiv.org/search/cs?searchtype=author&query=Ericsson,+L), [Da Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+D), [Timothy M. Hospedales](https://arxiv.org/search/cs?searchtype=author&query=Hospedales,+T+M)\n\nView a PDF of the paper titled Better Practices for Domain Adaptation, by Linus Ericsson and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2309.03879)\n\n> Abstract:Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology including Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and Test Time Adaptation (TTA). While the results show that realistically achievable performance is often worse than expected, they also show that using proper validation splits is beneficial, as well as showing that some previously unexplored validation metrics provide the best options to date. Altogether, our improved practices covering data, training, validation and hyperparameter optimisation form a new rigorous pipeline to improve benchmarking, and hence research progress, within this important field going forward.\n\n|     |     |\n| --- | --- |\n| Comments: | AutoML 2023 (Best paper award) |\n| Subjects: | Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2309.03879](https://arxiv.org/abs/2309.03879) \\[cs.LG\\] |\n|  | (or [arXiv:2309.03879v1](https://arxiv.org/abs/2309.03879v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2309.03879](https://doi.org/10.48550/arXiv.2309.03879)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Linus Ericsson \\[ [view email](https://arxiv.org/show-email/8d84eb61/2309.03879)\\]\n\n**\\[v1\\]**\nThu, 7 Sep 2023 17:44:18 UTC (252 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Better Practices for Domain Adaptation, by Linus Ericsson and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2309.03879)\n- [TeX Source](https://arxiv.org/src/2309.03879)\n- [Other Formats](https://arxiv.org/format/2309.03879)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2309.03879&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2309.03879&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-09](https://arxiv.org/list/cs.LG/2023-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2309.03879?context=cs)\n\n[cs.CV](https://arxiv.org/abs/2309.03879?context=cs.CV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2309.03879)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2309.03879)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2309.03879)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2309.03879&description=Better Practices for Domain Adaptation) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2309.03879&title=Better Practices for Domain Adaptation)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2309.03879) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2309.03879"
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    },
    {
      "title": "Competing in a data science contest without reading the data",
      "text": "Machine learning competitions have become an extremely popular format for\nsolving prediction and classification problems of all sorts. The most famous\nexample is perhaps the Netflix prize. An even better example is\n[Kaggle](http://www.kaggle.com), an awesome startup that\u2019s\norganized more than a hundred competitions over the past few years.\n\nThe central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of _holdout labels_ not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.\n\nPublic leaderboard of the Heritage Health Prize ( [Source](http://www.heritagehealthprize.com/c/hhp/leaderboard/public))\n\nIn this post, I will describe a method to climb the public leaderboard _without even looking at the data_. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle\u2019s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some\nchallenges that while fundamental have only recently seen increased attention. A follow-up post will describe a [recent paper](http://arxiv.org/abs/1502.04585) with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.\n\nLet me be very clear that my point is _not_ to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I\u2019m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.\n\n## The Kaggle leaderboard mechanism\n\nAt first sight, the Kaggle mechanism looks like the classic _holdout method_. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in . Think of the score as prediction error (smaller is better). For concreteness, let\u2019s fix it to be the _misclassification rate_. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in .\n\nKaggle further splits its \\\\(N\\\\) private labels randomly into \\\\(n\\\\) holdout labels and \\\\(N-n\\\\) test labels. Typically, \\\\(n=0.3N\\\\). The public leaderboard is a sorting of all teams according to their score computed only on the \\\\(n\\\\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels. I will let \\\\(s\\_H(y)\\\\) denote the public score of a submission \\\\(y\\\\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.\n\n## The cautionary tale of wacky boosting\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there\u2019s an unknown set of labels \\\\(y\\\\in\\\\{0,1\\\\}^N\\\\) that I need to predict. Well, I know nothing about \\\\(y\\\\). So here\u2019s what I\u2019m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we\u2019re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I\u2019m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here\u2019s what I do:\n\n**Algorithm** (Wacky Boosting):\n\n1. Choose \\\\(y\\_1,\\\\dots,y\\_k\\\\in\\\\{0,1\\\\}^N\\\\) uniformly at random.\n2. Let \\\\(I = \\\\{ i\\\\in\\[k\\] \\\\colon s\\_H(y\\_i) < 0.5 \\\\}\\\\).\n3. Output \\\\(\\\\hat y=\\\\mathrm{majority} \\\\{ y\\_i \\\\colon i \\\\in I \\\\} \\\\), where the majority is component-wise.\n\nLo and behold, this is what happens:\n\nIn this plot, \\\\(n=4000\\\\) and all numbers are averaged over 5 independent repetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would\u2019ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at DeepCompeting.ly, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso. Two months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from DeepCompeting.ly days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.\n\n### What just happened\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \\\\(y\\_i\\\\) has loss around \\\\(1/2\\\\pm1/\\\\sqrt{n}\\\\). We\u2019re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \\\\(w\\_i\\\\) is roughly \\\\(1/2-c/\\\\sqrt{n}\\\\) for some positive constant \\\\(c>0\\\\). Put differently, each selected \\\\(y\\_i\\\\) is giving us a guess about each label in the unknown holdout set \\\\(H\\\\subseteq \\[N\\]\\\\) that\u2019s correct with probability \\\\(1/2 + \\\\Omega(1/\\\\sqrt{n})\\\\). Since the public score doesn\u2019t depend on labels outside of \\\\(H\\\\), the conditioning does not affect the final test set. The labels outside of \\\\(H\\\\) are still unbiased. Finally, we need to argue that the majority vote \u201cboosts\u201d our slightly biased coin tosses into a stronger bias. More formally, we can show that \\\\(\\\\hat y\\\\) gives us a guess for each label in \\\\(H\\\\) that\u2019s correct with probability\n\\\\\\[\n\\\\frac12 + \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nHence, the public score of \\\\(y\\\\) satisfies\n\\\\\\[\ns\\_H(y) < \\\\frac12 - \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nOutside of \\\\(H\\\\), however, we\u2019re just random guessing with no advantage.\nTo summarize, wacky boosting gives us _a bias of \\\\(\\\\sqrt{k}\\\\) standard deviations on the public score with \\\\(k\\\\) submissions_.\n\nWhat\u2019s important is that the same algorithm still \u201cworks\u201d even if we don\u2019t get exact answers. All we need are answers that are accurate to an additive error of \\\\(1/\\\\sqrt{n}\\\\). This is important since Kaggle rounds its answers to 5 digits of precision. In particular, this attack will work so long as \\\\(n< 10^{10}\\\\).\n\n### Why the holdout method breaks down\n\nThe idea behind the holdout method is that the holdout data serve as a fresh sample providing an unbiased ...",
      "url": "https://blog.mrtz.org/2015/03/09/competition.html"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    },
    {
      "title": "Kaggle Handbook: Fundamentals to Survive a Kaggle Shake-up",
      "text": "<div><div><div><h2>A guide to Kaggle competitions with tips &amp; tricks to get on the good side of the \u201cShake-up\u201d.</h2><div><a href=\"https://medium.com/@ertugruldemir?source=post_page---byline--3dec0c085bc8---------------------------------------\"><div><p></p></div></a></div></div><p>If you\u2019re interested in data science, you probably heard about Kaggle, a platform where hundreds of DS and ML enthusiasts meet, discuss, share and compete.</p><p>In the first post of this series, I\u2019m going to talk briefly about the format of Kaggle competitions and then move on to the fundamental techniques that will help you end up on the better side of the biggest pitfalls of Kaggle competitions: <em>Shake-up</em>s.</p><h2><strong>How do Kaggle competitions work?</strong></h2><p><strong><em>The competition process is basically as follows:</em></strong></p><ul><li>An individual, or more often an organization, identifies a problem and assumes that this issue can be solved by machine learning.</li><li>The data for the problem might already exist or be collected/generated for the competition. Then, a proper metric is established.</li><li>Depending on the sponsors and the complexity of the problem, a prize pool is set.</li><li>The organizers provide Kaggle staff with the necessary data and materials, and finally, the organizers become the Kaggle competition host.</li><li>Participants submit their solutions during the competition phase and receive their final ranking based on the competition metrics.</li></ul><h2>This looks straightforward<strong>. What\u2019s the catch?</strong></h2><p><strong><em>The catch is in the second stage of the competition where your models are tested on data they have never seen before.</em></strong></p><figure><figcaption>Public vs. Private LB, Source: Kaggle</figcaption></figure><p>Kaggle competitions usually consist of two stages. The results of the first stage are displayed on the <strong>Public Leaderboard</strong> \u2014 a live scoreboard \u2014 and the results of the second stage are displayed on the <strong>Private Leaderboard</strong>. The results of the first stage are not decisive for the final ranking, but they are your guide for the private leaderboard. Organizers usually leave only a small part of the test set for the public leaderboard, and the rest is reserved for the private leaderboard, which is the most important for the final rankings. The private leaderboard is hidden from the participants until the competition ends.</p><p>In terms of restrictions and complexity, there are similar contests on Kaggle called <strong>Code Competitions</strong>. In these contests, the test set features are hidden from the participants, which takes the restrictions one step further by limiting the inference times and preventing hand labeling or exploratory data analysis on test data.</p><h2>You were talking about shake-what?!</h2><p><strong><em>That\u2019s the part where your final outcome reveals.</em></strong></p><p>The shake-up is the difference in rank between public and private leaderboards. One of the main goals of Kaggle competitions is to <em>survive </em>the shake-ups. Is it sheer luck to end up on the good side of shake-ups, or are there better ways? How can we assess our chances of survival in shake-ups? What techniques can we use?</p><p>Data scientists are not the kind of people who shut their eyes and hope for the best. So let us figure out how to better <strong><em>estimate</em></strong> the unknown.</p><h2><strong>How do I know what my model will do on the private leaderboard?</strong></h2><p><strong><em>Let\u2019s talk about fundamentals.</em></strong></p><p><strong>Bias-Variance Tradeoff and Overfitting</strong></p><p>Today we can easily train models with a large number of parameters and create highly complex models. Complex models can be very useful, but they can also be very punishing if we are not careful about the <strong>bias-variance tradeoff</strong>. Such models are risky because they are prone to overfitting. Overfitting occurs when a model memorizes the noise of training data instead of learning about the actual pattern behind it. We want our models to <strong>generalize </strong>and capture the actual nature of the data. That\u2019s what we want in Kaggle competitions too, so we can make predictions on a test set that our model hasn\u2019t trained on. The results of these predictions determine your private leaderboard standings. To assess the generalization capabilities of our model, we need to apply <strong>validation techniques</strong> and understand how our model acts in different cases.</p><p>Bias-Variance Tradeoff is beyond the scope of this post, but I highly recommend you take a look if you are not familiar with this concept.</p><h2>What are these validation techniques?</h2><p><strong><em>There are numerous validation techniques but we\u2019re going to talk about more common and Kaggle-specific ones.</em></strong></p><p><strong>Validation Set Approach</strong></p><p>The validation set approach is pretty straightforward. You separate away a fraction of your training data and this new set is called <strong>Hold-Out</strong>:</p><blockquote><p>The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.</p></blockquote><p><em>\u2014 Hands-On Machine Learning with Scikit-Learn and TensorFlow, 2nd Edition.</em></p><p>The latter part of the quote above bears a resemblance to the public leaderboard concept. We can evaluate our final model on the public test set to get an estimate of the generalization error. However, this approach has its own downsides:</p><ul><li>The validation estimate of our model metric can be highly variable, depending on which observations are included in the training set and which are included in the validation set. This is especially true for the <strong>Public Test Set</strong> since we don\u2019t know if the Private Test Set is coming from similar distribution.</li><li>If we take a subset of our training data as the validation set, we decrease the number of observations to train on, which can hamper our model generalization again.</li></ul><p><strong>Cross Validation Approach</strong></p><figure><figcaption>General k-fold cross validation approach with Kaggle competition additions.</figcaption></figure><blockquote><p>This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k \u2212 1 folds.</p></blockquote><p><em>\u2014 Page 203, An Introduction to Statistical Learning, 2021.</em></p><p>This method can solve the problems with the single hold-out set approach that I mentioned above. By creating multiple <strong>folds</strong>, we can test the metric on different cases while observing the variance and eventually using the whole training data! This way, we can have stronger indicators for our model and estimate private leaderboard scores more confidently. Of course, we still assume training and private test set come from similar distributions. However, we train our models on different sets, so together they\u2019re more likely to generalize better than a single holdout set approach.</p><p><strong>Trust Your CV (Cross Validation)</strong></p><p><strong>Trust Your CV </strong>became a pretty popular tagline among Kaggle competitors, reminding them not to build their models based on public leaderboard scores, and I concur. I have participated in quite a few Kaggle competitions over the last few years, and...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8"
    },
    {
      "title": "Validation Schemes",
      "text": "Validation Schemes \u00b7datadocs\n[## datadocs\n](https://polakowo.io/datadocs/)\n[Edit](https://github.com/polakowo/datadocs/edit/master/docs/machine-learning/validation-schemes.md)# Validation Schemes\n* Never use data you train on to measure the quality of your model (resubstitution).\n* As the model becomes more computational, the variance (dispersion) of the estimate increases, but bias decreases.\n* Overfitting arises due to the fact that the training set is memorized completely instead of generalizing.\n* This effect is only visible when the same estimator is run on similar data other than training.\n* Hold out part of the available data as a test set.\n* There is still a risk of overfitting because the parameters can be tweaked until the estimator performs optimally.\n* Only the final evaluation should be done on the test set.\n* Set up validation to mimic train/test split.\n* In a competition, you need to identify the train/test split made by organizers.\n* In most cases, data is split by rows, time, groups or combined.\n* Logic of feature engineering depends on the data splitting strategy.\n![](https://polakowo.io/datadocs/assets/grid_search_workflow.png)[Credit](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n#### [](#validation-stage)Validation stage\n* We can observe that:\n* High deviation of the local CV scores.\n* Possible causes:\n* Too little data.\n* Data is too diverse and inconsistent (e.g., December and January for store sales).\n* Extensive validation techniques:\n* Perform k-fold split multiple times with different seeds, and average the scores.\n* Tune a model on one split, evaluate the model on the other.\n* Following problems can be identified before the submission stage:\n* Different scores/optimal parameters between folds.\n* Public leaderboard score will be unreliable because of too little data.\n* Train and test data are from different distributions (using EDA).#### [](#submission-stage)Submission stage\n* We can observe that:\n* LB score is consistently lower/higher than the local score:\n* LB score is not correlated with the local score.\n* Possible causes:\n* We may already have a high variance of CV scores: calculate mean and std of CV scores and estimate if LB is expected.\n* Too little data in public leaderboard: trust your local validation.\n* Train and test are from different distributions: adjust distributions or perform leaderboard probing.\n* Overfitting\n* Incorrect cross-validation strategy\n* Expect LB shuffle because of:\n* Randomness can shuffle scores on the private leaderboard.\n* Little amount of training or/and testing data.\n* Different public/private data or target distributions (e.g., time-based split).\n* [How to Select Your Final Models in a Kaggle Competition](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)## [](#base-models)Base models\n* The following validation schemes are supposed to be used to estimate quality of the model.\n* For getting test predictions don't forget to retrain your model using all training data.#### [](#holdout)Holdout\n* Procedure (`train\\_test\\_split`):\n* Split*train*into two parts:*trainA*and*trainB*(usually 80/20).\n* Fit the model on*trainA*and validate it on*trainB*.\n* Use holdout if scores on each fold are roughly the same.#### [](#k-fold)K-Fold\n* Procedure (`KFold`):\n* Split*train*into \\\\(K\\\\) folds.\n* Iterate though each fold:\n* Refit the model on all folds except the current one.\n* Validate the model on the current fold.\n* Assumes that the samples are i.i.d.\n* The performance measure reported by k-fold CV is the average of the values computed in the loop.\n* Scores deviation in KFold can help to select statistically significant change in scores while tuning a model.\n* The value of \\\\(K\\\\) being large could lead to low bias and high variance (overfitting).\n* The advantage is that entire data is used for training and validation.\n* For classification problems that exhibit a large imbalance (`StratifiedKFold`):\n* Stratified k-fold ensures that relative class frequencies are preserved in each train and validation fold.\n* In this case we would like to know if a model generalizes well to the unseen groups (`GroupKFold`):\n* Group k-fold ensures that the same group is not represented in both testing and training sets.#### [](#loo)LOO\n* LOO is a k-fold scheme where \\\\(K=N\\\\) (`LeaveOneOut`).\n* LOO often results in high variance as an estimator for the test error.\n* 5-10 fold cross validation should be preferred to LOO.\n* Mostly used for sparse datasets.#### [](#time-based-validation)Time-based validation\n* Procedure (`TimeSeriesSplit`):\n* Split*train*into chunks of duration \\\\(T\\\\). Select first \\\\(M\\\\) chunks.\n* Fit a model on these \\\\(M\\\\) chunks and predict for the chunk \\\\(M+1\\\\).\n* Then repeat this procedure for the next chunk and so on (imagine a moving window).\n* Used if the samples have been generated using a time-dependent process.\n* Time series data is characterised by the correlation between observations (autocorrelation).\n* Does not assume that the samples are i.i.d.## [](#meta-models)Meta models\n#### [](#simple-holdout-scheme)Simple holdout scheme\n* Procedure:\n* Split*train*into three parts:*trainA*,*trainB*and*trainC*.\n* Fit \\\\(N\\\\) diverse models on*trainA*.\n* Predict for*trainB*,*trainC*, and*test*(getting meta-features*trainB\\_meta*,*trainC\\_meta*and*test\\_meta*).\n* Fit a meta-model on*trainB\\_meta*and validate it on*trainC\\_meta*.\n* When the meta-model is validated, fit it to*[trainB\\_meta, trainC\\_meta]*and predict for*test\\_meta*.\n* This scheme is usually preferred over the other schemes if dataset is large.\n* Fair validation scheme (validation set of meta-models not used in any way by base models)#### [](#meta-holdout-scheme-with-oof-meta-features)Meta holdout scheme with OOF meta-features\n* Procedure:\n* Split*train*into \\\\(K\\\\) folds.\n* Iterate though each fold:\n* Refit \\\\(N\\\\) diverse models on all folds except the current one.\n* Predict for the current fold.\n* For each object in*train*, we now have \\\\(N\\\\) meta-features (out-of-fold predictions, OOF) (getting*train\\_meta*)\n* Fit the models on*train*and predict for*test*(getting*test\\_meta*)\n* Split*train\\_meta*into two parts:*train\\_metaA*and*train\\_metaB*.\n* Fit a meta-model on*train\\_metaA*and validate it on*train\\_metaB*.\n* When the meta-model is validated, fit it to*train\\_meta*and predict for*test\\_meta*.#### [](#meta-kfold-scheme-with-oof-meta-features)Meta KFold scheme with OOF meta-features\n* Procedure:\n* Obtain OOF predictions for*train\\_meta*and*test\\_meta*.\n* Use KFold scheme on*train\\_meta*to validate the meta-model (with same seed as for OOF).\n* When the meta-model is validated, fit it to*train\\_meta*and predict for*test\\_meta*.#### [](#holdout-scheme-with-oof-meta-features)Holdout scheme with OOF meta-features\n* Procedure:\n* Split*train*into two parts:*trainA*and*trainB*.\n* Fit models to*trainA*and predict for*trainB*(getting*trainB\\_meta*).\n* Obtain OOF predictions for*trainA\\_meta*.\n* Fit a meta-model to*trainA\\_meta*and validate on*trainB\\_meta*.\n* Obtain OOF predictions for*train\\_meta*and*test\\_meta*.\n* Fit the meta-model to*train\\_meta*and predict for*test\\_meta*.\n* Fair validation scheme.#### [](#kfold-scheme-with-oof-meta-features)KFold scheme with OOF meta-features\n* The same as holdout scheme with OOF meta-features but with \\\\(K\\\\) folds instead of*trainA*and*trainB*.\n* This scheme gives the validation score with the least variance compared to the other schemes.\n* But it is also the least efficient one from the computational perspective.\n* Fair validation scheme.#### [](#kfold-scheme-in-time-series)KFold scheme in time series\n* Procedure:\n* Obtain OOF meta-features using time-series split starting with \\\\(M\\\\) chunks.\n* Now we have meta-features for the chunks starting from \\\\(M+1\\\\) (getting*train\\_meta*).\n* Fit the models on*train*and predict for*test*(getting*test\\_meta*).\n* Perform time-series aware cross validation on meta-features.\n* Fit the meta-model t...",
      "url": "https://polakowo.io/datadocs/docs/machine-learning/validation-schemes"
    }
  ]
}