{
  "query": "How do top Kaggle competitors handle distribution shift when test data has fundamentally different characteristics than training data in chemistry prediction competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle teams treat a chemistry competition\u2019s distribution shift as a\u202f*domain\u2011adaptation* problem and deliberately build models that can bridge the gap between the training (in\u2011distribution) compounds and the out\u2011of\u2011distribution test set\u202f([arXiv\u202f2506.11877](https://arxiv.org/abs/2506.11877); [arXiv\u202f2503.08674v2](https://arxiv.org/html/2503.08674v2)).\n\n**Typical workflow (5\u20117 essential steps)**  \n\n1. **Analyze the shift** \u2013 compare feature statistics (e.g., molecular fingerprints, scaffold frequencies) between train and test splits to identify covariate\u2011shift patterns.  \n2. **Enrich the training pool** \u2013 add publicly available unlabeled molecules that resemble the test chemistry and use them for self\u2011supervised pre\u2011training or pseudo\u2011labeling.  \n3. **Meta\u2011learn to interpolate** \u2013 train a meta\u2011learning model that explicitly learns how to adapt from in\u2011distribution (ID) to out\u2011of\u2011distribution (OOD) data, using the unlabeled pool as a bridge. This lets the model \u201clearn to generalize\u201d beyond the original training chemistry\u202f([arXiv\u202f2506.11877](https://arxiv.org/abs/2506.11877)).  \n4. **Apply data\u2011centric augmentation** \u2013 generate scaffold\u2011aware augmentations (e.g., fragment recombination, conformer sampling) to densify scarce labeled data and reduce the gap between ID and OOD samples\u202f([arXiv\u202f2506.11877](https://arxiv.org/abs/2506.11877)).  \n5. **Use uncertainty\u2011aware predictions** \u2013 equip the model with Bayesian or ensemble\u2011based uncertainty estimates; high\u2011uncertainty predictions can be down\u2011weighted or flagged for post\u2011hoc correction.  \n6. **Blend multiple robust models** \u2013 ensemble diverse architectures (graph neural nets, transformer\u2011based encoders, physics\u2011informed force\u2011field models) that have been trained with the above adaptation steps; ensembling smooths out shift\u2011specific errors\u202f([arXiv\u202f2503.08674v2](https://arxiv.org/html/2503.08674v2)).  \n7. **Validate on a held\u2011out OOD slice** \u2013 before final submission, create a synthetic OOD validation set that mimics the test distribution and tune hyper\u2011parameters (regularization strength, learning\u2011rate schedules) to maximize performance on this slice.\n\nFollowing this pipeline lets competitors mitigate covariate shift, improve out\u2011of\u2011distribution generalization, and achieve higher leaderboard scores in chemistry prediction challenges.",
      "url": ""
    },
    {
      "title": "Understanding and Mitigating Distribution Shifts \n For Machine Learning Force Fields",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2503.08674v2"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "How to win your first Kaggle competition? - dataroots",
      "text": "How to win your first Kaggle competition?\n[![symbol](https://dataroots.io/_next/static/media/symbol-rainbow.66f0e23b.svg)](https://dataroots.io/)\n![dataroots hero](https://dataroots.io/_next/static/media/glow-bottom-green.eb20c0f6.svg)\n# How to win your first Kaggle competition?\n[Get In Touch-&gt;](https://dataroots.io/contact-us)\n[Careers](https://dataroots.io/careers)\n[DNAOur DNA](https://dataroots.io/our-dna)\n[file-articleBlog](https://dataroots.io/blog)\n[podcastPodcast](https://dataroots.io/datatopics)\nByAdrien Debray, Johannes Lootens\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n## All you need to know about Kaggle competitions\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled-1.png)Kaggle competition overview page\n\ud83d\udca1Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\nFirstly there are the**getting-started competitions**, such as the[Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io)or[Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io)ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the**community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\nWhat actually tends to attract people to Kaggle are the**competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\nEvery single one of those competitions is defined by a**dataset**and an**evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\nWhile the********************train set********************is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the**public leaderboard test set**which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the**private leaderboard test set.**This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n### Notebooks\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n## How to take the W in a Kaggle competition ?\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\nWe participated in the[Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io)competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n### 1. Have a good understanding of the competition and how to tackle the problem\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n* Read the competition overview and linked resources thoroughly\n* Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n* Check existing literature on approaches that were tried/succeeded in solving this or similar problems### 2. Get inspired by other participants\u2019 work to get started\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled--1--1.png)Go in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\nBased on your readings, choose a clear and simple notebook with a decent LB score as**baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n### 3. Improve your model in an efficient way\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n##### Create datasets for intermediate results / preprocessed data\nSaved preprocessed datasets and trained models will make your results comparis...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    },
    {
      "title": "Kaggle forecasting competitions: An overlooked learning opportunity",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- [Access through\u00a0**your organization**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0169207020301114)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0169207020301114/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-snippets)\n- [References (39)](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-references)\n- [Cited by (178)](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-cited-by)\n\n## [International Journal of Forecasting](https://www.sciencedirect.com/journal/international-journal-of-forecasting)\n\n[Volume 37, Issue 2](https://www.sciencedirect.com/journal/international-journal-of-forecasting/vol/37/issue/2), April\u2013June 2021, Pages 587-603\n\n# Kaggle forecasting competitions: An overlooked learning opportunity [\u2606](https://www.sciencedirect.com/www.sciencedirect.com\\#aep-article-footnote-id1)\n\nAuthor links open overlay panelCasper SolheimBojer, Jens PederMeldgaard\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.ijforecast.2020.07.007](https://doi.org/10.1016/j.ijforecast.2020.07.007) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0169207020301114&orderBeanReset=true)\n\n## Abstract\n\nWe review the results of six forecasting competitions based on the online [data science](https://www.sciencedirect.com/topics/social-sciences/data-science) platform Kaggle, which have been largely overlooked by the forecasting community. In contrast to the M competitions, the competitions reviewed in this study feature daily and weekly [time series](https://www.sciencedirect.com/topics/social-sciences/time-series) with exogenous variables, business hierarchy information, or both. Furthermore, the Kaggle data sets all exhibit higher entropy than the M3 and M4 competitions, and they are intermittent.\n\nIn this review, we confirm the conclusion of the M4 competition that ensemble models using cross-learning tend to outperform local [time series](https://www.sciencedirect.com/topics/economics-econometrics-and-finance/time-series) models and that gradient boosted decision trees and [neural networks](https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/neural-network) are strong forecast methods. Moreover, we present insights regarding the use of external information and validation strategies, and discuss the impacts of data characteristics on the choice of statistics or machine learning methods. Based on these insights, we construct nine ex-ante hypotheses for the outcome of the M5 competition to allow empirical validation of our findings.\n\n## Introduction\n\nForecasting is concerned with accurately predicting the future and it provides critical inputs for many planning processes in business, such as financial planning, inventory management, and capacity planning. There has been considerable interest in both industry and academia in the development of methods that are capable of accurate and reliable forecasting, and many new methods have been proposed each year. In forecasting competitions, methods are compared and evaluated empirically based on a variety of time series, and they are widely considered the standard by the forecasting community because they evaluate forecasts constructed ex-ante and they are consistent with real-life forecasting settings (Hyndman, 2020).\n\nMany forecasting competitions have been held by the forecasting community during the past 50\u00a0years, but the M competitions have attracted the most attention. The most recent was the M4 competition, which attempted to test new methods developed in the past 20\u00a0years and it also addressed criticisms regarding the design of previous competitions by including more data frequencies, as well as evaluating prediction intervals and using error measures with better statistical properties (Petropoulos & Makridakis, 2020).\n\nDespite these improvements to the competition\u2019s design, the relevance of the findings obtained from the M4 competition for the business forecasting domain have been subject to criticism. In particular, practitioners have questioned the representativeness of the data set used in the competition, which they argued is not representative of many of the forecasting tasks conducted by business organizations (Darin and Stellwagen, 2020, Fry and Brundage, 2020). The main criticisms concern the underrepresentation of high-frequency series at weekly, daily, and sub-daily levels, and the lack of access to valuable information external1 to the time series, such as exogenous variables and the business hierarchy.\n\nThe organizers of the M4 competition have acknowledged both criticisms (Makridakis et al., 2020b), and thus further research is still required regarding the relative performance of methods for forecasting higher frequency business time series with access to external information. To facilitate this research, the M5 competition was announced by M Open Forecasting Center (2020), which will be hosted by the online data science platform Kaggle and feature daily time series, as well as including exogenous variables and business hierarchy information. Kaggle is a platform that hosts data science competitions for business problems, recruitment, and academic research purposes. Several forecasting competitions that address real-life high-frequency business forecasting problems with access to external information have already been completed on Kaggle, but the forecasting community has largely overlooked the results obtained.\n\nWe consider that these competitions present a learning opportunity for the forecasting community and that they may foretell the findings of the M5 competition. In the following, to provide an overview of what the forecasting community might learn from forecasting competitions on Kaggle, we:\n\n- \u2022\nIdentify six forecasting competitions featuring daily or weekly time series with access to external information;\n\n- \u2022\nAnalyze the competition data sets and compare them with those used in the M3 and M4 competitions;\n\n- \u2022\nBenchmark the Kaggle solutions to ensure that they add value beyond simple methods;\n\n- \u2022\nReview the six competitions and contrast their findings with those obtained in the M4 competition;\n\n- \u2022\nProvide hypotheses regarding the findings of the M5 competition.\n\n\n## Section snippets\n\n## Background\n\nThe M competitions have been highly influential in the forecasting community because they focused the attention of the community on the empirical accuracy of methods rather than the theoretical properties of models. In addition, the competitions allowed anyone to participate, thereby enabling contestants with different preferences and skill sets to use their favorite models. The openness of the competitions facilitated fairer comparisons of methods and tapped into the diverse modeling\n\n## Analysis of competitions\n\nInitially, we examined the database of competitions from the online data science platform Kaggle, and we only retained the competitions focused on forecasting for further consideration, which resulted in nine forecasting competitions in the history of the platform. We decided to exclude the two earliest competitions comprising _Tourism Forecasting_ and _GEFCOM 2012_ because they were academically hosted competitions with published results and analyses (Athanasopoulos et al., 2011, Hong et al., 2014\n\n## Competition review\n\nTo conduct the review, we read through the Kaggle forum posts for each of the competitions. We gathered ...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0169207020301114"
    },
    {
      "title": "Best Practices for Data Science Competitions in Education",
      "text": "[Skip to content](https://the-learning-agency.com/the-learning-agency.com#content)\n\n[Back to Guides & Resources](https://the-learning-agency.com/guides-and-resources)\n\n- Guides & Resources\n\n## Best Practices for Data Science Competitions in Education\n\n# How To Structure and Run A Data Science Competition\n\nData science competitions are an effective and efficient way to crowdsource innovative artificial intelligence (AI) solutions to pressing problems in education. However, successful competitions require planning and preparation. This report outlines best practices for competition management and key decision points on how to create and launch smoothly running competitions that will result in innovative, high-quality solutions. These elements were identified and outlined in consultation with experts from the field.\n\nCompetition organizers should carefully plan and execute their challenges to create a smooth process that will yield quality results. Specifically, competition organizers should:\n\n- Select or build a robust, novel, diverse dataset to enhance competition outcomes. For instance, when aiming to develop an AI essay detection algorithm, the dataset should contain both AI and human writing, including demographic diversity to assess potential bias. Additionally, organizers should differentiate the dataset by its size or by eliminating spelling errors to prevent easy human identification. By investing in dataset curation, organizers can help ensure the competition\u2019s results will be generalizable, accurate, and innovative.\n- Design the competition and its metrics to reflect the project\u2019s goals. While there are many formats and adaptations for competitions, organizers must find the one that will work best for their dataset. For example, when creating a free public resource using a large, text-based dataset, an open competition on a popular data science competition platform may be the best choice. However, when utilizing a small unusual dataset \u2013 such as one based on audio or visual clips \u2013 recruiting specialists to a private competition might spur more dedicated and targeted results.\n- Create a collaborative environment with clear, consistent communication and guidelines for competitors and other stakeholders. For example, detailed rules and metrics provide potential participants the ability to understand the tasks, timeline, and parameters for the competition. This allows them to jump into task-related problem-solving and increase participants\u2019 engagement in the competition. Furthermore, competitions that use communication tactics like discussion boards can also prevent competitor burnout by providing a place where people can collaborate on tough problems and alleviate their frustration when they encounter issues or roadblocks. Overall, strong communication can help participants persist through difficult tasks or when they are not on the leaderboard.\n- Prepare for cost considerations of data and competitions. For example, the size of datasets and the complexity of a task can greatly impact the overall costs. Awareness of the general costs prevents mistakes in planning and preparation before projects even begin.\n\n# What Is A Data Science Competition?\n\nTechnology is advancing so quickly that systems are hard-pressed to keep up or adapt. Half of experts said that [human-level AI will exist before 2061, and 90 percent said it would happen within the next 100 years](https://ourworldindata.org/ai-timelines). The shifts are both exciting and daunting. The latest advancements in artificial intelligence have revolutionized industries like big tech and health care, but others, like education, have been slower to adapt. In this evolving landscape, aligning educational strategies with AI advancements is becoming even more paramount to improve education and learning. Data science competitions can act as a catalyst for bringing AI tools into the classroom. This report outlines how to maximize the value of data science competitions for education.\n\nData science competitions can crowdsource innovative artificial intelligence solutions to pressing educational needs. However, to achieve this, competitions require foresight, planning, and preparation.\n\nEducational data science competitions have the potential to create a lasting impact in the education field and can pioneer innovation in student- and teacher-facing tools. They can attract critical talent to education, establish new benchmarks for developing and evaluating machine learning models, and help democratize access to AI algorithms. They promote transparency in AI by creating an open-source community for sharing innovations.\n\n> Educational data science competitions have the potential to create a lasting impact in the education field and can pioneer innovation in student- and teacher-facing tools. They can attract critical talent to education, establish new benchmarks for developing and evaluating machine learning models, and help democratize access to AI algorithms.\n\nTake, for example, [the Automated State Assessment Prize (ASAP)](https://www.kaggle.com/competitions/asap-aes). Launched with the support of [the Hewlett Foundation](https://hewlett.org/) 14 years ago, the ASAP dataset helped launch the practice of automated essay scoring, and even today, the researchers and technologists actively discuss on message boards how they could better parse the data.\n\nLast year, [The Learning Agency Lab](https://the-learning-agency-lab.com/) along with Georgia State University and Vanderbilt University launched five competitions that engaged over 8,000 data scientists, with the winning algorithms matching human-level performance. These algorithms ranged in tasks from evaluating student-written summaries to predicting student performance in game-based learning environments. These public, open-source solutions can then be adopted by popular edtech platforms, showing how data science competitions are not just interesting research projects or thought experiments \u2013 they can have a real, lasting impact on students and teachers.\n\n# Best Practices For Running A Data Science Competition\n\nThe various best practices of competition management can be categorized into three sections: choosing a dataset, designing the competition, and competition communications.\n\n### Section 1: Select or build a robust, novel, diverse dataset to enhance competition outcomes.\n\nDatasets are the backbone of any data science competition, and the resulting algorithms areonly as good as the datasets they are trained on. There are many variables to consider when identifying whether a dataset is fit for a data science competition. To begin, organizers should pay particular attention to their dataset\u2019s novelty or uniqueness, its size, demographic representation, and the potential impact of the data. Importantly, organizers must assess whether the data can be used to complete the task that the competition is built around.\n\nSecondly, organizers should anticipate that participants who can quickly and easily understand the data will find the competition more enjoyable and productive. Additional considerations such as the sharing terms of the data, the train/test set split, and methodology explanations are important for framing a competition and for aligning the data to suit the desired outcome.\n\nWe identified eight important elements to consider when designing a competition dataset:\n\n1. Uniqueness of Data Collection\n2. Size\n3. Demographic Information\n4. Potential Impact\n5. Viable Competition Task\n6. Ethical Data Collection\n7. Building a Test Set\n8. Data Sharing Terms\n\n### Uniqueness of Data Collection\n\nThe dataset should contain information on a novel subject matter or have a level of detail that has not been previously collected. Novel and granular datasets enable contestants to develop innovative models that could not be created otherwise.\n\n### Size\n\nDatasets used in competitions to produce machine-learning models must be large enough to train, build, and evalua...",
      "url": "https://the-learning-agency.com/guides-resources/best-practices-for-data-science-competitions-in-education"
    },
    {
      "title": "Kaggle Winning Solutions: AI Trends & Insights",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/tahaalselwii/kaggle-winning-solutions-ai-trends-insights"
    },
    {
      "title": "[PDF] Learnings from Kaggle's Forecasting Competitions - arXiv",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/pdf/2009.07701"
    },
    {
      "title": "Winning solutions of kaggle competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions"
    }
  ]
}