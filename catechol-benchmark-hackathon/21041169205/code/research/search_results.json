{
  "query": "What is the exact submission format required for the Catechol Benchmark Hackathon NeurIPS 2025 Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The provided sources do not contain the specific submission\u2011file format (e.g., required columns, file type, naming convention) for the Catechol Benchmark Hackathon\u202fNeurIPS\u202f2025 Kaggle competition. ([arXiv\u202f2506.07619](https://arxiv.org/abs/2506.07619), [NeurIPS\u202f2025 Data Hosting Guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines))",
      "url": ""
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "NeurIPS 2025 Data Hosting Guidelines",
      "text": "NeurIPS 2025 Data Hosting Guidelines\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/accounts/login)\n## **Instruction Guide: Datasets &amp; Benchmarks Track Dataset Hosting Platforms**\nThis webpage serves as an instruction guide for authors making a submission to the Datasets &amp; Benchmarks Track CFP. At a high level, authors of datasets will be required to make their dataset available along with Croissant machine-readable metadata in order to streamline the review process and meet industry standards for high quality documentation, reproducibility, and accessibility. As a part of these new requirements, we are recommending use of preferred hosting platforms\u2013Dataverse, Kaggle, Hugging Face, and OpenML\u2013which will make compliance simple for authors and reviewers. For more detailed information regarding the rationale for submission requirements new in 2025, please read our blog post.\nThe dataset hosting process as part of submitting to the Datasets &amp; Benchmarks Track involves:\n1. Choosing among 4 options to host your dataset: Harvard Dataverse, Kaggle, Hugging Face, and OpenML\n2. Using platform tooling to download the automatically generated Croissant file\n3. Including a URL to your dataset and uploading the generated Croissant file in Open Review\n4. If your submission is accepted: making your dataset public by the camera ready deadline\n## **Choose a Preferred Hosting Platforms**\nHarvard Dataverse, Kaggle, Hugging Face, and OpenML platforms are the preferred hosting platforms for datasets. These platforms automatically generate a Croissant file and allow for programmatic metadata verification and dataset assessment which will streamline and standardize the review process.**When you make your dataset accessible via one of these platforms, making a submission will be as simple as providing a URL to the dataset and uploading a generated Croissant file**.\nThe table below outlines key platform features to help authors choose where to host their dataset. Authors may make their dataset accessible via more than one platform at any time.\n|\n**Automatically generated Croissant file**\n|\n**Client libraries: Croissant download, data download, data loader**\n|\n**Hosting restrictions**\n|\n**Private preview URL access**\n|\n**Credentialized (gated) access**\n|\n**Paper Linking**\n|\n**DOIs**\n|\n**Harvard Dataverse**\n|\n\u2705|\n\u2705\u2705\u2705|\n1TB per dataset (2.5GB per file)\nAny file types\n|\n\u2705|\n\u2705|\n\u2705|\n\u2705|\n**Kaggle**\n|\n\u2705|\n\u2705\u2705\u2705|\n200GB per dataset\nAny file types\n|\n\u2705|\n\u274c|\n\u274c|\n\u2705|\n**Hugging Face**\n|\n\u2705|\n\u2705\u2705\u2705|\n300GB per dataset public\nAny file types\n|\n\u274c|\n\u2705|\n\u2705|\n\u2705|\n**OpenML**\n|\n\u2705|\n\u2705\u2705\u2705|\n200GB per dataset\nAny file types\n|\n\u274c|\n\u274c|\n\u2705|\n\u274c|\nAuthors are responsible for reviewing and complying with the Terms of Service of the platform(s) they choose to use.\n## **Alternatives**\n#### **Self-hosting your Dataset and Other Data Storage Platforms**\nIf you choose NOT to release your dataset via one of these preferred platforms, you can self-host the data or use other platforms, but you will still be required to make your dataset accessible to reviewers via URL (e.g., to a GitHub repo, public cloud bucket, Zenodo, etc.) and manually generate and upload a Croissant file representation of your dataset as part of the Open Review submission process.\n#### **Generating a Croissant File**\nThis[Python tutorial](https://colab.sandbox.google.com/github/mlcommons/croissant/blob/main/python/mlcroissant/recipes/introduction.ipynb)demonstrates how to generate a Croissant file. Find more documentation on the[Croissant GitHub repository](https://github.com/mlcommons/croissant).\nYou can also try the[Croissant editor](https://huggingface.co/spaces/MLCommons/croissant-editor)or run it locally ([GitHub](https://github.com/mlcommons/croissant/tree/main/editor)).\n## **How to Publish on Preferred Hosting Platforms**\nThis section provides specific guidance and documentation on how to make your dataset and its Croissant metadata file accessible via the preferred hosting platforms:[Harvard Dataverse](https://docs.google.com/document/d/1_bWYjvOJp4Wx5EFJJJ8ploOkoeFkDdR7OtXgNx7hQns/edit?tab=t.g0yr7okr429#heading=h.cje8hg537cbq),[Kaggle](https://docs.google.com/document/d/1_bWYjvOJp4Wx5EFJJJ8ploOkoeFkDdR7OtXgNx7hQns/edit?tab=t.g0yr7okr429#heading=h.w14fyk84wy4n),[Hugging Face](https://docs.google.com/document/d/1_bWYjvOJp4Wx5EFJJJ8ploOkoeFkDdR7OtXgNx7hQns/edit?tab=t.g0yr7okr429#heading=h.w94uvwqu0dl8), and[OpenML](https://docs.google.com/document/d/1_bWYjvOJp4Wx5EFJJJ8ploOkoeFkDdR7OtXgNx7hQns/edit?tab=t.g0yr7okr429#heading=h.xfpvhvh8u33x).\n|\n**How to upload**\n|\n**How to download (files, Croissant)**\n|\n**How to get help, e.g., to request additional storage quota**\n|\n**Harvard Dataverse**\n|\n[Upload a Dataset via UI](https://dataverse.harvard.edu/dataset.xhtml?ownerId=1)(after login) or[CLI](https://guides.dataverse.org/en/6.5/user/dataset-management.html))\nRequirements\n* An email-verified Harvard Dataverse user account\n* Publicly shared (or \u201cLink Sharing\u201d turned on in the \u201cEdit\u201d menu\u00a0 to generate a Preview URL for a private dataset) at time of submission to D&amp;B track\nPlatform restrictions\n* [Documentation](https://guides.dataverse.org/en/6.5/user/dataset-management.html#)\n* [1TB per dataset (2.5GB per file)](https://support.dataverse.harvard.edu/researchers)|\nDownload files\n* Click \u201cAccess Dataset\u201d and choose an option\nDownload Croissant\n* Click \u201cMetadata\u201d, Click \u201cExport Metadata\u201d, and select \u201cCroissant\u201d\n* [Via Python client](https://guides.dataverse.org/en/6.5/api/native-api.html#export-dataset-metadata-api)|\nContact[support@dataverse.harvard.edu](mailto:support@dataverse.harvard.edu)\n|\n**Kaggle**\n|\n[Upload a Dataset via UI](https://www.kaggle.com/datasets?new=true)(after login) or[Python client](https://github.com/Kaggle/kagglehub?tab=readme-ov-file#upload-dataset)\nRequirements\n* A phone-verified Kaggle user account\n* Publicly shared (or \u201cLink Sharing\u201d turned on in \u201cSettings\u201d\u00a0 to generate a Preview URL for a private dataset) at time of submission to D&amp;B track\n* (Optional) An[approved Organization profile](https://www.kaggle.com/contact#/organizations/request-creation)to host the data under if preferred, e.g., your research lab\nPlatform restrictions\n* [Documentation](https://www.kaggle.com/docs/datasets#technical-specifications)\n* 200GB per dataset|\nDownload files\n* Click \u201cDownload\u201d and choose \u201cDownload dataset as zip\u201d\n* Via[kagglehub Python client](https://github.com/Kaggle/kagglehub?tab=readme-ov-file#download-dataset)\nDownload Croissant\n* Click \u201cDownload\u201d and choose \u201cExport metadata as Croissant\u201d\n* Click \u201cCode\u201d, select \u201cLoad via mlcroissant\u201d, and copy Python code|\nContact[kaggle-datasets@kaggle.com](mailto:kaggle-datasets@kaggle.com)\n|\n**Hugging Face**\n|\nUpload a Dataset via UI (after login) or Python client\nRequirements\n* A Hugging Face user account\n* Publicly shared at time of submission to D&amp;B track\n* Must be a[format listed here](https://huggingface.co/docs/hub/en/datasets-adding#file-formats)in order to generate a Croissant fi...",
      "url": "https://neurips.cc/Conferences/2025/DataHostingGuidelines"
    },
    {
      "title": "NeurIPS 2025 San Diego Datasets & Benchmarks",
      "text": "NeurIPS 2025 San Diego Datasets &amp; BenchmarksNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/events/datasets-benchmarks-2025)\n# San Diego Datasets &amp; Benchmarks\n**494 Events\n**\nPoster### [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://neurips.cc/virtual/2025/poster/121603)\nKuicai Dong &middot; CHANG YUJING &middot; Shijie Huang &middot; Yasheng Wang &middot; Ruiming Tang &middot; Yong Liu\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nDocument Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration. Key findings reveal that advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121603)\nPoster### [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://neurips.cc/virtual/2025/poster/121386)\nHyungyung Lee &middot; Geon Choi &middot; Jung-Oh Lee &middot; Hangyul Yoon &middot; Hyuk Hong &middot; Edward Choi\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nRecent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121386)\nPoster### [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://neurips.cc/virtual/2025/poster/121553)\nAtharva Gundawar &middot; Som Sagar &middot; Ransalu Senanayake\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nVision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that is largely unverified. To perform actions reliably, robots must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state like being closed). Despite their ubiquitous use in manipulation, we argue that off-the-shelf VLMs may lack this granular, physically-grounded understanding, as these specific prerequisites are often overlooked during training. Addressing this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of these core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with more than 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, 1\u20133 affordances defined per object class), 100 real-world humanoid view scenarios, and 120 unique simulated constraint scenarios across four tasks. Our evaluations reveal significant gaps in the ability of VLMs to grasp fundamental physical concepts, underscoring their current limitations for reliable robot manipulation and pointing to key areas that require targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating the physical reasoning capabilities of VLMs guiding the development of more robust and physically grounded models for robot manipulation.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121553)\nPoster### [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://neurips.cc/virtual/2025/poster/121706)\nXinran Wang &middot; Songyu Xu &middot; Shan Xiangxuan &middot; Yuxuan Zhang &middot; Muxi Diao &middot; Xueyan Duan &middot; Yanhua huang &middot; Kongming Liang &middot; Zhanyu Ma\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nCinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and repro...",
      "url": "https://neurips.cc/virtual/2025/loc/san-diego/events/datasets-benchmarks-2025"
    },
    {
      "title": "UK Dementias Platform - UKRI's Gateway",
      "text": "GtR\n\ud83d\udce3Help Shape the Future of UKRI's Gateway to Research (GtR)\nWe're improving UKRI's Gateway to Research and are seeking your input!\rIf you would be interested in being interviewed about the improvements\rwe're making and to have your say about how we can make GtR more user-friendly,\rimpactful, and effective for the Research and Innovation community,\rplease email[gateway@ukri.org](mailto:gateway@ukri.org).\n# UK Dementias Platform\nLead Research Organisation:[University of Oxford](https://gtr.ukri.org/organisation/AD55AA55-BC5D-4243-BD1F-ADA38F21963C)\nDepartment Name: Psychiatry\n* [Overview](#tabOverview)\n* Organisations\n* People\n* Publications\n* Outcomes\n* Related Projects\n### Abstract\n[Funding\ndetails](#)\nThe UK Dementias Platform (UKDP) is a radically new approach to dementias research, bringing together data from around 2,000,000 study participants from 22 cohorts to try and discover the causes of dementia and to find out ways of slowing it down. The platform has been funded to the level of &pound;4.5M and invited to apply for a further &pound;7.5M.\nTo achieve our goals of discovering the causes of dementia and finding out ways to slow it down, we will analyse data collected over many decades from throughout the UK to identify targets for drugs studies.\nWe will also enrich strategically selected cohorts to provide resources for dementias research in general. This will involve finding out how the early dementia processes begin, how our genes affect our risk of dementia and where our body goes wrong in maintaining and repairing itself.\nThere are many practical challenges involved in doing this work including knowing how to assess dementia in large studies where having everyone examined by a doctor would be prohibitively expensive. Another challenge is how to measure cognitive decline in the population as when people begin to decline they are frequently less motivated to do cognitive tests.\nThere are also important ethical issues surrounding this work and we have specific projects looking at ethical legal and social issues surrounding taking part in detailed measurement studies as well as in considering brain donation.\nTo help make an impact as soon as possible, we will work closely with industry partners who are best placed to turn basic scientific knowledge into effective therapies.\nEven if we cannot cure dementia in the short term, it would count as a success if we found ways of slowing it down so that nobody need lose their dignity as they grew older.\n### Technical Summary\nThe UK Dementias Platform (UKDP) is a radically new approach to dementias research, providing a highly efficient and cost-effective translation pipeline from basic discovery through to early phase trials (Fig 1). By combining the analytic and statistical power of &gt;20 clinical and population cohorts representing 2M participants and creating a large readiness cohort, UKDP will create an optimal environment for basic discovery activities relevant to the clinical progression and human impact of dementia. Although UKDP is intended as a strategic and sustainable resource generating increasing scientific benefit over time, it will also deliver short and medium term benefits through the analysis of existing data, the enrichment of strategically selected cohorts, developing new ways of working with industry, and generating interest and further funding for dementias research generally.\nA single portal informatics hub (funded in stage 1) will be used for integrating data from participating cohorts and triangulating evidence between cohorts. The stage 2 proposal focuses on providing strategic resources for EM by enriching strategically selected cohorts, addressing key methodological issues and pump-priming a programme of EM studies.\nFurther development of the platform is anticipated as the opportunities it provides are more widely understood. For example, predicated on the ability of the platform's ability to provide accurately risk stratification, MRC and NIHR have recently funded a deep and frequent Phenotyping (DFP) feasibility study with a view to providing a further &pound;5M for the full study. The DFP project may be considered a stage 3 development of UKDP and is an example of how UKDP can be used to leverage further resource for dementias research.\n### Planned Impact\nThe platform will use several vehicles to deliver impact.\nThese were described in stage 1.\nIn summary:\nFor the academic community, in addition to the proposals mad in stage 1, we will develop a network of partnerships and operate a system of working groups to actively consult the UK academic community and world experts on the direction, technologies and collaborations of the platform. Our aim is to 1) base the science of the platform on the best information available, 2) create momentum in dementias research by being inclusive of, and synergistic with, other initiatives.\nFor industry, partnership discussions are already underway with exchanges of ides, interests and needs between academic and industry stakeholders. Industry have identified their need for access to conversion (early MVI to dementia cohorts and for experimental medicine studies to conform to regulatory requirements.\nWe remain committed to raising the profile of contemporary debate about dementia and its treatment. We wish to encourage a culture of commitment to solving this problem. By increasing awareness at all levels of society we intend to leverage resources for the platform and for dementia research in general, to increase awareness of the need for earlier interventions and better targeted treatment in general by health service providers and the public alike.\nEngagement with the general public and with patients and carers is a very important part of our mission. This serves not only to communicate our research findings and their relevance but also to address such issues as stigma in society and the research culture in the NHS in relation to dementia and older people. In addition to using the platform web-site to communicate to the general public, we will also liaise with charities and advocate groups such as Age UK and the Alzheimer's Society to promote our work and findings and to engage them in shaping the work programme.\nIn addition we will have a dedicated free-phone number available 6 days a week and a communications officer at 50% FTE over 5 years whose responsibility is to develop and implement a communications and public engagement strategy.\n### Funded Value:\n**&pound;15,256,623**### Funded Period:\nAug 15\r-\rJul 23### Funder:\nMRC\n### Project Status:\nClosed\n### Project Category:\nResearch Grant### Project Reference:\nMR/L023784/2\n### Principal Investigator:\n[John Gallacher](https://gtr.ukri.org/person/C5A8CFA4-A312-4718-8080-ABD32DA6CFA9/)\n### Health Category:\n[Unclassified](https://gtr.ukri.org/resources/classificationprojects.html?id=6CFA1E1F-F25C-4C23-8FE1-C47AE53E333E&type=Health_Category&text=Unclassified)\n### Organisations\n* [University of Oxford (Collaboration, Lead Research Organisation)](https://gtr.ukri.org/organisation/AD55AA55-BC5D-4243-BD1F-ADA38F21963C)\n* [Medical Research Council (MRC) (Collaboration)](https://gtr.ukri.org/organisation/E1CB4FE5-0A23-42E8-9D4B-15D06D7CC5D6)\n* [University of Milan (Collaboration)](https://gtr.ukri.org/organisation/447869BE-0C68-401F-9654-5486EC1C0EE8)\n* [National Institute on Aging (Collaboration)](https://gtr.ukri.org/organisation/10894923-37CC-4B64-997B-96B30D9C32C9)\n* [Ferrer Internacional (Collaboration)](https://gtr.ukri.org/organisation/9F119AAA-2BFD-4636-96E3-1599F2F7EBA1)\n* [Progressive Supranuclear Palsy Association (PSPA) (Collaboration)](https://gtr.ukri.org/organisation/2E21E2ED-26D7-4A20-8368-A0B9D021959B)\n* [Catalan Health Institute (ICS) (Collaboration)](https://gtr.ukri.org/organisation/B90A37AF-FE06-4167-BA94-ACFEA23A5706)\n* [University of California, San Francisco (Collaboration)](https://gtr.ukri.org/organisation/BCC840D5-8325-4449-A757-E10E90D36C27)\n* [UK Dementia Research In...",
      "url": "https://gtr.ukri.org/projects?pn=0&fetchSize=10&selectedSortableField=date&selectedSortOrder=ASC"
    },
    {
      "title": "NeurIPS 2025 Datasets & Benchmarks Track Call for Papers",
      "text": "Call For Datasets &amp; Benchmarks 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/accounts/login)\n# **NeurIPS 2025 Datasets &amp; Benchmarks Track Call for Papers**\nThe**NeurIPS Datasets and Benchmarks track**serves as a venue for high-quality publications on highly valuable machine learning datasets and benchmarks crucial for the development and continuous improvement of machine learning methods. Previous editions of the Datasets and Benchmarks track were highly successful and continuously growing (accepted papers[2021](https://proceedings.neurips.cc/paper_files/paper/2021),[2002](https://proceedings.neurips.cc/paper_files/paper/2022), and[2023](https://proceedings.neurips.cc/paper_files/paper/2023), and best paper awards[2021](https://blog.neurips.cc/2021/11/30/announcing-the-neurips-2021-award-recipients/),[2022](https://blog.neurips.cc/2022/11/21/announcing-the-neurips-2022-awards/),[2023](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)and[2024](https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/). Read our[original blog post](https://neuripsconf.medium.com/announcing-the-neurips-2021-datasets-and-benchmarks-track-644e27c1e66c)for more about why we started this track, and the 2025[blog post](https://blog.neurips.cc/2025/03/10/neurips-datasets-benchmarks-raising-the-bar-for-dataset-submissions/)announcing this year's track updates.\n**Dates and Guidelines**\nPlease note that the Call for Papers of the NeurIPS2025 Datasets &amp; Benchmarks Track this year**will follow the**[**Call for Papers of the NeurIPS2025 Main Track**](https://neurips.cc/Conferences/2025/CallForPapers)**, with the addition of three track-specific points:**\n* Single-blind submissions\n* Required dataset and benchmark code submission\n* Specific scope for datasets and benchmarks paper submission\nThe dates are also identical to the main track:\n* **Abstract submission deadline:**May 11, 2025 AoE\n* **Full paper submission deadline:**May 15, 2025 AoE (all authors must have an OpenReview profile when submitting)\n* **Technical appendices and supplemental materials deadline:**May 22, 2025 AoE\n* **Author notification:**Sep 18, 2025 AoE\n* **Camera-ready:**Oct 23, 2025 AoE\nAccepted papers will be published in the NeurIPS proceedings and presented at the conference alongside the main track papers. As such, we aim for an equally stringent review as in the main conference track, while also allowing for**track-specific guidelines**, which we introduce below. For details on everything else, e.g. formatting, code of conduct, ethics review, important dates, and any other submission related topics, please refer to the[main track CFP.](https://neurips.cc/Conferences/2025/CallForPapers)\n**OpenReview**\nSubmit at:[https://openreview.net/group?id=NeurIPS.cc/2025/Datasets\\_and\\_Benchmarks\\_Track](https://openreview.net/group?id=NeurIPS.cc/2025/Datasets_and_Benchmarks_Track#tab-your-consoles)\nThe site will start accepting submissions on April 3, 2025 (at the same time as the main track).\n**Note:**submissions meant for the main track should be submitted to a different OpenReview portal, as shown[here](https://neurips.cc/Conferences/2025/CallForPapers). Papers will not be transferred between the main and the Datasets and Benchmarks tracks after the submission is closed.\n## **DB track specifics**\nThe Datasets and Benchmarks track adds**three changes in the submission and review guidelines that are better suited for the review of datasets and benchmarks**. Below we briefly introduce them. For more details on how to host datasets, what metadata to use to describe them and what access requirements we have, please consult[these guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines).\n1. SINGLE-BLIND SUBMISSIONS\nDatasets are often not possible to be reviewed in a double-blind fashion, and hence full anonymization will not be required for D&amp;B paper submissions. Submissions to this track will be**reviewed according to a set of criteria and best practices specifically designed for datasets and benchmarks**, as described below.**Authors can choose to submit either single-blind or double-blind**. If it is possible to properly review the submission double-blind, i.e., if reviewers do not need access to non-anonymous repositories to review the work, then authors can also choose to submit the work anonymously.\n**Note:**NeurIPS does not tolerate any collusion whereby authors secretly cooperate with reviewers, ACs, or SACs to obtain favorable reviews.\n2. REQUIRED DATASET AND BENCHMARK CODE SUBMISSION\nThis year,**we are introducing a more stringent set of criteria in the submission process**regarding the hosting, accessibility and documentation of datasets and code**at submission time**:\n* **Hosting:**New****datasets should be hosted at one of the hosting sites dedicated to ML datasets (Dataverse, Kaggle, Hugging Face, or OpenML) or at a bespoke hosting site if the dataset requires it. See the[guidelines for data hosting](https://neurips.cc/Conferences/2025/DataHostingGuidelines)for details. Code should be made accessible in an executable format via a hosting platform (e.g., GitHub, Bitbucket).\u00a0 See the main track[code guidelines](https://neurips.cc/public/guides/CodeSubmissionPolicy)for details.\n* **Access:***Datasets and code should be*[*available and accessible*](https://www.nsf.gov/eng/general/ENG_DMP_Policy.pdf)*to all reviewers, ACs and SACs at the time of submission.*Data should be found and obtained without a personal request to the PI. Code should be documented and executable. Non-compliance justifies the desk rejection of the paper.\n* **Metadata:**Authors should use the[Croissant machine-readable format](https://mlcommons.org/working-groups/data/croissant/)to document their datasets in a machine-readable way and include the Croissant file with their paper submission in OpenReview.\n* **If your data is hosted on one of the dedicated ML data hosting sites**(Kaggle, OpenML, Hugging Face, Dataverse)\n* a Croissant metadata file is automatically generated for you\n* **If you choose to host your data on a different hosting site**\n* you need to generate the Croissant metadata file yourself\n* You can verify the validity of your Croissant file[via this online tool](https://huggingface.co/spaces/JoaquinVanschoren/croissant-checker).\nAll accepted papers should have their code and datasets*documented*and*publicly available by the camera-ready deadline*.\n3. SPECIFIC SCOPE FOR DATASETS &amp; BENCHMARKS PAPER SUBMISSION\nThe NeurIPS 2025 D&amp;B track welcomes all work on*data-centric machine learning research (DMLR)*that enable or accelerate ML research, covering ML datasets and benchmarks as well as algorithms, tools, methods, and analyses for working with ML data. The D&amp;B track is proud to support the open source movement by*encouraging submissions**of open-source libraries and tools*that enable or accelerate ML research.\nThe scope incl...",
      "url": "https://neurips.cc/Conferences/2025/CallForDatasetsBenchmarks"
    },
    {
      "title": "NeurIPS Datasets & Benchmarks: Raising the Bar for Dataset Submissions",
      "text": "NeurIPS Datasets &#038; Benchmarks: Raising the Bar for Dataset Submissions &#8211; NeurIPS Blog[Skip to content](#content)\n[Menu](#mobile-menu)\n[![NeurIPS Blog](https://blog.neurips.cc/wp-content/uploads/neurips-logo-svg.svg)](https://neurips.cc)\n[Search](#)\n[Close Menu](#)\nMarch102025\n# [NeurIPS Datasets &#038; Benchmarks: Raising the Bar for Dataset Submissions](https://blog.neurips.cc/2025/03/10/neurips-datasets-benchmarks-raising-the-bar-for-dataset-submissions/)\n[Communications Chairs 2025](https://blog.neurips.cc/author/alexlu/)[2025 Conference](https://blog.neurips.cc/category/2025-conference/)\nAuthors:\nDB Track chairs: Lora Aroyo, Francesco Locatello, Konstantina Palla,\nDB Track resource and metadata chairs: Meg Risdal, Joaquin Vanschoren\nThe NeurIPS Datasets &amp; Benchmarks Track exists to highlight the crucial role that high-quality datasets and benchmarks play in advancing machine learning research. While algorithmic innovation often takes center stage, the progress of AI depends just as much on the quality, accessibility, and rigor of the datasets that fuel these models. Our goal is to ensure that impactful dataset contributions receive the recognition and scrutiny they deserve.\nThis blog post accompanies the release of the Call for Papers for the 2025 Datasets &amp; Benchmarks Track ([https://neurips.cc/Conferences/2025/CallForDatasetsBenchmarks](https://neurips.cc/Conferences/2025/CallForDatasetsBenchmarks)), outlining key updates to submission requirements and best practices. Please note that*this year Datasets &amp; Benchmarks Track this year will follow the*[*NeurIPS2025 Main Track Call for Papers*](https://neurips.cc/Conferences/2025/CallForPapers)*, with the addition of three track-specific points:*****(1) Single-blind submissions, (2) Required dataset and benchmark code submission and (3) Specific scope for datasets and benchmarks paper submission. See the Call for Papers for details.\n## **The Challenge of Assessing High-Quality Datasets**\nUnlike traditional research papers that have well-established peer review standards, dataset and benchmark papers require unique considerations on their review evaluation. A high-quality dataset must be well-documented, reproducible, and accessible while adhering to best practices in data collection and ethical considerations. Without clear guidelines and automated validation, reviewers face inconsistencies in their assessments, and valuable contributions risk being overlooked.\nTo address these challenges, we have developed submission and review guidelines that align with widely recognized frameworks in the research community and the open-source movement. For instance, in 2024, we encouraged authors to use established documentation standards such as[datasheets for datasets](https://arxiv.org/abs/1803.09010),[dataset nutrition labels](https://arxiv.org/abs/1805.03677),[data statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00041/43452/Data-Statements-for-Natural-Language-Processing),[data cards](https://sites.research.google/datacardsplaybook/), and[accountability frameworks](https://arxiv.org/abs/2010.13561).****By promoting these frameworks, we aim to ensure that dataset contributions are well-documented and transparent, making it easier for researchers to assess their reliability and impact.\n## **Raising the Bar: Machine-Readable Metadata with Croissant**\nA persistent challenge has been the lack of a standardized, reliable way for reviewers to assess datasets against industry best practices. Unlike the main track, which has commonly accepted standards for paper submissions, dataset reviews still have to mature in this respect.\nIn 2024, we took a significant step toward improving dataset review by encouraging authors to generate a[Croissant machine-readable metadata](https://proceedings.neurips.cc/paper_files/paper/2024/hash/9547b09b722f2948ff3ddb5d86002bc0-Abstract-Datasets_and_Benchmarks_Track.html)file to document their datasets. Croissant is an[open community effort](https://mlcommons.org/working-groups/data/croissant/)created because existing standards for dataset metadata lack ML-specific support and lag behind AI\u2019s dynamically evolving requirements. Croissant records ML-specific metadata that enables datasets to be loaded directly into ML frameworks and tools, streamlines usage and community sharing independent of hosting platforms, and includes responsible AI metadata. At that time, Croissant tooling was still in its early stages, and many authors found the process burdensome. Since then, Croissant has matured significantly and gained industry and community adoption. Platforms like Hugging Face, Kaggle, OpenML, and Dataverse now natively support Croissant, making metadata generation effortless.\n## **Making High-Quality Dataset Submissions the Standard**\nWith these improvements in tooling and ecosystem support, we are now**requiring**dataset authors to ensure that their datasets are properly hosted. That means that they release their datasets via a**data repository**(e.g., Hugging Face, Kaggle, OpenML, or Dataverse) or provide a custom hosting solution that supports and ensures long-term access and includes a croissant description. We also provide[detailed guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines)for authors to make the process as smooth as possible. This requirement ensures that:\n* **Datasets are easily accessible and discoverable**through widely used research platforms over long periods of time.\n* **Standard interfaces**(e.g., via Python client libraries) simplify dataset retrieval for both researchers and reviewers.\n* **Metadata is automatically validated**to streamline the review process.\nBy enforcing this requirement, we are lowering the barriers to high-quality dataset documentation while improving the overall transparency and reproducibility of dataset contributions.\n## **Looking Ahead**\nThe NeurIPS Datasets &amp; Benchmarks Track is committed to evolving alongside the broader research community. By integrating best practices and leveraging industry standards like Croissant, we aim to enhance the visibility, impact, and reliability of dataset contributions. These changes will help ensure that machine learning research is built on a foundation of well-documented, high-quality datasets that drive meaningful progress.\nIf you are preparing a dataset submission for NeurIPS, we encourage you to explore Croissant-integrated repositories today and take advantage of the powerful tools available to streamline your metadata generation. Let\u2019s work together to set a new standard for dataset contributions\n[NeurIPS 2024 Experiment on Improving the Paper-Reviewer Assignment](https://blog.neurips.cc/2024/12/12/neurips-2024-experiment-on-improving-the-paper-reviewer-assignment/)[Self-nomination for reviewing at NeurIPS 2025](https://blog.neurips.cc/2025/03/10/self-nomination-for-reviewing-at-neurips-2025/)\n### Related Posts\n[2025 Conference](https://blog.neurips.cc/category/2025-conference/)\n#### [NeurIPS Datasets &amp; Benchmarks Track: From Art to Science in AI Evaluations](https://blog.neurips.cc/2025/12/05/neurips-datasets-benchmarks-track-from-art-to-science-in-ai-evaluations/)\n[2025 Conference](https://blog.neurips.cc/category/2025-conference/)\n#### [Reaffirming our Code of Conduct](https://blog.neurips.cc/2025/11/28/reaffirming-our-code-of-conduct/)\n[2025 Conference](https://blog.neurips.cc/category/2025-conference/)\n#### [Announcing the 2025 Sejnowski-Hinton Prize](https://blog.neurips.cc/2025/11/26/announcing-the-2025-sejnowski-hinton-prize/)\n[Feed](https://blog.neurips.cc/feed)\n#### Recent Posts\n* [NeurIPS 2026 Call for Organizer Nominations](https://blog.neurips.cc/2026/01/07/neurips-2026-call-for-organizer-nominations/)January 7, 2026\n* [Supporting Our Community&#8217;s Infrastructure: NeurIPS Foundation&#8217;s Donation to OpenReview](https://blog.neurips.cc/2025/12/15/supporting-our-communitys-infrastructure-neurips-foundations...",
      "url": "https://blog.neurips.cc/2025/03/10/neurips-datasets-benchmarks-raising-the-bar-for-dataset-submissions"
    },
    {
      "title": "NeurIPS 2025 Datasets and Benchmarks FAQ",
      "text": "NeurIPS 2025 Datasets and Benchmarks FAQ\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/accounts/login)\n## NeurIPS 2025 Datasets and Benchmarks FAQ\nThis FAQ will be continually updated. Please bookmark this page and review it before submitting any questions.\n**Note:**Authors are also advised to consult the[NeurIPS Main Track FAQs](https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ), as general policies apply to D&amp;B submissions as well.\n### General FAQs\n**What is the LaTeX template for D&amp;B track?**\nIt\u2019s the same as the main track template. Check \u201cPaper Formatting Instructions\u201d here (https://neurips.cc/Conferences/2025/CallForPapers)\n**Are there guidelines for submissions which are from the 2024 Competitions track, e.g., reporting on competition results?**\nNo, there are no special guidelines. Please follow the D&amp;B CFP (this page) and data[hosting guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines). Your submission will be reviewed according to the same standards alongside all other D&amp;B track submissions.\n**KDD 2025 notification date is May 16th which is a day after the NeurIPS D&amp;B track full paper submission deadline. Will this change?**\nNo. We will not be making changes to the D&amp;B track deadlines as they align with the main conference.\n**Is my paper a good fit for D&amp;B track?**\nPlease carefully read the CFP (this page) and use your best judgment. Track chairs cannot advise on the relevance of your paper.\n**Are dataset/code submissions due at May 15th (full paper deadline) or May 22nd (appendices/supplementary material deadline)?**\nDatasets/code are not supplementary materials in the D&amp;B track. If your submission includes data/code, it needs to be submitted in full and final form by May 15th along with the full paper submission.\n**What is the LaTeX configuration for a single-blind submission?**\nPlease use \\\\usepackage[preprint]{neurips\\_2025} if you wish to make your submission single-blind for the Datasets &amp;&amp; Benchmarks track.\n**My submission is a benchmark consisting of an environment for evaluation only. Do I need to follow the data-hosting guidelines?**\nNo. If your submission is not a dataset, you do not need to follow data-hosting guidelines. You do need to follow code-hosting guidelines.\n### Dataset hosting FAQs\n**The Croissant format can\u2019t handle the file type(s) in my dataset submission. What should I do?**\nYou should still submit a Croissant file. You can choose to only provide dataset-level metadata and a description of the resources contained in the dataset (FileObject and FileSet). You can omit RecordSets in this scenario. The recommended Croissant-compatible data hosting platforms should handle this gracefully for you, but you will need to manually address this in case you decide to self-host your dataset.\n**I have a submission consisting of multiple datasets, how do I submit the Croissant files?**\nYou should submit a Croissant file for every dataset (and check whether they are all valid). You can combine the .json files into a .zip folder and upload that during submission. In the dataset URL, refer to a webpage that documents the collection of datasets as a whole. The URLs for the individual datasets must be included in the Croissant files.\n**How do we handle our submission which includes a private hold-out set which we wish to keep private and unreleased, e.g., to avoid potential contamination?**\nYou should mention that you have a private hold-out set and describe it in your paper, but the main contribution of your paper should be the publicly released portion of your dataset. The publicly released portion of your dataset needs to conform to the data hosting guidelines. It also may contain a public validation and test set collected with the same protocol as the private one.\n**Can my submission be a synthetic dataset?**\nYes. All data hosting guidelines apply to synthetic datasets, too.\n**How should I include code as part of my submission?**\nPlease see the[guidelines for code](https://neurips.cc/public/guides/CodeSubmissionPolicy). You will be asked to provide a URL to a hosting platform (e.g., GitHub, Bitbucket). If that is not an option, you can alternatively attach it as a ZIP file as supplementary material with your paper. All code should be documented and executable.\n**I don\u2019t want to make my dataset publicly accessible at the time of submission. What are my options?**\nHarvard Dataverse and Kaggle platforms both offer private URL preview link sharing. This means your dataset is only accessible to those with whom you share its special URL, e.g., reviewers.\u00a0Note that you will be required to make your dataset public by the camera-ready deadline. Failure to do so may result in removal from the conference and proceedings.\n**Can I make changes to my dataset after I\u2019ve made my submission to Open Review?**\nYou can make changes until the submission deadline. After the submission deadline, we will perform automated verification checks of your dataset to assist in streamlining and standardizing reviews.\u00a0If it changes in a way that invalidates the original reviews at any time between the submission deadline and by the camera ready deadline or publication of proceedings, we reserve the right to remove it from the conference or proceedings.\n**I\u2019m experiencing problems with the platform I\u2019m using to release my dataset. What should I do?**\nWe have worked with maintainers of the dataset hosting platforms to identify the appropriate contact information authors should use to contact for support in case of issues or help with workarounds for storage quotas, etc. Find this contact information in the section above\u201cHow to Publish on Preferred Hosting Platforms\u201d.\n**I need to require credentialized (AKA gated) access to my dataset**\nThis will be possible on the condition that a credentialization is necessary for the public good (e.g. because of ethically sensitive medical data), and that an established credentialization procedure is in place that is 1) open to a large section of the public; 2) provides rapid response and access to the data; and 3) is guaranteed to be maintained for many years. A good example here is PhysioNet Credentialing, where users must first understand how to handle data with human subjects, yet is open to anyone who has learned and agrees with the rules.\n*This should be seen as an exceptional measure, and NOT as a way to limit access to data for other reasons*(e.g., to shield data behind a Data Transfer Agreement). Misuse would be grounds for desk rejection. During submission, you can indicate that your dataset involves open credentialized access, in which case the necessity, openness, and efficiency of the credentialization process itself will also be checked.\n**Our dataset requires credentialized access. How do we preserve single-blind review, i.e., ensure the identities of reviewers isn\u2019t shared with authors?**\nIf it\u2019s possible to share a private preview...",
      "url": "https://neurips.cc/Conferences/2025/DatasetsBenchmarks-FAQ"
    }
  ]
}