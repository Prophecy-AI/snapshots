{
  "query": "What are the most effective techniques to reduce distribution shift between training and test data in chemistry prediction competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the gap between the distribution of training molecules and the unseen test set is a central challenge in chemistry\u2011focused ML competitions.  Recent work shows that **test\u2011time refinement** can be very effective: by adjusting the graph structure of a molecule so that its edge patterns resemble those seen during training (a spectral\u2011graph\u2011theory step) and by updating the model\u2019s latent representation with a few gradient steps that do not require expensive quantum\u2011chemical labels, predictions become more robust to out\u2011of\u2011distribution (OOD) chemistry\u202f([arxiv.org](https://arxiv.org/abs/2503.08674)).  A closely related study on universal interatomic potentials reaches the same conclusion, emphasizing lightweight, label\u2011free adaptation at inference time as a practical way to mitigate shift\u202f([pubs.rsc.org](https://pubs.rsc.org/kr/content/articlehtml/2026/dd/d5dd00260e)).\n\nAnother powerful family of techniques focuses on **data\u2011centric strategies**.  Selecting a training subset that is chemically homogeneous with the target test set\u2014using attribution scores that capture assay compatibility\u2014has been shown to improve performance even when the test labels are unknown, effectively \u201ccurating\u201d the training distribution\u202f([arxiv.org](https://arxiv.org/abs/2511.16087)).  Complementary to selection, **meta\u2011learning with unlabeled data** can learn to interpolate between in\u2011distribution (ID) and OOD regions, allowing the model to adapt its parameters based on the structure of new, unlabeled compounds\u202f([arxiv.org](https://arxiv.org/abs/2506.11877)).  These approaches reduce covariate shift without requiring additional expensive experiments.\n\nFinally, **model\u2011level and evaluation practices** matter.  Benchmarks such as BOOM demonstrate that models with strong inductive biases (e.g., physics\u2011informed graph networks) and careful pre\u2011training on diverse chemical corpora tend to generalize better to OOD tasks, especially when combined with rigorous, domain\u2011aware cross\u2011validation protocols that mimic the competition\u2019s test conditions\u202f([openreview.net](https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf)).  Probabilistic calibration methods (e.g., DIONYSUS) further improve reliability on low\u2011data, shifted domains\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  Together, test\u2011time adaptation, curated/meta\u2011learned training data, inductive\u2011bias\u2011rich architectures, and robust validation form the most effective toolkit for narrowing distribution shift in chemistry prediction competitions.",
      "url": ""
    },
    {
      "title": "Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2503.08674** (cs)\n\n\\[Submitted on 11 Mar 2025 ( [v1](https://arxiv.org/abs/2503.08674v1)), last revised 29 May 2025 (this version, v2)\\]\n\n# Title:Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields\n\nAuthors: [Tobias Kreiman](https://arxiv.org/search/cs?searchtype=author&query=Kreiman,+T), [Aditi S. Krishnapriyan](https://arxiv.org/search/cs?searchtype=author&query=Krishnapriyan,+A+S)\n\nView a PDF of the paper titled Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields, by Tobias Kreiman and Aditi S. Krishnapriyan\n\n[View PDF](https://arxiv.org/pdf/2503.08674) [HTML (experimental)](https://arxiv.org/html/2503.08674v2)\n\n> Abstract:Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at [this https URL](https://tkreiman.github.io/projects/mlff_distribution_shifts/).\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM) |\n| Cite as: | [arXiv:2503.08674](https://arxiv.org/abs/2503.08674) \\[cs.LG\\] |\n| (or [arXiv:2503.08674v2](https://arxiv.org/abs/2503.08674v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.08674](https://doi.org/10.48550/arXiv.2503.08674) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tobias Kreiman \\[ [view email](https://arxiv.org/show-email/dfd6e11c/2503.08674)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.08674v1)**\nTue, 11 Mar 2025 17:54:29 UTC (8,980 KB)\n**\\[v2\\]**\nThu, 29 May 2025 17:53:47 UTC (19,211 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields, by Tobias Kreiman and Aditi S. Krishnapriyan\n\n- [View PDF](https://arxiv.org/pdf/2503.08674)\n- [HTML (experimental)](https://arxiv.org/html/2503.08674v2)\n- [TeX Source](https://arxiv.org/src/2503.08674)\n- [Other Formats](https://arxiv.org/format/2503.08674)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2503.08674&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2503.08674&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2503.08674?context=cond-mat) [cond-mat.mtrl-sci](https://arxiv.org/abs/2503.08674?context=cond-mat.mtrl-sci) [cs](https://arxiv.org/abs/2503.08674?context=cs) [physics](https://arxiv.org/abs/2503.08674?context=physics) [physics.chem-ph](https://arxiv.org/abs/2503.08674?context=physics.chem-ph) [q-bio](https://arxiv.org/abs/2503.08674?context=q-bio) [q-bio.BM](https://arxiv.org/abs/2503.08674?context=q-bio.BM)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.08674)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.08674)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.08674)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.08674) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2503.08674"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "AssayMatch: Learning to Select Data for Molecular Activity Models",
      "text": "<div><div>\n<p><a href=\"https://arxiv.org/pdf/2511.16087\">View PDF</a>\n<a href=\"https://arxiv.org/html/2511.16087v1\">HTML (experimental)</a></p><blockquote>\n<span>Abstract:</span>The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at <a href=\"https://github.com/Ozymandias314/AssayMatch\">this https URL</a>.\n</blockquote>\n</div><div>\n<h2>Submission history</h2><p> From: Vincent Fan [<a href=\"https://arxiv.org/show-email/352d0906/2511.16087\">view email</a>] <br/> <strong>[v1]</strong>\nThu, 20 Nov 2025 06:25:51 UTC (6,748 KB)<br/>\n</p></div></div>",
      "url": "https://arxiv.org/abs/2511.16087"
    },
    {
      "title": "Understanding and mitigating distribution shifts for universal machine learning interatomic potentials",
      "text": "Understanding and mitigating distribution shifts for universal machine learning interatomic potentials - Digital Discovery (RSC Publishing) DOI:10.1039/D5DD00260E\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2026/dd/d5dd00260e)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D5DD00260E](https://doi.org/10.1039/D5DD00260E)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2026, Advance Article\n# Understanding and mitigating distribution shifts for universal machine learning interatomic potentials\nTobias Kreiman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0009-0002-1142-5744)\\*aandAditi S. Krishnapriyanab\naUC Berkeley, USA. E-mail:[tkreiman@berkeley.edu](mailto:tkreiman@berkeley.edu)\nbLBNL, USA\nReceived 11th June 2025, Accepted 17th November 2025\nFirst published on 4th December 2025\n## Abstract\nMachine Learning Interatomic Potentials (MLIPs) are a promising alternative to expensiveab initioquantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how universal MLIPs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLIPs\u2014that is, changes between the training and testing distributions\u2014we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large universal models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLIPs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLIPs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensiveab initioreference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLIPs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLIPs. Our code is available at https://tkreiman.github.io/projects/mlff\\_distribution\\_shifts/.\n## 1 Introduction\nUnderstanding the quantum mechanical properties of atomistic systems is crucial for the discovery and development of new molecules and materials. Computational methods like Density Functional Theory (DFT) are essential for studying these systems, but the high computational demands of such methods limit their scalability. Machine Learning Interatomic Potentials (MLIPs) have emerged as a promising alternative, learning to predict energies and forces from reference quantum mechanical calculations. MLIPs are faster than traditionalab initiomethods, and their accuracy is rapidly improving for modeling complex atomistic systems.[5,7,29,57](#cit5)\nGiven the computational expense ofab initiosimulations for all chemical spaces of interest, there has been a push to train larger and more accurate MLIPs, designed to work well across many different systems. Developing models with general representations that accurately capture diverse chemistries has the potential to reduce or even eliminate the need to recollect data and retrain a model for each new system. To determine which systems an MLIP can accurately describe and to assess the reliability of its predictions, it is important to understand how MLIPs generalize beyond their training distributions. This understanding is essential for applying MLIPLs to new and diverse chemical spaces, ensuring that they perform well not only on the data they were trained on, but also on unseen, potentially more complex systems.\nIn other fields of machine learning (ML), model generalization has been extensively studied through the lens of distribution shifts: changes between the training and testing distributions.[43,61,63,70,71](#cit43)In computer vision, for example, a distribution shift would occur if a model trained on color images is evaluated on black and white images. The extensive work to both categorize and mitigate distribution shifts in the broader field of ML has helped practitioners determine where models can be reliably applied.\nWe conduct an in-depth exploration to identify and understand distribution shifts for MLIPs. On example chemical datasets, we find that many large-scale models struggle with common distribution shifts[6,46,50,58](#cit6)(see Section 3). These generalization challenges suggest that current supervised training methods for MLIPs overfit to training distributions and do not enable MLIPs to generalize accurately. We demonstrate that there are multiple reasons that this is the case, including challenges associated with poorly-connected graphs and learning unregularized representations, evidenced by jagged predicted potential energy surfaces for out-of-distribution systems.\nBuilding on our observations, we take initial steps to mitigate distribution shifts for MLIPs without test set reference labels by proposing two approaches: test-time radius refinement and test-time training.[27,42,62](#cit27)For test-time radius refinement, we modify the construction of test-graphs to match the training Laplacian spectrum, overcoming differences between training and testing graph structures. For test-time training (TTT), we address distribution shifts by taking gradient steps on an auxiliary objective at test time. Analogous to self-supervised objectives in computer vision TTT works,[27,36,62](#cit27)we use an efficient prior as a target to improve representations at test time.\nAlthough completely closing the out-of-distribution to in-distribution gap remains a challenging open machine learning problem,[27,62](#cit27)our extensive experiments show that our test-time refinement strategies are effective in mitigating distribution shifts for MLIPs. Our experiments demonstrate that low quality data can be used to improve generalization for MLIPs, and they establish clear benchmarks that highlight ambitious but important generalization goals for the next generation of MLIPs.\nWe summarize our main contributions here:\n(1) We run diagnostic experiments on different chemical datasets to characterize and understand common distribution shifts for MLIPs in Section 3.\n(2) Based on (1), we take first steps at mitigating MLIP distribution shifts in Section 4 with two test-time refinement strategies.\n(3) The success of these methods, validated through extensive experiments in Section 5, suggests that MLIPs are not being adequately trained to generalize, despite current models having the expressivity to close the gap on the distribution shifts explored in Section 3.\n## 2 Related work\n### 2.1 Distribution shifts\nSince most machine learning algorithms assume that the training and testing data are independently and identically distributed (the I.I.D. assumption), a long line of work has studied violations of this assumption, commonly referred to as distr...",
      "url": "https://pubs.rsc.org/kr/content/articlehtml/2026/dd/d5dd00260e"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "",
      "text": "BOOM: Benchmarking Out-Of-distribution Molecular\nProperty Predictions of Machine Learning Models\nEvan R. Antoniuk\u2020\u2217 Shehtab Zaman\u2021\u2217 Tal Ben-Nun\u2020 Peggy Li\u2020\nJames Diffenderfer\u2020 Busra Demirci\u2021 Obadiah Smolenski\u2021 Tim Hsu\u2020\nAnna M. Hiszpanski\u2020 Kenneth Chiu\u2021 Bhavya Kailkhura\u2020\nBrian Van Essen\u2020\n\u2020 Lawrence Livermore National Laboratory, Livermore, CA\n\u2021 Binghamton University, Binghamton, NY\nAbstract\nData-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML)\nand generative modeling to filter and design novel molecules. Discovering novel molecules\nrequires accurate out-of-distribution (OOD) predictions, but ML models struggle to general\u0002ize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks.\nWe present BOOM, benchmarks for out-of-distribution molecular property predictions: a\nchemically-informed benchmark for OOD performance on common molecular property\nprediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning\nmodels on OOD performance. Overall, we find that no existing model achieves strong gener\u0002alization across all tasks: even the top-performing model exhibited an average OOD error 3\u00d7\nhigher than in-distribution. Current chemical foundation models do not show strong OOD ex\u0002trapolation, while models with high inductive bias can perform well on OOD tasks with sim\u0002ple, specific properties. We perform extensive ablation experiments, highlighting how data\ngeneration, pre-training, hyperparameter optimization, model architecture, and molecular\nrepresentation impact OOD performance. Developing models with strong out-of-distribution\n(OOD) generalization is a new frontier challenge in chemical machine learning (ML). This\nopen-source benchmark is available at https://github.com/FLASK-LLNL/BOOM.\n1 Introduction\nMolecular discovery pipelines have increasingly relied upon machine learning (ML) models [Bohacek\net al., 1996, Reymond, 2015, Kailkhura et al., 2019]. These models discover new molecules by\neither screening a list of enumerated molecules or by guiding a generative model towards molecules\nof interest [Wang et al., 2023a]. Molecular discovery is inherently an out-of-distribution (OOD)\nprediction problem, since the molecules need to either (i) exhibit properties that extrapolate beyond\nthe training dataset, or (ii) possess a previously unconsidered chemical substructure. In either case,\nsuccess depends on the learned model\u2019s ability to make accurate predictions on samples that are not\nin the same distribution as the training data.\nDespite the importance of OOD performance to real-world molecular discovery, the OOD per\u0002formance of common ML models for molecular property prediction has yet to be systematically\nexplored. Due to the lack of standardized splits for testing models, especially splits based on the\ndata distribution, we believe that current ML models are optimizing in-distribution performance on\n\u2217Equal Contribution\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\ninsufficiently challenging datasets that do not adequately measure real-world performance. Currently,\nlittle empirical knowledge exists about how choices regarding the pretraining task, model architecture,\nand/or dataset diversity impact the generalization performance of chemistry foundation models that\nare expected to generalize across all chemical systems.\nIn this work, we develop BOOM, benchmarks for out-of-distribution molecular property predictions,\na standardized benchmark for assessing the OOD generalization performance of molecule property\nprediction models. Our work consists of the following main contributions:\n\u2022 We develop a general and robust methodology for evaluating the performance of chemical\nproperty prediction models for property values beyond their training distribution. We intro\u0002duce OOD-specific metrics such as binned R2\nto allow comparisons of OOD performance\nacross all models.\n\u2022 We perform the first large-scale OOD performance benchmarking of state-of-the-art ML\nchemical property prediction models. Across 10 diverse OOD tasks and 15 models, we\ndo not find any existing models that show strong OOD generalization across all tasks. We\ntherefore put forth BOOM OOD property prediction as a frontier challenge for chemical\nfoundation models.\n\u2022 Our work highlights insights into how pretraining strategies, model architecture, molecular\nrepresentation, and data augmentation impact OOD performance. These findings point\ntowards strategies for the chemistry community to achieve chemical foundation models with\nstrong OOD generalization across all chemical systems.\n2 BOOM\nDefining Out-of-distribution. Consider a supervised dataset D with N molecules M \u2208\n{M1,M2, ...,MN } and associated labels or properties y \u2208 {y1, y2, ..., yN }. The problem of\nout-of-distribution prediction can be defined as the mismatch in the probability distribution, P of the\ntraining and test sets, Dtrain and Dtest such that,\nP(M, y|Dtest) \u0338= P(M, y|Dtrain) (1)\nThe key question is defining the density function P(M, y) over a set of molecules and their respective\nproperties. The density can be defined over the chemical structure or molecule features, or over the\nproperties. Formally, we define out-of-distribution as low-density regions over the property space,\nsuch that:\n0 < P(ytest) \u2264 P(ytrain) (2)\nFarquhar and Gal [2022] define this as a complement distribution conditioned on the targets. This is\nknown as concept or label shift as well [Liu et al., 2024]. While we focus on designing splits with a\nconcept shift, it is important to note that depending on the property, this may result in a covariate shit,\nresulting in a structural or chemical imbalance. The probability density over the labels is determined\nusing kernel density estimation (KDE), allowing us to generalize to multimodal distributions. The\nsplit strategy algorithm for each dataset is detailed in Appendix A.1. The lowest probability samples\nfrom the KDE estimated distribution are held-out (see Fig. 1) to evaluate the consistency of ML\nmodels to discover molecules with state-of-the-art properties that extrapolate beyond the training\ndata.\nDatasets. BOOM consists of 10 quantum chemical molecular property datasets derived from\nQM9 [Ramakrishnan et al., 2014] and the 10k Dataset [Antoniuk et al., 2025], derived from the Cam\u0002bridge Structural Database. The 10k Dataset was sourced from 10,206 experimentally synthesized,\nsmall organic molecules and contains the density functional theory calculated values of their molecu\u0002lar density and solid heat of formation (HoF). We collect 8 molecular property datasets from the QM9\nDataset: isotropic polarizability (\u03b1), heat capacity (Cv), highest occupied molecular orbital (HOMO)\nenergy, lowest unoccupied molecular orbital (LUMO) energy, HOMO-LUMO gap, dipole moment\n(\u00b5), electronic spatial extent (\nR2\n\u000b\n), and zero point vibrational energy (ZPVE). We also select a\nrandom subset of the dataset to serve as the ID test set, detailed in Appendix A. To further expand the\napplication space of BOOM, we also perform benchmarking on the Lipophilicity dataset[Wu et al.,\n2018] of 4,200 experimental measurements of the octanol/water distribution coefficient, which is of\nrelevance for drug compounds. The inclusion of the Lipophilicity dataset serves as an exemplary\n2\nFigure 1: (Left) An example OOD dataset included in the BOOM benchmark. To assess OOD\nperformance, we split each chemical property dataset into an out-of-distribution (OOD) Test Set\n(blue), an in-distribution (ID) Test Set (orange) and a Train Set (green), as described in Section 2.\n(Right) Example model predictions on this task exhibiting weak correlation on the OOD samples.\ndataset for performing OOD evaluations on experimentally measured properties, rather than only\ncomputed physicochemical properties (See Table 9).\nMetrics. We also propose standardized metrics over the ID and OOD to compare models. We\nuse root mean square error (RMSE) over respective data spli...",
      "url": "https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "<div>\n<div>\n<p><a href=\"https://www.cornell.edu/\"></a>\n</p>\n<div>\n<p><a href=\"https://confluence.cornell.edu/x/ALlRF\">We gratefully acknowledge support from<br/>\nthe Simons Foundation and member institutions.</a>\n</p></div>\n</div>\n<div>\n<h2><a href=\"https://arxiv.org/\">\n</a></h2>\n</div>\n</div>",
      "url": "https://arxiv.org/abs/2512.08896"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    }
  ]
}