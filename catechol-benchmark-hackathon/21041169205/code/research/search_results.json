{
  "query": "What is the catechol benchmark hackathon NeurIPS 2025 and what approaches achieve the best scores?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The Catechol Benchmark hackathon at NeurIPS\u202f2025 was a competition built around a new time\u2011series solvent\u2011selection dataset for few\u2011shot machine\u2011learning, containing over 1,200 continuous process conditions for yield prediction\u202f([arxiv.org](https://arxiv.org/abs/2506.07619), 9\u202fJun\u202f2025). Transfer\u2011learning models and active\u2011learning strategies achieved the highest prediction scores on this benchmark\u202f([arxiv.org](https://arxiv.org/html/2506.07619v1)).",
      "url": ""
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection ...",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "NeurIPS 2025: A Guide to Key Papers, Trends & Stats | IntuitionLabs",
      "text": "NeurIPS 2025: A Guide to Key Papers, Trends &amp; Stats | IntuitionLabs\n[\n![Intuition Labs Logo](https://intuitionlabs.ai/_next/image?url=%2Fimages%2Flogos%2Fintuitionlabs%2FIntuitionLabs-icon-L-only.svg&amp;w=2048&amp;q=75)\nIntuitionLabs](https://intuitionlabs.ai/)\n[Contact](https://intuitionlabs.ai/about/contact)\n[Back to Articles](https://intuitionlabs.ai/articles)|By[Adrien Laurent](https://intuitionlabs.ai/articles/authors/adrien-laurent)\n|Updated on1/15/2026|35 min read|[Next Article](https://intuitionlabs.ai/articles/ram-shortage-2025-ai-demand)\nMore\nDownload PDFPDF\n# NeurIPS 2025: A Guide to Key Papers, Trends &amp; Stats\n![NeurIPS 2025: A Guide to Key Papers, Trends &amp; Stats](https://intuitionlabs.ai/_next/image?url=%2Fimages%2Farticles%2Fneurips-2025-conference-summary-trends.avif&amp;w=2048&amp;q=75)\n[neurips 2025](https://intuitionlabs.ai/articles/tags/neurips-2025)[machine learning](https://intuitionlabs.ai/articles/tags/machine-learning)[ai conference](https://intuitionlabs.ai/articles/tags/ai-conference)[large language models](https://intuitionlabs.ai/articles/tags/large-language-models)[ai research trends](https://intuitionlabs.ai/articles/tags/ai-research-trends)[reinforcement learning](https://intuitionlabs.ai/articles/tags/reinforcement-learning)[ai ethics](https://intuitionlabs.ai/articles/tags/ai-ethics)[datasets and benchmarks](https://intuitionlabs.ai/articles/tags/datasets-and-benchmarks)[test-of-time award](https://intuitionlabs.ai/articles/tags/test-of-time-award)\n# Executive Summary\nNeurIPS (Neural Information Processing Systems) 2025 \u2013the 39th annual meeting \u2013represents both continuity and change in the world\u2019s premier[machine learning](https://intuitionlabs.ai/solutions/digital-transformation/process-automation)conference. Held December 2\u20137, 2025 in San Diego (with a simultaneous secondary site in Mexico City), NeurIPS 2025 attracted**over 5,200 accepted papers**(24.5% of \\~21,575 submissions) and enormous attention from industry, academia, and media. Major trends in 2025 included an emphasis on[large language and foundation models](https://intuitionlabs.ai/articles/mistral-large-3-moe-llm-explained), reproducibility and data-centric research via the growing*Datasets &amp; Benchmarks Track*, and explicit attention to[societal impacts of AI](https://intuitionlabs.ai/articles/ai-impact-graduate-jobs-2025). For the first time, NeurIPS introduced a*Position Paper Track*to host perspective pieces on broad AI implications, and a*Journal Track*featuring 34 top papers from leading journals (14 from JMLR and 20 from AoS) embedded as posters ([[1]](<https://blog.neurips.cc/2025/10/22/bridging-journals-and-the-neurips-community-journal-track-at-neurips-2025/#:~:text=This year, we are thrilled,enrich the broader NeurIPS community>)). The conference featured six high-profile invited talks (e.g. by Sutton, Tufekci, Choi, Mitchell, Cho, Saxe) probing fundamental and applied issues in AI ([[2]](<https://blog.neurips.cc/category/2025-conference/#:~:text=At 8:30am on Dec 3,,level and learnable.\u201d>)) ([[3]](<https://blog.neurips.cc/category/2025-conference/#:~:text=At 2:30pm on Dec 4,,solving>)). Major awards recognized frontier research: the**Best Paper Award**went to Qiu et al.\u2019s scaled LLM gating work ([[4]](https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/#:~:text=Abstract)) ([[5]](<https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/#:~:text=the Qwen3>)), and runner-ups included an important analysis of reinforcement learning for LLM reasoning ([[6]](<https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/#:~:text=Reinforcement Learning with Verifiable Rewards,RL algorithms, and math/coding/visual reasoning>)) ([[7]](<https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/#:~:text=This paper delivers a masterfully,an important finding which will>)). In the*Test-of-Time*category (honoring 2015 papers), the classic*Faster R-CNN*object detector by Ren et al. was celebrated, with over 56,700 citations noted ([[8]](<https://blog.neurips.cc/2025/11/26/announcing-the-test-of-time-paper-award-for-neurips-2025/#:~:text=The Faster R,RPN) and detection network>)).\nNeurIPS 2025 also continued to grapple with practical challenges of scale and integrity. The program committee processed a record \\~21.6k submissions with \\~20,500 reviewers, 1,663 area chairs, and 199 senior chairs ([[9]](<https://blog.neurips.cc/2025/09/30/reflections-on-the-2025-review-process-from-the-program-committee-chairs/#:~:text=Like most AI conferences, NeurIPS,a larger scale, but this>)). To ensure quality, chairs introduced \u201cResponsible Reviewing\u201d policies (e.g. penalizing negligent reviewers) and refined their calibration processes ([[10]](<https://blog.neurips.cc/2025/09/30/reflections-on-the-2025-review-process-from-the-program-committee-chairs/#:~:text=We understand and we are,who have adopted similar policies>)) ([[11]](<https://blog.neurips.cc/2025/09/30/reflections-on-the-2025-review-process-from-the-program-committee-chairs/#:~:text=Review processes at NeurIPS are,the topic of responsible reviewing>)). Notably, the conference publicly reaffirmed strict confidentiality after an OpenReview leak, warning that any exploitation of reviewer identities would be punished severely ([[12]](<https://blog.neurips.cc/category/2025-conference/#:~:text=The NeurIPS 2025 organizing committee,of our peer review process>)). To embrace new authoring tools, NeurIPS issued a detailed**LLM usage policy**, allowing AI-assisted writing but requiring authors to verify and cite all content ([[13]](<https://neurips.cc/Conferences/2025/LLM#:~:text=Use of Large Language Models,entire content of the paper>)).\nBeyond technical content, NeurIPS 2025 underscored community and societal dimensions. It hosted numerous*affinity events*(e.g. Women in ML, LatinX in AI, Queer in AI, Muslims in ML, etc.) to foster inclusion ([[14]](<https://neurips.cc/virtual/2025/affinity_events#:~:text=LatinX in AI>)) ([[15]](<https://neurips.cc/virtual/2025/affinity_events#:~:text=Muslims in ML>)), and 8 \u201csocial\u201d sessions on topics like AI safety, data openness, or AI art. Industry involvement was immense: Google alone reported*175 accepted papers*by its researchers and sponsored workshops (e.g. Women in ML, LatinX) ([[16]](<https://research.google/conferences-and-events/google-at-neurips-2025/#:~:text=We have a strong presence,the broader ML research community>)); Microsoft likewise staffed booths and talks; and many companies recruited aggressively (as noted by press).\nIn sum, NeurIPS 2025 was a landmark event, reflecting the maturation of machine learning: a conference at global \u201cmega-event\u201d scale, deeply integrated with industry and societal concerns, yet still grounded in rigorous scientific standards. This report offers an in-depth analysis of NeurIPS 2025: its history and organization; submission and review processes; key themes and findings from the technical program;[case studies](https://intuitionlabs.ai/resources/case-studies)(best papers, invited talks, workshops); and discussion of implications for the future of AI research. All observations are supported by data from the official NeurIPS publications, authoritative news, and expert reports.\n# 1. Introduction and Background\nThe*Conference on Neural Information Processing Systems*(NeurIPS, formerly*NIPS*) has been held annually since 1987 and is widely recognized as the premier academic conference in machine learning and computational neuroscience ([[17]](<https://static.hlt.bme.hu/semantics/external/pages/LSTM/en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems.html#:~:text=History ,https://neurips.cc>)). Founded as an interdisciplinary meeting on neural networks, NeurIPS has grown exponentially over the decades: by 2016 it hosted \\~5,000 participants, by 2017 \\~8,000 ([[18]](<https://static.hlt.bme.hu/semantics/external/...",
      "url": "https://intuitionlabs.ai/articles/neurips-2025-conference-summary-trends"
    },
    {
      "title": "NeurIPS 2025 Papers",
      "text": "NeurIPS 2025 PapersNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/virtual/2025/loc/san-diego/poster/115406)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/papers.html)\nLayout:\nminicompacttopicdetail\n&times;\nNo topics availableNo sessions availabletitleauthortopicsession\nshuffle\nby\nserendipitybookmarked firstvisited firstnot visited firstbookmarked but not visited\nWe use cookies to store which papers have been visited.\nI agree\nSuccessful Page Load\nNeurIPS uses cookies for essential functions only. We do not sell your personal\ninformation.[Our Privacy Policy &raquo;&raquo;](https://neurips.cc/public/PrivacyPolicy)|Accept|\nWe use cookies to store which papers have been visited.\nI agree",
      "url": "https://neurips.cc/virtual/2025/papers.html"
    },
    {
      "title": "NeurIPS 2025 San Diego Datasets & Benchmarks",
      "text": "NeurIPS 2025 San Diego Datasets &amp; BenchmarksNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/events/datasets-benchmarks-2025)\n# San Diego Datasets &amp; Benchmarks\n**494 Events\n**\nPoster### [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://neurips.cc/virtual/2025/poster/121603)\nKuicai Dong &middot; CHANG YUJING &middot; Shijie Huang &middot; Yasheng Wang &middot; Ruiming Tang &middot; Yong Liu\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nDocument Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration. Key findings reveal that advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121603)\nPoster### [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://neurips.cc/virtual/2025/poster/121386)\nHyungyung Lee &middot; Geon Choi &middot; Jung-Oh Lee &middot; Hangyul Yoon &middot; Hyuk Hong &middot; Edward Choi\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nRecent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121386)\nPoster### [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://neurips.cc/virtual/2025/poster/121553)\nAtharva Gundawar &middot; Som Sagar &middot; Ransalu Senanayake\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nVision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that is largely unverified. To perform actions reliably, robots must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state like being closed). Despite their ubiquitous use in manipulation, we argue that off-the-shelf VLMs may lack this granular, physically-grounded understanding, as these specific prerequisites are often overlooked during training. Addressing this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of these core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with more than 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, 1\u20133 affordances defined per object class), 100 real-world humanoid view scenarios, and 120 unique simulated constraint scenarios across four tasks. Our evaluations reveal significant gaps in the ability of VLMs to grasp fundamental physical concepts, underscoring their current limitations for reliable robot manipulation and pointing to key areas that require targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating the physical reasoning capabilities of VLMs guiding the development of more robust and physically grounded models for robot manipulation.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121553)\nPoster### [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://neurips.cc/virtual/2025/poster/121706)\nXinran Wang &middot; Songyu Xu &middot; Shan Xiangxuan &middot; Yuxuan Zhang &middot; Muxi Diao &middot; Xueyan Duan &middot; Yanhua huang &middot; Kongming Liang &middot; Zhanyu Ma\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nCinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and repro...",
      "url": "https://neurips.cc/virtual/2025/loc/san-diego/events/datasets-benchmarks-2025"
    },
    {
      "title": "NeurIPS 2025 Datasets and Benchmarks Track  Submissions",
      "text": "Submissions | OpenReview\n\u00d7### BibTeX Record\n```\n```\n*Click anywhere on the box above to highlight complete record*\nDone\n[![back arrow](https://openreview.net/images/arrow_left.svg)Back to**NeurIPS**](https://openreview.net/venue?id=NeurIPS.cc)\n# NeurIPS 2025 Datasets and Benchmarks TrackSubmissions\n* #### [Demystifying Network Foundation Models](https://openreview.net/forum?id=S4YDNW5eCx)\n[Roman Beltiukov](https://openreview.net/profile?id=~Roman_Beltiukov1),[Satyandra Guthula](https://openreview.net/profile?id=~Satyandra_Guthula1),[Wenbo Guo](https://openreview.net/profile?id=~Wenbo_Guo1),[Walter Willinger](https://openreview.net/profile?id=~Walter_Willinger2),[Arpit Gupta](https://openreview.net/profile?id=~Arpit_Gupta2)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems](https://openreview.net/forum?id=fm77rDf9JS)\n[Yu Shang](https://openreview.net/profile?id=~Yu_Shang1),[Peijie Liu](https://openreview.net/profile?id=~Peijie_Liu1),[Yuwei Yan](https://openreview.net/profile?id=~Yuwei_Yan1),[Zijing Wu](https://openreview.net/profile?id=~Zijing_Wu3),[Leheng Sheng](https://openreview.net/profile?id=~Leheng_Sheng2),[Yuanqing Yu](https://openreview.net/profile?id=~Yuanqing_Yu1),[Chumeng Jiang](https://openreview.net/profile?id=~Chumeng_Jiang1),[An Zhang](https://openreview.net/profile?id=~An_Zhang2),[Fengli Xu](https://openreview.net/profile?id=~Fengli_Xu1),[Yu Wang](https://openreview.net/profile?id=~Yu_Wang3),[Min Zhang](https://openreview.net/profile?id=~Min_Zhang15),[Yong Li](https://openreview.net/profile?id=~Yong_Li7)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track spotlight\n* Readers:Everyone\n* #### [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://openreview.net/forum?id=anzoPBV4jI)\n[Kiril Vasilev](https://openreview.net/profile?id=~Kiril_Vasilev1),[Alexandre Misrahi](https://openreview.net/profile?id=~Alexandre_Misrahi1),[Eeshaan Jain](https://openreview.net/profile?id=~Eeshaan_Jain1),[Phil F Cheng](https://openreview.net/profile?id=~Phil_F_Cheng1),[Petros Liakopoulos](https://openreview.net/profile?id=~Petros_Liakopoulos1),[Olivier Michielin](https://openreview.net/profile?id=~Olivier_Michielin1),[Michael Moor](https://openreview.net/profile?id=~Michael_Moor1),[Charlotte Bunne](https://openreview.net/profile?id=~Charlotte_Bunne1)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives](https://openreview.net/forum?id=3rY182JOOZ)\n[Wisdom Oluchi Ikezogwo](https://openreview.net/profile?id=~Wisdom_Oluchi_Ikezogwo1),[Kevin Minghan Zhang](https://openreview.net/profile?id=~Kevin_Minghan_Zhang1),[Mehmet Saygin Seyfioglu](https://openreview.net/profile?id=~Mehmet_Saygin_Seyfioglu1)\n* Published: 18 Sept 2025, Last Modified: 14 Jan 2026\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [FalconEye: Cross-Modal Perception for Comprehensive Safety Evaluation](https://openreview.net/forum?id=a44GQ8sjKp)\n[Qi Xue](https://openreview.net/profile?id=~Qi_Xue1),[Minrui Jiang](https://openreview.net/profile?id=~Minrui_Jiang1),[Runjia Zhang](https://openreview.net/profile?id=~Runjia_Zhang1),[Xiurui Xie](https://openreview.net/profile?id=~Xiurui_Xie1),[Guisong Liu](https://openreview.net/profile?id=~Guisong_Liu2)\n* 12 May 2025 (modified: 30 Oct 2025)\n* Submitted to NeurIPS 2025 Datasets and Benchmarks Track\n* Readers:Everyone\n* #### [Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models](https://openreview.net/forum?id=DHA9uoeMQx)\n[Daoyuan Chen](https://openreview.net/profile?id=~Daoyuan_Chen1),[Yilun Huang](https://openreview.net/profile?id=~Yilun_Huang1),[Xuchen Pan](https://openreview.net/profile?id=~Xuchen_Pan1),[Jiang Nana](https://openreview.net/profile?id=~Jiang_Nana1),[Haibin Wang](https://openreview.net/profile?id=~Haibin_Wang1),[Yilei Zhang](https://openreview.net/profile?id=~Yilei_Zhang1),[Ce Ge](https://openreview.net/profile?id=~Ce_Ge1),[Yushuo Chen](https://openreview.net/profile?id=~Yushuo_Chen2),[Wenhao Zhang](https://openreview.net/profile?id=~Wenhao_Zhang5),[Zhijian Ma](https://openreview.net/profile?id=~Zhijian_Ma1),[Jun Huang](https://openreview.net/profile?id=~Jun_Huang4),[Wei Lin](https://openreview.net/profile?id=~Wei_Lin5),[Yaliang Li](https://openreview.net/profile?id=~Yaliang_Li1),[Bolin Ding](https://openreview.net/profile?id=~Bolin_Ding3),[Jingren Zhou](https://openreview.net/profile?id=~Jingren_Zhou1)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track spotlight\n* Readers:Everyone\n* #### [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://openreview.net/forum?id=w8uII2qAmd)\n[Yimeng Chen](https://openreview.net/profile?id=~Yimeng_Chen1),[Piotr Pi\u0119kos](https://openreview.net/profile?id=~Piotr_Pi%C4%99kos2),[Mateusz Ostaszewski](https://openreview.net/profile?id=~Mateusz_Ostaszewski1),[Firas Laakom](https://openreview.net/profile?id=~Firas_Laakom1),[J\u00fcrgen Schmidhuber](https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [NAVIX: Scaling MiniGrid Environments with JAX](https://openreview.net/forum?id=lPVz01z0DI)\n[Eduardo Pignatelli](https://openreview.net/profile?id=~Eduardo_Pignatelli1),[Jarek Luca Liesen](https://openreview.net/profile?id=~Jarek_Luca_Liesen1),[Robert Tjarko Lange](https://openreview.net/profile?id=~Robert_Tjarko_Lange1),[Chris Lu](https://openreview.net/profile?id=~Chris_Lu1),[Pablo Samuel Castro](https://openreview.net/profile?id=~Pablo_Samuel_Castro1),[Laura Toni](https://openreview.net/profile?id=~Laura_Toni1)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models](https://openreview.net/forum?id=FEVfIPMy5b)\n[Christopher Chiu](https://openreview.net/profile?id=~Christopher_Chiu1),[Silviu Pitis](https://openreview.net/profile?id=~Silviu_Pitis1),[Mihaela van der Schaar](https://openreview.net/profile?id=~Mihaela_van_der_Schaar2)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [OpenLex3D: A Tiered Benchmark for Open-Vocabulary 3D Scene Representations](https://openreview.net/forum?id=XZPGUWOXeP)\n[Christina Kassab](https://openreview.net/profile?id=~Christina_Kassab1),[Sacha Morin](https://openreview.net/profile?id=~Sacha_Morin1),[Martin B\u00fcchner](https://openreview.net/profile?id=~Martin_B%C3%BCchner1),[Matias Mattamala](https://openreview.net/profile?id=~Matias_Mattamala1),[Kumaraditya Gupta](https://openreview.net/profile?id=~Kumaraditya_Gupta1),[Abhinav Valada](https://openreview.net/profile?id=~Abhinav_Valada1),[Liam Paull](https://openreview.net/profile?id=~Liam_Paull1),[Maurice Fallon](https://openreview.net/profile?id=~Maurice_Fallon1)\n* Published: 18 Sept 2025, Last Modified: 30 Oct 2025\n* NeurIPS 2025 Datasets and Benchmarks Track poster\n* Readers:Everyone\n* #### [Tactile MNIST: Benchmarking Active Tactile Perception](https://openreview.net/forum?id=QXq5BoaNPI)\n[Tim Schneider](https://openreview.net/profile?id=~Tim_Schneider2),[Guillaume Duret](https://openreview.net/profile?id=~Guillaume_Duret1),[Cristiana de Farias](https://openreview.net/profile?id=~Cristiana_de_Farias1),[Roberto Calandra](https://openreview.net/profile?id=~Roberto_Calandra1),[Liming Chen](https://openreview.net/profile?id=~Liming_Chen1),[Jan Peters](https://openreview.net/profile?id=~Jan_Peters3)\n* 12 May 2025...",
      "url": "https://openreview.net/submissions?venue=NeurIPS.cc%2F2025%2FDatasets_and_Benchmarks_Track"
    },
    {
      "title": "NeurIPS 2025 Datasets & Benchmarks Track Call for Papers",
      "text": "Call For Datasets &amp; Benchmarks 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/accounts/login)\n# **NeurIPS 2025 Datasets &amp; Benchmarks Track Call for Papers**\nThe**NeurIPS Datasets and Benchmarks track**serves as a venue for high-quality publications on highly valuable machine learning datasets and benchmarks crucial for the development and continuous improvement of machine learning methods. Previous editions of the Datasets and Benchmarks track were highly successful and continuously growing (accepted papers[2021](https://proceedings.neurips.cc/paper_files/paper/2021),[2002](https://proceedings.neurips.cc/paper_files/paper/2022), and[2023](https://proceedings.neurips.cc/paper_files/paper/2023), and best paper awards[2021](https://blog.neurips.cc/2021/11/30/announcing-the-neurips-2021-award-recipients/),[2022](https://blog.neurips.cc/2022/11/21/announcing-the-neurips-2022-awards/),[2023](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)and[2024](https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/). Read our[original blog post](https://neuripsconf.medium.com/announcing-the-neurips-2021-datasets-and-benchmarks-track-644e27c1e66c)for more about why we started this track, and the 2025[blog post](https://blog.neurips.cc/2025/03/10/neurips-datasets-benchmarks-raising-the-bar-for-dataset-submissions/)announcing this year's track updates.\n**Dates and Guidelines**\nPlease note that the Call for Papers of the NeurIPS2025 Datasets &amp; Benchmarks Track this year**will follow the**[**Call for Papers of the NeurIPS2025 Main Track**](https://neurips.cc/Conferences/2025/CallForPapers)**, with the addition of three track-specific points:**\n* Single-blind submissions\n* Required dataset and benchmark code submission\n* Specific scope for datasets and benchmarks paper submission\nThe dates are also identical to the main track:\n* **Abstract submission deadline:**May 11, 2025 AoE\n* **Full paper submission deadline:**May 15, 2025 AoE (all authors must have an OpenReview profile when submitting)\n* **Technical appendices and supplemental materials deadline:**May 22, 2025 AoE\n* **Author notification:**Sep 18, 2025 AoE\n* **Camera-ready:**Oct 23, 2025 AoE\nAccepted papers will be published in the NeurIPS proceedings and presented at the conference alongside the main track papers. As such, we aim for an equally stringent review as in the main conference track, while also allowing for**track-specific guidelines**, which we introduce below. For details on everything else, e.g. formatting, code of conduct, ethics review, important dates, and any other submission related topics, please refer to the[main track CFP.](https://neurips.cc/Conferences/2025/CallForPapers)\n**OpenReview**\nSubmit at:[https://openreview.net/group?id=NeurIPS.cc/2025/Datasets\\_and\\_Benchmarks\\_Track](https://openreview.net/group?id=NeurIPS.cc/2025/Datasets_and_Benchmarks_Track#tab-your-consoles)\nThe site will start accepting submissions on April 3, 2025 (at the same time as the main track).\n**Note:**submissions meant for the main track should be submitted to a different OpenReview portal, as shown[here](https://neurips.cc/Conferences/2025/CallForPapers). Papers will not be transferred between the main and the Datasets and Benchmarks tracks after the submission is closed.\n## **DB track specifics**\nThe Datasets and Benchmarks track adds**three changes in the submission and review guidelines that are better suited for the review of datasets and benchmarks**. Below we briefly introduce them. For more details on how to host datasets, what metadata to use to describe them and what access requirements we have, please consult[these guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines).\n1. SINGLE-BLIND SUBMISSIONS\nDatasets are often not possible to be reviewed in a double-blind fashion, and hence full anonymization will not be required for D&amp;B paper submissions. Submissions to this track will be**reviewed according to a set of criteria and best practices specifically designed for datasets and benchmarks**, as described below.**Authors can choose to submit either single-blind or double-blind**. If it is possible to properly review the submission double-blind, i.e., if reviewers do not need access to non-anonymous repositories to review the work, then authors can also choose to submit the work anonymously.\n**Note:**NeurIPS does not tolerate any collusion whereby authors secretly cooperate with reviewers, ACs, or SACs to obtain favorable reviews.\n2. REQUIRED DATASET AND BENCHMARK CODE SUBMISSION\nThis year,**we are introducing a more stringent set of criteria in the submission process**regarding the hosting, accessibility and documentation of datasets and code**at submission time**:\n* **Hosting:**New****datasets should be hosted at one of the hosting sites dedicated to ML datasets (Dataverse, Kaggle, Hugging Face, or OpenML) or at a bespoke hosting site if the dataset requires it. See the[guidelines for data hosting](https://neurips.cc/Conferences/2025/DataHostingGuidelines)for details. Code should be made accessible in an executable format via a hosting platform (e.g., GitHub, Bitbucket).\u00a0 See the main track[code guidelines](https://neurips.cc/public/guides/CodeSubmissionPolicy)for details.\n* **Access:***Datasets and code should be*[*available and accessible*](https://www.nsf.gov/eng/general/ENG_DMP_Policy.pdf)*to all reviewers, ACs and SACs at the time of submission.*Data should be found and obtained without a personal request to the PI. Code should be documented and executable. Non-compliance justifies the desk rejection of the paper.\n* **Metadata:**Authors should use the[Croissant machine-readable format](https://mlcommons.org/working-groups/data/croissant/)to document their datasets in a machine-readable way and include the Croissant file with their paper submission in OpenReview.\n* **If your data is hosted on one of the dedicated ML data hosting sites**(Kaggle, OpenML, Hugging Face, Dataverse)\n* a Croissant metadata file is automatically generated for you\n* **If you choose to host your data on a different hosting site**\n* you need to generate the Croissant metadata file yourself\n* You can verify the validity of your Croissant file[via this online tool](https://huggingface.co/spaces/JoaquinVanschoren/croissant-checker).\nAll accepted papers should have their code and datasets*documented*and*publicly available by the camera-ready deadline*.\n3. SPECIFIC SCOPE FOR DATASETS &amp; BENCHMARKS PAPER SUBMISSION\nThe NeurIPS 2025 D&amp;B track welcomes all work on*data-centric machine learning research (DMLR)*that enable or accelerate ML research, covering ML datasets and benchmarks as well as algorithms, tools, methods, and analyses for working with ML data. The D&amp;B track is proud to support the open source movement by*encouraging submissions**of open-source libraries and tools*that enable or accelerate ML research.\nThe scope incl...",
      "url": "https://neurips.cc/Conferences/2025/CallForDatasetsBenchmarks"
    },
    {
      "title": "NeurIPS 2025 Schedule",
      "text": "NeurIPS 2025 Schedule\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/calendar)\nShow Detail|\n[Schedule](https://neurips.cc/virtual/2025/calendar)[Sun](https://neurips.cc/virtual/2025/loc/san-diego/day/11/30)[Mon](https://neurips.cc/virtual/2025/loc/san-diego/day/12/1)[Tue](https://neurips.cc/virtual/2025/loc/san-diego/day/12/2)[Wed](https://neurips.cc/virtual/2025/loc/san-diego/day/12/3)[Thu](https://neurips.cc/virtual/2025/loc/san-diego/day/12/4)[Fri](https://neurips.cc/virtual/2025/loc/san-diego/day/12/5)[Sat](https://neurips.cc/virtual/2025/loc/san-diego/day/12/6)[Sun](https://neurips.cc/virtual/2025/loc/san-diego/day/12/7)\n|\nTimezone: America/Los\\_Angeles\n|\nFilter EventsAffinity EventAffinity Poster SessionAwardBreakCompetitionCreative AI SessionEducation Track SessionExhibitor Spot TalksExpo DemonstrationExpo Talk PanelExpo WorkshopInvited TalkMexico City Oral SessionPanelPoster SessionReceptionRegistration DeskRemarksSan Diego Oral SessionSocialStreaming LoungeTest Of TimeTown HallTutorialWorkshop**\nFilter Rooms:Don Alberto 1Don Alberto 2Don Alberto 3Don Alberto 4Exhibit Hall A,BExhibit Hall A,B + Sails PavilionExhibit Hall C,D,EExhibit Hall D FoyerExhibit Hall FExhibit Hall F,G,HExhibit Hall G,HFoyerJulian &amp; SolMezzanine Room 15ABSal 1 (Business Center)San: Exhibit Hall CSocorroTerraceUpper Level Ballroom 20AUpper Level Ballroom 20ABUpper Level Ballroom 20CUpper Level Ballroom 20CDUpper Level Ballroom 20DUpper Level Ballroom 6AUpper Level Ballroom 6ABUpper Level Ballroom 6BUpper Level Ballroom 6CDEFUpper Level Ballroom 6CFUpper Level Ballroom 6DEUpper Level Room 10Upper Level Room 11ABUpper Level Room 1ABUpper Level Room 2Upper Level Room 23ABCUpper Level Room 24ABCUpper Level Room 25ABCUpper Level Room 26ABUpper Level Room 27ABUpper Level Room 28A-EUpper Level Room 29A-DUpper Level Room 3Upper Level Room 30A-EUpper Level Room 31ABCUpper Level Room 32ABUpper Level Room 33ABCUpper Level Room 4Upper Level Room 5ABUpper Level Room 7Upper Level Room 8Upper Level Room 9Upper Level Sails Pavilion**\nMON 1 DEC\nnoon\n[Registration Desk](https://neurips.cc/virtual/2025/registration-desk/129038)\n(ends 6:00 PM)\nTUE 2 DEC\n7:30 a.m.\n[Registration Desk](https://neurips.cc/virtual/2025/registration-desk/129039)\n(ends 6:00 PM)\n8 a.m.\nAffinity Event:\n[Women in Machine Learning](https://neurips.cc/virtual/2025/affinity-event/126050)\n(ends 5:00 PM)\n8:30 a.m.\nExpo Talk Panel:\n[GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection](https://neurips.cc/virtual/2025/128668)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Foundational Generative Recommendations for E-Commerce](https://neurips.cc/virtual/2025/128667)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Beyond Benchmarks: Rethinking Reasoning in Language Models](https://neurips.cc/virtual/2025/128665)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Data\u202fScout: \u201cFrom Prompt to Corpus: Accelerating Domain-Specific Data Collection with LLMGuided Discovery\u201d](https://neurips.cc/virtual/2025/128656)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Multimodal Data Foundation at Industry-Scale](https://neurips.cc/virtual/2025/128659)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Toward General Full Autonomy: Open Research Challenges in Scalable Self-Driving](https://neurips.cc/virtual/2025/128661)\n(ends 9:30 AM)\nExpo Talk Panel:\n[Telling Stories at Scale: Multimodal ML in the Global Media Landscape](https://neurips.cc/virtual/2025/128652)\n(ends 9:30 AM)\n9 a.m.\nBreak:\n[Coffee Break](https://neurips.cc/virtual/2025/break/123910)\n(ends 10:00 AM)\nAffinity Event:\n[LatinX in AI](https://neurips.cc/virtual/2025/affinity-event/126048)\n(ends 5:00 PM)\n9:30 a.m.\nTutorial:\n[Model Merging: Theory, Practice and Applications](https://neurips.cc/virtual/2025/109593)\n(ends 12:00 PM)\nTutorial:\n[Explain AI Models: Methods and Opportunities in Explainable AI, Data-Centric AI, and Mechanistic Interpretability](https://neurips.cc/virtual/2025/109599)\n(ends 12:00 PM)\nTutorial:\n[Human-AI Alignment: Foundations, Methods, Practice, and Challenges](https://neurips.cc/virtual/2025/109592)\n(ends 12:00 PM)\nTutorial:\n[Foundations of Tensor/Low-Rank Computations for AI](https://neurips.cc/virtual/2025/109591)\n(ends 12:00 PM)\nTutorial:\n[Energy and Power as First-Class ML Design Metrics](https://neurips.cc/virtual/2025/109589)\n(ends 12:00 PM)\nTutorial:\n[New Frontiers of Hyperparameter Optimization: Recent advances and open challenges in theory and practice](https://neurips.cc/virtual/2025/109594)\n(ends 12:00 PM)\nTutorial:\n[Planning in the Era of Language Models](https://neurips.cc/virtual/2025/109596)\n(ends 12:00 PM)\n10 a.m.\n[Education Track[10:00-5:00]](https://neurips.cc/virtual/2025/session/129859)\ns10:00-5:00\n[10:00]\nEducation Material Showcase\n[10:00]\nTutorial led by Julien Besset\n[11:00]\nCoffee Break\n[11:30]\nContributed Talk #1 - Application of Multi-Agent Systems for Essay Scoring\n[11:40]\nContributed Talk #2 - Understanding How Neural Networks See\n[11:50]\nContributed Talk #3 - MCP Explorer: Interactive Learning Experience\n[12:00]\nContributed Talk #4 - The Invisible Fingerprints: Protecting Your Digital Images\n[12:10]\nContributed Talk #5 - How to Train a Model: A hands-on, interactive guide to understanding how calculus is used to train AI models.\n[12:20]\nContributed Talk #6 - IoT-MCP: Design and Control IoT Systems via LLMs\n[12:30]\nLunch Break\n[1:30]\nInvited Talk #1 - Serena Booth\n[2:00]\nInvited Talk #2 - Eunice Jun\n[2:30]\nCoffee Break\n[3:00]\nContributed Talk #7 - Understanding Bias and Ethics in AI\n[3:12]\nContributed Talk #8 - Benchmarks 101\n[3:24]\nContributed Talk #9 - AI-Powered Agritourism Toolkit: Revolutionizing Agritourism with AI and Machine Learning\n[3:36]\nContributed Talk #10 - Interpreting Large Language Models (LLMs) with Geometry\n[3:48]\nContributed Talk #11 - AI &amp; Human Co-Creativity: LLM Hallucinations are Features, Prompts are Experiments, AI is not a being\n[4:00]\nPanel Discussion\n(ends 5:00 PM)\nnoon\nExpo Workshop:\n[Large-Scale Real-World Physical AI Systems](https://neurips.cc/virtual/2025/128672)\n(ends 1:30 PM)\nExpo Workshop:\n[Exploring Trust and Reliability in LLM Evaluation](https://neurips.cc/virtual/2025/128673)\n(ends 1:30 PM)\nExpo Workshop:\n[CausalFairness: An Open-Source Python Library for Causal Fairness Analysis](https://neurips.cc/virtual/2025/128677)\n(ends 1:30 PM)\nExpo Workshop:\n[Introduction to Generative Computing](https://neurips.cc/virtual/2025/128678)\n(ends 1:30 PM)\nBreak:\n[Hosted Lunch](https://neurips.cc/virtual/2025/break/123911)\n(ends 1:30 PM)\nExpo Workshop:\n[Multimodal Superintelligence Workshop](https://neurips.cc/virtual/2025/128681)\n(ends 1:30 PM)\nExhibit Sponsor Hall:\n[Exhibit Sponsor Hall](https://neurips.cc/virtual/2025/break/131820)\n(ends 8:00 PM)\nExpo Demonstra...",
      "url": "https://neurips.cc/virtual/2025/calendar"
    }
  ]
}