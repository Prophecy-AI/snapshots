{
  "query": "How do machine learning models achieve good generalization to unseen solvents in chemical reaction yield prediction using transfer learning or domain adaptation?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Good generalization to solvents that were not present in the training set can be obtained by first learning a broad chemical representation on a large source domain (e.g., quantum\u2011chemical calculations or millions of literature reactions) and then adapting that knowledge to the sparse target domain (the experimental yields in the new solvent) with chemistry\u2011aware domain\u2011transformation or active\u2011transfer strategies\u202f([arXiv\u202f2504.02848](https://arxiv.org/abs/2504.02848); [RSC\u202f2022\u202fd1sc06932b](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b); [ChemRxiv\u202f2025\u202f67adcd81](https://chemrxiv.org/engage/chemrxiv/article-details/67adcd81fa469535b9285f22); [JACS\u202f2023\u202f10.1021/jacs.3c10921](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Collect a large, heterogeneous source dataset**  \n   - Gather computational reaction data (e.g., DFT\u2011derived activation energies, catalyst descriptors) or millions of literature reactions that cover many solvents, catalysts, and substrates.  \n   - This provides a rich \u201csimulation\u201d space that the model can learn general chemical patterns from\u202f([arXiv\u202f2504.02848](https://arxiv.org/abs/2504.02848)).\n\n2. **Pre\u2011train a deep model on the source data**  \n   - Use a chemistry\u2011informed architecture (e.g., a recurrent neural network or BERT\u2011style transformer) that encodes reactants, reagents, and solvent descriptors as token sequences.  \n   - Pre\u2011training captures universal reaction\u2011level features and yields a robust latent space\u202f([RSC\u202f2022\u202fd1dd00052g](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00052g); [ChemRxiv\u202f2025\u202f67adcd81](https://chemrxiv.org/engage/chemrxiv/article-details/67adcd81fa469535b9285f22)).\n\n3. **Map the source domain to the target experimental domain**  \n   - Apply a chemistry\u2011informed domain transformation that aligns the simulated feature distribution with the experimental one (e.g., scaling DFT energies, adding solvent polarity indices).  \n   - The transformation leverages known physical relationships between source and target quantities to bridge the gap\u202f([arXiv\u202f2504.02848](https://arxiv.org/abs/2504.02848)).\n\n4. **Fine\u2011tune on a small set of experimental yields in the new solvent**  \n   - Freeze most of the pretrained layers and retrain only the final regression head (or a few shallow layers) using the limited target data.  \n   - Keeping the model shallow (e.g., a few decision\u2011tree ensembles or shallow neural nets) preserves interpretability and prevents over\u2011fitting to the small solvent\u2011specific set\u202f([RSC\u202f2022\u202fd1sc06932b](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)).\n\n5. **Employ active\u2011learning or domain\u2011adversarial training to further reduce solvent bias**  \n   - Iteratively select the most informative reactions (e.g., those with highest prediction uncertainty) to measure experimentally and add to the fine\u2011tuning set, improving coverage of the unseen solvent space.  \n   - Alternatively, train a domain\u2011adversarial classifier that forces the latent representations to be indistinguishable between source and target solvents, encouraging solvent\u2011invariant features\u202f([RSC\u202f2022\u202fd1sc06932b](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)).\n\n6. **Validate on truly unseen solvents**  \n   - Hold out reactions performed in solvents that were never used during pre\u2011training or fine\u2011tuning, and evaluate prediction error (e.g., mean absolute error of yield).  \n   - Successful domain\u2011adapted models typically retain high accuracy because the learned chemistry\u2011level embeddings capture solvent effects implicitly\u202f([JACS\u202f2023\u202f10.1021/jacs.3c10921](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921)).\n\n7. **Deploy the model for rapid solvent screening**  \n   - Use the fine\u2011tuned model to predict yields across a library of candidate solvents; prioritize those with highest predicted yields for experimental testing.  \n   - The workflow leverages the initial large\u2011scale knowledge and minimal new data to generalize reliably to solvents that were never seen during training.  \n\nBy following these steps\u2014pre\u2011training on abundant computational or literature data, chemically aligning source and target domains, fine\u2011tuning with shallow, interpretable heads, and optionally enriching the target set via active learning\u2014machine\u2011learning models can achieve robust generalization to unseen solvents in reaction\u2011yield prediction.",
      "url": ""
    },
    {
      "title": "Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2504.02848** (physics)\n\n\\[Submitted on 20 Mar 2025\\]\n\n# Title:Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation\n\nAuthors: [Yuta Yahagi](https://arxiv.org/search/physics?searchtype=author&query=Yahagi,+Y), [Kiichi Obuchi](https://arxiv.org/search/physics?searchtype=author&query=Obuchi,+K), [Fumihiko Kosaka](https://arxiv.org/search/physics?searchtype=author&query=Kosaka,+F), [Kota Matsui](https://arxiv.org/search/physics?searchtype=author&query=Matsui,+K)\n\nView a PDF of the paper titled Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation, by Yuta Yahagi and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2504.02848) [HTML (experimental)](https://arxiv.org/html/2504.02848v1)\n\n> Abstract:Simulation-to-Real (Sim2Real) transfer learning, the machine learning technique that efficiently solves a real-world task by leveraging knowledge from computational data, has received increasing attention in materials science as a promising solution to the scarcity of experimental data. We proposed an efficient transfer learning scheme from first-principles calculations to experiments based on the chemistry-informed domain transformation, that integrates the heterogeneous source and target domains by harnessing the underlying physics and chemistry. The proposed method maps the computational data from the simulation space (source domain) into the space of experimental data (target domain). During this process, these qualitatively different domains are efficiently bridged by prior knowledge of chemistry, the statistical ensemble and the relationship between source and target quantities. As a proof-of-concept, we predict the catalyst activity for the reverse water-gas shift reaction by using the abundant first-principles data in addition to the experimental data. Through the demonstration, we confirmed that the transfer learning model exhibits positive transfer in accuracy and data efficiency. In particular, a significantly high accuracy was achieved despite using a few (less than ten) target data in domain transformation, whose accuracy is one order of magnitude smaller than that of a full scratch model trained with over 100 target data. This result indicates that the proposed method leverages the high prediction performance with few target data, which helps to save the number of trials in real laboratories.\n\n|     |     |\n| --- | --- |\n| Comments: | 36 pages, 19 figures, 8 tables |\n| Subjects: | Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph) |\n| MSC classes: | 92E99 |\n| ACM\u00a0classes: | I.2.1; J.2 |\n| Cite as: | [arXiv:2504.02848](https://arxiv.org/abs/2504.02848) \\[physics.chem-ph\\] |\n|  | (or [arXiv:2504.02848v1](https://arxiv.org/abs/2504.02848v1) \\[physics.chem-ph\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2504.02848](https://doi.org/10.48550/arXiv.2504.02848)<br>Focus to learn more<br>arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Yuta Yahagi \\[ [view email](https://arxiv.org/show-email/6b078344/2504.02848)\\]\n\n**\\[v1\\]**\nThu, 20 Mar 2025 15:14:23 UTC (6,580 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation, by Yuta Yahagi and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2504.02848)\n- [HTML (experimental)](https://arxiv.org/html/2504.02848v1)\n- [TeX Source](https://arxiv.org/src/2504.02848)\n- [Other Formats](https://arxiv.org/format/2504.02848)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2504.02848&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2504.02848&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2025-04](https://arxiv.org/list/physics.chem-ph/2025-04)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2504.02848?context=cond-mat)\n\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2504.02848?context=cond-mat.mtrl-sci)\n\n[cs](https://arxiv.org/abs/2504.02848?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2504.02848?context=cs.LG)\n\n[physics](https://arxiv.org/abs/2504.02848?context=physics)\n\n[physics.comp-ph](https://arxiv.org/abs/2504.02848?context=physics.comp-ph)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.02848)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.02848)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.02848)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2504.02848&description=Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2504.02848&title=Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv....",
      "url": "https://arxiv.org/abs/2504.02848"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc01662a)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05681f)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D1SC06932B](https://doi.org/10.1039/D1SC06932B)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022, **13**, 6655-6668\n\n# Predicting reaction conditions from limited data through active transfer learning [\u2020](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b\\#fn1)\n\nEunjae\nShim\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-4085-9659)a,\nJoshua A.\nKammeraad\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0386-7198)ab,\nZiping\nXu\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2591-0356)b,\nAmbuj\nTewari\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6969-7844)bc,\nTim\nCernak\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5407-0643)\\*ad and Paul M.\nZimmerman\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-7444-1314)\\*a\n\naDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [paulzim@umich.edu](mailto:paulzim@umich.edu)\n\nbDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\n\ncDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA\n\ndDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [tcernak@med.umich.edu](mailto:tcernak@med.umich.edu)\n\nReceived\n10th December 2021\n, Accepted 10th May 2022\n\nFirst published on 11th May 2022\n\n* * *\n\n## Abstract\n\nTransfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.\n\n* * *\n\n## Introduction\n\nComputers are becoming increasingly capable of performing high-level chemical tasks. [1\u20134](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit1) Machine learning approaches have demonstrated viable retrosynthetic analyses, [5\u20137](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit5) product prediction, [8\u201311](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit8) reaction condition suggestion, [12\u201316](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit12) prediction of stereoselectivity, [17\u201320](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit17) regioselectivity, [19,21\u201324](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) and reaction yield [25,26](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit25) and optimization of reaction conditions. [27\u201330](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit27) These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (e.g. reaction conditions) within a particular domain relate to an outcome (e.g. yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.\n\nExpert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain, [31](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) emulates a tactic commonly employed by human chemists.\n\nTransfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain. [31,32](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Models are first created using the source data, then transferred to the target domain using various algorithms. [19,33\u201335](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (e.g., descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible. [31,36](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data. [37](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit37) The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.\n\nAnother machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data. [38,39](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit38) Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental result...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b"
    },
    {
      "title": "A transfer learning protocol for chemical catalysis using a recurrent neural network adapted from natural language processing \u2020",
      "text": "A transfer learning protocol for chemical catalysis using a recurrent neural network adapted from natural language processing - Digital Discovery (RSC Publishing) DOI:10.1039/D1DD00052G\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/dd/d1dd00052g)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00043h)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00034a)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D1DD00052G](https://doi.org/10.1039/D1DD00052G)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2022,**1**, 303-312\n# A transfer learning protocol for chemical catalysis using a recurrent neural network adapted from natural language processing[\u2020](#fn1)\nSukriti Singh\\*aandRaghavan B. Sunoj[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-6484-2878)\\*ab\naDepartment of Chemistry, Indian Institute of Technology Bombay, Mumbai 400076, India. E-mail:[sukriti243@gmail.com](mailto:sukriti243@gmail.com);[sunoj@chem.iitb.ac.in](mailto:sunoj@chem.iitb.ac.in)\nbCentre for Machine Intelligence and Data Science, Indian Institute of Technology Bombay, Mumbai 400076, India\nReceived 21st December 2021, Accepted 11th April 2022\nFirst published on 11th April 2022\n## Abstract\nMinimizing the time and material investments in discovering molecular catalysis would be immensely beneficial. Given the high contemporary importance of homogeneous catalysis in general, and asymmetric catalysis in particular, makes them the most compelling systems for leveraging the power of machine learning (ML). We see an overarching connection between the powerful ML tools such as the transfer learning (TL) used in natural language processing (NLP) and the chemical space, when the latter is described using the SMILES strings conducive for representation learning. We developed a TL protocol, trained on 1 million molecules first, and exploited its ability for accurate predictions of the yield and enantiomeric excess for three diverse reaction classes, encompassing over 5000 transition metal- and organo-catalytic reactions. The TL predicted yields in the Pd-catalyzed Buchwald\u2013Hartwig cross-coupling reaction offered the highest accuracy, with an impressive RMSE of 4.89 implying that 97% of the predicted yields were within 10 units of the actual experimental value. In the case of catalytic asymmetric reactions, such as the enantioselectiveN,S-acetal formation and asymmetric hydrogenation, RMSEs of 8.65 and 8.38 could be obtained respectively, with the predicted enantioselectivities (%ee) within 10 units of its true value in \u223c90% of the time. The method is highly time-economic as the workflow bypasses collecting the molecular descriptors and hence of direct implication to high throughput discovery of catalytic transformations.\n## Introduction\nChemical catalysis is a vibrant domain of research pursued alike in industry and academia owing to its importance in energy, automobile, fine chemicals, pharmaceuticals and so on.[1,2](#cit1)The drive to invent newer catalytic protocols or to impart superior efficiency to the known processes has been perpetual.[3,4](#cit3)The design of new catalysts, such as for homogeneous molecular catalysis, has remained in the forefront for decades.[5,6](#cit5)Such efforts are generally guided by chemical intuition and might even demand tiresome loops of trial and error.[7](#cit7)In recent years, the traditional approaches in catalysis were augmented by tools such as linear regression,[8](#cit8)machine learning (ML),[9](#cit9)active learning,[10](#cit10)and robotics;[11,12](#cit11)all seem to point to an emerging synergism in reaction discovery.[13\u201315](#cit13)\nFrom a sustainability standpoint, it is high time that we endeavor to develop faster, reliable, and less resource intensive (e.g., time and material) invention workflows. To make this goal more realistic, ML driven protocols have a highly promising role to play.[16,17](#cit16)The yield and enantiomeric excess are countable indicators of how good a (asymmetric)catalytic method is, particularly in its developmental stage. One would inevitably encounter a high-dimensional chemical space composed of relevant molecular features of catalysts, substrates, solvent, additives,etc., for training ML models to predict the yields/ee of catalytic reactions.[18,19](#cit18)For instance, Rothenberget al.built a classification and regression model for predicting the turnover number (TON) and turnover frequency (TOF) for 412 Heck reactions.[20](#cit20)A total of 74 physical organic descriptors were employed for the reaction components such as the substrate, ligand, solvent and so on in addition to the inclusion of reaction conditions (time, temperature, catalyst loading,etc.). The artificial neural networks were found to perform better than the linear regression techniques. The trained model was then utilized to predict the TON and TOF of a virtual library (in silico) of 60![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)000 Heck reactions. In the recent years, there has been a visible increase in efforts in developing new molecular representations capable of improved performance and generalizability. Several approaches, other than those relying on quantum chemically derived molecular descriptors, have emerged. These methods primarily involve the use of various genres of structure-based representations.[21\u201323](#cit21)\nThe representation learning methods such as the deep neural networks (DNNs) built on engineered features have found profound applications in chemical space.[24,25](#cit24)DNNs can also be trained on molecular representations, such as the SMILES (simplified molecular input line entry system) strings, to learn the feature representation involving minimal feature engineering.[26](#cit26)This approach can grasp the underlying patterns in atomic connectivity and capture the relationship between such features and molecular properties. During the developmental phase in catalysis research, only smaller or fragmented datasets are typically available, thereby necessitating a work-around, should one choose to deploy DNNs. It may be possible that tools from seemingly disparate domains become valuable for a task at hand, provided that shared or latent characteristics exist between them.\nNatural language processing (NLP) is one of the most visible domains of artificial intelligence that provides computers the capability to generate and analyze text/speech.[27](#cit27)The large/labeled data requirements in NLP could be circumvented by using transfer learning methods,[28](#cit28)wherein the knowledge acquired from one task (source task) is retained and subsequently utilized for other related tasks (target task). Therefore, NLP could be deployed for those tasks that rely on the language or similar textual data. The SMILES representation of molecules can be considered analogous to a natural language. In fact, interesting applications of NLP-based methods to chemical reactions are now becoming available.[29\u201331](#cit29)The use of NLP-based models for accurate prediction of various properties of molecules is well-known.[32,33](#cit32)On the other hand, predicting the reaction outcome that is known to depend on the molecular attributes of catalysts, reactants, solvents and several other factors is challenging and has seldom been reported using language models.\nCurrently, most of the ML models f...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00052g"
    },
    {
      "title": "Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization,  Molecular Representation, and Pre-training Data Augmentation",
      "text": "Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pre-training Data Augmentation | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pre-training Data Augmentation\n19 February 2025, Version 1\nWorking Paper\n## Authors\n* [Adrian Krzyzanowski](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Adrian%20Krzyzanowski)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-3604-9274),\n* [Stephen Pickett](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stephen%20Pickett),\n* [Peter Pog\u00e1ny](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Peter%20Pog%C3%A1ny)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-3536-0746)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nPredicting reaction yields in synthetic chemistry remains a significant challenge. This study systematically evaluates the impact of tokenization, molecular representation, pre-training data, and adversarial training on a BERT-based model for yield prediction of Buchwald-Hartwig and Suzuki-Miyaura coupling reactions using publicly available HTE datasets. We demonstrate that molecular representation choice (SMILES, DeepSMILES, SELFIES, Morgan fingerprint-based notation, IUPAC names) has minimal impact on model performance, while typically BPE and SentencePiece tokenization outperform other methods. WordPiece is strongly discouraged for SELFIES and fingerprint-based notation. Furthermore, pre-training with relatively small data sets (&lt;100K reactions) achieves comparable performance to larger data sets containing millions of examples. A use of artificially generated domain-specific pre-training data is proposed. The artificially generated sets prove to be a good surrogate to the reaction schemes extracted from reaction data sets such as Pistachio or Reaxys. The best performance was observed for hybrid pre-training sets combining the real and the domain-specific, artificial data. Finally, we show that a novel adversarial training approach, perturbing input embeddings dynamically, improves model robustness and generalisability for yield and reaction success prediction. These findings provide valuable insights for developing robust and practical machine learning models for yield prediction in synthetic chemistry. GSK\u2019s BERT training code base is made available to the community with this work.\n## Keywords\n[Predictive Synthesis](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Predictive%20Synthesis)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[BERT](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=BERT)\n[Molecular Representation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20Representation)\n[Buchwald-Hartwig Reaction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Buchwald-Hartwig%20Reaction)\n[Suzuki-Miyaura Reaction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Suzuki-Miyaura%20Reaction)\n[Adversarial Training](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Adversarial%20Training)\n[Data Augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=%20Data%20Augmentation)\n[Artificial Data](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Artificial%20Data)\n## Supplementary materials\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\nSupporting Information for the Main Manuscript\n**Description**\nFile containing supporting results presented in tables and figures.\n**Actions**\n**Download(6 MB)**\n**Title**\nArtificially Generated Reaction Schemes\n**Description**\nFile containing artificially generated Suzuki-Miyaura and Buchwald-Hartwig reaction schemes. Training and validation data are provided.\n**Actions**\n**Download(141 MB)**\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nSynthCoder GitHub Repository\n**Description**\nGitHub repository for the GSK's SynthCoder code base.\n**Actions**\n[**View**](https://github.com/gskcheminformatics/SynthCoder)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\nFeb 19, 2025 Version 1\n## Metrics\n1,829\n954\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2025-nq3xn\nD O I: 10.26434/chemrx...",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67adcd81fa469535b9285f22"
    },
    {
      "title": "Spectroscopy-Guided Deep Learning Predicts Solid\u2013Liquid Surface Adsorbate Properties in Unseen Solvents",
      "text": "![](https://d.adroll.com/cm/b/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/g/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/index/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/n/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/o/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/outbrain/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/pubmatic/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/r/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/taboola/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/triplelift/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/x/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)\n\nRecently Viewed [close modal](javascript:void(0))\n\nRecently Viewed\n\n#### You have not visited any articles yet, Please visit some articles to see contents here.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nYou\u2019ve supercharged your research process with ACS and Mendeley!\n\nContinue\n\n###### STEP 1:\n\nLogin with ACS IDLogged in SuccessClick to create an ACS ID\n\n###### STEP 2:\n\nLogin with MendeleyLogged in Success [Create a Mendeley account](https://id.elsevier.com/as/authorization.oauth2?state=c33c27125763433d4d32a15accaacc18&prompt=login&scope=openid%20email%20profile%20els_auth_info&authType=SINGLE_SIGN_IN&response_type=code&platSite=MDY%2Fmendeley&redirect_uri=https%3A%2F%2Fwww.mendeley.com%2Fcallback%2F&client_id=MENDELEY)\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease login with your ACS ID before connecting to your Mendeley account.\n\nLogin with ACS ID\n\nMENDELEY PAIRING EXPIREDReconnect\n\nYour Mendeley pairing has expired. Please reconnect\n\n![Figure 1](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921)![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n\n[Download Hi-Res Image](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) [Download to MS-PowerPoint](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) [**Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/jacs.3c10921&href=/doi/full/10.1021/jacs.3c10921) _J. Am. Chem. Soc._ 2024, 146, 1, 811-823\n\n[ADVERTISEMENT](http://acsmediakit.org)\n\n[RETURN TO ISSUE](https://pubs.acs.org/toc/jacsat/146/1) [PREV](https://pubs.acs.org/doi/10.1021/jacs.3c10864) Article [NEXT](https://pubs.acs.org/doi/10.1021/jacs.3c10963)\n\n[![Journal Logo](https://pubs.acs.org/doi/full/10.1021/specs/products/achs/releasedAssets/images/loading/loader-128b5db1cc3a83761a15cf2e5c9b452d.gif)](https://pubs.acs.org/journal/jacsat)\n\n[Get e-Alerts](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) close\n\n# Spectroscopy-Guided Deep Learning Predicts Solid\u2013Liquid Surface Adsorbate Properties in Unseen Solvents\n\n- Wenjie Du\n\n\n\n\nWenjie Du\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Wenjie Du](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Wenjie++Du)\n\n- ,\n- Fenfen Ma\n\n\n\n\nFenfen Ma\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nGusu Laboratory of Materials, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Fenfen Ma](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Fenfen++Ma)\n\n- ,\n- Baicheng Zhang\n\n\n\n\nBaicheng Zhang\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\n\n\nMore by [Baicheng Zhang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Baicheng++Zhang)\n\n\n\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-1899-028X](https://orcid.org/0000-0002-1899-028X)\n\n- ,\n- Jiahui Zhang\n\n\n\n\nJiahui Zhang\n\n\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Jiahui Zhang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Jiahui++Zhang)\n\n- ,\n- Di Wu\n\n\n\n\nDi Wu\n\n\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Di Wu](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Di++Wu)\n\n- ,\n- Edward Sharman\n\n\n\n\nEdward Sharman\n\n\n\n\n\nDepartment of Neurology, University of California, Irvine, California 92697, United States\n\n\n\n\n\nMore by [Edward Sharman](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Edward++Sharman)\n\n- ,\n- Jun Jiang **\\***\n\n\n\n\nJun Jiang\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\n**\\*** Email: [jiangj1@ustc.edu.cn](mailto:jiangj1@ustc.edu.cn)\n\nMore by [Jun Jiang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Jun++Jiang)\n\n\n\n![Orcid](https://pubs.acs.org/products/a...",
      "url": "https://pubs.acs.org/doi/full/10.1021/jacs.3c10921"
    },
    {
      "title": "",
      "text": "Predicting reaction conditions from limited data\nthrough active transfer learning\u2020\nEunjae Shim, a Joshua A. Kammeraad, ab Ziping Xu, b Ambuj Tewari, bc\nTim Cernak *ad and Paul M. Zimmerman *a\nTransfer and active learning have the potential to accelerate the development of new chemical reactions,\nusing prior data and new experiments to inform models that adapt to the target area of interest. This article\nshows how specifically tuned machine learning models, based on random forest classifiers, can expand the\napplicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First,\nmodel transfer is shown to be effective when reaction mechanisms and substrates are closely related, even\nwhen models are trained on relatively small numbers of data points. Then, a model simplification scheme is\ntested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen\nreagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit\nover random selection, an active transfer learning strategy is introduced to improve model predictions.\nSimple models, composed of a small number of decision trees with limited depths, are crucial for\nsecuring generalizability, interpretability, and performance of active transfer learning.\nIntroduction\nComputers are becoming increasingly capable of performing\nhigh-level chemical tasks.1\u20134 Machine learning approaches have\ndemonstrated viable retrosynthetic analyses,5\u20137 product predic\u0002tion,8\u201311 reaction condition suggestion,12\u201316 prediction of ster\u0002eoselectivity,17\u201320 regioselectivity,19,21\u201324 and reaction yield25,26\nand optimization of reaction conditions.27\u201330 These advances\nallow computers to assist synthesis planning for functional\nmolecules using well-established chemistry. For machine\nlearning to aid the development of new reactions, a model\nbased on established chemical knowledge must be able to\ngeneralize its predictions to reactivity that lies outside of the\ndataset. However, because most supervised learning algorithms\nlearn how features (e.g. reaction conditions) within a particular\ndomain relate to an outcome (e.g. yield), the model is not ex\u0002pected to be accurate outside its domain. This situation\nrequires chemists to consider other machine learning methods\nfor navigating new reactivity.\nExpert knowledge based on known reactions plays a central\nrole in the design of new reactions. The assumption that\nsubstrates with chemically similar reaction centers have trans\u0002ferable performance provides a plausible starting point for\nexperimental exploration. This concept of chemical similarity,\ntogether with literature data, guides expert chemists in the\ndevelopment of new reactions. Transfer learning, which\nassumes that data from a nearby domain, called the source\ndomain, can be leveraged to model the problem of interest in\na new domain, called the target domain,31 emulates a tactic\ncommonly employed by human chemists.\nTransfer learning is a promising strategy when limited data\nis available in the domain of interest, but a sizeable dataset is\navailable in a related domain.31,32 Models are \ue103rst created using\nthe source data, then transferred to the target domain using\nvarious algorithms.19,33\u201335 For new chemical targets where no\nlabeled data is available, the head start in predictivity a source\nmodel can provide becomes important. However, when a shi\ue09d\nin distribution of descriptor values occurs (e.g., descriptors\noutside of the original model ranges) in the target data, making\npredictions becomes challenging. For such a situation, the\nobjective of transfer learning becomes training a model that is\nas predictive in the target domain as possible.31,36 Toward this\nend, cross-validation is known to improve generalizability by\nproviding a procedure to avoid over\ue103tting on the training data.37\nThe reduction of generalization error, however, may not be\nsufficient outside the source domain. Accordingly, new\nmethods that enhance the applicability of a transferred model\nto new targets would be bene\ue103cial for reaction condition\nprediction.\nAnother machine learning method that can help tackle data\nscarcity is active learning. By making iterative queries of\na\nDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail:\npaulzim@umich.edu\nb\nDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\nc\nDepartment of Electrical Engineering and Computer Science, University of Michigan,\nAnn Arbor, MI, USA\nd\nDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA.\nE-mail: tcernak@med.umich.edu\n\u2020 Electronic supplementary information (ESI) available: Additional results. See\nhttps://doi.org/10.1039/d1sc06932b.\nCite this: Chem. Sci., 2022, 13, 6655\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 10th December 2021\nAccepted 10th May 2022\nDOI: 10.1039/d1sc06932b\nrsc.li/chemical-science\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2022, 13, 6655\u20136668 | 6655\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 11 May 2022. Downloaded on 1/15/2026 9:04:44 AM. This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence. View Article Online View Journal | View Issue\nlabeling a small number of datapoints, active learning updates\nmodels with knowledge from newly labeled data. As a result,\nexploration is guided into the most informative areas and\navoids collection of unnecessary data.38,39 Active learning is\ntherefore well-suited for reaction development, which greatly\nbene\ue103ts from efficient exploration and where chemists conduct\nthe next batch of reactions based on previous experimental\nresults. Based on this analogy, reaction optimization27,28 and\nreaction condition identi\ue103cation40 have been demonstrated to\nbene\ue103t from active learning. However, these prior works initiate\nexploration with randomly selected data points (Fig. 1A) which\ndoes not leverage prior knowledge, and therefore does not\nre\ue104ect how expert chemists initiate exploration. Initial search\ndirected by transfer learning could identify productive regions\nearly on, which in turn will help build more useful models for\nsubsequent active learning steps.\nTo align transfer and active learning closer to how expert\nchemists develop new reactions, appropriate chemical reaction\ndata is necessary.41 Available datasets42 that are o\ue09den used for\nmachine learning are overrepresented by positive reactions,\nfailing to re\ue104ect reactions with negative outcomes. On the other\nhand, reaction condition screening data of methodology\nreports\u2014which chemists o\ue09den refer to\u2014only constitute\na sparse subset of possible reagent combinations, making it\nhard for machine learning algorithms to extract meaningful\nknowledge.43\nHigh-throughput experimentation44\u201346 (HTE) data can \ue103ll\nthis gap. HTE provides reaction data16,25,27,47,48 with reduced\nvariations in outcome due to systematic experimentation. Pd\u0002catalyzed coupling data was therefore collected from reported\nwork using nanomole scale HTE in 1536 well plates.49\u201351 In the\ncurrent work, subsets of this data, classi\ue103ed by nucleophile type\nas shown in Fig. 2A, were selected to a dataset size of approxi\u0002mately 100 datapoints, which captured both positive and\nnegative reaction performance.\nReaction condition exploration could be made more efficient\nif algorithmic strategies could leverage prior knowledge. Toward\nthis goal, model transfer and its combination with active\nlearning were evaluated. Taking advantage of diverse campaigns,\nthis study will show that transferred models can be effective in\napplying prior reaction conditions to a new substrate type under\ncertain conditions. Next, the source model's ability to predict\nreaction conditions with new combinations of reagents will also\nbe evaluated. Lastly, challenging scenarios are considered where\nproductive reaction conditions for one class of substrate...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b"
    },
    {
      "title": "Generalizing property prediction of ionic liquids from limited labeled data: a one-stop framework empowered by transfer learning",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2023/dd/d3dd00040k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00147k)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00121g)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D3DD00040K](https://doi.org/10.1039/D3DD00040K)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023, **2**, 591-601\n\n# Generalizing property prediction of ionic liquids from limited labeled data: a one-stop framework empowered by transfer learning [\u2020](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k\\#fn1)\n\nGuzhong\nChen\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0515-8010)ab,\nZhen\nSong\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-9219-1833)\\*a,\nZhiwen\nQi\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2037-2234)\\*a and Kai\nSundmacher\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-3251-0593)bc\n\naState Key Laboratory of Chemical Engineering, School of Chemical Engineering, East China University of Science and Technology, 130 Meilong Road, Shanghai 200237, China. E-mail: [songz@ecust.edu.cn](mailto:songz@ecust.edu.cn); [zwqi@ecust.edu.cn](mailto:zwqi@ecust.edu.cn)\n\nbProcess Systems Engineering, Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstr. 1, D-39106 Magdeburg, Germany\n\ncProcess Systems Engineering, Otto-von-Guericke University Magdeburg, Universit\u00e4tsplatz 2, D-39106 Magdeburg, Germany\n\nReceived\n14th March 2023\n, Accepted 12th May 2023\n\nFirst published on 12th May 2023\n\n* * *\n\n## Abstract\n\nIonic liquids (ILs) could find use in almost every chemical process due to their wide spectrum of unique properties. The crux of the matter lies in whether a task-specific IL selection from enormous chemical space can be achieved by property prediction, for which limited labeled data represents a major obstacle. Here, we propose a one-stop ILTransR (IL transfer learning of representations) that employs large-scale unlabeled data for generalizing IL property prediction from limited labeled data. By first pre-training on \u223c10 million IL-like molecules, IL representations are derived from the encoder state of a transformer model. Employing the pre-trained IL representations, convolutional neural network (CNN) models for IL property prediction are trained and tested on eleven datasets of different IL properties. The obtained ILTransR presents superior performance as opposed to state-of-the-art models in all benchmarks. The application of ILTransR is exemplified by extensive screening of CO2 absorbent from a huge database of 8![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)333![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)096 synthetically-feasible ILs.\n\n* * *\n\n## 1 Introduction\n\nIonic liquids (ILs) are molten salts comprised fully of cations and anions, which can remain in liquid state around room temperature. In recent years, ILs have attracted remarkable attention in various applications, both in chemistry and engineering, [1,2](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit1) due to their unique physicochemical properties such as negligible vapor pressure, high thermal and electrochemical stability, wide liquidus range, etc. [2,3](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit2) More importantly, ILs also offer great potential to tune their physical and chemical properties by judicious selection of the cations and anions. For this reason, ILs could be designed to offer desirable properties to meet specific requirements for arbitrary given applications. The challenge, however, is to accurately evaluate various IL properties related to the target performance and identify optimal ILs from the nearly infinite combinations of possible cations and anions. [4,5](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit4)\n\nSo far, IL selection toward a specific process mainly relies on laborious trial-and-error experiments. However, such approaches are not only very time-consuming but also limited to a small IL chemical space, leaving many potentially promising structures unexplored. Alternatively, computational methods can be used for estimating the properties of ILs and IL-involved mixtures. [6](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit6) Traditional models such as equations of states (EoSs) [7](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit7) and group contribution models (GCMs) [8,9](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit8) have been widely employed for estimating the thermodynamic, transport, and EHS (environment, health, and safety) related properties of ILs. Nevertheless, both the two schemes are prone to the inherent weakness of limited predictive power and/or insufficient accuracy. [8](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit8) Another computational method for IL property prediction is the quantitative structure\u2013property relationship (QSPR) approach, wherein a property of interest is correlated quantitatively with certain descriptors of involved molecules [9,10](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit9) (for which machine learning methods have recently gained popularity [11\u201317](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit11)). Notably, the availability of IL property databases like ILThermo [18](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit18) has stimulated the use of ML methods for modeling IL properties, wherein diverse types of molecular descriptors were used as IL representation. [19\u201326](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit19) However, despite the high accuracy achieved by these models, such models still suffer from the inherent weakness of molecular descriptors for IL representation as well as the relatively limited databases of IL properties available for model development. Moreover, manually engineered IL descriptors usually require expert knowledge of specific types of ILs and the properties to be modeled, which could work well for specific tasks but may not generalize well for others. [27](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit27) In the past few years, there has been rapid progress in ML methods, particularly deep neural networks (DNNs). These DNN-based methods have garnered significant attention due to their ability to overcome the limitations of conventional models and achieve high accuracy in predicting complex tasks. [28\u201331](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit28) The growth of deep learning (DL) has offered excellent flexibility and performance to learn molecular representations from data, without explicit guides from experts. [32\u201334](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit32) Typically, a sufficiently large labeled training dataset is desirable for developing DL approaches. [35](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit35) This is practical in areas like image classification as the number of labeled samples could easily reach se...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k"
    },
    {
      "title": "Transfer learning for solvation free energies: From quantum chemistry to experiments",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS1385894721008925)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S1385894721008925/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-snippets)\n- [References (34)](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-references)\n- [Cited by (108)](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/chemical-engineering-journal)\n\n## [Chemical Engineering Journal](https://www.sciencedirect.com/journal/chemical-engineering-journal)\n\n[Volume 418](https://www.sciencedirect.com/journal/chemical-engineering-journal/vol/418/suppl/C), 15 August 2021, 129307\n\n[![Chemical Engineering Journal](https://ars.els-cdn.com/content/image/1-s2.0-S1385894721X00095-cov150h.gif)](https://www.sciencedirect.com/journal/chemical-engineering-journal/vol/418/suppl/C)\n\n# Transfer learning for solvation free energies: From quantum chemistry to experiments\n\nAuthor links open overlay panelFlorence H.Vermeire, [William H.Green](https://www.sciencedirect.com/author/7402259989/william-h-green)\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.cej.2021.129307](https://doi.org/10.1016/j.cej.2021.129307) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S1385894721008925&orderBeanReset=true)\n\n## Highlights\n\n- \u2022\nMachine learning for solvation free energies.\n\n- \u2022\nDatabase of COSMO-RS calculations and experimental solvation free energies.\n\n- \u2022\nAleatoric uncertainty or test data noise as a limit to model performance.\n\n- \u2022\nTransfer learning to compensate for small dataset sizes.\n\n- \u2022\nTransfer learning for improved out-of-sample predictions.\n\n\n## Abstract\n\nData scarcity, bias, and experimental noise are all frequently encountered problems in the application of deep learning to chemical and material science disciplines. Transfer learning has proven effective in compensating for the lack in data. The use of quantum calculations in machine learning enables the generation of a diverse dataset and ensures that learning is less affected by noise inherent to experimental databases. In this work, we propose a transfer learning approach for the prediction of solvation free energies that combines fundamentals from quantum calculations with the higher accuracy of experimental measurements using two new databases CombiSolv-QM and CombiSolv-Exp. The employed model architecture is based on the directed-message passing neural network for the molecular embedding of solvent and solute molecules. A significant advantage of models pre-trained on quantum calculations is demonstrated for small experimental datasets and for out-of-sample predictions. The improved out-of-sample performance is shown for new solvents, for new solute elements, and for the extension to higher molar mass solutes. The overall performance of the pre-trained models is limited by the noise in the experimental test data, known as the aleatoric uncertainty. On a random test split, a mean absolute error of 0.21 kcal/mol is achieved. This is a significant improvement compared to the mean absolute error of the quantum calculations (0.40 kcal/mol). The error can be further reduced to 0.09 kcal/mol if the model performance is assessed on a more accurate subset of the experimental data.\n\n## Introduction\n\nDeep learning has emerged as an effective technique for property prediction in the field of chemical engineering and material science. In the last decade, many efforts have been made to replace structure-based estimation methods by deep neural networks\u00a0\\[1\\], \\[2\\]. One major problem that is often encountered is data scarcity. Compared to other disciplines like image recognition and natural language processing, the availability and size of datasets in chemical engineering and material science are very limited. Transfer learning has been proposed as a technique to solve the problem of the low data regime\u00a0\\[3\\]. Success has been demonstrated in other disciplines, such as the transfer of knowledge from general image recognition to more specific medical imaging. Data scarcity is not the only problem related to the experimental nature of databases in chemical engineering and material science. They are often biased towards certain groups of components, cover only a limited domain of chemical space, and have an uncertainty associated with the experimental nature of the data. With transfer learning and the use of quantum chemical calculations, one can compensate for this bias and cover a larger chemical space by generating additional and diverse data.\n\nThe advantage of transfer learning with respect to data scarcity in chemical engineering and material science has been demonstrated in recent work. Within quantum machine learning, transfer learning has been used to calculate thermodynamic properties of molecules in vacuum at the coupled cluster level of theory, the gold standard of quantum chemical calculations. This has been done by Grambow et\u00a0al.\u00a0\\[4\\] and Smith et\u00a0al.\u00a0\\[5\\] Large DFT-based datasets have been used to pre-train models that were further fine-tuned on computationally expensive coupled cluster calculations. Ma et\u00a0al.\u00a0\\[6\\] demonstrated the advantage of transfer learning for gas adsorption on metal organic frameworks. Parameters were transferred from a model trained on a large dataset of hydrogen gas adsorption at 100 bar and 243 K to initialize the parameters of a model fine-tuned at 130 K and for methane adsorption with a smaller dataset. This technique has also been effective at transferring knowledge between disciplines of materials. Yamada et\u00a0al.\u00a0\\[7\\] proposed a shot-gun transfer learning approach where models trained on small molecules were used to aid learning of polymer properties, and models trained on organic materials were used to aid learning of inorganic material properties. Jha et\u00a0al.\u00a0\\[8\\] used transfer learning to predict the formation enthalpy of crystal structures starting from the elemental composition. In their approach, a model was pre-trained on a large dataset of DFT calculations. All model parameters were used to initialize a new model that was fine-tuned on two other smaller DFT databases and an experimental database.\n\nIn this work, we introduce an inductive transfer learning approach with the transfer of model parameters from models trained on quantum chemistry calculations to models trained on experimental data, similar to the approach reported by Jha et\u00a0al.\u00a0\\[8\\] The transfer learning algorithm uses inductive biases from the quantum chemistry data to improve learning of small and biased experimental datasets. In the present work, the transfer learning method is applied to the prediction of solvation free energies in a variety of solvents. For the purpose of this work we provide two databases in supplementary material: (i) CombiSolv-QM - a quantum chemistry database with solvation free energies for 1 million solvent/solute combinations, calculated according to the COSMO-RS theory, and (ii) CombiSolv-Exp - an experimental database with 10\u00a0145 solvent/solute combinations c...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925"
    }
  ]
}