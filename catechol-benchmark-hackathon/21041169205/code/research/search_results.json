{
  "query": "What are the top leaderboard scores in the Catechol Benchmark Hackathon NeurIPS 2025 Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The available sources do not provide the top leaderboard scores for the Catechol Benchmark Hackathon\u202fNeurIPS\u202f2025 Kaggle competition. ([arxiv.org](https://arxiv.org/abs/2506.07619), [kaggle.com](https://www.kaggle.com/competitions/konwinski-prize/leaderboard))",
      "url": ""
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions/konwinski-prize/leaderboard"
    },
    {
      "title": "NeurIPS 2025 Awards",
      "text": "NeurIPS 2025 AwardsNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://nips.cc/static/core/img/neurips-navbar-logo.svg)](https://nips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://nips.cc/FAQ)\n* [Contact NeurIPS](https://nips.cc/Help/Contact)\n* [Code of Ethics](https://nips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://nips.cc/public/CodeOfConduct)\n* [Create Profile](https://nips.cc/Profile/create)\n* [Journal To Conference Track](https://nips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://nips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://nips.cc/Conferences/FutureMeetings)\n* [Press](https://nips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://nips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://nips.cc/public/PrivacyPolicy)\n* [Downloads](https://nips.cc/Downloads)\n* [My Stuff](https://nips.cc/MyStuff)\n**\n[Login](https://nips.cc/accounts/login?nextp=/virtual/2025/loc/mexico-city/poster/116816)\n* ![San Diego graphic](https://nips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://nips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://nips.cc/virtual/2025/awards_detail)\nTest of Time Award\n|\n[Test of Time Award](https://nips.cc/virtual/2025/test-of-time/128328)\nTest Of Time\nShaoqing Ren &middot; Kaiming He &middot; Ross Girshick &middot; Jian Sun\n**Exhibit Hall F,G,H\n**Abstract\n[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Test of Time Award)](https://papers.neurips.cc/paper\\_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html) \\*haoqing Ren, Kaiming He, Ross Girshick, Jian Sun\\*\r&lt;&lt;strong&gt;&gt;Paper Abstract&lt;&lt;/strong&gt;&gt;: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps \u2026|\nBest Paper Runner-up\n|\n[Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://nips.cc/virtual/2025/poster/119944)\n{location} Oral Poster\nYang Yue &middot; Zhiqi Chen &middot; Rui Lu &middot; Andrew Zhao &middot; Zhaokai Wang &middot; Yang Yue &middot; Shiji Song &middot; Gao Huang\n**Exhibit Hall C,D,E\n![thumbnail](https://nips.cc/media/PosterPDFs/NeurIPS%202025/119944-thumb.png?t=1762183461.1122088)**Abstract\nReinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \\\\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\\\textit{k} at large \\\\textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\\\emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of $k$ (\\\\eg, $k$=1), base models \u2026|\nBest Paper\n|\n[Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://nips.cc/virtual/2025/poster/120216)\n{location} Oral Poster\nZihan Qiu &middot; Zekun Wang &middot; Bo Zheng &middot; Zeyu Huang &middot; Kaiyue Wen &middot; Songlin Yang &middot; Rui Men &middot; Le Yu &middot; Fei Huang &middot; Suozhi Huang &middot; Dayiheng Liu &middot; Jingren Zhou &middot; Junyang Lin\n**Exhibit Hall C,D,E\n![thumbnail](https://nips.cc/static/core/img/NeurIPS-logo.svg)**Abstract\nGating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification\u2014applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)\u2014consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find \u2026|\nBest Paper\n|\n[1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities](https://nips.cc/virtual/2025/poster/115731)\n{location} Oral Poster\nKevin Wang &middot; Ishaan Javali &middot; Micha\u0142 Bortkiewicz &middot; Tomasz Trzcinski &middot; Benjamin Eysenbach\n**Exhibit Hall C,D,E\n![thumbnail](https://nips.cc/static/core/img/NeurIPS-logo.svg)**Abstract\nScaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\\\\times$ -- $50\\\\times$, outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors \u2026|\nBest Paper\n|\n[Why Diffusion Models Don\u2019t Memorize: The Role of Implicit Dynamical Regularization in Training](https://nips.cc/virtual/2025/poster/119372)\n{location} Oral Poster\nTony Bonnaire &middot; Rapha\u00ebl Urfin &middot; Giulio Biroli &middot; Marc Mezard\n**Exhibit Hall C,D,E\n![thumbnail](https://nips.cc/static/core/img/NeurIPS-logo.svg)**Abstract\nDiffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigat...",
      "url": "https://nips.cc/virtual/2025/awards_detail"
    },
    {
      "title": "NeurIPS 2025 San Diego Datasets & Benchmarks",
      "text": "NeurIPS 2025 San Diego Datasets &amp; BenchmarksNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/events/datasets-benchmarks-2025)\n# San Diego Datasets &amp; Benchmarks\n**494 Events\n**\nPoster### [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://neurips.cc/virtual/2025/poster/121603)\nKuicai Dong &middot; CHANG YUJING &middot; Shijie Huang &middot; Yasheng Wang &middot; Ruiming Tang &middot; Yong Liu\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nDocument Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration. Key findings reveal that advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121603)\nPoster### [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://neurips.cc/virtual/2025/poster/121386)\nHyungyung Lee &middot; Geon Choi &middot; Jung-Oh Lee &middot; Hangyul Yoon &middot; Hyuk Hong &middot; Edward Choi\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nRecent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121386)\nPoster### [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://neurips.cc/virtual/2025/poster/121553)\nAtharva Gundawar &middot; Som Sagar &middot; Ransalu Senanayake\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nVision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that is largely unverified. To perform actions reliably, robots must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state like being closed). Despite their ubiquitous use in manipulation, we argue that off-the-shelf VLMs may lack this granular, physically-grounded understanding, as these specific prerequisites are often overlooked during training. Addressing this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of these core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with more than 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, 1\u20133 affordances defined per object class), 100 real-world humanoid view scenarios, and 120 unique simulated constraint scenarios across four tasks. Our evaluations reveal significant gaps in the ability of VLMs to grasp fundamental physical concepts, underscoring their current limitations for reliable robot manipulation and pointing to key areas that require targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating the physical reasoning capabilities of VLMs guiding the development of more robust and physically grounded models for robot manipulation.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121553)\nPoster### [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://neurips.cc/virtual/2025/poster/121706)\nXinran Wang &middot; Songyu Xu &middot; Shan Xiangxuan &middot; Yuxuan Zhang &middot; Muxi Diao &middot; Xueyan Duan &middot; Yanhua huang &middot; Kongming Liang &middot; Zhanyu Ma\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nCinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and repro...",
      "url": "https://neurips.cc/virtual/2025/loc/san-diego/events/datasets-benchmarks-2025"
    },
    {
      "title": "Leaderboard",
      "text": "Leaderboard | EEG Challenge (2025)\n**Update:**\ud83c\udfc6 The final results are out! Check them out in the[Leaderboard page](https://eeg2025.github.io/leaderboard/).\n&times;\n# Leaderboard\n## Final Leaderboard\nThank you so much for participating in the 2025 NeurIPS EEG competition. There were 1,183 teams/participants and more than 8,000 submissions on the open source platform Codabench. CodaBench is the newer version of \u201cCodalab competitions\u201d (ranked first by mlcontests.com, ahead of Kaggle and Tianchi of Alibaba), according to mlcontests.com. Our competition is by far the largest, having been organized on Codabench, so far. Our EEGDash library was installed more than 67,000 times, facilitating open-source data transfer and restructuring for AI/ML training and inference. By every metric, this competition was a tremendous success, and we thank you for your contribution.\nThe jury reviewed the top entries, and no changes were made to the rankings of the leading teams based on that analysis. To maintain transparency and clarify prize distribution, we have updated the final prize structure.\nAs organizers, we made an error by not randomizing samples in Challenge 2, which allowed some teams to exploit the fact that contiguous trials likely came from the same subjects. During office hours and on the forum, several contestants inquired whether using this information was permitted; however, due to a communication breakdown, they received contradictory answers. Even with this advantage, Challenge 2 remained extremely difficult \u2013only three teams achieved scores below 0.99, our threshold for including this challenge in the final scoring (recall that a score of 1 represents predicting the mean target value).\nTherefore, we have decided to award Challenge 1 and Challenge 2 separately. To accommodate this new structure, we secured an additional $2,500 cash prize for the prize pool. Cash prizes and travel awards will go to the top two teams in Challenge 1 and the top two teams in Challenge 2. Thanks to Meta for sponsoring these awards.\nThe winners are as follows:\n**Challenge 1:**\n1. \ud83c\udfc6Team KUL\\_EEG: 0.88668\n2. \ud83e\udd48Team Sigma Nova: 0.90932\n3. \ud83e\udd49Team MIN\\~C\u00b2 (MIND-CICO): 0.91026\n4. Team Meta Brain &amp; AI: 0.9106\n5. Team MBZUAI [dsml.kz]: 0.91394\n6. Team BCILab: 0.91856\n7. Team MostlyBerlin: 0.91946\n8. Team Team Marque: 0.91982\n9. Team CyberBobBeta: 0.92112\n10. Team JLShen: 0.92991\n**Challenge 2:**\n1. \ud83e\udd47Team JLShen: 0.97843\n2. \ud83e\udd48Team MBZUAI [dsml.kz] : 0.98519\n3. \ud83e\udd49Team MIN\\~C\u00b2 (MIND-CICO): 0.98817\nThe**diversity prize**is awarded to team RIML, with a score on Challenge 1 of 0.93106. This award, even if it comes with no monetary reward, is a recognition of the team\u2019s hard work and an encouragement to take on more challenges.\nAgain, thank you for putting so much energy into this challenge. Keep your models ready, as we are planning other challenges.\nThe 2025 EEG Organizing Committee\n## Interactive Leaderboard\nInteractive leaderboard summary from Codabench results.\nSee all[results](https://www.codabench.org/competitions/9975/#/results-tab)on CodaBench.\nLast updated: 2025-11-14T23:11:34Z (UTC)",
      "url": "https://eeg2025.github.io/leaderboard"
    },
    {
      "title": "Announcing the NeurIPS 2025 Best Paper Awards",
      "text": "Announcing the NeurIPS 2025 Best Paper Awards &#8211; NeurIPS Blog[Skip to content](#content)\n[Menu](#mobile-menu)\n[![NeurIPS Blog](https://blog.neurips.cc/wp-content/uploads/neurips-logo-svg.svg)](https://neurips.cc)\n[Search](#)\n[Close Menu](#)\nNovember262025\n# [Announcing the NeurIPS 2025 Best Paper Awards](https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/)\n[Communications Chairs 2025](https://blog.neurips.cc/author/alexlu/)[2025 Conference](https://blog.neurips.cc/category/2025-conference/)\nThe Best Paper Award Committee members were nominated by the Program Chairs and the Database and Benchmark track chairs, who selected leading researchers across machine learning topics. These nominations were approved by the General Chairs and Next Generation and Accessibility Chairs.\nThe best paper award committees were tasked with selecting a handful of highly impactful papers from the Main Track and the Datasets &amp;&amp; Benchmark Track of the conference.\nWith that, we are excited to share the news that the best and runner-up paper awards this year go to seven groundbreaking papers, including four best papers (one of which is from the datasets and benchmarks track) and three runner-ups. The seven papers highlight advances in diffusion model theory, self-supervised reinforcement learning, attention mechanisms for large language models, reasoning capabilities in LLMs, online learning theory, neural scaling laws, and benchmarking methodologies for language model diversity.\nThe winners are presented here in alphabetical order by title.\n[Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://openreview.net/forum?id=saDOrrnNTz)\n# **[Liwei Jiang](https://openreview.net/profile?id=~Liwei_Jiang2),[Yuanjun Chai](https://openreview.net/profile?id=~Yuanjun_Chai1),[Margaret Li](https://openreview.net/profile?id=~Margaret_Li1),[Mickel Liu](https://openreview.net/profile?id=~Mickel_Liu1),[Raymond Fok](https://openreview.net/profile?id=~Raymond_Fok1),[Nouha Dziri](https://openreview.net/profile?id=~Nouha_Dziri2),[Yulia Tsvetkov](https://openreview.net/profile?id=~Yulia_Tsvetkov1),[Maarten Sap](https://openreview.net/profile?id=~Maarten_Sap1),[Yejin Choi](https://openreview.net/profile?id=~Yejin_Choi1)**\n**Abstract**\nLarge language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. To address this gap, we introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., creative content generation, brainstorm &amp; ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that state-of-the-art LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.\n**Reflections from the Selection Committee**\nThis paper makes a substantial and timely contribution to the understanding of diversity, pluralism, and societal impact in modern language models. The authors introduce Infinity-Chat, a rigorously constructed benchmark of 26K real-world open-ended queries paired with 31K dense human annotations, enabling systematic evaluation of creative generation, ideation, and subjective preference alignment, dimensions historically underexamined in AI evaluation. Beyond releasing a valuable dataset, the paper provides deep analytical insights through the first comprehensive taxonomy of open-ended prompts and an extensive empirical study across more than 70 models, revealing the Artificial Hivemind effect: pronounced intra- and inter-model homogenization that raises serious concerns about long-term risks to human creativity, value plurality, and independent thinking. The findings expose critical miscalibration between current reward models, automated judges, and diverse human preferences, highlighting the tension between alignment and diversity and establishing a foundation for future work on preserving heterogeneity in AI systems. Overall, this work sets a new standard for datasets and benchmarks that advance scientific understanding and address pressing societal challenges rather than solely improving technical performance.\n[Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://openreview.net/forum?id=1b7whO4SfY)\n### [Zihan Qiu](https://openreview.net/profile?id=~Zihan_Qiu1),[Zekun Wang](https://openreview.net/profile?id=~Zekun_Wang1),[Bo Zheng](https://openreview.net/profile?id=~Bo_Zheng4),[Zeyu Huang](https://openreview.net/profile?id=~Zeyu_Huang1),[Kaiyue Wen](https://openreview.net/profile?id=~Kaiyue_Wen1),[Songlin Yang](https://openreview.net/profile?id=~Songlin_Yang1),[Rui Men](https://openreview.net/profile?id=~Rui_Men2),[Le Yu](https://openreview.net/profile?id=~Le_Yu2),[Fei Huang](https://openreview.net/profile?id=~Fei_Huang3),[Suozhi Huang](https://openreview.net/profile?id=~Suozhi_Huang1),[Dayiheng Liu](https://openreview.net/profile?id=~Dayiheng_Liu1),[Jingren Zhou](https://openreview.net/profile?id=~Jingren_Zhou1),[Junyang Lin](https://openreview.net/profile?id=~Junyang_Lin1)\n**Abstract**\nGating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification\u2014applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)\u2014consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates massive activation, attention sink and enhances long-context extrapolation performance. We also release related codes ([https://github.com/qiuzh20/gated\\_attention}](https://github.com/qiuzh20/gated_attention})) and ...",
      "url": "https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards"
    },
    {
      "title": "NeurIPS 2025 Best Papers Winners Announced",
      "text": "NeurIPS 2025 Best Papers Winners Announced: Decade-Old Classic Work by He Kaiming, Sun Jian, Etc. Wins Award\nEnglish\n\u4e2d\u6587Deutsch\n[](https://eu.36kr.com/en/p/3570970959395720)[](https://eu.36kr.com/zh/p/3570970959395720)[](https://eu.36kr.com/de/p/3570970959395720)\n[Home](https://eu.36kr.com/en)\nArticle\n# The winners of the best papers at NeurIPS 2025 were announced. A classic work from a decade ago by He Kaiming, Sun Jian, etc. won the award.\n\u65b0\u667a\u51432025-11-27 15:21\nHalf of them are ethnic Chinese.\n**Today, the best papers of NeurIPS 2025 have been announced! Among the four best papers, the majority are from Chinese researchers. The paper \"Faster R-CNN\" proposed by He Kaiming, Sun Jian and others has won the \"Test of Time Award\", which is well-deserved.**\nThe results of the best papers of NeurIPS 2025 are out!\nToday, the organizing committee of NeurIPS announced the list of winners of this year's \"Best Paper\" award. There are a total of four best papers.\n![](https://img.36krcdn.com/hsossms/20251127/v2_5d7862545c6d4d499c1966c71aa45e70@5888275_oswg64254oswg1080oswg387_img_000?x-oss-process=image/format,jpg/interlace,1)\nIn addition, there are also three runner-up papers (Runners Up) that have won awards. These seven award-winning papers span multiple fields:\nTheory of diffusion models, self-supervised RL, attention mechanism, reasoning ability of LLM, theory of online learning, neural scaling, and benchmark evaluation methods for measuring the diversity of language models\nWhat's more significant is that this time, the \"Test of Time Award\" has been awarded to the paper \"Faster R-CNN\", co-authored by Ren Shaoqing, He Kaiming, Ross Gisshick, and Sun Jian.\n![](https://img.36krcdn.com/hsossms/20251127/v2_3b1ab510c3a144339991402fdda8a73e@5888275_oswg31524oswg1080oswg226_img_000?x-oss-process=image/format,jpg/interlace,1)\nThis year is the 39th annual meeting of NeurIPS. Different from previous years, NeurIPS 2025 is the first dual-city conference, which will be held respectively:\nFrom December 2nd to 7th at the San Diego Convention Center\nFrom November 30th to December 5th in Mexico City\n![](https://img.36krcdn.com/hsossms/20251127/v2_26633e1e28434b97bf5296eb8e4b5ef1@5888275_oswg729917oswg574oswg552_img_000?x-oss-process=image/format,jpg/interlace,1)\nCurrently, during the session of the Mexico City branch, the best papers have been announced together.\nLet's take a look at which big names have won the awards?\n## **Among the best papers, Chinese researchers hold a significant position in AI**\n### **Paper 1: Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)**\nAuthors: Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Yejin Choi\nInstitutions: University of Washington, Carnegie Mellon University, Allen Institute for Artificial Intelligence, Lila Sciences, Stanford University\n![](https://img.36krcdn.com/hsossms/20251127/v2_f8dec8fbd03f4f2b9229c7601d2eefe5@5888275_oswg118113oswg1054oswg570_img_000?x-oss-process=image/format,jpg/interlace,1)\nPaper link: https://openreview.net/forum?id=saDOrrnNTz\nLarge language models often struggle to generate diverse and human-like creative content, raising concerns that long-term exposure to homogeneous outputs may lead to the convergence of human thinking.\nHowever, currently, there is still a lack of scalable methods for evaluating the diversity of LM outputs, especially when going beyond narrow tasks such as random number generation or single-model repeated sampling scenarios.\nTo fill this gap, researchers from institutions such as the University of Washington have launched the large-scale dataset Infinity-Chat.\nInfinity-Chat contains 26,000 real-world open-ended user queries that allow multiple reasonable answers to coexist without a single standard solution.\n![](https://img.36krcdn.com/hsossms/20251127/v2_1aa1623c5e804c01a88579828b656748@5888275_oswg579522oswg1078oswg832_img_000?x-oss-process=image/format,jpg/interlace,1)\nFigure 1: Clustering of responses to the query \"Write a metaphor about time\" (visualized by reducing the sentence embeddings to a two-dimensional space through principal component analysis)\nThis is the first time a complete classification system for open-ended prompts of LMs has been proposed, including six top-level categories (such as creative content generation, brainstorming and ideation) and 17 subcategories under them.\n![](https://img.36krcdn.com/hsossms/20251127/v2_f8460328189f4296859a3362273cc3ac@5888275_oswg484180oswg1080oswg707_img_000?x-oss-process=image/format,jpg/interlace,1)\nThrough Infinity-Chat, researchers conducted a large-scale study on the model collapse of LMs and found a significant \"Artificial Hivemind effect\" in open-ended generation, specifically manifested as:\nInternal repetition within the model - a single model continuously generates similar responses;\n![](https://img.36krcdn.com/hsossms/20251127/v2_d287e57182864e9f8375439d1a73cc16@5888275_oswg454430oswg1080oswg628_img_000?x-oss-process=image/format,jpg/interlace,1)\nHomogeneity between models - different models produce surprisingly similar outputs.\n![](https://img.36krcdn.com/hsossms/20251127/v2_8695d52623794674a091772db8345478@5888275_oswg727081oswg1080oswg1151_img_000?x-oss-process=image/format,jpg/interlace,1)\nThe dataset also contains 31,250 human annotations, including absolute ratings and pairwise preference comparisons. Each example was independently judged by 25 annotators, making it possible to study group and individual preferences in open-ended queries.\nThe study shows that the most advanced LMs, reward models, and LM judges, when faced with model-generated results that trigger individual preferences of annotators, although maintaining comparable overall quality, have difficulty calibrating human ratings.\n![](https://img.36krcdn.com/hsossms/20251127/v2_5b51f3a85ed44e1eb30358ba6ea7c529@5888275_oswg253165oswg1074oswg922_img_000?x-oss-process=image/format,jpg/interlace,1)\nOverall, Infinity-Chat is the first large-scale resource for systematically studying real-world open-ended LLM queries, providing key insights for alleviating the long-term AI security risks brought about by the artificial hivemind.\n### **Paper 2: GatedAttentionfor Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free**\nAuthors: Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin\nInstitutions: Alibaba Qwen Team, University of Edinburgh, Stanford University, MIT, Tsinghua University\n![](https://img.36krcdn.com/hsossms/20251127/v2_d6a1bfe76f0f42f0b37207971306be05@5888275_oswg55627oswg1080oswg442_img_000?x-oss-process=image/format,jpg/interlace,1)\nPaper link: https://openreview.net/pdf?id=1b7whO4SfY\nThe gating mechanism has been widely used since the early LSTM and highway networks, and can still be seen in recent state space models, linear attention, and Softmax attention.\nHowever, existing studies rarely conduct in-depth analyses of the specific effects of gating.\nThis study comprehensively explored the gated enhanced Softmax attention variants through systematic experiments: a 15B mixture-of-experts model (30 variants) and a 1.7B dense model were trained on a 3.5 trillion token dataset for comparative analysis.\n![](https://img.36krcdn.com/hsossms/20251127/v2_2b6f939d8c8d4be2b06ebdeb656024fa@5888275_oswg339839oswg1080oswg637_img_000?x-oss-process=image/format,jpg/interlace,1)\nThe core findings show that**simply introducing a head-specific Sigmoid gating after the scaled dot-product attention (SDPA)**can continuously improve the model performance. This improvement also enhances the training stability, allows for a larger learning rate, and improves the scaling characteristics.\n![](https://img.36krcdn.com/hsossms/20251127/v2_963620659b67488eacb9c504b0a14aac@5888275_oswg390080oswg1080oswg786_img_000?x-oss-process=image/format,jpg/interlace,1)\nBy comparing different...",
      "url": "https://eu.36kr.com/en/p/3570970959395720"
    },
    {
      "title": "NeurIPS 2025 Datasets and Benchmarks FAQ",
      "text": "NeurIPS 2025 Datasets and Benchmarks FAQ\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/accounts/login)\n## NeurIPS 2025 Datasets and Benchmarks FAQ\nThis FAQ will be continually updated. Please bookmark this page and review it before submitting any questions.\n**Note:**Authors are also advised to consult the[NeurIPS Main Track FAQs](https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ), as general policies apply to D&amp;B submissions as well.\n### General FAQs\n**What is the LaTeX template for D&amp;B track?**\nIt\u2019s the same as the main track template. Check \u201cPaper Formatting Instructions\u201d here (https://neurips.cc/Conferences/2025/CallForPapers)\n**Are there guidelines for submissions which are from the 2024 Competitions track, e.g., reporting on competition results?**\nNo, there are no special guidelines. Please follow the D&amp;B CFP (this page) and data[hosting guidelines](https://neurips.cc/Conferences/2025/DataHostingGuidelines). Your submission will be reviewed according to the same standards alongside all other D&amp;B track submissions.\n**KDD 2025 notification date is May 16th which is a day after the NeurIPS D&amp;B track full paper submission deadline. Will this change?**\nNo. We will not be making changes to the D&amp;B track deadlines as they align with the main conference.\n**Is my paper a good fit for D&amp;B track?**\nPlease carefully read the CFP (this page) and use your best judgment. Track chairs cannot advise on the relevance of your paper.\n**Are dataset/code submissions due at May 15th (full paper deadline) or May 22nd (appendices/supplementary material deadline)?**\nDatasets/code are not supplementary materials in the D&amp;B track. If your submission includes data/code, it needs to be submitted in full and final form by May 15th along with the full paper submission.\n**What is the LaTeX configuration for a single-blind submission?**\nPlease use \\\\usepackage[preprint]{neurips\\_2025} if you wish to make your submission single-blind for the Datasets &amp;&amp; Benchmarks track.\n**My submission is a benchmark consisting of an environment for evaluation only. Do I need to follow the data-hosting guidelines?**\nNo. If your submission is not a dataset, you do not need to follow data-hosting guidelines. You do need to follow code-hosting guidelines.\n### Dataset hosting FAQs\n**The Croissant format can\u2019t handle the file type(s) in my dataset submission. What should I do?**\nYou should still submit a Croissant file. You can choose to only provide dataset-level metadata and a description of the resources contained in the dataset (FileObject and FileSet). You can omit RecordSets in this scenario. The recommended Croissant-compatible data hosting platforms should handle this gracefully for you, but you will need to manually address this in case you decide to self-host your dataset.\n**I have a submission consisting of multiple datasets, how do I submit the Croissant files?**\nYou should submit a Croissant file for every dataset (and check whether they are all valid). You can combine the .json files into a .zip folder and upload that during submission. In the dataset URL, refer to a webpage that documents the collection of datasets as a whole. The URLs for the individual datasets must be included in the Croissant files.\n**How do we handle our submission which includes a private hold-out set which we wish to keep private and unreleased, e.g., to avoid potential contamination?**\nYou should mention that you have a private hold-out set and describe it in your paper, but the main contribution of your paper should be the publicly released portion of your dataset. The publicly released portion of your dataset needs to conform to the data hosting guidelines. It also may contain a public validation and test set collected with the same protocol as the private one.\n**Can my submission be a synthetic dataset?**\nYes. All data hosting guidelines apply to synthetic datasets, too.\n**How should I include code as part of my submission?**\nPlease see the[guidelines for code](https://neurips.cc/public/guides/CodeSubmissionPolicy). You will be asked to provide a URL to a hosting platform (e.g., GitHub, Bitbucket). If that is not an option, you can alternatively attach it as a ZIP file as supplementary material with your paper. All code should be documented and executable.\n**I don\u2019t want to make my dataset publicly accessible at the time of submission. What are my options?**\nHarvard Dataverse and Kaggle platforms both offer private URL preview link sharing. This means your dataset is only accessible to those with whom you share its special URL, e.g., reviewers.\u00a0Note that you will be required to make your dataset public by the camera-ready deadline. Failure to do so may result in removal from the conference and proceedings.\n**Can I make changes to my dataset after I\u2019ve made my submission to Open Review?**\nYou can make changes until the submission deadline. After the submission deadline, we will perform automated verification checks of your dataset to assist in streamlining and standardizing reviews.\u00a0If it changes in a way that invalidates the original reviews at any time between the submission deadline and by the camera ready deadline or publication of proceedings, we reserve the right to remove it from the conference or proceedings.\n**I\u2019m experiencing problems with the platform I\u2019m using to release my dataset. What should I do?**\nWe have worked with maintainers of the dataset hosting platforms to identify the appropriate contact information authors should use to contact for support in case of issues or help with workarounds for storage quotas, etc. Find this contact information in the section above\u201cHow to Publish on Preferred Hosting Platforms\u201d.\n**I need to require credentialized (AKA gated) access to my dataset**\nThis will be possible on the condition that a credentialization is necessary for the public good (e.g. because of ethically sensitive medical data), and that an established credentialization procedure is in place that is 1) open to a large section of the public; 2) provides rapid response and access to the data; and 3) is guaranteed to be maintained for many years. A good example here is PhysioNet Credentialing, where users must first understand how to handle data with human subjects, yet is open to anyone who has learned and agrees with the rules.\n*This should be seen as an exceptional measure, and NOT as a way to limit access to data for other reasons*(e.g., to shield data behind a Data Transfer Agreement). Misuse would be grounds for desk rejection. During submission, you can indicate that your dataset involves open credentialized access, in which case the necessity, openness, and efficiency of the credentialization process itself will also be checked.\n**Our dataset requires credentialized access. How do we preserve single-blind review, i.e., ensure the identities of reviewers isn\u2019t shared with authors?**\nIf it\u2019s possible to share a private preview...",
      "url": "https://neurips.cc/Conferences/2025/DatasetsBenchmarks-FAQ"
    }
  ]
}