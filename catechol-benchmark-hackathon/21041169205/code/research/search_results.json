{
  "query": "What pre-trained molecular embeddings or transfer learning approaches work best for predicting chemical reaction yields on unseen solvents?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting reaction yields on solvents that were not seen during training benefits most from **reaction\u2011level, condition\u2011aware transfer\u2011learning models** rather than from generic molecular\u2011only embeddings.  A recent benchmark of 25 pretrained molecular\u2011embedding models found that, for most downstream tasks, the classic ECFP fingerprint performed on par with the deep\u2011learning embeddings and only the CLAMP fingerprint gave a modest statistical edge\u202f([arxiv](https://arxiv.org/html/2508.06199v2)).  However, when the target is yield prediction under varying solvents, models that encode the whole reaction string and explicitly learn solvent effects outperform these molecule\u2011centric embeddings.\n\nTwo approaches stand out:\n\n1. **Transformer\u2011based reaction models** such as **ReactionT5**, which is pretrained on the Open Reaction Database and then fine\u2011tuned on yield data, achieve high coefficient\u2011of\u2011determination (R\u00b2\u202f\u2248\u202f0.95) even with limited fine\u2011tuning data\u202f([pmc](https://pmc.ncbi.nlm.nih.gov/articles/PMC12366004)).  The model treats reactants, products, and all reaction conditions\u2014including solvent SMILES\u2014as a single sequence, allowing it to generalise to unseen solvents after fine\u2011tuning.\n\n2. **BERT\u2011style models with reaction\u2011condition contrastive learning**, exemplified by **Egret**, which combines masked\u2011language\u2011model pretraining with a contrastive task that forces the encoder to distinguish reactions that differ only in conditions (e.g., solvent).  Egret matches or exceeds the best prior yield predictors on four benchmark datasets and is specifically reported to capture subtle solvent\u2011dependent yield variations\u202f([spj.science.org](https://spj.science.org/doi/10.34133/research.0292)).\n\nA complementary line of work uses **graph neural networks (GNNs) that ingest continuous solvent descriptors** to learn solvent effects directly from flow data, showing promising performance on a catechol rearrangement benchmark\u202f([arxiv](https://arxiv.org/abs/2512.19530)).  In practice, the most reliable pipeline for unseen solvents is to start from a reaction\u2011level transformer (ReactionT5 or a BERT\u2011based Egret) pretrained on large reaction corpora, optionally augment it with a solvent\u2011contrastive loss or a GNN\u2011based solvent encoder, and then fine\u2011tune on the specific yield dataset.  This combination consistently outperforms pure molecular embeddings such as ECFP or CLAMP for the solvent\u2011generalisation task.",
      "url": ""
    },
    {
      "title": "Benchmarking Pretrained Molecular Embedding Models ...",
      "text": "# Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning\n\nMateusz Praski\\\\equalcontrib,\nJakub Adamczyk\\\\equalcontrib,\nWojciech Czech\n\n###### Abstract\n\nPretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.\n\n## Introduction\n\nThe translation of molecular structures into information-rich numerical representations is a cornerstone of modern chemoinformatics and a critical step for harnessing machine learning in drug discovery and materials science. Molecular representation learning has emerged as a powerful paradigm, moving beyond handcrafted descriptors to automatically learn salient features from molecular data. Recently, the field has seen a surge in the development of pretrained molecular embedding models. This trend mirrors the transformative impact of representation learning in other scientific domains, where models are pretrained on vast unlabeled datasets to produce powerful general-purpose embeddings. Notable successes include models based on self-supervised learning (SSL), such as DINO (Caron et\u00a0al. [2021](https://arxiv.org/html/2508.06199v2#bib.bib9)) and DINOv2 in computer vision, Sentence Transformers and TSDAE (Wang, Reimers, and Gurevych [2021](https://arxiv.org/html/2508.06199v2#bib.bib45)) for dense text representations in NLP, and multimodal architectures such as CLIP and BLIP-2 that connect images and text. Inspired by these advances, molecular models are similarly trained on massive chemical databases to generate universal embeddings.\n\nThis study focuses specifically on evaluating these static embeddings rather than the alternative approach of task-specific fine-tuning. The rationale is threefold: first, we probe the fundamental knowledge encoded during pretraining and assess the intrinsic generalization capabilities of the learned representations themselves; second, to evaluate their utility in unsupervised applications such as molecular similarity searching and clustering, where embeddings are used directly; and third, to address the challenge of low-data learning, a common scenario in chemistry where fine-tuning complex models would lead to severe overfitting.\n\nThe proliferation of pretrained models, each with unique architectures and pretraining objectives, has created a pressing need for their systematic and rigorous evaluation. The absence of standardized and comprehensive benchmarking of the embeddings themselves hinders the ability to make informed decisions about model selection and impedes further progress in the field. To address this critical gap, this paper presents a comprehensive benchmarking study of state-of-the-art pretrained molecular embedding models. We evaluated their performance in a wide range of molecular representation learning tasks, providing a clear and objective comparison of their capabilities.\n\nOur findings challenge the prevailing view of progress in this domain. We reveal that despite their sophistication, modern pretrained models struggle to outperform traditional, much simpler methods. Specifically, we find that traditional chemical fingerprints often remain the top-performing representations. Furthermore, embeddings derived from Graph Neural Networks (GNNs) generally exhibit poor performance across the tested benchmarks. Although pretrained transformers that incorporate a strong chemical inductive bias perform acceptably, they do not demonstrate a definitive advantage. These results suggest that significant progress is still required to unlock the full potential of deep learning for universal molecular representation.\n\n## Molecular Embedding Approaches\n\nMolecular representation learning has long been a central focus in chemoinformatics and molecular machine learning, encompassing a wide range of techniques and approaches. Classical methods rely on deterministic feature extraction techniques, exemplified by molecular fingerprints. Subsequent advances in neural networks for molecular data have enabled transfer learning from large pretraining datasets through embedding models. These approaches can be categorized according to their input modality, architecture, and pretraining objectives.\n\nMolecular graphs, based on atom and bond structures, provide a natural representation of chemical compounds. In most cases, models use the topological (2D) graph, while ignoring the spatial (3D) conformation, as 3D structures are expensive and often difficult (or even impossible) to compute. However, the 2D graph retains much of the practically useful information. Architectures that operate on this representation include Graph Neural Networks (GNNs) and graph transformers.\n\nCompounds can also be serialized as SMILES or SELFIES strings, which are the standard formats for molecular datasets. Many models inspired by natural language processing (NLP) operate directly on these textual representations, primarily using transformer-based architectures.\n\nFinally, there are hybrid models that utilize multimodal representations or pretraining objectives, for example, incorporating the inductive biases of graph-based representations into more scalable and easily trainable text-based models.\n\nIn the following sections, we describe a range of molecular embedding models that encompass these different approaches, all of which have been implemented for benchmarking in this study. Model selection was based on the availability of code and pretrained weights, as well as the ability to successfully run the code. Due to space limitations, the following descriptions omit some details, such as specific model variants or pretraining datasets. We refer interested readers to the original publications for further information.\n\n### Molecular fingerprints\n\nMolecular fingerprints are feature extraction methods based on identifying small subgraphs within a molecule and detecting their presence or counting their occurrences, yielding binary and count variants, respectively. They can be broadly classified into substructural and hashed types (Adamczyk and Ludynia [2024](https://arxiv.org/html/2508.06199v2#bib.bib3)). Substructural fingerprints detect predefined patterns, such as functional groups or ring systems, that are typically determined by expert chemists. Hashed fingerprints, on the contrary, define general shapes of extracted subgraphs, convert them into numerical identifiers, and hash them using a modulo function into a fixed-length output vector. Common examples include circular neighborhoods in Extended Connectivity FingerPrint (ECFP) (Rogers and Hahn [2010](https://arxiv.org/html/2508.06199v2#bib.bib35)), paths of length 4 in the Topological Torsion (TT) (Nilakantan et\u00a0al. [1987](https://arxiv.org/html/2508.06199v2#bib.bib33)), and the shortest paths between atom pairs in Atom Pair (AP) fingerprint.\n\nAlthough not task-adaptive, hashed fingerprints remain widely used in chemoinformatics and molecular machine learning due to their flexibility, computational efficiency, and consistently strong performance. In many cases, they continue to outperform more complex approaches, such as ...",
      "url": "https://arxiv.org/html/2508.06199v2"
    },
    {
      "title": "Enhancing Generic Reaction Yield Prediction through ...",
      "text": "<div><div><section><h2>Abstract</h2><p>Deep learning (DL)-driven efficient synthesis planning may profoundly transform the paradigm for designing novel pharmaceuticals and materials. However, the progress of many DL-assisted synthesis planning (DASP) algorithms has suffered from the lack of reliable automated pathway evaluation tools. As a critical metric for evaluating chemical reactions, accurate prediction of reaction yields helps improve the practicality of DASP algorithms in the real-world scenarios. Currently, accurately predicting yields of interesting reactions still faces numerous challenges, mainly including the absence of high-quality generic reaction yield datasets and robust generic yield predictors. To compensate for the limitations of high-throughput yield datasets, we curated a generic reaction yield dataset containing 12 reaction categories and rich reaction condition information. Subsequently, by utilizing 2 pretraining tasks based on chemical reaction masked language modeling and contrastive learning, we proposed a powerful bidirectional encoder representations from transformers (BERT)-based reaction yield predictor named Egret. It achieved comparable or even superior performance to the best previous models on 4 benchmark datasets and established state-of-the-art performance on the newly curated dataset. We found that reaction-condition-based contrastive learning enhances the model\u2019s sensitivity to reaction conditions, and Egret is capable of capturing subtle differences between reactions involving identical reactants and products but different reaction conditions. Furthermore, we proposed a new scoring function that incorporated Egret into the evaluation of multistep synthesis routes. Test results showed that yield-incorporated scoring facilitated the prioritization of literature-supported high-yield reaction pathways for target molecules. In addition, through meta-learning strategy, we further improved the reliability of the model\u2019s prediction for reaction types with limited data and lower data quality. Our results suggest that Egret holds the potential to become an essential component of the next-generation DASP tools.</p></section><div><section><h2>Introduction</h2><p>Efficient chemical synthesis is crucial to satisfying the future demands for pharmaceuticals, materials, and energy [<a href=\"#B1\">1</a>]. Corey and Wipke [<a href=\"#B2\">2</a>] first proposed the concept of computer-aided synthesis planning (CASP) in the 1960s. CASP programs take the target molecule as input and return a series of single-step reactions that decompose the target molecule into a set of commercially available starting compounds or simple precursors that can be easily synthesized [<a href=\"#B3\">3</a>]. A feasible synthesis plan may dramatically accelerate the synthesis of desired molecules [<a href=\"#B4\">4</a>]. In recent years, with the development of data science, deep learning (DL) algorithms, and computing power, DL-assisted synthesis planning (DASP) has gained considerable interest [<a href=\"#B5\">5</a>\u2013<a href=\"#B17\">17</a>]. Modern DASP programs can quickly plan multiple potential retrosynthetic pathways for a given target molecule according to the constraints set by the user for the retrosynthetic search (such as the overall search time and number of single-step expansion steps) [<a href=\"#B18\">18</a>]. However, these theoretically feasible reaction pathways often become impractical because of such factors as incomplete conversion of reactants, side reactions, or inadequate purification [<a href=\"#B19\">19</a>]. Therefore, retrosynthetic route planning is only a major component of a successful DASP system [<a href=\"#B20\">20</a>]. To provide feasible suggestions that can be implemented by chemists in the laboratory, it is necessary to identify the optimal reaction conditions for the retrosynthetic route [<a href=\"#B21\">21</a>] and evaluate the quality of the overall synthesis route, and reaction yield is one of the most scientific and intuitive metrics for screening reaction conditions and evaluating synthesis pathway [<a href=\"#B22\">22</a>,<a href=\"#B23\">23</a>].</p><p>Reaction yield refers to the percentage of reactants that are successfully converted to the desired product [<a href=\"#B24\">24</a>]. Models that can reliably predict actual yields not only serve as scoring functions of DASP but also help chemists evaluate the overall yield of complex reaction pathways, giving priority to high-yield reactions to save time and cost in wet experiments [<a href=\"#B25\">25</a>]. However, because of the complexity of molecular structures, the multidimensionality of chemical reactions, and the limited availability of data, it is still a great challenge to predict the yields of chemical reactions under specific conditions [<a href=\"#B26\">26</a>]. The current yield prediction models are mainly built on high-throughput experimental (HTE) datasets, and Buchwald\u2013Hartwig reactions [<a href=\"#B26\">26</a>\u2013<a href=\"#B28\">28</a>] and Suzuki\u2013Miyaura reactions [<a href=\"#B29\">29</a>,<a href=\"#B30\">30</a>] are the 2 most well-studied HTE yield datasets (Fig. <a href=\"#F1\">1</a>A and B). Early studies utilized computed physicochemical descriptors [<a href=\"#B26\">26</a>], one-hot encoding of reactions [<a href=\"#B27\">27</a>], or structure-based molecular fingerprints [<a href=\"#B28\">28</a>] to predict the yields for these 2 datasets. Recently, the DL fingerprint rxnfp developed by Schwaller et\u00a0al. [<a href=\"#B31\">31</a>] and the differential reaction fingerprint drfp developed by Probst et\u00a0al. [<a href=\"#B32\">32</a>] have substantially outperformed previous methods. Rxnfp and drfp respectively achieved the best performance on test sets of Buchwald\u2013Hartwig dataset and Suzuki\u2013Miyaura dataset (70:30 random split), with coefficient of determination (<i>R</i><sup>2</sup>) scores of 0.95 and 0.85. However, by studying previous experimental results, we found that most of the aforementioned methods did not achieve the ideal predictive performance on the out-of-sample test sets of Buchwald\u2013Hartwig reactions containing additional additives. This indicates the limitations of using HTE datasets for yield prediction. HTE datasets usually involve specific classes of reactions and focus on a narrow chemical space. When using yield prediction models to explore unknown chemical spaces, this performance degradation problem will be prevalent because the unknown chemical space to be predicted can be very large [<a href=\"#B33\">33</a>]. Yield prediction models trained on HTE datasets cannot be applied in the real-world scenarios aimed at predicting the yields for a broad variety of reactions. Therefore, curating generic reaction yield datasets that are not limited to specific reaction classes is the first step to promote the practical application of yield prediction models.</p><div><figure><figcaption><span>Fig.\u00a01</span>. Overall reaction and variables for the Buchwald\u2013Hartwig (B-H) (A), Suzuki\u2013Miyaura (B), and Reaxys-MultiCondi-Yield (C) datasets.</figcaption></figure></div><p>Another key point to apply yield prediction models to chemical synthesis practice is to adopt effective modeling methods for generic reaction yield datasets. Reaction Simplified Molecular Input Line Entry System (SMILES) is a simplified chemical language for representing chemical reactions [<a href=\"#B34\">34</a>]. Therefore, SMILES-based yield prediction can be viewed as a natural language processing (NLP) problem, extracting molecular features directly from reaction SMILES without relying on any manually generated feature. In 2017, Vaswani et\u00a0al. [<a href=\"#B35\">35</a>] proposed the transformer architecture for handling various NLP tasks, which achieved excellent feature extraction capability through the self-attention mechanism. In recent years, many pretraining language models such as bidirectional encoder representations from transformers (BERT) [<a href=\"#B36\">36</a>] and generative pretrained transformer (GPT)...",
      "url": "https://spj.science.org/doi/10.34133/research.0292"
    },
    {
      "title": "ReactionT5: a pre-trained transformer model for accurate ...",
      "text": "ReactionT5: a pre-trained transformer model for accurate chemical reaction prediction with limited data - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1186/s13321-025-01075-4)\n* [](pdf/13321_2025_Article_1075.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Journal of Cheminformatics logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jcheminfo.png)\nJ Cheminform\n. 2025 Aug 19;17:126. doi:[10.1186/s13321-025-01075-4](https://doi.org/10.1186/s13321-025-01075-4)\n# ReactionT5: a pre-trained transformer model for accurate chemical reaction prediction with limited data\n[Tatsuya Sagawa](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Sagawa T\"[Author]>)\n### Tatsuya Sagawa\n1Graduate School of Pharmaceutical Sciences, Kyoto University, Kyoto, 606\u20138501 Japan\n3RIKEN BDR, Kobe, 650-0047 Japan\nFind articles by[Tatsuya Sagawa](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Sagawa T\"[Author]>)\n1,3,[Ryosuke Kojima](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kojima R\"[Author]>)\n### Ryosuke Kojima\n2Graduate School of Medicine, Kyoto University, Kyoto, 606\u20138501 Japan\n3RIKEN BDR, Kobe, 650-0047 Japan\nFind articles by[Ryosuke Kojima](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kojima R\"[Author]>)\n2,3,\u2709\n* Author information\n* Article notes\n* Copyright and License information\n1Graduate School of Pharmaceutical Sciences, Kyoto University, Kyoto, 606\u20138501 Japan\n2Graduate School of Medicine, Kyoto University, Kyoto, 606\u20138501 Japan\n3RIKEN BDR, Kobe, 650-0047 Japan\n\u2709Corresponding author.\nReceived 2025 Apr 6; Accepted 2025 Aug 1; Collection date 2025.\n\u00a9The Author(s) 2025\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC12366004\u00a0\u00a0PMID:[40830907](https://pubmed.ncbi.nlm.nih.gov/40830907/)\n## Abstract\nAccurate chemical reaction prediction is critical for reducing both cost and time in drug development. This study introduces ReactionT5, a transformer-based chemical reaction foundation model pre-trained on the Open Reaction Database\u2014a large publicly available reaction dataset. In benchmarks for product prediction, retrosynthesis, and yield prediction, ReactionT5 outperformed existing models. Specifically, ReactionT5 achieved 97.5% accuracy in product prediction, 71.0% in retrosynthesis, and a coefficient of determination of 0.947 in yield prediction. Remarkably, ReactionT5, when fine-tuned with only a limited dataset of reactions, achieved performance on par with models fine-tuned on the complete dataset. Additionally, the visualization of ReactionT5 embeddings illustrates that the model successfully captures and represents the chemical reaction space, indicating effective learning of reaction properties.\n### Graphical Abstract\n[![graphic file with name 13321_2025_1075_Figa_HTML.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b294/12366004/bb7d7c0421b5/13321_2025_1075_Figa_HTML.jpg)](<https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click on image to zoom&amp;p=PMC3&amp;id=12366004_13321_2025_1075_Figa_HTML.jpg>)\n### Supplementary Information\nThe online version contains supplementary material available at 10.1186/s13321-025-01075-4.\n**Keywords:**Foundation model, Chemical reaction, Reaction space, Deep learning, Machine learning, Transformers, Organic chemistry\n## Scientific contribution\nWe propose ReactionT5, a foundation model for chemical reactions pre-trained on a large reaction dataset that outperforms existing models in product prediction, retrosynthesis, and yield prediction tasks, all while maintaining high performance even with limited training data. We also show that the learned embeddings effectively capture the chemical reaction space, and we make the training code and model weights publicly available. We believe that fine-tuning ReactionT5 with users\u2019 in-house data will be beneficial for many chemical applications.\n### Supplementary Information\nThe online version contains supplementary material available at 10.1186/s13321-025-01075-4.\n## Introduction\nPredicting chemical reactions is pivotal for progress in organic chemistry and drug discovery. Highly accurate predictive models can dramatically reduce the costs associated with exploratory experimentation by forecasting the outcomes of chemical experiments before they are conducted. Consequently, machine learning models that adeptly encapsulate the complexity of organic reactions [[1](#CR1)] are anticipated to bolster the efforts of experimental chemists. Deep learning models have surfaced as viable substitutes in recent years, providing data-driven insights derived from extensive reaction datasets and surmounting numerous constraints of traditional methods [[2](#CR2)].\nIn this context, large-scale pre-trained models utilizing compound libraries have attracted increasing interest in organic chemistry research [[3](#CR3)]. These models often conceptualize molecules as symbolic sequences, similar to natural languages. For instance, SMILES-BERT [[4](#CR4)] has demonstrated high efficacy in predicting molecular properties by leveraging unsupervised pre-training on molecular structures encoded in the Simplified Molecular-Input Line-Entry System (SMILES) format [[5](#CR5)]. While substantial progress has been made with models focused on single molecules, for example, SMILES-BERT [[4](#CR4)], ChemBERTa [[6](#CR6)], MolE [[7](#CR7)], MolCLR [[8](#CR8)], and Molformer [[9](#CR9)], research on models addressing multiple molecules, such as those used in chemical reactions, remains relatively scarce. A notable exception is T5Chem [[10](#CR10)], which, alt...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12366004"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "Exploring BERT for Reaction Yield Prediction: Evaluating ...",
      "text": "Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pre-training Data Augmentation | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pre-training Data Augmentation\n19 February 2025, Version 1\nWorking Paper\n## Authors\n* [Adrian Krzyzanowski](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Adrian%20Krzyzanowski)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-3604-9274),\n* [Stephen Pickett](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stephen%20Pickett),\n* [Peter Pog\u00e1ny](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Peter%20Pog%C3%A1ny)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-3536-0746)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nPredicting reaction yields in synthetic chemistry remains a significant challenge. This study systematically evaluates the impact of tokenization, molecular representation, pre-training data, and adversarial training on a BERT-based model for yield prediction of Buchwald-Hartwig and Suzuki-Miyaura coupling reactions using publicly available HTE datasets. We demonstrate that molecular representation choice (SMILES, DeepSMILES, SELFIES, Morgan fingerprint-based notation, IUPAC names) has minimal impact on model performance, while typically BPE and SentencePiece tokenization outperform other methods. WordPiece is strongly discouraged for SELFIES and fingerprint-based notation. Furthermore, pre-training with relatively small data sets (&lt;100K reactions) achieves comparable performance to larger data sets containing millions of examples. A use of artificially generated domain-specific pre-training data is proposed. The artificially generated sets prove to be a good surrogate to the reaction schemes extracted from reaction data sets such as Pistachio or Reaxys. The best performance was observed for hybrid pre-training sets combining the real and the domain-specific, artificial data. Finally, we show that a novel adversarial training approach, perturbing input embeddings dynamically, improves model robustness and generalisability for yield and reaction success prediction. These findings provide valuable insights for developing robust and practical machine learning models for yield prediction in synthetic chemistry. GSK\u2019s BERT training code base is made available to the community with this work.\n## Keywords\n[Predictive Synthesis](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Predictive%20Synthesis)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[BERT](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=BERT)\n[Molecular Representation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20Representation)\n[Buchwald-Hartwig Reaction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Buchwald-Hartwig%20Reaction)\n[Suzuki-Miyaura Reaction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Suzuki-Miyaura%20Reaction)\n[Adversarial Training](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Adversarial%20Training)\n[Data Augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=%20Data%20Augmentation)\n[Artificial Data](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Artificial%20Data)\n## Supplementary materials\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\nSupporting Information for the Main Manuscript\n**Description**\nFile containing supporting results presented in tables and figures.\n**Actions**\n**Download(6 MB)**\n**Title**\nArtificially Generated Reaction Schemes\n**Description**\nFile containing artificially generated Suzuki-Miyaura and Buchwald-Hartwig reaction schemes. Training and validation data are provided.\n**Actions**\n**Download(141 MB)**\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nSynthCoder GitHub Repository\n**Description**\nGitHub repository for the GSK's SynthCoder code base.\n**Actions**\n[**View**](https://github.com/gskcheminformatics/SynthCoder)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\nFeb 19, 2025 Version 1\n## Metrics\n1,829\n954\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2025-nq3xn\nD O I: 10.26434/chemrx...",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67adcd81fa469535b9285f22"
    },
    {
      "title": "Expediting hit-to-lead progression in drug discovery ...",
      "text": "Expediting hit-to-lead progression in drug discovery through reaction prediction and multi-dimensional optimization | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-025-66324-4?error=cookies_not_supported&code=a780991d-434f-41ee-927b-14ca0ac6d0ad)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nExpediting hit-to-lead progression in drug discovery through reaction prediction and multi-dimensional optimization\n[Download PDF](https://www.nature.com/articles/s41467-025-66324-4_reference.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-025-66324-4_reference.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:26 November 2025# Expediting hit-to-lead progression in drug discovery through reaction prediction and multi-dimensional optimization\n* [David F. Nippa](#auth-David_F_-Nippa-Aff1)[ORCID:orcid.org/0000-0002-0346-3786](https://orcid.org/0000-0002-0346-3786)[1](#Aff1)[na1](#na1),\n* [Kenneth Atz](#auth-Kenneth-Atz-Aff1)[ORCID:orcid.org/0000-0002-2628-1619](https://orcid.org/0000-0002-2628-1619)[1](#Aff1)[na1](#na1),\n* [Yannick Stenzhorn](#auth-Yannick-Stenzhorn-Aff1)[ORCID:orcid.org/0009-0007-7444-3946](https://orcid.org/0009-0007-7444-3946)[1](#Aff1),\n* [Alex T. M\u00fcller](#auth-Alex_T_-M_ller-Aff1)[ORCID:orcid.org/0000-0001-8063-9952](https://orcid.org/0000-0001-8063-9952)[1](#Aff1),\n* [Andreas Tosstorff](#auth-Andreas-Tosstorff-Aff1)[ORCID:orcid.org/0000-0002-4969-3667](https://orcid.org/0000-0002-4969-3667)[1](#Aff1),\n* [J\u00f6rg Benz](#auth-J_rg-Benz-Aff1)[1](#Aff1),\n* [Hayley Binch](#auth-Hayley-Binch-Aff1)[1](#Aff1),\n* [Markus B\u00fcrkler](#auth-Markus-B_rkler-Aff1)[1](#Aff1),\n* [Achi Haider](#auth-Achi-Haider-Aff1)[1](#Aff1),\n* [Dominik Heer](#auth-Dominik-Heer-Aff1)[1](#Aff1),\n* [Remo Hochstrasser](#auth-Remo-Hochstrasser-Aff1)[1](#Aff1),\n* [Christian Kramer](#auth-Christian-Kramer-Aff1)[1](#Aff1),\n* [Michael Reutlinger](#auth-Michael-Reutlinger-Aff1)[ORCID:orcid.org/0000-0001-9393-108X](https://orcid.org/0000-0001-9393-108X)[1](#Aff1),\n* [Petra Schneider](#auth-Petra-Schneider-Aff2)[2](#Aff2),\n* [Thierry Shema](#auth-Thierry-Shema-Aff3)[3](#Aff3),\n* [Andreas Topp](#auth-Andreas-Topp-Aff1)[1](#Aff1),\n* [Alexander Walter](#auth-Alexander-Walter-Aff1)[1](#Aff1),\n* [Matthias B. Wittwer](#auth-Matthias_B_-Wittwer-Aff1)[ORCID:orcid.org/0000-0003-1359-4795](https://orcid.org/0000-0003-1359-4795)[1](#Aff1),\n* [Jens Wolfard](#auth-Jens-Wolfard-Aff1)[ORCID:orcid.org/0000-0001-7338-4103](https://orcid.org/0000-0001-7338-4103)[1](#Aff1),\n* [Bernd Kuhn](#auth-Bernd-Kuhn-Aff1)[ORCID:orcid.org/0000-0002-4301-562X](https://orcid.org/0000-0002-4301-562X)[1](#Aff1),\n* [Mario van der Stelt](#auth-Mario-Stelt-Aff3)[ORCID:orcid.org/0000-0002-1029-5717](https://orcid.org/0000-0002-1029-5717)[3](#Aff3),\n* [Rainer E. Martin](#auth-Rainer_E_-Martin-Aff1)[ORCID:orcid.org/0000-0001-7895-497X](https://orcid.org/0000-0001-7895-497X)[1](#Aff1),\n* [Uwe Grether](#auth-Uwe-Grether-Aff1)[ORCID:orcid.org/0000-0002-3164-9270](https://orcid.org/0000-0002-3164-9270)[1](#Aff1)&amp;\n* \u2026* [Gisbert Schneider](#auth-Gisbert-Schneider-Aff2)[2](#Aff2)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms), Article\u00a0number:(2025)[Cite this article](#citeas)\n* 4414Accesses\n* 4Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-025-66324-4/metrics)\nWe are providing an unedited version of this manuscript to give early access to its\nfindings. Before final publication, the manuscript will undergo further editing. Please note\nthere may be errors present which affect the content, and all legal disclaimers apply.\n### Subjects\n* [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n* [Lead optimization](https://www.nature.com/subjects/lead-optimization)\n## Abstract\nThe rapid and economical synthesis of novel bioactive compounds remains a hurdle in drug discovery efforts. This study demonstrates an integrated medicinal chemistry workflow that effectively diversifies hit and lead structures, enabling an acceleration of the critical hit-to-lead optimization phase. Employing high-throughput experimentation (HTE), we generated a comprehensive data set encompassing 13,490 novel Minisci-type C-H alkylation reactions. These data served as the foundation for training deep graph neural networks to accurately predict reaction outcomes. Scaffold-based enumeration of potential Minisci reaction products, starting from moderate inhibitors of monoacylglycerol lipase (MAGL), yielded a virtual library containing 26,375 molecules. This virtual chemical library was evaluated using reaction prediction, physicochemical property assessment, and structure-based scoring, identifying 212 MAGL inhibitor candidates. Of these, 14 compounds were synthesized and exhibited subnanomolar activity, representing a potency improvement of up to 4500 times over the original hit compound. These ligands also showed favorable pharmacological profiles. Co-crystallization of three computationally designed ligands with the MAGL protein provided structural insights into their binding modes. This study demonstrates the potential of combining miniaturized HTE with deep learning and optimization of molecular properties to reduce cycle times in hit-to-lead progression.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41573-025-01225-1/MediaObjects/41573_2025_1225_Fig1_HTML.png)\n### [The changing landscape of medicinal chemistry optimization](https://www.nature.com/articles/s41573-025-01225-1?fromPaywallRec=false)\nArticle07 July 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-023-01047-5/MediaObjects/42004_2023_1047_Fig1_HTML.png)\n### [Identifying opportunities for late-stage C-H alkylation with high-throughput experimentation and in silico reaction screening](https://www.nature.com/articles/s42004-023-01047-5?fromPaywallRec=false)\nArticleOpen access20 November 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-05905-z/MediaObjects/41586_2023_5905_Fig1_HTML.png)\n### [Computational approaches streamlining drug discovery](https://www.nature.com/articles/s41586-023-05905-z?fromPaywallRec=false)\nArticle26 April 2023\n## Data availability\n**Training data:**The SURF-formatted experimental data set, containing 13,490 Minisci-type C-H alkylation reactions, has been made publicly available via Figshare ([https://doi.org/10.6084/m9.figshare.28294850](https://doi.org/10.6084/m9.figshare.28294850))[60](https://www.nature.com/articles/s41467-025-66324-4#ref-CR60).**Co-crystal structures:**The coordinates of the MAGL co-crystal structure of the initial hit compound**17**have been deposited in the PDB under accession code[7PRM](https://doi.org/10.2210/pdb7PRM/pdb). The three co-crystal structures of optimized molecules (**23**,**27**, and**29**) are available under accession codes[9I5J](https://doi.org/10.2210/pdb9I5J/p...",
      "url": "https://www.nature.com/articles/s41467-025-66324-4"
    },
    {
      "title": "From SMILES Tokens to Molecular Embeddings (2/3)",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa118e7589686&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40anrizal05%2Fchemical-foundation-models-from-smiles-tokens-to-molecular-embeddings-2-3-a118e7589686&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40anrizal05%2Fchemical-foundation-models-from-smiles-tokens-to-molecular-embeddings-2-3-a118e7589686&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Chemical Foundation Models: From SMILES Tokens to Molecular Embeddings (2/3)\n\n## **Teaching AI to Understand Molecules**\n\n[![Anwar Rizal](https://miro.medium.com/v2/resize:fill:64:64/1*izbYYF2_or-7reAIHlfPWQ.jpeg)](https://medium.com/@anrizal05?source=post_page---byline--a118e7589686---------------------------------------)\n\n[Anwar Rizal](https://medium.com/@anrizal05?source=post_page---byline--a118e7589686---------------------------------------)\n\n14 min read\n\n\u00b7\n\nJul 7, 2025\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nPhoto by [Poul Hoang](https://unsplash.com/@poulhoang?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nIn [Part 1,](https://medium.com/@anrizal05/chemical-foundation-models-from-smiles-tokens-to-molecular-embeddings-1-3-1a6f12852c30) we explored the fundamentals: how molecules become text through SMILES and SELFIES, the challenges of chemical tokenization, and the benchmark landscape that defines success in molecular AI. We ended with a roadmap of three architectural paradigms that have shaped the field.\n\nNow we dive into the first tasks of chemical foundation models: teaching transformers to understand molecular structure and properties.\n\nThis part examines two distinct evolutionary phases in chemical AI.\n\n**First**, we\u2019ll explore the encoder-only solution, where **ChemBERTa** (Chithrananda, et.al, 2020)(Ahmad, et.al 2022) and **MolBERT** (Li & Jiang, 2021) reported two different approaches to molecular understanding. One bet on massive scale and chemistry-agnostic learning. The other injected chemical knowledge upfront for efficient training. Both succeeded, creating a philosophical divide that still influences the field today.\n\n**Then** we\u2019ll examine IBM\u2019s latest **SMI-TED289M** (Soares, et.al. 2025), which modeled molecular representations through learnable compression. Instead of losing information through simple averaging, SMI-TED learned to compress molecules while preserving reconstruction capability. This architectural innovation led to representations that self-organize according to chemical principles \u2014 emergent intelligence arising from the right training objective.\n\nBy the end of this part, you\u2019ll understand how different architectural choices affect what chemical AI systems learn, and why the path from simple property prediction to rich molecular understanding wasn\u2019t just about scaling up \u2014 it was about choosing the right inductive biases.\n\nLet\u2019s see how transformers learned to think about molecules.\n\n# 4\\. Encoder-Only Models: BERT for Molecules\n\nThe most natural starting point for chemical transformers was adapting BERT\u2019s (Devlin et.al, 2018) encoder-only architecture. Two pioneering approaches \u2014 ChemBERTa and MolBERT \u2014 took different paths to the same goal, creating a fork that still defines the field today.\n\n## 4.1 The Fundamental Fork: Two Tokenization Philosophies\n\nBefore diving into specific models, let\u2019s establish the **core architectural choice** that defines this space:\n\n**Strategy 1: Chemistry-Agnostic (ChemBERTa family)**\n\n- Treat SMILES as text strings\n- Use standard NLP tokenization (BPE, character-level)\n- Let the transformer learn chemistry from scratch\n- **Trade-off**: Needs massive datasets but more generalizable\n\n**Strategy 2: Chemistry-Aware (MolBERT)**\n\n- Pre-process molecules into chemical substructures.\n- Each token represents a meaningful chemical fragment\n- Embed domain knowledge into tokenization\n- **Trade-off**: More efficient but requires chemical preprocessing\n\nThis choice cascades into everything else \u2014 dataset requirements, performance patterns, and practical implementation. Both follow the same high-level recipe:\n\n1. Convert molecules to sequences\n2. Apply masked language modeling \u2014 hide 15% of tokens and predict them\n3. Fine-tune the resulting embeddings for downstream tasks\n\nBut the tokenization step creates completely different models.\n\n## 4.2 ChemBERTa: The Scale-Heavy Approach\n\nBuilt on RoBERTa with 6 layers and 12 attention heads, ChemBERTa treated molecules like any other text. This chemistry-agnostic approach had profound implications.\n\n## Tokenization Evolution\n\nChemBERTa explored three tokenization strategies:\n\n**ChemBERTa (** Chithrananda, et.al, 2020 **)**: Tested BPE with 52K tokens vs. SMILES-aware tokenizer. The chemistry-aware approach won narrowly (\u2206PRC-AUC = +0.015), but barely.\n\n**ChemBERTa-2 (**(Ahmad, et.al 2022): Simplified to 591 tokens using \u201ccommon SMILES characters\u201d \u2014 a 99% vocabulary reduction that actually improved performance.\n\nThis evolution shows that even within the text-based approach, chemical awareness helps. But the core philosophy remained: let the transformer figure out chemistry from character patterns.\n\n## The Scale Imperative\n\nBecause ChemBERTa treats molecules like generic text, it needed **massive datasets**. Training on datasets from 100K to 77M compounds showed consistent improvements: \u2206ROC-AUC = +0.110 on average.\n\nThis scaling requirement makes sense \u2014 the model had to learn basic chemistry (what benzene rings are, how atoms bond) alongside the prediction tasks. With 77M compounds, it finally had enough data to discover chemical patterns from scratch.\n\n## ChemBERTa-2\u2019s Pretraining Innovation\n\nChemBERTa-2 introduced Multi-Task Regression (MTR) as an alternative to masked language modeling. This created two model families that differ by pretraining approach:\n\n**MLM Models (Traditional Approach)**:\n\n- Objective: Predict masked SMILES tokens (standard BERT approach)\n- Models: ChemBERTa-2-MLM-5M, MLM-10M, MLM-77M\n\n**MTR Models (Chemistry-Specific Approach)**:\n\n- Objective: Simultaneously predict 200 molecular properties calculated from RDKit (molecular weight, aromatic rings, hydrogen bond donors, etc.)\n- Models: ChemBERTa-2-MTR-5M, MTR-10M, MTR-77M\n\nThe numbers (5M, 10M, 77M) refer to training dataset sizes, not model parameters.\n\nThe MTR results were compelling:\n\n- Consistently outperformed MLM on every downstream task\n- MLM loss predicted MTR performance \u2014 enabling efficient hyperparameter search with fast MLM, then training best architectures with superior but slower MTR\n\nThis showed that even within the text-based approach, chemistry-specific pretraining objectives work better than generic masked language modeling.\n\n## 4.3 MolBERT: The Knowledge-Injection Approach\n\nMolBERT took the opposite bet: **inject chemical knowledge upfront, then train efficiently**. Instead of treating SMILES as text, it used Morgan fingerprints to create structure-aware tokens.\n\n## The Morgan Fingerprint Strategy\n\nThink of Morgan fingerprints as \u201cchemical bigrams\u201d that capture local neighborhood patterns. Just like bigrams in...",
      "url": "https://medium.com/@anrizal05/chemical-foundation-models-from-smiles-tokens-to-molecular-embeddings-2-3-a118e7589686"
    },
    {
      "title": "ReactionT5: a large-scale pre-trained model towards application of limited reaction data",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2311.06708** (physics)\n\n\\[Submitted on 12 Nov 2023\\]\n\n# Title:ReactionT5: a large-scale pre-trained model towards application of limited reaction data\n\nAuthors: [Tatsuya Sagawa](https://arxiv.org/search/physics?searchtype=author&query=Sagawa,+T), [Ryosuke Kojima](https://arxiv.org/search/physics?searchtype=author&query=Kojima,+R)\n\nView a PDF of the paper titled ReactionT5: a large-scale pre-trained model towards application of limited reaction data, by Tatsuya Sagawa and Ryosuke Kojima\n\n[View PDF](https://arxiv.org/pdf/2311.06708)\n\n> Abstract:Transformer-based deep neural networks have revolutionized the field of molecular-related prediction tasks by treating molecules as symbolic sequences. These models have been successfully applied in various organic chemical applications by pretraining them with extensive compound libraries and subsequently fine-tuning them with smaller in-house datasets for specific tasks. However, many conventional methods primarily focus on single molecules, with limited exploration of pretraining for reactions involving multiple molecules. In this paper, we propose ReactionT5, a novel model that leverages pretraining on the Open Reaction Database (ORD), a publicly available large-scale resource. We further fine-tune this model for yield prediction and product prediction tasks, demonstrating its impressive performance even with limited fine-tuning data compared to traditional models. The pre-trained ReactionT5 model is publicly accessible on the Hugging Face platform.\n\n|     |     |\n| --- | --- |\n| Subjects: | Chemical Physics (physics.chem-ph); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2311.06708](https://arxiv.org/abs/2311.06708) \\[physics.chem-ph\\] |\n| (or [arXiv:2311.06708v1](https://arxiv.org/abs/2311.06708v1) \\[physics.chem-ph\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2311.06708](https://doi.org/10.48550/arXiv.2311.06708) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tatsuya Sagawa \\[ [view email](https://arxiv.org/show-email/75d2fcc2/2311.06708)\\] **\\[v1\\]**\nSun, 12 Nov 2023 02:25:00 UTC (773 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ReactionT5: a large-scale pre-trained model towards application of limited reaction data, by Tatsuya Sagawa and Ryosuke Kojima\n\n- [View PDF](https://arxiv.org/pdf/2311.06708)\n- [TeX Source](https://arxiv.org/src/2311.06708)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2311.06708&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2311.06708&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2023-11](https://arxiv.org/list/physics.chem-ph/2023-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2311.06708?context=cs) [cs.LG](https://arxiv.org/abs/2311.06708?context=cs.LG) [physics](https://arxiv.org/abs/2311.06708?context=physics)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.06708)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.06708)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.06708)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.06708) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2311.06708"
    }
  ]
}