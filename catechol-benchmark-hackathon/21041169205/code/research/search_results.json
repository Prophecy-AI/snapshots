{
  "query": "What are the top leaderboard scores achieved in the Catechol Benchmark Hackathon Kaggle competition and what techniques did they use?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most recent source (the arXiv pre\u2011print describing the Catechol Benchmark, submitted 9\u202fJun\u202f2025) does not provide the top leaderboard scores or the techniques used by the leading Kaggle hackathon participants, so current score details are unavailable. ([arxiv.org](https://www.arxiv.org/abs/2506.07619))",
      "url": ""
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://www.arxiv.org/abs/2506.07619"
    },
    {
      "title": "How to Score better in Kaggle Competition",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/vithal2311/how-to-score-better-in-kaggle-competition"
    },
    {
      "title": "How to improve your kaggle competition leaderboard ranking | by HD",
      "text": "<div><div><div><h2>Tips from a new \u2018Kaggler\u2019 building CNN\u2019s for blindness detection</h2><div><a href=\"https://medium.com/@hdln?source=post_page---byline--bcd16643eddf---------------------------------------\"><div><p></p></div></a></div></div><figure></figure><blockquote><p>After recently competing in the 2019 APTOS Blindness Detection Kaggle Competition and finishing in top 32%, I thought I would share my process for training convolutional neural networks. My only prior deep learning experience was completing the Deeplearning.ai Specialisation, hence this is all you should need to read this article.</p></blockquote><h2><strong>Sections to this article</strong></h2><ol><li>Competition context</li><li>Keeping a logbook</li><li>Get more data</li><li>Leveraging existing kernels</li><li>Preprocessing images</li><li>Training is a very very slow process (but don\u2019t worry)</li><li>Transfer learning</li><li>Model selection</li></ol><h2>Competition context</h2><p>I spent the last 2\u20133 months working on and off on the<a href=\"https://www.kaggle.com/c/aptos2019-blindness-detection\"> APTOS 2019 Blindness Detection Competition</a> on Kaggle, which required you to grade images of people\u2019s eyes to 5 categories of diabetic retinopathy severity. In the remainder of this article I talk about some tips and tricks you can use in <em>any kaggle vision competition, </em>as I feel that the things I learned from this competition are pretty much universally applicable<em>.</em></p><h2>Keep a logbook</h2><p>Like any good scientific experiment, we change one thing at a time and compare our results to our control. Hence when training a CNN (Convolutional Neural Network) we should do likewise and record the change and the results in a logbook. Heres the one I used from the blindness detection challenge.</p><figure><figcaption>My logbook for Kaggle APTOS Blindness Detection Challenge</figcaption></figure><p>I don\u2019t claim that the exact table I use here is ideal (far from it), but I found it useful to be able to at least identify each time I made a change whether the model improved or not cumulatively on the previous changes. Regardless I highly recommend you keep some form of logbook as its very difficult to identify if anything your doing is working otherwise.</p><p>Some ideas I have for my next competition logbook is to:</p><ol><li>Establish a single baseline model to compare all future changes to</li><li>Come up with a bunch of tweaks you want to try and run modified versions of the baseline for each tweak independently rather than in a cumulative fashion.</li><li>Maintain the same (and smallest) CNN Architecture for as long as possible as it will make iteration quicker and with some look many of the hyper-parameters should transfer decently to larger more complex models.</li></ol><h2>Get more data</h2><p>Do some research before you start coding and see if a similar competition has been run before or if there are any databases of similar labelled training sets you can use. More data is never really harmful to your model (assuming the quality of labelling is decent), so get as much of it as you can, but just don\u2019t forget to keep your validation and test sets from the original dataset provided to you or you may end up with a train- test mismatch<em>.</em></p><h2>Leveraging existing kernels</h2><p>If your new to deep learning competitions (like me) you probably don\u2019t want to write your entire notebook from scratch \u2014 especially when someone else has probably already posted a starter kernel for your competition (Why reinvent the wheel right?). This will probably save you a bunch of time on debugging and get you onto learning new stuff faster by just tweaking someone else\u2019s model.</p><p><strong>This was a good starter kernel</strong> that I used and retrofitted for almost all of my further trials.</p><p><strong>A word of warning: </strong>If a kernel suggests a bunch of techniques to use for your model you should check if they state the resultant performance gains, otherwise be skeptical and conduct tests yourself before blindly incorporating them into your own models :)</p><h2>Preprocessing Images</h2><p><strong>Cropping &amp; Other Augmentations: </strong>This step is a must. Training images may be in a very raw state. For example in the blindness detection challenge the images were all cropped at different ratios which meant a dumb algorithm could overfit to the black space around the eye which was more prevalent in one class than another.</p><figure><figcaption>Source: <a href=\"https://www.kaggle.com/taindow/be-careful-what-you-train-on\">https://www.kaggle.com/taindow/be-careful-what-you-train-on</a></figcaption></figure><p>Hence cropping and resizing images in a robust way was a crucial part of this competition. There were also many image augmentation techniques such as random cropping, rotation, contrast and brightness etc, which I had varying degrees of success with.</p><p><strong>Imbalanced classes:</strong> Invariably there are more training examples for some classes than others, so you need to fix this before you start training. A combination of techniques that work ok are <em>over / under-sampling </em>as well as <em>mixup </em>(Zhang et al., 2019) during mini batch gradient descent.</p><p><strong>Preprocessing Computation: </strong>Often the dataset will be quite large and applying rudimentary procedures such as standardising size and cropping of images should be done in a separate kernel t (or offline dep. on the size of the dataset)and re-uploaded as a modified version of the original data \u2014 otherwise you will have to do this computation at every epoch / run of your model (which is a terrible waste of time).</p><h2>Training is a very very slow process</h2><p>Now that you\u2019ve written your first kernel you need to test it out! Kaggle kernels can run for up to 9 hours (the kernel time limit may vary by competition), the site is also running many models and can be slower at some times of the day than others as a result. My best advice is to first quickly run it in browser for 1 or 2 iterations to make sure you haven\u2019t made any errors then get several ideas you want to test out simultaneously and just hit commit on all of them and check back in a few hours. Note that if you hit commit rather than just running the kernel you don\u2019t have to keep your laptop running :).</p><h2>Transfer Learning</h2><p>You won\u2019t be training any model from scratch which is sufficiently large. Typically we will just take a large model pre-trained on imagenet or some other large dataset and fine-tune it for our purposes. In almost all cases you should unfreeze all layers of the model during fine-tuning as the results are likely to be most stable.</p><blockquote><p>This is nicely illustrated by this chart (Yosinski et al. 2014) where two networks are trained on datasets A and B then the network is chopped at layer n and the layers before are either frozen or fine tuned (indicated by +). The conclusion being seen in the second figure with the to line AnB+ with all 7 layers being tuned producing the best top-1 accuracy.</p></blockquote><figure><figcaption>(Yosinski et al. 2014)</figcaption></figure><h2>Model selection</h2><p>Your probably best off starting with a smaller model (like ResNet 50), then trying some larger architectures such as (Resnet-101, InceptionNets, EfficientNets). All of these networks have papers available and are definitely worth a read before you go ahead and use them, typically though you should expect to get better accuracy with newer models than older ones.</p><h2>Closing Remarks</h2><p>With the information i\u2019ve provided above you should be able to get a really decent score on both the public and private leaderboards.</p><p>My intuition from competing in this challenge would suggest that getting into the top 30 on the public leaderboard is sufficient to have a good chance at finishing in the top 10 on the private board due to the uncertainty associated with the remaining held-out...",
      "url": "https://medium.com/data-science/how-to-improve-your-kaggle-competition-leaderboard-ranking-bcd16643eddf"
    },
    {
      "title": "Progression System - Kaggle",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/progression#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/progression)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/progression#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fprogression)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fprogression)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n# Kaggle Progression System\n\nKaggle's Progression System uses performance tiers to track your growth as a data scientist on Kaggle. Along the way, you\u2019ll earn medals for your achievements and compete for data science glory on live leaderboards.\n\n![](https://www.kaggle.com/static/images/tier-animation-transparent.gif)\n\n## Categories of Expertise\n\nThe Progression System is designed around four Kaggle categories of data science expertise: Competitions, Notebooks, Datasets, and Discussion. Advancement through performance tiers is done independently within each category of expertise.\n\n## Performance Tiers\n\nWithin each category of expertise, there are five performance tiers that can be achieved in accordance with the quality and quantity of work you produce: Novice, Contributor, Expert, Master, and Grandmaster.\n\nFor example, you could be a Competitions Master, a Datasets Expert, a Notebooks Grandmaster, and a Discussion Expert:\n\n![](https://www.kaggle.com/static/images/tiers-example.png)\n\nThe highest tier you have achieved in any of the categories of expertise will be displayed on your profile and under your avatar across the site. Tiers are awarded on the basis of medals earned in each category.\n\n![](https://www.kaggle.com/static/images/tiers/novice.svg)\n\n### Novice\n\nYou\u2019ve joined the community.\n\n_check\\_box\\_outline\\_blank_ Register!\n\n![](https://www.kaggle.com/static/images/tiers/contributor.svg)\n\n### Contributor\n\nYou\u2019ve completed your profile, engaged with the community, and fully explored Kaggle\u2019s platform.\n\n_check\\_box\\_outline\\_blank_ Run 1 notebook or script\n\n_check\\_box\\_outline\\_blank_ Make 1 competition submission\n\n_check\\_box\\_outline\\_blank_ Make 1 comment\n\n_check\\_box\\_outline\\_blank_ Give 1 upvote\n\n![](https://www.kaggle.com/static/images/tiers/expert.svg)\n\n### Expert\n\nYou\u2019ve completed a significant body of work on Kaggle in one or more categories of expertise. Once you\u2019ve reached the expert tier for a category, you will be entered into the site wide Kaggle Ranking for that category.\n\n###### Competitions\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-bronze.svg)2 bronze medals\n\n###### Datasets\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-bronze.svg)3 bronze medals\n\n###### Notebooks\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-bronze.svg)5 bronze medals\n\n###### Discussion\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-bronze.svg)50 bronze medals\n\n![](https://www.kaggle.com/static/images/tiers/master.svg)\n\n### Master\n\nYou\u2019ve demonstrated excellence in one or more categories of expertise on Kaggle to reach this prestigious tier. Masters in the Competitions category are eligible for exclusive Master-Only competitions.\n\n###### Competitions\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)1 gold medal\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)2 silver medals\n\n###### Datasets\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)1 gold medal\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)4 silver medals\n\n###### Notebooks\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)10 silver medals\n\n###### Discussion\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)50 silver medals\n\n_check\\_box\\_outline\\_blank_ 200 medals in total\n\n![](https://www.kaggle.com/static/images/tiers/grandmaster.svg)\n\n### Grandmaster\n\nYou\u2019ve consistently demonstrated outstanding performance in one or more categories of expertise on Kaggle to reach this pinnacle tier. You\u2019re the best of the best.\n\n###### Competitions\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)5 gold medals\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)Solo gold medal\n\n###### Datasets\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)5 gold medals\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)5 silver medals\n\n###### Notebooks\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)15 gold medals\n\n###### Discussion\n\n_check\\_box\\_outline\\_blank_ ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)50 gold medals\n\n_check\\_box\\_outline\\_blank_ 500 medals in total\n\n## Medals\n\nTo reward your best work\n\nMedals are a standardized way of recognizing and rewarding excellent pieces of work across the categories of expertise on Kaggle. Each medal is awarded for a single accomplishment: a great competition result, a popular notebook, a useful dataset or an insightful comment.\n\n## Competition Medals\n\nCompetition medals are awarded for top competition results. The number of medals awarded per competition varies depending on the size of the competition. Percentage calculations are rounded down. For example, a competition with 9 teams will not award any gold medals. Note that Community, Playground, and Getting Started competitions typically do not award medals.\n\n|  | 0-99 Teams | 100-249 Teams | 250-999 Teams | 1000+ Teams |\n| --- | --- | --- | --- | --- |\n| ![](https://www.kaggle.com/static/images/medals/reflection-bronze.svg)Bronze | Top 40% | Top 40% | Top 100 | Top 10% |\n| ![](https://www.kaggle.com/static/images/medals/reflection-silver.svg)Silver | Top 20% | Top 20% | Top 50 | Top 5% |\n| ![](https://www.kaggle.com/static/images/medals/reflection-gold.svg)Gold | Top 10% | Top 10 | Top 10 + 0.2%\\* | Top 10 + 0.2%\\* |\n\n\\\\* (Top 10 + 0.2%) means that an extra gold medal will be awarded for every 500 additional teams in the competition. For example, a competition with 500 teams will award gold medals to the top 11 teams and a competition with 5000 teams will award gold medals to the top 20 teams.\n\n## Dataset Medals\n\nDataset Medals are awarded to popular public datasets published to the site, as measured by number of upvotes. Not all upvotes count towards medals: self-votes and votes by novices are excluded from medal calculation.\n...",
      "url": "https://www.kaggle.com/progression"
    },
    {
      "title": "Kaggle Rankings",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n# Kaggle Rankings\n\nThis live leaderboard shows the absolute best data scientists on Kaggle. Each category on Kaggle has its own leaderboard and point system. A Kaggler must be an Expert tier or higher to be ranked for that category. [Learn more](https://www.kaggle.com/progression/about)\n\n[Competition Rankings](https://www.kaggle.com/rankings/competitions) [Dataset Rankings](https://www.kaggle.com/rankings/datasets) [Code Rankings](https://www.kaggle.com/rankings/code) [Grandmasters](https://www.kaggle.com/rankings/grandmasters) [Awards](https://www.kaggle.com/rankings/awards)\n\n373 Grandmasters\n\n2,195 Masters\n\n10,839 Experts\n\nsearch\u200b\n\nlocation\\_onCityexpand\\_morebusinessOrganizationexpand\\_more\n\n| Rank | Tier | User | Joined | Medals | Points |\n| --- | --- | --- | --- | --- | --- |\n| 1 | Dieter | 8 years ago | 47173 | 157,882 |\n| 2 | c-number | 5 years ago | 1130 | 143,900 |\n| 3 | yuanzhe zhou | 6 years ago | 11119 | 136,998 |\n| 4 | hyd | 4 years ago | 14130 | 133,996 |\n| 5 | tascj | 10 years ago | 19113 | 115,263 |\n| 6 | Raja Biswas | 7 years ago | 1051 | 90,732 |\n| 7 | Psi | 13 years ago | 38100 | 82,509 |\n| 8 | Bartley | 5 years ago | 362 | 81,979 |\n| 9 | Nikita Babych | 5 years ago | 241 | 81,355 |\n| 10 | Eduardo Rocha de Andrade | 8 years ago | 9161 | 80,235 |\n| 11 | Pascal Pfeiffer | 7 years ago | 27123 | 79,981 |\n| 12 | Rib~ | 6 years ago | 10142 | 78,692 |\n| 13 | Harshit Sheoran | 7 years ago | 484 | 78,676 |\n| 14 | Darragh | 11 years ago | 27143 | 77,961 |\n| 15 | greySnow | 4 years ago | 610 | 76,922 |\n| 16 | Mathurin Ach\u00e9 | 14 years ago | 104958 | 76,741 |\n| 17 | Ethan | 8 years ago | 13133 | 74,951 |\n| 18 | Takoi | 9 years ago | 17153 | 73,927 |\n| 19 | Jeroen Cottaar | 2 years ago | 320 | 72,051 |\n| 20 | daiwakun | 4 years ago | 640 | 71,208 |\n| 21 | guo dashuai | 9 months ago | 220 | 68,314 |\n| 22 | Ahmet Erdem | 10 years ago | 28316 | 68,250 |\n| 23 | charmq | 6 years ago | 18127 | 64,878 |\n| 24 | sayoulala | 6 years ago | 661 | 64,781 |\n| 25 | ns64 | 9 years ago | 5106 | 63,706 |\n| 26 | RihanPiggy | 5 years ago | 742 | 63,292 |\n| 27 | Volodymyr | 7 years ago | 8146 | 61,134 |\n| 28 | yu4u | 9 years ago | 12166 | 60,832 |\n| 29 | James Day | 6 years ago | 351 | 59,976 |\n| 30 | Manuel Campos | 8 years ago | 34040 | 59,736 |\n| 31 | Arseny Poyda | 5 years ago | 530 | 58,367 |\n| 32 | YumeNeko | 6 years ago | 760 | 58,170 |\n| 33 | CPMP | 13 years ago | 27354 | 57,342 |\n| 34 | Nischay Dhankhar | 5 years ago | 123415 | 57,052 |\n| 35 | linrock | 3 years ago | 200 | 56,549 |\n| 36 | Theo Viel | 7 years ago | 21154 | 56,094 |\n| 37 | nagiss | 7 years ago | 970 | 55,918 |\n| 38 | Mike Kim | 13 years ago | 84539 | 54,980 |\n| 39 | senkin13 | 9 years ago | 1398 | 54,912 |\n| 40 | Filtered | 4 years ago | 226 | 54,840 |\n| 41 | (\u2299\ufe4f\u2299) | 8 years ago | 823 | 54,318 |\n| 42 | Jonathan Chan | 15 years ago | 21015 | 54,295 |\n| 43 | Camaro | 8 years ago | 8121 | 54,254 |\n| 44 | Eugene Khvedchenya | 8 years ago | 785 | 53,841 |\n| 45 | conor | 5 years ago | 400 | 53,447 |\n| 46 | old man in galaxy | 2 years ago | 331 | 52,519 |\n| 47 | NANACHI | 5 years ago | 243 | 52,252 |\n| 48 | flg | 7 years ago | 530 | 52,036 |\n| 49 | mohammad odeh | 8 years ago | 110 | 52,033 |\n| 50 | Junhua Yang | 2 years ago | 187 | 52,004 |\n| 51 | Patrick Yam | 7 years ago | 6143 | 51,802 |\n| 52 | gezi | 13 years ago | 1060 | 51,626 |\n| 53 | Leon | 6 years ago | 5128 | 51,475 |\n| 54 | ForcewithMe | 4 years ago | 9137 | 51,369 |\n| 55 | Chris Deotte | 7 years ago | 24249 | 51,337 |\n| 56 | kami | 7 years ago | 653 | 49,765 |\n| 57 | \ud83d\udc22 Jun Koda | 8 years ago | 592 | 49,598 |\n| 58 | Ilya Novoselskiy | 8 years ago | 361 | 48,696 |\n| 59 | tattaka | 7 years ago | 896 | 48,551 |\n| 60 | Guanshuo Xu | 10 years ago | 27252 | 48,024 |\n| 61 | Giba | 13 years ago | 645734 | 47,776 |\n| 62 | samson | 2 years ago | 446 | 47,485 |\n| 63 | vialactea | 8 years ago | 584 | 46,463 |\n| 64 | zhudong1949 | 5 years ago | 212 | 46,118 |\n| 65 | takaito | 5 years ago | 1917 | 45,754 |\n| 66 | kibuna | 8 years ago | 811 | 45,523 |\n| 67 | Kirderf | 6 years ago | 02039 | 45,514 |\n| 68 | bestfitting | 9 years ago | 42132 | 44,724 |\n| 69 | kwang | 8 years ago | 73517 | 44,502 |\n| 70 | Yevhenii Maslov | 4 years ago | 582 | 44,485 |\n| 71 | \u0110\u0103ng Nguy\u1ec5n H\u1ed3ng | 3 years ago | 322 | 44,360 |\n| 72 | Ali | 4 years ago | 1613 | 44,331 |\n| 73 | Nazarko99 | 3 years ago | 290 | 43,756 |\n| 74 | tomirol | 6 years ago | 581 | 43,486 |\n| 75 | hengck23 | 12 years ago | 10248 | 43,343 |\n| 76 | Victor Shlepov | 3 years ago | 210 | 43,134 |\n| 77 | AkiraIshikawa | 8 years ago | 01218 | 43,108 |\n| 78 | Khoi Nguyen | 8 years ago | 733 | 43,012 |\n| 79 | Ryota | 6 years ago | 6133 | 42,440 |\n| 80 | Joseph | 6 years ago | 4152 | 42,304 |\n| 81 | baellouf | 2 years ago | 112 | 42,288 |\n| 82 | sakaku | 7 years ago | 673 | 42,088 |\n| 83 | Qishen Ha | 10 years ago | 23112 | 41,653 |\n| 84 | shanzhong8 | a year ago | 153 | 41,134 |\n| 85 | sqrt4kaido | 6 years ago | 5124 | 41,119 |\n| 86 | kenyeung.tech | 2 years ago | 201 | 41,019 |\n| 87 | kapenon | 6 years ago | 682 | 40,702 |\n| 88 | Kamal Das | 7 years ago | 0217 | 39,903 |\n| 89 | Tim Riggins | 7 years ago | 71611 | 39,594 |\n| 90 | suguuuuu | 6 years ago | 474 | 39,108 |\n| 91 | Guillermo Barbadillo | 11 years ago | 471 | 38,960 |\n| 92 | Yauhen Babakhin | 10 years ago | 1656 | 38,651 |\n| 93 | Nat Bel ML Fun | 9 years ago | 01632 | 38,619 |\n| 94 | heng | 8 years ago | 71310 | 38,571 |\n| 95 | yuuniee | 4 years ago | 253 | 37,857 |\n| 96 | lhwcv | 9 years ago | 591 | 37,469 |\n| 97 | Anil Ozturk | 7 years ago | 372 | 37,302 |\n| 98 | Dracarys | 6 years ago | 4238 | 37,256 |\n| 99 | dott | 13 years ago | 1672 | 37,114 |\n| 100 | x-creative | 9 years ago | 4174 | 36,771 |",
      "url": "https://www.kaggle.com/rankings"
    },
    {
      "title": "How I Made It To The Kaggle Leaderboard - Wandb",
      "text": "<div><div><h2>Privacy Preference Center</h2><p>When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n<br/><a href=\"https://cookiepedia.co.uk/giving-consent-to-cookies\">More information</a></p><section><h3> Manage Consent Preferences</h3><div><h4>Strictly Necessary Cookies</h4><p>Always Active</p><p>These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.</p></div><div><h4>Functional Cookies</h4><p> </p><p>These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p></div><div><h4>Targeting Cookies</h4><p> </p><p>These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.</p></div><div><h4>Performance Cookies</h4><p> </p><p>These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.</p></div></section></div></div>",
      "url": "https://wandb.ai/lavanyashukla/kaggle-feature-encoding/reports/How-I-Made-It-To-The-Kaggle-Leaderboard--Vmlldzo2NzQyNQ"
    },
    {
      "title": "Kaggle Playground: How Top Competitors Actually Win in 2025",
      "text": "Kaggle Playground: How Top Competitors Actually Win in 2025 | by Gaurab Baral | Dec, 2025 | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@gauurab/kaggle-playground-how-top-competitors-actually-win-in-2025-c75d4b380bb5&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@gauurab/kaggle-playground-how-top-competitors-actually-win-in-2025-c75d4b380bb5&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Kaggle Playground: How Top Competitors Actually Win in 2025\n[\n![Gaurab Baral](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*qXh_x_jpd7a9k8RK)\n](https://medium.com/@gauurab?source=post_page---byline--c75d4b380bb5---------------------------------------)\n[Gaurab Baral](https://medium.com/@gauurab?source=post_page---byline--c75d4b380bb5---------------------------------------)\n2 min read\n\u00b7Dec 5, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/c75d4b380bb5&amp;operation=register&amp;redirect=https://medium.com/@gauurab/kaggle-playground-how-top-competitors-actually-win-in-2025-c75d4b380bb5&amp;user=Gaurab+Baral&amp;userId=9730844b568e&amp;source=---header_actions--c75d4b380bb5---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/c75d4b380bb5&amp;operation=register&amp;redirect=https://medium.com/@gauurab/kaggle-playground-how-top-competitors-actually-win-in-2025-c75d4b380bb5&amp;source=---header_actions--c75d4b380bb5---------------------bookmark_footer------------------)\nListen\nShare\n**A Fun Yet Fierce Training Ground for Machine Learning**\nKaggle Playground competitions are the perfect middle step between beginner \u201cGetting Started\u201d contests and the high-stakes Featured competitions. They come out every month with clean tabular datasets that thousands of people join. In Season 5 (2025), we saw challenges like predicting podcast listening time (3,310 teams), calorie burn (4,316 teams), rainfall patterns (4,381 teams), and optimal fertilizers (2,648 teams). Prizes are just t-shirts and stickers for the top three, so the real reward is rapid skill growth. Because the datasets are approachable but the leaderboard is brutally competitive, Playground has become the place where serious data scientists test new ideas without pressure.\n**The Winning Recipe: Ensembles + Smart Features + Speed**\nIn 2024 and 2025, one truth stands out: almost nobody wins with a single model. The best scores come from blending dozens (sometimes 70+) different models. AutoGluon dominated 2024, grabbing medals in 15 of 18 tabular contests and seven gold medals. Grandmaster Chris Deotte took first place in the April 2025 Podcast competition with a three-level stack of 72 models built using RAPIDS cuML on GPU: XGBoost, LightGBM, CatBoost, neural nets, TabPFN, KNN, SVR, Ridge, and Random Forest, all combined with another layer of Ridge and gradient boosting. Feature engineering is just as important. Winners routinely create thousands of new features (groupby statistics, interaction terms, binning, digit extraction) and let the model decide what works. A common trick is treating every number as both numeric and categorical at the same time. When the original dataset is public, blending it with the synthetic Playground data often gives the final boost.\n**Why Champions Keep Pulling Ahead**\nThe gap between silver and gold is usually decided by three habits. First, perfect cross-validation discipline: every single model must use exactly the same folds so out-of-fold predictions can be stacked safely. Second, real model diversity: different features, different preprocessing, different seeds, different algorithms. Third, fast experimentation with GPUs. Tools like RAPIDS cuML and cuDF let top players train hundreds of models in hours instead of days, then use hill-climbing or simple greedy selection to build the final ensemble. The clear pattern across recent winning solutions is simple: one great model rarely wins. Ten good models beat one great model. Seventy slightly different good models, carefully stacked, dominate the leaderboard.\nPlayground competitions look casual, but the techniques that win them are the same ones that win $100,000 Featured contests. Jump in, copy what the medalists are doing, and you will level up faster than almost anywhere else in machine learning. The next season is already live. See you on the leaderboard.\n[\n![Gaurab Baral](https://miro.medium.com/v2/resize:fill:96:96/0*qXh_x_jpd7a9k8RK)\n](https://medium.com/@gauurab?source=post_page---post_author_info--c75d4b380bb5---------------------------------------)\n[\n![Gaurab Baral](https://miro.medium.com/v2/resize:fill:128:128/0*qXh_x_jpd7a9k8RK)\n](https://medium.com/@gauurab?source=post_page---post_author_info--c75d4b380bb5---------------------------------------)\n[## Written byGaurab Baral\n](https://medium.com/@gauurab?source=post_page---post_author_info--c75d4b380bb5---------------------------------------)\n[2 followers](https://medium.com/@gauurab/followers?source=post_page---post_author_info--c75d4b380bb5---------------------------------------)\n\u00b7[2 following](https://medium.com/@gauurab/following?source=post_page---post_author_info--c75d4b380bb5---------------------------------------)\nI am interested in learning about AI, computers and the universe.\n## No responses yet\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--c75d4b380bb5---------------------------------------)\n[\nHelp\n](https://help.medium.com/hc/en-us?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nStatus\n](https://status.medium.com/?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nAbout\n](https://medium.com/about?autoplay=1&amp;source=post_page-----c75d4b380bb5---------------------------------------)\n[\nCareers\n](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nPress\n](mailto:pressinquiries@medium.com)\n[\nBlog\n](https://blog.medium.com/?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nPrivacy\n](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nRules\n](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nTerms\n](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c75d4b380bb5---------------------------------------)\n[\nText to speech\n](https://speechify.com/medium?source=post_page-----c75d4b380bb5---------------------------------------)",
      "url": "https://medium.com/@gauurab/kaggle-playground-how-top-competitors-actually-win-in-2025-c75d4b380bb5"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    }
  ]
}