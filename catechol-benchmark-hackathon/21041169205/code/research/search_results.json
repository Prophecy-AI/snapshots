{
  "query": "How do Kaggle competition winners handle when cross-validation score is much better than leaderboard score due to distribution shift in chemistry prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on insights from Kaggle practitioners and recent drug\u2011discovery research, winners typically mitigate a large CV\u2011vs\u2011leaderboard gap caused by distribution shift in chemistry challenges by **re\u2011designing validation and model\u2011selection pipelines** ([Chioka\u202f.in](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio);\u202f[PMC\u202farticle](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)):\n\n1. **Replace random k\u2011fold CV with an out\u2011of\u2011distribution (forward) split** \u2013 e.g., a k\u2011fold *n\u2011step forward* cross\u2011validation that respects temporal or scaffold\u2011based ordering of molecules. This mimics the test\u2011set shift and yields a more realistic estimate of leaderboard performance.  \n2. **Add domain\u2011specific validation metrics** such as discovery yield and novelty error (or applicability\u2011domain scores) to complement the primary metric; they highlight where the model may fail on novel chemistry.  \n3. **Run multiple CV repetitions** (different seeds, more folds) and average the results to reduce variance and detect over\u2011optimistic folds.  \n4. **Select a small set of diverse models** (e.g., one tree\u2011based, one graph\u2011neural\u2011network) rather than many similar variants; diversity lowers the risk that all models suffer the same distribution\u2011shift error.  \n5. **Ensemble the diverse models** (simple averaging or weighted stacking) and validate the ensemble with the forward CV; ensembles tend to be more robust to shift.  \n6. **Calibrate the final submission on a hold\u2011out \u201cprivate\u2011leaderboard\u2011like\u201d slice** (if available) before the final upload, adjusting thresholds or blending weights to better match the observed private score.  \n7. **Document the shift** \u2013 keep a log of CV vs. leaderboard gaps and the validation scheme used, so future iterations can be compared and the approach can be reproduced.  \n\nFollowing these steps lets winners align their CV estimates with the true leaderboard distribution, reducing the surprise drop caused by chemistry\u2011related data shift.",
      "url": ""
    },
    {
      "title": "How to Select Your Final Models in a Kaggle Competition",
      "text": "Did your rank just drop sharp in the private leaderboard in a Kaggle\u00a0competition?\n[![picard palm](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)\nI\u2019ve been through that, too. We all learn about overfitting when we started machine learning,\u00a0but Kaggle makes\u00a0you really feel the pain of overfitting.\u00a0Should I have been more careful in the [Higgs Boson Machine Learning competition](http://www.kaggle.com/c/higgs-boson/), I would have selected a solution that would gave me a rank 4 than rank 22.\nI vow to come out with some principles systematically select\u00a0final models. Here are the lessons learnt:\n- **Always do cross-validation to get a reliable metric.**\u00a0If you don\u2019t, the validation score you get on a single\u00a0validation set\u00a0is unlikely to reflect the model performance in general. Then, you will likely see\u00a0a model improvement in that single validation set, but actually performs worse in general. **_Keep in mind the CV score can be optimistic, but your model is still overfitting._**\n- **Trust your CV\u00a0score, and not LB\u00a0score.** The leaderboard score\u00a0is scored only on a small percentage of the full test set. In some cases, it\u2019s only a few hundred test cases. Your cross-validation score will be much more reliable in general.\n- If your CV score is not stable (perhaps due to ensembling methods), you can\u00a0run your CV with more folds and multiple times to\u00a0take average.\n- If a single\u00a0CV\u00a0run is very slow, use a subset of the data to run\u00a0the CV. This will help your CV loop to run faster. Of course, the subset should not be too small or else the CV score will not be representative.\n- **For the final 2 models, pick very different models.** Picking two very similar solutions means that your solutions either fail together or win together, effectively meaning that you only pick one model. You should reduce your risk by picking two confident but very different models.\u00a0_**You should not depend on the leaderboard score at all.**_\n- Try to group your solutions by methodologies. Then, pick the best CV score model from each group. Then compare these best candidates of each group, pick two.\n- Example: I have different groups 1)\u00a0Bagging of SVMs 2) RandomForest 3) Neural Networks 4) LinearModels. Then, each group should produce one single best model, then you pick 2 out of these.\n- **Pick a robust methodology.** Here is the tricky part\u00a0which depends on experience, even if you have done cross validation, you can still get burned:\u00a0Sketchy methods of improving the CV score like\u00a0making cubic features, cubic root features, boosting like crazy, magical\u00a0numbers(without understanding it), etc, will likely be a bad model to pick even if the CV score is good. Unfortunately, you will probably have to make this mistake once to know what this means. =\\]\nApplying the above\u00a0principles to the recent competition\u00a0[Africa Soil Property Prediction Challenge](http://www.kaggle.com/c/afsis-soil-properties), plus a bit of luck, I picked the top 1 and top 2 models.\n[![top score](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)\nSorted by private score\nI ended up Top 10% with a rank of\u00a090 by spending just\u00a0a week time and mostly in Mexico in a vacation. I guess,\u00a0not too bad?",
      "url": "https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Winning solutions of kaggle competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions"
    },
    {
      "title": "How to Win Kaggle Competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/discussions/getting-started/44997"
    },
    {
      "title": "Kaggle Winning Solutions: AI Trends & Insights",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/tahaalselwii/kaggle-winning-solutions-ai-trends-insights"
    },
    {
      "title": "How to Use Kaggle: Competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions"
    },
    {
      "title": "The Most Comprehensive List of Kaggle Solutions and Ideas",
      "text": "Kaggle Solutions\n[\n**Kaggle**Solutions\n](https://farid.one/kaggle-solutions/)\n[![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png)](https://github.com/faridrashidi/kaggle-solutions)\n## The Most Comprehensive List of Kaggle Solutions and Ideas\nThis repository contains a comprehensive collection of solutions and ideas shared by top performers from past Kaggle competitions. The list is continuously updated with new insights after each competition concludes. If you discover a solution not yet listed here, feel free to contribute by submitting a pull request. You can find a guide to the symbols used in this list[here](https://farid.one/kaggle-solutions/resources/symbols.html).\nIf you find this resource valuable, consider giving it a star or forking it to support and expand the community!\nExplore these insightful pages for:\n* [Top Kagglers Interviews and Lectures](https://farid.one/kaggle-solutions/resources/videos.html)\n* [Kernels of The Week](https://farid.one/kaggle-solutions/resources/kernels.html)\n</br>\n****Last Updated:**January 3, 2026\n||Title & Description|Details|Solutions|Pins|\n[![](https://farid.one/kaggle-solutions/assets/logos/91723.webp)](https://www.kaggle.com/c/playground-series-s5e12)|[**682. Playground Series - Season 5, Episode 12**](https://www.kaggle.com/c/playground-series-s5e12)\n</br>\nDiabetes Prediction Challenge\n|\nPrize: Swag\nTeam: 4,206\nKind: Playground\nMetric: Roc Auc Score\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/playground-series-s5e12/writeups/1st-place-solution-hill-climbing-ridge-ensembl)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/playground-series-s5e12/writeups/2nd-place-solution-winning-based-on-id-shift-an)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/playground-series-s5e12/writeups/add-tabm-for-diversity)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/114250.webp)](https://www.kaggle.com/c/nfl-big-data-bowl-2026-analytics)|[**681. NFL Big Data Bowl 2026 - Analytics**](https://www.kaggle.com/c/nfl-big-data-bowl-2026-analytics)\n</br>\nUnderstand player movement while the ball is in the air\n|\nPrize: $50,000\nTeam: 278\nKind: Featured\nMetric: -\nYear: 2025\n|\n* **Not Available!|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/59156.webp)](https://www.kaggle.com/c/MABe-mouse-behavior-detection)|[**680. MABe Challenge - Social Action Recognition in Mice**](https://www.kaggle.com/c/MABe-mouse-behavior-detection)\n</br>\nDetect unique behaviors from pose estimates of mice.\n|\nPrize: $50,000\nTeam: 1,412\nKind: Research\nMetric: MABe F Beta\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/2nd-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)3rd place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/3rd-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/4th-place-solution-xgb-nn-ensemble)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/5th-place-gnn-egocentric-squeezeformer)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)7th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/7th-place-gold-cnn-transformer-with-invariant-fe)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)9th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/9th-place-gold)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)10th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/10th-place-solution-st-gcn-transformer)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)12th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/12th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)15th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/15th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)17th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/key-improvements-in-our-solution-a-technical-summ)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)19th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/19th-place-4-body-parts-invariant-features-with)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)26th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/27th-place-solution)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/120570.webp)](https://www.kaggle.com/c/gemini-3)|[**679. Google DeepMind - Vibe Code with Gemini 3 Pro in Al Studio**](https://www.kaggle.com/c/gemini-3)\n</br>\nBuild with Gemini 3 and compete for $500,000 in credits\n|\nPrize: $500,000\nTeam: 4,102\nKind: Featured\nMetric: -\nYear: 2025\n|\n* **Not Available!|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/91722.webp)](https://www.kaggle.com/c/playground-series-s5e11)|[**678. Playground Series - Season 5, Episode 11**](https://www.kaggle.com/c/playground-series-s5e11)\n</br>\nPredicting Loan Payback\n|\nPrize: Swag\nTeam: 3,724\nKind: Playground\nMetric: Roc Auc Score\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/playground-series-s5e11/writeups/1st-place-a-lot-of-features-a-lot-of-models-an)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/playground-series-s5e11/writeups/2nd-place-solution-7-models-but-1-was-also-enou)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/4th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/5th-place-solution-xgb-lgbm-tabm5seeds-ag)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)6th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/6-solution-ensembling-was-the-key)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)8th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/rank8-approach-trust-the-cv-score)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)10th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/a-10th-place-experiment)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/91496.webp)](https://www.kaggle.com/c/arc-prize-2025)|[**677. ARC Prize 2025**](https://www.kaggle.com/c/arc-prize-2025)\n</br>\nCreate an AI capable of novel reasoning\n|\nPrize: $1,000,000\nTeam: 1,554\nKind: Featured\nMetric: Abstraction and Reasoning Challenge\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/arc-prize-2025/writeups/nvarc)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)3rd place](https://www.kaggle.com/c/arc-prize-2025/writeups/mindsai-and-tufa-labs-arc-prize-2025-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/arc-prize-2025/writeups/arc-prize-2025-competition-writeup-5th-place)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/arc-prize-2025/wri...",
      "url": "https://farid.one/kaggle-solutions"
    },
    {
      "title": "How to win your first Kaggle competition? - dataroots",
      "text": "How to win your first Kaggle competition?\n[![symbol](https://dataroots.io/_next/static/media/symbol-rainbow.66f0e23b.svg)](https://dataroots.io/)\n![dataroots hero](https://dataroots.io/_next/static/media/glow-bottom-green.eb20c0f6.svg)\n# How to win your first Kaggle competition?\n[Get In Touch-&gt;](https://dataroots.io/contact-us)\n[Careers](https://dataroots.io/careers)\n[DNAOur DNA](https://dataroots.io/our-dna)\n[file-articleBlog](https://dataroots.io/blog)\n[podcastPodcast](https://dataroots.io/datatopics)\nByAdrien Debray, Johannes Lootens\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n## All you need to know about Kaggle competitions\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled-1.png)Kaggle competition overview page\n\ud83d\udca1Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\nFirstly there are the**getting-started competitions**, such as the[Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io)or[Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io)ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the**community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\nWhat actually tends to attract people to Kaggle are the**competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\nEvery single one of those competitions is defined by a**dataset**and an**evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\nWhile the********************train set********************is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the**public leaderboard test set**which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the**private leaderboard test set.**This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n### Notebooks\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n## How to take the W in a Kaggle competition ?\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\nWe participated in the[Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io)competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n### 1. Have a good understanding of the competition and how to tackle the problem\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n* Read the competition overview and linked resources thoroughly\n* Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n* Check existing literature on approaches that were tried/succeeded in solving this or similar problems### 2. Get inspired by other participants\u2019 work to get started\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled--1--1.png)Go in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\nBased on your readings, choose a clear and simple notebook with a decent LB score as**baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n### 3. Improve your model in an efficient way\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n##### Create datasets for intermediate results / preprocessed data\nSaved preprocessed datasets and trained models will make your results comparis...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    }
  ]
}