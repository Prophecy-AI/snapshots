## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (153%)
- Submissions remaining: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES
- **CRITICAL**: Intercept (0.0525) is 151% of target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)
- **CONCLUSION**: Linear CV improvements CANNOT reach target. We must REDUCE THE INTERCEPT.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The sophisticated ensemble experiment was well-executed.
- Evaluator's top priority: Implement FULL pipeline from gentilless kernel (non-linear mixture, advanced features, stronger hyperparameters, SE attention). **AGREE - this is the highest leverage action.**
- Key concerns raised: 
  1. Non-linear mixture features NOT implemented in exp_046
  2. Advanced feature engineering NOT implemented
  3. Hyperparameters too weak (200 vs 12000 iterations)
  4. SE attention blocks NOT implemented
- **CRITICAL INSIGHT**: Simply adding model diversity (exp_046) doesn't help. We need the FULL pipeline.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop47_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions fall on the same CV-LB line (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL error from distribution shift
  3. Test solvents are fundamentally different from training solvents
  4. Sophisticated ensemble (exp_046) was 17.82% WORSE than baseline

## CRITICAL INSIGHT: What Top Kernels Do Differently

### gentilless/best-work-here Kernel Techniques (NOT YET IMPLEMENTED):

1. **Non-Linear Mixture Formula** (CRITICAL):
```python
# Instead of: mixture = (1 - r) * A + r * B
# Use: mixture = A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)
```

2. **Advanced Feature Engineering**:
```python
# Polynomial features
features.append(numeric_feat ** 2)
features.append(np.sqrt(np.abs(numeric_feat) + 1e-8))

# Interaction terms
features.append((numeric_feat[:, 0] * numeric_feat[:, 1]).reshape(-1, 1))
features.append((numeric_feat[:, 0] * numeric_feat[:, 2]).reshape(-1, 1))
features.append((numeric_feat[:, 1] * numeric_feat[:, 2]).reshape(-1, 1))

# Statistical features from molecular descriptors
mol_stats = np.column_stack([
    mol_feat.mean(axis=1),
    mol_feat.std(axis=1),
    mol_feat.max(axis=1),
    mol_feat.min(axis=1)
])
```

3. **Much Stronger Hyperparameters** (60x more iterations):
- CatBoost: 3000+ iterations, depth=8-9, early_stop=100
- XGBoost: 3000+ rounds, eta=0.02, depth=8-9
- LightGBM: 3000+ rounds, lr=0.015, leaves=127
- Neural Network: 300+ epochs, hidden=[512, 256, 128, 64]

4. **SE Attention Blocks**:
```python
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        return x * self.fc(x)
```

5. **Adaptive Ensemble Weighting**:
```python
# Weight models inversely proportional to their validation MSE
weights = [1 / (mse ** 2.5) for mse in val_mses]
weights = [w / sum(weights) for w in weights]
```

### matthewmaree/ens-model Kernel Techniques:

1. **Combined Feature Table** from multiple sources:
   - spange_descriptors (priority 5)
   - acs_pca_descriptors (priority 4)
   - drfps_catechol (priority 3)
   - fragprints (priority 2)
   - smiles (priority 1)

2. **Correlation-Based Feature Filtering** with priority ordering

3. **CatBoost + XGBoost Ensemble** with different weights:
   - Single solvent: CatBoost 7.0, XGBoost 6.0
   - Full data: CatBoost 1.0, XGBoost 2.0

4. **Numeric Feature Engineering**:
```python
X_num["T_x_RT"] = T * rt  # Interaction term
X_num["RT_log"] = np.log(rt + 1e-6)  # Log transformation
X_num["T_inv"] = 1 / T  # Inverse temperature
X_num["RT_scaled"] = rt / rt.mean()  # Scaled residence time
```

## Recommended Approaches (Priority Order)

### Priority 1: Full gentilless Pipeline Implementation
**Why**: This kernel has techniques we haven't fully implemented. The key differentiators are:
1. Non-linear mixture formula with interaction term
2. Advanced feature engineering (polynomial, interaction, statistical)
3. Much stronger hyperparameters (60x more iterations)
4. SE attention blocks in neural networks
5. Adaptive ensemble weighting

**Implementation Strategy**:
1. Start with our best model (GP+MLP+LGBM from exp_030)
2. Add non-linear mixture formula
3. Add advanced feature engineering
4. Increase hyperparameters (at least 3000 iterations for tree models)
5. Add SE attention blocks to MLP
6. Use adaptive ensemble weighting

**HYPOTHESIS**: These techniques may CHANGE the CV-LB relationship, not just improve CV. The intercept may decrease.

### Priority 2: CatBoost (Not Yet Tried)
**Why**: The matthewmaree kernel uses CatBoost as one of its models. We haven't tried CatBoost yet.
- CatBoost handles categorical features natively
- May have different behavior than XGBoost/LightGBM
- The matthewmaree kernel uses CatBoost with weight 7.0 for single solvent

**Implementation**:
```python
from catboost import CatBoostRegressor

cb_params = {
    'iterations': 3000,
    'learning_rate': 0.02,
    'depth': 8,
    'l2_leaf_reg': 2.5,
    'subsample': 0.88,
    'early_stopping_rounds': 100,
    'verbose': False
}
```

### Priority 3: Combined Feature Table
**Why**: The matthewmaree kernel combines features from multiple sources with priority-based correlation filtering.
- spange_descriptors (physicochemical properties)
- acs_pca_descriptors (PCA of ACS descriptors)
- drfps_catechol (molecular fingerprints)
- fragprints (fragment fingerprints)
- smiles (SMILES-based features)

**Implementation**:
```python
def build_combined_features():
    sources = ['spange_descriptors', 'acs_pca_descriptors', 'drfps_catechol', 'fragprints']
    dfs = [load_features(src) for src in sources]
    combined = pd.concat(dfs, axis=1)
    # Filter correlated features with priority ordering
    return filter_correlated_features(combined, threshold=0.9)
```

## What NOT to Try
1. **Simple model diversity** - exp_046 showed this doesn't help (17.82% worse)
2. **Mean reversion** - exp_045 showed this hurts CV
3. **GNN** - exp_040 failed with 8.4x worse MSE
4. **ChemBERTa embeddings** - exp_041 failed
5. **Learned embeddings** - exp_039 failed (test solvents never seen)
6. **Weak hyperparameters** - 200 iterations is too few, need 3000+

## Validation Notes
- Use Leave-One-Solvent-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for mixtures (13 folds)
- CV-LB gap is ~4.3x, but intercept is the real problem
- **CRITICAL**: Monitor if new approach changes the CV-LB relationship

## Submission Strategy
- 5 submissions remaining
- Only submit if:
  1. CV improves significantly (>5% improvement)
  2. Using a fundamentally different approach (full gentilless pipeline)
  3. Need to verify if new approach changes CV-LB relationship

## Key Insight for Executor
The problem is NOT model quality - it's DISTRIBUTION SHIFT. All 12 submissions fall on the same CV-LB line. To reach target, we need to:
1. Implement the FULL pipeline from top kernels (not just model diversity)
2. Use non-linear mixture formula with interaction term
3. Add advanced feature engineering (polynomial, interaction, statistical)
4. Use much stronger hyperparameters (3000+ iterations)
5. Try CatBoost (not yet tried)
6. Use adaptive ensemble weighting based on validation MSE

**DO NOT** keep optimizing standard ML approaches with weak hyperparameters. The intercept won't change.

## Specific Implementation for Next Experiment

Create a model that implements:
1. **Non-linear mixture formula**: `A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)`
2. **Advanced feature engineering**: polynomial (X^2, sqrt(|X|)), interaction (X1*X2), statistical (mean, std, max, min of molecular features)
3. **CatBoost + XGBoost + LightGBM ensemble** with adaptive weighting
4. **Stronger hyperparameters**: 3000+ iterations for tree models
5. **SE attention blocks** in MLP (if time permits)

This is the highest-leverage experiment we can run.
