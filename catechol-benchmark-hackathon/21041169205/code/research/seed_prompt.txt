## Current Status
- Best CV score: 0.0081 from exp_049/exp_050 (CatBoost+XGBoost, but submission errors)
- Best LB score: 0.0877 from exp_030/exp_067 (GP+MLP+LGBM)
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)
- Submissions remaining: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (R² = 0.9558)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES
- **CRITICAL**: Intercept (0.0520) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.004 (IMPOSSIBLE)

**This is a STRUCTURAL DISTRIBUTION SHIFT problem. Standard CV optimization CANNOT reach the target.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_091 implementation was correct but performed poorly (CV=0.014242, 76% worse than best).
- Evaluator's top priority: DO NOT submit exp_091, try non-linear mixture features. **STRONGLY AGREE**.
- Key concerns raised:
  1. Per-target HGB/ETR approach doesn't work for our setup - **AGREED, CV was 76% worse**
  2. The intercept problem remains unsolved - **AGREED, this is the core issue**
  3. Best CV model (exp_049-053) has submission errors - **AGREED, need to debug**
  4. Non-linear mixture features from best-work-here kernel are untried - **AGREED, high priority**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop96_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. **CV-LB relationship is extremely linear** (R²=0.9558) - all approaches fall on the same line
  2. **Intercept (0.052) > Target (0.0347)** - standard CV optimization cannot reach target
  3. **Best LB (0.0877) achieved with GP+MLP+LGBM ensemble** - exp_030, exp_067
  4. **Best CV (0.0081) achieved with CatBoost+XGBoost** - but submission errors

## Recommended Approaches

### PRIORITY 1: Non-Linear Mixture Features (HIGH PRIORITY - UNTRIED)
**Rationale**: The best-work-here kernel uses non-linear mixture features that capture solvent-solvent interactions:
```python
# Current linear mixing:
mixture_feats = A * (1 - r) + B * r

# Non-linear mixing (from best-work-here):
mixture_feats = A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)
```

**Why this could change the CV-LB relationship**:
1. The non-linear term `0.05 * A * B * r * (1 - r)` captures interaction effects
2. This is fundamentally different from all 96 experiments we've run
3. Could reduce the intercept by better modeling mixture behavior

**Implementation**:
```python
class NonLinearMixtureFeaturizer:
    def featurize(self, X, flip=False):
        # ... standard features ...
        if self.mixed:
            r = X["SolventB%"].values.reshape(-1, 1) / 100.0
            A = self.lookup.loc[X["SOLVENT A NAME"]].values
            B = self.lookup.loc[X["SOLVENT B NAME"]].values
            
            # Non-linear mixture features
            linear = A * (1 - r) + B * r
            interaction = 0.05 * A * B * r * (1 - r)
            mixture_feats = linear + interaction
            
            return np.hstack([X_kinetic, mixture_feats])
```

Apply this to the best GP+MLP+LGBM ensemble (exp_030) which achieved LB=0.0877.

### PRIORITY 2: Fix Submission Errors for Best CV Model
**Rationale**: exp_049-053 achieved CV=0.0081 (best) but failed with "Evaluation metric raised an unexpected error".

**Debugging steps**:
1. Check submission format matches template exactly
2. Ensure no extra cells after final submission cell
3. Verify no NaN or infinite values in predictions
4. Check column names and data types match expected format

If fixed, predicted LB = 4.36 * 0.0081 + 0.052 = 0.0873 (close to best 0.0877).

### PRIORITY 3: Adaptive Ensemble Weighting
**Rationale**: From best-work-here kernel - weight models by inverse validation error:
```python
def adaptive_ensemble(val_preds, val_scores, power=2.5):
    # Higher power = more weight to better models
    inv_errors = [1.0 / (score + 1e-8) for score in val_scores]
    weights = [(ie ** power) / sum([x ** power for x in inv_errors]) for ie in inv_errors]
    return sum(p * w for p, w in zip(val_preds, weights))
```

This gives more weight to better-performing models per fold, potentially reducing variance.

### PRIORITY 4: Solvent Similarity-Based Conservative Predictions
**Rationale**: The intercept represents extrapolation error to unseen solvents.

**Implementation**:
1. Calculate similarity of test solvent to training solvents (using Spange descriptors)
2. When test solvent is dissimilar (low max similarity), blend toward population mean
3. This could reduce extrapolation error (the intercept)

```python
def similarity_weighted_prediction(pred, test_solvent, train_solvents, alpha=0.3):
    # Calculate max similarity to training solvents
    similarities = [cosine_similarity(test_solvent, train) for train in train_solvents]
    max_sim = max(similarities)
    
    # Blend toward mean when extrapolating
    population_mean = 0.33  # Approximate mean yield
    blend_factor = alpha * (1 - max_sim)  # More blending when dissimilar
    return pred * (1 - blend_factor) + population_mean * blend_factor
```

## What NOT to Try
1. **Per-target heterogeneous ensemble (HGB/ETR)** - exp_091 showed 76% worse CV
2. **Neural network architectures (GNN, GAT, ChemBERTa)** - 5+ consecutive failures, all 100%+ worse
3. **Aggressive correlation filtering** - exp_090 dropped 70% features, hurt performance
4. **Standard CV optimization alone** - intercept > target means this cannot reach target

## Validation Notes
- CV scheme: Leave-One-Out for single solvent, Leave-One-Ramp-Out for full data
- CV-LB gap: ~4.36x multiplier + 0.052 intercept
- **Key insight**: The intercept (0.052) > target (0.0347) means we need to CHANGE the relationship, not just improve CV
- Submit only if:
  1. CV < 0.0083 (best LB achieved with CV=0.0083)
  2. AND approach is fundamentally different (could change intercept)

## Submission Strategy (4 remaining)
1. **DO NOT submit exp_091** - CV=0.014242 is 76% worse than best
2. **Try non-linear mixture features first** - if CV improves, consider submitting
3. **Fix submission errors for best CV model** - if fixed, submit to validate
4. **Save 1-2 submissions** for final approaches that could change the CV-LB relationship

## Key Insight
The target (0.0347) IS achievable - top competitors have achieved it. The key is to find an approach that:
1. Changes the CV-LB relationship (reduces the intercept)
2. Or achieves CV so low that even with the current relationship, LB < 0.0347

The non-linear mixture features are the most promising untried approach because:
1. They capture solvent-solvent interactions that linear mixing misses
2. They're from a top kernel (best-work-here) that likely performs well
3. They're fundamentally different from all 96 experiments we've run
4. They could reduce the intercept by better modeling mixture behavior

## Implementation Notes
The model MUST follow the competition template structure:
1. Last 3 cells must be the official CV cells (unchanged except model definition line)
2. Model must have `train_model(X, Y)` and `predict(X)` methods
3. Predictions must be clipped to [0, 1] range
4. Output must be torch tensor with shape [N, 3]

Start with the GP+MLP+LGBM ensemble from exp_030 (best LB=0.0877) and add non-linear mixture features.
