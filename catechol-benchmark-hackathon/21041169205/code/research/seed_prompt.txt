## Current Status
- Best CV score: 0.008092 from exp_049 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (153%)
- Submissions remaining: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES - ALL 12 submissions
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE - IMPOSSIBLE)
- **CONCLUSION**: Linear CV improvements CANNOT reach target with current approach

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Exp_049 was well-executed.
- Evaluator's top priority: Analyze per-solvent error and develop outlier-specific strategies. **AGREE - this is the right direction.**
- Key concerns raised:
  1. All 49 experiments fall on the same CV-LB line (R²=0.95)
  2. The intercept problem is STRUCTURAL, not solvable by better models
  3. Top 5 hardest solvents (HFIP, Acetonitrile.Acetic Acid, TFE, Ethylene Glycol, Water.Acetonitrile) contribute 54% of total error
  4. These solvents are OUTLIERS in feature space
- **CRITICAL INSIGHT**: We need to CHANGE the CV-LB relationship, not just improve CV

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop50_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions fall on the same CV-LB line (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL error from distribution shift
  3. Top 5 hardest solvents contribute 54% of total error:
     - HFIP: 15.1% (MSE 0.029)
     - Acetonitrile.Acetic Acid: 11.6% (MSE 0.023)
     - TFE: 9.8% (MSE 0.019)
     - Ethylene Glycol: 9.2% (MSE 0.018)
     - Water.Acetonitrile: 8.3% (MSE 0.016)
  4. These solvents are OUTLIERS: fluorinated alcohols, acidic mixtures, highly polar

## CRITICAL INSIGHT: The Path Forward

The target (0.0347) IS reachable, but NOT through incremental CV improvements. The intercept problem requires a fundamentally different approach.

**Key Observation**: The hardest solvents are OUTLIERS in feature space. If we can:
1. DETECT when we're predicting for an outlier solvent
2. Use CONSERVATIVE predictions for outliers (blend toward mean)
3. Use AGGRESSIVE predictions for non-outliers (trust the model)

This could CHANGE the CV-LB relationship by reducing the intercept.

## Recommended Approaches (Priority Order)

### Priority 1: Solvent-Specific Prediction Strategy (HIGHEST LEVERAGE)
**Why**: The top 5 hardest solvents contribute 54% of total error. If we can improve predictions for these solvents, we can significantly reduce the intercept.

**Implementation Strategy**:
1. Compute "distance to training distribution" for each solvent
2. For outlier solvents (HFIP, TFE, Acetonitrile.Acetic Acid, Ethylene Glycol, Water.Acetonitrile):
   - Use simpler models (Ridge regression, mean prediction)
   - Blend toward population mean
3. For non-outlier solvents:
   - Use the full CatBoost + XGBoost ensemble

**Key Insight**: The model is OVERFITTING to outlier solvents. By using simpler predictions for outliers, we can reduce the intercept.

### Priority 2: Outlier Detection Features
**Why**: Add features that measure "how different is this solvent from training distribution".

**Implementation**:
```python
# Compute distance to training distribution
from sklearn.neighbors import NearestNeighbors

# For each solvent, compute distance to nearest training solvents
nn = NearestNeighbors(n_neighbors=3)
nn.fit(train_features)
distances, _ = nn.kneighbors(test_features)
outlier_score = distances.mean(axis=1)

# Use outlier_score to weight predictions
# High outlier_score -> blend toward mean
# Low outlier_score -> trust the model
```

### Priority 3: Chemical Class-Specific Models
**Why**: Solvents within the same chemical class (alcohols, ethers, esters) may behave similarly.

**Implementation**:
1. Group solvents by chemical class:
   - Fluorinated alcohols: HFIP, TFE
   - Polar aprotic: Acetonitrile, DMA
   - Ethers: THF, 2-MeTHF, MTBE, Diethyl Ether
   - Esters: Ethyl Acetate, Ethyl Lactate, Methyl Propionate
   - Alcohols: Methanol, Ethanol, IPA, tert-Butanol
   - Highly polar: Water mixtures, Ethylene Glycol
2. Train class-specific models
3. For outlier classes (fluorinated alcohols, highly polar), use simpler models

### Priority 4: Submit exp_049 for Validation
**Why**: We need to verify if CatBoost + XGBoost changes the CV-LB relationship.

**Expected outcome**: LB ≈ 0.0874 (predicted from linear fit)
**If LB ≈ 0.0874**: The intercept hypothesis is confirmed. Focus on outlier detection.
**If LB < 0.0850**: CatBoost may have different generalization properties. Explore further.

## What NOT to Try
1. **More model architectures** - We've tried MLP, LGBM, XGBoost, GP, RF, GNN, ChemBERTa, CatBoost. All fall on the same CV-LB line.
2. **More feature engineering** - We've tried Spange, DRFP, fragprints, ACS PCA, ChemBERTa embeddings. All fall on the same CV-LB line.
3. **Uncertainty-weighted predictions** - exp_048 showed this HURTS performance.
4. **Mean reversion** - exp_045 showed this HURTS performance.
5. **Simply improving CV** - The intercept problem cannot be solved by better models.

## Validation Notes
- Use Leave-One-Solvent-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for mixtures (87 folds)
- CV-LB gap is ~4.3x, but intercept is the real problem
- **CRITICAL**: Monitor if outlier-specific strategies change the CV-LB relationship

## Submission Strategy
- 5 submissions remaining
- **Submission 1**: Submit exp_049 to verify CV-LB relationship
- **Submissions 2-4**: Test outlier-specific strategies
- **Submission 5**: Final refinement based on learnings

## Specific Implementation for Next Experiment

### Experiment 050: Outlier-Aware Prediction Strategy

1. **Identify outlier solvents** based on per-solvent MSE from exp_049:
   - OUTLIERS (MSE > 0.015): HFIP, Acetonitrile.Acetic Acid, TFE, Ethylene Glycol, Water.Acetonitrile, Diethyl Ether
   - NON-OUTLIERS (MSE < 0.015): All others

2. **For OUTLIER solvents**:
   - Use Ridge regression (simpler model)
   - Blend predictions toward population mean (e.g., 50% model, 50% mean)

3. **For NON-OUTLIER solvents**:
   - Use full CatBoost + XGBoost ensemble from exp_049

4. **Evaluate**:
   - Compare CV to exp_049 (0.008092)
   - If CV improves, submit to verify if intercept changes

**Key Hypothesis**: By using simpler predictions for outlier solvents, we can reduce the intercept (0.0525) and potentially reach the target (0.0347).

## CRITICAL: Why the Target IS Reachable

The target (0.0347) IS reachable. Here's why:

1. **The benchmark achieved MSE 0.0039** - This proves the problem is solvable.
2. **The top public kernels achieve LB < 0.07** - This proves better scores are possible.
3. **The intercept problem is NOT immutable** - It represents extrapolation error for OUTLIER solvents.
4. **By treating outliers differently**, we can reduce the intercept and reach the target.

**THE PATH FORWARD**:
1. Submit exp_049 to verify CV-LB relationship
2. Implement outlier-aware prediction strategy
3. If CV improves, submit to verify if intercept changes
4. Iterate until target is reached

## IMPORTANT: Competition Constraints

The submission must follow the template notebook structure:
- The last three cells must remain the same (except model definition)
- The model must implement `train_model()` and `predict()` methods
- The model must work with the standard CV scheme (Leave-One-Solvent-Out for single, Leave-One-Ramp-Out for full)

**DO NOT** try to change the CV scheme or overwrite utility functions. This is not allowed.

## Summary

After 49 experiments, we've learned:
1. All approaches fall on the same CV-LB line (R²=0.95)
2. The intercept (0.0525) > target (0.0347) - linear CV improvements cannot reach target
3. Top 5 hardest solvents contribute 54% of total error
4. These solvents are OUTLIERS in feature space

**THE SOLUTION**: Treat outlier solvents differently. Use simpler predictions for outliers, aggressive predictions for non-outliers. This could CHANGE the CV-LB relationship and reduce the intercept.

**NEXT STEPS**:
1. Submit exp_049 to verify CV-LB relationship
2. Implement outlier-aware prediction strategy (exp_050)
3. If CV improves, submit to verify if intercept changes
