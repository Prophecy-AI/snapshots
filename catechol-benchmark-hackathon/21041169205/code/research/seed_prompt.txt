## Current Status
- Best CV score: 0.0081 from exp_049/050 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (153% above target)
- Remaining submissions: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

**CRITICAL INSIGHT:** The intercept (0.0525) is HIGHER than the target (0.0347). This means the target is MATHEMATICALLY UNREACHABLE by improving CV alone with current approaches.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_057.
- Evaluator correctly identified that exp_057 CV (0.009524) is WORSE than best CV (0.008092).
- Key concern: The CV-LB intercept problem means we need fundamentally different approaches.

**CRITICAL ISSUE:** exp_056 (per_target_model) failed with "Evaluation metric raised an unexpected error". This is the 7th+ consecutive submission failure (exp_049-057 all have empty LB scores). The submission format looks correct but something is causing evaluation errors.

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns: Leave-one-solvent-out CV simulates predicting unseen solvents
- The test set likely contains "harder" solvents (more extreme properties)
- SM target is consistently the hardest to predict

## IMMEDIATE PRIORITY: Fix Submission Pipeline

The last 7+ submissions have failed. We need to:

1. **Regenerate submission from exp_030** (best LB 0.0877) - This submission worked before
   - Open `/home/code/experiments/030_gp_ensemble/gp_ensemble.ipynb`
   - Re-run all cells to generate a fresh submission
   - This will overwrite the current (broken) submission file

2. **Verify the submission format** matches the official template exactly:
   - Columns: id, index, task, fold, row, target_1, target_2, target_3
   - Task 0: 656 rows, 24 folds (single solvent)
   - Task 1: 1227 rows, 13 folds (full data)
   - All targets in [0, 1] range

3. **Submit to verify** the pipeline works before trying new approaches

## Recommended Approaches (After Fixing Submissions)

### Priority 1: Extrapolation Detection + Conservative Predictions
- Add features measuring solvent distance to training distribution
- When extrapolating, blend predictions toward population mean
- This should reduce error on "hard" test solvents and potentially reduce the intercept

### Priority 2: Uncertainty-Weighted Predictions
- Use GP with uncertainty estimates
- High uncertainty → conservative prediction (closer to mean)
- Ensemble variance as uncertainty proxy

### Priority 3: Study Top Public Kernels
- "mixall" kernel uses GroupKFold (5 splits) instead of Leave-One-Out
- "ens-model" kernel combines ALL feature sources
- What do the top LB scorers do differently?

## What NOT to Try
- More CV optimization without addressing the intercept problem
- Simple model variations that fall on the same CV-LB line
- Feature engineering that only improves CV

## Validation Notes
- CV scheme: Leave-one-solvent-out (24 folds for single, 13 folds for full)
- The CV-LB relationship is very strong (R² = 0.95)
- All model types fall on the same line - this is a STRUCTURAL problem

## CRITICAL ACTION ITEMS

1. **FIRST:** Re-run exp_030 notebook to regenerate a working submission
2. **SECOND:** Verify submission format is correct
3. **THIRD:** Submit to confirm the pipeline works
4. **FOURTH:** If working, implement extrapolation detection to try to reduce the intercept

**DO NOT GIVE UP.** The target IS reachable - we just haven't found the right approach yet.
