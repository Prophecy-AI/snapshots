## Current Status
- Best CV score: 0.008092 from exp_050/051/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)
- **CRITICAL: Last 5 submissions (exp_049-054) all FAILED with "Evaluation metric raised an unexpected error"**

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 * CV + 0.0528 (R² = 0.9523)
- Intercept = 0.0528
- Target = 0.0347
- **CRITICAL: Intercept (0.0528) > Target (0.0347)**
- Required CV to hit target: (0.0347 - 0.0528) / 4.29 = -0.0042 (NEGATIVE - IMPOSSIBLE)
- Even with CV = 0 (perfect training), predicted LB would be 0.0528
- **All approaches fall on the same CV-LB line** - MLP, LightGBM, XGBoost, CatBoost, GP, Ridge all follow this relationship

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The submission format is correct.
- Evaluator's top priority: Submit to verify format is correct, then pivot to intercept-reduction strategies.
- Key concerns raised: 
  1. The intercept problem - target is below the intercept, making it mathematically unreachable with current approach
  2. Submission failures - last 5 submissions failed with evaluation errors
  3. Diminishing returns - CV improvements translate to smaller LB improvements
- **I AGREE with the evaluator's assessment.** The current approach has hit a ceiling. We need to:
  1. First, understand why submissions are failing
  2. Then, pivot to strategies that could CHANGE the CV-LB relationship (reduce the intercept)

## IMMEDIATE PRIORITY: Fix Submission Failures

The last 5 submissions all failed. The submission format looks correct:
- 1883 rows (656 single + 1227 full)
- Correct columns: id, index, task, fold, row, target_1, target_2, target_3
- Task 0: 24 folds, Task 1: 13 folds
- All targets in [0, 1] range
- No NaN or Inf values

**Possible causes of failure:**
1. Kaggle-side evaluation issue (unlikely to persist for 5 submissions)
2. Something subtle about the evaluation we're missing
3. The evaluation expects specific fold/row ordering

**Action:** Study the "mixall" kernel (research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/) which is known to work. Implement their exact approach and verify it produces a valid submission.

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  - Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV
  - Full data: 1227 samples, 13 solvent pairs, leave-one-pair-out CV
  - Targets: Product 2, Product 3, SM (starting material)
  - Features: Spange descriptors (13 features), ACS PCA (10 features), DRFP (2048 sparse), fragprints

## Recommended Approaches

### PRIORITY 1: Implement "mixall" Kernel Approach
The "mixall" kernel (research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/) uses:
- Ensemble of MLP + XGBoost + RandomForest + LightGBM
- Weights: [0.4, 0.2, 0.2, 0.2]
- Spange descriptors as features
- Linear mixture interpolation for full data: (1-pct)*A + pct*B
- Standard scaling

**Why:** This kernel is known to work on Kaggle. Implementing it exactly will:
1. Verify our submission format is correct
2. Provide a working baseline to build upon
3. Potentially achieve a better LB score

### PRIORITY 2: Per-Solvent Error Analysis (if submissions work)
Once we have a working submission, analyze which solvents cause the most error:
```python
for solvent in all_solvents:
    test_mask = X['SOLVENT NAME'] == solvent
    train_mask = ~test_mask
    model.fit(X[train_mask], Y[train_mask])
    preds = model.predict(X[test_mask])
    error = np.mean((preds - Y[test_mask]) ** 2)
    print(f"{solvent}: MSE = {error:.6f}")
```
This will reveal if certain solvents are "hard" and causing the intercept problem.

### PRIORITY 3: Per-Target Model Selection
The public kernel "catechol-strategy-to-get-0-11161" uses different model types for different targets:
- SM target: HistGradientBoostingRegressor (harder target, needs more regularization)
- Product 2, Product 3: ExtraTreesRegressor (easier targets)

**Why:** Different targets may have different characteristics. Using specialized models could improve predictions.

### PRIORITY 4: Extrapolation Detection + Conservative Predictions
When predicting for solvents far from the training distribution:
1. Compute distance to nearest training solvent (using Spange descriptors)
2. When distance is high (extrapolating), blend predictions toward population mean
3. This could reduce the intercept by making more conservative predictions for "hard" solvents

## What NOT to Try
- More CV optimization without addressing the intercept problem
- Complex architectures (deep residual networks, attention) - they fall on the same CV-LB line
- More feature engineering without understanding why submissions are failing

## Validation Notes
- CV scheme: Leave-one-solvent-out for single (24 folds), leave-one-pair-out for full (13 folds)
- The CV-LB relationship is very strong (R² = 0.95), so CV is a reliable signal
- BUT the intercept (0.0528) is higher than the target (0.0347)
- To reach the target, we must CHANGE the CV-LB relationship, not just improve CV

## Key Insight from Analysis
**The target (0.0347) is mathematically unreachable with the current approach** because:
1. All model types fall on the same CV-LB line
2. The intercept (0.0528) is higher than the target
3. Even with CV = 0, the predicted LB would be 0.0528

**To reach the target, we need strategies that REDUCE THE INTERCEPT:**
1. Extrapolation detection + conservative predictions
2. Per-solvent error analysis to identify and handle "hard" solvents
3. Per-target model selection
4. Study what top competitors are doing differently

## Submission Strategy (5 remaining today)
1. **First:** Implement "mixall" kernel approach exactly to verify format works
2. **If it works:** Submit and get LB feedback
3. **Then:** Try intercept-reduction strategies based on learnings
