## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)
- **CRITICAL: Last 6+ submissions failed with "Evaluation metric raised an unexpected error"**

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept = 0.0525
- Target = 0.0347
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV to hit target: -0.0041 (NEGATIVE - IMPOSSIBLE with current approach)
- Even with CV = 0, predicted LB would be 0.0525

## Response to Evaluator
- Technical verdict was CONCERNS due to wrong CV scheme (GroupKFold vs Leave-One-Out)
- Evaluator's top priority: Fix CV scheme first, then implement per-target model selection
- **ISSUE:** Even with the official CV scheme (exp_055), the submission failed
- The submission format appears correct (1883 rows, 24 folds for task 0, 13 folds for task 1)
- All targets are in [0, 1] range, no NaN or Inf values
- The error may be on the evaluation side, not our submission

## CRITICAL: Submission Failure Analysis

The last 6+ submissions have failed with "Evaluation metric raised an unexpected error":
- exp_049, exp_050, exp_052, exp_053, exp_054, exp_055 all failed

**Possible causes:**
1. Evaluation system issue (not our fault)
2. Some subtle format difference we're missing
3. The evaluation expects something specific we don't know about

**What we've verified:**
- Submission format matches the official template exactly
- 1883 total rows (656 single + 1227 full)
- 24 folds for task 0, 13 folds for task 1
- All targets in [0, 1] range
- No NaN or Inf values
- Correct column names and data types

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  - Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV
  - Full data: 1227 samples, 13 solvent pairs, leave-one-pair-out CV
  - Targets: Product 2, Product 3, SM (starting material)
  - Features: Spange (13), ACS PCA (5), DRFP filtered (122), Arrhenius kinetics (5) = 145 total

## Recommended Approaches

### PRIORITY 1: Debug Submission Format - Create Minimal Working Submission
The submission format appears correct, but something is causing the evaluation to fail.

**IMMEDIATE ACTION:**
1. Create a MINIMAL submission using the EXACT code from the official template
2. Use the simplest possible model (e.g., just predict the mean)
3. Verify this submission is accepted by the evaluation
4. If it works, gradually add complexity until we find what breaks

**Implementation:**
```python
# Use the EXACT official template code
# Only change the model definition to something very simple
class SimpleModel:
    def train_model(self, X, Y):
        self.mean = Y.mean().values  # Just learn the mean
    
    def predict(self, X):
        return torch.tensor(np.tile(self.mean, (len(X), 1)))
```

### PRIORITY 2: Copy Known-Working Kernel Exactly
If the minimal submission fails, copy the EXACT code from a known-working kernel:
- "Arrhenius Kinetics + TTA" (LB 0.09831) - known to work
- Copy the entire notebook, don't modify anything
- Verify it produces a valid submission

### PRIORITY 3: Once Working, Improve Model
Once we have a working submission pipeline:
1. Try per-target model selection (HGB for SM, ETR for Products)
2. Try different ensemble weights
3. Try different feature combinations

### PRIORITY 4: Intercept Reduction Strategies
To reach the target, we must reduce the intercept:
1. Extrapolation detection + conservative predictions
2. Per-solvent error analysis
3. Uncertainty-weighted predictions

## What NOT to Try
- More complex models until we verify the submission format works
- Any approach that doesn't first verify the submission is accepted

## Validation Notes
- CV scheme: Leave-one-solvent-out for single (24 folds), leave-one-pair-out for full (13 folds)
- The CV-LB relationship is very strong (R² = 0.95)
- The intercept (0.0525) is higher than the target (0.0347)

## Key Insight from Analysis

**The submission format appears correct, but the evaluation is failing.**

This is blocking all progress. We need to:
1. Debug the submission format issue FIRST
2. Get a working submission to verify the pipeline
3. Then focus on improving the model

## Submission Strategy (5 remaining today)
1. **FIRST:** Create a minimal submission with the simplest possible model
2. **VERIFY:** The submission format is accepted by the evaluation
3. **THEN:** Improve the model while keeping the working format
4. **FINALLY:** Try intercept-reduction strategies

## THE TARGET IS REACHABLE

The target (0.0347) IS reachable. Our best LB (0.0877) is already better than public kernels.
The path forward:
1. Debug the submission format issue
2. Get a working submission
3. Find strategies that reduce the intercept
4. The solution exists - we just need to find it!

## IMPORTANT: Focus on Getting a Working Submission

Before trying any advanced techniques, we MUST get a working submission.
The current priority is:
1. Debug why submissions are failing
2. Create a minimal working submission
3. Then iterate on the model

Do NOT spend time on complex models until we have a working submission pipeline.