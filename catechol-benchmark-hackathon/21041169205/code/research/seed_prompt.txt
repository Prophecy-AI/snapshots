## Current Status
- Best CV score: 0.0083 from exp_030 and exp_067
- Best LB score: 0.0877 (exp_030 and exp_067)
- Target: 0.0347 | Gap to target: 0.0530 (2.5x worse)
- Submissions remaining: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (R² = 0.956)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES (R² = 0.956 across 13 submissions)
- **CRITICAL: Intercept (0.0520) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.0040 (IMPOSSIBLE)

**This means standard CV optimization CANNOT reach the target.**
**We MUST find approaches that CHANGE the CV-LB relationship, not just improve CV.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The implementation is sound.
- Evaluator's top priority: DO NOT submit exp_074 (prob_norm) - CV regression (0.0083 → 0.0136) suggests LB will be worse.
- **I AGREE**: Probability normalization hurt CV significantly because yields don't sum to 1 in the actual data.
- Key concerns raised: The intercept problem persists. All approaches fall on the same CV-LB line.
- **I AGREE**: We need approaches that fundamentally change the CV-LB relationship.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop77_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. The CV-LB relationship is extremely tight (R² = 0.956)
  2. The intercept (0.052) represents STRUCTURAL DISTRIBUTION SHIFT
  3. Test solvents are fundamentally different from training solvents
  4. Yields do NOT sum to 1 (probability normalization hurts)

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Extrapolation Detection with Conservative Fallback
**Hypothesis**: The intercept represents error on "outlier" test solvents that are far from training distribution.
**Implementation**:
```python
# Compute distance of test solvent to training solvents using Spange descriptors
def compute_outlier_score(test_solvent, train_solvents, spange_df):
    test_feats = spange_df.loc[test_solvent].values
    train_feats = spange_df.loc[train_solvents].values
    distances = np.linalg.norm(train_feats - test_feats, axis=1)
    return np.min(distances)  # Distance to nearest training solvent

# Blend predictions based on outlier score
def blend_predictions(pred, outlier_score, population_mean, threshold=0.5, max_blend=0.5):
    if outlier_score > threshold:
        blend_weight = min(max_blend, (outlier_score - threshold) / threshold)
        return pred * (1 - blend_weight) + population_mean * blend_weight
    return pred
```
**Why this might work**: Reduces variance on outlier solvents, potentially reducing the intercept.

### PRIORITY 2: Uncertainty-Weighted Predictions using GP Variance
**Hypothesis**: GP provides uncertainty estimates. High uncertainty → blend toward conservative prediction.
**Implementation**:
- Use GP variance as uncertainty estimate
- When variance is high, blend predictions toward population mean
- This should reduce variance on outlier solvents

### PRIORITY 3: Chemical Class-Specific Models
**Hypothesis**: Different chemical classes (alcohols, ethers, fluorinated) have different behavior.
**Implementation**:
- Group solvents by chemical class
- Train separate models for each class
- Use class-specific model when test solvent is in known class
- Use ensemble of all class models when test solvent is novel

### PRIORITY 4: Study the "mixall" Kernel Approach
**Observation**: The "mixall" kernel uses GroupKFold (5 splits) instead of Leave-One-Out CV.
**Hypothesis**: This different CV scheme might have different CV-LB characteristics.
**Implementation**: Try implementing the mixall approach with our best model (GP+MLP+LGBM).

### PRIORITY 5: Pseudo-Labeling
**Hypothesis**: Use confident test predictions to augment training, adapting to test distribution.
**Implementation**:
- Make predictions on test set
- Select high-confidence predictions (low variance)
- Add these as pseudo-labels to training
- Retrain model

## What NOT to Try
1. **Probability normalization** - exp_074 showed this hurts CV significantly (0.0083 → 0.0136)
2. **More complex models** - Deep residual networks failed (exp_004)
3. **Just improving CV** - The intercept problem means CV improvement alone won't reach target
4. **Standard ensemble weight tuning** - All approaches fall on the same CV-LB line

## Validation Notes
- CV scheme: Leave-One-Out (24 folds for single solvent, 13 folds for full data)
- CV-LB gap: ~10x (CV 0.0083 → LB 0.0877)
- The gap is STRUCTURAL, not random noise
- Any approach that just improves CV will follow the same LB = 4.36*CV + 0.052 line

## Critical Insight
The target (0.0347) is BELOW the intercept (0.052). This means:
1. Standard ML approaches CANNOT reach the target
2. We need approaches that fundamentally change the CV-LB relationship
3. The key is to reduce the INTERCEPT, not just improve CV

## Submission Strategy
- DO NOT submit exp_074 (prob_norm) - CV regression suggests LB will be worse
- Try extrapolation detection with conservative fallback FIRST
- If CV improves AND the approach is fundamentally different, submit to test if it changes the CV-LB relationship
- Save at least 1 submission for end-of-day best attempt

## Key Files
- Best model: `/home/code/experiments/030_gp_ensemble/gp_ensemble.ipynb`
- CV-LB analysis: `/home/code/exploration/evolver_loop77_analysis.ipynb`
- Public kernels: `/home/code/research/kernels/`
