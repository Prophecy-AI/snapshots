## Current Status
- Best CV score: 0.0083 from exp_030 and exp_067
- Best LB score: 0.0877 (exp_030, exp_067)
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (R² = 0.956)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES
- CRITICAL: Intercept (0.0520) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.0040 (IMPOSSIBLE)

## LEADERBOARD INSIGHT (CRITICAL)
- 1st place: 0.03470 (EXACTLY at target!)
- 2nd place: 0.07074
- HUGE gap (2x) between 1st and 2nd place
- This suggests 1st place found a FUNDAMENTALLY DIFFERENT approach
- Standard approaches (MLP, XGBoost, etc.) cluster around 0.07-0.09
- To reach target, we need a BREAKTHROUGH technique

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_078 (GroupKFold approach)
- Evaluator's top priority: VERIFY RULE COMPLIANCE before submitting exp_078
- Key concerns raised: 
  1. Overriding CV split functions may violate competition rules
  2. GroupKFold CV (0.0150) is 80% higher than Leave-One-Out CV (0.0081)
- My response: AGREE with evaluator. DO NOT submit exp_078 due to rule compliance risk.
- Instead, focus on rule-compliant approaches that only change the model definition.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop82_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. CV-LB intercept (0.052) exceeds target (0.0347) - standard optimization cannot reach target
  2. Top kernels use non-linear mixture features: A*(1-r) + B*r + 0.05*A*B*r*(1-r)
  3. Probability normalization (triple normalization) ensures row sums = 1
  4. Squeeze-and-Excitation blocks for feature recalibration

## Recommended Approaches (PRIORITY ORDER)

### 1. IMPLEMENT best-work-here KERNEL TECHNIQUES (HIGH PRIORITY)
The best-work-here kernel has several advanced techniques we haven't fully implemented:
- **Non-linear mixture features**: `A*(1-r) + B*r + 0.05*A*B*r*(1-r)` captures non-linear solvent interactions
- **Probability normalization**: Triple normalization (clip -> normalize -> clip -> normalize)
- **Squeeze-and-Excitation blocks**: Feature recalibration in neural network
- **Adaptive ensemble**: CatBoost + XGBoost + LightGBM + Neural Network with validation-based weights
- **Advanced feature engineering**: Polynomial features, interaction terms, statistical features

Implementation:
```python
# Non-linear mixture features
def mix_features(A, B, r):
    return A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)

# Triple probability normalization
probs = np.clip(probs, 1e-10, 1.0)
probs = probs / probs.sum(axis=1, keepdims=True)
probs = np.clip(probs, 1e-10, 1.0)
probs = probs / probs.sum(axis=1, keepdims=True)
```

### 2. DEEP KERNEL LEARNING (GP + Neural Network) (HIGH PRIORITY)
Research suggests Deep Kernel Learning achieves state-of-the-art for reaction prediction:
- GP whose kernel is parameterized by a neural network
- Provides uncertainty estimates for extrapolation detection
- Can guide conservative predictions for unseen solvents

### 3. ATTENTION-AUGMENTED ENSEMBLE (MEDIUM PRIORITY)
Research shows attention mechanisms improve performance for unseen solvents:
- Add attention layer to extract relevant descriptor information
- Weight features based on their relevance to the specific solvent

### 4. EXTRAPOLATION DETECTION + CONSERVATIVE PREDICTIONS (MEDIUM PRIORITY)
When model detects it's extrapolating to unseen solvents:
- Blend predictions toward population mean
- Use ensemble variance as uncertainty proxy
- High uncertainty → more conservative prediction

### 5. PHYSICS-INFORMED CONSTRAINTS (MEDIUM PRIORITY)
Add domain constraints that hold even for unseen solvents:
- Arrhenius kinetics (already implemented)
- Solvent polarity, dielectric constant relationships
- Hydrogen bonding capacity effects

## What NOT to Try
- **GroupKFold(5) approach (exp_078)**: Rule compliance risk - DO NOT SUBMIT
- **Standard CV optimization**: Intercept problem means this cannot reach target
- **Simple hyperparameter tuning**: Won't change the CV-LB relationship

## Validation Notes
- Use Leave-One-Out CV (official competition CV scheme)
- Monitor both CV and predicted LB (using CV-LB relationship)
- Focus on approaches that might CHANGE the CV-LB relationship, not just improve CV

## CRITICAL INSIGHT
The 1st place score (0.0347) is 2x better than 2nd place (0.0707). This is NOT just better hyperparameters - it's a fundamentally different approach. We need to find what that approach is.

Possible breakthrough approaches:
1. Graph Neural Networks (benchmark paper mentions GNN achieving 0.0039)
2. Transfer learning from larger chemistry datasets
3. Deep Kernel Learning with uncertainty quantification
4. Novel physics-informed constraints
5. Attention mechanisms for feature selection

## NEXT EXPERIMENT
Implement the best-work-here kernel techniques in a rule-compliant way:
1. Non-linear mixture features: A*(1-r) + B*r + 0.05*A*B*r*(1-r)
2. Probability normalization: Triple normalization
3. Squeeze-and-Excitation blocks in neural network
4. Adaptive ensemble: CatBoost + XGBoost + LightGBM + NN with validation-based weights
5. Advanced feature engineering: Polynomial features, interaction terms, statistical features

This should be a comprehensive implementation that combines all the best techniques from top kernels while remaining rule-compliant. The model class should be self-contained and only require changing the model definition line in the template.

## IMPORTANT NOTES
1. DO NOT submit exp_078 (GroupKFold) - rule compliance risk
2. Focus on techniques that might CHANGE the CV-LB relationship
3. The target IS achievable - 1st place got exactly 0.0347
4. We have 4 submissions remaining - use them wisely for calibration