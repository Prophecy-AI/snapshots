## Current Status
- Best CV score: 0.0081 from exp_049/exp_050/exp_053 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030/exp_067 (GP+MLP+LGBM)
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- Submissions remaining: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (RÂ² = 0.9558)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES - all 13 submissions follow this pattern
- **CRITICAL**: Intercept (0.0520) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.004 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_096 conservative blending experiment was sound but CV degraded 34%.
- Evaluator's top priority: Refine conservative blending with GP uncertainty. I AGREE this is the right direction but we need a different approach.
- Key concerns raised: blend_strength=0.3 too aggressive, k-NN distance not optimal. I AGREE - the implementation needs refinement.
- However, with only 4 submissions remaining, we should also try approaches from TOP PUBLIC KERNELS that have proven LB scores.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop100_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All model types (MLP, LGBM, XGB, CatBoost, GP) fall on the SAME CV-LB line
  2. The intercept (0.052) represents STRUCTURAL distribution shift to unseen solvents
  3. Conservative blending (exp_096) hurt CV without clear LB benefit
  4. Ridge regression (exp_095) performed much worse (CV=0.0158)

## KEY INSIGHTS FROM PUBLIC KERNELS

### 1. ens-model kernel (matthewmaree) - 9 votes
**CRITICAL INSIGHT**: Uses DIFFERENT ensemble weights for single vs full data:
- Single solvent: CatBoost weight=7.0, XGBoost weight=6.0 (normalized: 0.538, 0.462)
- Full data: CatBoost weight=1.0, XGBoost weight=2.0 (normalized: 0.333, 0.667)

This is DIFFERENT from our approach where we use the same weights for both tasks!

Features used:
- Combines ALL feature sources: spange + acs_pca + drfps + fragprints + smiles
- Uses correlation-based feature filtering (threshold=0.90)
- Adds numeric features: T_x_RT, RT_log, T_inv, RT_scaled

### 2. mixall kernel (lishellliang) - 9 votes
Uses 4-model ensemble: MLP + XGBoost + RF + LightGBM with weighted averaging.
Claims "good CV/LB ratio" - may have different CV-LB relationship.

## Recommended Approaches

### PRIORITY 1: Implement ens-model approach with task-specific weights
**Rationale**: The ens-model kernel uses different weights for single vs full data. This is a KEY difference from our approach. The different weights may help with the distribution shift problem.

```python
# Single solvent: CatBoost dominant
if data == "single":
    cat_weight = 7.0 / 13.0  # 0.538
    xgb_weight = 6.0 / 13.0  # 0.462
# Full data: XGBoost dominant  
else:
    cat_weight = 1.0 / 3.0   # 0.333
    xgb_weight = 2.0 / 3.0   # 0.667
```

Also implement their feature engineering:
- Correlation-based feature filtering
- Numeric features: T_x_RT, RT_log, T_inv, RT_scaled

### PRIORITY 2: Submit exp_049 (CatBoost+XGBoost, CV=0.0081)
**Rationale**: This is our best CV score. Even if predicted LB is ~0.087, it may have a different CV-LB relationship than GP+MLP+LGBM. Worth testing with 4 submissions remaining.

### PRIORITY 3: Try 5-model ensemble with task-specific weights
Combine our best approaches:
- GP (for uncertainty)
- MLP (for non-linear patterns)
- LightGBM (for tree-based patterns)
- CatBoost (for categorical handling)
- XGBoost (for gradient boosting)

With DIFFERENT weights for single vs full data.

### PRIORITY 4: Refined conservative blending
If time permits, try conservative blending with:
- Lower blend_strength (0.1 instead of 0.3)
- GP uncertainty instead of k-NN distance
- Only blend for high-uncertainty samples

## What NOT to Try
- Ridge regression (exp_095 showed 89% worse CV)
- Aggressive conservative blending (exp_096 showed 34% worse CV)
- GNN/ChemBERTa (performed worse than tabular models)
- Deep residual networks (exp_004 failed)

## Validation Notes
- Use official Leave-One-Out CV functions (unmodified)
- Template compliance: last 3 cells must match exactly
- The CV-LB gap is STRUCTURAL - improving CV alone won't reach target
- Focus on approaches that might CHANGE the CV-LB relationship

## CRITICAL REMINDER
The target (0.0347) IS reachable - the leaderboard shows scores below this.
The CV-LB intercept (0.0520) is the bottleneck.
We need approaches that REDUCE THE INTERCEPT, not just improve CV.

The ens-model kernel's task-specific weights may be the key insight we're missing.
