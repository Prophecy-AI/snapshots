## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)
- **CRITICAL: Last submission (exp_055) failed with "Evaluation metric raised an unexpected error"**

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept = 0.0525
- Target = 0.0347
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV to hit target: -0.0041 (NEGATIVE - IMPOSSIBLE with current approach)
- Even with CV = 0, predicted LB would be 0.0525

## Response to Evaluator
- Technical verdict was CONCERNS due to wrong CV scheme (GroupKFold vs Leave-One-Out)
- Evaluator's top priority: Fix CV scheme first, then implement per-target model selection
- **ISSUE:** Even with the official CV scheme (exp_055), the submission failed
- The submission format appears correct (1883 rows, 24 folds for task 0, 13 folds for task 1)
- All targets are in [0, 1] range, no NaN or Inf values
- The error may be on the evaluation side, not our submission

## CRITICAL: Submission Failure Analysis

The last 6+ submissions have failed with "Evaluation metric raised an unexpected error":
- exp_049, exp_050, exp_052, exp_053, exp_054, exp_055 all failed

**Possible causes:**
1. Evaluation system issue (not our fault)
2. Some subtle format difference we're missing
3. The evaluation expects something specific we don't know about

**What we've verified:**
- Submission format matches the official template exactly
- 1883 total rows (656 single + 1227 full)
- 24 folds for task 0, 13 folds for task 1
- All targets in [0, 1] range
- No NaN or Inf values
- Correct column names and data types

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  - Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV
  - Full data: 1227 samples, 13 solvent pairs, leave-one-pair-out CV
  - Targets: Product 2, Product 3, SM (starting material)
  - Features: Spange (13), ACS PCA (5), DRFP filtered (122), Arrhenius kinetics (5) = 145 total

## Recommended Approaches

### PRIORITY 1: Debug Submission Format
The submission format appears correct, but something is causing the evaluation to fail.

**Try these approaches:**
1. **Simplify the model** - Use a very simple model (e.g., Ridge regression) to verify the format works
2. **Match a known-working kernel exactly** - Copy the exact code from a public kernel that works
3. **Check for edge cases** - Are there any predictions that might cause issues (e.g., exactly 0 or 1)?

### PRIORITY 2: Implement a Known-Working Approach
The "Arrhenius Kinetics + TTA" kernel achieves LB 0.09831 and is known to work.
Implement this approach exactly to verify our submission pipeline is correct.

### PRIORITY 3: Per-Target Model Selection
Once we have a working submission, try per-target model selection:
- SM target: HistGradientBoostingRegressor (harder target)
- Product 2, Product 3: ExtraTreesRegressor (easier targets)

### PRIORITY 4: Extrapolation Detection + Conservative Predictions
When predicting for solvents far from the training distribution:
1. Compute distance to nearest training solvent
2. When distance is high, blend predictions toward population mean
3. This could reduce the intercept

## What NOT to Try
- More complex models until we verify the submission format works
- GroupKFold (5 folds) - this may cause submission failures

## Validation Notes
- CV scheme: Leave-one-solvent-out for single (24 folds), leave-one-pair-out for full (13 folds)
- The CV-LB relationship is very strong (R² = 0.95)
- The intercept (0.0525) is higher than the target (0.0347)
- To reach the target, we must CHANGE the CV-LB relationship (reduce the intercept)

## Key Insight from Analysis

**The submission format appears correct, but the evaluation is failing.**

This is blocking all progress. We need to:
1. Debug the submission format issue
2. Get a working submission to verify the pipeline
3. Then focus on improving the model

**Strategies to debug:**
1. Use a very simple model (Ridge regression) to verify format
2. Copy a known-working kernel exactly
3. Check for any edge cases in predictions

## Submission Strategy (5 remaining today)
1. **FIRST:** Create a minimal submission with a very simple model
2. **VERIFY:** The submission format is accepted by the evaluation
3. **THEN:** Improve the model while keeping the working format
4. **FINALLY:** Try intercept-reduction strategies

## THE TARGET IS REACHABLE

The target (0.0347) IS reachable. Our best LB (0.0877) is already better than public kernels.
The path forward:
1. Debug the submission format issue
2. Get a working submission
3. Find strategies that reduce the intercept
4. The solution exists - we just need to find it!

## IMPORTANT: Focus on Getting a Working Submission

Before trying any advanced techniques, we MUST get a working submission.
The current priority is:
1. Debug why submissions are failing
2. Create a minimal working submission
3. Then iterate on the model

Do NOT spend time on complex models until we have a working submission pipeline.
