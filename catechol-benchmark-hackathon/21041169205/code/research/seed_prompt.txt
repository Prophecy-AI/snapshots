## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (153%)
- Submissions remaining: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES - ALL 12 submissions
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)
- **CONCLUSION**: Linear CV improvements CANNOT reach target. We must CHANGE THE APPROACH.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The uncertainty-weighted experiment was well-executed.
- Evaluator's top priority: Test GroupKFold validation to see if CV-LB relationship changes. **AGREE - this is worth investigating.**
- Key concerns raised:
  1. Uncertainty-weighted predictions FAILED - blending toward mean hurts performance
  2. All 48 experiments fall on the same CV-LB line (R²=0.95)
  3. The intercept problem is STRUCTURAL, not solvable by better models
  4. The "mixall" kernel uses GroupKFold (5 splits) instead of Leave-One-Out (24 folds)
- **CRITICAL INSIGHT**: We have NOT tried CatBoost, which is used in the top "ens-model" kernel.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop49_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions fall on the same CV-LB line (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL error from distribution shift
  3. Test solvents are fundamentally different from training solvents
  4. Uncertainty-weighted predictions (exp_048) made things WORSE

## CRITICAL INSIGHT: What We Haven't Tried

### 1. CatBoost (HIGHEST PRIORITY)
The "ens-model" kernel uses CatBoost + XGBoost ensemble:
- CatBoost has DIFFERENT regularization than LightGBM
- CatBoost handles categorical features natively
- CatBoost may generalize better to unseen solvents
- We have tried LGBM, XGBoost, MLP, GP, RF, GNN, ChemBERTa - but NOT CatBoost!

### 2. Output Normalization (sum to 1 constraint)
The "ens-model" kernel normalizes outputs:
```python
out = np.clip(out, a_min=0.0, a_max=None)
totals = out.sum(axis=1, keepdims=True)
divisor = np.maximum(totals, 1.0)
out = out / divisor
```
This ensures predictions sum to at most 1 (chemical constraint).
We have NOT tried this constraint!

### 3. Combined Feature Table with Correlation Filtering
The "ens-model" kernel combines ALL feature sources:
- spange_descriptors (13 features)
- acs_pca_descriptors
- drfps_catechol (filtered)
- fragprints (filtered)
- smiles

Then applies correlation-based filtering (threshold=0.90) to remove redundant features.
We have tried individual feature sets, but NOT this combined approach with filtering.

### 4. Different Ensemble Weights for Single vs Full Data
The "ens-model" kernel uses:
- Single solvent: CatBoost(7) + XGBoost(6) = 54% CatBoost, 46% XGBoost
- Full data: CatBoost(1) + XGBoost(2) = 33% CatBoost, 67% XGBoost

We have used the same weights for both datasets.

## Recommended Approaches (Priority Order)

### Priority 1: CatBoost + XGBoost Ensemble (HIGHEST LEVERAGE)
**Why**: CatBoost is the ONLY major gradient boosting library we haven't tried.
- CatBoost has ordered boosting (reduces overfitting)
- CatBoost handles categorical features natively
- CatBoost has different regularization than LGBM/XGBoost
- The "ens-model" kernel uses CatBoost as the PRIMARY model

**Implementation Strategy**:
```python
from catboost import CatBoostRegressor

# CatBoost parameters (from ens-model kernel)
cat_params = {
    'iterations': 500,
    'learning_rate': 0.03,
    'depth': 6,
    'l2_leaf_reg': 3.0,
    'random_seed': 42,
    'verbose': False
}

# Train one CatBoost per target
cat_models = []
for i in range(3):
    model = CatBoostRegressor(**cat_params)
    model.fit(X_train, y_train[:, i])
    cat_models.append(model)
```

### Priority 2: Output Normalization
**Why**: Chemical constraint - yields should sum to at most 1.
- SM + Product 2 + Product 3 ≤ 1 (mass balance)
- Enforcing this constraint may improve generalization

**Implementation**:
```python
# After prediction
out = np.clip(out, 0, None)  # Non-negative
totals = out.sum(axis=1, keepdims=True)
out = out / np.maximum(totals, 1.0)  # Normalize if sum > 1
```

### Priority 3: Combined Feature Table with Correlation Filtering
**Why**: May capture complementary information from different feature sources.

**Implementation**:
```python
# Load all feature sources
spange = load_features('spange_descriptors')
acs = load_features('acs_pca_descriptors')
drfps = load_features('drfps_catechol')
fragprints = load_features('fragprints')

# Combine and filter
combined = pd.concat([spange, acs, drfps, fragprints], axis=1)
combined = filter_correlated_features(combined, threshold=0.90)
```

## What NOT to Try
1. **Uncertainty-weighted predictions** - exp_048 showed this HURTS performance
2. **Mean reversion** - exp_045 showed this HURTS performance
3. **GNN** - exp_040 failed with 8.4x worse MSE
4. **ChemBERTa embeddings** - exp_041 failed
5. **Deeper/more complex models** - exp_004 showed this doesn't help
6. **Simply improving CV** - The intercept problem cannot be solved by better models

## Validation Notes
- Use Leave-One-Solvent-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for mixtures (13 folds)
- CV-LB gap is ~4.3x, but intercept is the real problem
- **CRITICAL**: Monitor if CatBoost changes the CV-LB relationship

## Submission Strategy
- 5 submissions remaining
- Submit if:
  1. CatBoost ensemble achieves CV < 0.0080 (better than exp_030)
  2. Need to verify if CatBoost changes CV-LB relationship
  3. Output normalization shows significant improvement

## Key Insight for Executor
The problem is NOT model quality - it's DISTRIBUTION SHIFT. All 12 submissions fall on the same CV-LB line with intercept 0.0525 > target 0.0347.

**HOWEVER**: We have NOT tried CatBoost, which is the PRIMARY model in the top "ens-model" kernel. CatBoost has different regularization and may generalize better to unseen solvents.

**THE TARGET IS REACHABLE** - but we need to try CatBoost before concluding the intercept is immutable.

## Specific Implementation for Next Experiment

Create a CatBoost + XGBoost ensemble that:
1. Uses CatBoost as the primary model (from ens-model kernel)
2. Combines with XGBoost for diversity
3. Applies output normalization (sum to 1 constraint)
4. Uses combined feature table with correlation filtering

**Key Parameters**:
- CatBoost: iterations=500, learning_rate=0.03, depth=6, l2_leaf_reg=3.0
- XGBoost: n_estimators=500, learning_rate=0.03, max_depth=6
- Ensemble weights: CatBoost(7) + XGBoost(6) for single, CatBoost(1) + XGBoost(2) for full

This is the highest-leverage experiment we can run because CatBoost is the ONLY major model type we haven't tried.

## CRITICAL: Why the Target IS Reachable

The benchmark achieved MSE 0.0039. The top public kernels achieve LB < 0.07. The target (0.0347) IS reachable.

The "ens-model" kernel uses CatBoost + XGBoost, which we have NOT tried. CatBoost may have different generalization properties that could CHANGE the CV-LB relationship.

**NEXT STEPS**:
1. Implement CatBoost + XGBoost ensemble
2. Add output normalization (sum to 1 constraint)
3. Test with combined feature table
4. If CV improves significantly, submit to verify CV-LB relationship
