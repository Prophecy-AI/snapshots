## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (but submission failed)
- Best LB score: 0.0877 from exp_030 (last successful submission)
- Target: 0.0347 | Gap to target: 0.0530 (60% reduction needed)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES - All 12 successful submissions fall on this line
- **CRITICAL: The intercept (0.0525) is ABOVE the target (0.0347)**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

## CRITICAL ISSUE: 10 Consecutive Submission Failures
All submissions from exp_049 to exp_065 have failed with "Evaluation metric raised an unexpected error".
- The submission format is correct (1883 rows, correct columns, no NaN/Inf)
- The notebook structure looks correct (same as exp_030 which worked)
- The issue is likely with the Kaggle evaluation system or notebook execution

## Response to Evaluator
- Technical verdict was TRUSTWORTHY - The notebook appears properly structured
- Evaluator's top priority: Submit exp_065 to verify the submission pipeline works
- **SUBMISSION FAILED** - The error "Evaluation metric raised an unexpected error" suggests the evaluation code is failing
- Key concerns: 10 consecutive submission failures indicate a systematic issue

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  1. Leave-one-solvent-out CV simulates predicting for unseen solvents
  2. Test solvents may be "harder" (more extreme properties)
  3. The CV-LB gap (~4.3x multiplier + 0.0525 intercept) is consistent across all models

## IMMEDIATE PRIORITY: Fix Submission Issue

The submission failures need to be fixed before we can make progress. Options:

### Option 1: Use a Simpler Model (RECOMMENDED)
Create a notebook that exactly matches the official template structure:
- Use the official `utils.py` functions from the competition
- Use a simple MLPModel that matches the template exactly
- No custom classes or complex ensembles
- This is the safest approach to verify the submission pipeline works

### Option 2: Debug the Notebook Structure
- Compare exp_030 (worked) with exp_065 (failed) in detail
- Check if there's something specific about the notebook that's causing problems
- The issue might be with the notebook execution on Kaggle, not the submission format

## Recommended Approaches (After Fixing Submission)

### Priority 1: Distribution-Shift-Aware Strategies
Since the intercept (0.0525) > target (0.0347), standard CV optimization cannot reach the target.
We need strategies that REDUCE THE INTERCEPT, not just improve CV:

1. **Extrapolation Detection Features**
   - Add features measuring solvent distance to training distribution
   - Use molecular fingerprint similarity (Tanimoto) to nearest training solvents
   - When extrapolating, blend predictions toward population mean

2. **Uncertainty-Weighted Predictions**
   - Use GP uncertainty estimates
   - High uncertainty → conservative prediction (closer to mean)
   - Blend complex model with simple baseline based on extrapolation degree

3. **Solvent Clustering**
   - Group solvents by chemical class (alcohols, ethers, esters, etc.)
   - Use class-specific models that generalize within chemical families
   - Detect when test solvent is in a known vs novel class

### Priority 2: Study Top Public Kernels
The top kernels achieve LB ~0.098 with simpler approaches:
- `sanidhyavijay24/arrhenius-kinetics-tta-0-09831`: MLP with Arrhenius kinetics + TTA
- `omarafik/system-malfunction-v1`: Simple MLP with Spange features

These kernels use simpler structures that work on Kaggle. Consider adapting their approach.

## What NOT to Try
- More complex ensembles (GP+MLP+LGBM) - These have been causing submission failures
- Hyperparameter tuning - The CV-LB relationship shows this won't help
- Standard CV optimization - The intercept problem persists

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvent, leave-one-ramp-out for full data
- CV-LB gap: ~4.3x multiplier + 0.0525 intercept
- The intercept represents EXTRAPOLATION ERROR that no model tuning can fix

## NEXT STEPS
1. **Create a minimal submission** that exactly matches the official template
2. **Submit to verify the pipeline works**
3. **If successful, implement distribution-shift-aware strategies**
4. **Focus on reducing the intercept, not just improving CV**

## Key Insight
The target IS reachable - the benchmark achieved MSE 0.0039 on this exact dataset.
But standard ML approaches cannot reach it. The solution requires:
1. Fixing the submission issue
2. Implementing fundamentally different strategies that address distribution shift
3. Learning from what top competitors do - they've solved this problem
