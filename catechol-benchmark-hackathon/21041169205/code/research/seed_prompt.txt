## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (153%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES
- **CRITICAL**: Intercept (0.0525) is 151% of target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)
- **CONCLUSION**: Linear CV improvements CANNOT reach target. We must REDUCE THE INTERCEPT.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Mean reversion experiment was well-executed.
- Evaluator's top priority: Non-linear mixture features + sophisticated ensemble. AGREE - this is a promising direction.
- Key concerns raised: The intercept problem cannot be fixed by model tuning. AGREE - we need fundamentally different approaches.
- Mean reversion FAILED because the problem is NOT prediction bias, it's DISTRIBUTION SHIFT.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop46_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions fall on the same CV-LB line (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL error from distribution shift
  3. Test solvents are fundamentally different from training solvents
  4. Mean reversion hurts CV (alpha=1.0 is best), confirming predictions are NOT biased

## CRITICAL INSIGHT: Why We're Stuck

The intercept (0.0525) is LARGER than the target (0.0347). This means:
- Even with perfect CV (CV=0), we'd still be 51% above target
- No amount of model tuning can fix this
- We need to CHANGE THE CV-LB RELATIONSHIP, not just improve CV

## Recommended Approaches (Priority Order)

### Priority 1: Implement Public Kernel Approach (lishellliang/mixall)
**Why**: This kernel uses GroupKFold (5 splits) instead of Leave-One-Out, which may have a DIFFERENT CV-LB relationship.

Key techniques from the kernel:
1. **GroupKFold validation** instead of Leave-One-Out
2. **Ensemble of 4 models**: MLP + XGBoost + RandomForest + LightGBM
3. **Optuna hyperparameter optimization**
4. **Weighted ensemble**: Dirichlet-like weights via normalization

Implementation:
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    groups = X["SOLVENT NAME"]
    n_splits = min(5, len(groups.unique()))
    gkf = GroupKFold(n_splits=n_splits)
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx])
```

### Priority 2: Sophisticated Ensemble with Diverse Models
**Why**: Diverse models may capture different aspects of the data, reducing structural error.

Implementation:
```python
# Ensemble weights (from lishellliang kernel)
weights = [0.35, 0.25, 0.15, 0.25]  # MLP, XGBoost, RF, LightGBM

# XGBoost params
xgb_params = {
    'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.05,
    'subsample': 0.8, 'colsample_bytree': 0.8
}

# LightGBM params
lgb_params = {
    'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.05,
    'num_leaves': 31, 'subsample': 0.8
}

# RandomForest params
rf_params = {
    'n_estimators': 200, 'max_depth': 10, 'min_samples_leaf': 5
}
```

### Priority 3: Physics-Informed Feature Engineering
**Why**: Physics constraints generalize to unseen solvents.

Features to add:
```python
# Arrhenius kinetics
temp_k = temp_c + 273.15
arrhenius = np.exp(-Ea / (R * temp_k))  # Activation energy term

# Solvent polarity interactions
polarity_interaction = polarity_A * polarity_B * pct_A * pct_B

# Hydrogen bonding capacity
h_bond_capacity = h_donor + h_acceptor
```

### Priority 4: Extrapolation Detection
**Why**: Identify when test samples are OOD and blend toward conservative predictions.

Implementation:
```python
# Calculate distance to training distribution
def extrapolation_score(test_features, train_features):
    # Use Mahalanobis distance or nearest neighbor distance
    distances = cdist(test_features, train_features, metric='mahalanobis')
    min_distances = distances.min(axis=1)
    return min_distances

# Blend predictions based on extrapolation score
def blend_predictions(model_pred, train_mean, extrapolation_score, threshold=2.0):
    blend_weight = np.clip(extrapolation_score / threshold, 0, 1)
    return (1 - blend_weight) * model_pred + blend_weight * train_mean
```

## What NOT to Try
1. **Mean reversion** - Already tested, HURTS CV (exp_045)
2. **Simple MLP improvements** - All approaches fall on same CV-LB line
3. **GNN** - Failed with MSE 0.068767 (exp_040), 8.4x worse than baseline
4. **ChemBERTa embeddings** - Failed (exp_041)
5. **Learned embeddings** - Failed because test solvents are never seen (exp_039)

## Validation Notes
- Use Leave-One-Solvent-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for mixtures (13 folds)
- **ALTERNATIVE**: Try GroupKFold (5 splits) as in lishellliang kernel
- CV-LB gap is ~4.3x, but intercept is the real problem

## Submission Strategy
- 5 submissions remaining
- Only submit if:
  1. CV improves significantly (>5% improvement)
  2. Using a fundamentally different approach (GroupKFold, new ensemble)
  3. Need to verify if new approach changes CV-LB relationship

## Key Insight for Executor
The problem is NOT model quality - it's DISTRIBUTION SHIFT. All 12 submissions fall on the same CV-LB line. To reach target, we need to:
1. Try fundamentally different validation (GroupKFold)
2. Use diverse ensemble that may have different CV-LB relationship
3. Add physics-informed features that generalize to unseen solvents
4. Detect extrapolation and blend toward conservative predictions

**DO NOT** keep optimizing standard ML approaches. The intercept won't change.