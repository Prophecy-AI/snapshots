## Current Status
- Best CV score: 0.008092 from exp_049/exp_050 (CatBoost + XGBoost ensemble) - ALREADY SUBMITTED, LB pending
- Best LB score: 0.0877 (exp_030, exp_067)
- Target: 0.0347 | Gap to target: 0.053 (152.8% above target)
- Submissions remaining: 4
- Many pending LB scores (exp_049, exp_050, exp_053, etc.)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.34 * CV + 0.0523 (R² = 0.9573)
- Intercept interpretation: Even at CV=0, expected LB is 0.0523
- Are all approaches on the same line? YES - all 86+ experiments fall on the same line
- **CRITICAL: Intercept (0.0523) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0523) / 4.34 = -0.004 (IMPOSSIBLE - negative!)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The pseudo-labeling implementation was correct.
- Evaluator's top priority: Study and replicate the ens-model kernel. AGREE - we need to understand what makes top kernels work.
- Key concerns raised: Pseudo-labeling doesn't address structural shift, intercept > target. AGREE - we need fundamentally different approaches.
- Evaluator correctly identified that exp_083 should NOT be submitted (CV 59% worse than best).

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop87_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL approaches (MLP, LightGBM, CatBoost, XGBoost, GP) fall on the same CV-LB line
  2. The intercept (0.0523) represents structural distribution shift to unseen solvents
  3. Top leaderboard score is 0.0347 (exactly at target), 2nd place is 0.0707 (2x worse)
  4. This HUGE gap suggests 1st place found a fundamentally different approach

## Key Insights from Top Kernels

### ens-model kernel (Official LOO-CV):
- Uses official Leave-One-Out CV (same as us)
- Key techniques:
  1. **Combined features**: spange + acs_pca + drfps + fragprints
  2. **Correlation-based feature filtering**: threshold=0.90, with priority (spange > acs > drfps > frag)
  3. **Numeric feature engineering**: T_inv, RT_log, T_x_RT, RT_scaled
  4. **CatBoost + XGBoost ensemble**: Different weights per dataset (single: 7:6, full: 1:2)
  5. **Probability normalization**: clip to non-negative, divide by max(sum, 1.0)
- We tried to replicate this in exp_080 but got CV=0.010266 (worse than our best)

## Recommended Approaches (Priority Order)

### 1. EXACT REPLICATION of ens-model kernel
- Our exp_080 attempt got worse CV (0.010266 vs 0.008092)
- We may have missed key details:
  - Exact feature combination and filtering (threshold=0.90)
  - Exact hyperparameters for CatBoost/XGBoost
  - Exact probability normalization (clip to non-negative, divide by max(sum, 1.0))
  - Numeric feature engineering (T_inv, RT_log, T_x_RT, RT_scaled)
- **IMPLEMENT EXACTLY AS IN THE KERNEL**

### 2. Try Graph Neural Networks (GNN) with proper implementation
- The GNN benchmark achieved 0.0039 CV
- GNNs can learn molecular structure directly
- May generalize better to unseen solvents
- exp_040 failed due to implementation issues
- Use PyTorch Geometric or DGL for proper implementation

### 3. Try Transfer Learning from pre-trained molecular models
- Use pre-trained molecular embeddings (e.g., ChemBERTa, MolBERT)
- Fine-tune on catechol data
- The pre-trained representations may generalize better

### 4. Physics-Informed Neural Networks
- Add physics constraints that hold for ALL solvents
- Arrhenius kinetics (already used, helps CV but not intercept)
- Solvent polarity constraints
- Reaction mechanism constraints

## What NOT to Try
- **Pseudo-labeling**: Doesn't work for LOO-CV (exp_083 showed this)
- **Similarity weighting**: Didn't help (exp_082)
- **Solvent clustering**: Made things worse (exp_081)
- **More MLP/LightGBM variations**: All fall on the same CV-LB line
- **Extrapolation detection with conservative blending**: Didn't change the intercept

## Validation Notes
- CV scheme: Official Leave-One-Out CV (24 folds for single, 13 folds for full)
- CV-LB relationship: LB = 4.34 * CV + 0.0523 (R² = 0.9573)
- The intercept (0.0523) > target (0.0347) means standard CV optimization cannot reach target
- We need approaches that REDUCE THE INTERCEPT, not just improve CV

## Immediate Next Steps
1. Implement ens-model kernel EXACTLY (study the code more carefully)
2. If ens-model doesn't help, try GNN with proper implementation
3. Try transfer learning from pre-trained molecular models
4. Save submissions for promising new approaches that might change the CV-LB relationship