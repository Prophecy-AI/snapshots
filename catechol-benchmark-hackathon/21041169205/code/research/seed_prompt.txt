## Current Status
- Best CV score: 0.008092 from exp_049/050/053 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 153% (0.0530 absolute)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 * CV + 0.0528 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- Are all approaches on the same line? YES
- **CRITICAL: Intercept (0.0528) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The submission format is correct.
- Evaluator's top priority: **DO NOT SUBMIT UNTIL THE ERROR PATTERN IS UNDERSTOOD**
- 7 consecutive submissions (exp_049-057) failed with "Evaluation metric raised an unexpected error"
- Key concerns: The submission format appears correct (1883 rows, 24+13 folds, values in [0,1])
- The evaluator correctly identifies that we're burning submission slots without feedback

## CRITICAL ISSUE: 7 Consecutive Submission Failures

**All recent submissions have failed with the same error.** This is the #1 priority to fix.

Possible causes:
1. **Notebook structure violation**: The competition requires EXACT last 3 cells from template
2. **Model definition line changes**: Only the `model = ...` line can be changed
3. **Additional cells or modifications**: Any extra cells or changes invalidate submission
4. **Data type issues**: Float32 vs float64 differences

**URGENT ACTION**: Before ANY new submission:
1. Compare exp_030 notebook (last successful) with recent failed notebooks
2. Ensure EXACT template structure in last 3 cells
3. Only change the `model = ...` line, nothing else
4. Test locally that submission.csv format matches exp_030's format

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop61_analysis.ipynb` for CV-LB analysis
- Key patterns:
  - CV-LB relationship is highly linear (R²=0.95)
  - Intercept (0.0528) exceeds target (0.0347)
  - All model types (MLP, LGBM, XGB, GP, CatBoost) fall on the same line
  - This indicates STRUCTURAL distribution shift, not model quality issue

## Recommended Approaches

### Priority 1: FIX SUBMISSION ERRORS (MUST DO FIRST)
1. **Examine exp_030 notebook structure** - This is the last successful submission
2. **Create new experiment using EXACT exp_030 structure** - Only change model definition
3. **Verify submission.csv format matches** - Same columns, same dtypes, same row counts

### Priority 2: Use Known-Working Model with Best CV
Since exp_030 (GP+MLP+LGBM) achieved best LB (0.0877) but exp_049/050 (CatBoost+XGBoost) achieved best CV (0.008092), try:
- Use exp_030's notebook structure (known working)
- Replace model with CatBoost+XGBoost ensemble
- This combines best CV model with known-working submission format

### Priority 3: Study Public Kernels for Intercept Reduction
From public kernel analysis:
- **mixall kernel**: Uses GroupKFold(5) instead of Leave-One-Out - different CV scheme
- **ens-model kernel**: CatBoost+XGBoost with ALL features combined (Spange+ACS+DRFP+Fragprints)
- Both use correlation-based feature filtering (threshold=0.90)

Key insight from ens-model:
```python
# Combines ALL feature sources
sources = ["spange_descriptors", "acs_pca_descriptors", "drfps_catechol", "fragprints"]
# Then filters by correlation (threshold=0.90)
```

### Priority 4: Approaches to Change CV-LB Relationship
Since intercept > target, we MUST try approaches that change the relationship:

1. **Feature combination from ens-model**: Combine all feature sources with correlation filtering
2. **Uncertainty-weighted predictions**: Use GP variance to blend toward mean for uncertain predictions
3. **Solvent similarity-based blending**: When test solvent is very different from training, use conservative predictions
4. **Physics-informed constraints**: Arrhenius kinetics, mass balance constraints

## What NOT to Try
- **More CV optimization without fixing submissions**: We can't get LB feedback
- **Extrapolation detection with current approach**: exp_058/059 showed it hurts CV without guaranteed intercept improvement
- **Complex architectures**: Deep residual networks failed (exp_004)
- **DRFP-only features**: Performed worse than Spange (exp_002)

## Validation Notes
- CV scheme: Leave-One-Out for single solvent (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- MUST use official template structure for last 3 cells
- Only the `model = ...` line can be changed
- Submission format: 1883 rows total (656 single + 1227 full)

## Specific Next Steps

### Step 1: Debug Submission Errors
```python
# Compare exp_030 notebook with recent failed ones
# Check:
# 1. Number of cells
# 2. Exact content of last 3 cells
# 3. Any modifications outside model definition line
```

### Step 2: Create Clean Submission
Use exp_030's exact notebook structure, only change model to:
```python
model = CatBoostXGBoostEnsemble(data='single')  # For single solvent
model = CatBoostXGBoostEnsemble(data='full')    # For full data
```

### Step 3: If Submission Works, Try Feature Combination
Implement ens-model's approach:
- Combine Spange + ACS PCA + DRFP + Fragprints
- Apply correlation filtering (threshold=0.90)
- Use CatBoost + XGBoost ensemble

## REMEMBER
- **5 submissions remaining** - Each one is precious
- **Fix submission errors FIRST** - No point optimizing if we can't submit
- **Target is 0.0347** - Current best LB is 0.0877 (153% gap)
- **Intercept problem is structural** - Need to change the CV-LB relationship, not just improve CV
