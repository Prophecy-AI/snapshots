## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost) - BUT SUBMISSION FAILED
- Best verified LB score: 0.0877 (exp_030, exp_067) - GP+MLP+LGBM ensemble
- Target: 0.0347 | Gap to target: 0.0530 (153% above target)
- Submissions remaining: 4
- Loop: 80 | Experiments: 80

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (R² = 0.9558)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES (R² = 0.96 across 13 verified submissions)
- **CRITICAL: Intercept (0.0520) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.0040 (IMPOSSIBLE)

**This means standard CV optimization CANNOT reach the target.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The implementation is sound.
- Evaluator's top priority: PIVOT to fundamentally different approaches. **I AGREE.**
- Key concerns raised: The intercept problem persists. All approaches fall on the same CV-LB line.
- **CRITICAL DISCOVERY**: The current submission.csv contains exp_074 (prob_norm) predictions where targets sum to 1.0 - this is WRONG for this competition!

## URGENT: Current Submission is INVALID

The current `/home/submission/submission.csv` contains predictions from exp_074 (probability normalization) where:
- All targets sum to exactly 1.0
- This is WRONG because yields in this competition do NOT sum to 1
- exp_074 had CV=0.0136 (64% WORSE than best)
- DO NOT SUBMIT the current submission.csv!

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: REGENERATE SUBMISSION FROM BEST VERIFIED MODEL
**Rationale**: The current submission.csv is invalid (prob_norm). We need to regenerate from exp_030 or exp_067.
**Implementation**:
1. Run exp_067 (sigmoid_output) which is faster than exp_030
2. Verify the submission.csv has correct format
3. Verify targets do NOT sum to 1

### PRIORITY 2: TRY CATBOOST + XGBOOST WITH CORRECT TEMPLATE STRUCTURE
**Rationale**: CatBoost + XGBoost achieved CV=0.0081 (best ever) but submissions failed due to format errors.
**Key insight**: The ens-model kernel uses:
- Different weights for single vs full data (7:6 vs 1:2)
- Correlation-based feature filtering (threshold=0.8)
- Feature priority: spange > acs > drfps > frag > smiles

**Implementation**:
1. Copy the EXACT template structure from the official template
2. Define CatBoost + XGBoost model BEFORE the template cells
3. Use ONLY the model definition line in the template cells
4. Ensure the last 3 cells are EXACTLY as in the template

### PRIORITY 3: TRY CONSERVATIVE PREDICTIONS (UNIFORM BLEND)
**Rationale**: The intercept represents extrapolation error. Blending toward mean might help.
**Implementation**:
1. Take the best model predictions
2. Blend ALL predictions toward population mean with fixed weight (e.g., 0.2-0.3)
3. This is different from adaptive extrapolation detection (which failed)

## What NOT to Try
1. **Probability normalization** - exp_074 showed this hurts CV significantly (0.0083 → 0.0136)
2. **Extrapolation detection with adaptive blending** - exp_048, 058, 059, 068-071 all failed
3. **Just improving CV** - The intercept problem means CV improvement alone won't reach target

## Validation Notes
- Current CV scheme: Leave-One-Out (24 folds for single solvent, 13 folds for full data)
- CV-LB gap: ~10x (CV 0.0083 → LB 0.0877)
- The gap is STRUCTURAL, not random noise

## Key Files
- Best verified model: `/home/code/experiments/067_sigmoid_output/sigmoid_output.ipynb` (faster)
- Best CV model (unverified): `/home/code/experiments/049_catboost_xgboost/`
- ens-model kernel: `/home/code/research/kernels/matthewmaree_ens-model/`
- Official template: `/home/code/research/kernels/josepablofolch_catechol-benchmark-hackathon-template/`

## NEVER GIVE UP
The target IS reachable. The GNN benchmark achieved 0.0039 MSE. We just need to find the right approach.