## Current Status
- Best CV score: 0.0081 from exp_050/051/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- **PENDING SUBMISSIONS**: exp_049, exp_050, exp_052, exp_053 all show "pending" - likely failed
- Current submission.csv: exp_054 (Simple MLP with exact template, CV=0.008504)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept = 0.0525 (151.4% of target!)
- Required CV to hit target: -0.0041 (NEGATIVE - IMPOSSIBLE)
- **ALL 12 successful submissions fall on the same line**
- **The target is MATHEMATICALLY UNREACHABLE by improving CV alone**

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - The evaluator confirmed that exp_054 is technically sound.

**Evaluator's top priority**: Submit the current submission to verify format is correct.

**Key concerns raised**:
1. The CV-LB intercept (0.0525) is higher than the target (0.0347)
2. All approaches fall on the same CV-LB line
3. The last 4 submissions may have failed with evaluation errors

**How I'm addressing these**:
1. exp_054 uses the EXACT template code structure to verify format
2. If format is correct, we need to pivot to intercept-reduction strategies
3. The evaluator correctly identifies that we need to CHANGE the CV-LB relationship

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop55_analysis.ipynb` for CV-LB analysis
- Key pattern: ALL approaches fall on the SAME CV-LB line (R²=0.95)
- The intercept (0.0525) represents STRUCTURAL distribution shift
- Test solvents are fundamentally different from training solvents

## Recommended Approaches (Priority Order)

### PRIORITY 1: Submit exp_054 to Verify Format
**IMMEDIATE ACTION**: Submit the current submission (exp_054) to verify format is correct.
- exp_054 uses the EXACT template code structure
- If this works, we know the format is correct
- If this fails, we need to investigate further

### PRIORITY 2: Implement "mixall" Kernel Approach
**Experiment 055: MLP + XGBoost + RandomForest + LightGBM Ensemble**
The "mixall" kernel (lishellliang) claims good CV/LB with a 4-model ensemble:
- MLP (40% weight)
- XGBoost (20% weight)
- RandomForest (20% weight)
- LightGBM (20% weight)

Key features:
- Uses GroupKFold (5 splits) instead of Leave-One-Out for faster training
- Weighted ensemble with tunable weights
- Standard scaling of features
- Spange descriptors for solvent features

### PRIORITY 3: Per-Solvent Error Analysis
**Experiment 056: Identify High-Error Solvents**
- Compute per-solvent CV error for all 24 solvents
- Identify which solvents cause the most error
- Analyze: Are high-error solvents outliers in feature space?
- Consider: Different models for different solvent classes

### PRIORITY 4: Uncertainty-Weighted Blending
**Experiment 057: Conservative Predictions for Outliers**
- Use ensemble variance to estimate prediction uncertainty
- For high-uncertainty predictions, blend toward population mean
- This could reduce the intercept by being more conservative on extrapolation

### PRIORITY 5: Solvent Clustering
**Experiment 058: Class-Specific Models**
- Group solvents by chemical class (alcohols, ethers, esters, etc.)
- Train separate models for each class
- Use class-specific predictions that generalize within chemical families

## What NOT to Try
1. **More CV optimization** - All models fall on the same CV-LB line
2. **More feature engineering** - We've tried many feature sets, none changed the relationship
3. **IWCV** - Already tested (exp_051), didn't help

## Validation Notes
- CV scheme: Leave-One-Solvent-Out for single solvents (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- CRITICAL: Always clip targets to [0, 1] before saving submission
- The CV-LB gap is ~4.3x (slope) plus 0.0525 (intercept)

## Submission Strategy (5 remaining)
1. **Submission 1**: exp_054 (exact template) - verify format works
2. **Submission 2**: Best CV model with verified format
3. **Submissions 3-5**: Test fundamentally different approaches (intercept reduction)

## Key Insight for Executor
The CV-LB relationship shows:
- LB = 4.31 * CV + 0.0525
- Intercept (0.0525) > Target (0.0347)
- **The target is UNREACHABLE by improving CV alone**

To reach the target, we must:
1. **First**: Verify submission format works (exp_054)
2. **Then**: Try approaches that REDUCE THE INTERCEPT

Approaches that could reduce the intercept:
1. Uncertainty-weighted predictions (conservative on extrapolation)
2. Solvent clustering (class-specific models)
3. Per-solvent error analysis (identify and handle outliers)
4. Ensemble diversity (different model types may have different intercepts)

## NEVER GIVE UP
The target IS reachable. The benchmark paper achieved MSE 0.0039 with a GNN. Top competitors have solved this problem. We just need to find the right approach.

## IMPORTANT: The "mixall" Kernel
The "mixall" kernel (lishellliang) uses a 4-model ensemble:
- MLP + XGBoost + RandomForest + LightGBM
- Weighted averaging (40/20/20/20)
- Claims good CV/LB ratio

This is worth trying because:
1. Different model types may have different CV-LB relationships
2. The ensemble diversity could reduce the intercept
3. It's a proven approach from a public kernel

## Code Reference
See `/home/code/research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/` for the full implementation.
