## Current Status
- Best CV score: 0.008092 from exp_050 (CatBoost + XGBoost Ensemble)
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM Ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (153% above target)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept = 0.0525
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Even with CV=0 (perfect training), predicted LB = 0.0525
- Required CV to hit target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE - IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The experiment was well-executed.
- Evaluator's top priority: Submit exp_050 to verify bug fix and validate CV-LB relationship.
- **SUBMISSION FAILED** with "Evaluation metric raised an unexpected error"
- This is a server-side error, not a format issue. The submission format matches the template exactly.
- Key concern: The intercept problem means the target is unreachable with current approaches.

## CRITICAL ISSUE: Submission Error

exp_049 and exp_050 both failed with "Evaluation metric raised an unexpected error". This is a server-side error during evaluation. Possible causes:
1. The evaluation code has a bug with certain prediction patterns
2. Something specific about CatBoost/XGBoost predictions triggers an edge case
3. The submission was corrupted during upload

**RECOMMENDED ACTION:** 
1. Fall back to a known-working approach (exp_030's GP + MLP + LGBM ensemble scored 0.0877)
2. Try a simpler model that we know works
3. Investigate if there's something specific about the CatBoost/XGBoost predictions

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop51_analysis.ipynb` for submission error investigation
- Key patterns:
  - Single solvent: 656 rows, 24 solvents (24 folds)
  - Full data: 1227 rows, 13 solvent pairs (13 folds)
  - Targets don't sum to 1 (mean sum ~0.80)
  - HFIP and TFE are the hardest solvents to predict

## The Intercept Problem

**ALL approaches fall on the same CV-LB line with intercept 0.0525 > target 0.0347.**

This means:
1. No amount of CV improvement can reach the target
2. The intercept represents STRUCTURAL distribution shift
3. We need approaches that CHANGE the CV-LB relationship, not just improve CV

**Strategies to reduce the intercept (not just improve CV):**

1. **Extrapolation Detection:**
   - Add features measuring distance to training distribution
   - When extrapolating, blend predictions toward population mean
   - Use molecular fingerprint similarity (Tanimoto) to nearest training solvents

2. **Uncertainty-Weighted Predictions:**
   - Use GP with uncertainty estimates
   - High uncertainty → conservative prediction (closer to mean)
   - Ensemble variance as uncertainty proxy

3. **Domain Adaptation:**
   - Importance-Weighted CV (IWCV) to reweight training examples
   - Pseudo-labeling with confident test predictions
   - Domain-specific constraints that hold for unseen solvents

4. **Solvent-Specific Strategies:**
   - Identify which solvents cause the most error
   - Use simpler models for "hard" solvents (HFIP, TFE, Water)
   - Blend complex model with simple baseline based on extrapolation degree

## Recommended Approaches

**IMMEDIATE PRIORITY: Fix the submission error**

1. **Fall back to exp_030 approach** - GP + MLP + LGBM ensemble that scored 0.0877 on LB
   - This approach is known to work
   - Use it as a baseline while investigating the CatBoost/XGBoost issue

2. **Investigate CatBoost/XGBoost predictions** - Check if there's something specific about the predictions that causes the evaluation to fail
   - Compare prediction distributions with successful submissions
   - Check for edge cases (very small values, exact zeros, etc.)

3. **Try a simpler ensemble** - MLP + LGBM without CatBoost
   - Remove the component that might be causing the issue
   - Verify the submission works before adding complexity

**AFTER FIXING SUBMISSION:**

4. **Extrapolation detection features** - Add features measuring solvent distance to training distribution
   - Tanimoto similarity to nearest training solvents
   - Blend toward mean when extrapolating

5. **Per-solvent error analysis** - Identify which solvents cause the most error
   - HFIP contributes 53.2% of total mixture MSE
   - Develop solvent-specific strategies

## What NOT to Try
- More CatBoost/XGBoost experiments until the submission error is resolved
- Approaches that just improve CV without changing the intercept
- Complex architectures (GNN, ChemBERTa) - they fall on the same CV-LB line

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvents (24 folds), Leave-one-solvent-pair-out for full data (13 folds)
- CV-LB gap: ~10x (CV 0.008 → LB 0.088)
- The intercept (0.0525) is the main bottleneck, not CV performance

## Submission Strategy
- 5 submissions remaining
- **DO NOT submit CatBoost/XGBoost until the error is understood**
- Use remaining submissions to test fundamentally different approaches
- Focus on approaches that could change the intercept