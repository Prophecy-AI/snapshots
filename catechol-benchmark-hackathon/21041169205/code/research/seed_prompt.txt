## Current Status
- Best CV score: 0.008092 from exp_049/exp_050 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030/exp_067 (GP+MLP+LGBM)
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)
- Submissions remaining: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (RÂ² = 0.9558)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES
- **CRITICAL**: Intercept (0.0520) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.004 (IMPOSSIBLE)

**This is a STRUCTURAL DISTRIBUTION SHIFT problem. Standard CV optimization CANNOT reach the target.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_090 implementation was correct but performed poorly.
- Evaluator's top priority: Try per-target heterogeneous ensemble. **STRONGLY AGREE** - this is fundamentally different from all 90+ experiments.
- Key concerns raised: 
  1. DO NOT submit exp_090 (CV=0.0109, predicted LB=0.0995) - **AGREED, will not submit**
  2. Correlation filtering removed useful features - **AGREED, 70% feature reduction was too aggressive**
  3. The intercept problem requires fundamentally different strategy - **AGREED, this is the core issue**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop95_analysis.ipynb` for CV-LB analysis
- Key patterns to exploit:
  1. **SM target behaves differently** - The strategy-to-get-0-11161 kernel uses different model types for SM vs Products
  2. **Feature sets matter** - ACS_PCA and Spange descriptors have different strengths
  3. **Ensemble weights vary by data type** - Single solvent vs full data may need different weights

## Recommended Approaches

### PRIORITY 1: Per-Target Heterogeneous Ensemble (HIGH PRIORITY - UNTRIED)
**Rationale**: The strategy-to-get-0-11161 kernel (LB=0.11161) uses DIFFERENT model types for different targets:
- SM target: HistGradientBoostingRegressor (HGB) - gradient boosting
- Product 2 & 3: ExtraTreesRegressor (ETR) - random forest variant
- Ensemble: 0.65 * ACS_PCA + 0.35 * Spange

**Why this could change the CV-LB relationship**:
1. Different targets may have different optimal model types
2. SM (starting material) may behave differently than products
3. This is fundamentally different from using the same model for all targets

**Implementation**:
```python
class PerTargetHeterogeneousModel:
    def __init__(self, data='single'):
        self.targets = ["Product 2", "Product 3", "SM"]
        self.models = {}
        for t in self.targets:
            if t == "SM":
                # Gradient boosting for SM
                self.models[t] = [
                    HGBModel(feature_table="acs_pca_descriptors"),
                    HGBModel(feature_table="spange_descriptors"),
                ]
            else:
                # ExtraTrees for Products
                self.models[t] = [
                    ETRModel(feature_table="acs_pca_descriptors"),
                    ETRModel(feature_table="spange_descriptors"),
                ]
    
    def predict(self, X):
        preds = []
        for t in self.targets:
            p1 = self.models[t][0].predict(X)
            p2 = self.models[t][1].predict(X)
            pred_t = 0.65 * p1 + 0.35 * p2  # Weighted ensemble
            preds.append(pred_t)
        return np.clip(np.hstack(preds), 0, 1)
```

### PRIORITY 2: Combine Best Approaches with Per-Target Logic
If per-target heterogeneous ensemble shows promise, combine with our best techniques:
- Add Arrhenius kinetics features (1/T, ln(t), T*t)
- Use CatBoost/XGBoost for some targets, GP for others
- Experiment with different feature combinations per target

### PRIORITY 3: Solvent Similarity-Based Prediction Adjustment
**Rationale**: The intercept represents extrapolation error to unseen solvents.
- Calculate similarity of test solvent to training solvents
- When test solvent is dissimilar (extrapolating), blend toward population mean
- This could reduce the intercept

### PRIORITY 4: Conservative Predictions for Outlier Solvents
- Identify solvents that are "outliers" (Water, extreme polarity)
- Use simpler models or blend toward mean for these solvents
- This could reduce extrapolation error

## What NOT to Try
1. **Neural network architectures** (GNN, GAT, ChemBERTa) - 5 consecutive failures, all 100%+ worse than best
2. **Aggressive correlation filtering** - exp_090 showed 70% feature reduction hurts performance
3. **Standard CV optimization** - All approaches fall on the same CV-LB line, intercept > target
4. **Complex deep learning** - Small dataset (656 single, 1227 full samples) doesn't support complex models

## Validation Notes
- CV scheme: Leave-One-Out for single solvent, Leave-One-Ramp-Out for full data
- CV-LB gap: ~4.36x multiplier + 0.052 intercept
- **Key insight**: The intercept (0.052) > target (0.0347) means we need to CHANGE the relationship, not just improve CV
- Submit only if CV < 0.0081 AND approach is fundamentally different (could change intercept)

## Submission Strategy (4 remaining)
1. **DO NOT submit exp_090** - CV=0.0109 is 34% worse than best
2. **Submit per-target heterogeneous ensemble** if CV < 0.0085 - fundamentally different approach
3. **Save submissions** for approaches that could change the CV-LB relationship
4. **Final submission**: Best performing approach with lowest predicted LB

## Key Insight
The target (0.0347) IS achievable - top competitors have achieved it. The key is to find an approach that:
1. Changes the CV-LB relationship (reduces the intercept)
2. Or achieves CV so low that even with the current relationship, LB < 0.0347

The per-target heterogeneous ensemble is the most promising untried approach because it's fundamentally different from all 90+ experiments we've run.
