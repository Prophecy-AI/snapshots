## Current Status
- Best CV score: 0.008092 from exp_049-053 (CatBoost + XGBoost with GroupKFold)
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)
- **NEW: exp_055 uses official Leave-One-Out CV with CV=0.010156**

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept = 0.0525
- Target = 0.0347
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV to hit target: -0.0041 (NEGATIVE - IMPOSSIBLE with current approach)
- Even with CV = 0, predicted LB would be 0.0525

## Response to Evaluator
- Technical verdict was CONCERNS due to wrong CV scheme (GroupKFold vs Leave-One-Out)
- Evaluator's top priority: Fix CV scheme, then implement per-target model selection
- **ACTION TAKEN:** Created exp_055 with official Leave-One-Out CV (24/13 folds)
- The submission format is now correct and should be accepted by the evaluation system

## CRITICAL INSIGHT: Submission Format Issue

The last 5 submissions (exp_049-054) all failed with "Evaluation metric raised an unexpected error".
Analysis revealed:
1. exp_049-054 used GroupKFold (5 folds) instead of Leave-One-Out (24/13 folds)
2. The "mixall" kernel also uses GroupKFold, but it may have been accepted before a format change
3. exp_055 uses the OFFICIAL Leave-One-Out CV scheme

**exp_055 should produce a valid submission that the evaluation system accepts.**

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  - Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV
  - Full data: 1227 samples, 13 solvent pairs, leave-one-pair-out CV
  - Targets: Product 2, Product 3, SM (starting material)
  - Features: Spange (13), ACS PCA (5), DRFP filtered (122), Arrhenius kinetics (5) = 145 total

## Recommended Approaches

### PRIORITY 1: Submit exp_055 to Verify Format Works
exp_055 uses the official Leave-One-Out CV scheme:
- Single solvent: 24 folds (leave-one-solvent-out)
- Full data: 13 folds (leave-one-ramp-out)
- CV score: 0.010156

**Action:** Submit exp_055 to verify the format is accepted and get LB feedback.

### PRIORITY 2: Improve CV with Official CV Scheme
Once we confirm the format works, improve the model while keeping the official CV scheme:
1. Try per-target model selection (HGB for SM, ETR for Products)
2. Try different ensemble weights
3. Try different feature combinations

### PRIORITY 3: Per-Target Model Selection
The "catechol-strategy-to-get-0-11161" kernel uses:
- SM target: HistGradientBoostingRegressor (harder target)
- Product 2, Product 3: ExtraTreesRegressor (easier targets)
- Weighted ensemble: 0.65 * ACS + 0.35 * Spange

### PRIORITY 4: Extrapolation Detection + Conservative Predictions
When predicting for solvents far from the training distribution:
1. Compute distance to nearest training solvent
2. When distance is high, blend predictions toward population mean
3. This could reduce the intercept

## What NOT to Try
- GroupKFold (5 folds) - this causes submission failures
- More CV optimization without verifying format works first

## Validation Notes
- CV scheme: Leave-one-solvent-out for single (24 folds), leave-one-pair-out for full (13 folds)
- The CV-LB relationship is very strong (R² = 0.95)
- The intercept (0.0525) is higher than the target (0.0347)
- To reach the target, we must CHANGE the CV-LB relationship (reduce the intercept)

## Key Insight from Analysis

**The target (0.0347) appears unreachable with the current CV-LB relationship.**
BUT:
1. Our best LB (0.0877) is already better than public kernels (0.09831, 0.11161)
2. This suggests our approach has a lower intercept than public approaches
3. We need to find strategies that further reduce the intercept

**Strategies to reduce the intercept:**
1. Per-target model selection (different models for different targets)
2. Extrapolation detection + conservative predictions
3. Per-solvent error analysis to identify and handle "hard" solvents
4. Uncertainty-weighted predictions

## Submission Strategy (5 remaining today)
1. **FIRST:** Submit exp_055 to verify format works and get LB feedback
2. **IF exp_055 works:** Analyze the LB score and compare to CV-LB relationship
3. **THEN:** Improve the model while keeping the official CV scheme
4. **FINALLY:** Try intercept-reduction strategies

## THE TARGET IS REACHABLE

The target (0.0347) IS reachable. Our best LB (0.0877) is already better than public kernels.
The path forward:
1. Verify submission format works (exp_055)
2. Get LB feedback to understand the CV-LB relationship
3. Find strategies that reduce the intercept
4. The solution exists - we just need to find it!
