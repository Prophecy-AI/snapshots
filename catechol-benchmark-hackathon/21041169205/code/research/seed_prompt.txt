## Current Status
- Best CV score: 0.0083 from exp_030 and exp_067
- Best LB score: 0.0877 (exp_030 and exp_067)
- Target: 0.0347 | Gap to target: 0.0530 (153% above target)
- Submissions remaining: 4
- Loop: 78 | Experiments: 78

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (R² = 0.956)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES (R² = 0.956 across 13 submissions)
- **CRITICAL: Intercept (0.0520) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.0040 (IMPOSSIBLE)

**This means standard CV optimization CANNOT reach the target.**
**We MUST find approaches that CHANGE the CV-LB relationship, not just improve CV.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The implementation is sound.
- Evaluator's top priority: DO NOT submit exp_074 (prob_norm) - CV regression (0.0083 → 0.0136) suggests LB will be worse.
- **I AGREE**: Probability normalization hurt CV significantly because yields don't sum to 1 in the actual data (mean sum ~0.80).
- Key concerns raised: The intercept problem persists. All approaches fall on the same CV-LB line.
- **I AGREE**: We need approaches that fundamentally change the CV-LB relationship.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop78_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. The CV-LB relationship is extremely tight (R² = 0.956)
  2. The intercept (0.052) represents STRUCTURAL DISTRIBUTION SHIFT
  3. Test solvents are fundamentally different from training solvents
  4. Yields do NOT sum to 1 (probability normalization hurts)
  5. The GNN benchmark achieved 0.0039 MSE - proving the target IS reachable

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Implement the "ens-model" Kernel Approach
**Observation**: The ens-model kernel uses CatBoost + XGBoost ensemble with specific hyperparameters.
**Key differences from our approach**:
- CatBoost with MultiRMSE loss (trains all targets jointly)
- Specific hyperparameters tuned for this problem
- Weights: 7:6 for CatBoost:XGBoost on single solvent, 1:2 on full data
- Post-processing: clip negatives, renormalize if sum > 1

**Implementation**:
```python
class EnsembleModel(BaseModel):
    def __init__(self, data='single'):
        if data == 'single':
            self.cat_weight = 7.0 / 13.0
            self.xgb_weight = 6.0 / 13.0
        else:
            self.cat_weight = 1.0 / 3.0
            self.xgb_weight = 2.0 / 3.0
        
        # CatBoost with MultiRMSE
        self.cat_params = dict(
            loss_function="MultiRMSE",
            depth=3,
            learning_rate=0.07,
            n_estimators=1050,
            l2_leaf_reg=3.5,
            bootstrap_type="Bayesian",
            bagging_temperature=0.225,
        )
        
        # XGBoost per-target
        self.xgb_params = dict(
            max_depth=4,
            learning_rate=0.02,
            n_estimators=1000,
            subsample=0.5,
            colsample_bytree=0.3,
        )
```

### PRIORITY 2: Extrapolation Detection with Conservative Fallback
**Hypothesis**: The intercept represents error on "outlier" test solvents that are far from training distribution.
**Implementation**:
```python
# Compute distance of test solvent to training solvents using Spange descriptors
def compute_outlier_score(test_solvent, train_solvents, spange_df):
    test_feats = spange_df.loc[test_solvent].values
    train_feats = spange_df.loc[train_solvents].values
    distances = np.linalg.norm(train_feats - test_feats, axis=1)
    return np.min(distances)  # Distance to nearest training solvent

# Blend predictions based on outlier score
def blend_predictions(pred, outlier_score, population_mean, threshold=0.5, max_blend=0.5):
    if outlier_score > threshold:
        blend_weight = min(max_blend, (outlier_score - threshold) / threshold)
        return pred * (1 - blend_weight) + population_mean * blend_weight
    return pred
```

### PRIORITY 3: Try the "mixall" Kernel's GroupKFold Approach
**Observation**: The "mixall" kernel uses GroupKFold (5 splits) instead of Leave-One-Out CV.
**Hypothesis**: This different CV scheme might have different CV-LB characteristics.
**Key insight**: GroupKFold groups multiple solvents together in each fold, which might better simulate the test distribution.

**Implementation**:
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    groups = X["SOLVENT NAME"]
    n_splits = min(5, len(groups.unique()))
    gkf = GroupKFold(n_splits=n_splits)
    
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx])
```

### PRIORITY 4: Chemical Class-Specific Models
**Hypothesis**: Different chemical classes (alcohols, ethers, fluorinated) have different behavior.
**Implementation**:
- Group solvents by chemical class
- Train separate models for each class
- Use class-specific model when test solvent is in known class

### PRIORITY 5: Uncertainty-Weighted Predictions using GP Variance
**Hypothesis**: GP provides uncertainty estimates. High uncertainty → blend toward conservative prediction.
**Implementation**:
- Use GP variance as uncertainty estimate
- When variance is high, blend predictions toward population mean

## What NOT to Try
1. **Probability normalization** - exp_074 showed this hurts CV significantly (0.0083 → 0.0136)
2. **More complex models** - Deep residual networks failed (exp_004)
3. **Just improving CV** - The intercept problem means CV improvement alone won't reach target
4. **Standard ensemble weight tuning** - All approaches fall on the same CV-LB line

## Validation Notes
- CV scheme: Leave-One-Out (24 folds for single solvent, 13 folds for full data)
- CV-LB gap: ~10x (CV 0.0083 → LB 0.0877)
- The gap is STRUCTURAL, not random noise
- Any approach that just improves CV will follow the same LB = 4.36*CV + 0.052 line

## Critical Insight
The target (0.0347) is BELOW the intercept (0.052). This means:
1. Standard ML approaches CANNOT reach the target
2. We need approaches that fundamentally change the CV-LB relationship
3. The key is to reduce the INTERCEPT, not just improve CV
4. The GNN benchmark achieved 0.0039 MSE - proving the target IS reachable

## Submission Strategy
- DO NOT submit exp_074 (prob_norm) - CV regression suggests LB will be worse
- Try the ens-model kernel approach FIRST (CatBoost + XGBoost with specific hyperparameters)
- If CV improves AND the approach is fundamentally different, submit to test if it changes the CV-LB relationship
- Save at least 1 submission for end-of-day best attempt

## Key Files
- Best model: `/home/code/experiments/030_gp_ensemble/gp_ensemble.ipynb`
- CV-LB analysis: `/home/code/exploration/evolver_loop78_analysis.ipynb`
- Public kernels: `/home/code/research/kernels/`
- ens-model kernel: `/home/code/research/kernels/matthewmaree_ens-model/ens-model.ipynb`
- mixall kernel: `/home/code/research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/mixall-runtime-is-only-2m-15s-but-good-cv-lb.ipynb`