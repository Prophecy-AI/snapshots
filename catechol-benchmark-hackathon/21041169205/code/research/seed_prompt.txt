## Current Status
- Best CV score: 0.008092 from exp_050 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030/exp_067 (GP+MLP+LGBM)
- Target: 0.0347 | Gap to target: 152.7% (0.053 MSE gap)
- Submissions remaining: 4
- Loop: 99 of 99 experiments completed

## CV-LB Relationship Analysis (CRITICAL - MUST READ)
- Linear fit: LB = 4.36 * CV + 0.052 (R² = 0.956)
- **Intercept (0.052) > Target (0.0347)** - FUNDAMENTAL PROBLEM
- Required CV for target: -0.004 (IMPOSSIBLE - would need negative CV)
- All 13 submissions with LB scores fall on the SAME line
- This means: **No amount of CV optimization will reach the target**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. exp_094 (ens-model replication) executed correctly.
- Evaluator's top priority: Investigate what top competitors do differently. **STRONGLY AGREE.**
- Key concerns raised:
  1. exp_094 (CV=0.009564) is 15.26% WORSE than best CV (0.008298) - **DO NOT SUBMIT**
  2. Current submission file is from exp_094 - **MUST REGENERATE FROM exp_030**
  3. All approaches fall on same CV-LB line - need fundamentally different strategy

## CRITICAL INSIGHT: The Target IS Reachable
- 1st place achieved 0.0347 (the target)
- 2nd place is at ~0.07 (103% worse than 1st!)
- Our best is 0.0877 (24% worse than 2nd)
- **The HUGE gap between 1st and 2nd suggests 1st place found something fundamentally different**
- 2nd place and below are using similar approaches to us
- 1st place must be using a completely different strategy

## Data Understanding
- Reference notebooks: `exploration/evolver_loop99_analysis.ipynb`
- Key patterns discovered:
  1. CV-LB intercept (0.052) represents structural distribution shift to unseen solvents
  2. All model types (MLP, LGBM, XGB, GP, CatBoost, GNN, GAT) fall on same CV-LB line
  3. Non-linear mixture features HURT performance (exp_093 proved this)
  4. Kernel replications (ens-model, mixall) did NOT improve our results
  5. Advanced approaches (GNN, GAT, ChemBERTa) all performed WORSE than simple tabular models

## What We've Tried (98 experiments)
- MLPs with various architectures ✓
- LightGBM, XGBoost, CatBoost ✓
- Gaussian Processes ✓
- GNNs and GATs ✓
- ChemBERTa embeddings ✓
- Uncertainty-weighted predictions ✓
- Non-linear mixture features ✓
- Various ensemble combinations ✓
- Kernel replications (ens-model, mixall) ✓

## Recommended Approaches (PRIORITY ORDER)

### 1. IMMEDIATE: Regenerate Submission from Best Model
Current submission is from exp_094 (CV=0.009564, WORSE).
**MUST regenerate from exp_030 (CV=0.0083, LB=0.0877) before any submission.**

### 2. TRY SIMPLER MODELS FOR BETTER GENERALIZATION
The intercept problem suggests overfitting to training distribution.
- **Ridge Regression** with Spange + ACS features (no DRFP)
- **Linear models** with strong regularization
- **Simpler ensembles** (2 models instead of 3-4)
- Hypothesis: Simpler models may have lower intercept

### 3. CONSERVATIVE PREDICTION STRATEGIES
When extrapolating to unseen solvents, blend toward population mean:
- Calculate mean prediction from training data
- For each test sample, compute "extrapolation score" (distance to training distribution)
- Blend: final_pred = (1 - alpha) * model_pred + alpha * mean_pred
- alpha increases with extrapolation score

### 4. SOLVENT-SPECIFIC ADJUSTMENTS
Some solvents are harder to predict (Water, HFIP with extreme properties):
- Identify "outlier" solvents by their descriptor values
- Use simpler/more conservative predictions for outliers
- Weight predictions by solvent similarity to training set

### 5. PHYSICS-INFORMED CONSTRAINTS
Chemistry knowledge that generalizes to unseen solvents:
- Yields must sum to ≤ 1 (already enforced)
- Arrhenius relationship: ln(k) = ln(A) - Ea/(RT)
- Solvent polarity effects on reaction rates
- Temperature-time trade-offs

## What NOT to Try (Exhausted)
- GNN/GAT approaches (exp_085-087 all worse)
- ChemBERTa embeddings (exp_088 worse)
- Non-linear mixture features (exp_093 worse)
- Complex ensemble architectures (all fall on same CV-LB line)
- Kernel replications (ens-model, mixall - did not help)
- More hyperparameter tuning (diminishing returns)

## Validation Notes
- Use official Leave-One-Out CV (24 folds for single, 13 folds for full)
- DO NOT modify the last 3 cells of the notebook
- CV-LB relationship: expect LB ≈ 4.36 * CV + 0.052
- A model with CV=0.008 should have LB ≈ 0.087

## SUBMISSION STRATEGY (4 remaining)
1. **DO NOT SUBMIT** exp_094 (CV=0.009564, worse than best)
2. If trying a new approach, only submit if CV < 0.0083 (best so far)
3. Consider submitting exp_050 (CV=0.0081) to verify if better CV → better LB
4. Save submissions for approaches that might CHANGE the CV-LB relationship

## THE PATH FORWARD
The target (0.0347) IS reachable - 1st place achieved it.
Our current approach is fundamentally limited by the CV-LB intercept (0.052).
We need to find what 1st place is doing differently.

**Key hypothesis**: 1st place is using an approach that:
1. Has a LOWER intercept (better generalization to unseen solvents)
2. OR has a DIFFERENT CV-LB relationship entirely
3. OR is using domain knowledge/constraints that we haven't discovered

**DO NOT GIVE UP.** The solution exists. Keep experimenting.