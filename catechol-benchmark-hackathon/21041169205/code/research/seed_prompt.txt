## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (but these had submission errors)
- Best verified LB score: 0.0877 from exp_030 and exp_067
- Target: 0.0347 | Gap to target: 0.0530 (153% above target)
- Remaining submissions: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.0520 (RÂ² = 0.9558)
- Intercept interpretation: Even at CV=0, expected LB is 0.0520
- Are all approaches on the same line? YES
- **CRITICAL**: Intercept (0.0520) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0520) / 4.36 = -0.0040 (IMPOSSIBLE)

**This means standard CV optimization CANNOT reach the target. We MUST change the CV-LB relationship.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_074 implementation was sound.
- Evaluator's top priority: Debug the full data CV regression. AGREE - this is critical.
- Key concerns raised:
  1. Full data CV went from 0.008488 (exp_030) to 0.043100 (exp_074) - 5x worse
  2. The base model itself is performing worse on full data
  3. Root cause: exp_074 used a single MLP with [128,64] instead of exp_030's WeightedMLPEnsemble with 5 models, [32,16], loss_weights=[1.0,1.0,2.0], and flip=True data augmentation

**The evaluator correctly identified the issue. The fix is to use exp_030's exact model architecture.**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop76_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All model types (MLP, LGBM, XGB, GP) fall on the same CV-LB line
  2. The intercept (0.0520) represents EXTRAPOLATION ERROR to unseen solvents
  3. Outlier solvents (HFIP, Cyclohexane, Water) have DIFFERENT behavior
  4. Public kernels use different approaches:
     - "best-work-here": Normalizes predictions to probabilities (row sums = 1)
     - "mixall": Uses GroupKFold(5) instead of Leave-One-Out CV

## KEY INSIGHT FROM PUBLIC KERNELS

The "best-work-here" kernel normalizes predictions to probabilities:
```python
# Triple normalization for maximum stability
probs = np.clip(probs, 1e-10, 1.0)
probs = probs / probs.sum(axis=1, keepdims=True)
probs = np.clip(probs, 1e-10, 1.0)
probs = probs / probs.sum(axis=1, keepdims=True)
```

This treats the three targets (Product 2, Product 3, SM) as a probability distribution.
This might help with the CV-LB relationship because:
1. It constrains predictions to sum to 1 (physical constraint)
2. It prevents extreme predictions that might hurt LB
3. It's a form of regularization that might reduce extrapolation error

## Recommended Approaches

### PRIORITY 1: Probability Normalization on exp_030
Add probability normalization to exp_030's predictions:
1. Use exp_030's exact GP+MLP+LGBM ensemble architecture
2. After getting predictions, normalize to probabilities:
   ```python
   preds = np.clip(preds, 1e-9, None)
   preds = preds / preds.sum(axis=1, keepdims=True)
   ```
3. This is a simple change that might change the CV-LB relationship

Expected results:
- CV might change slightly (normalization affects MSE)
- LB might improve if the normalization helps with extrapolation

### PRIORITY 2: NN Blending on Single Solvents Only
If probability normalization doesn't help, try NN blending:
1. Use exp_030's exact model for full data (no changes)
2. Apply NN blending only to single solvents:
   - Compute outlier scores based on Spange descriptor distance
   - For outliers, blend toward k-nearest training solvents
   - Use blend_threshold=1.5, k_neighbors=3

### PRIORITY 3: Uncertainty-Weighted Predictions
Use GP variance to weight predictions toward conservative values:
```python
# Get GP predictions with uncertainty
gp_pred, gp_std = gp.predict(X_test, return_std=True)

# Blend toward mean when uncertainty is high
uncertainty_weight = np.clip(gp_std / gp_std.max(), 0, 1)
conservative_pred = (1 - uncertainty_weight) * raw_pred + uncertainty_weight * train_mean
```

### PRIORITY 4: Chemical Class-Specific Models
Train separate models for different solvent families:
- Alcohols (Methanol, Ethanol, IPA, TFE, HFIP)
- Ethers (THF, 2-MeTHF, Diethyl Ether, MTBE)
- Polar aprotic (Acetonitrile, DMA, DMSO)
- Non-polar (Cyclohexane, Toluene)

## What NOT to Try
- Blending toward global mean (proven to hurt performance in exp_071)
- Deep residual networks (exp_004 failed badly)
- DRFP-only features with PCA (exp_002 was much worse)
- Aggressive extrapolation detection with low threshold
- Changing the full data model architecture (exp_074 showed this hurts performance)

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvents (24 folds), leave-one-ramp-out for full data (13 folds)
- CV-LB gap: ~4.36x multiplier + 0.0520 intercept
- The intercept is the key problem - we need to reduce it, not just improve CV

## Key Insight
The target (0.0347) is BELOW the intercept (0.0520). This means:
1. Standard ML optimization CANNOT reach the target
2. We need approaches that CHANGE THE CV-LB RELATIONSHIP
3. The GNN benchmark achieved MSE 0.0039 - the target IS reachable
4. We need to find what makes the test set different from training

## Submission Strategy
With only 4 remaining submissions:
1. **FIRST**: Try probability normalization on exp_030 (simple change, might help)
2. **IF CV improves**: Submit to test if it changes the CV-LB relationship
3. **IF NOT**: Try NN blending on single solvents only
4. **THEN**: Try uncertainty weighting or chemical class-specific models

## Specific Implementation for Next Experiment

Create experiment 074_prob_norm that:
1. Uses exp_030's exact GP+MLP+LGBM ensemble architecture
2. Adds probability normalization to predictions:
   ```python
   # After getting predictions
   preds = np.clip(preds, 1e-9, None)
   preds = preds / preds.sum(axis=1, keepdims=True)
   ```
3. Keeps everything else unchanged from exp_030

Expected results:
- CV might change slightly (normalization affects MSE)
- LB might improve if the normalization helps with extrapolation

## THE TARGET IS REACHABLE

The GNN benchmark achieved MSE 0.0039 on this exact dataset. The target (0.0347) is 8.9x worse than the GNN result, proving it's very achievable. The key is to find an approach that changes the CV-LB relationship, not just improves CV.

**NEXT STEPS:**
1. **Try**: Probability normalization on exp_030
2. **Submit**: Test if this changes the CV-LB relationship
3. **Iterate**: Based on LB feedback, adjust the approach