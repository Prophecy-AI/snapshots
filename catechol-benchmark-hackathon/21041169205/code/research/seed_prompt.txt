## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 (exp_030 and exp_067)
- Target: 0.0347 | Gap to target: 0.0530 (152.7% worse)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.36 * CV + 0.052 (R² = 0.956)
- Intercept interpretation: Even at CV=0, expected LB is 0.052
- **Are all approaches on the same line? YES**
- **CRITICAL: Intercept (0.052) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.052) / 4.36 = -0.004 (IMPOSSIBLE)

**This means standard CV optimization CANNOT reach the target.**
The 1st place score (0.0347) is BELOW our intercept, meaning they found a way to CHANGE the CV-LB relationship.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Execution is sound.
- Evaluator's top priority: RETURN TO CONSERVATIVE BLENDING. I agree this is a reasonable direction, but the conservative blending experiments (exp_096, exp_097) both made CV worse.
- Key concerns raised: The intercept problem is being ignored; recent experiments are regressive. I fully agree - we've been stuck optimizing CV when the real problem is the intercept.
- Evaluator correctly identified that adding more models (5-model ensemble) didn't help. The 3-model ensemble (GP+MLP+LGBM) remains optimal.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop103_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All model types (MLP, LGBM, GP, CatBoost, XGBoost) fall on the SAME CV-LB line
  2. The intercept (0.052) represents extrapolation error to unseen solvents
  3. 1st place (0.0347) found something fundamentally different
  4. Recent experiments (exp_094-098) all made CV WORSE than exp_030

## Recommended Approaches

**PRIORITY 1: Per-Target Model with Different Architectures**
The dabansherwani kernel uses different model types for different targets:
- SM: HistGradientBoosting (hardest target, needs more regularization)
- Product 2/3: ExtraTreesRegressor (easier targets, can use more complex models)
- Weights: 0.65 * ACS_PCA + 0.35 * Spange features

This is worth trying because:
1. SM is consistently the hardest target (highest error)
2. Different targets may have different optimal model types
3. We haven't tried ExtraTreesRegressor yet

```python
class PerTargetEnsemble:
    def __init__(self, data='single'):
        self.targets = ["Product 2", "Product 3", "SM"]
        self.models = {}
        for t in self.targets:
            if t == "SM":
                # SM is hardest - use HistGradientBoosting
                self.models[t] = [
                    HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04),
                    HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04)
                ]
            else:
                # P2/P3 are easier - use ExtraTrees
                self.models[t] = [
                    ExtraTreesRegressor(n_estimators=900, min_samples_leaf=2),
                    ExtraTreesRegressor(n_estimators=900, min_samples_leaf=2)
                ]
```

**PRIORITY 2: Yield Normalization Post-Processing**
The nocarbonintelligence kernel normalizes yields to sum to 1 (mass balance constraint):
```python
# Post-processing
predictions = np.clip(predictions, 0, None)
sum_yields = predictions.sum(axis=1, keepdims=True)
predictions = predictions / np.clip(sum_yields, 1e-12, None)
predictions = np.clip(predictions, 0, 1)
```

This enforces a physics constraint that should generalize to unseen solvents.

**PRIORITY 3: Combine GP+MLP+LGBM with Per-Target Weighting**
Instead of uniform weights across targets, use target-specific weights:
- SM: Higher GP weight (more conservative for hardest target)
- P2/P3: Higher MLP/LGBM weight (more aggressive for easier targets)

**PRIORITY 4: Feature Importance Analysis**
Before trying more models, analyze which features are most important:
- Are Spange features more important than DRFP?
- Are ACS PCA features adding value?
- Which features have the highest correlation with targets?

This could guide feature selection to reduce overfitting.

## What NOT to Try
- ❌ More models in ensemble (5-model was worse than 3-model)
- ❌ Conservative blending with high blend_strength (made CV worse)
- ❌ GP uncertainty blending (exp_097 was 7.6% worse)
- ❌ CatBoost + XGBoost only (exp_094 was 15.2% worse)
- ❌ Ridge regression (exp_095 was 89.8% worse)

## Validation Notes
- CV scheme: Leave-One-Out for single solvents, Leave-One-Ramp-Out for mixtures
- CV-LB gap: ~4.36x multiplier + 0.052 intercept
- The intercept is the real bottleneck - any approach that doesn't reduce it won't reach target

## Key Insight
The 1st place score (0.0347) is BELOW our intercept (0.052). This means they found a fundamentally different approach that doesn't follow our CV-LB line. Possible explanations:
1. Features that generalize better to unseen solvents
2. Physics-based constraints that hold for all solvents
3. Different validation scheme that better matches test distribution
4. Post-processing that adjusts predictions for test distribution

## Submission Strategy
- Only 4 submissions remaining
- DO NOT submit unless CV < 0.0083 (our best)
- Consider submitting if a new approach shows promise (different CV-LB relationship)
- The per-target ensemble approach is worth trying first
