## Current Status
- Best CV score: 0.0083 from exp_030
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)
- Remaining submissions: 5
- Latest experiment: exp_057 (CV=0.009524) - CatBoost+XGBoost with ALL features

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE = IMPOSSIBLE)
- **CONCLUSION: Improving CV alone CANNOT reach the target**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The submission format is correct.
- Evaluator's top priority: Submit exp_057 to get LB feedback.
- Key concerns raised: CV (0.009524) is 17.7% worse than best CV (0.008092).
- **My response**: I AGREE we should submit exp_057 to test if combining ALL features changes the CV-LB relationship. Even though CV is worse, the hypothesis is that more comprehensive features might generalize better to unseen solvents (lower intercept). Predicted LB is 0.0935, but if the relationship changes, actual LB could be better.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop59_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions with LB feedback fall on the same linear CV-LB relationship (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL distribution shift
  3. Test solvents are fundamentally different from training solvents
  4. SM target is hardest (highest variance, most outliers)

## THE FUNDAMENTAL PROBLEM

The target (0.0347) is BELOW the intercept (0.0525) of our CV-LB relationship. This means:
- Even with PERFECT CV (CV=0), we'd still get LB=0.0525
- No amount of model tuning can reach the target
- We need to CHANGE the CV-LB relationship itself

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Submit exp_057 for LB Feedback
- Tests if combining ALL 5 feature sources changes the CV-LB relationship
- Even though CV is worse (0.009524 vs 0.0083), the relationship might be different
- This is a STRATEGIC submission to test a hypothesis

### PRIORITY 2: Return to Best Approach (GP+MLP+LGBM) with Modifications
If exp_057 doesn't improve the relationship, return to exp_030 approach and try:

1. **Extrapolation Detection + Conservative Predictions:**
   - Compute Tanimoto similarity to nearest training solvents using fingerprints
   - When similarity is low (extrapolating), blend predictions toward population mean
   - This could REDUCE the intercept by being conservative on hard cases

2. **Per-Solvent Error Analysis:**
   - Identify which solvents cause the most error (HFIP, TFE, Water are likely)
   - These are outliers in polarity/hydrogen bonding
   - Apply special handling for these solvents

3. **Uncertainty-Weighted Predictions:**
   - Use GP uncertainty estimates
   - High uncertainty → conservative prediction (closer to mean)
   - This could reduce extrapolation error

### PRIORITY 3: Study Top Public Kernels More Deeply
Top kernels that might have insights:
- "System Malfunction V1" (29 votes) - Simple MLP, but what's different?
- "Alchemy | Baseline" (12 votes) - Multi-GPU optimized MLP with numeric FE
- "mixall" (9 votes) - Uses GroupKFold instead of Leave-One-Out

Key differences to investigate:
1. **Numeric feature engineering**: rt², temp², log1p(rt), log1p(temp), rt*temp
2. **StandardScaler on features** (we use BatchNorm)
3. **CosineAnnealingLR** instead of ReduceLROnPlateau
4. **SmoothL1Loss (Huber)** with beta=1.0

### PRIORITY 4: Solvent Clustering + Class-Specific Models
1. Group solvents by chemical class (alcohols, ethers, esters, halogenated, etc.)
2. Train class-specific models that generalize within chemical families
3. For test solvents, identify their class and use appropriate model

## What NOT to Try
- ❌ More hyperparameter tuning on existing models (won't change intercept)
- ❌ Different ensemble weights alone (won't change intercept)
- ❌ Normalizing predictions to sum to 1 (WRONG - yields don't sum to 1)
- ❌ Submitting experiments with worse CV unless testing a specific hypothesis

## Validation Notes
- Use official Leave-One-Out CV (24 folds for single, 13 folds for full)
- Track both CV and predicted LB (using the linear relationship)
- Look for approaches that give LOWER predicted LB for same CV (changing the relationship)

## Concrete Next Steps

1. **SUBMIT exp_057** to get LB feedback and test if ALL features change the relationship
2. **If exp_057 doesn't help**: Return to GP+MLP+LGBM (exp_030) and add:
   - Extrapolation detection features
   - Conservative predictions for outlier solvents
   - Numeric feature engineering from top kernels (rt², temp², log1p, etc.)

3. **Analyze LB feedback**: If exp_057 LB is better than predicted (< 0.0935), the relationship is changing. If worse or as predicted, the approach doesn't help.

## CRITICAL REMINDER

The target IS reachable. Top competitors have achieved it. We need to find what they're doing differently. The key is NOT improving CV - it's changing the CV-LB relationship.

**5 submissions remaining - use them strategically to test hypotheses about changing the relationship, not just improving CV.**
