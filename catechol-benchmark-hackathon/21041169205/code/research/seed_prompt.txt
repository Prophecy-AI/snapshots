## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (but submission failed)
- Best LB score: 0.0877 from exp_030 (last successful submission)
- Target: 0.0347 | Gap to target: 0.0530 (60% reduction needed)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 * CV + 0.0528 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- Are all approaches on the same line? YES - All 12 successful submissions fall on this line
- **CRITICAL: The intercept (0.0528) is ABOVE the target (0.0347)**
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was CONCERNS - Predictions outside [0,1] range may cause evaluation errors
- Evaluator's top priority: Add prediction clipping to ensure outputs are in [0,1] range
- Key concerns raised:
  1. Predictions outside [0,1] range (15.2% of predictions have values < 0 or > 1)
  2. CV cell has escaped newlines (minor issue)
  3. Very basic model (CV 0.0827 vs best 0.008)
- **ADDRESSING**: The evaluator is correct that predictions outside [0,1] could cause evaluation errors. We need to add clipping.

## IMMEDIATE PRIORITY: Fix Submission Issue

### CRITICAL FIX NEEDED: Add Prediction Clipping
The evaluator correctly identified that predictions outside [0,1] range could cause evaluation errors.
- Ground truth is in [0, 1.08] range (SM can be slightly above 1.0)
- Current predictions: min=-0.4460, max=1.1912
- **FIX**: Add `torch.clamp(predictions, 0, 1)` or use sigmoid output

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  1. Leave-one-solvent-out CV simulates predicting for unseen solvents
  2. Test solvents may be "harder" (more extreme properties)
  3. The CV-LB gap (~4.3x multiplier + 0.0528 intercept) is consistent across all models

## Recommended Approaches

### Priority 1: Fix Submission with Sigmoid Output (MUST DO FIRST)
Create a new experiment that:
1. Uses the exact official template structure (7 cells, no extra cells after final)
2. Adds sigmoid output to MLPModel to ensure predictions are in [0,1]
3. Uses simple Spange descriptors (like the official template)
4. NO extra cells after the final submission cell

**Code change needed in MLPModel:**
```python
def forward(self, x):
    return torch.sigmoid(self.model(x))  # Ensures output in [0, 1]
```

### Priority 2: Restore Best Model with Clipping
If the simple model submission succeeds:
1. Restore the GP+MLP+LGBM ensemble from exp_030
2. Add prediction clipping: `predictions = torch.clamp(predictions, 0, 1)`
3. Expected LB ~0.087 (same as exp_030)

### Priority 3: Distribution-Shift-Aware Strategies
Since the intercept (0.0528) > target (0.0347), standard CV optimization cannot reach the target.
We need strategies that REDUCE THE INTERCEPT, not just improve CV:

1. **Extrapolation Detection Features**
   - Add features measuring solvent distance to training distribution
   - Use molecular fingerprint similarity (Tanimoto) to nearest training solvents
   - When extrapolating, blend predictions toward population mean

2. **Uncertainty-Weighted Predictions**
   - Use GP uncertainty estimates
   - High uncertainty → conservative prediction (closer to mean)
   - Blend complex model with simple baseline based on extrapolation degree

## What NOT to Try
- More complex ensembles without fixing the clipping issue first
- Hyperparameter tuning - The CV-LB relationship shows this won't help
- Standard CV optimization - The intercept problem persists

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvent, leave-one-ramp-out for full data
- CV-LB gap: ~4.3x multiplier + 0.0528 intercept
- The intercept represents EXTRAPOLATION ERROR that no model tuning can fix

## Key Insight
The target IS reachable - the benchmark achieved MSE 0.0039 on this exact dataset.
But standard ML approaches cannot reach it. The solution requires:
1. Fixing the submission issue (add sigmoid output)
2. Implementing fundamentally different strategies that address distribution shift
3. Learning from what top competitors do - they've solved this problem

## NEXT STEPS
1. **Create exp_067**: Simple MLP with sigmoid output (ensures [0,1] predictions)
2. **Verify locally that all predictions are in [0,1] range**
3. **Submit to verify pipeline works**
4. **If successful, restore best model with clipping**
5. **Focus on distribution-shift strategies to reduce the intercept**