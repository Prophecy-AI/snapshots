## Current Status
- Best CV score: 0.0083 from exp_030
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)
- Remaining submissions: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE = IMPOSSIBLE)
- **CONCLUSION: Improving CV alone CANNOT reach the target**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The submission format is correct.
- Evaluator's top priority: Submit exp_057 to get LB feedback, then revisit best approach.
- Key concerns raised: CV (0.009263) is 14% worse than best CV (0.008092).
- **My response**: I DISAGREE with submitting exp_056 - it's predicted to give LB ~0.0925 (worse than best 0.0877). We should NOT waste a submission on a worse approach. Instead, we need approaches that CHANGE the CV-LB relationship.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop58_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 submissions with LB feedback fall on the same linear CV-LB relationship (R²=0.95)
  2. The intercept (0.0525) represents STRUCTURAL distribution shift
  3. Test solvents are fundamentally different from training solvents
  4. SM target is hardest (highest variance, most outliers)

## THE FUNDAMENTAL PROBLEM

The target (0.0347) is BELOW the intercept (0.0525) of our CV-LB relationship. This means:
- Even with PERFECT CV (CV=0), we'd still get LB=0.0525
- No amount of model tuning can reach the target
- We need to CHANGE the CV-LB relationship itself

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Implement 'ens-model' Kernel Approach (matthewmaree)
This kernel combines ALL 5 feature sources which we haven't fully tried:

1. **Load ALL feature sources:**
   - spange_descriptors (13 features)
   - acs_pca_descriptors (10 features)
   - drfps_catechol (2048 sparse features)
   - fragprints (sparse features)
   - smiles (SMILES strings)

2. **Apply correlation-based feature filtering:**
   - Remove constant columns
   - Remove highly correlated features (threshold 0.8-0.9)
   - Keep features with priority: spange > acs > drfps > frag > smiles

3. **Add numeric feature engineering:**
   - T_x_RT (temperature * residence time)
   - RT_log (log of residence time)
   - T_inv (1/temperature in Kelvin)
   - RT_scaled (residence time / mean)

4. **Train CatBoost + XGBoost ensemble:**
   - Single solvent: weights 7:6 (CatBoost:XGBoost)
   - Full data: weights 1:2 (CatBoost:XGBoost)

5. **Clip predictions to [0, 1]** (DO NOT normalize to sum to 1)

**Hypothesis**: Combining ALL feature sources may provide better generalization to unseen solvents.

### PRIORITY 2: Extrapolation Detection + Conservative Predictions
When predicting for solvents that are "far" from training distribution:
1. Compute Tanimoto similarity to nearest training solvents using fingerprints
2. When similarity is low (extrapolating), blend predictions toward population mean
3. This could REDUCE the intercept by being conservative on hard cases

### PRIORITY 3: Solvent Clustering + Class-Specific Models
1. Group solvents by chemical class (alcohols, ethers, esters, halogenated, etc.)
2. Train class-specific models that generalize within chemical families
3. For test solvents, identify their class and use appropriate model

## What NOT to Try
- ❌ More hyperparameter tuning on existing models (won't change intercept)
- ❌ Different ensemble weights (won't change intercept)
- ❌ Per-target model selection (exp_056 showed worse CV)
- ❌ Normalizing predictions to sum to 1 (WRONG - yields don't sum to 1)
- ❌ Submitting exp_056 (predicted LB ~0.0925, worse than best 0.0877)

## Validation Notes
- Use official Leave-One-Out CV (24 folds for single, 13 folds for full)
- Track both CV and predicted LB (using the linear relationship)
- Look for approaches that give LOWER predicted LB for same CV (changing the relationship)

## Concrete Next Experiment: exp_057

**Implement the 'ens-model' kernel approach with ALL 5 feature sources combined.**

Key implementation details from the kernel:
1. Build combined solvent feature table from all sources
2. Apply correlation filtering with feature priority
3. Use CatBoost + XGBoost ensemble with optimized weights
4. Proper clipping to [0, 1] without normalization

This approach may change the CV-LB relationship by using more comprehensive features that generalize better to unseen solvents.

## CRITICAL REMINDER

The target IS reachable. Top competitors have achieved it. We need to find what they're doing differently. The key is NOT improving CV - it's changing the CV-LB relationship.