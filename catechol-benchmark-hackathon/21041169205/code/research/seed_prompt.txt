## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)
- **exp_055 already submitted - need new experiment**

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept = 0.0525
- Target = 0.0347
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV to hit target: -0.0041 (NEGATIVE - IMPOSSIBLE with current approach)
- **ALL model types fall on the same CV-LB line - this is STRUCTURAL DISTRIBUTION SHIFT**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_055
- Evaluator's top priority: Submit to verify format, then implement per-target model selection
- **ISSUE:** exp_055 was already submitted with a different model
- **ACTION:** Create exp_056 with per-target model selection

## CRITICAL INSIGHT: The Intercept Problem

The CV-LB analysis reveals a fundamental issue:
- **Intercept (0.0525) > Target (0.0347)**
- This means the target is MATHEMATICALLY UNREACHABLE with current approaches
- All model types (MLP, LGBM, XGB, GP, Ridge, CatBoost) fall on the same line
- Improving CV alone will NOT reach the target

**To reach the target, we must CHANGE THE CV-LB RELATIONSHIP:**
1. Reduce the intercept (address distribution shift)
2. Or find an approach that doesn't follow this linear pattern

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns:
  - Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV
  - Full data: 1227 samples, 13 solvent pairs, leave-one-pair-out CV
  - Targets: Product 2, Product 3, SM (starting material)
  - Features: Spange (13), ACS PCA (5), DRFP filtered (122), Arrhenius kinetics (5) = 145 total

## Recommended Approaches

### PRIORITY 1: Per-Target Model Selection (exp_056)
The public kernel "catechol-strategy-to-get-0-11161" achieved LB 0.11161 using:
- Different model types for different targets (HGB for SM, ETR for Products)
- Weighted ensemble of two feature sets (0.65 * ACS + 0.35 * Spange)

**This approach has NOT been tried yet and could potentially change the CV-LB relationship.**

Implementation for exp_056:
```python
from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor

class PerTargetModel:
    def __init__(self, data='single'):
        self.data = data
        self.targets = ["Product 2", "Product 3", "SM"]
        self.models = {}
        self.scalers = {}
        
    def train_model(self, train_X, train_Y, device=None, verbose=False):
        # Prepare features
        X_features = self._prepare_features(train_X)
        
        for i, target in enumerate(self.targets):
            y = train_Y.iloc[:, i].values
            
            if target == "SM":
                # SM is hardest - use HistGradientBoostingRegressor
                self.models[target] = HistGradientBoostingRegressor(
                    max_iter=500, max_depth=5, learning_rate=0.05
                )
            else:
                # Products are easier - use ExtraTreesRegressor
                self.models[target] = ExtraTreesRegressor(
                    n_estimators=200, max_depth=10, random_state=42
                )
            
            self.models[target].fit(X_features, y)
    
    def predict(self, test_X):
        X_features = self._prepare_features(test_X)
        
        predictions = []
        for target in self.targets:
            pred = self.models[target].predict(X_features)
            predictions.append(pred)
        
        # Stack predictions: [N, 3]
        preds = np.column_stack(predictions)
        preds = np.clip(preds, 0, 1)
        return torch.tensor(preds, dtype=torch.double)
```

### PRIORITY 2: Extrapolation Detection + Conservative Predictions
Add features measuring solvent distance to training distribution:
- When extrapolating, blend predictions toward population mean
- This could reduce the intercept by being more conservative on unseen solvents

### PRIORITY 3: Solvent Clustering
Group solvents by chemical class and use class-specific models:
- Alcohols, ethers, esters, etc.
- Detect when test solvent is in a known vs novel class

## What NOT to Try
- More variations that just improve CV without addressing the intercept
- Any approach that falls on the same CV-LB line

## Validation Notes
- CV scheme: Leave-one-solvent-out for single (24 folds), leave-one-pair-out for full (13 folds)
- The CV-LB relationship is very strong (R² = 0.95)
- The intercept (0.0525) is higher than the target (0.0347)

## Submission Strategy (5 remaining today)
1. **Create exp_056** with per-target model selection
2. **Submit exp_056** to see if it changes the CV-LB relationship
3. **If no improvement:** Try extrapolation detection
4. **If still no improvement:** Try solvent clustering

## THE TARGET IS REACHABLE

The target (0.0347) IS reachable, but NOT through incremental CV improvements.

**The path forward requires:**
1. Find approaches that CHANGE the CV-LB relationship
2. Reduce the intercept through:
   - Per-target model selection (NEXT)
   - Extrapolation detection
   - Uncertainty weighting
   - Solvent clustering

**The solution exists - we just need to find an approach that breaks the current CV-LB pattern!**

## IMPORTANT: The Intercept is the Bottleneck

DO NOT keep optimizing CV if all approaches fall on the same CV-LB line.
The intercept (0.0525) represents extrapolation error that no amount of model tuning can fix.

**Focus on strategies that could REDUCE THE INTERCEPT:**
1. Per-target model selection (different models for different targets) - TRY THIS NEXT
2. Extrapolation detection + conservative predictions
3. Solvent clustering + class-specific models
4. Physics-informed constraints that generalize to unseen solvents
