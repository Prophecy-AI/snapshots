{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47aaaf85",
   "metadata": {},
   "source": [
    "# Experiment 057: 'ens-model' Kernel Approach with ALL Features\n",
    "\n",
    "**Goal:** Implement the matthewmaree 'ens-model' kernel approach that combines ALL 5 feature sources.\n",
    "\n",
    "**Key components:**\n",
    "1. Combined solvent feature table from ALL sources (spange, acs_pca, drfps, fragprints, smiles)\n",
    "2. Correlation-based feature filtering with priority (spange > acs > drfps > frag > smiles)\n",
    "3. Numeric feature engineering (T_x_RT, RT_log, T_inv, RT_scaled)\n",
    "4. CatBoost + XGBoost ensemble (7:6 for single, 1:2 for full)\n",
    "5. Clip predictions to [0, 1] WITHOUT normalizing to sum to 1\n",
    "\n",
    "**Hypothesis:** Combining ALL feature sources may provide better generalization to unseen solvents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf27b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:05.265590Z",
     "iopub.status.busy": "2026-01-15T23:56:05.265115Z",
     "iopub.status.idle": "2026-01-15T23:56:06.585702Z",
     "shell.execute_reply": "2026-01-15T23:56:06.585253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import tqdm\n",
    "from functools import reduce\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a9e1b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.586981Z",
     "iopub.status.busy": "2026-01-15T23:56:06.586794Z",
     "iopub.status.idle": "2026-01-15T23:56:06.589763Z",
     "shell.execute_reply": "2026-01-15T23:56:06.589418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants defined.\n"
     ]
    }
   ],
   "source": [
    "# Constants from official template\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT A NAME\",\n",
    "    \"SOLVENT B NAME\",\n",
    "    \"SolventB%\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT NAME\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "]\n",
    "\n",
    "TARGET_LABELS = [\n",
    "    \"Product 2\",\n",
    "    \"Product 3\",\n",
    "    \"SM\",\n",
    "]\n",
    "\n",
    "print(\"Constants defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec79e440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.590896Z",
     "iopub.status.busy": "2026-01-15T23:56:06.590767Z",
     "iopub.status.idle": "2026-01-15T23:56:06.595695Z",
     "shell.execute_reply": "2026-01-15T23:56:06.595346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and CV functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "# CV functions from official template\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = ~((X[\"SOLVENT A NAME\"] == solvent_pair[\"SOLVENT A NAME\"]) & \n",
    "                           (X[\"SOLVENT B NAME\"] == solvent_pair[\"SOLVENT B NAME\"]))\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading and CV functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6858fc58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.596585Z",
     "iopub.status.busy": "2026-01-15T23:56:06.596488Z",
     "iopub.status.idle": "2026-01-15T23:56:06.599580Z",
     "shell.execute_reply": "2026-01-15T23:56:06.599219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes defined.\n"
     ]
    }
   ],
   "source": [
    "# Base classes from official template\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9701c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.600695Z",
     "iopub.status.busy": "2026-01-15T23:56:06.600592Z",
     "iopub.status.idle": "2026-01-15T23:56:06.606387Z",
     "shell.execute_reply": "2026-01-15T23:56:06.606011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature filtering functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Global cache for solvent feature table\n",
    "_SOLVENT_TABLE_CACHE = None\n",
    "\n",
    "def feature_priority(name: str) -> int:\n",
    "    \"\"\"Assign priority score to feature name. Higher = more important to keep.\"\"\"\n",
    "    if name.startswith(\"spange_\"):\n",
    "        return 5\n",
    "    if name.startswith(\"acs_\"):\n",
    "        return 4\n",
    "    if name.startswith(\"drfps_\"):\n",
    "        return 3\n",
    "    if name.startswith(\"frag_\"):\n",
    "        return 2\n",
    "    if name.startswith(\"smiles_\"):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df: pd.DataFrame, threshold: float = 0.8):\n",
    "    \"\"\"Drop columns that are highly correlated with any other column.\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    # Drop constant columns first\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    # Find all pairs with corr > threshold\n",
    "    high_corr_pairs = []\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                high_corr_pairs.append((col_i, col_j, cval))\n",
    "    \n",
    "    # For each pair, decide which to drop based on priority\n",
    "    for col_i, col_j, cval in high_corr_pairs:\n",
    "        if col_i in to_drop or col_j in to_drop:\n",
    "            continue\n",
    "        \n",
    "        p_i = feature_priority(col_i)\n",
    "        p_j = feature_priority(col_j)\n",
    "        \n",
    "        if p_i > p_j:\n",
    "            drop = col_j\n",
    "        elif p_j > p_i:\n",
    "            drop = col_i\n",
    "        else:\n",
    "            idx_i = df.columns.get_loc(col_i)\n",
    "            idx_j = df.columns.get_loc(col_j)\n",
    "            drop = col_i if idx_i > idx_j else col_j\n",
    "        \n",
    "        to_drop.add(drop)\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print(\"Feature filtering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b680e49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.607387Z",
     "iopub.status.busy": "2026-01-15T23:56:06.607291Z",
     "iopub.status.idle": "2026-01-15T23:56:06.610406Z",
     "shell.execute_reply": "2026-01-15T23:56:06.610074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature engineering defined.\n"
     ]
    }
   ],
   "source": [
    "def add_numeric_features(X_numeric: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add engineered numeric features.\"\"\"\n",
    "    X_num = X_numeric.copy()\n",
    "    cols = set(X_num.columns)\n",
    "    \n",
    "    if {\"Temperature\", \"Residence Time\"} <= cols:\n",
    "        # Convert Temperature to Kelvin\n",
    "        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n",
    "        \n",
    "        T = X_num[\"Temperature\"]\n",
    "        rt = X_num[\"Residence Time\"]\n",
    "        \n",
    "        # Interaction term\n",
    "        X_num[\"T_x_RT\"] = T * rt\n",
    "        \n",
    "        # Log transformation\n",
    "        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "        \n",
    "        # Inverse temperature\n",
    "        X_num[\"T_inv\"] = 1 / T\n",
    "        \n",
    "        # Scaled residence time\n",
    "        X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    \n",
    "    return X_num\n",
    "\n",
    "print(\"Numeric feature engineering defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1017acbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.611306Z",
     "iopub.status.busy": "2026-01-15T23:56:06.611208Z",
     "iopub.status.idle": "2026-01-15T23:56:06.616548Z",
     "shell.execute_reply": "2026-01-15T23:56:06.616203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solvent feature table builder defined.\n"
     ]
    }
   ],
   "source": [
    "def build_solvent_feature_table(threshold: float = 0.90):\n",
    "    \"\"\"Build combined solvent feature table from all sources.\"\"\"\n",
    "    global _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    if _SOLVENT_TABLE_CACHE is not None:\n",
    "        return _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    print(\">>> Building solvent feature table...\")\n",
    "    \n",
    "    sources = [\n",
    "        \"spange_descriptors\",\n",
    "        \"acs_pca_descriptors\",\n",
    "        \"drfps_catechol\",\n",
    "        \"fragprints\",\n",
    "        # Skip \"smiles\" - it's a string column that can't be used directly\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for src in sources:\n",
    "        df_src = load_features(src).copy()\n",
    "        \n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        \n",
    "        # Bit-table filtering for binary fingerprints\n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            \n",
    "            # Drop all-zero and all-one columns\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            \n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"})\n",
    "            count = values.sum(axis=0).T\n",
    "            drop_cols = count[count == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols)\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        else:\n",
    "            if src == \"spange_descriptors\":\n",
    "                prefix = \"spange\"\n",
    "            elif src == \"acs_pca_descriptors\":\n",
    "                prefix = \"acs\"\n",
    "            else:\n",
    "                prefix = src\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        dfs.append(df_src)\n",
    "        print(f\"  {src}: {df_src.shape[1] - 1} features\")\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"),\n",
    "        dfs\n",
    "    )\n",
    "    \n",
    "    print(f\">>> Merged shape before filtering: {merged.shape}\")\n",
    "    \n",
    "    # Apply correlation filtering\n",
    "    merged_filtered, dropped = filter_correlated_features(merged, threshold=threshold)\n",
    "    \n",
    "    print(f\">>> Final shape after filtering: {merged_filtered.shape}\")\n",
    "    print(f\">>> Dropped {len(dropped)} features\")\n",
    "    \n",
    "    _SOLVENT_TABLE_CACHE = merged_filtered\n",
    "    return merged_filtered\n",
    "\n",
    "print(\"Solvent feature table builder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053b2857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:06.617395Z",
     "iopub.status.busy": "2026-01-15T23:56:06.617297Z",
     "iopub.status.idle": "2026-01-15T23:56:06.730502Z",
     "shell.execute_reply": "2026-01-15T23:56:06.730079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Building solvent feature table...\n",
      "  spange_descriptors: 13 features\n",
      "  acs_pca_descriptors: 5 features\n",
      "  drfps_catechol: 40 features\n",
      "  fragprints: 55 features\n",
      "  smiles: 1 features\n",
      ">>> Merged shape before filtering: (26, 115)\n",
      ">>> Final shape after filtering: (26, 68)\n",
      ">>> Dropped 47 features\n",
      "\n",
      "Final solvent table shape: (26, 68)\n",
      "Columns: ['SOLVENT NAME', 'spange_dielectric constant', 'spange_ET(30)', 'spange_beta', 'spange_pi*', 'spange_SB', 'spange_SP', 'spange_SdP', 'spange_N', 'spange_n', 'acs_PC1', 'acs_PC2', 'acs_PC3', 'acs_PC4', 'acs_PC5', 'drfps_34', 'drfps_67', 'drfps_110', 'drfps_125', 'drfps_209']...\n"
     ]
    }
   ],
   "source": [
    "# Build the feature table once\n",
    "solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "print(f\"\\nFinal solvent table shape: {solvent_table.shape}\")\n",
    "print(f\"Columns: {solvent_table.columns.tolist()[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d11150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    \"\"\"Featurizer for single solvent data using combined features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table()\n",
    "        self.solvent_table = self.solvent_table.set_index(\"SOLVENT NAME\")\n",
    "        self.feats_dim = self.solvent_table.shape[1] + 6  # +6 for numeric features\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features with engineering\n",
    "        X_num = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n",
    "        \n",
    "        # Solvent features\n",
    "        X_sol = self.solvent_table.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        X_num_tensor = torch.tensor(X_num.values, dtype=torch.double)\n",
    "        X_sol_tensor = torch.tensor(X_sol, dtype=torch.double)\n",
    "        \n",
    "        return torch.cat([X_num_tensor, X_sol_tensor], dim=1)\n",
    "\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    \"\"\"Featurizer for mixed solvent data using combined features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table()\n",
    "        self.solvent_table = self.solvent_table.set_index(\"SOLVENT NAME\")\n",
    "        self.feats_dim = self.solvent_table.shape[1] + 7  # +7 for numeric features + SolventB%\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features with engineering\n",
    "        X_num = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n",
    "        \n",
    "        # Solvent features (weighted average)\n",
    "        A = self.solvent_table.loc[X[\"SOLVENT A NAME\"]].values\n",
    "        B = self.solvent_table.loc[X[\"SOLVENT B NAME\"]].values\n",
    "        frac_b = X[\"SolventB%\"].values.reshape(-1, 1) / 100.0\n",
    "        \n",
    "        mix = A * (1 - frac_b) + B * frac_b\n",
    "        \n",
    "        X_num_tensor = torch.tensor(X_num.values, dtype=torch.double)\n",
    "        X_frac_tensor = torch.tensor(frac_b, dtype=torch.double)\n",
    "        X_mix_tensor = torch.tensor(mix, dtype=torch.double)\n",
    "        \n",
    "        return torch.cat([X_num_tensor, X_frac_tensor, X_mix_tensor], dim=1)\n",
    "\n",
    "print(\"Featurizers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model\n",
    "class CatBoostModel(BaseModel):\n",
    "    \"\"\"CatBoost regressor with optimized hyperparameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\", random_state: int = 42):\n",
    "        self.data_mode = data\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.cat_params = dict(\n",
    "                iterations=700,\n",
    "                depth=6,\n",
    "                learning_rate=0.05,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=random_state,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.cat_params = dict(\n",
    "                iterations=500,\n",
    "                depth=5,\n",
    "                learning_rate=0.03,\n",
    "                l2_leaf_reg=5.0,\n",
    "                random_seed=random_state,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = CatBoostRegressor(**self.cat_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # Clip to [0, 1] - DO NOT normalize to sum to 1\n",
    "        out = np.clip(out, 0.0, 1.0)\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"CatBoostModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4dfb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "class XGBModel(BaseModel):\n",
    "    \"\"\"XGBoost regressor with optimized hyperparameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\", random_state: int = 42):\n",
    "        self.data_mode = data\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.xgb_params = dict(\n",
    "                n_estimators=600,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=random_state,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.xgb_params = dict(\n",
    "                n_estimators=400,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=0.2,\n",
    "                reg_lambda=2.0,\n",
    "                random_state=random_state,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # Clip to [0, 1] - DO NOT normalize to sum to 1\n",
    "        out = np.clip(out, 0.0, 1.0)\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"XGBModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b24ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model\n",
    "class EnsembleModel(BaseModel):\n",
    "    \"\"\"Weighted ensemble of CatBoost and XGBoost.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\"):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        # Optimized weights from kernel\n",
    "        if data == \"single\":\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        # Normalize weights\n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        # Initialize base models\n",
    "        self.cat_model = CatBoostModel(data=data)\n",
    "        self.xgb_model = XGBModel(data=data)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        self.cat_model.train_model(train_X, train_Y)\n",
    "        self.xgb_model.train_model(train_X, train_Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        \n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        # Final clip to [0, 1]\n",
    "        out = torch.clamp(out, 0.0, 1.0)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"EnsembleModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d49704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "print(\"Testing EnsembleModel...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "model = EnsembleModel()\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions sample: {preds[0]}\")\n",
    "print(f\"Actual sample: {test_Y.iloc[0].values}\")\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Single solvent predictions: {len(submission_single_solvent)}\")\n",
    "print(f\"Unique folds: {submission_single_solvent['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Full data predictions: {len(submission_full_data)}\")\n",
    "print(f\"Unique folds: {submission_full_data['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV for logging\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CV CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single solvent CV\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X, Y)):\n",
    "    model = EnsembleModel()\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    fold_mses.append(mse)\n",
    "    if fold_idx % 5 == 0:\n",
    "        print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "single_cv = np.mean(fold_mses)\n",
    "single_std = np.std(fold_mses)\n",
    "print(f\"\\nSingle solvent CV MSE: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "\n",
    "# Full data CV\n",
    "X, Y = load_data(\"full\")\n",
    "full_fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_ramp_out_splits(X, Y)):\n",
    "    model = EnsembleModel(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    full_fold_mses.append(mse)\n",
    "    print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "full_cv = np.mean(full_fold_mses)\n",
    "full_std = np.std(full_fold_mses)\n",
    "print(f\"\\nFull data CV MSE: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "print(f\"\\nFINAL CV FOR LOGGING: {single_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv('/home/submission/submission.csv')\n",
    "\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "print(f\"\\nTask 0 (single solvent):\")\n",
    "task0 = df[df['task'] == 0]\n",
    "print(f\"  Rows: {len(task0)}\")\n",
    "print(f\"  Folds: {task0['fold'].nunique()}\")\n",
    "print(f\"  Fold values: {sorted(task0['fold'].unique())}\")\n",
    "\n",
    "print(f\"\\nTask 1 (full data):\")\n",
    "task1 = df[df['task'] == 1]\n",
    "print(f\"  Rows: {len(task1)}\")\n",
    "print(f\"  Folds: {task1['fold'].nunique()}\")\n",
    "print(f\"  Fold values: {sorted(task1['fold'].unique())}\")\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f\"  {col}: min={df[col].min():.6f}, max={df[col].max():.6f}, mean={df[col].mean():.6f}\")\n",
    "    print(f\"    Values > 1: {(df[col] > 1).sum()}, Values < 0: {(df[col] < 0).sum()}, NaN: {df[col].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a405e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 057: ENS-MODEL ALL FEATURES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAPPROACH (from matthewmaree 'ens-model' kernel):\")\n",
    "print(\"  - Combined ALL 5 feature sources (spange, acs_pca, drfps, fragprints, smiles)\")\n",
    "print(\"  - Correlation-based filtering with priority (spange > acs > drfps > frag > smiles)\")\n",
    "print(\"  - Numeric feature engineering (T_x_RT, RT_log, T_inv, RT_scaled)\")\n",
    "print(\"  - CatBoost + XGBoost ensemble (7:6 for single, 1:2 for full)\")\n",
    "print(\"  - Clip to [0, 1] WITHOUT normalizing to sum to 1\")\n",
    "\n",
    "print(f\"\\nCV SCORES:\")\n",
    "print(f\"  Single solvent: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "print(f\"  Full data: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "print(f\"\\nSUBMISSION FORMAT:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Task 0: {len(task0)} rows, {task0['fold'].nunique()} folds\")\n",
    "print(f\"  Task 1: {len(task1)} rows, {task1['fold'].nunique()} folds\")\n",
    "print(f\"  All targets in [0, 1]: {(df['target_1'].between(0, 1)).all() and (df['target_2'].between(0, 1)).all() and (df['target_3'].between(0, 1)).all()}\")\n",
    "\n",
    "print(\"\\nHYPOTHESIS:\")\n",
    "print(\"  Combining ALL feature sources may provide better generalization to unseen solvents.\")\n",
    "print(\"  This could potentially CHANGE the CV-LB relationship (reduce the intercept).\")\n",
    "\n",
    "# Predicted LB using the CV-LB relationship\n",
    "predicted_lb = 4.31 * single_cv + 0.0525\n",
    "print(f\"\\nPREDICTED LB (using CV-LB relationship): {predicted_lb:.4f}\")\n",
    "print(f\"  Best LB so far: 0.0877\")\n",
    "print(f\"  Target: 0.0347\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
