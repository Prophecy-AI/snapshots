{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bd0fdc",
   "metadata": {},
   "source": [
    "# Experiment 079: Best-Work-Here Kernel Techniques (Rule-Compliant)\n",
    "\n",
    "**Rationale**: Implement the key techniques from the best-work-here kernel in a rule-compliant way:\n",
    "1. Non-linear mixture features: `A*(1-r) + B*r + 0.05*A*B*r*(1-r)`\n",
    "2. Squeeze-and-Excitation blocks for feature recalibration\n",
    "3. Residual blocks with LayerNorm and GELU\n",
    "4. Adaptive ensemble: CatBoost + XGBoost + LightGBM + Neural Network\n",
    "5. Advanced feature engineering (polynomial, interaction, statistical features)\n",
    "\n",
    "**Rule Compliance**: The model class is self-contained and only requires changing the model definition line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd5f985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:21.586549Z",
     "iopub.status.busy": "2026-01-16T11:19:21.585997Z",
     "iopub.status.idle": "2026-01-16T11:19:23.386018Z",
     "shell.execute_reply": "2026-01-16T11:19:23.385556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/data/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Imports done')\n",
    "print(f'GPU available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd59103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:23.387417Z",
     "iopub.status.busy": "2026-01-16T11:19:23.387254Z",
     "iopub.status.idle": "2026-01-16T11:19:23.391061Z",
     "shell.execute_reply": "2026-01-16T11:19:23.390700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data functions defined\n"
     ]
    }
   ],
   "source": [
    "# Local data loading functions\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'drfps':\n",
    "        return pd.read_csv('/home/data/drfps_catechol_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'fragprints':\n",
    "        return pd.read_csv('/home/data/fragprints_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'acs_pca_descriptors':\n",
    "        return pd.read_csv('/home/data/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912d37b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:23.392033Z",
     "iopub.status.busy": "2026-01-16T11:19:23.391938Z",
     "iopub.status.idle": "2026-01-16T11:19:23.396460Z",
     "shell.execute_reply": "2026-01-16T11:19:23.396099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV split functions defined\n"
     ]
    }
   ],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate leave-one-out splits across the solvents.\"\"\"\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate leave-one-ramp-out splits across the solvent ramps.\"\"\"\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print('CV split functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4befb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:23.397352Z",
     "iopub.status.busy": "2026-01-16T11:19:23.397246Z",
     "iopub.status.idle": "2026-01-16T11:19:23.403548Z",
     "shell.execute_reply": "2026-01-16T11:19:23.403196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network architecture defined\n"
     ]
    }
   ],
   "source": [
    "# Squeeze-and-Excitation Block\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for feature recalibration\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, max(channels // reduction, 4), bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(channels // reduction, 4), channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.fc(x)\n",
    "\n",
    "# Residual Block with SE attention\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Enhanced residual block with SE attention\"\"\"\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        self.se = SEBlock(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = self.se(out)\n",
    "        out = self.activation(residual + out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "# Advanced Neural Network\n",
    "class AdvancedNN(nn.Module):\n",
    "    \"\"\"Neural network with SE blocks and residual connections\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], output_dim=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        # Input projection\n",
    "        layers.extend([\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.LayerNorm(hidden_dims[0]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "        \n",
    "        # Hidden layers with residual blocks\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                nn.LayerNorm(hidden_dims[i+1]),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                ResidualBlock(hidden_dims[i+1], dropout)\n",
    "            ])\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output head with sigmoid for [0,1] range\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(hidden_dims[-1] // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.output(x)\n",
    "\n",
    "print('Neural network architecture defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6996bc33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:23.404635Z",
     "iopub.status.busy": "2026-01-16T11:19:23.404531Z",
     "iopub.status.idle": "2026-01-16T11:19:23.410547Z",
     "shell.execute_reply": "2026-01-16T11:19:23.410176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced featurizer defined\n"
     ]
    }
   ],
   "source": [
    "# Advanced Featurizer with non-linear mixture features\n",
    "class AdvancedFeaturizer:\n",
    "    \"\"\"Featurizer with non-linear mixture features and advanced engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, features='spange_descriptors', mixed=False):\n",
    "        self.features_df = load_features(features)\n",
    "        self.mixed = mixed\n",
    "        self._cache = {}\n",
    "        \n",
    "    def _get_molecular(self, row):\n",
    "        \"\"\"Extract molecular features with caching\"\"\"\n",
    "        if not self.mixed:\n",
    "            key = row[\"SOLVENT NAME\"]\n",
    "            if key not in self._cache:\n",
    "                self._cache[key] = self.features_df.loc[key].values\n",
    "            return self._cache[key]\n",
    "        else:\n",
    "            A_name = row[\"SOLVENT A NAME\"]\n",
    "            B_name = row[\"SOLVENT B NAME\"]\n",
    "            r = row[\"SolventB%\"] / 100.0  # Normalize to [0,1]\n",
    "            \n",
    "            if A_name not in self._cache:\n",
    "                self._cache[A_name] = self.features_df.loc[A_name].values\n",
    "            if B_name not in self._cache:\n",
    "                self._cache[B_name] = self.features_df.loc[B_name].values\n",
    "            \n",
    "            A, B = self._cache[A_name], self._cache[B_name]\n",
    "            # NON-LINEAR MIXING: Key technique from best-work-here kernel\n",
    "            return A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)\n",
    "    \n",
    "    def _create_advanced_features(self, numeric_feat, mol_feat):\n",
    "        \"\"\"Engineer advanced features\"\"\"\n",
    "        features = [numeric_feat, mol_feat]\n",
    "        \n",
    "        # Polynomial features\n",
    "        if numeric_feat.shape[1] > 0:\n",
    "            features.append(numeric_feat ** 2)\n",
    "            features.append(np.sqrt(np.abs(numeric_feat) + 1e-8))\n",
    "        \n",
    "        # Interaction terms\n",
    "        if numeric_feat.shape[1] >= 2:\n",
    "            features.append((numeric_feat[:, 0] * numeric_feat[:, 1]).reshape(-1, 1))\n",
    "        \n",
    "        # Statistical features from molecular descriptors\n",
    "        mol_stats = np.column_stack([\n",
    "            mol_feat.mean(axis=1),\n",
    "            mol_feat.std(axis=1),\n",
    "            mol_feat.max(axis=1),\n",
    "            mol_feat.min(axis=1)\n",
    "        ])\n",
    "        features.append(mol_stats)\n",
    "        \n",
    "        return np.concatenate(features, axis=1)\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        \"\"\"Convert DataFrame to feature matrix\"\"\"\n",
    "        if self.mixed:\n",
    "            numeric_cols = ['Residence Time', 'Temperature', 'SolventB%']\n",
    "        else:\n",
    "            numeric_cols = ['Residence Time', 'Temperature']\n",
    "        \n",
    "        numeric = X[numeric_cols].values.astype(np.float32)\n",
    "        mol = np.vstack([self._get_molecular(X.iloc[i]) for i in range(len(X))]).astype(np.float32)\n",
    "        \n",
    "        # Advanced feature engineering\n",
    "        combined = self._create_advanced_features(numeric, mol)\n",
    "        combined = np.nan_to_num(combined, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        return combined.astype(np.float32)\n",
    "\n",
    "print('Advanced featurizer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262faea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:19:23.411513Z",
     "iopub.status.busy": "2026-01-16T11:19:23.411411Z",
     "iopub.status.idle": "2026-01-16T11:19:23.420940Z",
     "shell.execute_reply": "2026-01-16T11:19:23.420548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestWorkHereModel defined\n"
     ]
    }
   ],
   "source": [
    "# Best-Work-Here Model: Adaptive Ensemble\n",
    "class BestWorkHereModel:\n",
    "    \"\"\"Adaptive ensemble with CatBoost, XGBoost, LightGBM, and Neural Network\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = AdvancedFeaturizer('spange_descriptors', mixed=self.mixed)\n",
    "        self.scaler = RobustScaler(quantile_range=(3, 97))\n",
    "        \n",
    "        # Will be set after featurization\n",
    "        self.input_dim = None\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Model weights (will be computed adaptively)\n",
    "        self.weights = [0.25, 0.25, 0.25, 0.25]  # [xgb, lgb, rf, nn]\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=150, lr=1e-3, batch_size=32):\n",
    "        # Featurize\n",
    "        X_np = self.featurizer.featurize(train_X)\n",
    "        y_np = train_Y.values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        self.input_dim = X_scaled.shape[1]\n",
    "        \n",
    "        # Split for validation-based weight computation\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_scaled, y_np, test_size=0.15, random_state=SEED\n",
    "        )\n",
    "        \n",
    "        # Train XGBoost\n",
    "        self.xgb = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "        self.xgb.fit(X_tr, y_tr)\n",
    "        xgb_val_pred = np.clip(self.xgb.predict(X_val), 0, 1)\n",
    "        xgb_mse = np.mean((xgb_val_pred - y_val) ** 2)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgb = MultiOutputRegressor(lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.05,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ))\n",
    "        self.lgb.fit(X_tr, y_tr)\n",
    "        lgb_val_pred = np.clip(self.lgb.predict(X_val), 0, 1)\n",
    "        lgb_mse = np.mean((lgb_val_pred - y_val) ** 2)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        self.rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.rf.fit(X_tr, y_tr)\n",
    "        rf_val_pred = np.clip(self.rf.predict(X_val), 0, 1)\n",
    "        rf_mse = np.mean((rf_val_pred - y_val) ** 2)\n",
    "        \n",
    "        # Train Neural Network\n",
    "        self.nn = AdvancedNN(self.input_dim, self.hidden_dims, 3, self.dropout)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.nn.to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_tr, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_tr, dtype=torch.float32)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.nn.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor, y_tensor),\n",
    "            batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.nn.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(self.nn(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Get NN validation predictions\n",
    "        self.nn.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            nn_val_pred = self.nn(X_val_tensor).cpu().numpy()\n",
    "        nn_mse = np.mean((nn_val_pred - y_val) ** 2)\n",
    "        \n",
    "        # Compute adaptive weights (inverse MSE weighting)\n",
    "        mses = np.array([xgb_mse, lgb_mse, rf_mse, nn_mse])\n",
    "        inv_mses = 1.0 / (mses + 1e-8)\n",
    "        self.weights = inv_mses / inv_mses.sum()\n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        X_np = self.featurizer.featurize(test_X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        xgb_pred = np.clip(self.xgb.predict(X_scaled), 0, 1)\n",
    "        lgb_pred = np.clip(self.lgb.predict(X_scaled), 0, 1)\n",
    "        rf_pred = np.clip(self.rf.predict(X_scaled), 0, 1)\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.nn.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            nn_pred = self.nn(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_pred = (\n",
    "            self.weights[0] * xgb_pred +\n",
    "            self.weights[1] * lgb_pred +\n",
    "            self.weights[2] * rf_pred +\n",
    "            self.weights[3] * nn_pred\n",
    "        )\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        final_pred = np.clip(final_pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_pred)\n",
    "\n",
    "print('BestWorkHereModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = BestWorkHereModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = BestWorkHereModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "print(f\"Expected: 656 (single) + 1227 (full) = 1883\")\n",
    "\n",
    "# Check prediction ranges\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "single_mses = [0.011459, 0.009993, 0.006660, 0.016773, 0.016585]  # From exp_078 for comparison\n",
    "# Use actual fold_mses from this run\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 079 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nKey techniques implemented:\")\n",
    "print(\"1. Non-linear mixture features: A*(1-r) + B*r + 0.05*A*B*r*(1-r)\")\n",
    "print(\"2. Squeeze-and-Excitation blocks for feature recalibration\")\n",
    "print(\"3. Residual blocks with LayerNorm and GELU\")\n",
    "print(\"4. Adaptive ensemble: XGBoost + LightGBM + RF + Neural Network\")\n",
    "print(\"5. Advanced feature engineering (polynomial, interaction, statistical)\")\n",
    "print(\"\\nThis is a RULE-COMPLIANT implementation that only changes the model definition.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
