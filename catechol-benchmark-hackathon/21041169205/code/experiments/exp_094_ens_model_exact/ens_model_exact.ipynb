{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0e9c5b",
   "metadata": {},
   "source": [
    "# Experiment 094: Exact ens-model Kernel Replication\n",
    "\n",
    "**Goal**: Replicate the ens-model kernel exactly with:\n",
    "1. Correlation filtering with threshold=0.90\n",
    "2. Feature priority: spange > acs > drfps > frag > smiles\n",
    "3. Different ensemble weights: Single (CatBoost=7/13, XGB=6/13), Full (CatBoost=1/3, XGB=2/3)\n",
    "4. Numeric features: T_x_RT, RT_log, T_inv, RT_scaled\n",
    "5. CatBoost + XGBoost ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b3b0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:23:37.984784Z",
     "iopub.status.busy": "2026-01-16T17:23:37.984168Z",
     "iopub.status.idle": "2026-01-16T17:23:39.523469Z",
     "shell.execute_reply": "2026-01-16T17:23:39.522990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print('Imports complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bd2eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:23:39.524904Z",
     "iopub.status.busy": "2026-01-16T17:23:39.524675Z",
     "iopub.status.idle": "2026-01-16T17:23:39.529878Z",
     "shell.execute_reply": "2026-01-16T17:23:39.529435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988ea921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:23:39.531466Z",
     "iopub.status.busy": "2026-01-16T17:23:39.531131Z",
     "iopub.status.idle": "2026-01-16T17:23:39.560803Z",
     "shell.execute_reply": "2026-01-16T17:23:39.560339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP: (24, 2048), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP: {DRFP_DF.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc0241f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:23:39.562216Z",
     "iopub.status.busy": "2026-01-16T17:23:39.562088Z",
     "iopub.status.idle": "2026-01-16T17:23:39.568108Z",
     "shell.execute_reply": "2026-01-16T17:23:39.567463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature priority function (from ens-model kernel)\n",
    "def feature_priority(name):\n",
    "    \"\"\"Higher number = more important to keep during correlation filtering.\"\"\"\n",
    "    if name.startswith(\"spange_\"):\n",
    "        return 5\n",
    "    if name.startswith(\"acs_\"):\n",
    "        return 4\n",
    "    if name.startswith(\"drfps_\"):\n",
    "        return 3\n",
    "    if name.startswith(\"frag_\"):\n",
    "        return 2\n",
    "    if name.startswith(\"smiles_\"):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.90):\n",
    "    \"\"\"Drop columns that are highly correlated with any other column.\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    # Drop constant columns first\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    # Find all pairs with corr > threshold\n",
    "    high_corr_pairs = []\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                high_corr_pairs.append((col_i, col_j, cval))\n",
    "    \n",
    "    # For each pair, decide which column to drop\n",
    "    for col_i, col_j, cval in high_corr_pairs:\n",
    "        if col_i in to_drop or col_j in to_drop:\n",
    "            continue\n",
    "        \n",
    "        p_i = feature_priority(col_i)\n",
    "        p_j = feature_priority(col_j)\n",
    "        \n",
    "        if p_i > p_j:\n",
    "            drop = col_j\n",
    "        elif p_j > p_i:\n",
    "            drop = col_i\n",
    "        else:\n",
    "            # Same priority; drop the one that appears later\n",
    "            idx_i = df.columns.get_loc(col_i)\n",
    "            idx_j = df.columns.get_loc(col_j)\n",
    "            drop = col_i if idx_i > idx_j else col_j\n",
    "        \n",
    "        to_drop.add(drop)\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print('Correlation filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89a6442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:23:39.569331Z",
     "iopub.status.busy": "2026-01-16T17:23:39.569212Z",
     "iopub.status.idle": "2026-01-16T17:23:39.679852Z",
     "shell.execute_reply": "2026-01-16T17:23:39.679398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 140, After filtering: 56\n",
      "Dropped 84 features\n",
      "Final solvent table shape: (26, 56)\n"
     ]
    }
   ],
   "source": [
    "# Build solvent feature table with correlation filtering\n",
    "def build_solvent_feature_table(threshold=0.90):\n",
    "    \"\"\"Build combined solvent feature table with correlation filtering.\"\"\"\n",
    "    \n",
    "    # Spange descriptors\n",
    "    spange_df = SPANGE_DF.copy()\n",
    "    spange_df = spange_df.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "    spange_cols = [c for c in spange_df.columns if c != \"SOLVENT NAME\"]\n",
    "    spange_df = spange_df.rename(columns={c: f\"spange_{c}\" for c in spange_cols})\n",
    "    \n",
    "    # ACS PCA descriptors\n",
    "    acs_df = ACS_PCA_DF.copy()\n",
    "    acs_df = acs_df.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "    acs_cols = [c for c in acs_df.columns if c != \"SOLVENT NAME\"]\n",
    "    acs_df = acs_df.rename(columns={c: f\"acs_{c}\" for c in acs_cols})\n",
    "    \n",
    "    # DRFP descriptors (filter zero-variance)\n",
    "    drfp_df = DRFP_DF.copy()\n",
    "    drfp_df = drfp_df.loc[:, (drfp_df != 0).any(axis=0)]  # Drop all-zero\n",
    "    drfp_df = drfp_df.loc[:, (drfp_df != 1).any(axis=0)]  # Drop all-one\n",
    "    drfp_df = drfp_df.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "    drfp_cols = [c for c in drfp_df.columns if c != \"SOLVENT NAME\"]\n",
    "    drfp_df = drfp_df.rename(columns={c: f\"drfps_{c}\" for c in drfp_cols})\n",
    "    \n",
    "    # Merge all\n",
    "    combined = spange_df.merge(acs_df, on=\"SOLVENT NAME\", how=\"outer\")\n",
    "    combined = combined.merge(drfp_df, on=\"SOLVENT NAME\", how=\"outer\")\n",
    "    combined = combined.set_index(\"SOLVENT NAME\")\n",
    "    \n",
    "    # Apply correlation filtering\n",
    "    combined_filtered, dropped = filter_correlated_features(combined, threshold=threshold)\n",
    "    \n",
    "    print(f\"Original features: {combined.shape[1]}, After filtering: {combined_filtered.shape[1]}\")\n",
    "    print(f\"Dropped {len(dropped)} features\")\n",
    "    \n",
    "    return combined_filtered\n",
    "\n",
    "# Build the table\n",
    "SOLVENT_TABLE = build_solvent_feature_table(threshold=0.90)\n",
    "print(f\"Final solvent table shape: {SOLVENT_TABLE.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f38e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric feature engineering (from ens-model kernel)\n",
    "def add_numeric_features(X_numeric):\n",
    "    \"\"\"Add engineered numeric features.\"\"\"\n",
    "    X_num = X_numeric.copy()\n",
    "    \n",
    "    # Convert Temperature to Kelvin\n",
    "    T = X_num[\"Temperature\"] + 273.15\n",
    "    rt = X_num[\"Residence Time\"]\n",
    "    \n",
    "    # Interaction term\n",
    "    X_num[\"T_x_RT\"] = T * rt\n",
    "    \n",
    "    # Log transformation\n",
    "    X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "    \n",
    "    # Inverse temperature\n",
    "    X_num[\"T_inv\"] = 1 / T\n",
    "    \n",
    "    # Scaled residence time\n",
    "    X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    \n",
    "    return X_num\n",
    "\n",
    "print('Numeric feature engineering defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer class (from ens-model kernel)\n",
    "class PrecomputedFeaturizer:\n",
    "    \"\"\"Featurizer for single solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = SOLVENT_TABLE\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        # Solvent features\n",
    "        solvent_feats = self.solvent_table.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        # Combine\n",
    "        combined = np.hstack([X_numeric.values, solvent_feats])\n",
    "        \n",
    "        # Scale\n",
    "        if not self.fitted:\n",
    "            combined = self.scaler.fit_transform(combined)\n",
    "            self.fitted = True\n",
    "        else:\n",
    "            combined = self.scaler.transform(combined)\n",
    "        \n",
    "        return torch.tensor(combined, dtype=torch.double)\n",
    "\n",
    "class PrecomputedFeaturizerMixed:\n",
    "    \"\"\"Featurizer for mixed solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = SOLVENT_TABLE\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        # Solvent A features\n",
    "        A_feats = self.solvent_table.loc[X[\"SOLVENT A NAME\"]].values\n",
    "        \n",
    "        # Solvent B features\n",
    "        B_feats = self.solvent_table.loc[X[\"SOLVENT B NAME\"]].values\n",
    "        \n",
    "        # Linear mixing\n",
    "        r = X[\"SolventB%\"].values.reshape(-1, 1)  # Already in [0, 1]\n",
    "        mixed_feats = A_feats * (1 - r) + B_feats * r\n",
    "        \n",
    "        # Combine\n",
    "        combined = np.hstack([X_numeric.values, mixed_feats])\n",
    "        \n",
    "        # Scale\n",
    "        if not self.fitted:\n",
    "            combined = self.scaler.fit_transform(combined)\n",
    "            self.fitted = True\n",
    "        else:\n",
    "            combined = self.scaler.transform(combined)\n",
    "        \n",
    "        return torch.tensor(combined, dtype=torch.double)\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model (from ens-model kernel)\n",
    "class CatBoostModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.featurizer = PrecomputedFeaturizer()\n",
    "            self.cat_params = dict(\n",
    "                iterations=700,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.cat_params = dict(\n",
    "                iterations=500,\n",
    "                learning_rate=0.03,\n",
    "                depth=5,\n",
    "                l2_leaf_reg=5.0,\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_tensor = self.featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(Y_np.shape[1]):\n",
    "            m = CatBoostRegressor(**self.cat_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds)\n",
    "        out = np.clip(out, 0.0, None)\n",
    "        \n",
    "        # Renormalize if sum > 1\n",
    "        totals = out.sum(axis=1, keepdims=True)\n",
    "        divisor = np.maximum(totals, 1.0)\n",
    "        out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('CatBoostModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model (from ens-model kernel)\n",
    "class XGBModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.featurizer = PrecomputedFeaturizer()\n",
    "            self.xgb_params = dict(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        else:\n",
    "            self.featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.xgb_params = dict(\n",
    "                n_estimators=400,\n",
    "                learning_rate=0.03,\n",
    "                max_depth=4,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=0.2,\n",
    "                reg_lambda=2.0,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_tensor = self.featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(Y_np.shape[1]):\n",
    "            m = xgb.XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds)\n",
    "        out = np.clip(out, 0.0, None)\n",
    "        \n",
    "        # Renormalize if sum > 1\n",
    "        totals = out.sum(axis=1, keepdims=True)\n",
    "        divisor = np.maximum(totals, 1.0)\n",
    "        out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('XGBModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model (from ens-model kernel)\n",
    "class EnsembleModel:\n",
    "    \"\"\"Weighted ensemble of CatBoost and XGBoost.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        # Optimized fixed weights per dataset (from ens-model kernel)\n",
    "        if data == 'single':\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            # Full dataset\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        # Normalize ensemble weights\n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        # Initialize base models\n",
    "        self.cat_model = CatBoostModel(data=data)\n",
    "        self.xgb_model = XGBModel(data=data)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        self.cat_model.train_model(train_X, train_Y)\n",
    "        self.xgb_model.train_model(train_X, train_Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        \n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        return out\n",
    "\n",
    "print('EnsembleModel defined')\n",
    "print(f'Single weights: CatBoost={7/13:.3f}, XGB={6/13:.3f}')\n",
    "print(f'Full weights: CatBoost={1/3:.3f}, XGB={2/3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV score (for verification only - NOT part of submission)\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "# Get actuals in same order as predictions\n",
    "actuals_single = []\n",
    "for solvent in sorted(X_single[\"SOLVENT NAME\"].unique()):\n",
    "    mask = X_single[\"SOLVENT NAME\"] == solvent\n",
    "    actuals_single.append(Y_single[mask].values)\n",
    "actuals_single = np.vstack(actuals_single)\n",
    "\n",
    "actuals_full = []\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "for _, row in ramps.iterrows():\n",
    "    mask = (X_full[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X_full[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"])\n",
    "    actuals_full.append(Y_full[mask].values)\n",
    "actuals_full = np.vstack(actuals_full)\n",
    "\n",
    "# Get predictions\n",
    "preds_single = submission_single_solvent[['target_1', 'target_2', 'target_3']].values\n",
    "preds_full = submission_full_data[['target_1', 'target_2', 'target_3']].values\n",
    "\n",
    "# Calculate MSE\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest previous CV: 0.008092 (CatBoost+XGBoost)')\n",
    "print(f'Best previous LB: 0.0877 (GP+MLP+LGBM)')\n",
    "print(f'exp_030 baseline (GP+MLP+LGBM): CV 0.008298')\n",
    "print(f'exp_090 (ens-model attempt): CV 0.010878')\n",
    "print(f'\\nThis (Exact ens-model replication): CV {overall_mse:.6f}')\n",
    "\n",
    "if overall_mse < 0.008092:\n",
    "    improvement = (0.008092 - overall_mse) / 0.008092 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "elif overall_mse < 0.008298:\n",
    "    improvement = (0.008298 - overall_mse) / 0.008298 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than exp_030!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008298) / 0.008298 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than exp_030')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
