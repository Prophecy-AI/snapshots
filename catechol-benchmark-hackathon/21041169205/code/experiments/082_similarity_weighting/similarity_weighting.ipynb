{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28b0d5a",
   "metadata": {},
   "source": [
    "# Experiment 082: Solvent Similarity Weighting (Continuous)\n",
    "\n",
    "**Rationale**: Instead of discrete clustering (which failed in exp_081), use CONTINUOUS similarity weighting:\n",
    "1. For each test solvent, compute similarity to ALL training solvents using Spange descriptors\n",
    "2. When test solvent is dissimilar to all training solvents, blend predictions toward population mean\n",
    "3. This addresses extrapolation by being conservative for dissimilar solvents\n",
    "\n",
    "**Key difference from exp_081**: Uses continuous similarity weights, not discrete clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0d64e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:57:47.907050Z",
     "iopub.status.busy": "2026-01-16T11:57:47.906491Z",
     "iopub.status.idle": "2026-01-16T11:57:49.258113Z",
     "shell.execute_reply": "2026-01-16T11:57:49.257649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4762da7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:57:49.259304Z",
     "iopub.status.busy": "2026-01-16T11:57:49.259138Z",
     "iopub.status.idle": "2026-01-16T11:57:49.262498Z",
     "shell.execute_reply": "2026-01-16T11:57:49.262142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data functions defined\n"
     ]
    }
   ],
   "source": [
    "# Local data loading functions\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0020cd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:57:49.263378Z",
     "iopub.status.busy": "2026-01-16T11:57:49.263283Z",
     "iopub.status.idle": "2026-01-16T11:57:49.267265Z",
     "shell.execute_reply": "2026-01-16T11:57:49.266933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV split functions defined\n"
     ]
    }
   ],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print('CV split functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125d8cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:57:49.268078Z",
     "iopub.status.busy": "2026-01-16T11:57:49.267987Z",
     "iopub.status.idle": "2026-01-16T11:57:49.276239Z",
     "shell.execute_reply": "2026-01-16T11:57:49.275895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilarityWeightedModel defined\n"
     ]
    }
   ],
   "source": [
    "# Similarity-weighted model\n",
    "class SimilarityWeightedModel:\n",
    "    \"\"\"Model that uses continuous similarity weighting for conservative predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', blend_strength=0.3):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.blend_strength = blend_strength  # How much to blend toward mean for dissimilar solvents\n",
    "        \n",
    "        # Load Spange descriptors\n",
    "        self.spange = load_features('spange_descriptors')\n",
    "        self.scaler_spange = StandardScaler()\n",
    "        self.spange_scaled = self.scaler_spange.fit_transform(self.spange.values)\n",
    "        self.spange_scaled_df = pd.DataFrame(self.spange_scaled, index=self.spange.index)\n",
    "        \n",
    "        # Feature scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Population mean (will be computed from training data)\n",
    "        self.population_mean = None\n",
    "        \n",
    "        # Training solvent descriptors (for similarity computation)\n",
    "        self.train_solvent_descriptors = None\n",
    "        self.train_solvents = None\n",
    "        \n",
    "    def _compute_similarity(self, test_solvent_desc, train_solvent_descs):\n",
    "        \"\"\"Compute similarity between test solvent and all training solvents.\n",
    "        Returns a value between 0 (dissimilar) and 1 (identical).\n",
    "        \"\"\"\n",
    "        # Use negative euclidean distance converted to similarity\n",
    "        distances = euclidean_distances(test_solvent_desc.reshape(1, -1), train_solvent_descs)[0]\n",
    "        \n",
    "        # Convert to similarity: higher distance = lower similarity\n",
    "        # Use exponential decay: similarity = exp(-distance / scale)\n",
    "        scale = np.median(distances) + 1e-6  # Adaptive scale based on typical distances\n",
    "        similarities = np.exp(-distances / scale)\n",
    "        \n",
    "        # Return max similarity (how similar is test to its most similar training solvent)\n",
    "        return np.max(similarities)\n",
    "    \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract features from data.\"\"\"\n",
    "        if self.mixed:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0\n",
    "            \n",
    "            # Get solvent features\n",
    "            feats_a = self.spange.loc[X['SOLVENT A NAME']].values\n",
    "            feats_b = self.spange.loc[X['SOLVENT B NAME']].values\n",
    "            \n",
    "            # Linear mixing\n",
    "            solvent_feats = (1 - sb_pct) * feats_a + sb_pct * feats_b\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, sb_pct, solvent_feats])\n",
    "        else:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            solvent_feats = self.spange.loc[X['SOLVENT NAME']].values\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, solvent_feats])\n",
    "        \n",
    "        return combined.astype(np.float32)\n",
    "    \n",
    "    def _get_solvent_descriptors(self, X):\n",
    "        \"\"\"Get scaled solvent descriptors for similarity computation.\"\"\"\n",
    "        if self.mixed:\n",
    "            sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0\n",
    "            feats_a = self.spange_scaled_df.loc[X['SOLVENT A NAME']].values\n",
    "            feats_b = self.spange_scaled_df.loc[X['SOLVENT B NAME']].values\n",
    "            return (1 - sb_pct) * feats_a + sb_pct * feats_b\n",
    "        else:\n",
    "            return self.spange_scaled_df.loc[X['SOLVENT NAME']].values\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_np = self._get_features(train_X)\n",
    "        y_np = train_Y.values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Store population mean\n",
    "        self.population_mean = y_np.mean(axis=0)\n",
    "        \n",
    "        # Store training solvent descriptors for similarity computation\n",
    "        self.train_solvent_descriptors = self._get_solvent_descriptors(train_X)\n",
    "        # Get unique training solvents\n",
    "        if self.mixed:\n",
    "            self.train_solvents = set(train_X['SOLVENT A NAME'].unique()) | set(train_X['SOLVENT B NAME'].unique())\n",
    "        else:\n",
    "            self.train_solvents = set(train_X['SOLVENT NAME'].unique())\n",
    "        \n",
    "        # Train CatBoost model\n",
    "        self.models = []\n",
    "        for t in range(3):\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                random_state=SEED,\n",
    "                verbose=False\n",
    "            )\n",
    "            model.fit(X_scaled, y_np[:, t])\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_np = self._get_features(test_X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Get model predictions\n",
    "        preds = np.column_stack([m.predict(X_scaled) for m in self.models])\n",
    "        \n",
    "        # Compute similarity for each test sample\n",
    "        test_solvent_descs = self._get_solvent_descriptors(test_X)\n",
    "        \n",
    "        # Get unique training solvent descriptors\n",
    "        if self.mixed:\n",
    "            train_unique_solvents = list(self.train_solvents)\n",
    "        else:\n",
    "            train_unique_solvents = list(self.train_solvents)\n",
    "        train_unique_descs = self.spange_scaled_df.loc[train_unique_solvents].values\n",
    "        \n",
    "        # Compute similarity and blend\n",
    "        final_preds = np.zeros_like(preds)\n",
    "        for i in range(len(test_X)):\n",
    "            similarity = self._compute_similarity(test_solvent_descs[i], train_unique_descs)\n",
    "            \n",
    "            # Blend factor: 0 = use model prediction, 1 = use population mean\n",
    "            # When similarity is low, blend more toward mean\n",
    "            blend_factor = self.blend_strength * (1 - similarity)\n",
    "            \n",
    "            final_preds[i] = (1 - blend_factor) * preds[i] + blend_factor * self.population_mean\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        final_preds = np.clip(final_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print('SimilarityWeightedModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = SimilarityWeightedModel(data='single', blend_strength=0.3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = SimilarityWeightedModel(data='full', blend_strength=0.3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "\n",
    "# Check prediction ranges\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 082 COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
