{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e35d9b",
   "metadata": {},
   "source": [
    "# Experiment 098: 5-Model Ensemble with Task-Specific Weights\n",
    "\n",
    "**Goal**: Combine our best approaches into a diverse 5-model ensemble:\n",
    "1. GP (for uncertainty and smooth predictions)\n",
    "2. MLP (for non-linear patterns)\n",
    "3. LightGBM (for tree-based patterns)\n",
    "4. CatBoost (for gradient boosting)\n",
    "5. XGBoost (for gradient boosting)\n",
    "\n",
    "**Task-Specific Weights** (from ens-model kernel insights):\n",
    "- Single solvent: More weight on CatBoost\n",
    "- Full data: More weight on XGBoost\n",
    "\n",
    "**Target**: CV < 0.007 (significant improvement from 0.0083)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9a1af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:36.309341Z",
     "iopub.status.busy": "2026-01-16T21:52:36.308775Z",
     "iopub.status.idle": "2026-01-16T21:52:38.015524Z",
     "shell.execute_reply": "2026-01-16T21:52:38.015068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1968e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:38.016696Z",
     "iopub.status.busy": "2026-01-16T21:52:38.016536Z",
     "iopub.status.idle": "2026-01-16T21:52:38.020981Z",
     "shell.execute_reply": "2026-01-16T21:52:38.020607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b59675c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:38.021945Z",
     "iopub.status.busy": "2026-01-16T21:52:38.021848Z",
     "iopub.status.idle": "2026-01-16T21:52:38.051653Z",
     "shell.execute_reply": "2026-01-16T21:52:38.051294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d14d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:38.052631Z",
     "iopub.status.busy": "2026-01-16T21:52:38.052528Z",
     "iopub.status.idle": "2026-01-16T21:52:38.057653Z",
     "shell.execute_reply": "2026-01-16T21:52:38.057301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer (for MLP, LGBM, CatBoost, XGBoost) - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            \n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                pct = 1 - pct\n",
    "                A_spange, B_spange = B_spange, A_spange\n",
    "                A_drfp, B_drfp = B_drfp, A_drfp\n",
    "                A_acs, B_acs = B_acs, A_acs\n",
    "            \n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "            X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "            X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e65e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:38.058499Z",
     "iopub.status.busy": "2026-01-16T21:52:38.058402Z",
     "iopub.status.idle": "2026-01-16T21:52:38.062778Z",
     "shell.execute_reply": "2026-01-16T21:52:38.062419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple feature dimension: 18\n"
     ]
    }
   ],
   "source": [
    "# Simple Featurizer (for GP) - 18 features\n",
    "class SimpleFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                pct = 1 - pct\n",
    "                A_spange, B_spange = B_spange, A_spange\n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange])\n",
    "\n",
    "print(f'Simple feature dimension: {SimpleFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7abfcf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T21:52:38.063857Z",
     "iopub.status.busy": "2026-01-16T21:52:38.063750Z",
     "iopub.status.idle": "2026-01-16T21:52:38.074081Z",
     "shell.execute_reply": "2026-01-16T21:52:38.073709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiveModelEnsemble defined\n",
      "Single weights: {'gp': 0.1, 'mlp': 0.2, 'lgb': 0.15, 'cat': 0.3, 'xgb': 0.25}\n",
      "Full weights: {'gp': 0.1, 'mlp': 0.2, 'lgb': 0.15, 'cat': 0.2, 'xgb': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# 5-Model Ensemble with Task-Specific Weights\n",
    "class FiveModelEnsemble:\n",
    "    \"\"\"Ensemble of GP + MLP + LGBM + CatBoost + XGBoost with task-specific weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        # Featurizers\n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        self.scaler_full = StandardScaler()\n",
    "        self.scaler_simple = StandardScaler()\n",
    "        \n",
    "        # Models\n",
    "        self.gp_models = []\n",
    "        self.mlp_model = None\n",
    "        self.lgb_models = []\n",
    "        self.cat_models = []\n",
    "        self.xgb_models = []\n",
    "        \n",
    "        # Task-specific weights (from ens-model kernel insights)\n",
    "        if data == 'single':\n",
    "            # Single solvent: CatBoost dominant\n",
    "            self.weights = {\n",
    "                'gp': 0.10,\n",
    "                'mlp': 0.20,\n",
    "                'lgb': 0.15,\n",
    "                'cat': 0.30,  # CatBoost dominant\n",
    "                'xgb': 0.25\n",
    "            }\n",
    "        else:\n",
    "            # Full data: XGBoost dominant\n",
    "            self.weights = {\n",
    "                'gp': 0.10,\n",
    "                'mlp': 0.20,\n",
    "                'lgb': 0.15,\n",
    "                'cat': 0.20,\n",
    "                'xgb': 0.35  # XGBoost dominant\n",
    "            }\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        # Get features\n",
    "        X_full = self.full_featurizer.featurize(train_X)\n",
    "        X_simple = self.simple_featurizer.featurize(train_X)\n",
    "        Y = train_Y.values\n",
    "        \n",
    "        # Data augmentation for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(train_X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(train_X, flip=True)\n",
    "            X_full = np.vstack([X_full, X_full_flip])\n",
    "            X_simple = np.vstack([X_simple, X_simple_flip])\n",
    "            Y = np.vstack([Y, Y])\n",
    "        \n",
    "        # Scale features\n",
    "        X_full_scaled = self.scaler_full.fit_transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.fit_transform(X_simple)\n",
    "        \n",
    "        # 1. Train GP models (on simple features)\n",
    "        self.gp_models = []\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        for i in range(3):\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42, normalize_y=True)\n",
    "            gp.fit(X_simple_scaled, Y[:, i])\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        # 2. Train MLP\n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.double).to(device)\n",
    "        \n",
    "        self.mlp_model = nn.Sequential(\n",
    "            nn.Linear(X_full_scaled.shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.Sigmoid()\n",
    "        ).double().to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp_model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.HuberLoss()\n",
    "        \n",
    "        self.mlp_model.train()\n",
    "        for epoch in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.mlp_model(X_tensor)\n",
    "            loss = loss_fn(pred, Y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        self.mlp_model.eval()\n",
    "        \n",
    "        # 3. Train LightGBM models\n",
    "        self.lgb_models = []\n",
    "        lgb_params = {'objective': 'regression', 'metric': 'mse', 'boosting_type': 'gbdt',\n",
    "                      'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.9,\n",
    "                      'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': -1, 'seed': 42}\n",
    "        for i in range(3):\n",
    "            train_data = lgb.Dataset(X_full_scaled, label=Y[:, i])\n",
    "            model = lgb.train(lgb_params, train_data, num_boost_round=500)\n",
    "            self.lgb_models.append(model)\n",
    "        \n",
    "        # 4. Train CatBoost models\n",
    "        self.cat_models = []\n",
    "        for i in range(3):\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "            model.fit(X_full_scaled, Y[:, i])\n",
    "            self.cat_models.append(model)\n",
    "        \n",
    "        # 5. Train XGBoost models\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            model.fit(X_full_scaled, Y[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        # Get features\n",
    "        X_full = self.full_featurizer.featurize(test_X)\n",
    "        X_simple = self.simple_featurizer.featurize(test_X)\n",
    "        \n",
    "        # Scale features\n",
    "        X_full_scaled = self.scaler_full.transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.transform(X_simple)\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_preds = np.zeros((len(test_X), 3))\n",
    "        for i in range(3):\n",
    "            gp_preds[:, i] = self.gp_models[i].predict(X_simple_scaled)\n",
    "        \n",
    "        # MLP predictions\n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        with torch.no_grad():\n",
    "            mlp_preds = self.mlp_model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgb_preds = np.zeros((len(test_X), 3))\n",
    "        for i in range(3):\n",
    "            lgb_preds[:, i] = self.lgb_models[i].predict(X_full_scaled)\n",
    "        \n",
    "        # CatBoost predictions\n",
    "        cat_preds = np.zeros((len(test_X), 3))\n",
    "        for i in range(3):\n",
    "            cat_preds[:, i] = self.cat_models[i].predict(X_full_scaled)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_preds = np.zeros((len(test_X), 3))\n",
    "        for i in range(3):\n",
    "            xgb_preds[:, i] = self.xgb_models[i].predict(X_full_scaled)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_preds = (\n",
    "            self.weights['gp'] * gp_preds +\n",
    "            self.weights['mlp'] * mlp_preds +\n",
    "            self.weights['lgb'] * lgb_preds +\n",
    "            self.weights['cat'] * cat_preds +\n",
    "            self.weights['xgb'] * xgb_preds\n",
    "        )\n",
    "        \n",
    "        # Clip to valid range\n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(ensemble_preds, dtype=torch.double)\n",
    "\n",
    "print('FiveModelEnsemble defined')\n",
    "print('Single weights:', {'gp': 0.10, 'mlp': 0.20, 'lgb': 0.15, 'cat': 0.30, 'xgb': 0.25})\n",
    "print('Full weights:', {'gp': 0.10, 'mlp': 0.20, 'lgb': 0.15, 'cat': 0.20, 'xgb': 0.35})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = FiveModelEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeeeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = FiveModelEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81810f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV score (for verification only - NOT part of submission)\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "# Get actuals in same order as predictions\n",
    "actuals_single = []\n",
    "for solvent in sorted(X_single[\"SOLVENT NAME\"].unique()):\n",
    "    mask = X_single[\"SOLVENT NAME\"] == solvent\n",
    "    actuals_single.append(Y_single[mask].values)\n",
    "actuals_single = np.vstack(actuals_single)\n",
    "\n",
    "actuals_full = []\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "for _, row in ramps.iterrows():\n",
    "    mask = (X_full[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X_full[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"])\n",
    "    actuals_full.append(Y_full[mask].values)\n",
    "actuals_full = np.vstack(actuals_full)\n",
    "\n",
    "# Get predictions\n",
    "preds_single = submission_single_solvent[['target_1', 'target_2', 'target_3']].values\n",
    "preds_full = submission_full_data[['target_1', 'target_2', 'target_3']].values\n",
    "\n",
    "# Calculate MSE\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest previous CV: 0.008092 (CatBoost+XGBoost)')\n",
    "print(f'Best previous LB: 0.0877 (GP+MLP+LGBM)')\n",
    "print(f'exp_030 baseline (GP+MLP+LGBM): CV 0.008298')\n",
    "print(f'\\nThis (5-Model Ensemble): CV {overall_mse:.6f}')\n",
    "\n",
    "if overall_mse < 0.008092:\n",
    "    improvement = (0.008092 - overall_mse) / 0.008092 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "elif overall_mse < 0.008298:\n",
    "    improvement = (0.008298 - overall_mse) / 0.008298 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than exp_030!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008298) / 0.008298 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than exp_030')\n",
    "\n",
    "# Predicted LB based on CV-LB relationship\n",
    "predicted_lb = 4.36 * overall_mse + 0.052\n",
    "print(f'\\nPredicted LB (based on CV-LB relationship): {predicted_lb:.4f}')\n",
    "print(f'Best LB so far: 0.0877')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
