{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39948324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:06:40.452220Z",
     "iopub.status.busy": "2026-01-16T05:06:40.451775Z",
     "iopub.status.idle": "2026-01-16T05:06:40.730229Z",
     "shell.execute_reply": "2026-01-16T05:06:40.729810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/data/utils.py\n",
      "/home/data/acs_pca_descriptors_lookup.csv\n",
      "/home/data/catechol_single_solvent_yields.csv\n",
      "/home/data/instructions.txt\n",
      "/home/data/smiles_lookup.csv\n",
      "/home/data/spange_descriptors_lookup.csv\n",
      "/home/data/description.md\n",
      "/home/data/drfps_catechol_lookup.csv\n",
      "/home/data/catechol_full_data_yields.csv\n",
      "/home/data/fragprints_lookup.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/home/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af5fe9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:06:40.731384Z",
     "iopub.status.busy": "2026-01-16T05:06:40.731243Z",
     "iopub.status.idle": "2026-01-16T05:06:40.736391Z",
     "shell.execute_reply": "2026-01-16T05:06:40.736055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils functions defined\n"
     ]
    }
   ],
   "source": [
    "# Define utils functions locally (adapted for local paths)\n",
    "from typing import Any, Generator\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "def load_data(name = \"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name = \"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).all(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\",\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_FEATURES = [\"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_FEATURES = [\"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "print('Utils functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0db1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:06:40.737222Z",
     "iopub.status.busy": "2026-01-16T05:06:40.737119Z",
     "iopub.status.idle": "2026-01-16T05:06:40.740902Z",
     "shell.execute_reply": "2026-01-16T05:06:40.740567Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396da7df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:06:40.741900Z",
     "iopub.status.busy": "2026-01-16T05:06:40.741803Z",
     "iopub.status.idle": "2026-01-16T05:06:41.456414Z",
     "shell.execute_reply": "2026-01-16T05:06:41.455966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_dtype(torch.double) \n",
    "\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features = 'spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles = X[INPUT_LABELS_SINGLE_FEATURES]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features = 'spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles = X[INPUT_LABELS_FULL_FEATURES]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = X_smiles_A_feat.values * (1 - X[\"SolventB%\"].values.reshape(-1, 1)) + X_smiles_B_feat.values * X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat_tensor)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class MLPModel(nn.Module, BaseModel):\n",
    "    def __init__(self, features = 'spange_descriptors', hidden_dims = [64, 64], output_dim = 3, dropout=0.0, data = 'single'):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer(features = features)\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed(features = features)\n",
    "        input_dim = self.smiles_featurizer.feats_dim\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def train_model(self, train_X, train_Y, criterion=nn.MSELoss, optimizer=torch.optim.Adam,\n",
    "                    num_epochs=100, batch_size=1048, device=\"cpu\", verbose=True, lr=1e-3):\n",
    "        self.train()\n",
    "        train_X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        train_Y_tensor = torch.tensor(train_Y.values)\n",
    "        train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        criterion = criterion()\n",
    "        optimizer = optimizer(self.parameters(), lr=lr)\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.smiles_featurizer.featurize(X)\n",
    "        return self.forward(X)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print('MLPModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MLPModel() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MLPModel(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
