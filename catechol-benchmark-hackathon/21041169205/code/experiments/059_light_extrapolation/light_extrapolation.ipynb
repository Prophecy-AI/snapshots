{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433c84e4",
   "metadata": {},
   "source": [
    "# Experiment 059: Light Extrapolation Detection\n",
    "\n",
    "**Goal:** Try lighter extrapolation detection parameters to reduce CV degradation.\n",
    "\n",
    "**Changes from exp_058:**\n",
    "- blend_strength: 0.3 → 0.1 (less aggressive blending)\n",
    "- blend_threshold: 0.5 → 0.7 (more selective - only blend for very dissimilar solvents)\n",
    "- Use Spange-based similarity instead of fragprint Tanimoto (aligns with model features)\n",
    "\n",
    "**Hypothesis:** Lighter blending will preserve more of the base model's CV while still providing some intercept reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a336177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import tqdm\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants from official template\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT A NAME\",\n",
    "    \"SOLVENT B NAME\",\n",
    "    \"SolventB%\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT NAME\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "]\n",
    "\n",
    "TARGET_LABELS = [\n",
    "    \"Product 2\",\n",
    "    \"Product 3\",\n",
    "    \"SM\",\n",
    "]\n",
    "\n",
    "print(\"Constants defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "# CV functions from official template\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = ~((X[\"SOLVENT A NAME\"] == solvent_pair[\"SOLVENT A NAME\"]) & \n",
    "                           (X[\"SOLVENT B NAME\"] == solvent_pair[\"SOLVENT B NAME\"]))\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading and CV functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5134f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes from official template\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'Solvents: {SPANGE_DF.index.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370839dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spange-based similarity (cosine similarity on normalized descriptors)\n",
    "def compute_spange_similarity_matrix(spange_df):\n",
    "    \"\"\"Compute pairwise cosine similarity matrix using Spange descriptors.\"\"\"\n",
    "    # Normalize each solvent's features\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "    solvents = spange_df.index.tolist()\n",
    "    features = spange_df.values\n",
    "    \n",
    "    # Normalize to unit vectors\n",
    "    features_norm = normalize(features, axis=1)\n",
    "    \n",
    "    # Compute cosine similarity (dot product of normalized vectors)\n",
    "    similarity_matrix = np.dot(features_norm, features_norm.T)\n",
    "    \n",
    "    return pd.DataFrame(similarity_matrix, index=solvents, columns=solvents)\n",
    "\n",
    "# Compute similarity matrix using Spange descriptors\n",
    "SIMILARITY_MATRIX = compute_spange_similarity_matrix(SPANGE_DF)\n",
    "print(f\"Similarity matrix shape: {SIMILARITY_MATRIX.shape}\")\n",
    "print(f\"\\nSample similarities for 'Acetonitrile':\")\n",
    "print(SIMILARITY_MATRIX.loc['Acetonitrile'].sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light Extrapolation-aware model\n",
    "class LightExtrapolationModel(BaseModel):\n",
    "    \"\"\"Model with LIGHT extrapolation detection.\n",
    "    \n",
    "    Key changes from exp_058:\n",
    "    - blend_strength: 0.1 (was 0.3) - less aggressive\n",
    "    - blend_threshold: 0.7 (was 0.5) - more selective\n",
    "    - Uses Spange-based cosine similarity (was fragprint Tanimoto)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', blend_threshold=0.7, blend_strength=0.1):\n",
    "        self.data = data\n",
    "        self.blend_threshold = blend_threshold\n",
    "        self.blend_strength = blend_strength\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.similarity_matrix = SIMILARITY_MATRIX\n",
    "        self.models = None\n",
    "        self.scaler = None\n",
    "        self.train_mean = None\n",
    "        self.train_solvents = None\n",
    "    \n",
    "    def _prepare_features(self, X):\n",
    "        \"\"\"Prepare features for the model.\"\"\"\n",
    "        # Numeric features with Arrhenius engineering\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.data == 'single':\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            return np.hstack([X_kinetic, X_spange])\n",
    "        else:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1) / 100.0\n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "            return np.hstack([X_kinetic, pct, X_spange])\n",
    "    \n",
    "    def _get_max_similarity_to_training(self, test_solvent, train_solvents):\n",
    "        \"\"\"Get maximum similarity between test solvent and any training solvent.\"\"\"\n",
    "        if test_solvent not in self.similarity_matrix.index:\n",
    "            return 0.0\n",
    "        \n",
    "        similarities = []\n",
    "        for train_solvent in train_solvents:\n",
    "            if train_solvent in self.similarity_matrix.columns:\n",
    "                sim = self.similarity_matrix.loc[test_solvent, train_solvent]\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        if not similarities:\n",
    "            return 0.0\n",
    "        return max(similarities)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        \"\"\"Train CatBoost + XGBoost ensemble.\"\"\"\n",
    "        X_features = self._prepare_features(train_X)\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        self.train_mean = Y_np.mean(axis=0)\n",
    "        \n",
    "        if self.data == 'single':\n",
    "            self.train_solvents = train_X[\"SOLVENT NAME\"].unique().tolist()\n",
    "        else:\n",
    "            self.train_solvents = list(set(train_X[\"SOLVENT A NAME\"].tolist() + train_X[\"SOLVENT B NAME\"].tolist()))\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_features)\n",
    "        \n",
    "        # Train CatBoost models\n",
    "        self.cat_models = []\n",
    "        for i in range(3):\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                depth=6,\n",
    "                learning_rate=0.05,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "            )\n",
    "            model.fit(X_scaled, Y_np[:, i])\n",
    "            self.cat_models.append(model)\n",
    "        \n",
    "        # Train XGBoost models\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=400,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "            )\n",
    "            model.fit(X_scaled, Y_np[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"Predict with light extrapolation-aware blending.\"\"\"\n",
    "        X_features = self._prepare_features(test_X)\n",
    "        X_scaled = self.scaler.transform(X_features)\n",
    "        \n",
    "        # Get base predictions from ensemble\n",
    "        cat_preds = np.column_stack([m.predict(X_scaled) for m in self.cat_models])\n",
    "        xgb_preds = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # Ensemble: 60% CatBoost + 40% XGBoost\n",
    "        base_preds = 0.6 * cat_preds + 0.4 * xgb_preds\n",
    "        \n",
    "        # Compute extrapolation score for each sample\n",
    "        if self.data == 'single':\n",
    "            test_solvents = test_X[\"SOLVENT NAME\"].values\n",
    "            extrapolation_scores = []\n",
    "            for solvent in test_solvents:\n",
    "                max_sim = self._get_max_similarity_to_training(solvent, self.train_solvents)\n",
    "                extrapolation_scores.append(1.0 - max_sim)\n",
    "            extrapolation_scores = np.array(extrapolation_scores).reshape(-1, 1)\n",
    "        else:\n",
    "            test_solvents_A = test_X[\"SOLVENT A NAME\"].values\n",
    "            test_solvents_B = test_X[\"SOLVENT B NAME\"].values\n",
    "            extrapolation_scores = []\n",
    "            for solv_a, solv_b in zip(test_solvents_A, test_solvents_B):\n",
    "                max_sim_a = self._get_max_similarity_to_training(solv_a, self.train_solvents)\n",
    "                max_sim_b = self._get_max_similarity_to_training(solv_b, self.train_solvents)\n",
    "                avg_sim = (max_sim_a + max_sim_b) / 2\n",
    "                extrapolation_scores.append(1.0 - avg_sim)\n",
    "            extrapolation_scores = np.array(extrapolation_scores).reshape(-1, 1)\n",
    "        \n",
    "        # Light blending - only for very dissimilar solvents\n",
    "        # extrapolation_score > (1 - threshold) triggers blending\n",
    "        # With threshold=0.7, only solvents with similarity < 0.3 trigger blending\n",
    "        blend_weights = np.clip(\n",
    "            (extrapolation_scores - (1 - self.blend_threshold)) / self.blend_threshold,\n",
    "            0.0, 1.0\n",
    "        ) * self.blend_strength\n",
    "        \n",
    "        # Final predictions\n",
    "        final_preds = (1 - blend_weights) * base_preds + blend_weights * self.train_mean\n",
    "        final_preds = np.clip(final_preds, 0.0, 1.0)\n",
    "        \n",
    "        return torch.tensor(final_preds, dtype=torch.double)\n",
    "\n",
    "print(\"LightExtrapolationModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "print(\"Testing LightExtrapolationModel...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "test_solvent = test_X['SOLVENT NAME'].iloc[0]\n",
    "print(f\"Test solvent: {test_solvent}\")\n",
    "\n",
    "# Check similarity to training solvents\n",
    "train_solvents = train_X['SOLVENT NAME'].unique().tolist()\n",
    "max_sim = max([SIMILARITY_MATRIX.loc[test_solvent, s] for s in train_solvents if s in SIMILARITY_MATRIX.columns])\n",
    "print(f\"Max similarity to training: {max_sim:.4f}\")\n",
    "print(f\"Extrapolation score: {1 - max_sim:.4f}\")\n",
    "\n",
    "model = LightExtrapolationModel(blend_threshold=0.7, blend_strength=0.1)\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions sample: {preds[0]}\")\n",
    "print(f\"Actual sample: {test_Y.iloc[0].values}\")\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = LightExtrapolationModel(blend_threshold=0.7, blend_strength=0.1) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Single solvent predictions: {len(submission_single_solvent)}\")\n",
    "print(f\"Unique folds: {submission_single_solvent['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76197b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = LightExtrapolationModel(data='full', blend_threshold=0.7, blend_strength=0.1) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Full data predictions: {len(submission_full_data)}\")\n",
    "print(f\"Unique folds: {submission_full_data['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad90bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV for logging\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CV CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single solvent CV\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X, Y)):\n",
    "    model = LightExtrapolationModel(blend_threshold=0.7, blend_strength=0.1)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    fold_mses.append(mse)\n",
    "    if fold_idx % 5 == 0:\n",
    "        print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "single_cv = np.mean(fold_mses)\n",
    "single_std = np.std(fold_mses)\n",
    "print(f\"\\nSingle solvent CV MSE: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "\n",
    "# Full data CV\n",
    "X, Y = load_data(\"full\")\n",
    "full_fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_ramp_out_splits(X, Y)):\n",
    "    model = LightExtrapolationModel(data='full', blend_threshold=0.7, blend_strength=0.1)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    full_fold_mses.append(mse)\n",
    "    print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "full_cv = np.mean(full_fold_mses)\n",
    "full_std = np.std(full_fold_mses)\n",
    "print(f\"\\nFull data CV MSE: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "print(f\"\\nFINAL CV FOR LOGGING: {single_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 059: LIGHT EXTRAPOLATION DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCHANGES FROM EXP_058:\")\n",
    "print(\"  - blend_strength: 0.3 → 0.1 (less aggressive)\")\n",
    "print(\"  - blend_threshold: 0.5 → 0.7 (more selective)\")\n",
    "print(\"  - Similarity: Fragprint Tanimoto → Spange cosine (aligns with model features)\")\n",
    "\n",
    "print(f\"\\nCV SCORES:\")\n",
    "print(f\"  Single solvent: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "print(f\"  Full data: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "# Compare to exp_058\n",
    "print(f\"\\nCOMPARISON TO EXP_058:\")\n",
    "print(f\"  exp_058 CV: 0.011541\")\n",
    "print(f\"  exp_059 CV: {single_cv:.6f}\")\n",
    "print(f\"  Improvement: {(0.011541 - single_cv) / 0.011541 * 100:.1f}%\")\n",
    "\n",
    "# Compare to best CV\n",
    "print(f\"\\nCOMPARISON TO BEST CV:\")\n",
    "print(f\"  Best CV: 0.008092\")\n",
    "print(f\"  exp_059 CV: {single_cv:.6f}\")\n",
    "print(f\"  Gap: {(single_cv - 0.008092) / 0.008092 * 100:.1f}%\")\n",
    "\n",
    "# Predicted LB\n",
    "predicted_lb = 4.31 * single_cv + 0.0525\n",
    "print(f\"\\nPREDICTED LB (using CV-LB relationship): {predicted_lb:.4f}\")\n",
    "print(f\"  Best LB so far: 0.0877\")\n",
    "print(f\"  Target: 0.0347\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
