{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998b0060",
   "metadata": {},
   "source": [
    "# Experiment 083: Pseudo-Labeling / Self-Training\n",
    "\n",
    "**Rationale**: Use confident predictions on test data to augment training data. This adapts the model to the test distribution.\n",
    "\n",
    "**Implementation**:\n",
    "1. Train initial model on training data\n",
    "2. Make predictions on test data (from CV folds)\n",
    "3. Select high-confidence predictions (low ensemble variance)\n",
    "4. Add pseudo-labels to training data\n",
    "5. Retrain model on augmented data\n",
    "\n",
    "**Key Insight**: The intercept (0.052) > target (0.0347). Standard CV optimization cannot reach the target. We need approaches that adapt to the test distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3bb736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:06:48.184391Z",
     "iopub.status.busy": "2026-01-16T12:06:48.183839Z",
     "iopub.status.idle": "2026-01-16T12:06:49.557976Z",
     "shell.execute_reply": "2026-01-16T12:06:49.557524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478bb1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:06:49.559170Z",
     "iopub.status.busy": "2026-01-16T12:06:49.559009Z",
     "iopub.status.idle": "2026-01-16T12:06:49.562537Z",
     "shell.execute_reply": "2026-01-16T12:06:49.562179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data functions defined\n"
     ]
    }
   ],
   "source": [
    "# Local data loading functions\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2998a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:06:49.563408Z",
     "iopub.status.busy": "2026-01-16T12:06:49.563311Z",
     "iopub.status.idle": "2026-01-16T12:06:49.567685Z",
     "shell.execute_reply": "2026-01-16T12:06:49.567348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV split functions defined\n"
     ]
    }
   ],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print('CV split functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb50bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:06:49.568648Z",
     "iopub.status.busy": "2026-01-16T12:06:49.568557Z",
     "iopub.status.idle": "2026-01-16T12:06:49.576463Z",
     "shell.execute_reply": "2026-01-16T12:06:49.576096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PseudoLabelingModel defined\n"
     ]
    }
   ],
   "source": [
    "# Pseudo-labeling model\n",
    "class PseudoLabelingModel:\n",
    "    \"\"\"Model that uses pseudo-labeling to adapt to test distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', n_iterations=2, confidence_threshold=0.8):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.n_iterations = n_iterations  # Number of pseudo-labeling iterations\n",
    "        self.confidence_threshold = confidence_threshold  # Threshold for selecting confident predictions\n",
    "        \n",
    "        # Load Spange descriptors\n",
    "        self.spange = load_features('spange_descriptors')\n",
    "        \n",
    "        # Feature scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract features from data.\"\"\"\n",
    "        if self.mixed:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0\n",
    "            \n",
    "            # Get solvent features\n",
    "            feats_a = self.spange.loc[X['SOLVENT A NAME']].values\n",
    "            feats_b = self.spange.loc[X['SOLVENT B NAME']].values\n",
    "            \n",
    "            # Linear mixing\n",
    "            solvent_feats = (1 - sb_pct) * feats_a + sb_pct * feats_b\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, sb_pct, solvent_feats])\n",
    "        else:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            solvent_feats = self.spange.loc[X['SOLVENT NAME']].values\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, solvent_feats])\n",
    "        \n",
    "        return combined.astype(np.float32)\n",
    "    \n",
    "    def _train_ensemble(self, X_scaled, y_np, n_models=5):\n",
    "        \"\"\"Train an ensemble of models for uncertainty estimation.\"\"\"\n",
    "        models = []\n",
    "        for seed in range(n_models):\n",
    "            model_list = []\n",
    "            for t in range(3):\n",
    "                m = CatBoostRegressor(\n",
    "                    iterations=300,\n",
    "                    learning_rate=0.05,\n",
    "                    depth=6,\n",
    "                    random_state=SEED + seed,\n",
    "                    verbose=False\n",
    "                )\n",
    "                m.fit(X_scaled, y_np[:, t])\n",
    "                model_list.append(m)\n",
    "            models.append(model_list)\n",
    "        return models\n",
    "    \n",
    "    def _predict_with_uncertainty(self, models, X_scaled):\n",
    "        \"\"\"Predict with uncertainty estimation from ensemble.\"\"\"\n",
    "        all_preds = []\n",
    "        for model_list in models:\n",
    "            preds = np.column_stack([m.predict(X_scaled) for m in model_list])\n",
    "            all_preds.append(preds)\n",
    "        \n",
    "        all_preds = np.array(all_preds)  # Shape: (n_models, n_samples, 3)\n",
    "        mean_pred = all_preds.mean(axis=0)\n",
    "        std_pred = all_preds.std(axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_np = self._get_features(train_X)\n",
    "        y_np = train_Y.values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Initial training\n",
    "        self.models = self._train_ensemble(X_scaled, y_np)\n",
    "        \n",
    "        # Store training data for pseudo-labeling\n",
    "        self.X_train_scaled = X_scaled\n",
    "        self.y_train = y_np\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_np = self._get_features(test_X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Pseudo-labeling iterations\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Get predictions with uncertainty\n",
    "            mean_pred, std_pred = self._predict_with_uncertainty(self.models, X_scaled)\n",
    "            \n",
    "            # Select confident predictions (low uncertainty)\n",
    "            # Confidence = 1 - normalized std\n",
    "            max_std = std_pred.max(axis=0) + 1e-6\n",
    "            normalized_std = std_pred / max_std\n",
    "            confidence = 1 - normalized_std.mean(axis=1)\n",
    "            \n",
    "            # Select samples with high confidence\n",
    "            confident_mask = confidence > self.confidence_threshold\n",
    "            \n",
    "            if confident_mask.sum() > 0:\n",
    "                # Add pseudo-labels to training data\n",
    "                X_pseudo = X_scaled[confident_mask]\n",
    "                y_pseudo = mean_pred[confident_mask]\n",
    "                \n",
    "                # Augment training data\n",
    "                X_augmented = np.vstack([self.X_train_scaled, X_pseudo])\n",
    "                y_augmented = np.vstack([self.y_train, y_pseudo])\n",
    "                \n",
    "                # Retrain models on augmented data\n",
    "                self.models = self._train_ensemble(X_augmented, y_augmented)\n",
    "        \n",
    "        # Final prediction\n",
    "        mean_pred, _ = self._predict_with_uncertainty(self.models, X_scaled)\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        mean_pred = np.clip(mean_pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(mean_pred)\n",
    "\n",
    "print('PseudoLabelingModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PseudoLabelingModel(data='single', n_iterations=2, confidence_threshold=0.7)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PseudoLabelingModel(data='full', n_iterations=2, confidence_threshold=0.7)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44581218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "\n",
    "# Check prediction ranges\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babce138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 083 COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
