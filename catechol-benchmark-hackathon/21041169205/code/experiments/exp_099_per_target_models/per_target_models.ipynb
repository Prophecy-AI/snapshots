{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fce7d8",
   "metadata": {},
   "source": [
    "# Experiment 099: Per-Target Models with Different Architectures\n",
    "\n",
    "**Approach from dabansherwani kernel**:\n",
    "- SM (hardest target): HistGradientBoosting with ACS_PCA + Spange features\n",
    "- Product 2/3 (easier targets): ExtraTreesRegressor with ACS_PCA + Spange features\n",
    "- Weights: 0.65 * ACS_PCA model + 0.35 * Spange model\n",
    "\n",
    "**Hypothesis**: Different targets may benefit from different model types.\n",
    "SM is consistently the hardest target and may need more regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print('Imports complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4327bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Target Model (from dabansherwani kernel)\n",
    "class PerTargetModel:\n",
    "    \"\"\"Model for a single target using a specific feature table.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_table='spange', base_type='hgb', mixed=False):\n",
    "        self.feature_table = feature_table\n",
    "        self.base_type = base_type\n",
    "        self.mixed = mixed\n",
    "        \n",
    "        if feature_table == 'spange':\n",
    "            self.lookup = SPANGE_DF\n",
    "        else:  # acs_pca\n",
    "            self.lookup = ACS_PCA_DF\n",
    "        \n",
    "        self.model = None\n",
    "    \n",
    "    def _vec(self, s):\n",
    "        \"\"\"Get feature vector for a solvent.\"\"\"\n",
    "        if s in self.lookup.index:\n",
    "            return self.lookup.loc[s].values\n",
    "        return np.zeros(self.lookup.shape[1])\n",
    "    \n",
    "    def _build_X(self, X):\n",
    "        \"\"\"Build feature matrix.\"\"\"\n",
    "        rt = X[\"Residence Time\"].values.reshape(-1, 1)\n",
    "        temp = X[\"Temperature\"].values.reshape(-1, 1)\n",
    "        \n",
    "        if not self.mixed:\n",
    "            # Single solvent\n",
    "            S = np.vstack([self._vec(s) for s in X[\"SOLVENT NAME\"]])\n",
    "            return np.hstack([rt, temp, S])\n",
    "        else:\n",
    "            # Mixed solvents - NOTE: SolventB% is already in [0, 1]\n",
    "            frac_b = X[\"SolventB%\"].values.reshape(-1, 1)  # Already in [0, 1]!\n",
    "            A = np.vstack([self._vec(s) for s in X[\"SOLVENT A NAME\"]])\n",
    "            B = np.vstack([self._vec(s) for s in X[\"SOLVENT B NAME\"]])\n",
    "            mix = (1 - frac_b) * A + frac_b * B\n",
    "            return np.hstack([rt, temp, frac_b, mix])\n",
    "    \n",
    "    def train_model(self, X, y):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        Xf = self._build_X(X)\n",
    "        \n",
    "        if self.base_type == 'hgb':\n",
    "            base = HistGradientBoostingRegressor(\n",
    "                max_depth=7, max_iter=700, learning_rate=0.04, random_state=42\n",
    "            )\n",
    "        else:  # etr\n",
    "            base = ExtraTreesRegressor(\n",
    "                n_estimators=900, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        self.model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reg', base)\n",
    "        ])\n",
    "        self.model.fit(Xf, y.values.ravel())\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict.\"\"\"\n",
    "        Xf = self._build_X(X)\n",
    "        return self.model.predict(Xf)\n",
    "\n",
    "print('PerTargetModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d88d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Target Ensemble Model\n",
    "class PerTargetEnsembleModel:\n",
    "    \"\"\"Ensemble with different model types for different targets.\n",
    "    \n",
    "    - SM (hardest): HistGradientBoosting\n",
    "    - Product 2/3 (easier): ExtraTreesRegressor\n",
    "    - Each target uses 2 models: ACS_PCA (0.65) + Spange (0.35)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        self.mixed = (data == 'full')\n",
    "        self.targets = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "        self.models = {}\n",
    "        \n",
    "        for t in self.targets:\n",
    "            if t == \"SM\":\n",
    "                # SM is hardest - use HistGradientBoosting\n",
    "                self.models[t] = [\n",
    "                    PerTargetModel('acs_pca', 'hgb', self.mixed),\n",
    "                    PerTargetModel('spange', 'hgb', self.mixed),\n",
    "                ]\n",
    "            else:\n",
    "                # P2/P3 are easier - use ExtraTrees\n",
    "                self.models[t] = [\n",
    "                    PerTargetModel('acs_pca', 'etr', self.mixed),\n",
    "                    PerTargetModel('spange', 'etr', self.mixed),\n",
    "                ]\n",
    "    \n",
    "    def train_model(self, X, Y):\n",
    "        \"\"\"Train all models.\"\"\"\n",
    "        for t in self.targets:\n",
    "            y_single = Y[[t]]\n",
    "            for m in self.models[t]:\n",
    "                m.train_model(X, y_single)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict with weighted ensemble.\"\"\"\n",
    "        preds = []\n",
    "        \n",
    "        for t in self.targets:\n",
    "            p1 = self.models[t][0].predict(X)  # ACS_PCA model\n",
    "            p2 = self.models[t][1].predict(X)  # Spange model\n",
    "            \n",
    "            # Weighted average: 0.65 * ACS_PCA + 0.35 * Spange\n",
    "            pred_t = 0.65 * p1 + 0.35 * p2\n",
    "            preds.append(pred_t.reshape(-1, 1))\n",
    "        \n",
    "        pred = np.clip(np.hstack(preds), 0, 1)\n",
    "        return torch.tensor(pred, dtype=torch.double)\n",
    "\n",
    "print('PerTargetEnsembleModel defined')\n",
    "print('  SM: HistGradientBoosting (ACS_PCA 0.65 + Spange 0.35)')\n",
    "print('  P2/P3: ExtraTreesRegressor (ACS_PCA 0.65 + Spange 0.35)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0629d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetEnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetEnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd92da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV score (for verification only - NOT part of submission)\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "# Get actuals in same order as predictions\n",
    "actuals_single = []\n",
    "for solvent in sorted(X_single[\"SOLVENT NAME\"].unique()):\n",
    "    mask = X_single[\"SOLVENT NAME\"] == solvent\n",
    "    actuals_single.append(Y_single[mask].values)\n",
    "actuals_single = np.vstack(actuals_single)\n",
    "\n",
    "actuals_full = []\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "for _, row in ramps.iterrows():\n",
    "    mask = (X_full[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X_full[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"])\n",
    "    actuals_full.append(Y_full[mask].values)\n",
    "actuals_full = np.vstack(actuals_full)\n",
    "\n",
    "# Get predictions\n",
    "preds_single = submission_single_solvent[['target_1', 'target_2', 'target_3']].values\n",
    "preds_full = submission_full_data[['target_1', 'target_2', 'target_3']].values\n",
    "\n",
    "# Calculate MSE\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest previous CV: 0.008092 (CatBoost+XGBoost)')\n",
    "print(f'Best previous LB: 0.0877 (GP+MLP+LGBM)')\n",
    "print(f'exp_030 baseline (GP+MLP+LGBM): CV 0.008298')\n",
    "print(f'\\nThis (Per-Target Ensemble): CV {overall_mse:.6f}')\n",
    "\n",
    "if overall_mse < 0.008092:\n",
    "    improvement = (0.008092 - overall_mse) / 0.008092 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "elif overall_mse < 0.008298:\n",
    "    improvement = (0.008298 - overall_mse) / 0.008298 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than exp_030!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008298) / 0.008298 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than exp_030')\n",
    "\n",
    "# Predicted LB based on CV-LB relationship\n",
    "predicted_lb = 4.36 * overall_mse + 0.052\n",
    "print(f'\\nPredicted LB (based on CV-LB relationship): {predicted_lb:.4f}')\n",
    "print(f'Best LB so far: 0.0877')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
