{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2db8a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:18.704426Z",
     "iopub.status.busy": "2026-01-16T01:27:18.703989Z",
     "iopub.status.idle": "2026-01-16T01:27:20.078832Z",
     "shell.execute_reply": "2026-01-16T01:27:20.078277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Experiment 063: CORRECT Final Cell Structure\n",
    "# THE FINAL CELL MUST BE EXACTLY THE TEMPLATE - NO EXTRA CODE!\n",
    "# CV calculation is in a SEPARATE cell AFTER the final cell\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define constants\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).all(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print('Imports and data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5256341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:20.080109Z",
     "iopub.status.busy": "2026-01-16T01:27:20.079943Z",
     "iopub.status.idle": "2026-01-16T01:27:20.085515Z",
     "shell.execute_reply": "2026-01-16T01:27:20.085195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering functions\n",
    "\n",
    "def feature_priority(name: str) -> int:\n",
    "    if name.startswith(\"spange_\"):\n",
    "        return 5\n",
    "    if name.startswith(\"acs_\"):\n",
    "        return 4\n",
    "    if name.startswith(\"drfps_\"):\n",
    "        return 3\n",
    "    if name.startswith(\"frag_\"):\n",
    "        return 2\n",
    "    if name.startswith(\"smiles_\"):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df: pd.DataFrame, threshold: float = 0.90):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    high_corr_pairs = []\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                high_corr_pairs.append((col_i, col_j, cval))\n",
    "    \n",
    "    for col_i, col_j, cval in high_corr_pairs:\n",
    "        if col_i in to_drop or col_j in to_drop:\n",
    "            continue\n",
    "        p_i = feature_priority(col_i)\n",
    "        p_j = feature_priority(col_j)\n",
    "        if p_i > p_j:\n",
    "            drop = col_j\n",
    "        elif p_j > p_i:\n",
    "            drop = col_i\n",
    "        else:\n",
    "            idx_i = df.columns.get_loc(col_i)\n",
    "            idx_j = df.columns.get_loc(col_j)\n",
    "            drop = col_i if idx_i > idx_j else col_j\n",
    "        to_drop.add(drop)\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print('Feature engineering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cdeb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:20.086415Z",
     "iopub.status.busy": "2026-01-16T01:27:20.086312Z",
     "iopub.status.idle": "2026-01-16T01:27:20.201713Z",
     "shell.execute_reply": "2026-01-16T01:27:20.201253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features: (26, 114) -> (26, 67) (dropped 47)\n",
      "Solvent table shape: (26, 67)\n"
     ]
    }
   ],
   "source": [
    "# Build combined solvent feature table\n",
    "\n",
    "def build_solvent_feature_table(threshold: float = 0.90):\n",
    "    sources = [\n",
    "        \"spange_descriptors\",\n",
    "        \"acs_pca_descriptors\",\n",
    "        \"drfps_catechol\",\n",
    "        \"fragprints\",\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for src in sources:\n",
    "        df_src = load_features(src).copy()\n",
    "        \n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        \n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"})\n",
    "            count = values.sum(axis=0).T\n",
    "            drop_cols = count[count == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols)\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        else:\n",
    "            if src == \"spange_descriptors\":\n",
    "                prefix = \"spange\"\n",
    "            elif src == \"acs_pca_descriptors\":\n",
    "                prefix = \"acs\"\n",
    "            else:\n",
    "                prefix = src\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        dfs.append(df_src)\n",
    "    \n",
    "    from functools import reduce\n",
    "    merged = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"),\n",
    "        dfs\n",
    "    )\n",
    "    \n",
    "    merged_filtered, dropped = filter_correlated_features(merged, threshold=threshold)\n",
    "    print(f\"Combined features: {merged.shape} -> {merged_filtered.shape} (dropped {len(dropped)})\")\n",
    "    \n",
    "    return merged_filtered\n",
    "\n",
    "solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "print(f\"Solvent table shape: {solvent_table.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c732785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:20.202760Z",
     "iopub.status.busy": "2026-01-16T01:27:20.202650Z",
     "iopub.status.idle": "2026-01-16T01:27:20.207984Z",
     "shell.execute_reply": "2026-01-16T01:27:20.207638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer class defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizer and Model classes\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CombinedFeaturizer:\n",
    "    def __init__(self, solvent_table, data='single'):\n",
    "        self.solvent_table = solvent_table\n",
    "        self.data_mode = data\n",
    "        self.scaler = None\n",
    "        self.feature_cols = None\n",
    "    \n",
    "    def featurize(self, X, fit_scaler=False):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.data_mode == 'single':\n",
    "            X_merged = X.merge(self.solvent_table, on='SOLVENT NAME', how='left')\n",
    "            numeric_cols = [c for c in X_merged.columns if c != 'SOLVENT NAME' and X_merged[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "            X_numeric = X_merged[numeric_cols].copy()\n",
    "        else:\n",
    "            solvent_table_a = self.solvent_table.copy()\n",
    "            solvent_table_a.columns = ['SOLVENT A NAME' if c == 'SOLVENT NAME' else f'{c}_A' for c in solvent_table_a.columns]\n",
    "            solvent_table_b = self.solvent_table.copy()\n",
    "            solvent_table_b.columns = ['SOLVENT B NAME' if c == 'SOLVENT NAME' else f'{c}_B' for c in solvent_table_b.columns]\n",
    "            \n",
    "            X_merged = X.merge(solvent_table_a, on='SOLVENT A NAME', how='left')\n",
    "            X_merged = X_merged.merge(solvent_table_b, on='SOLVENT B NAME', how='left')\n",
    "            \n",
    "            numeric_cols = [c for c in X_merged.columns if c not in ['SOLVENT A NAME', 'SOLVENT B NAME'] and X_merged[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "            X_numeric = X_merged[numeric_cols].copy()\n",
    "        \n",
    "        X_numeric = X_numeric.fillna(0.0)\n",
    "        \n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_scaled = self.scaler.fit_transform(X_numeric)\n",
    "            self.feature_cols = numeric_cols\n",
    "        else:\n",
    "            if self.scaler is None:\n",
    "                self.scaler = StandardScaler()\n",
    "                X_scaled = self.scaler.fit_transform(X_numeric)\n",
    "                self.feature_cols = numeric_cols\n",
    "            else:\n",
    "                X_scaled = self.scaler.transform(X_numeric)\n",
    "        \n",
    "        return torch.tensor(X_scaled, dtype=torch.double)\n",
    "\n",
    "print('Featurizer class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a26efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:20.208904Z",
     "iopub.status.busy": "2026-01-16T01:27:20.208810Z",
     "iopub.status.idle": "2026-01-16T01:27:20.214949Z",
     "shell.execute_reply": "2026-01-16T01:27:20.214592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostXGBEnsemble class defined\n"
     ]
    }
   ],
   "source": [
    "# CatBoost + XGBoost Ensemble Model\n",
    "\n",
    "class CatBoostXGBEnsemble(BaseModel):\n",
    "    def __init__(self, data='single', verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        self.featurizer = CombinedFeaturizer(solvent_table, data=data)\n",
    "        \n",
    "        # Ensemble weights from ens-model kernel\n",
    "        if data == 'single':\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        # CatBoost parameters\n",
    "        if data == 'single':\n",
    "            self.cat_params = dict(\n",
    "                random_seed=42,\n",
    "                loss_function=\"MultiRMSE\",\n",
    "                depth=3,\n",
    "                learning_rate=0.07,\n",
    "                n_estimators=1050,\n",
    "                l2_leaf_reg=3.5,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.cat_params = dict(\n",
    "                random_seed=42,\n",
    "                loss_function=\"MultiRMSE\",\n",
    "                depth=3,\n",
    "                learning_rate=0.06,\n",
    "                n_estimators=1100,\n",
    "                l2_leaf_reg=2.5,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        # XGBoost parameters\n",
    "        self.xgb_params = dict(\n",
    "            random_state=42,\n",
    "            n_estimators=1000,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=0,\n",
    "        )\n",
    "        \n",
    "        self.cat_model = None\n",
    "        self.xgb_models = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.featurizer.featurize(train_X, fit_scaler=True)\n",
    "        X_np = X_tensor.numpy()\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        # Train CatBoost (multi-target)\n",
    "        self.cat_model = CatBoostRegressor(**self.cat_params)\n",
    "        self.cat_model.fit(X_np, Y_np)\n",
    "        \n",
    "        # Train XGBoost (one per target)\n",
    "        self.xgb_models = []\n",
    "        for t in range(Y_np.shape[1]):\n",
    "            xgb = XGBRegressor(**self.xgb_params)\n",
    "            xgb.fit(X_np, Y_np[:, t])\n",
    "            self.xgb_models.append(xgb)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.featurizer.featurize(X, fit_scaler=False)\n",
    "        X_np = X_tensor.numpy()\n",
    "        \n",
    "        # CatBoost predictions\n",
    "        cat_pred = self.cat_model.predict(X_np)\n",
    "        cat_pred = np.clip(cat_pred, 0.0, None)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_preds = [m.predict(X_np) for m in self.xgb_models]\n",
    "        xgb_pred = np.column_stack(xgb_preds)\n",
    "        xgb_pred = np.clip(xgb_pred, 0.0, None)\n",
    "        \n",
    "        # Ensemble\n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        # Normalize to sum to 1 (if sum > 1)\n",
    "        totals = out.sum(axis=1, keepdims=True)\n",
    "        divisor = np.maximum(totals, 1.0)\n",
    "        out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('CatBoostXGBEnsemble class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46d0cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:27:30.244023Z",
     "iopub.status.busy": "2026-01-16T01:27:30.243572Z",
     "iopub.status.idle": "2026-01-16T01:27:53.983686Z",
     "shell.execute_reply": "2026-01-16T01:27:53.983286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:02,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:03,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:04,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:05,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:05,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:06,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:07,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:08,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:09,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:10,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [00:11,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:12,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [00:13,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [00:14,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [00:15,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [00:16,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [00:17,  1.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [00:18,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [00:19,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [00:20,  1.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [00:21,  1.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [00:22,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [00:23,  1.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [00:23,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = CatBoostXGBEnsemble() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7099ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:28:03.995951Z",
     "iopub.status.busy": "2026-01-16T01:28:03.995777Z",
     "iopub.status.idle": "2026-01-16T01:28:20.985928Z",
     "shell.execute_reply": "2026-01-16T01:28:20.985497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:02,  1.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:03,  1.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:05,  1.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:06,  1.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:07,  1.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:08,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:10,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:11,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:12,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:14,  1.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [00:15,  1.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:16,  1.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:16,  1.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = CatBoostXGBEnsemble(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee15dce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:28:30.974236Z",
     "iopub.status.busy": "2026-01-16T01:28:30.973697Z",
     "iopub.status.idle": "2026-01-16T01:28:30.984671Z",
     "shell.execute_reply": "2026-01-16T01:28:30.984078Z"
    }
   },
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df8e7fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:28:30.986071Z",
     "iopub.status.busy": "2026-01-16T01:28:30.985696Z",
     "iopub.status.idle": "2026-01-16T01:28:31.037431Z",
     "shell.execute_reply": "2026-01-16T01:28:31.036802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent CV MSE: 0.008811\n",
      "Full Data CV MSE: 0.015203\n",
      "Submission saved with 1883 rows\n"
     ]
    }
   ],
   "source": [
    "# CV CALCULATION - This cell is AFTER the final submission cell\n",
    "# It will be ignored by Kaggle but useful for local evaluation\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Save to /home/submission for local use\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission.to_csv('/home/submission/submission.csv', index=True)\n",
    "\n",
    "# Single solvent CV\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "split_gen = list(generate_leave_one_out_splits(X_single, Y_single))\n",
    "all_y_true, all_y_pred = [], []\n",
    "for fold_idx, split in enumerate(split_gen):\n",
    "    (_, _), (_, test_Y) = split\n",
    "    fold_preds = submission_single_solvent[submission_single_solvent['fold'] == fold_idx]\n",
    "    all_y_true.append(test_Y.values)\n",
    "    all_y_pred.append(fold_preds[['target_1', 'target_2', 'target_3']].values)\n",
    "mse_single = mean_squared_error(np.vstack(all_y_true), np.vstack(all_y_pred))\n",
    "\n",
    "# Full data CV\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "split_gen = list(generate_leave_one_ramp_out_splits(X_full, Y_full))\n",
    "all_y_true, all_y_pred = [], []\n",
    "for fold_idx, split in enumerate(split_gen):\n",
    "    (_, _), (_, test_Y) = split\n",
    "    fold_preds = submission_full_data[submission_full_data['fold'] == fold_idx]\n",
    "    all_y_true.append(test_Y.values)\n",
    "    all_y_pred.append(fold_preds[['target_1', 'target_2', 'target_3']].values)\n",
    "mse_full = mean_squared_error(np.vstack(all_y_true), np.vstack(all_y_pred))\n",
    "\n",
    "print(f'Single Solvent CV MSE: {mse_single:.6f}')\n",
    "print(f'Full Data CV MSE: {mse_full:.6f}')\n",
    "print(f'Submission saved with {len(submission)} rows')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
