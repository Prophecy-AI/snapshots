{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21726bf",
   "metadata": {},
   "source": [
    "# Experiment 081: Solvent Clustering with Class-Specific Models\n",
    "\n",
    "**Rationale**: Group solvents by chemical class using Spange descriptors, then train class-specific models that generalize within chemical families. This might reduce the intercept by improving generalization within clusters.\n",
    "\n",
    "**Key Insight**: The CV-LB relationship has intercept 0.052 > target 0.0347. Standard CV optimization cannot reach the target. We need approaches that CHANGE the relationship, not just improve CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data loading functions\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print('CV split functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze solvent clusters using Spange descriptors\n",
    "spange = load_features('spange_descriptors')\n",
    "print(f\"Spange descriptors shape: {spange.shape}\")\n",
    "print(f\"Solvents: {spange.index.tolist()}\")\n",
    "\n",
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "spange_scaled = scaler.fit_transform(spange.values)\n",
    "\n",
    "# Try different numbers of clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for n_clusters in [3, 4, 5, 6]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "    labels = kmeans.fit_predict(spange_scaled)\n",
    "    score = silhouette_score(spange_scaled, labels)\n",
    "    print(f\"K={n_clusters}: Silhouette score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7955c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4 clusters based on silhouette score\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(spange_scaled)\n",
    "\n",
    "# Create cluster mapping\n",
    "solvent_clusters = pd.DataFrame({\n",
    "    'SOLVENT NAME': spange.index,\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "print(\"\\nSolvent clusters:\")\n",
    "for c in range(n_clusters):\n",
    "    solvents = solvent_clusters[solvent_clusters['cluster'] == c]['SOLVENT NAME'].tolist()\n",
    "    print(f\"\\nCluster {c} ({len(solvents)} solvents):\")\n",
    "    print(f\"  {solvents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP for each cluster\n",
    "class ClusterMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('ClusterMLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-aware model\n",
    "class ClusterAwareModel:\n",
    "    \"\"\"Model that uses cluster information to improve predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', n_clusters=4):\n",
    "        self.data = data\n",
    "        self.n_clusters = n_clusters\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        # Load Spange descriptors and create cluster mapping\n",
    "        self.spange = load_features('spange_descriptors')\n",
    "        self.scaler_spange = StandardScaler()\n",
    "        spange_scaled = self.scaler_spange.fit_transform(self.spange.values)\n",
    "        \n",
    "        # Cluster solvents\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "        self.cluster_labels = self.kmeans.fit_predict(spange_scaled)\n",
    "        self.solvent_to_cluster = dict(zip(self.spange.index, self.cluster_labels))\n",
    "        \n",
    "        # Feature scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Models per cluster (will be trained)\n",
    "        self.models = {}\n",
    "        \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract features from data.\"\"\"\n",
    "        if self.mixed:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0\n",
    "            \n",
    "            # Get solvent features\n",
    "            feats_a = self.spange.loc[X['SOLVENT A NAME']].values\n",
    "            feats_b = self.spange.loc[X['SOLVENT B NAME']].values\n",
    "            \n",
    "            # Linear mixing\n",
    "            solvent_feats = (1 - sb_pct) * feats_a + sb_pct * feats_b\n",
    "            \n",
    "            # Get cluster for dominant solvent\n",
    "            clusters = []\n",
    "            for i in range(len(X)):\n",
    "                if sb_pct[i, 0] < 0.5:\n",
    "                    clusters.append(self.solvent_to_cluster.get(X.iloc[i]['SOLVENT A NAME'], 0))\n",
    "                else:\n",
    "                    clusters.append(self.solvent_to_cluster.get(X.iloc[i]['SOLVENT B NAME'], 0))\n",
    "            clusters = np.array(clusters)\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, sb_pct, solvent_feats])\n",
    "        else:\n",
    "            res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "            temp = X['Temperature'].values.reshape(-1, 1)\n",
    "            solvent_feats = self.spange.loc[X['SOLVENT NAME']].values\n",
    "            clusters = np.array([self.solvent_to_cluster.get(s, 0) for s in X['SOLVENT NAME']])\n",
    "            \n",
    "            combined = np.hstack([res_time, temp, solvent_feats])\n",
    "        \n",
    "        return combined.astype(np.float32), clusters\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32):\n",
    "        X_np, clusters = self._get_features(train_X)\n",
    "        y_np = train_Y.values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Train a model for each cluster\n",
    "        for c in range(self.n_clusters):\n",
    "            mask = clusters == c\n",
    "            if mask.sum() < 5:  # Skip if too few samples\n",
    "                continue\n",
    "            \n",
    "            X_c = X_scaled[mask]\n",
    "            y_c = y_np[mask]\n",
    "            \n",
    "            # Use XGBoost for each cluster\n",
    "            model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                random_state=SEED,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "            model.fit(X_c, y_c)\n",
    "            self.models[c] = model\n",
    "        \n",
    "        # Train a global model for fallback\n",
    "        self.global_model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "        self.global_model.fit(X_scaled, y_np)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_np, clusters = self._get_features(test_X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Predict using cluster-specific models\n",
    "        predictions = np.zeros((len(test_X), 3))\n",
    "        \n",
    "        for i in range(len(test_X)):\n",
    "            c = clusters[i]\n",
    "            if c in self.models:\n",
    "                pred = self.models[c].predict(X_scaled[i:i+1])\n",
    "            else:\n",
    "                pred = self.global_model.predict(X_scaled[i:i+1])\n",
    "            predictions[i] = pred\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        \n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('ClusterAwareModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bec89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ClusterAwareModel(data='single', n_clusters=4)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ClusterAwareModel(data='full', n_clusters=4)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "\n",
    "# Check prediction ranges\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea81a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 081 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nKey techniques implemented:\")\n",
    "print(\"1. Cluster solvents using Spange descriptors (K-means, 4 clusters)\")\n",
    "print(\"2. Train cluster-specific XGBoost models\")\n",
    "print(\"3. Use global model as fallback for unseen clusters\")\n",
    "print(\"\\nThis approach aims to improve generalization within chemical families.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
