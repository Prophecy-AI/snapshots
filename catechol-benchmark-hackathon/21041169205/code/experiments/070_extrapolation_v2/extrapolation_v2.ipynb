{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f780d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 070: Extrapolation Detection v2\n",
    "# \n",
    "# Key insight: When comparing to ALL solvents, the test solvent is close to ITSELF\n",
    "# So we need to compare to all solvents EXCEPT the test solvent\n",
    "# \n",
    "# This identifies solvents that are truly \"outliers\" in the solvent space\n",
    "# Water, HFIP, Ethylene Glycol might be outliers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print('Imports successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        test_idcs_mask = X[\"SOLVENT NAME\"] == solvent_name\n",
    "        train_X = X[train_idcs_mask]\n",
    "        train_Y = Y[train_idcs_mask]\n",
    "        test_X = X[test_idcs_mask]\n",
    "        test_Y = Y[test_idcs_mask]\n",
    "        yield (train_X, train_Y), (test_X, test_Y)\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvents_A = X[\"SOLVENT A NAME\"].unique()\n",
    "    all_solvents_B = X[\"SOLVENT B NAME\"].unique()\n",
    "    all_solvents = np.union1d(all_solvents_A, all_solvents_B)\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = (X[\"SOLVENT A NAME\"] != solvent_name) & (X[\"SOLVENT B NAME\"] != solvent_name)\n",
    "        test_idcs_mask = (X[\"SOLVENT A NAME\"] == solvent_name) | (X[\"SOLVENT B NAME\"] == solvent_name)\n",
    "        train_X = X[train_idcs_mask]\n",
    "        train_Y = Y[train_idcs_mask]\n",
    "        test_X = X[test_idcs_mask]\n",
    "        test_Y = Y[test_idcs_mask]\n",
    "        yield (train_X, train_Y), (test_X, test_Y)\n",
    "\n",
    "# Load Spange descriptors for ALL solvents (global)\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "print(f'Spange descriptors shape: {SPANGE_DF.shape}')\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8459dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which solvents are outliers\n",
    "# Compute distance from each solvent to all OTHER solvents\n",
    "\n",
    "solvent_scaler = StandardScaler()\n",
    "scaled_features = solvent_scaler.fit_transform(SPANGE_DF.values)\n",
    "\n",
    "# For each solvent, compute mean distance to k nearest OTHER solvents\n",
    "k = 3\n",
    "outlier_scores = {}\n",
    "\n",
    "for i, solvent in enumerate(SPANGE_DF.index):\n",
    "    # Get all other solvents\n",
    "    other_indices = [j for j in range(len(SPANGE_DF)) if j != i]\n",
    "    other_features = scaled_features[other_indices]\n",
    "    \n",
    "    # Compute distance to k nearest\n",
    "    distances = cdist([scaled_features[i]], other_features, metric='euclidean')[0]\n",
    "    k_nearest_dist = np.sort(distances)[:k].mean()\n",
    "    outlier_scores[solvent] = k_nearest_dist\n",
    "\n",
    "# Sort by outlier score\n",
    "sorted_scores = sorted(outlier_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Solvent outlier scores (higher = more isolated):\")\n",
    "for solvent, score in sorted_scores:\n",
    "    print(f\"  {solvent}: {score:.4f}\")\n",
    "\n",
    "# Compute mean and std\n",
    "mean_score = np.mean(list(outlier_scores.values()))\n",
    "std_score = np.std(list(outlier_scores.values()))\n",
    "print(f\"\\nMean: {mean_score:.4f}, Std: {std_score:.4f}\")\n",
    "print(f\"Threshold (mean + 1*std): {mean_score + std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "    def predict(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae019ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "        X_smiles_feat = X_smiles_A_feat.values * (1 - X_pct/100) + X_smiles_B_feat.values * (X_pct/100)\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39edc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute outlier scores for all solvents (excluding self)\n",
    "# This is done ONCE globally\n",
    "\n",
    "def compute_solvent_outlier_scores(k=3):\n",
    "    \"\"\"Compute outlier score for each solvent based on distance to k nearest OTHER solvents.\"\"\"\n",
    "    solvent_scaler = StandardScaler()\n",
    "    scaled_features = solvent_scaler.fit_transform(SPANGE_DF.values)\n",
    "    \n",
    "    outlier_scores = {}\n",
    "    for i, solvent in enumerate(SPANGE_DF.index):\n",
    "        other_indices = [j for j in range(len(SPANGE_DF)) if j != i]\n",
    "        other_features = scaled_features[other_indices]\n",
    "        distances = cdist([scaled_features[i]], other_features, metric='euclidean')[0]\n",
    "        k_nearest_dist = np.sort(distances)[:k].mean()\n",
    "        outlier_scores[solvent] = k_nearest_dist\n",
    "    \n",
    "    return outlier_scores\n",
    "\n",
    "SOLVENT_OUTLIER_SCORES = compute_solvent_outlier_scores(k=3)\n",
    "mean_outlier_score = np.mean(list(SOLVENT_OUTLIER_SCORES.values()))\n",
    "std_outlier_score = np.std(list(SOLVENT_OUTLIER_SCORES.values()))\n",
    "\n",
    "print(f\"Pre-computed outlier scores. Mean: {mean_outlier_score:.4f}, Std: {std_outlier_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e05433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation-Aware MLP Model v2\n",
    "# Uses pre-computed outlier scores to identify solvents that need conservative predictions\n",
    "\n",
    "class ExtrapolationAwareMLPModelV2(nn.Module, BaseModel):\n",
    "    def __init__(self, features='spange_descriptors', hidden_dims=[64, 64], output_dim=3, \n",
    "                 dropout=0.0, data='single', blend_threshold=1.5):\n",
    "        super(ExtrapolationAwareMLPModelV2, self).__init__()\n",
    "        self.data_type = data\n",
    "        self.blend_threshold = blend_threshold  # Number of std devs above mean to start blending\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer(features=features)\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed(features=features)\n",
    "        \n",
    "        input_dim = self.smiles_featurizer.feats_dim\n",
    "        prev_dim = input_dim\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())  # Ensure [0,1] output\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        self.train_Y = None\n",
    "        self.input_scaler = StandardScaler()\n",
    "    \n",
    "    def get_blend_weight(self, solvent_name):\n",
    "        \"\"\"Get blend weight based on pre-computed outlier score.\"\"\"\n",
    "        score = SOLVENT_OUTLIER_SCORES.get(solvent_name, mean_outlier_score)\n",
    "        # Normalize: how many std devs above mean?\n",
    "        z_score = (score - mean_outlier_score) / std_outlier_score\n",
    "        # Blend weight: 0 for normal solvents, increases for outliers\n",
    "        blend_weight = np.clip((z_score - self.blend_threshold) / 2.0, 0, 1)\n",
    "        return blend_weight\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        self.train_Y = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "        \n",
    "        X_featurized = self.smiles_featurizer.featurize(X_train)\n",
    "        X_scaled = self.input_scaler.fit_transform(X_featurized.numpy())\n",
    "        X_tensor = torch.tensor(X_scaled)\n",
    "        Y_tensor = torch.tensor(y_train.values if hasattr(y_train, 'values') else y_train)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.train()\n",
    "        for epoch in range(200):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(batch_X)\n",
    "                loss = criterion(pred, batch_Y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_featurized = self.smiles_featurizer.featurize(X)\n",
    "            X_scaled = self.input_scaler.transform(X_featurized.numpy())\n",
    "            X_tensor = torch.tensor(X_scaled)\n",
    "            raw_pred = self.model(X_tensor).numpy()\n",
    "        \n",
    "        # Get blend weights for each sample\n",
    "        if self.data_type == 'single':\n",
    "            solvent_names = X[\"SOLVENT NAME\"].values\n",
    "            blend_weights = np.array([self.get_blend_weight(s) for s in solvent_names])\n",
    "        else:\n",
    "            # For mixtures, use max of both solvents' blend weights\n",
    "            solvent_a_names = X[\"SOLVENT A NAME\"].values\n",
    "            solvent_b_names = X[\"SOLVENT B NAME\"].values\n",
    "            blend_weights_a = np.array([self.get_blend_weight(s) for s in solvent_a_names])\n",
    "            blend_weights_b = np.array([self.get_blend_weight(s) for s in solvent_b_names])\n",
    "            blend_weights = np.maximum(blend_weights_a, blend_weights_b)\n",
    "        \n",
    "        # Compute population mean from training data\n",
    "        mean_pred = self.train_Y.mean(axis=0)\n",
    "        \n",
    "        # Blend: for outliers, move toward mean\n",
    "        blended = (1 - blend_weights.reshape(-1, 1)) * raw_pred + blend_weights.reshape(-1, 1) * mean_pred\n",
    "        \n",
    "        # Debug: print blend weights for first fold\n",
    "        unique_solvents = X[\"SOLVENT NAME\"].unique() if self.data_type == 'single' else np.union1d(X[\"SOLVENT A NAME\"].unique(), X[\"SOLVENT B NAME\"].unique())\n",
    "        if len(unique_solvents) == 1:\n",
    "            solvent = unique_solvents[0]\n",
    "            print(f\"  Solvent: {solvent}, Outlier score: {SOLVENT_OUTLIER_SCORES.get(solvent, 0):.4f}, Blend weight: {blend_weights[0]:.4f}\")\n",
    "        \n",
    "        return torch.tensor(blended)\n",
    "\n",
    "print('ExtrapolationAwareMLPModelV2 defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExtrapolationAwareMLPModelV2(blend_threshold=1.0) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"Single solvent predictions shape: {submission_single_solvent.shape}\")\n",
    "\n",
    "# Calculate CV score for single solvent\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_mse = []\n",
    "for fold_idx, split in enumerate(split_generator):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    fold_preds = submission_single_solvent[submission_single_solvent['fold'] == fold_idx]\n",
    "    pred_values = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    true_values = test_Y.values\n",
    "    mse = ((pred_values - true_values) ** 2).mean()\n",
    "    all_mse.append(mse)\n",
    "print(f\"Single solvent CV MSE: {np.mean(all_mse):.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExtrapolationAwareMLPModelV2(data='full', blend_threshold=1.0) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"Full data predictions shape: {submission_full_data.shape}\")\n",
    "\n",
    "# Calculate CV score for full data\n",
    "X, Y = load_data(\"full\")\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_mse = []\n",
    "for fold_idx, split in enumerate(split_generator):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    fold_preds = submission_full_data[submission_full_data['fold'] == fold_idx]\n",
    "    pred_values = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    true_values = test_Y.values\n",
    "    mse = ((pred_values - true_values) ** 2).mean()\n",
    "    all_mse.append(mse)\n",
    "print(f\"Full data CV MSE: {np.mean(all_mse):.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a844c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to /home/submission/\n",
    "import shutil\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "\n",
    "print(f\"Submission saved. Shape: {submission.shape}\")\n",
    "print(f\"Predictions range: target_1 [{submission['target_1'].min():.3f}, {submission['target_1'].max():.3f}]\")\n",
    "print(f\"Predictions range: target_2 [{submission['target_2'].min():.3f}, {submission['target_2'].max():.3f}]\")\n",
    "print(f\"Predictions range: target_3 [{submission['target_3'].min():.3f}, {submission['target_3'].max():.3f}]\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
