{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 069: Fixed Extrapolation Detection\n",
    "# \n",
    "# FIX: Compare test solvents to ALL 24 solvents, not just training fold solvents\n",
    "# This addresses the fundamental flaw identified by the evaluator\n",
    "#\n",
    "# Key changes from exp_068:\n",
    "# 1. Compare to ALL 24 solvents, not just training fold solvents\n",
    "# 2. Use k=3 nearest neighbors, not k=1\n",
    "# 3. Normalize by mean inter-solvent distance\n",
    "# 4. Only blend for TRUE outliers (distance > mean + threshold * std)\n",
    "# 5. Use a good base model (MLP with proper training)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print('Imports successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        test_idcs_mask = X[\"SOLVENT NAME\"] == solvent_name\n",
    "        train_X = X[train_idcs_mask]\n",
    "        train_Y = Y[train_idcs_mask]\n",
    "        test_X = X[test_idcs_mask]\n",
    "        test_Y = Y[test_idcs_mask]\n",
    "        yield (train_X, train_Y), (test_X, test_Y)\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvents_A = X[\"SOLVENT A NAME\"].unique()\n",
    "    all_solvents_B = X[\"SOLVENT B NAME\"].unique()\n",
    "    all_solvents = np.union1d(all_solvents_A, all_solvents_B)\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = (X[\"SOLVENT A NAME\"] != solvent_name) & (X[\"SOLVENT B NAME\"] != solvent_name)\n",
    "        test_idcs_mask = (X[\"SOLVENT A NAME\"] == solvent_name) | (X[\"SOLVENT B NAME\"] == solvent_name)\n",
    "        train_X = X[train_idcs_mask]\n",
    "        train_Y = Y[train_idcs_mask]\n",
    "        test_X = X[test_idcs_mask]\n",
    "        test_Y = Y[test_idcs_mask]\n",
    "        yield (train_X, train_Y), (test_X, test_Y)\n",
    "\n",
    "# Load Spange descriptors for ALL solvents (global)\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "print(f'Spange descriptors shape: {SPANGE_DF.shape}')\n",
    "print(f'All solvents: {list(SPANGE_DF.index)}')\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "    def predict(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "        X_smiles_feat = X_smiles_A_feat.values * (1 - X_pct/100) + X_smiles_B_feat.values * (X_pct/100)\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a197059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Extrapolation-Aware MLP Model\n",
    "# Key fix: Compare to ALL solvents, not just training fold solvents\n",
    "\n",
    "class FixedExtrapolationAwareMLPModel(nn.Module, BaseModel):\n",
    "    def __init__(self, features='spange_descriptors', hidden_dims=[64, 64], output_dim=3, \n",
    "                 dropout=0.0, data='single', blend_threshold=2.0):\n",
    "        super(FixedExtrapolationAwareMLPModel, self).__init__()\n",
    "        self.data_type = data\n",
    "        self.blend_threshold = blend_threshold\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer(features=features)\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed(features=features)\n",
    "        \n",
    "        input_dim = self.smiles_featurizer.feats_dim\n",
    "        prev_dim = input_dim\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())  # Ensure [0,1] output\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # CRITICAL FIX: Pre-compute extrapolation detection on ALL solvents\n",
    "        # This is done ONCE at initialization, not per fold\n",
    "        self.all_solvent_features = SPANGE_DF.values  # All 24-26 solvents\n",
    "        self.solvent_scaler = StandardScaler()\n",
    "        self.scaled_all_solvents = self.solvent_scaler.fit_transform(self.all_solvent_features)\n",
    "        \n",
    "        # Compute mean inter-solvent distance for normalization\n",
    "        self.mean_inter_solvent_dist = np.mean(pdist(self.scaled_all_solvents))\n",
    "        print(f\"Mean inter-solvent distance: {self.mean_inter_solvent_dist:.4f}\")\n",
    "        \n",
    "        # Fit NN on ALL solvents\n",
    "        self.nn_model = NearestNeighbors(n_neighbors=3, metric='euclidean')\n",
    "        self.nn_model.fit(self.scaled_all_solvents)\n",
    "        \n",
    "        self.train_Y = None\n",
    "        self.input_scaler = StandardScaler()\n",
    "    \n",
    "    def compute_extrapolation_score(self, X):\n",
    "        \"\"\"Compute extrapolation score based on distance to nearest solvents from ALL solvents.\"\"\"\n",
    "        if self.data_type == 'single':\n",
    "            solvent_names = X[\"SOLVENT NAME\"].values\n",
    "            test_features = SPANGE_DF.loc[solvent_names].values\n",
    "        else:\n",
    "            # For mixtures, use weighted average of solvent features\n",
    "            solvent_a_names = X[\"SOLVENT A NAME\"].values\n",
    "            solvent_b_names = X[\"SOLVENT B NAME\"].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1) / 100\n",
    "            feat_a = SPANGE_DF.loc[solvent_a_names].values\n",
    "            feat_b = SPANGE_DF.loc[solvent_b_names].values\n",
    "            test_features = feat_a * (1 - pct) + feat_b * pct\n",
    "        \n",
    "        # Scale test features using the same scaler fitted on ALL solvents\n",
    "        test_scaled = self.solvent_scaler.transform(test_features)\n",
    "        \n",
    "        # Compute distance to nearest 3 solvents (from ALL solvents)\n",
    "        distances, indices = self.nn_model.kneighbors(test_scaled)\n",
    "        avg_dist = distances.mean(axis=1)\n",
    "        \n",
    "        # Normalize by mean inter-solvent distance\n",
    "        normalized_dist = avg_dist / self.mean_inter_solvent_dist\n",
    "        \n",
    "        return normalized_dist\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        self.train_Y = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "        \n",
    "        X_featurized = self.smiles_featurizer.featurize(X_train)\n",
    "        X_scaled = self.input_scaler.fit_transform(X_featurized.numpy())\n",
    "        X_tensor = torch.tensor(X_scaled)\n",
    "        Y_tensor = torch.tensor(y_train.values if hasattr(y_train, 'values') else y_train)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.train()\n",
    "        for epoch in range(200):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(batch_X)\n",
    "                loss = criterion(pred, batch_Y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_featurized = self.smiles_featurizer.featurize(X)\n",
    "            X_scaled = self.input_scaler.transform(X_featurized.numpy())\n",
    "            X_tensor = torch.tensor(X_scaled)\n",
    "            raw_pred = self.model(X_tensor).numpy()\n",
    "        \n",
    "        # Compute extrapolation scores\n",
    "        extrap_scores = self.compute_extrapolation_score(X)\n",
    "        \n",
    "        # Only blend for TRUE outliers (normalized_dist > 1.0 + threshold)\n",
    "        # blend_weight = 0 for normal solvents, increases for outliers\n",
    "        blend_weights = np.clip((extrap_scores - 1.0) / self.blend_threshold, 0, 1)\n",
    "        \n",
    "        # Compute population mean from training data\n",
    "        mean_pred = self.train_Y.mean(axis=0)\n",
    "        \n",
    "        # Blend: for outliers, move toward mean\n",
    "        blended = (1 - blend_weights.reshape(-1, 1)) * raw_pred + blend_weights.reshape(-1, 1) * mean_pred\n",
    "        \n",
    "        # Debug: print blend weights for first few samples\n",
    "        if len(extrap_scores) <= 20:\n",
    "            print(f\"  Extrap scores: {extrap_scores[:5]}\")\n",
    "            print(f\"  Blend weights: {blend_weights[:5]}\")\n",
    "        \n",
    "        return torch.tensor(blended)\n",
    "\n",
    "print('FixedExtrapolationAwareMLPModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = FixedExtrapolationAwareMLPModel(blend_threshold=2.0) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"Single solvent predictions shape: {submission_single_solvent.shape}\")\n",
    "\n",
    "# Calculate CV score for single solvent\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_mse = []\n",
    "for fold_idx, split in enumerate(split_generator):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    fold_preds = submission_single_solvent[submission_single_solvent['fold'] == fold_idx]\n",
    "    pred_values = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    true_values = test_Y.values\n",
    "    mse = ((pred_values - true_values) ** 2).mean()\n",
    "    all_mse.append(mse)\n",
    "print(f\"Single solvent CV MSE: {np.mean(all_mse):.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc77a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = FixedExtrapolationAwareMLPModel(data='full', blend_threshold=2.0) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"Full data predictions shape: {submission_full_data.shape}\")\n",
    "\n",
    "# Calculate CV score for full data\n",
    "X, Y = load_data(\"full\")\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_mse = []\n",
    "for fold_idx, split in enumerate(split_generator):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    fold_preds = submission_full_data[submission_full_data['fold'] == fold_idx]\n",
    "    pred_values = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    true_values = test_Y.values\n",
    "    mse = ((pred_values - true_values) ** 2).mean()\n",
    "    all_mse.append(mse)\n",
    "print(f\"Full data CV MSE: {np.mean(all_mse):.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa01dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to /home/submission/\n",
    "import shutil\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "\n",
    "print(f\"Submission saved. Shape: {submission.shape}\")\n",
    "print(f\"Predictions range: target_1 [{submission['target_1'].min():.3f}, {submission['target_1'].max():.3f}]\")\n",
    "print(f\"Predictions range: target_2 [{submission['target_2'].min():.3f}, {submission['target_2'].max():.3f}]\")\n",
    "print(f\"Predictions range: target_3 [{submission['target_3'].min():.3f}, {submission['target_3'].max():.3f}]\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
