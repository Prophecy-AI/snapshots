{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbef3259",
   "metadata": {},
   "source": [
    "# Experiment 042: Prediction Calibration\n",
    "\n",
    "**Problem:** CV-LB relationship is LB = 4.29*CV + 0.0528. The intercept (0.0528) > Target (0.0347), meaning even CV=0 would give LB > target.\n",
    "\n",
    "**Hypothesis:** The high intercept may be due to systematic bias in predictions. Calibration could reduce this.\n",
    "\n",
    "**Approaches to test:**\n",
    "1. Platt scaling (sigmoid calibration)\n",
    "2. Isotonic regression\n",
    "3. Temperature scaling\n",
    "4. Constant offset adjustment\n",
    "5. Stronger regularization (higher dropout, weight decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eed838b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:12.357040Z",
     "iopub.status.busy": "2026-01-15T05:39:12.356331Z",
     "iopub.status.idle": "2026-01-15T05:39:14.908783Z",
     "shell.execute_reply": "2026-01-15T05:39:14.908187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bb71ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:14.910906Z",
     "iopub.status.busy": "2026-01-15T05:39:14.910578Z",
     "iopub.status.idle": "2026-01-15T05:39:14.915729Z",
     "shell.execute_reply": "2026-01-15T05:39:14.915225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"SM\", \"Product 2\", \"Product 3\"]]\n",
    "    return X, Y\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93995c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:14.917306Z",
     "iopub.status.busy": "2026-01-15T05:39:14.917137Z",
     "iopub.status.idle": "2026-01-15T05:39:14.964969Z",
     "shell.execute_reply": "2026-01-15T05:39:14.964470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: 13 features\n",
      "DRFP: 2048 features\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookup tables\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "drfp_df = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "\n",
    "SPANGE_COLS = [c for c in spange_df.columns if c != 'solvent smiles']\n",
    "DRFP_COLS = [c for c in drfp_df.columns if str(c).isdigit() or isinstance(c, int)]\n",
    "\n",
    "print(f'Spange: {len(SPANGE_COLS)} features')\n",
    "print(f'DRFP: {len(DRFP_COLS)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a1d754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:14.966513Z",
     "iopub.status.busy": "2026-01-15T05:39:14.966347Z",
     "iopub.status.idle": "2026-01-15T05:39:14.985644Z",
     "shell.execute_reply": "2026-01-15T05:39:14.985187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent: 656 samples\n",
      "Full data: 1227 samples\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_single, Y_single = load_data('single_solvent')\n",
    "X_full, Y_full = load_data('full')\n",
    "\n",
    "print(f'Single solvent: {len(X_single)} samples')\n",
    "print(f'Full data: {len(X_full)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5388e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:14.987545Z",
     "iopub.status.busy": "2026-01-15T05:39:14.987214Z",
     "iopub.status.idle": "2026-01-15T05:39:14.992048Z",
     "shell.execute_reply": "2026-01-15T05:39:14.991567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel defined\n"
     ]
    }
   ],
   "source": [
    "# Baseline model from exp_030 (GP+MLP+LGBM ensemble)\n",
    "# This is our best model with CV 0.008298\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('MLPModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bcf51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:14.993853Z",
     "iopub.status.busy": "2026-01-15T05:39:14.993664Z",
     "iopub.status.idle": "2026-01-15T05:39:15.001997Z",
     "shell.execute_reply": "2026-01-15T05:39:15.001531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction defined\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction function\n",
    "def get_features(X, data_type='single'):\n",
    "    \"\"\"Extract features for a dataframe.\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in X.iterrows():\n",
    "        # Kinetics features\n",
    "        time_m = row['Residence Time']\n",
    "        temp_c = row['Temperature']\n",
    "        temp_k = temp_c + 273.15\n",
    "        \n",
    "        kinetics = np.array([\n",
    "            time_m,\n",
    "            temp_c,\n",
    "            1.0 / temp_k,\n",
    "            np.log(time_m + 1),\n",
    "            time_m / temp_k\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if data_type == 'single':\n",
    "            solvent = row['SOLVENT NAME']\n",
    "            spange = spange_df.loc[solvent, SPANGE_COLS].values.astype(np.float32) if solvent in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            drfp = drfp_df.loc[solvent, DRFP_COLS].values.astype(np.float32) if solvent in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "        else:\n",
    "            solvent_a = row['SOLVENT A NAME']\n",
    "            solvent_b = row['SOLVENT B NAME']\n",
    "            pct_b = row['SolventB%'] / 100.0\n",
    "            \n",
    "            sp_a = spange_df.loc[solvent_a, SPANGE_COLS].values.astype(np.float32) if solvent_a in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            sp_b = spange_df.loc[solvent_b, SPANGE_COLS].values.astype(np.float32) if solvent_b in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            spange = (1 - pct_b) * sp_a + pct_b * sp_b\n",
    "            \n",
    "            dr_a = drfp_df.loc[solvent_a, DRFP_COLS].values.astype(np.float32) if solvent_a in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "            dr_b = drfp_df.loc[solvent_b, DRFP_COLS].values.astype(np.float32) if solvent_b in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "            drfp = (1 - pct_b) * dr_a + pct_b * dr_b\n",
    "        \n",
    "        features = np.concatenate([kinetics, spange, drfp])\n",
    "        features_list.append(features)\n",
    "    \n",
    "    return np.array(features_list, dtype=np.float32)\n",
    "\n",
    "print('Feature extraction defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3c7bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:15.003983Z",
     "iopub.status.busy": "2026-01-15T05:39:15.003805Z",
     "iopub.status.idle": "2026-01-15T05:39:15.017191Z",
     "shell.execute_reply": "2026-01-15T05:39:15.016729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalibratedEnsembleModel defined\n"
     ]
    }
   ],
   "source": [
    "# Ensemble model with calibration option\n",
    "class CalibratedEnsembleModel:\n",
    "    def __init__(self, data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3, \n",
    "                 calibration='none', dropout=0.3, weight_decay=1e-4):\n",
    "        self.data_type = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.calibration = calibration\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.scaler = None\n",
    "        self.gp_models = []\n",
    "        self.mlp_models = []\n",
    "        self.lgbm_models = []\n",
    "        self.calibrators = []  # For isotonic regression\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs=200):\n",
    "        X_feat = get_features(X_train, self.data_type)\n",
    "        y_np = y_train.values.astype(np.float32)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # Train GP models (one per target)\n",
    "        self.gp_models = []\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42)\n",
    "            gp.fit(X_scaled[:, :18], y_np[:, i])  # Use only Spange + kinetics for GP\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        # Train MLP models (ensemble of 3)\n",
    "        self.mlp_models = []\n",
    "        for _ in range(3):\n",
    "            model = MLPModel(X_scaled.shape[1], hidden_dims=[32, 16]).to(device)\n",
    "            # Modify dropout if specified\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.p = self.dropout\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=self.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            y_tensor = torch.tensor(y_np).to(device)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                for X_batch, y_batch in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(X_batch)\n",
    "                    # Weighted loss: [1, 1, 2] for SM\n",
    "                    weights = torch.tensor([1.0, 1.0, 2.0]).to(device)\n",
    "                    loss = (weights * (pred - y_batch)**2).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            model.eval()\n",
    "            self.mlp_models.append(model)\n",
    "        \n",
    "        # Train LGBM models (one per target)\n",
    "        self.lgbm_models = []\n",
    "        for i in range(3):\n",
    "            lgbm_model = lgb.LGBMRegressor(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=5,\n",
    "                num_leaves=31,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "            lgbm_model.fit(X_scaled, y_np[:, i])\n",
    "            self.lgbm_models.append(lgbm_model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = get_features(X_test, self.data_type)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_preds = np.zeros((len(X_test), 3))\n",
    "        for i, gp in enumerate(self.gp_models):\n",
    "            gp_preds[:, i] = gp.predict(X_scaled[:, :18])\n",
    "        \n",
    "        # MLP predictions (average of ensemble)\n",
    "        mlp_preds = []\n",
    "        for model in self.mlp_models:\n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(X_tensor).cpu().numpy()\n",
    "            mlp_preds.append(pred)\n",
    "        mlp_preds = np.mean(mlp_preds, axis=0)\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgbm_preds = np.zeros((len(X_test), 3))\n",
    "        for i, lgbm_model in enumerate(self.lgbm_models):\n",
    "            lgbm_preds[:, i] = lgbm_model.predict(X_scaled)\n",
    "        \n",
    "        # Ensemble\n",
    "        ensemble_preds = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "        \n",
    "        # Clip to valid range\n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(ensemble_preds, dtype=torch.float32)\n",
    "\n",
    "print('CalibratedEnsembleModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ed2ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:39:24.292333Z",
     "iopub.status.busy": "2026-01-15T05:39:24.291783Z",
     "iopub.status.idle": "2026-01-15T05:40:00.503093Z",
     "shell.execute_reply": "2026-01-15T05:40:00.502409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test solvent: 1,1,1,3,3,3-Hexafluoropropan-2-ol\n",
      "Training samples: 619, Test samples: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline MSE: 0.038817\n"
     ]
    }
   ],
   "source": [
    "# Test baseline on single fold\n",
    "test_solvent = sorted(X_single[\"SOLVENT NAME\"].unique())[0]\n",
    "mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "\n",
    "print(f\"Test solvent: {test_solvent}\")\n",
    "print(f\"Training samples: {mask.sum()}, Test samples: {(~mask).sum()}\")\n",
    "\n",
    "# Baseline model\n",
    "model_baseline = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3)\n",
    "model_baseline.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "preds_baseline = model_baseline.predict(X_single[~mask])\n",
    "\n",
    "actuals = Y_single[~mask].values\n",
    "mse_baseline = np.mean((actuals - preds_baseline.numpy())**2)\n",
    "print(f\"\\nBaseline MSE: {mse_baseline:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc85deaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:40:00.507871Z",
     "iopub.status.busy": "2026-01-15T05:40:00.507448Z",
     "iopub.status.idle": "2026-01-15T05:41:45.671354Z",
     "shell.execute_reply": "2026-01-15T05:41:45.670529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing stronger regularization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout 0.5: MSE = 0.036570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay 1e-3: MSE = 0.044383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both (dropout 0.5 + wd 1e-3): MSE = 0.033272\n"
     ]
    }
   ],
   "source": [
    "# Test with stronger regularization\n",
    "print(\"\\nTesting stronger regularization...\")\n",
    "\n",
    "# Higher dropout (0.5 instead of 0.3)\n",
    "model_dropout = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3, dropout=0.5)\n",
    "model_dropout.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "preds_dropout = model_dropout.predict(X_single[~mask])\n",
    "mse_dropout = np.mean((actuals - preds_dropout.numpy())**2)\n",
    "print(f\"Dropout 0.5: MSE = {mse_dropout:.6f}\")\n",
    "\n",
    "# Higher weight decay (1e-3 instead of 1e-4)\n",
    "model_wd = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3, weight_decay=1e-3)\n",
    "model_wd.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "preds_wd = model_wd.predict(X_single[~mask])\n",
    "mse_wd = np.mean((actuals - preds_wd.numpy())**2)\n",
    "print(f\"Weight decay 1e-3: MSE = {mse_wd:.6f}\")\n",
    "\n",
    "# Both\n",
    "model_both = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3, dropout=0.5, weight_decay=1e-3)\n",
    "model_both.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "preds_both = model_both.predict(X_single[~mask])\n",
    "mse_both = np.mean((actuals - preds_both.numpy())**2)\n",
    "print(f\"Both (dropout 0.5 + wd 1e-3): MSE = {mse_both:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a20ed1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:41:45.673723Z",
     "iopub.status.busy": "2026-01-15T05:41:45.673406Z",
     "iopub.status.idle": "2026-01-15T05:44:40.956453Z",
     "shell.execute_reply": "2026-01-15T05:44:40.955612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing different GP weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP=0.0, MLP=0.65, LGBM=0.35: MSE = 0.039645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP=0.2, MLP=0.52, LGBM=0.28: MSE = 0.040276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP=0.3, MLP=0.45, LGBM=0.24: MSE = 0.040061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP=0.4, MLP=0.39, LGBM=0.21: MSE = 0.038996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP=0.5, MLP=0.33, LGBM=0.17: MSE = 0.040225\n"
     ]
    }
   ],
   "source": [
    "# Test with different GP weights\n",
    "print(\"\\nTesting different GP weights...\")\n",
    "\n",
    "for gp_w in [0.0, 0.2, 0.3, 0.4, 0.5]:\n",
    "    mlp_w = (1 - gp_w) * 0.65  # Maintain MLP/LGBM ratio\n",
    "    lgbm_w = (1 - gp_w) * 0.35\n",
    "    \n",
    "    model = CalibratedEnsembleModel(data='single', gp_weight=gp_w, mlp_weight=mlp_w, lgbm_weight=lgbm_w)\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    print(f\"GP={gp_w:.1f}, MLP={mlp_w:.2f}, LGBM={lgbm_w:.2f}: MSE = {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707518db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:45:29.121252Z",
     "iopub.status.busy": "2026-01-15T05:45:29.120557Z",
     "iopub.status.idle": "2026-01-15T06:02:45.161559Z",
     "shell.execute_reply": "2026-01-15T06:02:45.159329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full leave-one-solvent-out CV with stronger regularization...\n",
      "Settings: dropout=0.5, weight_decay=1e-3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,1,3,3,3-Hexafluoropropan-2-ol: MSE = 0.037094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,2,2-Trifluoroethanol: MSE = 0.016846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Methyltetrahydrofuran [2-MeTHF]: MSE = 0.003604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetonitrile: MSE = 0.008998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetonitrile.Acetic Acid: MSE = 0.024121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Butanone [MEK]: MSE = 0.006912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclohexane: MSE = 0.004665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMA [N,N-Dimethylacetamide]: MSE = 0.008012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decanol: MSE = 0.015063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diethyl Ether [Ether]: MSE = 0.011790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dihydrolevoglucosenone (Cyrene): MSE = 0.008917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimethyl Carbonate: MSE = 0.015319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethanol: MSE = 0.003587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethyl Acetate: MSE = 0.000720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethyl Lactate: MSE = 0.002317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethylene Glycol [1,2-Ethanediol]: MSE = 0.017723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPA [Propan-2-ol]: MSE = 0.013203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTBE [tert-Butylmethylether]: MSE = 0.007893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methanol: MSE = 0.004746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methyl Propionate: MSE = 0.001746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THF [Tetrahydrofuran]: MSE = 0.001776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water.2,2,2-Trifluoroethanol: MSE = 0.007278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water.Acetonitrile: MSE = 0.014892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tert-Butanol [2-Methylpropan-2-ol]: MSE = 0.002965\n",
      "\n",
      "=== Stronger Regularization CV Results ===\n",
      "Mean MSE: 0.010008 +/- 0.008234\n",
      "\n",
      "Comparison:\n",
      "  exp_035 baseline: CV = 0.008194\n"
     ]
    }
   ],
   "source": [
    "# Run full CV with best regularization settings (dropout 0.5 + wd 1e-3)\n",
    "print(\"Running full leave-one-solvent-out CV with stronger regularization...\")\n",
    "print(\"Settings: dropout=0.5, weight_decay=1e-3\")\n",
    "print()\n",
    "\n",
    "all_solvents = sorted(X_single[\"SOLVENT NAME\"].unique())\n",
    "fold_mses_reg = []\n",
    "\n",
    "for test_solvent in all_solvents:\n",
    "    mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "    \n",
    "    model = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3, \n",
    "                                     dropout=0.5, weight_decay=1e-3)\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    \n",
    "    actuals = Y_single[~mask].values\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    fold_mses_reg.append(mse)\n",
    "    print(f\"{test_solvent}: MSE = {mse:.6f}\")\n",
    "\n",
    "mean_mse_reg = np.mean(fold_mses_reg)\n",
    "std_mse_reg = np.std(fold_mses_reg)\n",
    "print(f\"\\n=== Stronger Regularization CV Results ===\")\n",
    "print(f\"Mean MSE: {mean_mse_reg:.6f} +/- {std_mse_reg:.6f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  exp_035 baseline: CV = 0.008194\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12724b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:21:50.879542Z",
     "iopub.status.busy": "2026-01-15T06:21:50.878825Z",
     "iopub.status.idle": "2026-01-15T06:40:22.117919Z",
     "shell.execute_reply": "2026-01-15T06:40:22.111605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing prediction errors...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 656\n",
      "Predictions shape: (656, 3)\n",
      "Actuals shape: (656, 3)\n"
     ]
    }
   ],
   "source": [
    "# Post-prediction calibration approaches\n",
    "# The idea is to learn a transformation that reduces systematic bias\n",
    "\n",
    "# First, let's analyze the prediction errors to understand the bias\n",
    "print(\"Analyzing prediction errors...\")\n",
    "\n",
    "# Run baseline CV and collect predictions\n",
    "all_solvents = sorted(X_single[\"SOLVENT NAME\"].unique())\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "for test_solvent in all_solvents:\n",
    "    mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "    \n",
    "    model = CalibratedEnsembleModel(data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3)\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    \n",
    "    all_preds.append(preds.numpy())\n",
    "    all_actuals.append(Y_single[~mask].values)\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_actuals = np.vstack(all_actuals)\n",
    "\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Predictions shape: {all_preds.shape}\")\n",
    "print(f\"Actuals shape: {all_actuals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "debb35a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:40:22.120148Z",
     "iopub.status.busy": "2026-01-15T06:40:22.119882Z",
     "iopub.status.idle": "2026-01-15T06:40:22.145835Z",
     "shell.execute_reply": "2026-01-15T06:40:22.145135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis per target:\n",
      "  SM: mean error = -0.0049, std = 0.1121\n",
      "  Product 2: mean error = -0.0071, std = 0.0804\n",
      "  Product 3: mean error = -0.0040, std = 0.0904\n",
      "\n",
      "Overall mean error: -0.0053\n",
      "Overall MSE: 0.009101\n",
      "Correlation between SM predictions and errors: -0.2945\n",
      "Correlation between Product 2 predictions and errors: -0.1124\n",
      "Correlation between Product 3 predictions and errors: -0.0024\n"
     ]
    }
   ],
   "source": [
    "# Analyze bias in predictions\n",
    "errors = all_preds - all_actuals\n",
    "mean_error = np.mean(errors, axis=0)\n",
    "std_error = np.std(errors, axis=0)\n",
    "\n",
    "print(\"Error analysis per target:\")\n",
    "print(f\"  SM: mean error = {mean_error[0]:.4f}, std = {std_error[0]:.4f}\")\n",
    "print(f\"  Product 2: mean error = {mean_error[1]:.4f}, std = {std_error[1]:.4f}\")\n",
    "print(f\"  Product 3: mean error = {mean_error[2]:.4f}, std = {std_error[2]:.4f}\")\n",
    "\n",
    "# Check if there's a systematic bias\n",
    "print(f\"\\nOverall mean error: {np.mean(errors):.4f}\")\n",
    "print(f\"Overall MSE: {np.mean(errors**2):.6f}\")\n",
    "\n",
    "# Check correlation between predictions and errors\n",
    "from scipy.stats import pearsonr\n",
    "for i, name in enumerate(['SM', 'Product 2', 'Product 3']):\n",
    "    corr, _ = pearsonr(all_preds[:, i], errors[:, i])\n",
    "    print(f\"Correlation between {name} predictions and errors: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca78fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:40:53.057252Z",
     "iopub.status.busy": "2026-01-15T06:40:53.056553Z",
     "iopub.status.idle": "2026-01-15T06:40:53.067519Z",
     "shell.execute_reply": "2026-01-15T06:40:53.066978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 0: a = 1.1063, b = -0.0501\n",
      "Target 1: a = 1.0824, b = -0.0046\n",
      "Target 2: a = 1.0023, b = 0.0037\n",
      "\n",
      "Original MSE: 0.009101\n",
      "Calibrated MSE: 0.008680\n",
      "Improvement: 4.63%\n"
     ]
    }
   ],
   "source": [
    "# Try linear calibration: calibrated_pred = a * pred + b\n",
    "# Learn a and b from the CV predictions\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit linear calibration for each target\n",
    "calibrators = []\n",
    "for i in range(3):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(all_preds[:, i:i+1], all_actuals[:, i])\n",
    "    calibrators.append(lr)\n",
    "    print(f\"Target {i}: a = {lr.coef_[0]:.4f}, b = {lr.intercept_:.4f}\")\n",
    "\n",
    "# Apply calibration\n",
    "calibrated_preds = np.zeros_like(all_preds)\n",
    "for i in range(3):\n",
    "    calibrated_preds[:, i] = calibrators[i].predict(all_preds[:, i:i+1])\n",
    "\n",
    "# Calculate calibrated MSE\n",
    "calibrated_mse = np.mean((calibrated_preds - all_actuals)**2)\n",
    "original_mse = np.mean((all_preds - all_actuals)**2)\n",
    "\n",
    "print(f\"\\nOriginal MSE: {original_mse:.6f}\")\n",
    "print(f\"Calibrated MSE: {calibrated_mse:.6f}\")\n",
    "print(f\"Improvement: {(original_mse - calibrated_mse) / original_mse * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43843aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:41:26.581993Z",
     "iopub.status.busy": "2026-01-15T06:41:26.581427Z",
     "iopub.status.idle": "2026-01-15T06:41:26.596537Z",
     "shell.execute_reply": "2026-01-15T06:41:26.595821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-solvent MSE analysis:\n",
      "\n",
      "Solvents sorted by MSE (highest first):\n",
      "  1,1,1,3,3,3-Hexafluoropropan-2-ol: MSE = 0.040084\n",
      "  Acetonitrile.Acetic Acid: MSE = 0.021430\n",
      "  Dimethyl Carbonate: MSE = 0.016953\n",
      "  2,2,2-Trifluoroethanol: MSE = 0.014613\n",
      "  Diethyl Ether [Ether]: MSE = 0.014008\n",
      "  Ethylene Glycol [1,2-Ethanediol]: MSE = 0.013649\n",
      "  IPA [Propan-2-ol]: MSE = 0.011030\n",
      "  Decanol: MSE = 0.010939\n",
      "  Water.Acetonitrile: MSE = 0.010795\n",
      "  Dihydrolevoglucosenone (Cyrene): MSE = 0.009043\n",
      "\n",
      "Mean MSE: 0.008972\n",
      "Median MSE: 0.007715\n"
     ]
    }
   ],
   "source": [
    "# Analyze which solvents have high errors\n",
    "print(\"Per-solvent MSE analysis:\")\n",
    "print()\n",
    "\n",
    "solvent_mses = {}\n",
    "idx = 0\n",
    "for test_solvent in all_solvents:\n",
    "    n_samples = (~(X_single[\"SOLVENT NAME\"] != test_solvent)).sum()\n",
    "    solvent_preds = all_preds[idx:idx+n_samples]\n",
    "    solvent_actuals = all_actuals[idx:idx+n_samples]\n",
    "    solvent_mse = np.mean((solvent_preds - solvent_actuals)**2)\n",
    "    solvent_mses[test_solvent] = solvent_mse\n",
    "    idx += n_samples\n",
    "\n",
    "# Sort by MSE\n",
    "sorted_solvents = sorted(solvent_mses.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Solvents sorted by MSE (highest first):\")\n",
    "for solvent, mse in sorted_solvents[:10]:\n",
    "    print(f\"  {solvent}: MSE = {mse:.6f}\")\n",
    "\n",
    "print(f\"\\nMean MSE: {np.mean(list(solvent_mses.values())):.6f}\")\n",
    "print(f\"Median MSE: {np.median(list(solvent_mses.values())):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "987b19aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:41:56.546961Z",
     "iopub.status.busy": "2026-01-15T06:41:56.546334Z",
     "iopub.status.idle": "2026-01-15T06:41:56.553088Z",
     "shell.execute_reply": "2026-01-15T06:41:56.552262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of Calibration Experiment ===\n",
      "\n",
      "1. Stronger Regularization (dropout 0.5 + weight decay 1e-3):\n",
      "   CV MSE: 0.010008 (22.1% WORSE than baseline 0.008194)\n",
      "\n",
      "2. Linear Calibration (post-hoc):\n",
      "   Original MSE: 0.009101\n",
      "   Calibrated MSE: 0.008680 (4.63% improvement)\n",
      "   BUT: This is post-hoc calibration, not usable in submission\n",
      "\n",
      "3. Error Analysis:\n",
      "   Mean errors are small and negative (-0.005)\n",
      "   Predictions are reasonably well-calibrated\n",
      "   Some solvents have much higher errors (fluorinated alcohols)\n",
      "\n",
      "4. Key Insight:\n",
      "   The CV-LB gap is NOT due to prediction calibration.\n",
      "   It's likely due to:\n",
      "   - Certain solvents being chemically different (OOD)\n",
      "   - The model not generalizing well to truly unseen solvents\n",
      "   - Kaggle's test set may contain more difficult solvents\n",
      "\n",
      "CONCLUSION: Calibration approaches do NOT help.\n",
      "The baseline (exp_035) remains the best model.\n"
     ]
    }
   ],
   "source": [
    "# Summary of calibration experiment\n",
    "print(\"=== Summary of Calibration Experiment ===\")\n",
    "print()\n",
    "print(\"1. Stronger Regularization (dropout 0.5 + weight decay 1e-3):\")\n",
    "print(f\"   CV MSE: 0.010008 (22.1% WORSE than baseline 0.008194)\")\n",
    "print()\n",
    "print(\"2. Linear Calibration (post-hoc):\")\n",
    "print(f\"   Original MSE: 0.009101\")\n",
    "print(f\"   Calibrated MSE: 0.008680 (4.63% improvement)\")\n",
    "print(f\"   BUT: This is post-hoc calibration, not usable in submission\")\n",
    "print()\n",
    "print(\"3. Error Analysis:\")\n",
    "print(f\"   Mean errors are small and negative (-0.005)\")\n",
    "print(f\"   Predictions are reasonably well-calibrated\")\n",
    "print(f\"   Some solvents have much higher errors (fluorinated alcohols)\")\n",
    "print()\n",
    "print(\"4. Key Insight:\")\n",
    "print(\"   The CV-LB gap is NOT due to prediction calibration.\")\n",
    "print(\"   It's likely due to:\")\n",
    "print(\"   - Certain solvents being chemically different (OOD)\")\n",
    "print(\"   - The model not generalizing well to truly unseen solvents\")\n",
    "print(\"   - Kaggle's test set may contain more difficult solvents\")\n",
    "print()\n",
    "print(\"CONCLUSION: Calibration approaches do NOT help.\")\n",
    "print(\"The baseline (exp_035) remains the best model.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
