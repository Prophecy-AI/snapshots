{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360bacb5",
   "metadata": {},
   "source": [
    "# Experiment 058: Extrapolation Detection + Conservative Predictions\n",
    "\n",
    "**Goal:** Reduce the CV-LB intercept by detecting when we're extrapolating and making conservative predictions.\n",
    "\n",
    "**Key insight:** The CV-LB relationship is LB = 4.31*CV + 0.0525. The intercept (0.0525) represents extrapolation error. If we can detect when we're extrapolating and make more conservative predictions, we might reduce this intercept.\n",
    "\n",
    "**Approach:**\n",
    "1. Compute solvent similarity using fingerprints (Tanimoto similarity)\n",
    "2. For each test solvent, find its similarity to nearest training solvents\n",
    "3. When similarity is low (extrapolating), blend predictions toward population mean\n",
    "4. This should reduce error on \"hard\" test solvents\n",
    "\n",
    "**Hypothesis:** Conservative predictions for outlier solvents will reduce the intercept, not just improve CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40b66b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:02.641903Z",
     "iopub.status.busy": "2026-01-16T00:13:02.641359Z",
     "iopub.status.idle": "2026-01-16T00:13:03.990094Z",
     "shell.execute_reply": "2026-01-16T00:13:03.989635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import tqdm\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd867ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:03.991321Z",
     "iopub.status.busy": "2026-01-16T00:13:03.991142Z",
     "iopub.status.idle": "2026-01-16T00:13:03.994113Z",
     "shell.execute_reply": "2026-01-16T00:13:03.993776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants defined.\n"
     ]
    }
   ],
   "source": [
    "# Constants from official template\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT A NAME\",\n",
    "    \"SOLVENT B NAME\",\n",
    "    \"SolventB%\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "    \"SOLVENT NAME\",\n",
    "]\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\n",
    "    \"Residence Time\",\n",
    "    \"Temperature\",\n",
    "]\n",
    "\n",
    "TARGET_LABELS = [\n",
    "    \"Product 2\",\n",
    "    \"Product 3\",\n",
    "    \"SM\",\n",
    "]\n",
    "\n",
    "print(\"Constants defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffff04b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:03.995056Z",
     "iopub.status.busy": "2026-01-16T00:13:03.994967Z",
     "iopub.status.idle": "2026-01-16T00:13:04.000050Z",
     "shell.execute_reply": "2026-01-16T00:13:03.999671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and CV functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "# CV functions from official template\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = ~((X[\"SOLVENT A NAME\"] == solvent_pair[\"SOLVENT A NAME\"]) & \n",
    "                           (X[\"SOLVENT B NAME\"] == solvent_pair[\"SOLVENT B NAME\"]))\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading and CV functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b483d2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:04.000921Z",
     "iopub.status.busy": "2026-01-16T00:13:04.000819Z",
     "iopub.status.idle": "2026-01-16T00:13:04.003871Z",
     "shell.execute_reply": "2026-01-16T00:13:04.003505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes defined.\n"
     ]
    }
   ],
   "source": [
    "# Base classes from official template\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3c81aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:04.004677Z",
     "iopub.status.busy": "2026-01-16T00:13:04.004583Z",
     "iopub.status.idle": "2026-01-16T00:13:04.045662Z",
     "shell.execute_reply": "2026-01-16T00:13:04.045262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13)\n",
      "DRFP: (24, 2048)\n",
      "ACS PCA: (24, 5)\n",
      "Fragprints: (24, 2133)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "FRAGPRINTS_DF = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'DRFP: {DRFP_DF.shape}')\n",
    "print(f'ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Fragprints: {FRAGPRINTS_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f275d09c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:04.046614Z",
     "iopub.status.busy": "2026-01-16T00:13:04.046515Z",
     "iopub.status.idle": "2026-01-16T00:13:04.068829Z",
     "shell.execute_reply": "2026-01-16T00:13:04.068455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix shape: (24, 24)\n",
      "\n",
      "Sample similarities for 'Acetonitrile':\n",
      "Water.Acetonitrile             1.000000\n",
      "Acetonitrile                   1.000000\n",
      "Acetonitrile.Acetic Acid       0.444444\n",
      "DMA [N,N-Dimethylacetamide]    0.100000\n",
      "Methanol                       0.083333\n",
      "Name: Acetonitrile, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute Tanimoto similarity between solvents using fingerprints\n",
    "def compute_tanimoto_similarity(fp1, fp2):\n",
    "    \"\"\"Compute Tanimoto similarity between two binary fingerprints.\"\"\"\n",
    "    intersection = np.sum(np.minimum(fp1, fp2))\n",
    "    union = np.sum(np.maximum(fp1, fp2))\n",
    "    if union == 0:\n",
    "        return 1.0  # Both are zero vectors\n",
    "    return intersection / union\n",
    "\n",
    "def compute_solvent_similarity_matrix(fingerprints_df):\n",
    "    \"\"\"Compute pairwise Tanimoto similarity matrix for all solvents.\"\"\"\n",
    "    solvents = fingerprints_df.index.tolist()\n",
    "    n = len(solvents)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            fp1 = fingerprints_df.iloc[i].values\n",
    "            fp2 = fingerprints_df.iloc[j].values\n",
    "            similarity_matrix[i, j] = compute_tanimoto_similarity(fp1, fp2)\n",
    "    \n",
    "    return pd.DataFrame(similarity_matrix, index=solvents, columns=solvents)\n",
    "\n",
    "# Compute similarity matrix using fragprints\n",
    "SIMILARITY_MATRIX = compute_solvent_similarity_matrix(FRAGPRINTS_DF)\n",
    "print(f\"Similarity matrix shape: {SIMILARITY_MATRIX.shape}\")\n",
    "print(f\"\\nSample similarities for 'Acetonitrile':\")\n",
    "print(SIMILARITY_MATRIX.loc['Acetonitrile'].sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8460322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:04.069827Z",
     "iopub.status.busy": "2026-01-16T00:13:04.069713Z",
     "iopub.status.idle": "2026-01-16T00:13:04.078854Z",
     "shell.execute_reply": "2026-01-16T00:13:04.078491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtrapolationAwareModel defined.\n"
     ]
    }
   ],
   "source": [
    "# Extrapolation-aware model\n",
    "class ExtrapolationAwareModel(BaseModel):\n",
    "    \"\"\"Model that detects extrapolation and makes conservative predictions.\n",
    "    \n",
    "    When predicting for a solvent that is dissimilar to training solvents,\n",
    "    blend the prediction toward the population mean.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', blend_threshold=0.5, blend_strength=0.3):\n",
    "        self.data = data\n",
    "        self.blend_threshold = blend_threshold  # Similarity below this triggers blending\n",
    "        self.blend_strength = blend_strength  # How much to blend toward mean (0-1)\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.similarity_matrix = SIMILARITY_MATRIX\n",
    "        self.models = None\n",
    "        self.scaler = None\n",
    "        self.train_mean = None\n",
    "        self.train_solvents = None\n",
    "    \n",
    "    def _prepare_features(self, X):\n",
    "        \"\"\"Prepare features for the model.\"\"\"\n",
    "        # Numeric features with Arrhenius engineering\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.data == 'single':\n",
    "            # Single solvent features\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            return np.hstack([X_kinetic, X_spange])\n",
    "        else:\n",
    "            # Mixed solvent features (weighted average)\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1) / 100.0\n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "            return np.hstack([X_kinetic, pct, X_spange])\n",
    "    \n",
    "    def _get_max_similarity_to_training(self, test_solvent, train_solvents):\n",
    "        \"\"\"Get maximum similarity between test solvent and any training solvent.\"\"\"\n",
    "        if test_solvent not in self.similarity_matrix.index:\n",
    "            return 0.0  # Unknown solvent - maximum extrapolation\n",
    "        \n",
    "        similarities = []\n",
    "        for train_solvent in train_solvents:\n",
    "            if train_solvent in self.similarity_matrix.columns:\n",
    "                sim = self.similarity_matrix.loc[test_solvent, train_solvent]\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        if not similarities:\n",
    "            return 0.0\n",
    "        return max(similarities)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        \"\"\"Train CatBoost + XGBoost ensemble.\"\"\"\n",
    "        X_features = self._prepare_features(train_X)\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        # Store training mean for blending\n",
    "        self.train_mean = Y_np.mean(axis=0)\n",
    "        \n",
    "        # Store training solvents for similarity computation\n",
    "        if self.data == 'single':\n",
    "            self.train_solvents = train_X[\"SOLVENT NAME\"].unique().tolist()\n",
    "        else:\n",
    "            self.train_solvents = list(set(train_X[\"SOLVENT A NAME\"].tolist() + train_X[\"SOLVENT B NAME\"].tolist()))\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_features)\n",
    "        \n",
    "        # Train CatBoost models\n",
    "        self.cat_models = []\n",
    "        for i in range(3):\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                depth=6,\n",
    "                learning_rate=0.05,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "            )\n",
    "            model.fit(X_scaled, Y_np[:, i])\n",
    "            self.cat_models.append(model)\n",
    "        \n",
    "        # Train XGBoost models\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=400,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "            )\n",
    "            model.fit(X_scaled, Y_np[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"Predict with extrapolation-aware blending.\"\"\"\n",
    "        X_features = self._prepare_features(test_X)\n",
    "        X_scaled = self.scaler.transform(X_features)\n",
    "        \n",
    "        # Get base predictions from ensemble\n",
    "        cat_preds = np.column_stack([m.predict(X_scaled) for m in self.cat_models])\n",
    "        xgb_preds = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # Ensemble: 60% CatBoost + 40% XGBoost\n",
    "        base_preds = 0.6 * cat_preds + 0.4 * xgb_preds\n",
    "        \n",
    "        # Compute extrapolation score for each sample\n",
    "        if self.data == 'single':\n",
    "            test_solvents = test_X[\"SOLVENT NAME\"].values\n",
    "            extrapolation_scores = []\n",
    "            for solvent in test_solvents:\n",
    "                max_sim = self._get_max_similarity_to_training(solvent, self.train_solvents)\n",
    "                # Extrapolation score: 0 = similar to training, 1 = very different\n",
    "                extrapolation_scores.append(1.0 - max_sim)\n",
    "            extrapolation_scores = np.array(extrapolation_scores).reshape(-1, 1)\n",
    "        else:\n",
    "            # For mixed solvents, use average similarity of both components\n",
    "            test_solvents_A = test_X[\"SOLVENT A NAME\"].values\n",
    "            test_solvents_B = test_X[\"SOLVENT B NAME\"].values\n",
    "            extrapolation_scores = []\n",
    "            for solv_a, solv_b in zip(test_solvents_A, test_solvents_B):\n",
    "                max_sim_a = self._get_max_similarity_to_training(solv_a, self.train_solvents)\n",
    "                max_sim_b = self._get_max_similarity_to_training(solv_b, self.train_solvents)\n",
    "                avg_sim = (max_sim_a + max_sim_b) / 2\n",
    "                extrapolation_scores.append(1.0 - avg_sim)\n",
    "            extrapolation_scores = np.array(extrapolation_scores).reshape(-1, 1)\n",
    "        \n",
    "        # Blend toward mean based on extrapolation score\n",
    "        # When extrapolation_score > threshold, blend more toward mean\n",
    "        blend_weights = np.clip(\n",
    "            (extrapolation_scores - self.blend_threshold) / (1.0 - self.blend_threshold),\n",
    "            0.0, 1.0\n",
    "        ) * self.blend_strength\n",
    "        \n",
    "        # Final predictions: blend between base prediction and training mean\n",
    "        final_preds = (1 - blend_weights) * base_preds + blend_weights * self.train_mean\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        final_preds = np.clip(final_preds, 0.0, 1.0)\n",
    "        \n",
    "        return torch.tensor(final_preds, dtype=torch.double)\n",
    "\n",
    "print(\"ExtrapolationAwareModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a3eb01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:13:15.260095Z",
     "iopub.status.busy": "2026-01-16T00:13:15.259555Z",
     "iopub.status.idle": "2026-01-16T00:13:16.370404Z",
     "shell.execute_reply": "2026-01-16T00:13:16.369986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ExtrapolationAwareModel...\n",
      "Single solvent data: X=(656, 3), Y=(656, 3)\n",
      "Test solvent: 1,1,1,3,3,3-Hexafluoropropan-2-ol\n",
      "Training solvents: ['Methanol' 'Ethylene Glycol [1,2-Ethanediol]'\n",
      " '2-Methyltetrahydrofuran [2-MeTHF]' 'Cyclohexane' 'IPA [Propan-2-ol]']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([37, 3])\n",
      "Predictions sample: tensor([0.0258, 0.0194, 0.9150])\n",
      "Actual sample: [0.01692854 0.02519112 0.95783321]\n",
      "Training mean: [0.13978304 0.11369471 0.54321009]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "print(\"Testing ExtrapolationAwareModel...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "print(f\"Test solvent: {test_X['SOLVENT NAME'].iloc[0]}\")\n",
    "print(f\"Training solvents: {train_X['SOLVENT NAME'].unique()[:5]}...\")\n",
    "\n",
    "model = ExtrapolationAwareModel(blend_threshold=0.5, blend_strength=0.3)\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions sample: {preds[0]}\")\n",
    "print(f\"Actual sample: {test_Y.iloc[0].values}\")\n",
    "print(f\"Training mean: {model.train_mean}\")\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExtrapolationAwareModel(blend_threshold=0.5, blend_strength=0.3) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Single solvent predictions: {len(submission_single_solvent)}\")\n",
    "print(f\"Unique folds: {submission_single_solvent['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExtrapolationAwareModel(data='full', blend_threshold=0.5, blend_strength=0.3) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Full data predictions: {len(submission_full_data)}\")\n",
    "print(f\"Unique folds: {submission_full_data['fold'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96940bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV for logging\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CV CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single solvent CV\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X, Y)):\n",
    "    model = ExtrapolationAwareModel(blend_threshold=0.5, blend_strength=0.3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    fold_mses.append(mse)\n",
    "    if fold_idx % 5 == 0:\n",
    "        print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "single_cv = np.mean(fold_mses)\n",
    "single_std = np.std(fold_mses)\n",
    "print(f\"\\nSingle solvent CV MSE: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "\n",
    "# Full data CV\n",
    "X, Y = load_data(\"full\")\n",
    "full_fold_mses = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_ramp_out_splits(X, Y)):\n",
    "    model = ExtrapolationAwareModel(data='full', blend_threshold=0.5, blend_strength=0.3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    full_fold_mses.append(mse)\n",
    "    print(f\"  Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "full_cv = np.mean(full_fold_mses)\n",
    "full_std = np.std(full_fold_mses)\n",
    "print(f\"\\nFull data CV MSE: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "print(f\"\\nFINAL CV FOR LOGGING: {single_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5374204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 058: EXTRAPOLATION DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAPPROACH:\")\n",
    "print(\"  - Compute Tanimoto similarity between solvents using fragprints\")\n",
    "print(\"  - When test solvent is dissimilar to training solvents (extrapolating),\")\n",
    "print(\"    blend predictions toward population mean\")\n",
    "print(\"  - blend_threshold=0.5, blend_strength=0.3\")\n",
    "print(\"  - Base model: CatBoost + XGBoost ensemble (60:40)\")\n",
    "\n",
    "print(f\"\\nCV SCORES:\")\n",
    "print(f\"  Single solvent: {single_cv:.6f} ± {single_std:.6f}\")\n",
    "print(f\"  Full data: {full_cv:.6f} ± {full_std:.6f}\")\n",
    "\n",
    "# Predicted LB using the CV-LB relationship\n",
    "predicted_lb = 4.31 * single_cv + 0.0525\n",
    "print(f\"\\nPREDICTED LB (using CV-LB relationship): {predicted_lb:.4f}\")\n",
    "print(f\"  Best LB so far: 0.0877\")\n",
    "print(f\"  Target: 0.0347\")\n",
    "\n",
    "print(\"\\nHYPOTHESIS:\")\n",
    "print(\"  Conservative predictions for outlier solvents should reduce the intercept,\")\n",
    "print(\"  not just improve CV. This could change the CV-LB relationship.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
