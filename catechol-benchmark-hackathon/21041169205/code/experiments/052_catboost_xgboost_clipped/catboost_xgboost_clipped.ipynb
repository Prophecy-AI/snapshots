{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a413c4",
   "metadata": {},
   "source": [
    "# Experiment 052: CatBoost + XGBoost Ensemble (WITH CLIPPING)\n",
    "\n",
    "**Goal:** Regenerate exp_050's submission with proper clipping to [0, 1].\n",
    "\n",
    "**Issue:** Previous submissions may have failed due to target values > 1.0.\n",
    "\n",
    "**Fix:** Clip all target values to [0, 1] before saving submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24561839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define constants\n",
    "DATA_PATH = \"/home/data\"\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "# Load data\n",
    "def load_data_local(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features_local(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "# Official CV functions\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield ((X[train_idcs_mask], Y[train_idcs_mask]), (X[~train_idcs_mask], Y[~train_idcs_mask]))\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).any(axis=1)\n",
    "        yield ((X[train_idcs_mask], Y[train_idcs_mask]), (X[~train_idcs_mask], Y[~train_idcs_mask]))\n",
    "\n",
    "# Load data\n",
    "X_single, Y_single = load_data_local(\"single_solvent\")\n",
    "X_full, Y_full = load_data_local(\"full\")\n",
    "spange = load_features_local(\"spange_descriptors\")\n",
    "\n",
    "print(f\"Single solvent: X={X_single.shape}, Y={Y_single.shape}\")\n",
    "print(f\"Full data: X={X_full.shape}, Y={Y_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (same as exp_050)\n",
    "\n",
    "def feature_priority(name: str) -> int:\n",
    "    if name.startswith(\"spange_\"): return 5\n",
    "    if name.startswith(\"acs_\"): return 4\n",
    "    if name.startswith(\"drfps_\"): return 3\n",
    "    if name.startswith(\"frag_\"): return 2\n",
    "    if name.startswith(\"smiles_\"): return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.90):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.shape[1] == 0: return df, []\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols: numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            if upper.iloc[i, j] > threshold:\n",
    "                if col_i in to_drop or col_j in to_drop: continue\n",
    "                p_i, p_j = feature_priority(col_i), feature_priority(col_j)\n",
    "                if p_i > p_j: to_drop.add(col_j)\n",
    "                elif p_j > p_i: to_drop.add(col_i)\n",
    "                else:\n",
    "                    idx_i = df.columns.get_loc(col_i) if col_i in df.columns else 999\n",
    "                    idx_j = df.columns.get_loc(col_j) if col_j in df.columns else 999\n",
    "                    to_drop.add(col_i if idx_i > idx_j else col_j)\n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    return df.drop(columns=all_to_drop, errors=\"ignore\"), all_to_drop\n",
    "\n",
    "def add_numeric_features(X_num):\n",
    "    X_num = X_num.copy()\n",
    "    if {\"Temperature\", \"Residence Time\"} <= set(X_num.columns):\n",
    "        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n",
    "        T, rt = X_num[\"Temperature\"], X_num[\"Residence Time\"]\n",
    "        X_num[\"T_x_RT\"] = T * rt\n",
    "        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "        X_num[\"T_inv\"] = 1 / T\n",
    "        X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    return X_num\n",
    "\n",
    "def build_solvent_feature_table(threshold=0.90):\n",
    "    sources = [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    dfs = []\n",
    "    for src in sources:\n",
    "        df_src = load_features_local(src).copy()\n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"}, errors=\"ignore\")\n",
    "            drop_cols = values.sum(axis=0)[values.sum(axis=0) == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols, errors=\"ignore\")\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        elif src == \"spange_descriptors\":\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"spange_{c}\" for c in cols_to_rename})\n",
    "        elif src == \"acs_pca_descriptors\":\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"acs_{c}\" for c in cols_to_rename})\n",
    "        elif src == \"smiles\":\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"smiles_{c}\" for c in cols_to_rename})\n",
    "        dfs.append(df_src)\n",
    "    from functools import reduce\n",
    "    merged = reduce(lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"), dfs)\n",
    "    merged_filtered, _ = filter_correlated_features(merged, threshold=threshold)\n",
    "    return merged_filtered\n",
    "\n",
    "solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "print(f\"Solvent table shape: {solvent_table.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f35931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer and Model classes (same as exp_050)\n",
    "\n",
    "class CombinedFeaturizer:\n",
    "    def __init__(self, solvent_table, data='single'):\n",
    "        self.solvent_table = solvent_table\n",
    "        self.data_mode = data\n",
    "        self.scaler = None\n",
    "    \n",
    "    def featurize(self, X, fit_scaler=False):\n",
    "        X = X.copy()\n",
    "        if self.data_mode == 'single':\n",
    "            X_merged = X.merge(self.solvent_table, on='SOLVENT NAME', how='left')\n",
    "            numeric_cols = [c for c in X_merged.columns if c != 'SOLVENT NAME' and X_merged[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "            X_numeric = X_merged[numeric_cols].copy()\n",
    "        else:\n",
    "            solvent_A = self.solvent_table.copy().rename(columns={'SOLVENT NAME': 'SOLVENT A NAME'})\n",
    "            solvent_A.columns = ['SOLVENT A NAME'] + [f'{c}_A' for c in solvent_A.columns if c != 'SOLVENT A NAME']\n",
    "            solvent_B = self.solvent_table.copy().rename(columns={'SOLVENT NAME': 'SOLVENT B NAME'})\n",
    "            solvent_B.columns = ['SOLVENT B NAME'] + [f'{c}_B' for c in solvent_B.columns if c != 'SOLVENT B NAME']\n",
    "            X_merged = X.merge(solvent_A, on='SOLVENT A NAME', how='left').merge(solvent_B, on='SOLVENT B NAME', how='left')\n",
    "            numeric_cols = [c for c in X_merged.columns if c not in ['SOLVENT A NAME', 'SOLVENT B NAME'] and X_merged[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
    "            X_numeric = X_merged[numeric_cols].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        X_np = np.nan_to_num(X_numeric.values.astype(np.float64), nan=0.0)\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_np = self.scaler.fit_transform(X_np)\n",
    "        elif self.scaler is not None:\n",
    "            X_np = self.scaler.transform(X_np)\n",
    "        return torch.tensor(X_np, dtype=torch.double)\n",
    "\n",
    "class CatBoostXGBEnsemble:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        self.featurizer = CombinedFeaturizer(solvent_table, data=data)\n",
    "        if data == 'single':\n",
    "            self.cat_weight, self.xgb_weight = 7.0/13, 6.0/13\n",
    "            self.cat_params = dict(random_seed=42, loss_function=\"MultiRMSE\", depth=3, learning_rate=0.07, n_estimators=1050, l2_leaf_reg=3.5, bootstrap_type=\"Bayesian\", bagging_temperature=0.225, grow_policy=\"SymmetricTree\", rsm=0.75, verbose=False)\n",
    "            self.xgb_params = dict(random_state=42, objective=\"reg:squarederror\", tree_method=\"hist\", subsample=0.5, reg_lambda=0.6, n_estimators=1000, max_depth=4, learning_rate=0.02, colsample_bytree=0.3, colsample_bylevel=0.6)\n",
    "        else:\n",
    "            self.cat_weight, self.xgb_weight = 1.0/3, 2.0/3\n",
    "            self.cat_params = dict(random_seed=42, loss_function=\"MultiRMSE\", depth=3, learning_rate=0.06, n_estimators=1100, l2_leaf_reg=2.5, bootstrap_type=\"Bayesian\", bagging_temperature=0.25, grow_policy=\"SymmetricTree\", rsm=0.75, verbose=False)\n",
    "            self.xgb_params = dict(random_state=42, objective=\"reg:squarederror\", tree_method=\"approx\", subsample=0.5, reg_lambda=0.6, n_estimators=1000, max_depth=4, learning_rate=0.02, grow_policy=\"lossguide\", colsample_bytree=0.3, colsample_bylevel=0.6)\n",
    "        self.cat_model = None\n",
    "        self.xgb_models = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_np = self.featurizer.featurize(train_X, fit_scaler=True).numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.cat_model = CatBoostRegressor(**self.cat_params)\n",
    "        self.cat_model.fit(X_np, Y_np)\n",
    "        self.xgb_models = [XGBRegressor(**self.xgb_params).fit(X_np, Y_np[:, t]) for t in range(Y_np.shape[1])]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_np = self.featurizer.featurize(X, fit_scaler=False).numpy()\n",
    "        cat_pred = np.asarray(self.cat_model.predict(X_np))\n",
    "        if cat_pred.ndim == 1: cat_pred = cat_pred.reshape(-1, 1)\n",
    "        xgb_pred = np.column_stack([m.predict(X_np) for m in self.xgb_models])\n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        # CRITICAL: Clip to [0, 1] and normalize\n",
    "        out = np.clip(out, 0.0, 1.0)  # Clip to [0, 1]\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            out = out / np.maximum(totals, 1.0)\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"Model classes defined with CLIPPING to [0, 1].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaae1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick CV check to verify model works\n",
    "print(\"Running quick CV check...\")\n",
    "\n",
    "fold_mses = []\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(list(generate_leave_one_out_splits(X_single, Y_single))[:3]):\n",
    "    model = CatBoostXGBEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    fold_mses.append(mse)\n",
    "    print(f\"Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "\n",
    "print(f\"\\nQuick check passed. Mean MSE: {np.mean(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a966680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SUBMISSION WITH CLIPPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single solvent predictions (24 folds)\n",
    "print(\"\\nGenerating single solvent predictions (24 folds)...\")\n",
    "all_predictions_single = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(tqdm.tqdm(list(generate_leave_one_out_splits(X_single, Y_single)))):\n",
    "    model = CatBoostXGBEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions_single.append({\n",
    "            \"task\": 0, \"fold\": fold_idx, \"row\": row_idx,\n",
    "            \"target_1\": row[0], \"target_2\": row[1], \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions_single)\n",
    "print(f\"Single solvent predictions: {len(submission_single_solvent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d0bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data predictions (13 folds by solvent PAIRS)\n",
    "print(\"\\nGenerating full data predictions (13 folds by solvent PAIRS)...\")\n",
    "all_predictions_full = []\n",
    "\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(tqdm.tqdm(list(generate_leave_one_ramp_out_splits(X_full, Y_full)))):\n",
    "    model = CatBoostXGBEnsemble(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions_full.append({\n",
    "            \"task\": 1, \"fold\": fold_idx, \"row\": row_idx,\n",
    "            \"target_1\": row[0], \"target_2\": row[1], \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions_full)\n",
    "print(f\"Full data predictions: {len(submission_full_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fa159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission with CLIPPING\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "# CRITICAL: Clip all targets to [0, 1]\n",
    "print(\"\\nApplying final clipping to [0, 1]...\")\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    before_clip = (submission[col] > 1).sum() + (submission[col] < 0).sum()\n",
    "    submission[col] = submission[col].clip(0, 1)\n",
    "    after_clip = (submission[col] > 1).sum() + (submission[col] < 0).sum()\n",
    "    print(f\"  {col}: {before_clip} values clipped\")\n",
    "\n",
    "# Save\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabce12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv('/home/submission/submission.csv')\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"Tasks: {df['task'].unique()}\")\n",
    "print(f\"Folds per task:\")\n",
    "print(df.groupby('task')['fold'].nunique())\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f\"  {col}: min={df[col].min():.6f}, max={df[col].max():.6f}\")\n",
    "    print(f\"    Values > 1: {(df[col] > 1).sum()}\")\n",
    "    print(f\"    Values < 0: {(df[col] < 0).sum()}\")\n",
    "\n",
    "print(f\"\\nTarget sums:\")\n",
    "df['sum'] = df['target_1'] + df['target_2'] + df['target_3']\n",
    "print(f\"  min={df['sum'].min():.6f}, max={df['sum'].max():.6f}, mean={df['sum'].mean():.6f}\")\n",
    "\n",
    "print(f\"\\n✓ All targets in [0, 1] range\")\n",
    "print(f\"✓ Submission format correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df134218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV for logging\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CV CALCULATION FOR LOGGING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single solvent CV\n",
    "print(\"\\nCalculating single solvent CV...\")\n",
    "single_fold_mses = []\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X_single, Y_single)):\n",
    "    model = CatBoostXGBEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    single_fold_mses.append(mse)\n",
    "\n",
    "single_cv = np.mean(single_fold_mses)\n",
    "single_cv_std = np.std(single_fold_mses)\n",
    "print(f\"Single solvent CV MSE: {single_cv:.6f} +/- {single_cv_std:.6f}\")\n",
    "\n",
    "# Full data CV\n",
    "print(\"\\nCalculating full data CV...\")\n",
    "full_fold_mses = []\n",
    "for fold_idx, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_ramp_out_splits(X_full, Y_full)):\n",
    "    model = CatBoostXGBEnsemble(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mse = np.mean((preds - test_Y.values) ** 2)\n",
    "    full_fold_mses.append(mse)\n",
    "\n",
    "full_cv = np.mean(full_fold_mses)\n",
    "full_cv_std = np.std(full_fold_mses)\n",
    "print(f\"Full data CV MSE: {full_cv:.6f} +/- {full_cv_std:.6f}\")\n",
    "\n",
    "# Weighted combined CV\n",
    "n_single, n_full = len(X_single), len(X_full)\n",
    "weighted_cv = (n_single * single_cv + n_full * full_cv) / (n_single + n_full)\n",
    "print(f\"\\nWeighted combined CV: {weighted_cv:.6f}\")\n",
    "\n",
    "print(f\"\\nBaseline (exp_050 without clipping):\")\n",
    "print(f\"  Single CV: 0.008092\")\n",
    "print(f\"  Full CV: 0.012482\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ca737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 052: SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nGOAL: Regenerate exp_050 with proper clipping to [0, 1]\")\n",
    "print(\"\\nFIX APPLIED:\")\n",
    "print(\"  - Clip predictions to [0, 1] in model.predict()\")\n",
    "print(\"  - Additional clipping before saving submission\")\n",
    "print(\"  - This should fix 'Evaluation metric raised an unexpected error'\")\n",
    "\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  Single solvent CV MSE: {single_cv:.6f}\")\n",
    "print(f\"  Full data CV MSE: {full_cv:.6f}\")\n",
    "print(f\"  Weighted combined CV: {weighted_cv:.6f}\")\n",
    "\n",
    "print(f\"\\nSUBMISSION:\")\n",
    "print(f\"  Saved to: /home/submission/submission.csv\")\n",
    "print(f\"  Total rows: {len(submission)}\")\n",
    "print(f\"  All targets in [0, 1]: YES\")\n",
    "\n",
    "print(f\"\\nEXPECTED LB (based on CV-LB relationship):\")\n",
    "print(f\"  LB = 4.31 * {single_cv:.6f} + 0.0525 = {4.31 * single_cv + 0.0525:.4f}\")\n",
    "print(f\"  Best LB so far: 0.0877 (exp_030)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc824c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv('/home/submission/submission.csv')\n",
    "\n",
    "# Check for any issues\n",
    "issues = []\n",
    "if df.isna().sum().sum() > 0:\n",
    "    issues.append(f\"NaN values: {df.isna().sum().sum()}\")\n",
    "if (df['target_1'] > 1).sum() > 0:\n",
    "    issues.append(f\"target_1 > 1: {(df['target_1'] > 1).sum()}\")\n",
    "if (df['target_2'] > 1).sum() > 0:\n",
    "    issues.append(f\"target_2 > 1: {(df['target_2'] > 1).sum()}\")\n",
    "if (df['target_3'] > 1).sum() > 0:\n",
    "    issues.append(f\"target_3 > 1: {(df['target_3'] > 1).sum()}\")\n",
    "if (df['target_1'] < 0).sum() > 0:\n",
    "    issues.append(f\"target_1 < 0: {(df['target_1'] < 0).sum()}\")\n",
    "if (df['target_2'] < 0).sum() > 0:\n",
    "    issues.append(f\"target_2 < 0: {(df['target_2'] < 0).sum()}\")\n",
    "if (df['target_3'] < 0).sum() > 0:\n",
    "    issues.append(f\"target_3 < 0: {(df['target_3'] < 0).sum()}\")\n",
    "\n",
    "if issues:\n",
    "    print(\"ISSUES FOUND:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"✓ No issues found\")\n",
    "    print(\"✓ Submission is ready for upload\")\n",
    "    print(f\"\\nCV for logging: {single_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final CV for easy copying\n",
    "print(f\"\\n\\nFINAL CV FOR LOGGING: {single_cv:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
