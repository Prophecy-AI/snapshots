{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f7b554",
   "metadata": {},
   "source": [
    "# Experiment 078: MixAll Kernel Approach with GroupKFold(5)\n",
    "\n",
    "**Rationale**: The mixall kernel uses GroupKFold(5) instead of Leave-One-Out CV. This is a fundamentally different CV scheme that may have a DIFFERENT CV-LB relationship. The key insight is that the intercept in our CV-LB relationship (0.052) exceeds the target (0.0347), so we need to try approaches that might change this relationship.\n",
    "\n",
    "**Key Changes**:\n",
    "1. Override `generate_leave_one_out_splits` to use GroupKFold(5)\n",
    "2. Override `generate_leave_one_ramp_out_splits` to use GroupKFold(5)\n",
    "3. Use ensemble: MLP + XGBoost + RF + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f02cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:06:24.717579Z",
     "iopub.status.busy": "2026-01-16T11:06:24.717160Z",
     "iopub.status.idle": "2026-01-16T11:06:26.183448Z",
     "shell.execute_reply": "2026-01-16T11:06:26.183014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/data/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define local load functions\n",
    "def load_data(data_type):\n",
    "    \"\"\"Load data from local paths.\"\"\"\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]  # Correct column names\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]  # Correct column names\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    \"\"\"Load precomputed features.\"\"\"\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'drfps':\n",
    "        return pd.read_csv('/home/data/drfps_catechol_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'fragprints':\n",
    "        return pd.read_csv('/home/data/fragprints_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'acs_pca_descriptors':\n",
    "        return pd.read_csv('/home/data/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Imports and local data functions done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151e63ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:06:26.184627Z",
     "iopub.status.busy": "2026-01-16T11:06:26.184474Z",
     "iopub.status.idle": "2026-01-16T11:06:26.189202Z",
     "shell.execute_reply": "2026-01-16T11:06:26.188837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold(5) CV functions defined\n"
     ]
    }
   ],
   "source": [
    "# Override the CV split functions to use GroupKFold(5) instead of Leave-One-Out\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate Group K-Fold splits across the solvents (5-fold).\"\"\"\n",
    "    groups = X[\"SOLVENT NAME\"]\n",
    "    n_groups = len(groups.unique())\n",
    "    n_splits = min(5, n_groups)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield (\n",
    "            (X.iloc[train_idx], Y.iloc[train_idx]),\n",
    "            (X.iloc[test_idx], Y.iloc[test_idx]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate Group K-Fold splits across the solvent ramps (5-fold).\"\"\"\n",
    "    groups = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    \n",
    "    n_groups = len(groups.unique())\n",
    "    n_splits = min(5, n_groups)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield (\n",
    "            (X.iloc[train_idx], Y.iloc[train_idx]),\n",
    "            (X.iloc[test_idx], Y.iloc[test_idx]),\n",
    "        )\n",
    "\n",
    "print('GroupKFold(5) CV functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b364ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:06:26.190286Z",
     "iopub.status.busy": "2026-01-16T11:06:26.190182Z",
     "iopub.status.idle": "2026-01-16T11:06:26.194791Z",
     "shell.execute_reply": "2026-01-16T11:06:26.194396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer:\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 2  # +2 for Time, Temp\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        feats = self.features.loc[solvent_names].values\n",
    "        final_feats = np.hstack([res_time, temp, feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "class PrecomputedFeaturizerMixed:\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 3  # +3 for Time, Temp, %B\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0  # Normalize to [0,1]\n",
    "        \n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        \n",
    "        # Linear mixing\n",
    "        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, sb_pct, mixture_feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b927f25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:06:26.195919Z",
     "iopub.status.busy": "2026-01-16T11:06:26.195784Z",
     "iopub.status.idle": "2026-01-16T11:06:26.199390Z",
     "shell.execute_reply": "2026-01-16T11:06:26.199035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())  # Outputs in [0,1]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('MLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a333e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:06:26.200640Z",
     "iopub.status.busy": "2026-01-16T11:06:26.200357Z",
     "iopub.status.idle": "2026-01-16T11:06:26.208512Z",
     "shell.execute_reply": "2026-01-16T11:06:26.208163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsembleModel defined\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Model (MLP + XGBoost + RF + LightGBM)\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, data='single', hidden_dims=[128, 64, 32], dropout=0.2, \n",
    "                 weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "        self.data = data\n",
    "        self.weights = weights\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer('spange_descriptors')\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed('spange_descriptors')\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp = EnhancedMLP(self.smiles_featurizer.feats_dim, 3, hidden_dims, dropout)\n",
    "        \n",
    "        # XGBoost - use MultiOutputRegressor\n",
    "        self.xgb_base = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Random Forest\n",
    "        self.rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # LightGBM - use MultiOutputRegressor\n",
    "        self.lgb_base = lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32, verbose=False):\n",
    "        # Featurize\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Train GBDT models with MultiOutputRegressor\n",
    "        self.xgb = MultiOutputRegressor(self.xgb_base)\n",
    "        self.xgb.fit(X_scaled, train_Y_np)\n",
    "        \n",
    "        self.rf = RandomForestRegressor(**self.rf_params)\n",
    "        self.rf.fit(X_scaled, train_Y_np)\n",
    "        \n",
    "        self.lgbm = MultiOutputRegressor(self.lgb_base)\n",
    "        self.lgbm.fit(X_scaled, train_Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        train_loader = DataLoader(TensorDataset(X_tensor_scaled, train_Y_tensor), \n",
    "                                  batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # MLP predictions\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "        \n",
    "        # GBDT predictions\n",
    "        xgb_preds = self.xgb.predict(X_scaled)\n",
    "        rf_preds = self.rf.predict(X_scaled)\n",
    "        lgb_preds = self.lgbm.predict(X_scaled)\n",
    "        \n",
    "        # Clip to [0,1]\n",
    "        xgb_preds = np.clip(xgb_preds, 0, 1)\n",
    "        rf_preds = np.clip(rf_preds, 0, 1)\n",
    "        lgb_preds = np.clip(lgb_preds, 0, 1)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_preds = (self.weights[0] * mlp_preds + \n",
    "                       self.weights[1] * xgb_preds + \n",
    "                       self.weights[2] * rf_preds + \n",
    "                       self.weights[3] * lgb_preds)\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print('EnsembleModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent data with GroupKFold(5)\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=5):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx}: Train {len(train_X)}, Test {len(test_X)}\")\n",
    "    print(f\"  Test solvents: {test_X['SOLVENT NAME'].unique()[:3]}...\")\n",
    "    \n",
    "    model = EnsembleModel(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    print(f\"  Fold MSE: {fold_mse:.6f}\")\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data with GroupKFold(5)\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "# Create ramp groups\n",
    "groups = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "print(f\"Number of unique ramps: {len(groups.unique())}\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=5):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx}: Train {len(train_X)}, Test {len(test_X)}\")\n",
    "    \n",
    "    model = EnsembleModel(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    print(f\"  Fold MSE: {fold_mse:.6f}\")\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head())\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "# The competition uses MSE across all predictions\n",
    "\n",
    "# Reload and verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"Submission rows: {len(submission_check)}\")\n",
    "print(f\"Expected: 656 (single) + 1227 (full) = 1883\")\n",
    "\n",
    "# Check prediction ranges\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 078 COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
