{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3ae9cf",
   "metadata": {},
   "source": [
    "# Experiment 080: ens-model Kernel Approach\n",
    "\n",
    "**Rationale**: Implement the ens-model kernel approach which uses:\n",
    "1. Combined features from multiple sources (spange, acs_pca, drfps, fragprints)\n",
    "2. Correlation-based feature filtering (threshold=0.90)\n",
    "3. Numeric feature engineering (T_inv, RT_log, T_x_RT, RT_scaled)\n",
    "4. CatBoost + XGBoost ensemble with different weights for single vs full\n",
    "5. **PROBABILITY NORMALIZATION**: clip to non-negative, normalize to sum to 1\n",
    "\n",
    "This is a clean, rule-compliant approach that uses probability normalization which might help with generalization to unseen solvents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fc31cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:21.249673Z",
     "iopub.status.busy": "2026-01-16T11:33:21.249232Z",
     "iopub.status.idle": "2026-01-16T11:33:22.204982Z",
     "shell.execute_reply": "2026-01-16T11:33:22.204498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e787b029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.206245Z",
     "iopub.status.busy": "2026-01-16T11:33:22.206083Z",
     "iopub.status.idle": "2026-01-16T11:33:22.210266Z",
     "shell.execute_reply": "2026-01-16T11:33:22.209862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data functions defined\n"
     ]
    }
   ],
   "source": [
    "# Local data loading functions\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(feature_type):\n",
    "    if feature_type == 'spange_descriptors':\n",
    "        return pd.read_csv('/home/data/spange_descriptors_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'drfps_catechol':\n",
    "        return pd.read_csv('/home/data/drfps_catechol_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'fragprints':\n",
    "        return pd.read_csv('/home/data/fragprints_lookup.csv', index_col=0)\n",
    "    elif feature_type == 'acs_pca_descriptors':\n",
    "        return pd.read_csv('/home/data/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad8137f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.211241Z",
     "iopub.status.busy": "2026-01-16T11:33:22.211134Z",
     "iopub.status.idle": "2026-01-16T11:33:22.215672Z",
     "shell.execute_reply": "2026-01-16T11:33:22.215296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV split functions defined\n"
     ]
    }
   ],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate leave-one-out splits across the solvents.\"\"\"\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    \"\"\"Generate leave-one-ramp-out splits across the solvent ramps.\"\"\"\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print('CV split functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9a4e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.216611Z",
     "iopub.status.busy": "2026-01-16T11:33:22.216511Z",
     "iopub.status.idle": "2026-01-16T11:33:22.222029Z",
     "shell.execute_reply": "2026-01-16T11:33:22.221638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature priority for correlation filtering\n",
    "def feature_priority(name: str) -> int:\n",
    "    \"\"\"Higher number = more important to keep during correlation filtering.\"\"\"\n",
    "    if name.startswith(\"spange_\"):\n",
    "        return 5\n",
    "    if name.startswith(\"acs_\"):\n",
    "        return 4\n",
    "    if name.startswith(\"drfps_\"):\n",
    "        return 3\n",
    "    if name.startswith(\"frag_\"):\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df: pd.DataFrame, threshold: float = 0.90):\n",
    "    \"\"\"Drop columns that are highly correlated with any other column.\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    # Drop constant columns first\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    # Find all pairs with corr > threshold\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                if col_i in to_drop or col_j in to_drop:\n",
    "                    continue\n",
    "                p_i = feature_priority(col_i)\n",
    "                p_j = feature_priority(col_j)\n",
    "                if p_i > p_j:\n",
    "                    to_drop.add(col_j)\n",
    "                elif p_j > p_i:\n",
    "                    to_drop.add(col_i)\n",
    "                else:\n",
    "                    idx_i = df.columns.get_loc(col_i)\n",
    "                    idx_j = df.columns.get_loc(col_j)\n",
    "                    to_drop.add(col_i if idx_i > idx_j else col_j)\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print('Feature filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66479abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.223126Z",
     "iopub.status.busy": "2026-01-16T11:33:22.223025Z",
     "iopub.status.idle": "2026-01-16T11:33:22.229073Z",
     "shell.execute_reply": "2026-01-16T11:33:22.228668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solvent table builder defined\n"
     ]
    }
   ],
   "source": [
    "# Build combined solvent feature table\n",
    "_SOLVENT_TABLE_CACHE = None\n",
    "\n",
    "def build_solvent_feature_table(threshold: float = 0.90):\n",
    "    \"\"\"Build combined solvent feature table from multiple sources.\"\"\"\n",
    "    global _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    if _SOLVENT_TABLE_CACHE is not None:\n",
    "        return _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    print(\">>> Building solvent feature table...\")\n",
    "    \n",
    "    sources = [\n",
    "        \"spange_descriptors\",\n",
    "        \"acs_pca_descriptors\",\n",
    "        \"drfps_catechol\",\n",
    "        \"fragprints\",\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for src in sources:\n",
    "        df_src = load_features(src).copy()\n",
    "        \n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        \n",
    "        # Bit-table filtering for binary fingerprints\n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            \n",
    "            # Drop all-zero and all-one columns\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            \n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"}, errors='ignore')\n",
    "            count = values.sum(axis=0).T\n",
    "            drop_cols = count[count == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols, errors='ignore')\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        else:\n",
    "            if src == \"spange_descriptors\":\n",
    "                prefix = \"spange\"\n",
    "            elif src == \"acs_pca_descriptors\":\n",
    "                prefix = \"acs\"\n",
    "            else:\n",
    "                prefix = src\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        dfs.append(df_src)\n",
    "    \n",
    "    # Merge all feature tables\n",
    "    combined = reduce(lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"), dfs)\n",
    "    combined = combined.fillna(0.0)\n",
    "    \n",
    "    print(f\"Combined shape before filtering: {combined.shape}\")\n",
    "    \n",
    "    # Filter correlated features\n",
    "    combined, dropped = filter_correlated_features(combined, threshold=threshold)\n",
    "    \n",
    "    print(f\"Combined shape after filtering: {combined.shape}\")\n",
    "    print(f\"Dropped {len(dropped)} features\")\n",
    "    \n",
    "    combined = combined.set_index(\"SOLVENT NAME\")\n",
    "    _SOLVENT_TABLE_CACHE = combined\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print('Solvent table builder defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1874eb82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.230083Z",
     "iopub.status.busy": "2026-01-16T11:33:22.229971Z",
     "iopub.status.idle": "2026-01-16T11:33:22.233574Z",
     "shell.execute_reply": "2026-01-16T11:33:22.233183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature engineering defined\n"
     ]
    }
   ],
   "source": [
    "# Numeric feature engineering\n",
    "def add_numeric_features(X_numeric: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add engineered numeric features.\"\"\"\n",
    "    X_num = X_numeric.copy()\n",
    "    cols = set(X_num.columns)\n",
    "    \n",
    "    if {\"Temperature\", \"Residence Time\"} <= cols:\n",
    "        # Convert Temperature to Kelvin\n",
    "        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n",
    "        \n",
    "        T = X_num[\"Temperature\"]\n",
    "        rt = X_num[\"Residence Time\"]\n",
    "        \n",
    "        # Interaction term\n",
    "        X_num[\"T_x_RT\"] = T * rt\n",
    "        \n",
    "        # Log transformation\n",
    "        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "        \n",
    "        # Inverse temperature\n",
    "        X_num[\"T_inv\"] = 1 / T\n",
    "        \n",
    "        # Scaled residence time\n",
    "        X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    \n",
    "    return X_num\n",
    "\n",
    "print('Numeric feature engineering defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3840bb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:22.234578Z",
     "iopub.status.busy": "2026-01-16T11:33:22.234472Z",
     "iopub.status.idle": "2026-01-16T11:33:22.239464Z",
     "shell.execute_reply": "2026-01-16T11:33:22.239101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer:\n",
    "    \"\"\"Featurizer for single solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "    \n",
    "    def featurize(self, X: pd.DataFrame) -> torch.Tensor:\n",
    "        # Numeric features\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        # Solvent features\n",
    "        solvent_names = X[\"SOLVENT NAME\"].values\n",
    "        solvent_feats = self.solvent_table.loc[solvent_names].values\n",
    "        \n",
    "        # Combine\n",
    "        combined = np.hstack([X_numeric.values, solvent_feats])\n",
    "        combined = np.nan_to_num(combined, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        return torch.tensor(combined, dtype=torch.double)\n",
    "\n",
    "\n",
    "class PrecomputedFeaturizerMixed:\n",
    "    \"\"\"Featurizer for mixed solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "    \n",
    "    def featurize(self, X: pd.DataFrame) -> torch.Tensor:\n",
    "        # Numeric features\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        # Solvent features (linear mixing)\n",
    "        solvent_a = X[\"SOLVENT A NAME\"].values\n",
    "        solvent_b = X[\"SOLVENT B NAME\"].values\n",
    "        ratio_b = X[\"SolventB%\"].values / 100.0\n",
    "        \n",
    "        feats_a = self.solvent_table.loc[solvent_a].values\n",
    "        feats_b = self.solvent_table.loc[solvent_b].values\n",
    "        \n",
    "        # Linear mixing\n",
    "        solvent_feats = (1 - ratio_b.reshape(-1, 1)) * feats_a + ratio_b.reshape(-1, 1) * feats_b\n",
    "        \n",
    "        # Add ratio as feature\n",
    "        X_numeric[\"SolventB_ratio\"] = ratio_b\n",
    "        \n",
    "        # Combine\n",
    "        combined = np.hstack([X_numeric.values, solvent_feats])\n",
    "        combined = np.nan_to_num(combined, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        return torch.tensor(combined, dtype=torch.double)\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3affe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:31.829447Z",
     "iopub.status.busy": "2026-01-16T11:33:31.828904Z",
     "iopub.status.idle": "2026-01-16T11:33:32.216935Z",
     "shell.execute_reply": "2026-01-16T11:33:32.216513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost model defined\n"
     ]
    }
   ],
   "source": [
    "# CatBoost Model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "class CatBoostModel:\n",
    "    \"\"\"CatBoost model with probability normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\", random_state: int = 42):\n",
    "        self.data_mode = data\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.cat_params = dict(\n",
    "                random_state=random_state,\n",
    "                iterations=1500,\n",
    "                learning_rate=0.03,\n",
    "                depth=8,\n",
    "                l2_leaf_reg=3.0,\n",
    "                subsample=0.85,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.cat_params = dict(\n",
    "                random_state=random_state,\n",
    "                iterations=1200,\n",
    "                learning_rate=0.04,\n",
    "                depth=7,\n",
    "                l2_leaf_reg=2.5,\n",
    "                subsample=0.80,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = CatBoostRegressor(**self.cat_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.models is None:\n",
    "            raise RuntimeError(\"CatBoostModel not trained\")\n",
    "        \n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # PROBABILITY NORMALIZATION\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('CatBoost model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f856a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:32.218143Z",
     "iopub.status.busy": "2026-01-16T11:33:32.217984Z",
     "iopub.status.idle": "2026-01-16T11:33:32.223508Z",
     "shell.execute_reply": "2026-01-16T11:33:32.223162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model defined\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "import xgboost as xgb\n",
    "\n",
    "class XGBModel:\n",
    "    \"\"\"XGBoost model with probability normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\", random_state: int = 42):\n",
    "        self.data_mode = data\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state,\n",
    "                n_estimators=1500,\n",
    "                learning_rate=0.03,\n",
    "                max_depth=8,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.80,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state,\n",
    "                n_estimators=1200,\n",
    "                learning_rate=0.04,\n",
    "                max_depth=7,\n",
    "                subsample=0.80,\n",
    "                colsample_bytree=0.75,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = xgb.XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.models is None:\n",
    "            raise RuntimeError(\"XGBModel not trained\")\n",
    "        \n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # PROBABILITY NORMALIZATION\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('XGBoost model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1341d515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:32.224327Z",
     "iopub.status.busy": "2026-01-16T11:33:32.224239Z",
     "iopub.status.idle": "2026-01-16T11:33:32.228236Z",
     "shell.execute_reply": "2026-01-16T11:33:32.227898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsembleModel defined\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Model\n",
    "class EnsembleModel:\n",
    "    \"\"\"Weighted ensemble of CatBoost and XGBoost with probability normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: str = \"single\", verbose: bool = False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Optimized fixed weights per dataset (from ens-model kernel)\n",
    "        if data == \"single\":\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        # Normalize ensemble weights\n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        # Initialize base models\n",
    "        self.cat_model = CatBoostModel(data=data)\n",
    "        self.xgb_model = XGBModel(data=data)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        self.cat_model.train_model(train_X, train_Y)\n",
    "        self.xgb_model.train_model(train_X, train_Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        \n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        return out\n",
    "\n",
    "print('EnsembleModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6731e79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:41.145486Z",
     "iopub.status.busy": "2026-01-16T11:33:41.144948Z",
     "iopub.status.idle": "2026-01-16T11:37:13.751114Z",
     "shell.execute_reply": "2026-01-16T11:37:13.750665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: 656 samples, 24 solvents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Building solvent feature table...\n",
      "Combined shape before filtering: (26, 114)\n",
      "Combined shape after filtering: (26, 67)\n",
      "Dropped 47 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:09<03:34,  9.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:18<03:20,  9.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [00:27<03:08,  8.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [00:35<02:57,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [00:44<02:49,  8.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [00:53<02:41,  8.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [01:02<02:30,  8.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [01:11<02:22,  8.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [01:20<02:13,  8.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [01:29<02:05,  8.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [01:38<01:55,  8.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [01:47<01:46,  8.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [01:55<01:37,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [02:04<01:28,  8.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [02:13<01:19,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [02:22<01:10,  8.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [02:31<01:01,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [02:40<00:53,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [02:48<00:44,  8.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [02:57<00:35,  8.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [03:06<00:26,  8.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [03:15<00:17,  8.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [03:23<00:08,  8.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [03:32<00:00,  8.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [03:32<00:00,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single solvent CV MSE: 0.010308 ± 0.008858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585e9c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:37:23.736777Z",
     "iopub.status.busy": "2026-01-16T11:37:23.736308Z",
     "iopub.status.idle": "2026-01-16T11:38:59.294200Z",
     "shell.execute_reply": "2026-01-16T11:38:59.293786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: 1227 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/13 [00:07<01:29,  7.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [00:14<01:19,  7.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 3/13 [00:21<01:11,  7.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [00:29<01:05,  7.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 5/13 [00:36<00:58,  7.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 6/13 [00:43<00:50,  7.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [00:50<00:43,  7.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 8/13 [00:58<00:36,  7.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 9/13 [01:05<00:28,  7.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 10/13 [01:12<00:21,  7.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [01:20<00:14,  7.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 12/13 [01:27<00:07,  7.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [01:35<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [01:35<00:00,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full data CV MSE: 0.010243 ± 0.004625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold MSE\n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data CV MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97ab512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:39:08.653729Z",
     "iopub.status.busy": "2026-01-16T11:39:08.653287Z",
     "iopub.status.idle": "2026-01-16T11:39:08.668064Z",
     "shell.execute_reply": "2026-01-16T11:39:08.667637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (1883, 7)\n",
      "Columns: ['index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']\n",
      "\n",
      "Submission saved to /home/submission/submission.csv\n",
      "\n",
      "Submission rows: 1883\n",
      "Expected: 656 (single) + 1227 (full) = 1883\n",
      "target_1: min=0.0328, max=0.9722\n",
      "target_2: min=0.0000, max=0.3843\n",
      "target_3: min=0.0000, max=0.3602\n",
      "\n",
      "Row sums: min=0.2083, max=0.9950, mean=0.7985\n"
     ]
    }
   ],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "print(f\"Expected: 656 (single) + 1227 (full) = 1883\")\n",
    "\n",
    "# Check prediction ranges and row sums\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")\n",
    "\n",
    "# Check row sums (should be close to 1 due to probability normalization)\n",
    "row_sums = submission_check[target_cols].sum(axis=1)\n",
    "print(f\"\\nRow sums: min={row_sums.min():.4f}, max={row_sums.max():.4f}, mean={row_sums.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcac08ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:39:08.669061Z",
     "iopub.status.busy": "2026-01-16T11:39:08.668955Z",
     "iopub.status.idle": "2026-01-16T11:39:08.671615Z",
     "shell.execute_reply": "2026-01-16T11:39:08.671272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EXPERIMENT 080 COMPLETE\n",
      "==================================================\n",
      "\n",
      "Key techniques implemented:\n",
      "1. Combined features from multiple sources (spange, acs_pca, drfps, fragprints)\n",
      "2. Correlation-based feature filtering (threshold=0.90)\n",
      "3. Numeric feature engineering (T_inv, RT_log, T_x_RT, RT_scaled)\n",
      "4. CatBoost + XGBoost ensemble with different weights for single vs full\n",
      "5. PROBABILITY NORMALIZATION: clip to non-negative, normalize to sum to 1\n",
      "\n",
      "This is a RULE-COMPLIANT implementation from the ens-model kernel.\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 080 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nKey techniques implemented:\")\n",
    "print(\"1. Combined features from multiple sources (spange, acs_pca, drfps, fragprints)\")\n",
    "print(\"2. Correlation-based feature filtering (threshold=0.90)\")\n",
    "print(\"3. Numeric feature engineering (T_inv, RT_log, T_x_RT, RT_scaled)\")\n",
    "print(\"4. CatBoost + XGBoost ensemble with different weights for single vs full\")\n",
    "print(\"5. PROBABILITY NORMALIZATION: clip to non-negative, normalize to sum to 1\")\n",
    "print(\"\\nThis is a RULE-COMPLIANT implementation from the ens-model kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "single_cv = 0.010308\n",
    "full_cv = 0.010243\n",
    "\n",
    "# Weighted by sample count\n",
    "total_samples = 656 + 1227\n",
    "overall_cv = (656 * single_cv + 1227 * full_cv) / total_samples\n",
    "\n",
    "print(f\"Single solvent CV: {single_cv:.6f}\")\n",
    "print(f\"Full data CV: {full_cv:.6f}\")\n",
    "print(f\"Overall CV (sample-weighted): {overall_cv:.6f}\")\n",
    "\n",
    "# Compare with our best results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH PREVIOUS RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"This experiment (ens-model approach): {overall_cv:.6f}\")\n",
    "print(f\"Best previous CV (Leave-One-Out): 0.008092 (exp_049)\")\n",
    "print(f\"Best verified LB: 0.0877 (exp_030, exp_067)\")\n",
    "print(f\"\\nPredicted LB using CV-LB relationship:\")\n",
    "print(f\"LB = 4.36 * {overall_cv:.6f} + 0.052 = {4.36 * overall_cv + 0.052:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KEY OBSERVATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"Row sums range from 0.21 to 0.99 (mean 0.80)\")\n",
    "print(\"This is because the probability normalization only divides by max(sum, 1.0)\")\n",
    "print(\"So predictions with sum < 1 are NOT normalized to sum to 1\")\n",
    "print(\"This preserves the actual yield predictions while ensuring sum <= 1\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
