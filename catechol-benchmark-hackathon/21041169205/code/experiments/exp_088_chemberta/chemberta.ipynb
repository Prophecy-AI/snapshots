{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d28f70",
   "metadata": {},
   "source": [
    "# Experiment 088: ChemBERTa Pre-trained Molecular Embeddings\n",
    "\n",
    "**Rationale**: The GNN benchmark achieved 0.0039 CV using pre-trained representations. Our GNN experiments without pre-training all failed (CV 0.018-0.020). The key missing ingredient is pre-training on large molecular datasets.\n",
    "\n",
    "**Approach**:\n",
    "1. Use ChemBERTa (pre-trained on 77M molecules from PubChem)\n",
    "2. Extract embeddings for each solvent SMILES (768-dim)\n",
    "3. Use embeddings as features instead of Spange descriptors\n",
    "4. Train simple MLP on ChemBERTa embeddings + T + RT\n",
    "\n",
    "**Key hypothesis**: Pre-trained embeddings capture molecular knowledge that generalizes to unseen solvents, potentially reducing the CV-LB intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39abd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:11:55.686076Z",
     "iopub.status.busy": "2026-01-16T15:11:55.685696Z",
     "iopub.status.idle": "2026-01-16T15:11:56.910396Z",
     "shell.execute_reply": "2026-01-16T15:11:56.909975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a950649f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:11:56.911625Z",
     "iopub.status.busy": "2026-01-16T15:11:56.911468Z",
     "iopub.status.idle": "2026-01-16T15:12:27.498490Z",
     "shell.execute_reply": "2026-01-16T15:12:27.498020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChemBERTa model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0d5f5b7394eb6b5bc83092a757dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52624ab1c5114cb3a78064839b033419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b60f59b8be4356a4f18d6342a3e3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da316983cc14737b47dc02f41ced5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748addfebc9d4921bbc70f6959e95267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c911da1596f54c98977a40d344f28aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemBERTa loaded: seyonec/ChemBERTa-zinc-base-v1\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Load ChemBERTa model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Loading ChemBERTa model...\")\n",
    "model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "chemberta = AutoModel.from_pretrained(model_name)\n",
    "chemberta = chemberta.to(device)\n",
    "chemberta.eval()\n",
    "print(f\"ChemBERTa loaded: {model_name}\")\n",
    "print(f\"Embedding dimension: {chemberta.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0fdd72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:27.499680Z",
     "iopub.status.busy": "2026-01-16T15:12:27.499563Z",
     "iopub.status.idle": "2026-01-16T15:12:27.505021Z",
     "shell.execute_reply": "2026-01-16T15:12:27.504611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 SMILES\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(data_type):\n",
    "    if data_type == \"single_solvent\":\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    elif data_type == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']]\n",
    "        Y = df[['SM', 'Product 2', 'Product 3']]\n",
    "    return X, Y\n",
    "\n",
    "# Load SMILES lookup\n",
    "smiles_df = pd.read_csv('/home/data/smiles_lookup.csv')\n",
    "smiles_dict = dict(zip(smiles_df['SOLVENT NAME'], smiles_df['solvent smiles']))\n",
    "print(f\"Loaded {len(smiles_dict)} SMILES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875b6e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:27.505895Z",
     "iopub.status.busy": "2026-01-16T15:12:27.505794Z",
     "iopub.status.idle": "2026-01-16T15:12:28.018918Z",
     "shell.execute_reply": "2026-01-16T15:12:28.018456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ChemBERTa embeddings for all solvents...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclohexane: C1CCCCC1... -> embedding shape (768,)\n",
      "Ethyl Acetate: O=C(OCC)C... -> embedding shape (768,)\n",
      "Acetic Acid: CC(=O)O... -> embedding shape (768,)\n",
      "2-Methyltetrahydrofuran [2-MeTHF]: O1C(C)CCC1... -> embedding shape (768,)\n",
      "1,1,1,3,3,3-Hexafluoropropan-2-ol: C(C(F)(F)F)(C(F)(F)F)O... -> embedding shape (768,)\n",
      "IPA [Propan-2-ol]: CC(O)C... -> embedding shape (768,)\n",
      "Ethanol: CCO... -> embedding shape (768,)\n",
      "Methanol: CO... -> embedding shape (768,)\n",
      "Ethylene Glycol [1,2-Ethanediol]: OCCO... -> embedding shape (768,)\n",
      "Acetonitrile: CC#N... -> embedding shape (768,)\n",
      "Water: O... -> embedding shape (768,)\n",
      "Diethyl Ether [Ether]: CCOCC... -> embedding shape (768,)\n",
      "MTBE [tert-Butylmethylether]: CC(C)(C)OC... -> embedding shape (768,)\n",
      "Dimethyl Carbonate: COC(=O)OC... -> embedding shape (768,)\n",
      "tert-Butanol [2-Methylpropan-2-ol]: CC(C)(C)O... -> embedding shape (768,)\n",
      "DMA [N,N-Dimethylacetamide]: CN(C)C(C)=O... -> embedding shape (768,)\n",
      "2,2,2-Trifluoroethanol: OCC(F)(F)F... -> embedding shape (768,)\n",
      "Dihydrolevoglucosenone (Cyrene): [C@H]12CCC([C@H](OC1)O2)=O... -> embedding shape (768,)\n",
      "Decanol: CCCCCCCCCCO... -> embedding shape (768,)\n",
      "Butanone [MEK]: CCC(=O)C... -> embedding shape (768,)\n",
      "Ethyl Lactate: CCOC(=O)C(C)O... -> embedding shape (768,)\n",
      "Methyl Propionate: O=C(OC)CC... -> embedding shape (768,)\n",
      "THF [Tetrahydrofuran]: C1CCOC1... -> embedding shape (768,)\n",
      "Water.Acetonitrile: O.CC#N... -> embedding shape (768,)\n",
      "Acetonitrile.Acetic Acid: CC#N.CC(=O)O... -> embedding shape (768,)\n",
      "Water.2,2,2-Trifluoroethanol: O.OCC(F)(F)F... -> embedding shape (768,)\n",
      "\n",
      "Total: 26 solvent embeddings\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Extract ChemBERTa embeddings for all solvents\n",
    "def get_chemberta_embedding(smiles):\n",
    "    \"\"\"Extract ChemBERTa embedding for a SMILES string.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = chemberta(**inputs)\n",
    "        # Use [CLS] token embedding (first token)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# Pre-compute embeddings for all solvents\n",
    "print(\"Computing ChemBERTa embeddings for all solvents...\")\n",
    "chemberta_embeddings = {}\n",
    "for name, smiles in smiles_dict.items():\n",
    "    emb = get_chemberta_embedding(smiles)\n",
    "    chemberta_embeddings[name] = emb\n",
    "    print(f\"{name}: {smiles[:30]}... -> embedding shape {emb.shape}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(chemberta_embeddings)} solvent embeddings\")\n",
    "print(f\"Embedding dimension: {list(chemberta_embeddings.values())[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8f219e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:41.546569Z",
     "iopub.status.busy": "2026-01-16T15:12:41.546035Z",
     "iopub.status.idle": "2026-01-16T15:12:41.550875Z",
     "shell.execute_reply": "2026-01-16T15:12:41.550476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV split functions defined\n"
     ]
    }
   ],
   "source": [
    "# Official CV split functions (DO NOT MODIFY)\n",
    "from typing import Any, Generator\n",
    "\n",
    "def generate_leave_one_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    for solvent in X[\"SOLVENT NAME\"].unique():\n",
    "        train_mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        test_mask = X[\"SOLVENT NAME\"] == solvent\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(\n",
    "    X: pd.DataFrame, Y: pd.DataFrame\n",
    ") -> Generator[\n",
    "    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    Any,\n",
    "    None,\n",
    "]:\n",
    "    ramps = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    for ramp in ramps.unique():\n",
    "        train_mask = ramps != ramp\n",
    "        test_mask = ramps == ramp\n",
    "        yield (\n",
    "            (X[train_mask], Y[train_mask]),\n",
    "            (X[test_mask], Y[test_mask]),\n",
    "        )\n",
    "\n",
    "print(\"CV split functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc336c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:41.551767Z",
     "iopub.status.busy": "2026-01-16T15:12:41.551660Z",
     "iopub.status.idle": "2026-01-16T15:12:41.555396Z",
     "shell.execute_reply": "2026-01-16T15:12:41.555037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemBERTaMLPModel defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model using ChemBERTa embeddings\n",
    "class ChemBERTaMLPModel(nn.Module):\n",
    "    def __init__(self, emb_dim=768, hidden_dim=256, out_dim=3):\n",
    "        super().__init__()\n",
    "        # Input: ChemBERTa embedding + T + RT\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim + 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, emb, T, RT):\n",
    "        x = torch.cat([emb, T.unsqueeze(1), RT.unsqueeze(1)], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"ChemBERTaMLPModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadc4b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:41.556213Z",
     "iopub.status.busy": "2026-01-16T15:12:41.556121Z",
     "iopub.status.idle": "2026-01-16T15:12:41.559692Z",
     "shell.execute_reply": "2026-01-16T15:12:41.559343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemBERTaMixtureMLPModel defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model for MIXTURES using ChemBERTa embeddings\n",
    "class ChemBERTaMixtureMLPModel(nn.Module):\n",
    "    def __init__(self, emb_dim=768, hidden_dim=256, out_dim=3):\n",
    "        super().__init__()\n",
    "        # Input: ChemBERTa_A + ChemBERTa_B + mix_frac + T + RT\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2 + 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, emb_A, emb_B, mix_frac, T, RT):\n",
    "        x = torch.cat([emb_A, emb_B, mix_frac.unsqueeze(1), T.unsqueeze(1), RT.unsqueeze(1)], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"ChemBERTaMixtureMLPModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440bdb75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:41.560538Z",
     "iopub.status.busy": "2026-01-16T15:12:41.560443Z",
     "iopub.status.idle": "2026-01-16T15:12:41.576630Z",
     "shell.execute_reply": "2026-01-16T15:12:41.576280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemBERTaModel wrapper defined\n"
     ]
    }
   ],
   "source": [
    "# Model wrapper\n",
    "class ChemBERTaModel:\n",
    "    def __init__(self, data='single', hidden_dim=256, num_epochs=300, lr=1e-3):\n",
    "        self.data_type = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.model = None\n",
    "        self.emb_dim = 768\n",
    "        \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        if self.mixed:\n",
    "            self._train_mixed(train_X, train_Y)\n",
    "        else:\n",
    "            self._train_single(train_X, train_Y)\n",
    "    \n",
    "    def _train_single(self, train_X, train_Y):\n",
    "        embeddings = []\n",
    "        temps = []\n",
    "        rts = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(train_X)):\n",
    "            row = train_X.iloc[i]\n",
    "            solvent_name = row['SOLVENT NAME']\n",
    "            \n",
    "            if solvent_name not in chemberta_embeddings:\n",
    "                continue\n",
    "            \n",
    "            embeddings.append(chemberta_embeddings[solvent_name])\n",
    "            temps.append(row['Temperature'])\n",
    "            rts.append(row['Residence Time'])\n",
    "            targets.append(train_Y.iloc[i].values)\n",
    "        \n",
    "        # Normalize\n",
    "        temps = np.array(temps)\n",
    "        rts = np.array(rts)\n",
    "        self.temp_mean, self.temp_std = temps.mean(), temps.std() + 1e-8\n",
    "        self.rt_mean, self.rt_std = rts.mean(), rts.std() + 1e-8\n",
    "        temps = (temps - self.temp_mean) / self.temp_std\n",
    "        rts = (rts - self.rt_mean) / self.rt_std\n",
    "        \n",
    "        embeddings = torch.tensor(np.array(embeddings), dtype=torch.float).to(device)\n",
    "        temps = torch.tensor(temps, dtype=torch.float).to(device)\n",
    "        rts = torch.tensor(rts, dtype=torch.float).to(device)\n",
    "        targets = torch.tensor(np.array(targets), dtype=torch.float).to(device)\n",
    "        \n",
    "        # Model\n",
    "        self.model = ChemBERTaMLPModel(emb_dim=self.emb_dim, hidden_dim=self.hidden_dim, out_dim=3).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.num_epochs, eta_min=1e-6)\n",
    "        \n",
    "        self.model.train()\n",
    "        batch_size = 32\n",
    "        n_samples = len(embeddings)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = min(start + batch_size, n_samples)\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                batch_emb = embeddings[batch_idx]\n",
    "                batch_T = temps[batch_idx]\n",
    "                batch_RT = rts[batch_idx]\n",
    "                batch_targets = targets[batch_idx]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_emb, batch_T, batch_RT)\n",
    "                loss = F.mse_loss(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item() * len(batch_idx)\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def _train_mixed(self, train_X, train_Y):\n",
    "        emb_A_list = []\n",
    "        emb_B_list = []\n",
    "        mix_fracs = []\n",
    "        temps = []\n",
    "        rts = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(train_X)):\n",
    "            row = train_X.iloc[i]\n",
    "            solvent_A = row['SOLVENT A NAME']\n",
    "            solvent_B = row['SOLVENT B NAME']\n",
    "            \n",
    "            if solvent_A not in chemberta_embeddings or solvent_B not in chemberta_embeddings:\n",
    "                continue\n",
    "            \n",
    "            emb_A_list.append(chemberta_embeddings[solvent_A])\n",
    "            emb_B_list.append(chemberta_embeddings[solvent_B])\n",
    "            mix_fracs.append(row['SolventB%'] / 100.0)\n",
    "            temps.append(row['Temperature'])\n",
    "            rts.append(row['Residence Time'])\n",
    "            targets.append(train_Y.iloc[i].values)\n",
    "        \n",
    "        # Normalize\n",
    "        temps = np.array(temps)\n",
    "        rts = np.array(rts)\n",
    "        mix_fracs = np.array(mix_fracs)\n",
    "        self.temp_mean, self.temp_std = temps.mean(), temps.std() + 1e-8\n",
    "        self.rt_mean, self.rt_std = rts.mean(), rts.std() + 1e-8\n",
    "        temps = (temps - self.temp_mean) / self.temp_std\n",
    "        rts = (rts - self.rt_mean) / self.rt_std\n",
    "        \n",
    "        emb_A = torch.tensor(np.array(emb_A_list), dtype=torch.float).to(device)\n",
    "        emb_B = torch.tensor(np.array(emb_B_list), dtype=torch.float).to(device)\n",
    "        temps = torch.tensor(temps, dtype=torch.float).to(device)\n",
    "        rts = torch.tensor(rts, dtype=torch.float).to(device)\n",
    "        mix_fracs = torch.tensor(mix_fracs, dtype=torch.float).to(device)\n",
    "        targets = torch.tensor(np.array(targets), dtype=torch.float).to(device)\n",
    "        \n",
    "        # Model\n",
    "        self.model = ChemBERTaMixtureMLPModel(emb_dim=self.emb_dim, hidden_dim=self.hidden_dim, out_dim=3).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.num_epochs, eta_min=1e-6)\n",
    "        \n",
    "        self.model.train()\n",
    "        batch_size = 32\n",
    "        n_samples = len(emb_A)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = min(start + batch_size, n_samples)\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                batch_emb_A = emb_A[batch_idx]\n",
    "                batch_emb_B = emb_B[batch_idx]\n",
    "                batch_mix = mix_fracs[batch_idx]\n",
    "                batch_T = temps[batch_idx]\n",
    "                batch_RT = rts[batch_idx]\n",
    "                batch_targets = targets[batch_idx]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_emb_A, batch_emb_B, batch_mix, batch_T, batch_RT)\n",
    "                loss = F.mse_loss(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item() * len(batch_idx)\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.mixed:\n",
    "                return self._predict_mixed(test_X)\n",
    "            else:\n",
    "                return self._predict_single(test_X)\n",
    "    \n",
    "    def _predict_single(self, test_X):\n",
    "        embeddings = []\n",
    "        temps = []\n",
    "        rts = []\n",
    "        \n",
    "        for i in range(len(test_X)):\n",
    "            row = test_X.iloc[i]\n",
    "            solvent_name = row['SOLVENT NAME']\n",
    "            \n",
    "            if solvent_name not in chemberta_embeddings:\n",
    "                embeddings.append(list(chemberta_embeddings.values())[0])\n",
    "            else:\n",
    "                embeddings.append(chemberta_embeddings[solvent_name])\n",
    "            \n",
    "            temps.append(row['Temperature'])\n",
    "            rts.append(row['Residence Time'])\n",
    "        \n",
    "        temps = (np.array(temps) - self.temp_mean) / self.temp_std\n",
    "        rts = (np.array(rts) - self.rt_mean) / self.rt_std\n",
    "        \n",
    "        embeddings = torch.tensor(np.array(embeddings), dtype=torch.float).to(device)\n",
    "        temps = torch.tensor(temps, dtype=torch.float).to(device)\n",
    "        rts = torch.tensor(rts, dtype=torch.float).to(device)\n",
    "        \n",
    "        outputs = self.model(embeddings, temps, rts)\n",
    "        return outputs\n",
    "    \n",
    "    def _predict_mixed(self, test_X):\n",
    "        emb_A_list = []\n",
    "        emb_B_list = []\n",
    "        mix_fracs = []\n",
    "        temps = []\n",
    "        rts = []\n",
    "        \n",
    "        for i in range(len(test_X)):\n",
    "            row = test_X.iloc[i]\n",
    "            solvent_A = row['SOLVENT A NAME']\n",
    "            solvent_B = row['SOLVENT B NAME']\n",
    "            \n",
    "            if solvent_A not in chemberta_embeddings:\n",
    "                emb_A_list.append(list(chemberta_embeddings.values())[0])\n",
    "            else:\n",
    "                emb_A_list.append(chemberta_embeddings[solvent_A])\n",
    "            \n",
    "            if solvent_B not in chemberta_embeddings:\n",
    "                emb_B_list.append(list(chemberta_embeddings.values())[0])\n",
    "            else:\n",
    "                emb_B_list.append(chemberta_embeddings[solvent_B])\n",
    "            \n",
    "            mix_fracs.append(row['SolventB%'] / 100.0)\n",
    "            temps.append(row['Temperature'])\n",
    "            rts.append(row['Residence Time'])\n",
    "        \n",
    "        temps = (np.array(temps) - self.temp_mean) / self.temp_std\n",
    "        rts = (np.array(rts) - self.rt_mean) / self.rt_std\n",
    "        \n",
    "        emb_A = torch.tensor(np.array(emb_A_list), dtype=torch.float).to(device)\n",
    "        emb_B = torch.tensor(np.array(emb_B_list), dtype=torch.float).to(device)\n",
    "        temps = torch.tensor(temps, dtype=torch.float).to(device)\n",
    "        rts = torch.tensor(rts, dtype=torch.float).to(device)\n",
    "        mix_fracs = torch.tensor(np.array(mix_fracs), dtype=torch.float).to(device)\n",
    "        \n",
    "        outputs = self.model(emb_A, emb_B, mix_fracs, temps, rts)\n",
    "        return outputs\n",
    "\n",
    "print(\"ChemBERTaModel wrapper defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b5ca8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:12:52.331982Z",
     "iopub.status.busy": "2026-01-16T15:12:52.331550Z",
     "iopub.status.idle": "2026-01-16T15:16:14.926186Z",
     "shell.execute_reply": "2026-01-16T15:16:14.925765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: 656 samples, 24 solvents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:08<03:15,  8.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:16<03:06,  8.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [00:25<02:57,  8.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [00:33<02:45,  8.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [00:41<02:38,  8.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [00:50<02:33,  8.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [00:59<02:24,  8.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [01:07<02:13,  8.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [01:15<02:06,  8.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [01:24<01:58,  8.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [01:32<01:49,  8.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [01:41<01:41,  8.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [01:49<01:32,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [01:58<01:24,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [02:06<01:16,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [02:15<01:07,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [02:23<00:59,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [02:31<00:50,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [02:40<00:42,  8.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [02:48<00:33,  8.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [02:57<00:25,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [03:05<00:16,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [03:14<00:08,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [03:22<00:00,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [03:22<00:00,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single solvent predictions: 656 rows\n",
      "Mean fold MSE: 0.034691 ± 0.044688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV for single solvent data\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {len(X)} samples, {len(X['SOLVENT NAME'].unique())} solvents\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ChemBERTaModel(data='single', hidden_dim=256, num_epochs=300, lr=1e-3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle solvent predictions: {len(submission_single_solvent)} rows\")\n",
    "print(f\"Mean fold MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full (mixture) data\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Full data: {len(X)} samples\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "fold_mses = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ChemBERTaModel(data='full', hidden_dim=256, num_epochs=300, lr=1e-3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    fold_mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "    fold_mses.append(fold_mse)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull data predictions: {len(submission_full_data)} rows\")\n",
    "print(f\"Mean fold MSE: {np.mean(fold_mses):.6f} ± {np.std(fold_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d43f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save submission\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "submission_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nSubmission rows: {len(submission_check)}\")\n",
    "\n",
    "target_cols = ['target_1', 'target_2', 'target_3']\n",
    "for col in target_cols:\n",
    "    print(f\"{col}: min={submission_check[col].min():.4f}, max={submission_check[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f351a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"=\"*50)\n",
    "print(\"EXPERIMENT 088 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nKey techniques:\")\n",
    "print(\"1. ChemBERTa pre-trained embeddings (768-dim)\")\n",
    "print(\"2. Pre-trained on 77M molecules from PubChem\")\n",
    "print(\"3. Simple MLP on embeddings + T + RT\")\n",
    "print(\"4. 300 epochs with cosine annealing LR\")\n",
    "print(\"\\nThis uses pre-trained molecular knowledge that should generalize to unseen solvents.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
