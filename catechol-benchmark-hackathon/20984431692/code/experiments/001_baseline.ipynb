{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf687879",
   "metadata": {},
   "source": [
    "# Baseline Experiment\n",
    "Implementing the seed strategy:\n",
    "- Merging full and single solvent datasets.\n",
    "- Feature engineering with Spange and PCA descriptors.\n",
    "- Deep Sets architecture for mixture handling.\n",
    "- Leave-One-Ramp-Out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9092962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T06:24:10.372778Z",
     "iopub.status.busy": "2026-01-14T06:24:10.372215Z",
     "iopub.status.idle": "2026-01-14T06:24:10.393619Z",
     "shell.execute_reply": "2026-01-14T06:24:10.393185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape: (1227, 5)\n",
      "Single data shape: (656, 3)\n",
      "Merged data shape: (1883, 5)\n",
      "Solvent features shape: (26, 18)\n",
      "Missing solvents: set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust paths for this environment\n",
    "DATA_DIR = '/home/data'\n",
    "\n",
    "# Load utils\n",
    "sys.path.append(DATA_DIR)\n",
    "# We will redefine load_data to use the correct path\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, 'catechol_full_data_yields.csv'))\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, 'catechol_single_solvent_yields.csv'))\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    # assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(os.path.join(DATA_DIR, f'{name}_lookup.csv'), index_col=0)\n",
    "    return features\n",
    "\n",
    "# Define constants from utils.py (since we can't easily import if it has bad paths)\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "# Load Data\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "print(\"Full data shape:\", X_full.shape)\n",
    "print(\"Single data shape:\", X_single.shape)\n",
    "\n",
    "# Preprocessing: Unify formats\n",
    "# Convert single to mixture format\n",
    "X_single_formatted = X_single.copy()\n",
    "X_single_formatted[\"SOLVENT A NAME\"] = X_single[\"SOLVENT NAME\"]\n",
    "X_single_formatted[\"SOLVENT B NAME\"] = X_single[\"SOLVENT NAME\"]\n",
    "X_single_formatted[\"SolventB%\"] = 0.0\n",
    "X_single_formatted = X_single_formatted[INPUT_LABELS_FULL_SOLVENT]\n",
    "\n",
    "# Merge\n",
    "X_all = pd.concat([X_full, X_single_formatted], axis=0).reset_index(drop=True)\n",
    "Y_all = pd.concat([Y_full, Y_single], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"Merged data shape:\", X_all.shape)\n",
    "\n",
    "# Load Features\n",
    "spange = load_features(\"spange_descriptors\")\n",
    "pca = load_features(\"acs_pca_descriptors\")\n",
    "\n",
    "# Merge features\n",
    "solvent_features = pd.concat([spange, pca], axis=1)\n",
    "# Fill NaNs if any (though lookups should be complete for used solvents)\n",
    "solvent_features = solvent_features.fillna(0)\n",
    "\n",
    "print(\"Solvent features shape:\", solvent_features.shape)\n",
    "\n",
    "# Check if all solvents in data are in features\n",
    "unique_solvents = set(X_all[\"SOLVENT A NAME\"].unique()) | set(X_all[\"SOLVENT B NAME\"].unique())\n",
    "missing_solvents = unique_solvents - set(solvent_features.index)\n",
    "print(\"Missing solvents:\", missing_solvents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb0d73a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T06:24:10.394614Z",
     "iopub.status.busy": "2026-01-14T06:24:10.394503Z",
     "iopub.status.idle": "2026-01-14T06:24:10.399783Z",
     "shell.execute_reply": "2026-01-14T06:24:10.399371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shapes: (5, 18) (5, 18) (5, 1) (5, 1) (5, 1) (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering Function\n",
    "def get_mixture_features(X_df, solvent_features_df):\n",
    "    # Normalize Process Conditions\n",
    "    # We will do this inside the CV loop to avoid leakage, but for the structure we define it here\n",
    "    # Actually, for Deep Sets, we pass raw features and let the network handle it, \n",
    "    # but we need to look up the solvent descriptors.\n",
    "    \n",
    "    # Map solvent names to descriptors\n",
    "    # We'll create arrays for Solvent A and Solvent B features\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feat_dim = solvent_features_df.shape[1]\n",
    "    \n",
    "    # Initialize arrays\n",
    "    n_samples = len(X_df)\n",
    "    feat_A = np.zeros((n_samples, feat_dim))\n",
    "    feat_B = np.zeros((n_samples, feat_dim))\n",
    "    \n",
    "    # We can use pandas mapping for speed\n",
    "    # Reindex solvent_features to match X_df order\n",
    "    \n",
    "    # Solvent A\n",
    "    feat_A = solvent_features_df.loc[X_df[\"SOLVENT A NAME\"]].values\n",
    "    \n",
    "    # Solvent B\n",
    "    feat_B = solvent_features_df.loc[X_df[\"SOLVENT B NAME\"]].values\n",
    "    \n",
    "    # Ratios\n",
    "    # SolventB% is in percentage (0-100 presumably, let's check)\n",
    "    ratio_B = X_df[\"SolventB%\"].values.reshape(-1, 1) / 100.0\n",
    "    ratio_A = 1.0 - ratio_B\n",
    "    \n",
    "    # Process conditions\n",
    "    temp = X_df[\"Temperature\"].values.reshape(-1, 1)\n",
    "    time = X_df[\"Residence Time\"].values.reshape(-1, 1)\n",
    "    \n",
    "    return feat_A, feat_B, ratio_A, ratio_B, temp, time\n",
    "\n",
    "# Test feature generation\n",
    "fA, fB, rA, rB, T, t = get_mixture_features(X_all.iloc[:5], solvent_features)\n",
    "print(\"Feature shapes:\", fA.shape, fB.shape, rA.shape, rB.shape, T.shape, t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becc041c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T06:24:10.400824Z",
     "iopub.status.busy": "2026-01-14T06:24:10.400709Z",
     "iopub.status.idle": "2026-01-14T06:24:10.409962Z",
     "shell.execute_reply": "2026-01-14T06:24:10.409535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class DeepSetsMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=3):\n",
    "        super(DeepSetsMLP, self).__init__()\n",
    "        \n",
    "        # Solvent Encoder (Shared)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Predictor\n",
    "        # Input: hidden_dim (from mixture) + 2 (Temp, Time)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, feat_A, feat_B, ratio_A, ratio_B, temp, time):\n",
    "        # Encode solvents\n",
    "        h_A = self.encoder(feat_A)\n",
    "        h_B = self.encoder(feat_B)\n",
    "        \n",
    "        # Weighted Aggregation (Deep Sets)\n",
    "        h_mix = ratio_A * h_A + ratio_B * h_B\n",
    "        \n",
    "        # Concatenate with process conditions\n",
    "        # Ensure shapes match\n",
    "        combined = torch.cat([h_mix, temp, time], dim=1)\n",
    "        \n",
    "        # Predict\n",
    "        out = self.predictor(combined)\n",
    "        return out\n",
    "\n",
    "# Wrapper class to satisfy the template requirement (conceptually)\n",
    "# The template asks for `model = MLPModel()`. \n",
    "# We will implement a class that handles training and prediction to keep the notebook clean.\n",
    "\n",
    "class MLPModel:\n",
    "    def __init__(self, input_dim=None):\n",
    "        self.model = None\n",
    "        self.input_dim = input_dim\n",
    "        self.scaler_temp = StandardScaler()\n",
    "        self.scaler_time = StandardScaler()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def preprocess(self, X_df, solvent_features, fit_scalers=False):\n",
    "        fA, fB, rA, rB, temp, time = get_mixture_features(X_df, solvent_features)\n",
    "        \n",
    "        if fit_scalers:\n",
    "            temp = self.scaler_temp.fit_transform(temp)\n",
    "            time = self.scaler_time.fit_transform(time)\n",
    "        else:\n",
    "            temp = self.scaler_temp.transform(temp)\n",
    "            time = self.scaler_time.transform(time)\n",
    "            \n",
    "        # Convert to tensors\n",
    "        fA = torch.FloatTensor(fA).to(self.device)\n",
    "        fB = torch.FloatTensor(fB).to(self.device)\n",
    "        rA = torch.FloatTensor(rA).to(self.device)\n",
    "        rB = torch.FloatTensor(rB).to(self.device)\n",
    "        temp = torch.FloatTensor(temp).to(self.device)\n",
    "        time = torch.FloatTensor(time).to(self.device)\n",
    "        \n",
    "        return fA, fB, rA, rB, temp, time\n",
    "\n",
    "    def fit(self, X_df, Y_df, solvent_features, epochs=100, batch_size=32, lr=0.001):\n",
    "        if self.input_dim is None:\n",
    "            self.input_dim = solvent_features.shape[1]\n",
    "            \n",
    "        self.model = DeepSetsMLP(self.input_dim).to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Preprocess\n",
    "        fA, fB, rA, rB, temp, time = self.preprocess(X_df, solvent_features, fit_scalers=True)\n",
    "        targets = torch.FloatTensor(Y_df.values).to(self.device)\n",
    "        \n",
    "        dataset = TensorDataset(fA, fB, rA, rB, temp, time, targets)\n",
    "        # Use drop_last=True to avoid BatchNorm error with batch size 1\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                b_fA, b_fB, b_rA, b_rB, b_temp, b_time, b_y = batch\n",
    "                preds = self.model(b_fA, b_fB, b_rA, b_rB, b_temp, b_time)\n",
    "                loss = criterion(preds, b_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            # if epoch % 10 == 0:\n",
    "            #     print(f\"Epoch {epoch}, Loss: {total_loss / len(loader):.4f}\")\n",
    "                \n",
    "    def predict(self, X_df, solvent_features):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            fA, fB, rA, rB, temp, time = self.preprocess(X_df, solvent_features, fit_scalers=False)\n",
    "            preds = self.model(fA, fB, rA, rB, temp, time)\n",
    "        return preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e98557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T06:24:10.411081Z",
     "iopub.status.busy": "2026-01-14T06:24:10.410965Z",
     "iopub.status.idle": "2026-01-14T06:28:12.356087Z",
     "shell.execute_reply": "2026-01-14T06:28:12.355637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Leave-One-Ramp-Out CV...\n",
      "Total splits: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV Results:\n",
      "            fold        mse        mae\n",
      "count  37.000000  37.000000  37.000000\n",
      "mean   19.000000   0.009073   0.063882\n",
      "std    10.824355   0.009026   0.025883\n",
      "min     1.000000   0.001173   0.027722\n",
      "25%    10.000000   0.003890   0.048155\n",
      "50%    19.000000   0.006586   0.059722\n",
      "75%    28.000000   0.011075   0.076156\n",
      "max    37.000000   0.051368   0.146515\n",
      "\n",
      "Mean MSE: 0.00907\n",
      "Mean MAE: 0.06388\n",
      "Saved cv_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# We need to implement `generate_leave_one_ramp_out_splits` logic here or import it.\n",
    "# Since we can't import easily due to path issues in utils.py, I'll copy the logic.\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    # Sort to ensure deterministic order\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    \n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        # Create mask for this pair\n",
    "        # We need to match both A and B. \n",
    "        # Note: In the merged dataset, single solvents have A=B.\n",
    "        \n",
    "        mask = (X[\"SOLVENT A NAME\"] == solvent_pair[\"SOLVENT A NAME\"]) & \\\n",
    "               (X[\"SOLVENT B NAME\"] == solvent_pair[\"SOLVENT B NAME\"])\n",
    "        \n",
    "        train_mask = ~mask\n",
    "        test_mask = mask\n",
    "        \n",
    "        yield (X[train_mask], Y[train_mask]), (X[test_mask], Y[test_mask])\n",
    "\n",
    "# Run CV\n",
    "scores = []\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"Starting Leave-One-Ramp-Out CV...\")\n",
    "\n",
    "# For speed in this baseline, we might limit folds or run all. \n",
    "# There are ~13 ramps + 24 single solvents? Let's check number of splits.\n",
    "splits = list(generate_leave_one_ramp_out_splits(X_all, Y_all))\n",
    "print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "# Limit to first 5 splits for quick testing if needed, but better to run all for baseline.\n",
    "# We will run all.\n",
    "\n",
    "fold = 0\n",
    "for (X_train, Y_train), (X_test, Y_test) in splits:\n",
    "    fold += 1\n",
    "    # print(f\"Fold {fold}/{len(splits)}\")\n",
    "    \n",
    "    model = MLPModel()\n",
    "    model.fit(X_train, Y_train, solvent_features, epochs=50, lr=0.001) # Reduced epochs for speed\n",
    "    \n",
    "    preds = model.predict(X_test, solvent_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(Y_test, preds)\n",
    "    mae = mean_absolute_error(Y_test, preds)\n",
    "    scores.append({\"fold\": fold, \"mse\": mse, \"mae\": mae})\n",
    "    \n",
    "    # Store preds for analysis\n",
    "    # We need to align preds with the original index or just store them\n",
    "    # Let's store them with the test dataframe\n",
    "    test_res = X_test.copy()\n",
    "    test_res[TARGET_LABELS] = Y_test\n",
    "    test_res[[f\"{t}_pred\" for t in TARGET_LABELS]] = preds\n",
    "    all_preds.append(test_res)\n",
    "\n",
    "# Aggregate results\n",
    "results_df = pd.DataFrame(scores)\n",
    "print(\"\\nCV Results:\")\n",
    "print(results_df.describe())\n",
    "\n",
    "mean_mse = results_df[\"mse\"].mean()\n",
    "mean_mae = results_df[\"mae\"].mean()\n",
    "print(f\"\\nMean MSE: {mean_mse:.5f}\")\n",
    "print(f\"Mean MAE: {mean_mae:.5f}\")\n",
    "\n",
    "# Save predictions\n",
    "all_preds_df = pd.concat(all_preds)\n",
    "all_preds_df.to_csv(\"cv_predictions.csv\", index=False)\n",
    "print(\"Saved cv_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
