{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72b2015",
   "metadata": {},
   "source": [
    "# Loop 1 LB Feedback Analysis\n",
    "\n",
    "**Critical Issue:** CV score 0.0111 vs LB score 0.0982 - a 9x gap!\n",
    "\n",
    "Possible causes:\n",
    "1. Notebook structure non-compliance (evaluator's concern)\n",
    "2. CV methodology mismatch with LB evaluation\n",
    "3. Data leakage in local CV\n",
    "4. Different evaluation metric on LB\n",
    "5. Distribution shift between train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load our submission\n",
    "submission = pd.read_csv('/home/submission/submission.csv')\n",
    "print('Submission shape:', submission.shape)\n",
    "print('\\nColumns:', submission.columns.tolist())\n",
    "print('\\nFirst 10 rows:')\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the submission format\n",
    "print('Task distribution:')\n",
    "print(submission['task'].value_counts())\n",
    "\n",
    "print('\\nFold distribution for task 0 (single solvent):')\n",
    "print(submission[submission['task']==0]['fold'].value_counts().sort_index())\n",
    "\n",
    "print('\\nFold distribution for task 1 (full data):')\n",
    "print(submission[submission['task']==1]['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prediction ranges\n",
    "print('Prediction statistics:')\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f'\\n{col}:')\n",
    "    print(f'  Min: {submission[col].min():.4f}')\n",
    "    print(f'  Max: {submission[col].max():.4f}')\n",
    "    print(f'  Mean: {submission[col].mean():.4f}')\n",
    "    print(f'  Std: {submission[col].std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actual data to compare\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "single_data = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "full_data = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('Single solvent data shape:', single_data.shape)\n",
    "print('Full data shape:', full_data.shape)\n",
    "\n",
    "# Target columns\n",
    "print('\\nTarget columns in single data:', [c for c in single_data.columns if c in ['SM', 'Product 2', 'Product 3']])\n",
    "print('Target columns in full data:', [c for c in full_data.columns if c in ['SM', 'Product 2', 'Product 3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target order - this is critical!\n",
    "# Our submission has target_1, target_2, target_3\n",
    "# The template uses Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "\n",
    "print('Target order in our submission:')\n",
    "print('target_1 = Product 2')\n",
    "print('target_2 = Product 3')\n",
    "print('target_3 = SM')\n",
    "\n",
    "# Let's verify by checking the actual values\n",
    "print('\\nActual target statistics from single solvent data:')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'{col}: mean={single_data[col].mean():.4f}, std={single_data[col].std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ce1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Check if our target order matches the expected order\n",
    "# The template loads Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "# So target_1 = Product 2, target_2 = Product 3, target_3 = SM\n",
    "\n",
    "# But wait - let me check what order we used in our model\n",
    "# In our baseline, we used: Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "# This should be correct!\n",
    "\n",
    "# Let's verify by checking the submission predictions vs actuals\n",
    "print('Checking if our predictions make sense...')\n",
    "print('\\nOur predictions (target_3 = SM):')\n",
    "print(f'  Mean: {submission[\"target_3\"].mean():.4f}')\n",
    "print(f'  Actual SM mean: {single_data[\"SM\"].mean():.4f}')\n",
    "\n",
    "print('\\nOur predictions (target_1 = Product 2):')\n",
    "print(f'  Mean: {submission[\"target_1\"].mean():.4f}')\n",
    "print(f'  Actual Product 2 mean: {single_data[\"Product 2\"].mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me check the template's expected submission format more carefully\n",
    "# The template saves predictions in order: target_1, target_2, target_3\n",
    "# And the model predicts in the order of Y columns\n",
    "\n",
    "# In utils.py, TARGET_LABELS is defined - let me check\n",
    "import sys\n",
    "sys.path.insert(0, '/home/data')\n",
    "\n",
    "# Read utils.py to find TARGET_LABELS\n",
    "with open('/home/data/utils.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    print('Looking for TARGET_LABELS in utils.py...')\n",
    "    for line in content.split('\\n'):\n",
    "        if 'TARGET' in line or 'LABEL' in line:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key insight: The LB evaluation likely uses a different CV procedure\n",
    "# or the submission format is different from what we're producing\n",
    "\n",
    "# Let me check if the issue is with how we're computing MSE locally\n",
    "# vs how the LB computes it\n",
    "\n",
    "# Our local MSE calculation:\n",
    "# MSE = mean((actuals - predictions)^2)\n",
    "\n",
    "# But the LB might be computing it differently\n",
    "# For example, it might be computing MSE per fold and then averaging\n",
    "\n",
    "# Let's recalculate our MSE in different ways\n",
    "print('Recalculating MSE in different ways...')\n",
    "\n",
    "# Method 1: Overall MSE (what we did)\n",
    "print('\\nMethod 1: Overall MSE (our method)')\n",
    "print('Single: 0.010429, Full: 0.011429, Overall: 0.011081')\n",
    "\n",
    "# Method 2: Average MSE per fold\n",
    "print('\\nMethod 2: Would need to recalculate per-fold MSE and average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469321c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS: The LB score of 0.0982 is suspiciously close to the reference kernel's score of 0.09831\n",
    "# This suggests our submission might have been evaluated correctly, but our LOCAL CV is wrong!\n",
    "\n",
    "# The reference kernel (arrhenius-kinetics-tta) achieved LB 0.09831\n",
    "# Our submission got LB 0.0982 - almost identical!\n",
    "\n",
    "# This means:\n",
    "# 1. Our model is working correctly on the LB\n",
    "# 2. Our LOCAL CV calculation is WRONG - it's too optimistic\n",
    "\n",
    "# The issue is likely that we're computing MSE on the wrong data or in the wrong way\n",
    "\n",
    "print('CRITICAL INSIGHT:')\n",
    "print('LB score 0.0982 ≈ Reference kernel score 0.09831')\n",
    "print('This suggests our model is working correctly!')\n",
    "print('The issue is our LOCAL CV calculation is too optimistic.')\n",
    "print('')\n",
    "print('Possible causes:')\n",
    "print('1. We might be computing MSE on training data instead of test data')\n",
    "print('2. We might have data leakage in our CV')\n",
    "print('3. The CV methodology might be different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me check our baseline notebook to see if there's a bug\n",
    "# Looking at the code, I see we store actuals and predictions correctly\n",
    "# But let me verify the MSE calculation\n",
    "\n",
    "# The issue might be that we're computing MSE differently\n",
    "# Let's check if the LB uses a different metric\n",
    "\n",
    "# Actually, looking at the competition description, it says:\n",
    "# \"Submissions will be evaluated according to a cross-validation procedure\"\n",
    "# This means the LB runs the ENTIRE CV procedure, not just evaluates predictions\n",
    "\n",
    "print('KEY INSIGHT:')\n",
    "print('The competition evaluates by RUNNING the CV procedure on Kaggle!')\n",
    "print('This means our local CV predictions are not directly comparable.')\n",
    "print('')\n",
    "print('The LB score of 0.0982 is the ACTUAL CV score from running our model.')\n",
    "print('Our local CV score of 0.0111 might be computed incorrectly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me verify by checking if our predictions are reasonable\n",
    "# If our model is predicting well, the predictions should be close to actuals\n",
    "\n",
    "# Load actuals for single solvent\n",
    "single_actuals = single_data[['Product 2', 'Product 3', 'SM']].values\n",
    "print('Single solvent actuals shape:', single_actuals.shape)\n",
    "\n",
    "# Our predictions for single solvent (task 0)\n",
    "single_preds = submission[submission['task']==0][['target_1', 'target_2', 'target_3']].values\n",
    "print('Single solvent predictions shape:', single_preds.shape)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = np.mean((single_actuals - single_preds) ** 2)\n",
    "print(f'\\nRecalculated Single Solvent MSE: {mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for full data\n",
    "full_actuals = full_data[['Product 2', 'Product 3', 'SM']].values\n",
    "print('Full data actuals shape:', full_actuals.shape)\n",
    "\n",
    "# Our predictions for full data (task 1)\n",
    "full_preds = submission[submission['task']==1][['target_1', 'target_2', 'target_3']].values\n",
    "print('Full data predictions shape:', full_preds.shape)\n",
    "\n",
    "# Calculate MSE\n",
    "mse_full = np.mean((full_actuals - full_preds) ** 2)\n",
    "print(f'\\nRecalculated Full Data MSE: {mse_full:.6f}')\n",
    "\n",
    "# Overall\n",
    "n_single = len(single_actuals)\n",
    "n_full = len(full_actuals)\n",
    "overall_mse = (mse * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "print(f'\\nOverall MSE: {overall_mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The recalculated MSE should match our original calculation\n",
    "# If it doesn't, there's a bug in our original code\n",
    "\n",
    "# Let me also check if the predictions are in the right order\n",
    "# The submission should have predictions for each fold in order\n",
    "\n",
    "print('Checking prediction order...')\n",
    "print('\\nTask 0 (single solvent):')\n",
    "for fold in range(24):\n",
    "    fold_data = submission[(submission['task']==0) & (submission['fold']==fold)]\n",
    "    print(f'  Fold {fold}: {len(fold_data)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I understand the issue!\n",
    "# The submission file contains predictions for EACH FOLD of the CV\n",
    "# But the actuals are the FULL dataset\n",
    "\n",
    "# The correct way to compute MSE is to match predictions to actuals BY FOLD\n",
    "# Each fold's predictions correspond to the test set for that fold\n",
    "\n",
    "# For single solvent CV (leave-one-solvent-out):\n",
    "# - Fold 0 predictions are for solvent 0's data\n",
    "# - Fold 1 predictions are for solvent 1's data\n",
    "# etc.\n",
    "\n",
    "# Let me verify this by checking the number of rows per fold\n",
    "print('Verifying fold structure...')\n",
    "print('\\nSingle solvent data by solvent:')\n",
    "solvent_counts = single_data['SOLVENT NAME'].value_counts().sort_index()\n",
    "print(solvent_counts)\n",
    "\n",
    "print('\\nSubmission rows per fold (task 0):')\n",
    "for fold in range(24):\n",
    "    fold_data = submission[(submission['task']==0) & (submission['fold']==fold)]\n",
    "    print(f'  Fold {fold}: {len(fold_data)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fold sizes should match the solvent counts!\n",
    "# Let me verify this more carefully\n",
    "\n",
    "solvents = sorted(single_data['SOLVENT NAME'].unique())\n",
    "print('Solvents in order:', solvents[:5], '...')\n",
    "\n",
    "for i, solvent in enumerate(solvents):\n",
    "    solvent_count = len(single_data[single_data['SOLVENT NAME'] == solvent])\n",
    "    fold_count = len(submission[(submission['task']==0) & (submission['fold']==i)])\n",
    "    match = '✓' if solvent_count == fold_count else '✗'\n",
    "    print(f'Fold {i} ({solvent}): solvent={solvent_count}, submission={fold_count} {match}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c602a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let me properly compute the MSE by matching predictions to actuals\n",
    "# For each fold, the predictions correspond to the test set (one solvent)\n",
    "\n",
    "def compute_cv_mse_single():\n",
    "    solvents = sorted(single_data['SOLVENT NAME'].unique())\n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, solvent in enumerate(solvents):\n",
    "        # Get actuals for this solvent\n",
    "        mask = single_data['SOLVENT NAME'] == solvent\n",
    "        actuals = single_data[mask][['Product 2', 'Product 3', 'SM']].values\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        fold_preds = submission[(submission['task']==0) & (submission['fold']==fold_idx)]\n",
    "        preds = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(actuals)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    mse = np.mean((all_actuals - all_preds) ** 2)\n",
    "    return mse, all_preds, all_actuals\n",
    "\n",
    "mse_single, preds_single, actuals_single = compute_cv_mse_single()\n",
    "print(f'Properly computed Single Solvent MSE: {mse_single:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for full data (leave-one-ramp-out)\n",
    "def compute_cv_mse_full():\n",
    "    # Get unique ramps\n",
    "    ramps = full_data[['SOLVENT A NAME', 'SOLVENT B NAME']].drop_duplicates()\n",
    "    ramps = ramps.sort_values(['SOLVENT A NAME', 'SOLVENT B NAME']).reset_index(drop=True)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, (_, row) in enumerate(ramps.iterrows()):\n",
    "        # Get actuals for this ramp\n",
    "        mask = (full_data['SOLVENT A NAME'] == row['SOLVENT A NAME']) & \\\n",
    "               (full_data['SOLVENT B NAME'] == row['SOLVENT B NAME'])\n",
    "        actuals = full_data[mask][['Product 2', 'Product 3', 'SM']].values\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        fold_preds = submission[(submission['task']==1) & (submission['fold']==fold_idx)]\n",
    "        preds = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "        \n",
    "        if len(actuals) != len(preds):\n",
    "            print(f'WARNING: Fold {fold_idx} mismatch: actuals={len(actuals)}, preds={len(preds)}')\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(actuals)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    mse = np.mean((all_actuals - all_preds) ** 2)\n",
    "    return mse, all_preds, all_actuals\n",
    "\n",
    "mse_full, preds_full, actuals_full = compute_cv_mse_full()\n",
    "print(f'Properly computed Full Data MSE: {mse_full:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fe9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall MSE\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== FINAL VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nLB Score: 0.0982')\n",
    "print(f'Gap: {abs(overall_mse - 0.0982):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCLUSION:\n",
    "# If the properly computed MSE matches our original calculation (~0.011),\n",
    "# then the issue is that the LB evaluates differently.\n",
    "\n",
    "# If the properly computed MSE is closer to 0.0982,\n",
    "# then our original calculation was wrong.\n",
    "\n",
    "print('\\nCONCLUSION:')\n",
    "if abs(overall_mse - 0.0982) < 0.01:\n",
    "    print('Our properly computed MSE matches LB - original calculation was wrong!')\n",
    "elif abs(overall_mse - 0.011) < 0.01:\n",
    "    print('Our MSE calculation is correct - LB evaluates differently!')\n",
    "    print('This could be due to:')\n",
    "    print('1. Different random seeds on Kaggle')\n",
    "    print('2. Different PyTorch/NumPy versions')\n",
    "    print('3. Different GPU behavior')\n",
    "else:\n",
    "    print(f'MSE is {overall_mse:.6f} - need to investigate further')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
