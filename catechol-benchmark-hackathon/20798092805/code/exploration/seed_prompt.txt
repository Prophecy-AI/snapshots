# Seed Prompt: Chemical Reaction Yield Prediction

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: data shapes, target distributions, CV structure, solvent counts
- Single solvent data: 656 samples, 24 unique solvents (leave-one-solvent-out CV = 24 folds)
- Full/mixture data: 1227 samples, 13 unique ramps (leave-one-ramp-out CV = 13 folds)
- Targets: SM, Product 2, Product 3 (yields in range 0-1, can slightly exceed 1)

## CRITICAL: Submission Structure Requirements
The submission MUST follow the exact template structure from the competition. Only the model definition line can be changed in the last three cells. The model class must implement:
- `train_model(X_train, Y_train)` method
- `predict(X_test)` method returning predictions

## Physics-Informed Feature Engineering (HIGHLY EFFECTIVE)

### Arrhenius Kinetics Features
Chemical reaction rates follow Arrhenius kinetics. Transform raw features:
- `inv_temp = 1000 / (Temperature + 273.15)` - Inverse temperature in Kelvin
- `log_time = ln(Residence Time)` - Logarithm of time
- `interaction = inv_temp * log_time` - Kinetic interaction term

These physics-informed features significantly improve predictions by encoding the underlying chemistry.

### Additional Feature Engineering
- `Reaction_Energy = Temperature * Residence Time`
- `B_Conc_Temp = SolventB% * Temperature` (for mixture data)

## Solvent Featurization
Available pre-computed featurizations (use `load_features()` from utils):
1. **spange_descriptors** (13 features) - Most commonly used baseline, includes dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, n, f(n), delta
2. **acs_pca_descriptors** - PCA-based from ACS Green Chemistry
3. **drfps_catechol** - Differential reaction fingerprints
4. **fragprints** - Fragment + fingerprint concatenation

For mixed solvents, use weighted average: `features = A * (1-pct) + B * pct`

## Chemical Symmetry (CRITICAL FOR MIXTURES)

### Training Data Augmentation
For mixed solvents, a mixture of "Solvent A + Solvent B" is physically identical to "Solvent B + Solvent A". 
- Train on BOTH (A,B) and (B,A flipped) versions to double training data
- This respects the physical symmetry of mixtures

### Test Time Augmentation (TTA)
During inference for mixed solvents:
1. Predict with input as (A, B)
2. Predict with input as (B, A) flipped
3. Final prediction = (Pred1 + Pred2) / 2

This mathematically guarantees symmetry and reduces variance.

## Model Architectures

### Neural Networks (MLP) - Recommended Baseline
Architecture that works well:
- Input BatchNorm
- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)
- Output: 3 neurons with Sigmoid activation (yields are 0-1)
- Loss: MSELoss or HuberLoss (more robust to outliers)
- Optimizer: Adam with lr=5e-4, weight_decay=1e-5
- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)
- Epochs: 300
- Batch size: 32
- Gradient clipping: max_norm=1.0

### Gradient Boosting (LightGBM/XGBoost)
Alternative approach with per-target regressors:
- 3 separate models (one per target)
- LightGBM: lr=0.03, max_depth=6, early_stopping=100 rounds
- XGBoost: n_estimators=1500, lr=0.015, max_depth=6

### Ensemble/Bagging
Bagging multiple models (e.g., 7 MLPs) and averaging predictions improves robustness.

## Post-Processing
- Clip predictions to [0, 1] range
- Optional: Normalize rows to sum to 1 (chemical constraint that yields should sum to ~1)

## Imbalanced Regression Consideration
Yield data is often skewed toward low-yield reactions. Consider:
- Cost-sensitive reweighting to improve high-yield predictions
- HuberLoss instead of MSELoss for robustness to outliers

## Validation Strategy
The competition uses specific CV splits:
- Single solvent: `generate_leave_one_out_splits()` - leaves one solvent out
- Full data: `generate_leave_one_ramp_out_splits()` - leaves one solvent ramp out

This tests generalization to UNSEEN solvents, making it challenging.

## Key Techniques Summary (Priority Order)
1. **Physics-informed features** (Arrhenius kinetics: 1/T, ln(t), interaction)
2. **Chemical symmetry** (data augmentation + TTA for mixtures)
3. **Bagging/ensemble** (average 5-7 models)
4. **Robust loss** (HuberLoss)
5. **Proper architecture** (BatchNorm, Dropout, Sigmoid output)
6. **Learning rate scheduling** (ReduceLROnPlateau)

## Reference Scores
- Baseline MLP: ~0.1 MSE
- With Arrhenius + Symmetry + Bagging: ~0.098 MSE
- Target to beat: 0.0333
