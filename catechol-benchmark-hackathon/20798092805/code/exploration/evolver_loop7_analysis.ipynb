{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acda8457",
   "metadata": {},
   "source": [
    "# Loop 7 Analysis: Simpler Model Results\n",
    "\n",
    "## Key Finding\n",
    "The simpler model [64, 32] achieved the **BEST CV score** (0.009749), a 6.5% improvement over the previous best.\n",
    "\n",
    "## Questions to Answer\n",
    "1. What does this mean for the overfitting hypothesis?\n",
    "2. Should we submit to LB?\n",
    "3. What's the predicted LB score?\n",
    "4. What should we try next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b43067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# All experiments\n",
    "experiments = [\n",
    "    {'exp': 'exp_000', 'name': 'Baseline MLP [128,128,64]', 'cv': 0.011081, 'lb': 0.0982, 'models': 3},\n",
    "    {'exp': 'exp_001', 'name': 'LightGBM', 'cv': 0.012297, 'lb': 0.1065, 'models': 3},\n",
    "    {'exp': 'exp_002', 'name': 'DRFP with PCA', 'cv': 0.016948, 'lb': None, 'models': 5},\n",
    "    {'exp': 'exp_003', 'name': 'Combined [256,128,64]', 'cv': 0.010501, 'lb': 0.0972, 'models': 5},\n",
    "    {'exp': 'exp_004', 'name': 'Deep Residual (FAILED)', 'cv': 0.051912, 'lb': None, 'models': 10},\n",
    "    {'exp': 'exp_005', 'name': 'Large Ensemble [256,128,64]', 'cv': 0.010430, 'lb': 0.0969, 'models': 15},\n",
    "    {'exp': 'exp_006', 'name': 'Simpler [64,32]', 'cv': 0.009749, 'lb': None, 'models': 5},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "df['ratio'] = df['lb'] / df['cv']\n",
    "print('=== ALL EXPERIMENTS ===')\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a61be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the simpler model result\n",
    "print('=== SIMPLER MODEL ANALYSIS ===')\n",
    "print()\n",
    "print('exp_006 (Simpler [64,32]) vs exp_005 (Large Ensemble [256,128,64]):')\n",
    "print(f'  CV: 0.009749 vs 0.010430 → {(0.010430 - 0.009749) / 0.010430 * 100:.1f}% BETTER')\n",
    "print(f'  Architecture: [64, 32] vs [256, 128, 64]')\n",
    "print(f'  Dropout: 0.1 vs 0.3')\n",
    "print(f'  Models: 5 vs 15')\n",
    "print(f'  Training time: ~63 min vs ~6.5 hours')\n",
    "print()\n",
    "print('KEY INSIGHT: The simpler model has BETTER CV, not worse!')\n",
    "print('This suggests the larger models were overfitting even within CV.')\n",
    "print()\n",
    "print('Breakdown by task:')\n",
    "print(f'  Single Solvent: 0.011120 (exp_006) vs 0.011533 (exp_005) → 3.6% better')\n",
    "print(f'  Full Data:      0.009016 (exp_006) vs 0.009841 (exp_005) → 8.4% better')\n",
    "print()\n",
    "print('The improvement is LARGER for Full Data (mixtures), suggesting:')\n",
    "print('  - Simpler models generalize better to mixture combinations')\n",
    "print('  - The complex model was overfitting to specific solvent pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be42348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict LB score for exp_006\n",
    "print('=== LB PREDICTION ===')\n",
    "print()\n",
    "\n",
    "# Calculate average ratio from submitted experiments\n",
    "submitted = df[df['lb'].notna()]\n",
    "avg_ratio = submitted['ratio'].mean()\n",
    "std_ratio = submitted['ratio'].std()\n",
    "\n",
    "print(f'Average CV-LB ratio: {avg_ratio:.2f}x (std: {std_ratio:.2f})')\n",
    "print()\n",
    "\n",
    "# Predict LB for exp_006\n",
    "exp006_cv = 0.009749\n",
    "predicted_lb = exp006_cv * avg_ratio\n",
    "predicted_lb_low = exp006_cv * (avg_ratio - std_ratio)\n",
    "predicted_lb_high = exp006_cv * (avg_ratio + std_ratio)\n",
    "\n",
    "print(f'exp_006 CV: {exp006_cv:.6f}')\n",
    "print(f'Predicted LB: {predicted_lb:.4f} (range: {predicted_lb_low:.4f} - {predicted_lb_high:.4f})')\n",
    "print()\n",
    "print(f'Best current LB: 0.0969 (exp_005)')\n",
    "print(f'Predicted improvement: {(0.0969 - predicted_lb) / 0.0969 * 100:.1f}%')\n",
    "print()\n",
    "print('If the CV-LB ratio holds, exp_006 should achieve LB ~0.088')\n",
    "print('This would be the BEST LB score so far!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we submit?\n",
    "print('=== SUBMISSION DECISION ===')\n",
    "print()\n",
    "print('Arguments FOR submitting exp_006:')\n",
    "print('  1. Best CV score (0.009749) - 6.5% better than previous best')\n",
    "print('  2. Validates simpler model hypothesis on LB')\n",
    "print('  3. If LB improves, opens direction for even simpler models')\n",
    "print('  4. 3 submissions remaining - can afford to test')\n",
    "print()\n",
    "print('Arguments AGAINST submitting exp_006:')\n",
    "print('  1. Could save submission for final ensemble')\n",
    "print('  2. CV improvement may not translate to LB')\n",
    "print()\n",
    "print('RECOMMENDATION: SUBMIT exp_006')\n",
    "print('  - The 6.5% CV improvement is significant')\n",
    "print('  - We need to validate if simpler models have different CV-LB ratio')\n",
    "print('  - This informs our strategy for remaining experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0734cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to try next based on result\n",
    "print('=== NEXT STEPS BASED ON LB RESULT ===')\n",
    "print()\n",
    "print('IF LB improves (e.g., 0.088 or better):')\n",
    "print('  → Simpler model hypothesis VALIDATED')\n",
    "print('  → Try even simpler: [32, 16], [32], or linear models')\n",
    "print('  → The optimal architecture may be much simpler than expected')\n",
    "print('  → Target may be achievable with very simple models')\n",
    "print()\n",
    "print('IF LB stays same (~0.097):')\n",
    "print('  → CV improvement does NOT translate to LB')\n",
    "print('  → The CV-LB gap is NOT due to model complexity')\n",
    "print('  → Need fundamentally different approach:')\n",
    "print('     - Gaussian Processes with Tanimoto kernel')\n",
    "print('     - Per-target models')\n",
    "print('     - Domain adaptation techniques')\n",
    "print()\n",
    "print('IF LB gets worse:')\n",
    "print('  → Simpler models underfit on LB')\n",
    "print('  → The complex model was actually better for generalization')\n",
    "print('  → Focus on other approaches (GPs, per-target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target analysis\n",
    "print('=== TARGET ANALYSIS ===')\n",
    "print()\n",
    "target = 0.0333\n",
    "best_lb = 0.0969\n",
    "best_cv = 0.009749\n",
    "\n",
    "print(f'Target: {target}')\n",
    "print(f'Best LB: {best_lb}')\n",
    "print(f'Best CV: {best_cv}')\n",
    "print()\n",
    "print(f'Gap to target: {(best_lb - target) / target * 100:.0f}% improvement needed')\n",
    "print()\n",
    "print('With 9x CV-LB ratio:')\n",
    "print(f'  To beat {target}, need CV < {target / avg_ratio:.6f}')\n",
    "print(f'  Current best CV: {best_cv}')\n",
    "print(f'  CV improvement needed: {(best_cv - target/avg_ratio) / best_cv * 100:.0f}%')\n",
    "print()\n",
    "print('REALITY CHECK:')\n",
    "print('  - We need ~62% CV improvement to beat target')\n",
    "print('  - Simpler model gave 6.5% improvement')\n",
    "print('  - Would need ~10 such improvements (unlikely)')\n",
    "print('  - BUT: simpler models may have DIFFERENT CV-LB ratio')\n",
    "print('  - If ratio drops to 3x, current CV would give LB ~0.029 (beats target!)')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
