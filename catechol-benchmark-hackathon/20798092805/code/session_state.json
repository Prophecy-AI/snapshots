{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-07T22:12:59.541311",
  "time_limit_minutes": 2100,
  "experiments": [
    {
      "id": "exp_000",
      "name": "Baseline MLP with Arrhenius Kinetics + TTA",
      "model_type": "MLP (3 models bagged)",
      "score": 0.011081,
      "notes": "First baseline experiment implementing key techniques from the strategy:\n1. Arrhenius kinetics features (1/T in Kelvin, ln(time), interaction term)\n2. Spange descriptors (13 features) for solvent featurization\n3. MLP architecture: BatchNorm -> [128, 128, 64] with ReLU, Dropout(0.2) -> Sigmoid output\n4. HuberLoss for robustness to outliers\n5. Data augmentation for mixtures (train on both A,B and B,A)\n6. Test Time Augmentation (TTA) for mixtures - average predictions from both orderings\n7. Bagging 3 models and averaging predictions\n8. Adam optimizer with lr=5e-4, weight_decay=1e-5\n9. ReduceLROnPlateau scheduler (factor=0.5, patience=20)\n10. 200 epochs, batch_size=32\n\nResults:\n- Single Solvent MSE: 0.010429 (24 folds, 656 samples)\n- Full Data MSE: 0.011429 (13 folds, 1227 samples)\n- Overall MSE: 0.011081 (weighted average)\n\nThis is significantly better than the target of 0.0333 and the reference baseline (~0.1 MSE). The physics-informed features and symmetry handling are highly effective.",
      "experiment_folder": "experiments/001_baseline_mlp",
      "timestamp": "2026-01-07T23:14:00.046167"
    },
    {
      "id": "exp_001",
      "name": "LightGBM Baseline with Arrhenius Kinetics + TTA",
      "model_type": "LightGBM (3 per-target regressors)",
      "score": 0.012297,
      "notes": "LightGBM baseline as a more stable/deterministic alternative to MLP.\n\nKey features:\n1. Arrhenius kinetics features (1/T, ln(t), interaction)\n2. Spange descriptors (13 features) for solvent featurization\n3. Per-target regressors (3 separate LightGBM models)\n4. Data augmentation for mixtures (train on both A,B and B,A)\n5. Test Time Augmentation (TTA) for mixtures\n6. LightGBM params: lr=0.03, max_depth=6, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1\n7. 1000 boosting rounds\n\nResults:\n- Single Solvent MSE: 0.012784 (24 folds, 656 samples)\n- Full Data MSE: 0.012037 (13 folds, 1227 samples)\n- Overall MSE: 0.012297 (weighted average)\n\nComparison to MLP baseline:\n- MLP CV: 0.011081 (slightly better)\n- LightGBM CV: 0.012297 (slightly worse)\n- BUT LightGBM is ~60x faster (45 sec vs 50 min)\n- LightGBM is deterministic - should have lower variance on LB\n\nThe key insight is that LightGBM may perform better on LB due to lower variance between runs, even though local CV is slightly worse.",
      "experiment_folder": "experiments/002_lightgbm",
      "timestamp": "2026-01-07T23:26:25.854553"
    },
    {
      "id": "exp_002",
      "name": "DRFP MLP with PCA (100 components)",
      "model_type": "MLP (5 models bagged)",
      "score": 0.016948,
      "notes": "Experiment using DRFP (Differential Reaction Fingerprints) features instead of Spange descriptors.\n\nKey changes from baseline:\n1. DRFP features (2048-dim, 97.4% sparse) instead of Spange (13-dim)\n2. PCA for dimensionality reduction (100 components)\n3. Combined with Arrhenius kinetics features (5 features)\n4. Larger hidden layers [256, 128, 64] to handle more features\n5. 5 models bagged, 250 epochs\n\nResults:\n- Single Solvent MSE: 0.019235 (WORSE than Spange baseline 0.010429)\n- Full Data MSE: 0.015725 (WORSE than Spange baseline 0.011429)\n- Overall MSE: 0.016948 (WORSE than Spange baseline 0.011081)\n\nDRFP features performed WORSE than Spange descriptors! Possible reasons:\n1. PCA loses important information from sparse fingerprints\n2. Linear mixing of DRFP for mixtures doesn't work well\n3. The GNN benchmark that achieved 0.0039 used graph neural networks, not simple MLP\n4. DRFP may need different architecture (e.g., attention mechanisms)\n\nKey insight: The GNN benchmark's success with DRFP was due to the GNN architecture (message-passing, attention), not just the features. Simple MLP with PCA-reduced DRFP doesn't capture the same information.\n\nNext steps: Try combining DRFP + Spange features, or use raw DRFP without PCA.",
      "experiment_folder": "experiments/003_drfp_mlp",
      "timestamp": "2026-01-08T01:10:36.723629"
    }
  ],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.011081,
      "model_name": "Baseline MLP with Arrhenius Kinetics + TTA",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-07T23:14:00.047928"
    },
    {
      "file_path": "/home/code/submission_candidates/candidate_001.csv",
      "score": 0.012297,
      "model_name": "LightGBM Baseline with Arrhenius Kinetics + TTA",
      "experiment_id": "exp_001",
      "timestamp": "2026-01-07T23:26:25.858746"
    },
    {
      "file_path": "/home/code/submission_candidates/candidate_002.csv",
      "score": 0.016948,
      "model_name": "DRFP MLP with PCA (100 components)",
      "experiment_id": "exp_002",
      "timestamp": "2026-01-08T01:10:36.729826"
    }
  ],
  "submissions": [
    {
      "candidate_id": "exp_000",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-07T23:18:48.980624+00:00",
      "model_name": "Baseline MLP with Arrhenius Kinetics + TTA",
      "cv_score": 0.011081,
      "lb_score": 0.09816,
      "error": null
    },
    {
      "candidate_id": "exp_001",
      "experiment_id": "exp_001",
      "timestamp": "2026-01-07T23:28:53.708646+00:00",
      "model_name": "LightGBM Baseline with Arrhenius Kinetics + TTA",
      "cv_score": 0.012297,
      "lb_score": 0.10649,
      "error": null
    }
  ],
  "strategy_history": [
    {
      "strategy": "# Seed Prompt: Chemical Reaction Yield Prediction\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: data shapes, target distributions, CV structure, solvent counts\n- Single solvent data: 656 samples, 24 unique solvents (leave-one-solvent-out CV = 24 folds)\n- Full/mixture data: 1227 samples, 13 unique ramps (leave-one-ramp-out CV = 13 folds)\n- Targets: SM, Product 2, Product 3 (yields in range 0-1, can slightly exceed 1)\n- Temperature range: 175-225\u00b0C, Residence Time: ~2-15 min\n\n## CRITICAL: Submission Structure Requirements\nThe submission MUST follow the exact template structure from the competition. Only the model definition line can be changed in the last three cells. The model class must implement:\n- `train_model(X_train, Y_train)` method\n- `predict(X_test)` method returning predictions\n\nThe utils.py file provides: `load_data()`, `load_features()`, `generate_leave_one_out_splits()`, `generate_leave_one_ramp_out_splits()`\n\n## STATE-OF-THE-ART APPROACH (MSE ~0.0039)\nRecent research (arXiv 2512.19530) achieved MSE of 0.0039 on this exact dataset using:\n- **Graph Attention Networks (GAT)** with molecular graph message-passing\n- **Differential Reaction Fingerprints (DRFP)** - 2048 features available in drfps_catechol_lookup.csv\n- **Mixture-aware solvent encodings** - continuous representation of solvent mixtures\n- Key insight: Explicit molecular graph message-passing and continuous mixture encoding are essential\n\n## Physics-Informed Feature Engineering (HIGHLY EFFECTIVE)\n\n### Arrhenius Kinetics Features\nChemical reaction rates follow Arrhenius kinetics. Transform raw features:\n- `inv_temp = 1000 / (Temperature + 273.15)` - Inverse temperature in Kelvin\n- `log_time = ln(Residence Time)` - Logarithm of time  \n- `interaction = inv_temp * log_time` - Kinetic interaction term\n\nThese physics-informed features significantly improve predictions by encoding the underlying chemistry.\n\n### Additional Feature Engineering\n- `Reaction_Energy = Temperature * Residence Time`\n- `B_Conc_Temp = SolventB% * Temperature` (for mixture data)\n- Polynomial features of temperature and time\n\n## Solvent Featurization\nAvailable pre-computed featurizations (use `load_features()` from utils):\n1. **spange_descriptors** (13 features) - Compact, interpretable: dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, n, f(n), delta\n2. **acs_pca_descriptors** (5 features) - PCA-based from ACS Green Chemistry, very compact\n3. **drfps_catechol** (2048 features) - Differential reaction fingerprints, HIGH-DIMENSIONAL BUT POWERFUL\n4. **fragprints** (2133 features) - Fragment + fingerprint concatenation, high-dimensional\n\nFor mixed solvents, use weighted average: `features = A * (1-pct) + B * pct`\n\n**Recommendation:** DRFP features (2048-dim) showed best results in GNN benchmarks. Consider combining with Arrhenius features.\n\n## Chemical Symmetry (CRITICAL FOR MIXTURES)\n\n### Training Data Augmentation\nFor mixed solvents, a mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\". \n- Train on BOTH (A,B) and (B,A flipped) versions to double training data\n- This respects the physical symmetry of mixtures\n\n### Test Time Augmentation (TTA)\nDuring inference for mixed solvents:\n1. Predict with input as (A, B)\n2. Predict with input as (B, A) flipped\n3. Final prediction = (Pred1 + Pred2) / 2\n\nThis mathematically guarantees symmetry and reduces variance.\n\n## Model Architectures\n\n### Graph Neural Networks (BEST PERFORMANCE)\nGNNs with attention mechanisms achieve state-of-the-art:\n- Graph Attention Networks (GAT) for molecular graphs\n- Message-passing to capture solute-solvent interactions\n- Combine with DRFP features for best results\n\n### Neural Networks (MLP) - Strong Baseline\nArchitecture that works well:\n- Input BatchNorm\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Output: 3 neurons with Sigmoid activation (yields are 0-1)\n- Loss: MSELoss or HuberLoss (more robust to outliers)\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Epochs: 300\n- Batch size: 32\n- Gradient clipping: max_norm=1.0\n\n### Gradient Boosting (LightGBM/XGBoost)\nAlternative approach with per-target regressors:\n- 3 separate models (one per target)\n- LightGBM: lr=0.03, max_depth=6, early_stopping=100 rounds\n- XGBoost: n_estimators=1500, lr=0.015, max_depth=6\n- Can achieve MSE as low as 0.001 on some folds\n\n### Gaussian Processes\nFor small datasets with LOO-CV, GPs with chemistry-aware kernels can be effective:\n- Use GAUCHE library for molecular kernels\n- Tune hyperparameters via LOO marginal likelihood\n- Good uncertainty quantification\n\n### Ensemble/Bagging\nBagging multiple models (e.g., 7 MLPs) and averaging predictions improves robustness.\nConsider stacking different model types (MLP + LightGBM + GP).\n\n## Advanced Techniques\n\n### Multi-Task Learning\nThe 3 targets (SM, Product 2, Product 3) are chemically related. Consider:\n- Shared hidden layers with task-specific heads\n- Multi-output models that learn correlations\n\n### Feature Concatenation\nCombine multiple featurizations:\n- DRFP (2048) + Arrhenius features (3) = powerful combination\n- Or: Spange (13) + ACS PCA (5) + Arrhenius features (3) = 21 features (compact)\n- Use PCA to reduce high-dimensional fingerprints if needed\n\n### Regularization\n- L2 regularization (weight_decay)\n- Dropout (0.2-0.3)\n- Early stopping based on validation loss\n\n## Post-Processing\n- Clip predictions to [0, 1] range\n- Optional: Normalize rows to sum to 1 (chemical constraint that yields should sum to ~1)\n\n## Imbalanced Regression Consideration\nYield data is often skewed toward low-yield reactions. Consider:\n- Cost-sensitive reweighting to improve high-yield predictions\n- HuberLoss instead of MSELoss for robustness to outliers\n- SMOGN or similar techniques for regression imbalance\n\n## Validation Strategy\nThe competition uses specific CV splits:\n- Single solvent: `generate_leave_one_out_splits()` - leaves one solvent out (24 folds)\n- Full data: `generate_leave_one_ramp_out_splits()` - leaves one solvent ramp out (13 folds)\n\nThis tests generalization to UNSEEN solvents, making it challenging. The model must learn transferable representations.\n\n## Key Techniques Summary (Priority Order)\n1. **DRFP features** (2048-dim differential reaction fingerprints) - PROVEN BEST\n2. **Physics-informed features** (Arrhenius kinetics: 1/T, ln(t), interaction) - PROVEN EFFECTIVE\n3. **Chemical symmetry** (data augmentation + TTA for mixtures) - PROVEN EFFECTIVE\n4. **Graph Neural Networks** with attention - STATE-OF-THE-ART\n5. **Bagging/ensemble** (average 5-7 models) - PROVEN EFFECTIVE\n6. **Robust loss** (HuberLoss)\n7. **Proper architecture** (BatchNorm, Dropout, Sigmoid output)\n8. **Learning rate scheduling** (ReduceLROnPlateau)\n\n## Reference Scores\n- Baseline MLP: ~0.1 MSE\n- With Arrhenius + Symmetry + Bagging: ~0.098 MSE (public kernel)\n- LightGBM best folds: ~0.001-0.004 MSE\n- **GNN + DRFP + mixture encoding: ~0.0039 MSE** (state-of-the-art)\n- **Target to beat: 0.0333**\n\n## Implementation Notes\n- Use torch.set_default_dtype(torch.double) for numerical stability\n- The model must work with pandas DataFrames (X_train, Y_train)\n- Predictions should be torch tensors or numpy arrays with shape [N, 3]\n- Order of targets: Product 2, Product 3, SM (check TARGET_LABELS in utils.py)\n- For DRFP features, they are sparse - consider sparse matrix handling\n",
      "agent": "explorer",
      "timestamp": "2026-01-07T22:20:22.096885"
    },
    {
      "strategy": "## Current Status\n- Best CV score: 0.0111 from exp_000 (Baseline MLP with Arrhenius Kinetics + TTA)\n- Best LB score: 0.0982 (from exp_000)\n- CV-LB gap: -0.0871 \u2192 **MASSIVE GAP - CV is unreliable for this competition**\n\n## Response to Evaluator\n\n**Technical verdict was TRUSTWORTHY** with one critical compliance issue. I agree with the evaluator's assessment.\n\n**Evaluator's top priority: RESTRUCTURE THE NOTEBOOK TO COMPLY WITH COMPETITION TEMPLATE**\n\nI **DISAGREE** that this is the top priority. Here's why:\n1. The reference kernel (arrhenius-kinetics-tta) that achieves LB 0.09831 **ALSO does not follow the template** - it has all code in one cell\n2. Our submission got LB 0.0982, which is nearly identical to the reference kernel's 0.09831\n3. This proves our submission WAS evaluated correctly despite not following the template exactly\n\n**Key concerns raised:**\n1. Notebook structure non-compliance \u2192 **Not the actual issue** - our submission was evaluated\n2. DRFP features unexplored \u2192 **AGREE - this is high priority**\n3. Suboptimal hyperparameters (3 models, 200 epochs) \u2192 **AGREE - should increase**\n\n**The REAL issue is the massive CV-LB gap (0.0111 vs 0.0982):**\n- Our local CV calculation is mathematically correct (verified in analysis)\n- The LB runs the ENTIRE notebook from scratch on Kaggle\n- Different random seeds, environment, and GPU behavior cause different results\n- The model has HIGH VARIANCE between runs\n\n## Data Understanding\n- Reference notebooks: `exploration/eda.ipynb`, `exploration/evolver_loop1_lb_feedback.ipynb`\n- Single solvent: 656 samples, 24 solvents (leave-one-solvent-out CV)\n- Full/mixture: 1227 samples, 13 ramps (leave-one-ramp-out CV)\n- Targets: Product 2, Product 3, SM (yields 0-1)\n- **CRITICAL**: Local CV is NOT predictive of LB score due to model variance\n\n## Recommended Approaches (Priority Order)\n\n### 1. **REDUCE MODEL VARIANCE** (HIGHEST PRIORITY)\nThe 9x CV-LB gap indicates our model has extremely high variance. To fix:\n- **Set random seeds explicitly** for reproducibility: `torch.manual_seed(42)`, `np.random.seed(42)`\n- **Increase bagging** from 3 to 7+ models (reference uses 7)\n- **Increase epochs** from 200 to 300 (reference uses 300)\n- **Use deterministic operations** in PyTorch: `torch.backends.cudnn.deterministic = True`\n\n### 2. **Try LightGBM as alternative** (HIGH PRIORITY)\nLightGBM achieved MSE as low as 0.001 on some folds in the reference kernel:\n- More stable/deterministic than neural networks\n- Per-target regressors (3 separate models)\n- Early stopping with 100 rounds patience\n- Hyperparameters: lr=0.03, max_depth=6, regularization\n- Should have much lower variance between runs\n\n### 3. **Try DRFP features** (MEDIUM PRIORITY)\n2048-dimensional differential reaction fingerprints:\n- Achieved MSE ~0.0039 in GNN benchmarks\n- May need PCA for dimensionality reduction\n- Combine with Arrhenius kinetics features\n\n### 4. **Ensemble MLP + LightGBM** (MEDIUM PRIORITY)\nCombine diverse model types for robustness:\n- Average predictions from MLP and LightGBM\n- Different model families capture different patterns\n- Reduces overall variance\n\n## What NOT to Try\n- **Don't obsess over template compliance** - our submission was evaluated correctly\n- **Don't trust local CV scores** - they are not predictive of LB performance\n- **Don't use fewer than 5 models** for bagging - variance is too high\n\n## Validation Notes\n- **LOCAL CV IS UNRELIABLE** for this competition\n- The LB runs the entire notebook from scratch\n- Focus on reducing model variance, not optimizing local CV\n- Use deterministic settings and more bagging\n- Consider LightGBM for more stable predictions\n\n## Key Insight\nThe target score is 0.0333. Our LB score is 0.0982. We need to improve by ~3x.\nThe reference MLP kernel also gets ~0.098, so MLP alone may not be enough.\nLightGBM or ensemble approaches may be needed to beat the target.",
      "agent": "evolver",
      "timestamp": "2026-01-07T23:23:02.378802"
    },
    {
      "strategy": "# Seed Prompt: Chemical Reaction Yield Prediction - Loop 2\n\n## Current Status\n- Best CV score: 0.0111 from exp_000 (MLP)\n- Best LB score: 0.0982 from exp_000 (MLP)\n- CV-LB gap: -0.0871 \u2192 **MASSIVE GAP - CV does not predict LB performance**\n- Target: 0.0333 (need ~66% improvement from current best LB)\n\n## Response to Evaluator\n\n**Technical verdict was TRUSTWORTHY.** The evaluator correctly identified that the LightGBM experiment was sound.\n\n**Evaluator's hypothesis was WRONG:** The evaluator hypothesized that LightGBM's deterministic nature would translate to better LB performance. This was disproven:\n- MLP LB: 0.0982\n- LightGBM LB: 0.1065 (WORSE, not better)\n\n**Key insight:** Tree models (LightGBM) generalize WORSE to unseen solvents than neural networks. The leave-one-solvent-out CV requires the model to extrapolate to completely unseen chemical environments. Neural networks with continuous embeddings handle this better than tree-based models that rely on discrete splits.\n\n**Evaluator's concern about notebook structure:** Valid. The competition requires EXACT template structure in the last 3 cells. Our notebooks may not comply. This needs to be fixed.\n\n## Critical Realization: CV-LB Gap\n\nThe massive CV-LB gap (0.011 vs 0.098) is NOT due to model variance. Both MLP and LightGBM show similar gaps. The gap exists because:\n\n1. **Our local CV calculation may differ from competition evaluation**\n2. **The reference kernel achieves the same LB score (0.098)** - our implementation is correct\n3. **The target of 0.0333 is achievable** - GNN benchmark achieved 0.0039\n\n## Data Understanding\n**Reference notebooks:**\n- `exploration/eda.ipynb` - Data shapes, CV structure, target distributions\n- `exploration/evolver_loop1_analysis.ipynb` - DRFP feature analysis\n- `exploration/evolver_loop2_lb_feedback.ipynb` - CV-LB gap analysis\n\n**Key data facts:**\n- Single solvent: 656 samples, 24 solvents (24-fold leave-one-solvent-out CV)\n- Full data: 1227 samples, 13 ramps (13-fold leave-one-ramp-out CV)\n- Targets: Product 2, Product 3, SM (yields 0-1)\n- All 24 solvents have DRFP features (2048-dim, 97.4% sparse)\n\n## What's Working\n1. **MLP with Arrhenius kinetics** - Achieves LB 0.0982 (matches reference kernel)\n2. **Chemical symmetry handling** - Data augmentation + TTA for mixtures\n3. **Spange descriptors** - 13 compact features, good baseline\n\n## What's NOT Working\n1. **LightGBM** - Worse LB than MLP (0.1065 vs 0.0982). Tree models don't generalize well to unseen solvents.\n2. **Local CV as LB predictor** - Massive gap makes local CV unreliable for model selection\n\n## Recommended Approaches (Priority Order)\n\n### 1. DRFP Features with MLP (HIGH PRIORITY)\n**Why:** GNN benchmark achieved MSE 0.0039 using DRFP features. This is 25x better than our current LB.\n**How:**\n- Load DRFP features (2048-dim) from `drfps_catechol_lookup.csv`\n- Combine with Arrhenius kinetics features (5 features)\n- Use PCA to reduce dimensionality if needed (try 50, 100, 200 components)\n- Train MLP with same architecture as baseline\n\n**Expected improvement:** Significant - DRFP captures molecular structure information that Spange descriptors miss.\n\n### 2. Combined Features: DRFP + Spange + Arrhenius (MEDIUM PRIORITY)\n**Why:** Different feature types capture different aspects of chemistry.\n**How:**\n- Concatenate: DRFP (PCA-reduced) + Spange (13) + Arrhenius (5)\n- Total: ~70-220 features depending on PCA components\n- May need regularization to prevent overfitting\n\n### 3. Ensemble: MLP + Different Feature Sets (MEDIUM PRIORITY)\n**Why:** Diversity in features leads to diversity in predictions.\n**How:**\n- Train MLP with Spange features (current baseline)\n- Train MLP with DRFP features\n- Average predictions\n\n### 4. Hyperparameter Tuning for MLP (LOW PRIORITY)\n**Why:** Current architecture may not be optimal for DRFP features.\n**What to try:**\n- Larger hidden layers for high-dimensional DRFP input\n- Different dropout rates\n- More/fewer bagging models\n\n## What NOT to Try\n1. **LightGBM/XGBoost** - Already proven to generalize worse than MLP on this task\n2. **GNN from scratch** - Too complex, requires molecular graph construction\n3. **Chasing local CV** - Local CV doesn't predict LB; focus on LB-validated approaches\n\n## Validation Notes\n- **DO NOT trust local CV for model selection** - Use LB submissions for validation\n- **Template compliance is CRITICAL** - Last 3 cells must match template exactly\n- **MLP is the baseline to beat** - Any new approach must beat LB 0.0982\n\n## Competition Template Requirements\nThe model class MUST implement:\n- `train_model(X_train, Y_train)` - Training method\n- `predict(X_test)` - Returns predictions (numpy array or tensor)\n\nThe last 3 cells must be EXACTLY as in the template:\n1. Single solvent CV loop with `model = YourModel()` \n2. Full data CV loop with `model = YourModel(data='full')`\n3. Submission file creation\n\n## Next Steps\n1. **Implement DRFP-based MLP** - This is the highest-leverage improvement\n2. **Ensure template compliance** - Critical for valid submission\n3. **Submit to validate** - Only LB scores matter for model selection\n\n## Target Path\n- Current best LB: 0.0982\n- Target: 0.0333\n- GNN benchmark: 0.0039\n\nThe path to beating the target is through better features (DRFP), not better models. The MLP architecture is sufficient; the bottleneck is feature representation.",
      "agent": "evolver",
      "timestamp": "2026-01-07T23:32:11.996419"
    }
  ],
  "feedback_history": [
    {
      "feedback": "## What I Understood\n\nThe junior researcher implemented a baseline MLP model with physics-informed features (Arrhenius kinetics: 1/T, ln(t), interaction term), chemical symmetry handling (data augmentation + TTA for mixtures), and model bagging (3 MLPs). The approach follows best practices identified in the research phase, including Spange descriptors for solvent featurization, HuberLoss for robustness, and proper architecture (BatchNorm, Dropout, Sigmoid output). The local CV score of 0.011081 is significantly better than the target of 0.0333.\n\n## Technical Execution Assessment\n\n**Validation**: The CV methodology is sound - leave-one-solvent-out for single solvent data (24 folds) and leave-one-ramp-out for mixture data (13 folds). This correctly tests generalization to unseen solvents. Per-fold MSE varies from 0.0003 to 0.034, which is expected given different solvent difficulties.\n\n**Leakage Risk**: None detected. The model is trained fresh for each fold, features are computed correctly per-fold, and there's no global fitting of scalers or encoders on test data.\n\n**Score Integrity**: Verified. I independently recalculated the MSE from the submission file and actuals:\n- Single Solvent MSE: 0.010429 \u2713\n- Full Data MSE: 0.011429 \u2713\n- Overall MSE: 0.011081 \u2713\n\n**Code Quality**: The implementation is clean and follows good practices. Seeds are not explicitly set for reproducibility, but this is a minor issue. The code executed successfully.\n\nVerdict: **TRUSTWORTHY** (with one critical compliance issue below)\n\n## Strategic Assessment\n\n**Approach Fit**: Excellent. The physics-informed features (Arrhenius kinetics) directly encode the underlying chemistry of reaction rates. The symmetry handling for mixtures is chemically correct. Using Spange descriptors (13 features) is a reasonable starting point.\n\n**Effort Allocation**: Good initial baseline. The researcher correctly prioritized:\n1. Physics-informed feature engineering (high leverage)\n2. Chemical symmetry handling (domain-specific insight)\n3. Robust architecture (BatchNorm, Dropout, HuberLoss)\n\n**Assumptions**: \n- Linear mixing of solvent features for mixtures (reasonable approximation)\n- Spange descriptors capture relevant solvent properties (validated in literature)\n- 3 models for bagging (could be increased)\n\n**Blind Spots**:\n1. **DRFP features (2048-dim)** - The strategy notes mention these achieved state-of-the-art results (MSE ~0.0039) but haven't been tried yet\n2. **More bagging** - Reference kernel uses 7 models, current uses 3\n3. **More epochs** - Reference uses 300 epochs, current uses 200\n4. **LightGBM/XGBoost** - Alternative model families that achieved MSE as low as 0.001 on some folds\n\n**Trajectory**: Very promising! The score of 0.011 is already 3x better than the target (0.0333). However, the state-of-the-art is reportedly 0.0039, suggesting significant room for improvement.\n\n## What's Working\n\n1. **Physics-informed features** - The Arrhenius kinetics features (1/T, ln(t), interaction) are highly effective\n2. **Chemical symmetry handling** - Data augmentation + TTA for mixtures is correctly implemented\n3. **Robust architecture** - BatchNorm, Dropout, HuberLoss, Sigmoid output all contribute to stability\n4. **Clean implementation** - Code is well-organized and follows the model interface requirements\n\n## Key Concerns\n\n### \ud83d\udea8 CRITICAL: Notebook Structure Non-Compliance\n\n**Observation**: The current notebook does NOT follow the competition template structure. The competition explicitly requires:\n- The last 3 cells must be EXACTLY as in the template\n- Only the model definition line (`model = MLPModel()`) can be changed\n- The template cells have specific \"DO NOT CHANGE\" comments\n\nThe current notebook has:\n- Different cell organization\n- Additional MSE calculation code\n- Different variable names (submission_single vs submission_single_solvent)\n\n**Why it matters**: This submission will likely be INVALID according to competition rules. The competition states: \"In order to ensure fair participation among all competitors, the submission must have the same last three cells as in the notebook template.\"\n\n**Suggestion**: Restructure the notebook to:\n1. Define all model classes and featurizers in earlier cells\n2. Use the EXACT last 3 cells from the template\n3. Only change the model definition line to `model = BaselineMLP(data='single')` and `model = BaselineMLP(data='full')`\n\n### Medium Priority: Unexplored High-Leverage Improvements\n\n**Observation**: The strategy notes mention DRFP features (2048-dim) achieved MSE of 0.0039 in GNN benchmarks, but these haven't been tried.\n\n**Why it matters**: There's potentially a 3x improvement available (0.011 \u2192 0.004) by using better features.\n\n**Suggestion**: Try DRFP features either alone or combined with Arrhenius features. May need dimensionality reduction (PCA) to handle the 2048 dimensions.\n\n### Low Priority: Suboptimal Hyperparameters\n\n**Observation**: Using 3 models and 200 epochs vs. reference kernel's 7 models and 300 epochs.\n\n**Why it matters**: More bagging and training could improve stability and performance.\n\n**Suggestion**: Increase to 5-7 models and 250-300 epochs for the final submission.\n\n## Top Priority for Next Experiment\n\n**RESTRUCTURE THE NOTEBOOK TO COMPLY WITH COMPETITION TEMPLATE**\n\nThis is non-negotiable. The current notebook structure will result in an invalid submission. The model implementation is excellent, but it needs to be packaged correctly:\n\n1. Move all model/featurizer class definitions to earlier cells\n2. Copy the EXACT last 3 cells from the template\n3. Only modify the model definition lines:\n   - Cell -3: `model = BaselineMLP(data='single', n_models=3)`\n   - Cell -2: `model = BaselineMLP(data='full', n_models=3)`\n\nOnce compliance is fixed, consider trying DRFP features for potentially significant improvement.\n",
      "experiment_id": "exp_000",
      "agent": "evaluator",
      "timestamp": "2026-01-07T23:17:23.457925"
    },
    {
      "feedback": "## What I Understood\n\nThe junior researcher implemented a LightGBM baseline as a more deterministic alternative to the MLP model, motivated by the critical finding that the MLP's local CV score (0.011) was dramatically different from its LB score (0.098). The hypothesis was that LightGBM's deterministic nature would reduce variance between runs and potentially perform better on the leaderboard. The implementation uses the same physics-informed features (Arrhenius kinetics), chemical symmetry handling (data augmentation + TTA for mixtures), and Spange descriptors as the MLP baseline.\n\n## Technical Execution Assessment\n\n**Validation**: The CV methodology is sound - leave-one-solvent-out for single solvent data (24 folds) and leave-one-ramp-out for mixture data (13 folds). This correctly tests generalization to unseen solvents. The approach matches the competition's evaluation structure.\n\n**Leakage Risk**: None detected. The model is trained fresh for each fold, features are computed correctly per-fold, and there's no global fitting of scalers or encoders on test data. The Spange descriptors are pre-computed lookup tables, not fitted on training data.\n\n**Score Integrity**: Verified in execution logs:\n- Single Solvent MSE: 0.012784 \u2713\n- Full Data MSE: 0.012037 \u2713\n- Overall MSE: 0.012297 \u2713\n\n**Code Quality**: Clean implementation with proper random seed setting (np.random.seed(42), seed=42 in LightGBM params). The code executed successfully in ~45 seconds (vs ~50 minutes for MLP). The model interface follows the competition requirements.\n\nVerdict: **TRUSTWORTHY**\n\n## Strategic Assessment\n\n**Approach Fit**: This is a smart strategic pivot. The discovery that local CV scores don't match LB scores (because the competition runs notebooks from scratch) is critical. LightGBM is a reasonable choice for reducing variance:\n- Deterministic with fixed seed\n- No GPU randomness issues\n- Much faster iteration time (~60x faster)\n\n**Effort Allocation**: Good prioritization. The researcher correctly identified that model stability/reproducibility is the bottleneck, not raw CV performance. The LightGBM CV (0.012297) is only slightly worse than MLP CV (0.011081), but if it's more stable on LB, it could actually score better.\n\n**Assumptions Being Made**:\n1. The CV-LB gap is primarily due to MLP variance (reasonable assumption given the reference kernel also got ~0.098)\n2. LightGBM will be more stable across environments (likely true)\n3. The slight CV degradation (0.011 \u2192 0.012) is worth the stability gain (needs LB validation)\n\n**Blind Spots**:\n1. **No LB submission of LightGBM yet** - The hypothesis that LightGBM will be more stable on LB is untested. This should be the immediate next step.\n2. **DRFP features still unexplored** - The strategy notes mention DRFP (2048-dim) achieved MSE ~0.0039 in GNN benchmarks. This is potentially a 3x improvement over current results.\n3. **Ensemble approach** - Could combine MLP + LightGBM predictions for potentially better results\n\n**Trajectory**: The research trajectory is sound. The team has:\n1. Established a strong baseline (MLP with physics features)\n2. Discovered a critical issue (CV-LB gap due to model variance)\n3. Proposed a reasonable solution (more deterministic model)\n\nThe next step should be to validate this hypothesis with an LB submission.\n\n## What's Working\n\n1. **Physics-informed features** - The Arrhenius kinetics features (1/T, ln(t), interaction) are highly effective and correctly implemented\n2. **Chemical symmetry handling** - Data augmentation + TTA for mixtures is correctly implemented\n3. **Strategic thinking** - The pivot to LightGBM shows good problem-solving: identify the real issue (variance) and address it directly\n4. **Fast iteration** - LightGBM runs in ~45 seconds vs ~50 minutes for MLP, enabling faster experimentation\n5. **Clean implementation** - Code is well-organized and follows the model interface requirements\n\n## Key Concerns\n\n### HIGH PRIORITY: Validate LightGBM Stability Hypothesis\n\n**Observation**: The LightGBM model has not been submitted to the leaderboard yet. The hypothesis that it will be more stable than MLP is untested.\n\n**Why it matters**: With only 4 submissions remaining, we need to be strategic. If LightGBM performs similarly to MLP on LB (~0.098), the stability hypothesis is wrong and we need a different approach. If it performs better, we've found a winning strategy.\n\n**Suggestion**: Submit the LightGBM model to validate the stability hypothesis. This is the most important next step.\n\n### MEDIUM PRIORITY: Notebook Structure Compliance\n\n**Observation**: The current notebook structure does NOT follow the competition template exactly. The competition requires the last 3 cells to be EXACTLY as in the template, with only the model definition line changed.\n\nCurrent issues:\n- Different variable names (submission_single vs submission_single_solvent)\n- Additional MSE calculation code in the CV cells\n- Different cell organization\n\n**Why it matters**: The competition explicitly states: \"In order to ensure fair participation among all competitors, the submission must have the same last three cells as in the notebook template.\" Non-compliant submissions may be invalid.\n\n**Suggestion**: Before submitting, restructure the notebook to:\n1. Define all model classes and featurizers in earlier cells\n2. Use the EXACT last 3 cells from the template\n3. Only change the model definition lines\n\n### MEDIUM PRIORITY: Unexplored High-Leverage Improvements\n\n**Observation**: DRFP features (2048-dim) reportedly achieved MSE ~0.0039 in GNN benchmarks but haven't been tried.\n\n**Why it matters**: This is potentially a 3x improvement (0.012 \u2192 0.004). Even with some degradation on LB, this could be significant.\n\n**Suggestion**: After validating the LightGBM approach, try DRFP features with LightGBM. May need dimensionality reduction (PCA) to handle the 2048 dimensions.\n\n## Top Priority for Next Experiment\n\n**SUBMIT LIGHTGBM TO LEADERBOARD TO VALIDATE STABILITY HYPOTHESIS**\n\nThe most critical unknown right now is whether LightGBM's deterministic nature translates to better LB performance. With 4 submissions remaining, this validation is essential before investing more effort in either direction.\n\nIf LightGBM LB score is:\n- **Better than MLP (< 0.098)**: Continue with LightGBM, try DRFP features\n- **Similar to MLP (~0.098)**: The variance hypothesis is wrong; investigate other causes (e.g., environment differences, evaluation metric)\n- **Worse than MLP (> 0.098)**: Stick with MLP, focus on reducing its variance (more bagging, fixed seeds everywhere)\n\nBefore submitting, ensure the notebook structure complies with the template requirements.\n",
      "experiment_id": "exp_001",
      "agent": "evaluator",
      "timestamp": "2026-01-07T23:28:13.789391"
    },
    {
      "feedback": "## What I Understood\n\nThe junior researcher tested DRFP (Differential Reaction Fingerprints) features with PCA dimensionality reduction, motivated by the GNN benchmark paper that achieved MSE 0.0039 using DRFP. The hypothesis was that DRFP's molecular structure information would improve upon the Spange descriptors (13 features). The implementation used PCA to reduce 2048-dim DRFP to 100 components, combined with Arrhenius kinetics features, and trained with a larger MLP architecture (256-128-64 vs 128-128-64).\n\n## Technical Execution Assessment\n\n**Validation**: Sound. The CV methodology correctly uses leave-one-solvent-out (24 folds) for single solvent and leave-one-ramp-out (13 folds) for mixtures. This matches the competition's evaluation structure.\n\n**Leakage Risk**: None detected. I verified that PCA is fitted on training solvents only (`fit_pca` extracts unique solvents from training data and fits PCA on those). The featurizer is re-initialized for each fold. No global fitting on test data.\n\n**Score Integrity**: Verified in execution logs:\n- Single Solvent MSE: 0.019235 \u2713\n- Full Data MSE: 0.015725 \u2713\n- Overall MSE: 0.016948 \u2713\n\n**Code Quality**: Clean implementation with proper random seeds. The code executed successfully (~96 minutes total). The model interface follows competition requirements.\n\nVerdict: **TRUSTWORTHY**\n\n## Strategic Assessment\n\n**Approach Fit**: This was a reasonable hypothesis to test. The GNN benchmark achieved 0.0039 with DRFP, suggesting these features contain valuable information. However, the researcher correctly identified in their notes that the GNN's success was due to the architecture (graph attention, message-passing), not just the features. Using DRFP with a simple MLP + PCA doesn't capture the same information.\n\n**Effort Allocation**: This experiment was worth trying but the results (0.016948 vs 0.011081 baseline) confirm that DRFP alone isn't the answer. The researcher's time would now be better spent on:\n1. Combining DRFP + Spange features (complementary information)\n2. Trying different dimensionality reduction (sparse methods, autoencoders)\n3. Focusing on what's actually working (Spange + Arrhenius)\n\n**Assumptions Being Made**:\n1. **PCA preserves important DRFP information** - This is likely FALSE. DRFP is 97.4% sparse with only ~52 non-zero features per solvent. PCA on sparse binary/count data often loses the discriminative information. The non-zero positions ARE the information.\n2. **Linear mixing of DRFP for mixtures works** - Questionable. DRFP encodes molecular structure; linear interpolation may not capture mixture chemistry.\n3. **More features = better** - Not necessarily. The 13 Spange features are carefully curated physicochemical descriptors that directly relate to solvent effects on reactions.\n\n**Blind Spots**:\n1. **Sparse feature handling**: PCA is designed for dense, continuous data. For sparse fingerprints, consider:\n   - Using raw DRFP without PCA (let the MLP learn the important features)\n   - Sparse PCA or NMF (Non-negative Matrix Factorization)\n   - Feature selection based on variance or mutual information\n2. **Feature combination**: Haven't tried DRFP + Spange together. The features capture different aspects (molecular structure vs physicochemical properties).\n3. **The CV-LB gap is understood but not solved**: The reference kernel achieves the same LB (0.098), confirming our implementation is correct. The gap exists because local CV and LB evaluate the same thing but with different random seeds/environments.\n\n**Trajectory**: The DRFP experiment was a reasonable dead-end to explore. The researcher correctly diagnosed why it failed (PCA loses information, GNN architecture was key). The trajectory should now pivot back to improving the Spange-based approach rather than pursuing DRFP further.\n\n## What's Working\n\n1. **Arrhenius kinetics features** - Consistently effective across all experiments\n2. **Chemical symmetry handling** - Data augmentation + TTA for mixtures is correctly implemented\n3. **Spange descriptors** - Still the best feature set (0.011 CV vs 0.017 DRFP)\n4. **Experimental methodology** - Clean implementation, proper validation, good documentation\n5. **Hypothesis testing** - The researcher is systematically testing ideas and learning from failures\n\n## Key Concerns\n\n### HIGH PRIORITY: PCA is Wrong for Sparse Fingerprints\n\n**Observation**: DRFP is 97.4% sparse with only ~52 non-zero features per solvent. PCA assumes dense, continuous data and finds directions of maximum variance - but for sparse binary data, the variance is dominated by the zeros.\n\n**Why it matters**: The information in DRFP is in WHICH features are non-zero, not in continuous variation. PCA likely destroyed this information, explaining why DRFP performed worse than Spange.\n\n**Suggestion**: If pursuing DRFP further:\n1. Try raw DRFP without dimensionality reduction (2048 \u2192 MLP with strong dropout/regularization)\n2. Use sparse-aware methods: Truncated SVD, NMF, or feature selection\n3. Consider a two-stage approach: first identify non-zero features, then use those\n4. Or simply abandon DRFP and focus on improving Spange-based approach\n\n### HIGH PRIORITY: Feature Combination Not Explored\n\n**Observation**: DRFP and Spange capture different information (molecular structure vs physicochemical properties). They haven't been combined.\n\n**Why it matters**: Ensemble of features often outperforms individual feature sets. The GNN benchmark used DRFP for molecular structure AND continuous mixture encodings. Combining both feature types could give the model access to complementary information.\n\n**Suggestion**: Try concatenating Spange (13) + Arrhenius (5) + DRFP-PCA (50-100) = ~70 features. Or even simpler: Spange + Arrhenius + raw DRFP with heavy regularization.\n\n### MEDIUM PRIORITY: The Target Requires 3x Improvement\n\n**Observation**: \n- Current best LB: 0.098 (MLP with Spange + Arrhenius)\n- Target: 0.0333\n- GNN benchmark: 0.0039\n\n**Why it matters**: We need a 3x improvement to beat the target. Feature engineering alone may not be enough. The GNN benchmark achieved 25x better performance using graph neural networks with attention mechanisms.\n\n**Suggestion**: Consider:\n1. More aggressive ensembling (average predictions from multiple diverse models)\n2. Per-target models (separate models for SM, Product 2, Product 3)\n3. Hyperparameter tuning on the Spange-based MLP (learning rate, architecture, regularization)\n4. If time permits, explore simpler graph-based approaches (e.g., molecular fingerprint similarity kernels)\n\n### LOW PRIORITY: Notebook Template Compliance\n\n**Observation**: The current notebook structure doesn't follow the competition template exactly. The competition requires the last 3 cells to be EXACTLY as in the template, with only the model definition line changed.\n\n**Why it matters**: Non-compliant submissions may be invalid.\n\n**Suggestion**: Before any submission, restructure to match template exactly:\n- Use `submission_single_solvent` and `submission_full_data` variable names\n- Remove MSE calculation code from the CV cells\n- Ensure the last 3 cells match the template exactly\n\n## Top Priority for Next Experiment\n\n**COMBINE SPANGE + DRFP FEATURES WITH PROPER HANDLING**\n\nThe DRFP experiment failed because PCA destroyed the sparse fingerprint information. But DRFP still contains valuable molecular structure information that Spange doesn't capture. The next experiment should:\n\n1. **Keep Spange + Arrhenius as the base** (this is working well)\n2. **Add DRFP features without PCA** - either:\n   - Use raw DRFP (2048 features) with strong L1/L2 regularization\n   - Or use feature selection to pick the most informative DRFP features\n   - Or use Truncated SVD instead of PCA (better for sparse data)\n3. **Increase model capacity** to handle the larger feature space\n4. **Add more regularization** (dropout, weight decay) to prevent overfitting\n\nExpected outcome: If DRFP adds complementary information to Spange, the combined model should outperform both individual approaches. If not, we've confirmed that Spange + Arrhenius is the optimal feature set for this problem.\n\n**Alternative priority**: If the combined approach doesn't work, focus on:\n1. Hyperparameter tuning the Spange-based MLP\n2. Ensemble of multiple Spange-based models with different architectures\n3. Per-target models (separate models for each output)\n\nThe target is 0.0333. Our best LB is 0.098. We need a 3x improvement. This is ambitious but achievable if we find the right feature combination or model architecture.\n",
      "experiment_id": "exp_002",
      "agent": "evaluator",
      "timestamp": "2026-01-08T01:13:48.422001"
    }
  ],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics features: 1/Temperature (in Kelvin), ln(Time), and their interaction term improve predictions for chemical reaction yields",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical symmetry TTA: For mixed solvents, predict twice (A,B) and (B,A flipped), then average. This respects physical symmetry and reduces variance.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging 7 MLP models with HuberLoss (robust to outliers), BatchNorm, Dropout 0.2, Sigmoid output, 300 epochs, lr=5e-4 with ReduceLROnPlateau scheduler",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data augmentation for training: For mixed solvents, train on both (A,B) and (B,A flipped) versions to double training data",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "LightGBM with per-target regressors (3 separate models), early stopping 100 rounds, lr=0.03, max_depth=6, regularization terms. Achieved MSE as low as 0.0010 on some folds.",
      "source": "../research/kernels/tobimichigan_catechol-pipeline-with-lightgbm/catechol-pipeline-with-lightgbm.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "XGBoost with feature engineering: Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip to [0,1] and normalize rows to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available solvent featurizations: spange_descriptors, acs_pca_descriptors, drfps_catechol, fragprints, smiles. Spange descriptors commonly used as baseline.",
      "source": "/home/data/utils.py",
      "agent": "explorer"
    },
    {
      "finding": "Single solvent CV: 24 folds (leave-one-solvent-out). Full data CV: 13 folds (leave-one-ramp-out). Targets are SM, Product 2, Product 3 (yields 0-1, but can exceed 1 slightly). Row sums don't always equal 1.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors: 13 features per solvent (dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, n, f(n), delta). 26 solvents in lookup.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Gaussian Processes with chemistry-aware kernels (GAUCHE library) are excellent for small datasets. Use LOO CV for hyperparameter tuning. PHOENICS and GRYFFIN are BO algorithms designed for chemistry with constraint handling.",
      "source": "WebSearch: GP and Bayesian optimization for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: spange_descriptors=13, acs_pca_descriptors=5, drfps_catechol=2048, fragprints=2133. Spange is compact, DRFP/fragprints are high-dimensional fingerprints.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GNN-based architecture with GAT + DRFP + mixture-aware solvent encodings achieved MSE of 0.0039 on Catechol benchmark - 60% error reduction over baselines. Explicit molecular graph message-passing and continuous mixture encoding are essential.",
      "source": "WebSearch: arXiv 2512.19530 - GNN benchmark on Catechol",
      "agent": "explorer"
    },
    {
      "finding": "DRFP features: 2048 dimensions, 97.43% sparse (only ~52.5 non-zero features per solvent on average). All 24 solvents in the data have DRFP features. Missing Water and Acetic Acid but these aren't used directly in the data.",
      "source": "exploration/evolver_loop1_analysis.ipynb",
      "agent": "evolver"
    },
    {
      "finding": "CRITICAL: The competition evaluates by RUNNING the entire notebook on Kaggle, not by evaluating pre-computed predictions. Local CV scores are not directly comparable to LB scores. The LB score of 0.0982 is the actual CV score when the model runs on Kaggle with different random seeds/environment.",
      "source": "exploration/evolver_loop1_lb_feedback.ipynb",
      "agent": "evolver"
    },
    {
      "finding": "Reference kernel (arrhenius-kinetics-tta) achieves LB 0.09831. Our baseline MLP with same techniques achieves LB 0.0982 - nearly identical. This confirms our implementation is correct but the model has high variance or the LB environment differs from local.",
      "source": "exploration/evolver_loop1_lb_feedback.ipynb",
      "agent": "evolver"
    },
    {
      "finding": "LightGBM (LB 0.1065) performed WORSE than MLP (LB 0.0982) despite being deterministic. Tree models may not generalize as well to unseen solvents in leave-one-out CV. The determinism hypothesis was wrong.",
      "source": "exploration/evolver_loop2_lb_feedback.ipynb",
      "agent": "evolver"
    },
    {
      "finding": "GNN benchmark (arXiv 2512.19530) achieved MSE 0.0039 on Catechol using: (1) DRFP features 2048-dim, (2) Graph Attention Networks with molecular graph message-passing, (3) Mixture-aware continuous solvent encodings. This is 25x better than tabular baselines (~0.099). The key is explicit molecular graph message-passing and continuous mixture encoding.",
      "source": "WebSearch: GNN benchmark on Catechol",
      "agent": "evolver"
    }
  ],
  "web_research": [
    {
      "query": "What machine learning techniques work best for predicting chemical reaction yields from solvent properties and reaction conditions?",
      "finding": "Predicting reaction yields from solvent properties and other experimental conditions is essentially a regression problem that benefits from models able to capture both complex, nonlinear relationships and, when available, temporal or textual information about the reaction.  In recent studies, **long short\u2011term memory (LSTM) neural networks** have proved highly effective for time\u2011series sensor data collected during reactions; in a Buchwald\u2011Hartwig coupling dataset, an LSTM learned which sensor signals (e.g., colour, temperature, pressure) were most informative and achieved a mean absolute error of just\u202f1.2\u202f% for instantaneous yield predictions ([sciencedirect](https://www.sciencedirect.com/science/article/pii/S1093326322002352)).  When reactions are represented by SMILES strings, **encoder\u2011transformer architectures** combined with a regression head outperform traditional fingerprint\u2011based models, delivering \u201coutstanding\u201d performance on high\u2011throughput Suzuki\u2011Miyaura and Buchwald\u2011Hartwig",
      "agent": "explorer"
    },
    {
      "query": "What are the best Gaussian Process or Bayesian optimization techniques for small chemical reaction datasets with leave-one-out cross validation?",
      "finding": "Gaussian\u202fprocess (GP) models are the de\u2011facto choice for Bayesian optimisation when only a handful of reaction experiments are available, because they provide calibrated uncertainty estimates that guide sample\u2011efficient exploration.  Recent chemistry\u2011focused work shows that the most reliable GP\u2011based BO pipelines combine **domain\u2011specific kernels** with rigorous model\u2011selection via **leave\u2011one\u2011out (LOO) cross\u2011validation**.  ChemBO, for example, implements a GP with several molecular kernels and introduces a novel **optimal\u2011transport\u2011based kernel** that directly encodes graph\u2011level information of reactants and products, dramatically improving prediction accuracy on very small organic\u2011molecule datasets\u202f([arxiv](https://arxiv.org/abs/1908.01425)).  The open\u2011source GAUCHE library makes these kernels (graph, string, and bit\u2011vector kernels) readily available and integrates them with standard BO loops, allowing chemists to plug\u2011in the most expressive kernel for their reaction representation\u202f(",
      "agent": "explorer"
    },
    {
      "query": "How can neural networks generalize to unseen solvents in chemical reaction prediction using transfer learning or domain adaptation?",
      "finding": "Neural\u2011network models can extend their predictive power to solvents that were never seen during training by treating the solvent as a separate, transferable domain and by learning a representation that can be adapted to new conditions.  A common recipe is to **pre\u2011train** a graph\u2011 or transformer\u2011based model on a very large, chemically diverse set of reactions or solvation calculations that include many solvent\u2013solute pairs, then **fine\u2011tune** the same network on the smaller target dataset.  In practice the model learns a solvent embedding (e.g., a vector derived from the solvent SMILES or from physicochemical descriptors) that is shared across all reactions; when a new solvent appears, its embedding can be inferred from its molecular graph and combined with the reaction graph, allowing the network to make predictions without retraining from scratch.  This approach has been shown to work for solvation free\u2011energy and kinetic\u2011rate predictions, where a BERT model pretrained on the USPTO\u2011S",
      "agent": "explorer"
    },
    {
      "query": "What are the best techniques for predicting chemical reaction yields when you need to generalize to unseen solvents in leave-one-out cross validation?",
      "finding": "Predicting reaction yields while forcing a model to extrapolate to solvents it has never seen is most reliably assessed with a **\u201csolvent\u2011out\u201d leave\u2011one\u2011out cross\u2011validation** scheme, which treats each solvent as a separate test fold and therefore gives an unbiased estimate of performance on truly novel solvents\u202f([semanticscholar](https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735)).  \n\nTo obtain strong predictive power under this regime, recent work recommends a combination of (i) **rich solvent representations**, (ii) **transfer\u2011learning or few\u2011shot strategies**, and (iii) **active\u2011learning loops**.  Continuous physicochemical descriptors (e.g., polarity, hydrogen\u2011bonding capacity) or learned embeddings from graph\u2011neural\u2011network (GNN) models have been shown to capture subtle solvent effects and improve regression accuracy on the Catechol rearrangement benchmark\u202f([arxiv](https://arxiv.",
      "agent": "evolver"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}