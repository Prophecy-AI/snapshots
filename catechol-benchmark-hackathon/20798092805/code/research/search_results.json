{
  "query": "What are the best techniques for improving ensemble predictions when individual models have already been optimized?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensemble performance can still be lifted after each base model has been individually tuned by focusing on **how the predictions are combined and how diversity is enforced**.  Classic combination rules such as majority\u2011vote for classification or simple/weighted averaging for regression are easy to implement and often give a solid boost\u202f([MachineLearningMastery](https://machinelearningmastery.com/combine-predictions-for-ensemble-learning))\u200b.  Adding a **weighting scheme**\u2014for example, assigning higher weights to models that perform best on a validation fold\u2014further reduces error, especially when the models are already strong but make different mistakes\u202f([Magai](https://magai.co/how-to-combine-models-for-accuracy-assessment))\u200b.\n\nA more powerful route is **stacked generalisation** (stacking).  Here a secondary \u201cmeta\u2011learner\u201d is trained on out\u2011of\u2011fold predictions of the base learners, learning optimal nonlinear combination weights and often improving both accuracy and interpretability\u202f([ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2772508122000254))\u200b.  To guarantee that the stacked ensemble is not merely a collection of near\u2011identical models, practitioners can **sample diverse parameter vectors using a global\u2011optimization metaheuristic** and then filter them with structural identifiability or observability checks; this enforces genuine diversity and reduces uncertainty in dynamic predictions\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9805594))\u200b.\n\nRecent research also shows that **joint regularization and calibration** across ensemble members can sharpen predictive performance while producing well\u2011calibrated uncertainties, a crucial factor for high\u2011risk domains\u202f([arXiv](https://arxiv.org/abs/2511.04160))\u200b.  Practical best practices include: using heterogeneous model families (e.g., trees, neural nets, SVMs), training each on different data subsets or with different random seeds (bagging), applying boosting\u2011style sequential re\u2011weighting for hard examples, and validating the ensemble with proper train/validation/test splits\u202f([DataScienceDojo](https://datasciencedojo.com/blog/ensemble-methods-in-machine-learning))\u200b.  Together, these techniques turn an already\u2011optimized set of models into a synergistic ensemble that often outperforms any single constituent.",
      "url": ""
    },
    {
      "title": "3 Primary Ensemble Methods to Enhance an ML Model's Accuracy",
      "text": "3 Primary Ensemble Methods to Enhance an ML Model&#039;s Accuracy\n[](https://datasciencedojo.com/agentic-ai-bootcamp/)\nFor a hands-on learning experience to develop Agentic AI applications, join our Agentic AI Bootcamp today.**Early Bird Discount**\n[](https://datasciencedojo.com/agentic-ai-bootcamp/)\n[**Register](https://datasciencedojo.com/agentic-ai-bootcamp/)\n[![data science dojo logo](https://datasciencedojo.com/wp-content/uploads/data-science-dojo-logo-Blue.png)](https://datasciencedojo.com)\n**\n**\n**\n**\n**\n**\n**\n**\n**\n[](https://datasciencedojo.com)/[Blog](https://datasciencedojo.com/blog/)/Ensemble Methods in Machine Learning: A Comprehensive Guide\n# Ensemble Methods in Machine Learning: A Comprehensive Guide\n![Ensemble Methods in Machine Learning: A Comprehensive Guide](https://datasciencedojo.com/wp-content/uploads/Image_fx22-6.jpg)\n* PublishedAugust 5, 2024\n[Machine Learning](https://datasciencedojo.com/blog-category/machine-learning/)\n![Picture of Muneeb Alam](https://secure.gravatar.com/avatar/9711f12beb58b85f780362930d7ded56?s=50&#038;d=mm&#038;r=g)\n#### Muneeb Alam\n## Want to Build AI agents that can reason, plan, and execute autonomously?\n[Learn More](https://datasciencedojo.com/agentic-ai-bootcamp)\n[Machine learning (ML)](https://datasciencedojo.com/blog/classifying-machine-learning-techniques/)is a field where both art and science converge to create models that can predict outcomes based on data. One of the most effective strategies employed in ML to enhance model performance is ensemble methods.\nRather than relying on a single model, ensemble methods combine multiple models to produce better results. This approach can significantly boost accuracy, reduce overfitting, and improve generalization.\nIn this blog, we&#8217;ll explore various ensemble techniques, their working principles, and their applications in real-world scenarios.\n[![llm bootcamp banner](https://datasciencedojo.com/wp-content/uploads/Banner_1-H_B1-2.png)](<https://datasciencedojo.com/bootcamps/llm-bootcamp/?utm_campaign=On-Site Marketing&amp;utm_source=blog_page&amp;utm_medium=banner&amp;utm_campaign=On-Site+Marketing&amp;utm_source=blog_page&amp;utm_medium=banner>)\n## What Are Ensemble Methods?\nEnsemble methods are techniques that create multiple models and then combine them to produce a more accurate and robust final prediction. The idea is that by aggregating the predictions of several base models, the ensemble can capture the strengths of each individual model while mitigating their weaknesses.\n***Also explore this:[Azure Machine Learning in 5 Simple Steps](https://datasciencedojo.com/blog/r-model-azure-machine-learning/)***\n### Why Use Ensemble Methods?\nEnsemble methods are used to improve the robustness and generalization of machine learning models by combining the predictions of multiple models. This can reduce overfitting and improve performance on unseen data.\n***Read more[Gini Index and Entropy](https://datasciencedojo.com/blog/gini-index-and-entropy/)***\n## Types of Ensemble Methods\nThere are three primary types of ensemble methods: Bagging, Boosting, and Stacking.\n### Bagging (Bootstrap Aggregating)\nBagging involves creating multiple subsets of the original dataset using bootstrap sampling (random sampling with replacement). Each subset is used to train a different model, typically of the same type, such as decision trees. The final prediction is made by averaging (for regression) or voting (for classification) the predictions of all models.\n![bagging - ensemble methods](https://datasciencedojo.com/wp-content/uploads/bagging-ensemble-methods.png)An outlook of bagging &#8211; Source: LinkedIn\nHow Bagging Works:\n* **Bootstrap Sampling:**Create multiple subsets from the original dataset by sampling with replacement.\n* **Model Training:**Train a separate model on each subset.\n* **Aggregation:**Combine the predictions of all models by averaging (regression) or majority voting (classification).### Random Forest\n[Random Forest](https://datasciencedojo.com/blog/random-forest-algorithm/)is a popular bagging method where multiple[decision trees](https://datasciencedojo.com/blog/classification-decision-trees/)are trained on different subsets of the data, and their predictions are averaged to get the final result.\n### Boosting\nBoosting is a sequential ensemble method where models are trained one after another, each new model focusing on the errors made by the previous models. The final prediction is a weighted sum of the individual model\u2019s predictions.\n![boosting - ensemble methods](https://datasciencedojo.com/wp-content/uploads/boosting-ensemble-methods.png)A representation of boosting &#8211; Source: Medium\nHow Boosting Works:\n* **Initialize Weights:**Start with equal weights for all data points.\n* **Sequential Training:**Train a model and adjust weights to focus more on misclassified instances.\n* **Aggregation:**Combine the predictions of all models using a weighted sum.### AdaBoost (Adaptive Boosting)\nIt assigns weights to each instance, with higher weights given to misclassified instances. Subsequent models focus on these hard-to-predict instances, gradually improving the overall performance.\n***You might also like:[ML using Python in Cloud](https://datasciencedojo.com/blog/machine-learning-using-python-in-cloud/)***\n### Gradient Boosting\nIt builds models sequentially, where each new model tries to minimize the residual errors of the combined ensemble of previous models using gradient descent.\n### XGBoost (Extreme Gradient Boosting)\nAn optimized version of Gradient Boosting, known for its speed and performance, is often used in competitions and real-world applications.\n### Stacking\nStacking, or stacked generalization, involves training multiple base models and then using their predictions as inputs to a higher-level meta-model. This meta-model is responsible for making the final prediction.\n![stacking - ensemble methods](https://datasciencedojo.com/wp-content/uploads/stacking-ensemble-methods.png)Visual concept of stacking &#8211; Source: ResearchGate\nHow Stacking Works:\n* **Base Model Training:**Train multiple base models on the training data.\n* **Meta-Model Training:**Use the predictions of the base models as features to train a meta-model.\nExample:\nA typical stacking ensemble might use[logistic regression](https://datasciencedojo.com/blog/logistic-regression-in-r-tutorial/)as the meta-model and decision trees, SVMs, and KNNs as base models.\n[![How generative AI and LLMs work ](https://no-cache.hubspot.com/cta/default/3274755/30ec7f2b-d549-4321-9c90-db54892ea2bc.png)](https://cta-redirect.hubspot.com/cta/redirect/3274755/30ec7f2b-d549-4321-9c90-db54892ea2bc)\n## Benefits of Ensemble Methods\n### Improved Accuracy\nBy combining multiple models, ensemble methods can significantly enhance prediction accuracy.\n### Robustness\nEnsemble models are less sensitive to the peculiarities of a particular dataset, making them more robust and reliable.\n### Reduction of Overfitting\nBy averaging the predictions of multiple models, ensemble methods reduce the risk of[overfitting](https://datasciencedojo.com/blog/overparameterization-in-llms/), especially in high-variance models like decision trees.\n### Versatility\nEnsemble methods can be applied to various types of data and problems, from classification to regression tasks.\n## Applications of Ensemble Methods\nEnsemble methods have been successfully applied in various domains, including:\n* **Healthcare:**Improving the accuracy of disease diagnosis by combining different predictive models.\n* **Finance:**Enhancing stock price prediction by aggregating multiple financial models.\n* **Computer Vision:**Boosting the performance of image classification tasks with ensembles of CNNs.\n***Here&#8217;s a list of the[top 7 books to master your learning on computer vision](https://datasciencedojo.com/blog/top-computer-vision-books/)***\n## Implementing Random Forest in Python\nNow let\u2019s walk through the implementation of a Random Forest classifier...",
      "url": "https://datasciencedojo.com/blog/ensemble-methods-in-machine-learning"
    },
    {
      "title": "Improving dynamic predictions with ensembles of observable models",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<section><h3>Motivation</h3>\n<p>Dynamic mechanistic modelling in systems biology has been hampered by the complexity and variability associated with the underlying interactions, and by uncertain and sparse experimental measurements. Ensemble modelling, a concept initially developed in statistical mechanics, has been introduced in biological applications with the aim of mitigating those issues. Ensemble modelling uses a collection of different models compatible with the observed data to describe the phenomena of interest. However, since systems biology models often suffer from a lack of identifiability and observability, ensembles of models are particularly unreliable when predicting non-observable states.</p></section><section><h3>Results</h3>\n<p>We present a strategy to assess and improve the reliability of a class of model ensembles. In particular, we consider kinetic models described using ordinary differential equations with a fixed structure. Our approach builds an ensemble with a selection of the parameter vectors found when performing parameter estimation with a global optimization metaheuristic. This technique enforces diversity during the sampling of parameter space and it can quantify the uncertainty in the predictions of state trajectories. We couple this strategy with structural identifiability and observability analysis, and when these tests detect possible prediction issues we obtain model reparameterizations that surmount them. The end result is an ensemble of models with the ability to predict the internal dynamics of a biological process. We demonstrate our approach with models of glucose regulation, cell division, circadian oscillations and the JAK-STAT signalling pathway.</p></section><section><h3>Availability and implementation</h3>\n<p>The code that implements the methodology and reproduces the results is available at <a href=\"https://doi.org/10.5281/zenodo.6782638\">https://doi.org/10.5281/zenodo.6782638</a>.</p></section><section><h3>Supplementary information</h3>\n<p>\n<a href=\"#sup1\">Supplementary data</a> are available at <em>Bioinformatics</em> online.</p></section></section><section><h2>1 Introduction</h2>\n<p>Modelling and analysis of cellular networks under uncertainty remain a fundamental challenge in systems biology, biotechnology and bioengineering (<a href=\"#btac755-B12\">Kaltenbach <em>et al.</em>, 2009</a>; <a href=\"#btac755-B20\">Mi\u0161kovi\u0107 and Hatzimanikatis, 2011</a>). Ensemble modelling, a concept initially developed in statistical mechanics (<a href=\"#btac755-B2\">Brown and Sethna, 2003</a>) that uses a collection of different models compatible with the observed data to describe the phenomena of interest, is a suitable strategy to handle a model\u2019s parametric and structural uncertainty (<a href=\"#btac755-B14\">Kirk <em>et al.</em>, 2015</a>; <a href=\"#btac755-B15\">Kremling <em>et al.</em>, 2018</a>; <a href=\"#btac755-B17\">Kuepfer <em>et al.</em>, 2007</a>).</p>\n<p>During the last two decades, the use of model ensembles has started to play an increasingly important role in the study of biological systems (<a href=\"#btac755-B26\">Swigon, 2012</a>), with applications in cell signalling (<a href=\"#btac755-B2\">Brown and Sethna, 2003</a>; <a href=\"#btac755-B17\">Kuepfer <em>et al.</em>, 2007</a>), metabolic networks (<a href=\"#btac755-B10\">Hameri <em>et al.</em>, 2019</a>; <a href=\"#btac755-B11\">Jia <em>et al.</em>, 2012</a>; <a href=\"#btac755-B23\">Saa and Nielsen, 2017</a>; <a href=\"#btac755-B28\">Tran <em>et al.</em>, 2008</a>) and gene expression and regulation (<a href=\"#btac755-B24\">Samee <em>et al.</em>, 2015</a>; <a href=\"#btac755-B30\">Ud-Dean and Gunawan, 2014</a>). However, building model ensembles is typically very computationally costly. Further, recent research has revealed that ensemble modelling can exhibit a number of important pitfalls (<a href=\"#btac755-B25\">Stumpf, 2020</a>), so ensembles must be carefully constructed in order to avoid them.</p>\n<p>Here, we present a strategy to assess and improve the reliability of a class of model ensembles. In particular, we consider kinetic models described using ordinary differential equations (ODEs) with a fixed structure. In previous work (<a href=\"#btac755-B33\">Villaverde <em>et al.</em>, 2015</a>), we developed a consensus-based technique, where the ensemble is built using the sampling from optimization runs of parameter estimation by means of a global optimization metaheuristic that enforces diversity during the sampling of parameter space. This method was successfully used to perform uncertainty quantification of state predictions of large kinetic models (<a href=\"#btac755-B37\">Villaverde <em>et al.</em>, 2022</a>).</p>\n<p>Our new contribution is based on the observation that most models in systems biology suffer from a lack of distinguishability, identifiability and observability (<a href=\"#btac755-B16\">Kreutz <em>et al.</em>, 2012</a>; <a href=\"#btac755-B27\">Szederk\u00e9nyi <em>et al.</em>, 2011</a>; <a href=\"#btac755-B38\">Wieland <em>et al.</em>, 2021</a>). As a consequence, we expect ensembles of models to be particularly unreliable when predicting non-observable states. In order to surmount these difficulties, we present a new methodology that starts by analysing structural identifiability and observability. When these analyses reveal deficiencies in the model structure that could lead to prediction issues, our method searches for model reformulations that surmount those difficulties. Once a fully identifiable and observable model structure has been obtained, we perform parameter estimation and use the results to build an ensemble of models following a systematic procedure described in this article. The resulting ensemble allows making predictions about the time course of internal (i.e. unmeasured) state variables, as well as quantifying their uncertainty. <a href=\"#btac755-F1\">Figure\u00a01</a> illustrates the core idea by means of the glucose regulation model, where the lack of identifiability and observability is surmounted by merging some of the non-identifiable parameters into new variables, yielding a fully observable model.</p>\n<figure><h3>Fig. 1.</h3>\n<p></p>\n<figcaption><p>Illustration of the core idea behind our method. The plots show ensemble predictions of the time course of insulin concentration, produced by the <em>\u03b2</em>IG model of glucose regulation (for details, see Section 3, <a href=\"#btac755-T1\">Table\u00a01</a> and the <a href=\"#sup1\">Supplementary Information</a>). The true trajectory is shown as a red line, the ensemble prediction as a black line, and the darker and lighter blue shaded areas represent the 40% and 80% confidence percentiles, respectively. The left-hand plot shows the ensemble prediction of the original model, in which for the state variable <em>x</em><sub>1</sub>, corresponding to insulin concentration, is unobservable. This model has three states (of which only one, glucose, is measured; the two unmeasured states are unobservable) and five parameters (two of which are unidentifiable). Due to the lack of structural identifiability and observability, the ensemble prediction does not reproduce the true trajectory. The right-hand plot shows the ensemble prediction of the reparameterized model, which is fully observable. As a result of insulin becoming observable, the new simulations of its time course are much closer to the true trajectory, and the confidence intervals are more accurate representations of the prediction uncertainty. The NRMSE for the original model (unobservable) is 0.8516, while the NRMSE for the reparameterized model (observable) is 0.1455, a reduction of 82.91%</p></figcaption></figure><p>In summary, in this article, we address three key issues. The first one is the analysis of the role of structural identifiability and observability in the context of ensemble modelling, a previously overlooked topic. Second, the comparison of the unc...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9805594"
    },
    {
      "title": "How to Combine Models for Accuracy Assessment - Magai",
      "text": "How to Combine Models for Accuracy Assessment\n[![magai icon](https://magai.co/wp-content/uploads/2024/02/magai-icon-color.svg)](https://magai.co/)\n[Magai](https://magai.co)\n# How to Combine Models for Accuracy Assessment\nWritten by:\nDustin W. Stout\n![How to Combine Models for Accuracy Assessment](https://magai.co/wp-content/uploads/2025/07/image_45565e2bea970c74d29276899e5012dc.jpeg)\nWhen it comes to improving predictions in AI, combining models &#8211; known as ensemble methods &#8211; can significantly boost accuracy. Instead of relying on a single model, ensembles use multiple models to balance out errors and improve reliability. Here\u2019s a quick breakdown:\n* **Why Combine Models?**\n* Reduces errors and overfitting.\n* Useful for complex tasks like medical diagnoses or financial predictions.\n* Real-world examples show 5\u201340% accuracy improvements in fields like weather forecasting and healthcare.\n* **Key Ensemble Methods:**\n* **Bagging (e.g.,[Random Forests](https://en.wikipedia.org/wiki/Random_forest)):**Reduces variance by averaging predictions from models trained on different data subsets.\n* **Boosting (e.g.,[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost),[Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)):**Sequentially corrects errors, focusing on hard-to-predict cases.\n* **Voting/Averaging:**Combines predictions from multiple models using majority votes or averages.\n* **Best Practices:**\n* Use diverse models to minimize overfitting.\n* Split data correctly (e.g., train/test/validation).\n* Evaluate with metrics like accuracy, precision, and recall.\nPlatforms like[Magai](https://magai.co/)simplify ensemble modeling by providing access to multiple AI models and tools for organizing and testing predictions.\n**Takeaway:**Ensemble methods are a proven way to improve AI accuracy, especially for challenging tasks, by leveraging the strengths of multiple models.\n## Ensemble Learning Techniques Voting Bagging Boosting Random Forest Stacking in ML by Mahesh Huddar\n[![Embedded YouTube video](https://img.youtube.com/vi/eNyUfpGBLts/0.jpg)](https://www.youtube.com/watch?v=eNyUfpGBLts)\n## Common Ensemble Methods Explained\nThis section dives into three widely-used ensemble methods for improving accuracy when combining models. Each method has its own advantages and is suited to specific situations.\n### Bagging (Bootstrap Aggregating)\nBagging, which stands for Bootstrap Aggregating, works by training multiple models on random subsets of your dataset and then combining their outputs for a more reliable prediction. These subsets are created using**bootstrapping**, a technique where data is sampled with replacement. As a result, each model sees a slightly different version of the dataset. For predictions, bagging averages results for regression tasks or uses majority voting for classification problems.\nThis approach shines with**high-variance models**like decision trees, which are prone to instability &#8211; small changes in the training data can lead to wildly different outcomes. By combining multiple models, bagging reduces variance and helps prevent overfitting.\n> > &#8220;If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy&#8221; &#8211; Leo Breiman\n> For example, bagging has been shown to improve model performance from 82.2% to 95.5%.\nOne of the most popular applications of bagging is**Random Forests**, which combine numerous decision trees trained on different subsets of data. This results in a robust and stable predictor. Plus, since each model is trained independently, bagging can be executed in parallel, making it computationally efficient.\nWhile bagging focuses on reducing variance, boosting methods aim to tackle bias by correcting errors sequentially.\n### Boosting Methods\nUnlike bagging, boosting builds models one after another, with each new model designed to fix the errors made by its predecessors.\nTake**AdaBoost**(Adaptive Boosting), for instance. It starts with simple &#8220;weak learners&#8221; that are only slightly better than random guessing. After each round, AdaBoost increases the weight of misclassified examples, encouraging the next model to focus on these harder cases.\n**Gradient Boosting**, on the other hand, uses a mathematical approach. Instead of adjusting weights like AdaBoost, it trains new models to predict the residual errors from previous models. This makes it particularly effective for handling complex data patterns.\nThe main distinction between bagging and boosting lies in their goals: bagging reduces variance by averaging out individual model noise, while boosting systematically improves weak learners to reduce bias.\n> > &#8220;Boosting is a method used in machine learning to reduce errors in predictive data analysis.&#8221; &#8211; Amazon Web Services\n> Finally, voting and averaging methods provide a simpler way to combine predictions without additional training.\n### Voting and Averaging Approaches\nVoting and averaging are straightforward methods for aggregating predictions from multiple models. They don\u2019t involve complex training but rely on combining outputs to improve overall performance.\nFor**classification tasks**, there are two main voting strategies:\n* **Hard voting**operates like a majority rule &#8211; each model votes for a class, and the class with the most votes wins. This works well when you have a group of strong, complementary models.\n* **Soft voting**takes it a step further by considering the probability scores assigned by each model. It averages these probabilities to make a final prediction, reducing the risk of bias toward any one model.\nFor**regression tasks**, averaging is commonly used:\n* **Simple averaging**takes the mean of all model predictions, smoothing out individual errors and reducing overfitting.\n* **Weighted averaging**assigns more influence to better-performing models by multiplying their predictions by a weight before averaging.|Method|Best For|Key Advantage|When to Use|\nHard Voting|Classification|Easy to understand|When using strong, complementary models|\nSoft Voting|Classification|Accounts for confidence|When models provide probability scores|\nSimple Averaging|Regression|Reduces overfitting|When all models are equally reliable|\nWeighted Averaging|Regression|Highlights better models|When model performance varies significantly|\nVoting and averaging are particularly effective when combining diverse models that have different strengths and weaknesses. By aggregating their outputs, these methods can smooth out individual errors while maintaining the overall accuracy of the ensemble.\n![group of tech specialists collaborating on a large transparent display illustrating the training of ensemble models](https://magai.co/wp-content/uploads/2025/07/magai-how-to-build-and-test-ensemble-models-1024x572.jpg)## How to Build and Test Ensemble Models\nCreating effective ensemble models requires a thoughtful approach that balances the need for diversity with the goal of accuracy. This involves carefully selecting and training base models, then evaluating how well their combined predictions improve over individual model performance.\n### Choosing and Setting Up Base Models\nThe foundation of a strong ensemble lies in selecting models that complement each other. The key is**diversity**&#8211; each model should have distinct strengths and weaknesses so they can compensate for one another\u2019s errors. This variety ensures the ensemble benefits from a range of predictions rather than amplifying similar biases.\nTo achieve this, you can:\n* Use different algorithms (e.g., decision trees, linear regression, neural networks).\n* Train models on different subsets of your data.\n* Adjust hyperparameters to create variation during training.\nFor example, pairing a high-variance decision tree with a low-variance linear regression model, along with a neural network, can provide a well-rounded ensemble.\nAt the same time, it\u2019s important to**bal...",
      "url": "https://magai.co/how-to-combine-models-for-accuracy-assessment"
    },
    {
      "title": "Model stacking to improve prediction and variable importance ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/digital-chemical-engineering\"><span><span></span></span></a></p><p><a href=\"https://www.sciencedirect.com/journal/digital-chemical-engineering/vol/3/suppl/C\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><h2>Abstract</h2><div><p>This paper presents an interpretable ensemble modelling method, in which the predictions of several individual base learners are combined together through Stacked generalisation, which makes use of a secondary layer model, or so called <em>meta-learner</em>, that is trained on the output cross-validation predictions of each base learner. To provide interpretability, the permutation variable importance (PVI) is computed on the ensemble, wherein variables are randomly shuffled and the reduction in predictive performance for the ensemble is calculated for each variable. This is a novel contribution, as no previous attempts have been made in the soft sensor literature to investigate the interpretability of ensemble models that use heterogeneous base learners. The Stacked ensemble model also avoids model selection, which is the process of choosing among many candidate models. Model selection is often based on cross-validation, which is not guaranteed to select the best model in terms of true generalisation performance on the test set. Instead, the proposed method combines multiple models instead of choosing a singular model, avoiding the need for model selection. The efficacy of the proposed methodology in terms of both variable importance and predictive performance is shown on a synthetic dataset, in which the variable importance is already known, and an industrial dataset of a refinery process provided by Dow. For the synthetic dataset, it is shown that the proposed method chooses the correct casual variables, whereas the in-built variable importance provided by the individual models, namely Partial least squares, Lasso, Random forests &amp; XGBoost, can give increased importance to non-causal, randomly generated variables. For the industrial study, the combined ensemble is shown to outperform all individual base models in terms of predictive performance, whilst also providing a new perspective in terms of variable importance compared to previous studies.</p></div></div><ul><li></li><li></li></ul><div><h2>Keywords</h2><p><span>Machine learning</span></p><p><span>Soft sensor</span></p><p><span>Model interpretability</span></p></div><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2022 The Author(s). Published by Elsevier Ltd on behalf of Institution of Chemical Engineers (IChemE).</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2772508122000254"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2511.04160] On Joint Regularization and Calibration in Deep Ensembles\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2511.04160\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2511.04160**(cs)\n[Submitted on 6 Nov 2025 ([v1](https://arxiv.org/abs/2511.04160v1)), last revised 7 Nov 2025 (this version, v2)]\n# Title:On Joint Regularization and Calibration in Deep Ensembles\nAuthors:[Laurits Fredsgaard](https://arxiv.org/search/cs?searchtype=author&amp;query=Fredsgaard,+L),[Mikkel N. Schmidt](https://arxiv.org/search/cs?searchtype=author&amp;query=Schmidt,+M+N)\nView a PDF of the paper titled On Joint Regularization and Calibration in Deep Ensembles, by Laurits Fredsgaard and 1 other authors\n[View PDF](https://arxiv.org/pdf/2511.04160)[HTML (experimental)](https://arxiv.org/html/2511.04160v2)> > Abstract:\n> Deep ensembles are a powerful tool in machine learning, improving both model performance and uncertainty calibration. While ensembles are typically formed by training and tuning models individually, evidence suggests that jointly tuning the ensemble can lead to better performance. This paper investigates the impact of jointly tuning weight decay, temperature scaling, and early stopping on both predictive performance and uncertainty quantification. Additionally, we propose a partially overlapping holdout strategy as a practical compromise between enabling joint evaluation and maximizing the use of data for training. Our results demonstrate that jointly tuning the ensemble generally matches or improves performance, with significant variation in effect size across different tasks and metrics. We highlight the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering an attractive practical solution. We believe our findings provide valuable insights and guidance for practitioners looking to optimize deep ensemble models. Code is available at: [> this https URL\n](https://github.com/lauritsf/ensemble-optimality-gap)> Comments:|39 pages, 8 figures, 11 tables|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2511.04160](https://arxiv.org/abs/2511.04160)[cs.LG]|\n|(or[arXiv:2511.04160v2](https://arxiv.org/abs/2511.04160v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2511.04160](https://doi.org/10.48550/arXiv.2511.04160)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|Transactions on Machine Learning Research (2025) ISSN: 2835-8856|\n## Submission history\nFrom: Laurits Fredsgaard [[view email](https://arxiv.org/show-email/235c68a3/2511.04160)]\n**[[v1]](https://arxiv.org/abs/2511.04160v1)**Thu, 6 Nov 2025 08:04:19 UTC (911 KB)\n**[v2]**Fri, 7 Nov 2025 02:19:40 UTC (911 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled On Joint Regularization and Calibration in Deep Ensembles, by Laurits Fredsgaard and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2511.04160)\n* [HTML (experimental)](https://arxiv.org/html/2511.04160v2)\n* [TeX Source](https://arxiv.org/src/2511.04160)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2511.04160&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2511.04160&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-11](https://arxiv.org/list/cs.LG/2025-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2511.04160?context=cs)\n[stat](https://arxiv.org/abs/2511.04160?context=stat)\n[stat.ML](https://arxiv.org/abs/2511.04160?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.04160)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.04160)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.04160)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2511.04160&amp;description=On Joint Regularization and Calibration in Deep Ensembles>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2511.04160&amp;title=On Joint Regularization and Calibration in Deep Ensembles>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index...",
      "url": "https://arxiv.org/abs/2511.04160"
    },
    {
      "title": "Ensemble Learning: Combining Models for Improved Performance",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6be8297bbe08&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40juanc.olamendy%2Fensemble-learning-combining-models-for-improved-performance-6be8297bbe08&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40juanc.olamendy%2Fensemble-learning-combining-models-for-improved-performance-6be8297bbe08&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Ensemble Learning: Combining Models for Improved Performance\n\n[Juan C Olamendy](https://medium.com/@juanc.olamendy?source=post_page---byline--6be8297bbe08---------------------------------------)\n\n12 min read\n\n\u00b7\n\nApr 16, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nIn the field of machine learning, ensemble learning has emerged as a powerful technique to improve the performance and robustness of predictive models.\n\nEnsemble learning involves combining multiple models to make more accurate and reliable predictions than any single model could achieve on its own.\n\nBy leveraging the strengths of different models and mitigating their weaknesses, ensemble learning has proven to be a valuable tool in various domains, from computer vision to natural language processing.\n\nIn this article, we will dive deep into the concepts, techniques, and applications of ensemble learning, exploring how it can help us build better machine learning systems.\n\n## The Wisdom of Crowds: Why Ensemble Learning Works\n\nThe fundamental idea behind ensemble learning is rooted in the concept of the \u201cwisdom of crowds.\u201d\n\nThis concept suggests that the collective opinion of a group of individuals is often more accurate than the opinion of any single individual within the group.\n\nIn the context of machine learning, this translates to the idea that combining the predictions of multiple models can lead to better results than relying on a single model.\n\nThere are several reasons why ensemble learning works:\n\n1. Diversity: Ensemble learning thrives on diversity among the individual models. When models make different errors on the same input, combining their predictions can effectively cancel out the errors and yield a more accurate overall prediction. By using models with different architectures, training data, or hyperparameters, ensemble learning can exploit the strengths of each model while mitigating their weaknesses.\n2. Bias-Variance Tradeoff: Ensemble learning can help navigate the bias-variance tradeoff, which is a fundamental challenge in machine learning. High-bias models tend to underfit the data, while high-variance models tend to overfit. Ensemble learning allows us to combine models with different bias-variance characteristics to achieve a better balance. For example, combining high-bias models can reduce the overall bias, while combining high-variance models can reduce the overall variance.\n3. Robustness: Ensemble learning can make predictions more robust to noise, outliers, and adversarial attacks. By aggregating the predictions of multiple models, ensemble learning can smooth out the impact of individual model errors and provide more stable and reliable predictions. This is particularly important in real-world applications where the data may be noisy or subject to adversarial manipulation.\n\n## Ensemble Learning Techniques\n\nThere are several popular ensemble learning techniques that have proven effective in practice.\n\nLet\u2019s explore some of the most widely used approaches:\n\n## Voting\n\nVoting is a straightforward ensemble technique where the predictions of multiple models are combined through a voting mechanism.\n\nIn classification tasks, the most common voting methods are:\n\n- Hard Voting: Each model casts a vote for the predicted class label, and the class with the majority of votes is chosen as the final prediction. In case of a tie, a predefined rule (e.g., choosing the class with the highest probability) can be used to break the tie.\n- Soft Voting: Instead of casting votes for class labels, each model provides a probability distribution over the classes. The probabilities are summed across all models for each class, and the class with the highest sum of probabilities is chosen as the final prediction. Soft voting allows models to express their confidence in each class and can lead to more nuanced predictions.\n- Voting can be effective when the individual models have similar performance but make different errors. By combining their predictions, voting can reduce the impact of individual model errors and improve the overall accuracy.\n\nPress enter or click to view image in full size\n\n## Bagging (Bootstrap Aggregating)\n\nBagging, short for Bootstrap Aggregating, is an ensemble technique that combines multiple models trained on different subsets of the training data.\n\nThe key steps in bagging are:\n\n1. Bootstrap Sampling: Create multiple subsets of the training data by randomly sampling with replacement. Each subset, called a bootstrap sample, has the same size as the original dataset, but some instances may be repeated while others may be omitted due to the random sampling.\n2. Model Training: Train a separate model on each bootstrap sample. The models are typically of the same type (e.g., decision trees) but can have different hyperparameters.\n3. Aggregation: Combine the predictions of all the models to obtain the final prediction.\n\n- For classification tasks, the most common aggregation method is majority voting, where the class predicted by the majority of the models is chosen as the final prediction.\n- For regression tasks, the average or weighted average of the predictions from all the models is used.\n\nBagging helps reduce overfitting by training models on different subsets of the data.\n\nEach model may overfit to its specific subset, but the aggregation of predictions from all the models helps to smooth out the individual model\u2019s biases and reduce the overall generalization error.\n\nBagging works well with unstable models, such as deep decision trees, where small changes in the training data can lead to significantly different models.\n\nRandom Forests, a popular ensemble method, is an extension of bagging that introduces additional randomness during the model training process.\n\nIn Random Forests, the individual models are decision trees, and at each split, only a random subset of features is considered.\n\nThis further increases the diversity among the trees and helps reduce overfitting.\n\nPress enter or click to view image in full size\n\n## Boosting\n\nBoosting is an ensemble technique that combines multiple weak learners (models that perform slightly better than random guessing) to create a strong learner with improved prediction accuracy.\n\nThe key idea behind boosting is to sequentially train weak learners, where each subsequent learner focuses on the instances that were misclassified by the previous learners.\n\nThe final prediction is obtained by weighted voting of all the weak learners.\n\nOne of the most popular boosting algorithms is AdaBoost (Adaptive Boosting).\n\nIn AdaBoost, the training instances are assigned weights, and the algorithm iteratively trains weak lea...",
      "url": "https://medium.com/@juanc.olamendy/ensemble-learning-combining-models-for-improved-performance-6be8297bbe08"
    },
    {
      "title": "How to Combine Predictions for Ensemble Learning",
      "text": "### [Navigation](https://machinelearningmastery.com/combine-predictions-for-ensemble-learning/\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onApril 27, 2021in[Ensemble Learning](https://machinelearningmastery.com/category/ensemble-learning/)[0](https://machinelearningmastery.com/combine-predictions-for-ensemble-learning/#respond)\n\nShare _Tweet_Share\n\nEnsemble methods involve combining the predictions from multiple models.\n\nThe **combination of the predictions** is a central part of the ensemble method and depends heavily on the types of models that contribute to the ensemble and the type of prediction problem that is being modeled, such as a classification or regression.\n\nNevertheless, there are common or standard techniques that can be used to combine predictions that can be easily implemented and often result in good or best predictive performance.\n\nIn this post, you will discover common techniques for combining predictions for ensemble learning.\n\nAfter reading this post, you will know:\n\n- Combining predictions from contributing models is a key property of an ensemble model.\n- Voting techniques are most commonly used when combining predictions for classification.\n- Statistical techniques are most commonly used when combining predictions for regression.\n\n**Kick-start your project** with my new book [Ensemble Learning Algorithms With Python](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n![How to Combine Predictions for Ensemble Learning](https://machinelearningmastery.com/wp-content/uploads/2020/11/How-to-Combine-Predictions-for-Ensemble-Learning.jpg)\n\nHow to Combine Predictions for Ensemble Learning\n\nPhoto by [cafuego](https://www.flickr.com/photos/cafuego/35846456894/), some rights reserved.\n\n## Tutorial Overview\n\nThis tutorial is divided into three parts; they are:\n\n1. Combining Predictions for Ensemble Learning\n2. Combining Classification Predictions\n1. Combining Predicted Class Labels\n2. Combining Predicted Class Probabilities\n3. Combining Regression Predictions\n\n## Combining Predictions for Ensemble Learning\n\nA key part of an ensemble learning method involves combining the predictions from multiple models.\n\nIt is through the combination of the predictions that the benefit of the ensemble learning method is achieved, namely better predictive performance. As such, there are many ways that predictions can be combined, so much so that it is an entire field of study.\n\n> After generating a set of base learners, rather than trying to find the best single learner, ensemble methods resort to combination to achieve a strong generalization ability, where the combination method plays a crucial role.\n\n\u2014 Page 67, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.\n\nStandard ensemble machine learning algorithms do prescribe how to combine predictions; nevertheless, it is important to consider the topic in isolation for a number of reasons, such as:\n\n- Interpreting the predictions made by standard ensemble algorithms.\n- Manually specifying a custom prediction combination method for an algorithm.\n- Developing your own ensemble methods.\n\nEnsemble learning methods are typically not very complex and developing your own ensemble method or specifying the manner in which predictions are combined is relatively easy and common practice.\n\nThe way that predictions are combined depends on the models that are making predictions and the type of prediction problem.\n\n> The strategy used in this step depends, in part, on the type of classifiers used as ensemble members. For example, some classifiers, such as support vector machines, provide only discrete-valued label outputs.\n\n\u2014 Page 6, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.\n\nFor example, the form of the predictions made by the models will match the type of prediction problem, such as regression for predicting numbers and classification for predicting class labels. Additionally, some model types may be only able to predict a class label or class probability distribution, whereas others may be able to support both for a classification task.\n\nWe will use this division of prediction type based on problem type as the basis for exploring the common techniques used to combine predictions from contributing models in an ensemble.\n\nIn the next section, we will take a look at how to combine predictions for classification predictive modeling tasks.\n\n## Combining Classification Predictions\n\n[Classification](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) refers to predictive modeling problems that involve predicting a class label given an input.\n\nThe prediction made by a model may be a crisp class label directly or may be a probability that an example belongs to each class, referred to as the probability of class membership.\n\nThe performance of a classification problem is often measured using accuracy or a related count or ratio of correct predictions. In the case of evaluating predicted probabilities, they may be converted to crisp class labels by selecting a cut-off threshold, or evaluated using specialized metrics such as [cross-entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/).\n\nWe will review combining predictions for classification separately for both class labels and probabilities.\n\n### Want to Get Started With Ensemble Learning?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nDownload Your FREE Mini-Course\n\n### Combining Predicted Class Labels\n\nA predicted class label is often mapped to something meaningful to the problem domain.\n\nFor example, a model may predict a color such as \u201c _red_\u201d or \u201c _green_\u201c. Internally though, the model predicts a numerical representation for the class label such as 0 for \u201c _red_\u201c, 1 for \u201c _green_\u201c, and 2 for \u201c _blue_\u201d for our color classification example.\n\nMethods for combining class labels are perhaps easier to consider if we work with the integer encoded class labels directly.\n\nPerhaps the simplest, most common, and often most effective approach is to combine the predictions by voting.\n\n> Voting is the most popular and fundamental combination method for nominal outputs.\n\n\u2014 Page 71, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.\n\n[Voting](https://machinelearningmastery.com/voting-ensembles-with-python/) generally involves each model that makes a prediction assigning a vote for the class that was predicted. The votes are tallied and an outcome is then chosen using the votes or tallies in some way.\n\nThere are many types of voting, so let\u2019s look at the four most common:\n\n- Plurality Voting.\n- Majority Voting.\n- Unanimous Voting.\n- Weighted Voting.\n\nSimple voting, called **plurality voting**, selects the class label with the most votes.\n\nIf two or more classes have the same number of votes, then the tie is broken arbitrarily, although in a consistent manner, such as sorting the class labels that have a tie and selecting the first, instead of selecting one randomly. This is important so that the same model with the same data always makes the same prediction.\n\nGiven ties, it is common to have an odd number of ensemble members in an attempt to automatically break ties, as opposed to an even number of ensemble members where ties may be more likely.\n\nFrom a statistical perspective, this is called the mode or the most common value from the collection of predictions.\n\nFor example, consider the three predictions made by a model for a three-class color prediction problem:\n\n- Model 1 predicts \u201c _green_\u201d or 1.\n- Model 2 predicts \u201c _green_\u201d or 1.\n- Model 3 predicts \u201c _red_\u201d or 0.\n\nThe votes are, therefore:\n\n- Red Votes: 1\n- Green Votes: 2\n- Blue Votes: 0\n\nThe prediction would be \u201c _green_\u201d given it has the most votes.\n\n**Majority voting** selects the class label that...",
      "url": "https://www.machinelearningmastery.com/combine-predictions-for-ensemble-learning"
    },
    {
      "title": "Ensemble Learning Methods for Deep Learning Neural Networks",
      "text": "Ensemble Learning Methods for Deep Learning Neural Networks - MachineLearningMastery.comEnsemble Learning Methods for Deep Learning Neural Networks - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Deep Learning Performance Crash-Course]()\n# Ensemble Learning Methods for Deep Learning Neural Networks\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 6, 2019in[Deep Learning Performance](https://machinelearningmastery.com/category/better-deep-learning/)[**45](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/#comments)\nShare*Post*Share\n#### How to Improve Performance By Combining Predictions From Multiple Models.\nDeep learning neural networks are nonlinear methods.\nThey offer increased flexibility and can scale in proportion to the amount of training data available. A downside of this flexibility is that they learn via a stochastic training algorithm which means that they are sensitive to the specifics of the training data and may find a different set of weights each time they are trained, which in turn produce different predictions.\nGenerally, this is referred to as neural networks having a high variance and it can be frustrating when trying to develop a final model to use for making predictions.\nA successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model.\nIn this post, you will discover methods for deep learning neural networks to reduce variance and improve prediction performance.\nAfter reading this post, you will know:\n* Neural network models are nonlinear and have a high variance, which can be frustrating when preparing a final model for making predictions.\n* Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error.\n* Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.\n**Kick-start your project**with my new book[Better Deep Learning](https://machinelearningmastery.com/better-deep-learning/), including*step-by-step tutorials*and the*Python source code*files for all examples.\nLet\u2019s get started.\n![Ensemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks](https://machinelearningmastery.com/wp-content/uploads/2018/12/Ensemble-Methods-to-Reduce-Variance-and-Improve-Performance-of-Deep-Learning-Neural-Networks.jpg)\nEnsemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks\nPhoto by[University of San Francisco&#8217;s Performing Arts](https://www.flickr.com/photos/usfperformingarts/8769648469/), some rights reserved.\n## Overview\nThis tutorial is divided into four parts; they are:\n1. High Variance of Neural Network Models\n2. Reduce Variance Using an Ensemble of Models\n3. How to Ensemble Neural Network Models\n4. Summary of Ensemble Techniques## High Variance of Neural Network Models\nTraining deep neural networks can be very computationally expensive.\nVery deep networks trained on millions of examples may take days, weeks, and sometimes months to train.\n> Google\u2019s baseline model [&#8230;] was a deep convolutional neural network [&#8230;] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.\n&#8212;[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), 2015.\nAfter the investment of so much time and resources, there is no guarantee that the final model will have low generalization error, performing well on examples not seen during training.\n> &#8230; train many different candidate networks and then to select the best, [&#8230;] and to discard the rest. There are two disadvantages with such an approach. First, all of the effort involved in training the remaining networks is wasted. Second, [&#8230;] the network which had best performance on the validation set might not be the one with the best performance on new test data.\n&#8212; Pages 364-365,[Neural Networks for Pattern Recognition](https://amzn.to/2I9gNMP), 1995.\nNeural network models are a nonlinear method. This means that they can learn complex nonlinear relationships in the data. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset.\nThis stochastic nature of the learning algorithm means that each time a neural network model is trained, it may learn a slightly (or dramatically) different version of the mapping function from inputs to outputs, that in turn will have different performance on the training and holdout datasets.\nAs such, we can think of a neural network as a method that has a low bias and high variance. Even when trained on large datasets to satisfy the high variance, having any variance in a final model that is intended to be used to make predictions can be frustrating.\n### Want Better Results with Deep Learning?\nTake my free 7-day email crash course now (with sample code).\nClick to sign-up and also get a free PDF Ebook version of the course.\nDownload Your FREE Mini-Course\n## Reduce Variance Using an Ensemble of Models\nA solution to the high variance of neural networks is to train multiple models and combine their predictions.\nThe idea is to combine the predictions from multiple good but different models.\nA good model has skill, meaning that its predictions are better than random chance. Importantly, the models must be good in different ways; they must make different prediction errors.\n> The reason that model averaging works is that different models will usually not make all the same errors on the test set.\n&#8212; Page 256,[Deep Learning](https://amzn.to/2NJW3gE), 2016.\nCombining the predictions from multiple neural networks adds a bias that in turn counters the variance of a single trained neural network model. The results are predictions that are less sensitive to the specifics of the training data, choice of training scheme, and the serendipity of a single training run.\nIn addition to reducing the variance in the prediction, the ensemble can also result in better predictions than any single best model.\n> &#8230; the performance of a committee can be better than the performance of the best single network used in isolation.\n&#8212; Page 365,[Neural Networks for Pattern Recognition](https://amzn.to/2I9gNMP), 1995.\nThis approach belongs to a general class of methods called &#8220;*ensemble learning*&#8221; that describes methods that attempt to make the best use of the predictions from multiple models prepared for the same problem.\nGenerally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a prediction before combining the predictions in some way to make a final outcome or prediction.\nIn fact, ensembling of models is a standard approach in applied machine learning to ensure that the most stable and best possible prediction is made.\nFor example, Alex Krizhevsky, et al. in their famous 2012 paper titled &#8220;[Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)&#8221; that introduced very deep convolutional neural networks for photo classification (i.e. AlexNet) used model averaging across multiple well-performing CNN models to achieve state...",
      "url": "https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks"
    }
  ]
}