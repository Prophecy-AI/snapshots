{
  "query": "What are the best Gaussian Process or Bayesian optimization techniques for small chemical reaction datasets with leave-one-out cross validation?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Gaussian\u202fprocess (GP) models are the de\u2011facto choice for Bayesian optimisation when only a handful of reaction experiments are available, because they provide calibrated uncertainty estimates that guide sample\u2011efficient exploration.  Recent chemistry\u2011focused work shows that the most reliable GP\u2011based BO pipelines combine **domain\u2011specific kernels** with rigorous model\u2011selection via **leave\u2011one\u2011out (LOO) cross\u2011validation**.  ChemBO, for example, implements a GP with several molecular kernels and introduces a novel **optimal\u2011transport\u2011based kernel** that directly encodes graph\u2011level information of reactants and products, dramatically improving prediction accuracy on very small organic\u2011molecule datasets\u202f([arxiv](https://arxiv.org/abs/1908.01425)).  The open\u2011source GAUCHE library makes these kernels (graph, string, and bit\u2011vector kernels) readily available and integrates them with standard BO loops, allowing chemists to plug\u2011in the most expressive kernel for their reaction representation\u202f([fsk\u2011lab](https://fsk-lab.github.io/proceedings.neurips.cc/paper_files/paper/2023/hash/f2b1b2e974fa5ea622dd87f22815f423-Abstract-Conference.html)).\n\nFor the optimisation engine itself, **PHOENICS** and **GRYFFIN** have been extended to handle arbitrary experimental constraints and discrete reaction parameters, and they have been benchmarked on both synthetic test functions and realistic flow\u2011reaction scenarios\u202f([rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00028h)).  These algorithms are particularly well\u2011suited to small datasets because they exploit the GP\u2019s posterior variance to propose the next experiment while respecting known feasibility limits.  When combined with LOO CV\u2014described in the classic GPML text as a reliable estimator of predictive performance and hyper\u2011parameter marginal likelihood\u2014researchers can objectively select kernel hyper\u2011parameters and compare competing kernel families on the same leave\u2011one\u2011out basis\u202f([gpml](https://gaussianprocess.org/gpml/chapters/RW5.pdf)).\n\nIn practice, the best workflow for a tiny reaction dataset therefore follows three steps: (1) encode reactions with a chemistry\u2011aware kernel (e.g., ChemBO\u2019s OT kernel or GAUCHE\u2019s graph kernels); (2) tune the kernel hyper\u2011parameters by maximising the LOO marginal likelihood; and (3) run a constraint\u2011aware BO loop using PHOENICS or GRYFFIN to suggest new experiments.  This combination has been shown to achieve high sample efficiency and robust predictive performance on the limited data typical of early\u2011stage reaction optimisation.",
      "url": ""
    },
    {
      "title": "ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:1908.01425** (cs)\n\n\\[Submitted on 5 Aug 2019 ( [v1](https://arxiv.org/abs/1908.01425v1)), last revised 22 Oct 2019 (this version, v2)\\]\n\n# Title:ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations\n\nAuthors: [Ksenia Korovina](https://arxiv.org/search/cs?searchtype=author&query=Korovina,+K), [Sailun Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+S), [Kirthevasan Kandasamy](https://arxiv.org/search/cs?searchtype=author&query=Kandasamy,+K), [Willie Neiswanger](https://arxiv.org/search/cs?searchtype=author&query=Neiswanger,+W), [Barnabas Poczos](https://arxiv.org/search/cs?searchtype=author&query=Poczos,+B), [Jeff Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider,+J), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+E+P)\n\nView a PDF of the paper titled ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations, by Ksenia Korovina and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/1908.01425)\n\n> Abstract:In applications such as molecule design or drug discovery, it is desirable to have an algorithm which recommends new candidate molecules based on the results of past tests. These molecules first need to be synthesized and then tested for objective properties. We describe ChemBO, a Bayesian optimization framework for generating and optimizing organic molecules for desired molecular properties. While most existing data-driven methods for this problem do not account for sample efficiency or fail to enforce realistic constraints on synthesizability, our approach explores the synthesis graph in a sample-efficient way and produces synthesizable candidates. We implement ChemBO as a Gaussian process model and explore existing molecular kernels for it. Moreover, we propose a novel optimal-transport based distance and kernel that accounts for graphical information explicitly. In our experiments, we demonstrate the efficacy of the proposed approach on several molecular optimization problems.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Chemical Physics (physics.chem-ph); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1908.01425](https://arxiv.org/abs/1908.01425) \\[cs.LG\\] |\n|  | (or [arXiv:1908.01425v2](https://arxiv.org/abs/1908.01425v2) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.1908.01425](https://doi.org/10.48550/arXiv.1908.01425)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ksenia Korovina \\[ [view email](https://arxiv.org/show-email/361a60e1/1908.01425)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1908.01425v1)**\nMon, 5 Aug 2019 00:12:54 UTC (2,270 KB)\n\n**\\[v2\\]**\nTue, 22 Oct 2019 00:36:27 UTC (2,664 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations, by Ksenia Korovina and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/1908.01425)\n- [TeX Source](https://arxiv.org/src/1908.01425)\n- [Other Formats](https://arxiv.org/format/1908.01425)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1908.01425&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1908.01425&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-08](https://arxiv.org/list/cs.LG/2019-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1908.01425?context=cs)\n\n[physics](https://arxiv.org/abs/1908.01425?context=physics)\n\n[physics.chem-ph](https://arxiv.org/abs/1908.01425?context=physics.chem-ph)\n\n[stat](https://arxiv.org/abs/1908.01425?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1908.01425?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1908.01425)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1908.01425)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1908.01425)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1908.html#abs-1908-01425) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1908-01425)\n\n[Kirthevasan Kandasamy](https://dblp.uni-trier.de/search/author?author=Kirthevasan%20Kandasamy)\n\n[Willie Neiswanger](https://dblp.uni-trier.de/search/author?author=Willie%20Neiswanger)\n\n[Barnab\u00e1s P\u00f3czos](https://dblp.uni-trier.de/search/author?author=Barnab%C3%A1s%20P%C3%B3czos)\n\n[Jeff Schneider](https://dblp.uni-trier.de/search/author?author=Jeff%20Schneider)\n\n[Eric P. Xing](https://dblp.uni-trier.de/search/author?author=Eric%20P.%20Xing)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1908.01425&description=ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1908.01425&title=ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorser...",
      "url": "https://arxiv.org/abs/1908.01425"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "# GAUCHE: A Library for Gaussian Processes in Chemistry\nRyan-Rhys Griffiths, Leo Klarner, Henry Moss, Aditya Ravuri, Sang Truong, Yuanqi Du, Samuel Stanton, Gary Tom, Bojana Rankovic, Arian Jamasb, Aryan Deshwal, Julius Schwartz, Austin Tripp, Gregory Kell, Simon Frieder, Anthony Bourached, Jacob Moss, Chengzhi Guo, Johannes P. D\u00fcrholt, Saudamini Chaurasia, Ji Won Park, Felix Strieth-Kalthoff, Alpha Lee, Bingquing Cheng, Al\u00e1n Aspuru-Guzik, Philippe Schwaller, Jian Tang\nDecember 2022\n[DOI](https://doi.org/https://doi.org/10.48550/arXiv.2212.04450)\n### Abstract\nWe introduce GAUCHE, an open-source library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to molecular representations, however, necessitates kernels defined over structured inputs such as graphs, strings and bit vectors. By providing such kernels in a modular, robust and easy-to-use framework, we seek to enable expert chemists and materials scientists to make use of state-of-the-art black-box optimization techniques. Motivated by scenarios frequently encountered in practice, we showcase applications for GAUCHE in molecular discovery, chemical reaction optimisation and protein design. The codebase is made available at [https://github.com/leojklarner/gauche](https://github.com/leojklarner/gauche).",
      "url": "https://fsk-lab.github.io/proceedings.neurips.cc/paper_files/paper/2023/hash/f2b1b2e974fa5ea622dd87f22815f423-Abstract-Conference.html"
    },
    {
      "title": "Bayesian optimization with known experimental and design constraints for chemistry applications",
      "text": "Received\n1st April 2022\n, Accepted 13th September 2022\n\nFirst published on 14th September 2022\n\n* * *\n\n## Abstract\n\nOptimization strategies driven by machine learning, such as Bayesian optimization, are being explored across experimental sciences as an efficient alternative to traditional design of experiment. When combined with automated laboratory hardware and high-performance computing, these strategies enable next-generation platforms for autonomous experimentation. However, the practical application of these approaches is hampered by a lack of flexible software and algorithms tailored to the unique requirements of chemical research. One such aspect is the pervasive presence of constraints in the experimental conditions when optimizing chemical processes or protocols, and in the chemical space that is accessible when designing functional molecules or materials. Although many of these constraints are known a priori, they can be interdependent, non-linear, and result in non-compact optimization domains. In this work, we extend our experiment planning algorithms PHOENICS and GRYFFIN such that they can handle arbitrary known constraints via an intuitive and flexible interface. We benchmark these extended algorithms on continuous and discrete test functions with a diverse set of constraints, demonstrating their flexibility and robustness. In addition, we illustrate their practical utility in two simulated chemical research scenarios: the optimization of the synthesis of o-xylenyl Buckminsterfullerene adducts under constrained flow conditions, and the design of redox active molecules for flow batteries under synthetic accessibility constraints. The tools developed constitute a simple, yet versatile strategy to enable model-based optimization with known experimental constraints, contributing to its applicability as a core component of autonomous platforms for scientific discovery.\n\n* * *\n\n## I. Introduction\n\nThe design of advanced materials and functional molecules often relies on combinatorial, high-throughput screening strategies enabled by high-performance computing and automated laboratory equipment. Despite the successes of high-throughput experimentation in chemistry, [1,2](http://pubs.rsc.org/pubs.rsc.org#cit1) biology, [3,4](http://pubs.rsc.org/pubs.rsc.org#cit3) and materials science, [5](http://pubs.rsc.org/pubs.rsc.org#cit5) these approaches typically employ exhaustive searches that scale exponentially with the size of the search space. Data-driven strategies that can adaptively search parameter spaces without the need for exhaustive exploration are thus replacing traditional design of experiment approaches in many instances. These strategies use machine-learnt surrogate models trained on all data generated through the experimental campaign, and are updated each time new data is collected. One such approach is Bayesian optimization which, based on the surrogate model, defines a utility function that prioritize experiments based on their expected informativeness and performance. [6\u20138](http://pubs.rsc.org/pubs.rsc.org#cit6) These data-driven optimization strategies have already demonstrated superior performance in chemistry and materials science applications, e.g., in reaction optimization, [9,10](http://pubs.rsc.org/pubs.rsc.org#cit9) the discovery of magnetic resonance imaging agents, [11](http://pubs.rsc.org/pubs.rsc.org#cit11) the fabrication of organic photovoltaic materials, [12,13](http://pubs.rsc.org/pubs.rsc.org#cit12) virtual screening of ultra-large chemical libraries, [14](http://pubs.rsc.org/pubs.rsc.org#cit14) and the design of mechanical structures with additively manufactured components. [15](http://pubs.rsc.org/pubs.rsc.org#cit15)\n\nMachine learning-driven experiment planning strategies can also be combined with automated laboratory hardware or high-performance computing to create self-driving platforms capable of achieving research goals autonomously. [16\u201322](http://pubs.rsc.org/pubs.rsc.org#cit16) Prototypes of these autonomous research platforms have already shown promise in diverse applications, including the optimization of chemical reaction conditions, [9,10](http://pubs.rsc.org/pubs.rsc.org#cit9) the design of photocatalysts for the production of hydrogen from water, [23](http://pubs.rsc.org/pubs.rsc.org#cit23) the discovery of battery electrolytes, [24](http://pubs.rsc.org/pubs.rsc.org#cit24) the design of nanoporous materials with tailored adsorption properties, [25](http://pubs.rsc.org/pubs.rsc.org#cit25) the optimization of multicomponent polymer blend formulations for organic photovoltaics, [12](http://pubs.rsc.org/pubs.rsc.org#cit12) the discovery of phase-change memory materials for photonic switching devices, [26](http://pubs.rsc.org/pubs.rsc.org#cit26) and self-optimization of metal nanoparticle synthesis, [27](http://pubs.rsc.org/pubs.rsc.org#cit27) to name a few. [13,28](http://pubs.rsc.org/pubs.rsc.org#cit13) While self-driving platforms seem poised to deliver a next-generation approach to scientific discovery, their practical application is hampered by a lack of flexible software and algorithms tailored to the unique requirements of chemical research.\n\nTo provide chemistry-tailored data-driven optimization tools, our group has developed PHOENICS [29](http://pubs.rsc.org/pubs.rsc.org#cit29) and GRYFFIN, [30](http://pubs.rsc.org/pubs.rsc.org#cit30) among others. [31\u201333](http://pubs.rsc.org/pubs.rsc.org#cit31) PHOENICS is a linear-scaling Bayesian optimizer for continuous spaces that uses a kernel regression surrogate model and natively supports batched optimization. GRYFFIN is an extension of this algorithm to categorical, as well as mixed continuous-categorical spaces. Furthermore, GRYFFIN is able to leverage expert knowledge in the form of descriptors to enhance its optimization performance, which was found particularly useful in combinatorial optimizations of molecules and materials. [30](http://pubs.rsc.org/pubs.rsc.org#cit30) As GRYFFIN is the more general algorithm, and PHOENICS is included within its capabilities, from here on we will refer only to GRYFFIN. These algorithms have already found applications ranging from the optimization of reaction conditions [10](http://pubs.rsc.org/pubs.rsc.org#cit10) and synthetic protocols, [27,34](http://pubs.rsc.org/pubs.rsc.org#cit27) to that of manufacturing of thin film materials [13](http://pubs.rsc.org/pubs.rsc.org#cit13) and organic photovoltaic devices. [12](http://pubs.rsc.org/pubs.rsc.org#cit12) However, a number of extensions are still required to make these tools suitable to the broadest range of chemistry applications. In particular, GRYFFIN, like the majority of Bayesian optimization tools available, does not handle known experimental or design constraints.\n\nThere are often many constraints on the experiment being performed or molecule being designed. A flexible data-driven optimization tool should be able to accommodate and handle such constraints. The type of constraints typically encountered may be separated into those that affect the objectives of the optimization (e.g., reaction yield, desired molecular properties), and those that affect the optimization parameters (e.g., reaction conditions). Those affecting the objectives usually arise in multi-objective optimization, where one would like to optimize a certain property while constraining another to be above/below a desired value. [31,35,36](http://pubs.rsc.org/pubs.rsc.org#cit31) For instance, we might want to improve the solubility of a drug candidate, while keeping its protein-binding affinity in the nanomolar range. Conversely, parameter constraints limit the range of experiments or molecules we have access to. Depending on the source of the constraints, these may be referred to as known or unknown. Known constraints are those we are aware of a priori, [37\u201340](http://pubs.rsc.org/pubs.rsc.org#cit37) while unknown ones are discovered through experimentation. [37\u201340](http://pubs.rsc.org/pubs.rs...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00028h"
    },
    {
      "title": "",
      "text": "C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006,\nISBN 026218253X. \rc 2006 Massachusetts Institute of Technology. www.GaussianProcess.org/gpml\nChapter 5\nModel Selection and\nAdaptation of\nHyperparameters\nIn chapters 2 and 3 we have seen how to do regression and classification using\na Gaussian process with a given fixed covariance function. However, in many\npractical applications, it may not be easy to specify all aspects of the covari\u0002ance function with confidence. While some properties such as stationarity of\nthe covariance function may be easy to determine from the context, we typically\nhave only rather vague information about other properties, such as the value\nof free (hyper-) parameters, e.g. length-scales. In chapter 4 several examples\nof covariance functions were presented, many of which have large numbers of\nparameters. In addition, the exact form and possible free parameters of the\nlikelihood function may also not be known in advance. Thus in order to turn\nGaussian processes into powerful practical tools it is essential to develop meth\u0002ods that address the model selection problem. We interpret the model selection model selection\nproblem rather broadly, to include all aspects of the model including the dis\u0002crete choice of the functional form for the covariance function as well as values\nfor any hyperparameters.\nIn section 5.1 we outline the model selection problem. In the following sec\u0002tions different methodologies are presented: in section 5.2 Bayesian principles\nare covered, and in section 5.3 cross-validation is discussed, in particular the\nleave-one-out estimator. In the remaining two sections the different methodolo\u0002gies are applied specifically to learning in GP models, for regression in section\n5.4 and classification in section 5.5.\nC. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006,\nISBN 026218253X. \rc 2006 Massachusetts Institute of Technology. www.GaussianProcess.org/gpml\n106 Model Selection and Adaptation of Hyperparameters\n5.1 The Model Selection Problem\nIn order for a model to be a practical tool in an application, one needs to make\ndecisions about the details of its specification. Some properties may be easy to\nspecify, while we typically have only vague information available about other\naspects. We use the term model selection to cover both discrete choices and the\nsetting of continuous (hyper-) parameters of the covariance functions. In fact,\nmodel selection can help both to refine the predictions of the model, and give\nenable interpretation a valuable interpretation to the user about the properties of the data, e.g. that\na non-stationary covariance function may be preferred over a stationary one.\nA multitude of possible families of covariance functions exists, including\nsquared exponential, polynomial, neural network, etc., see section 4.2 for an\nhyperparameters overview. Each of these families typically have a number of free hyperparameters\nwhose values also need to be determined. Choosing a covariance function for a\nparticular application thus comprises both setting of hyperparameters within a\nfamily, and comparing across different families. Both of these problems will be\ntreated by the same methods, so there is no need to distinguish between them,\nand we will use the term \u201cmodel selection\u201d to cover both meanings. We will\ntraining refer to the selection of a covariance function and its parameters as training of\na Gaussian process.1In the following paragraphs we give example choices of\nparameterizations of distance measures for stationary covariance functions.\nCovariance functions such as the squared exponential can be parameterized\nin terms of hyperparameters. For example\nk(xp, xq) = \u03c3\n2\nf\nexp \u2212\n1\n2\n(xp \u2212 xq)\n>M(xp \u2212 xq)\n\u0001\n+ \u03c3\n2\nn\n\u03b4pq, (5.1)\nwhere \u03b8 = ({M}, \u03c32\nf\n, \u03c32\nn\n)\n> is a vector containing all the hyperparameters,2 and\n{M} denotes the parameters in the symmetric matrix M. Possible choices for\nthe matrix M include\nM1 = `\n\u22122\nI, M2 = diag(`)\n\u22122\n, M3 = \u039b\u039b> + diag(`)\n\u22122\n, (5.2)\nwhere ` is a vector of positive values, and \u039b is a D \u00d7 k matrix, k < D. The\nproperties of functions with these covariance functions depend on the values of\nthe hyperparameters. For many covariance functions it is easy to interpret the\nmeaning of the hyperparameters, which is of great importance when trying to\nunderstand your data. For the squared exponential covariance function eq. (5.1)\nwith distance measure M2 from eq. (5.2), the `1, . . . , `D hyperparameters play\ncharacteristic the r\u02c6ole of characteristic length-scales; loosely speaking, how far do you need\nlength-scale to move (along a particular axis) in input space for the function values to be\u0002automatic relevance come uncorrelated. Such a covariance function implements automatic relevance\ndetermination determination (ARD) [Neal, 1996], since the inverse of the length-scale deter\u0002mines how relevant an input is: if the length-scale has a very large value, the\n1This contrasts the use of the word in the SVM literature, where \u201ctraining\u201d usually refers\nto finding the support vectors for a fixed kernel.\n2Sometimes the noise level parameter, \u03c32\nn is not considered a hyperparameter; however it\nplays an analogous role and is treated in the same way, so we simply consider it a hyperpa\u0002rameter.\nC. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006,\nISBN 026218253X. \rc 2006 Massachusetts Institute of Technology. www.GaussianProcess.org/gpml\n5.1 The Model Selection Problem 107\n\u22122\n0\n2\n\u22122\n0\n2\n\u22122\n\u22121\n0\n1\n2\ninput x1 input x2\noutput y\n(a)\n\u22122\n0\n2\n\u22122\n0\n2\n\u22122\n\u22121\n0\n1\n2\ninput x1 input x2\noutput y\n\u22122\n0\n2\n\u22122\n0\n2\n\u22122\n\u22121\n0\n1\n2\ninput x1 input x2\noutput y\n(b) (c)\nFigure 5.1: Functions with two dimensional input drawn at random from noise free\nsquared exponential covariance function Gaussian processes, corresponding to the\nthree different distance measures in eq. (5.2) respectively. The parameters were: (a)\n` = 1, (b) ` = (1, 3)>, and (c) \u039b = (1, \u22121)>, ` = (6, 6)>. In panel (a) the two inputs\nare equally important, while in (b) the function varies less rapidly as a function of x2\nthan x1. In (c) the \u039b column gives the direction of most rapid variation .\ncovariance will become almost independent of that input, effectively removing\nit from the inference. ARD has been used successfully for removing irrelevant\ninput by several authors, e.g. Williams and Rasmussen [1996]. We call the pa\u0002rameterization of M3 in eq. (5.2) the factor analysis distance due to the analogy factor analysis distance\nwith the (unsupervised) factor analysis model which seeks to explain the data\nthrough a low rank plus diagonal decomposition. For high dimensional datasets\nthe k columns of the \u039b matrix could identify a few directions in the input space\nwith specially high \u201crelevance\u201d, and their lengths give the inverse characteristic\nlength-scale for those directions.\nIn Figure 5.1 we show functions drawn at random from squared exponential\ncovariance function Gaussian processes, for different choices of M. In panel\n(a) we get an isotropic behaviour. In panel (b) the characteristic length-scale\nis different along the two input axes; the function varies rapidly as a function\nof x1, but less rapidly as a function of x2. In panel (c) the direction of most\nrapid variation is perpendicular to the direction (1, 1). As this figure illustrates,\nC. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006,\nISBN 026218253X. \rc 2006 Massachusetts Institute of Technology. www.GaussianProcess.org/gpml\n108 Model Selection and Adaptation of Hyperparameters\nthere is plenty of scope for variation even inside a single family of covariance\nfunctions. Our task is, based on a set of training data, to make inferences about\nthe form and parameters of the covariance function, or equivalently, about the\nrelationships in the data.\nIt should be clear from the above example that model selection is essentially\nop...",
      "url": "https://gaussianprocess.org/gpml/chapters/RW5.pdf"
    },
    {
      "title": "Bayesian reaction optimization as a tool for chemical synthesis",
      "text": "Bayesian reaction optimization as a tool for chemical synthesis | Nature\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature](https://media.springernature.com/full/nature-cms/uploads/product/nature/header-86f1267ea01eccd46b530284be10585e.svg)](/)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41586-021-03213-y?error=cookies_not_supported&code=1e877825-e714-4b2d-bef5-02d6d9877221)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Subscribe](/nature/subscribe)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41586)\n* [RSS feed](https://www.nature.com/nature.rss)\n* Article\n* Published:03 February 2021# Bayesian reaction optimization as a tool for chemical synthesis\n* [Benjamin J. Shields](#auth-Benjamin_J_-Shields-Aff1)[1](#Aff1),\n* [Jason Stevens](#auth-Jason-Stevens-Aff2)[ORCID:orcid.org/0000-0003-1671-1539](https://orcid.org/0000-0003-1671-1539)[2](#Aff2),\n* [Jun Li](#auth-Jun-Li-Aff2)[ORCID:orcid.org/0000-0002-0594-7143](https://orcid.org/0000-0002-0594-7143)[2](#Aff2),\n* [Marvin Parasram](#auth-Marvin-Parasram-Aff1)[1](#Aff1),\n* [Farhan Damani](#auth-Farhan-Damani-Aff3)[3](#Aff3),\n* [Jesus I. Martinez Alvarado](#auth-Jesus_I__Martinez-Alvarado-Aff1)[1](#Aff1),\n* [Jacob M. Janey](#auth-Jacob_M_-Janey-Aff2)[ORCID:orcid.org/0000-0001-7697-1709](https://orcid.org/0000-0001-7697-1709)[2](#Aff2),\n* [Ryan P. Adams](#auth-Ryan_P_-Adams-Aff3)[ORCID:orcid.org/0000-0002-5704-6654](https://orcid.org/0000-0002-5704-6654)[3](#Aff3)&amp;\n* \u2026* [Abigail G. Doyle](#auth-Abigail_G_-Doyle-Aff1)[ORCID:orcid.org/0000-0002-6641-0833](https://orcid.org/0000-0002-6641-0833)[1](#Aff1)Show authors\n[*Nature*](/)**volume590**,pages89\u201396 (2021)[Cite this article](#citeas)\n* 88kAccesses\n* 780Citations\n* 174Altmetric\n* [Metricsdetails](/articles/s41586-021-03213-y/metrics)\n### Subjects\n* [Automation](/subjects/automation)\n* [Computer science](/subjects/computer-science)\n* [Process chemistry](/subjects/process-chemistry)\n* [Scientific data](/subjects/scientific-data)\n## Abstract\nReaction optimization is fundamental to synthetic chemistry, from optimizing the yield of industrial processes to selecting conditions for the preparation of medicinal candidates[1](/articles/s41586-021-03213-y#ref-CR1). Likewise, parameter optimization is omnipresent in artificial intelligence, from tuning virtual personal assistants to training social media and product recommendation systems[2](/articles/s41586-021-03213-y#ref-CR2). Owing to the high cost associated with carrying out experiments, scientists in both areas set numerous (hyper)parameter values by evaluating only a small subset of the possible configurations. Bayesian optimization, an iterative response surface-based global optimization algorithm, has demonstrated exceptional performance in the tuning of machine learning models[3](/articles/s41586-021-03213-y#ref-CR3). Bayesian optimization has also been recently applied in chemistry[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](/articles/s41586-021-03213-y#ref-CR9); however, its application and assessment for reaction optimization in synthetic chemistry has not been investigated. Here we report the development of a framework for Bayesian reaction optimization and an open-source software tool that allows chemists to easily integrate state-of-the-art optimization algorithms into their everyday laboratory practices. We collect a large benchmark dataset for a palladium-catalysed direct arylation reaction, perform a systematic study of Bayesian optimization compared to human decision-making in reaction optimization, and apply Bayesian optimization to two real-world optimization efforts (Mitsunobu and deoxyfluorination reactions). Benchmarking is accomplished via an online game that links the decisions made by expert chemists and engineers to real experiments run in the laboratory. Our findings demonstrate that Bayesian optimization\u00a0outperforms human decisionmaking in both average optimization efficiency (number of experiments) and consistency (variance of outcome against initially available data). Overall, our studies suggest that adopting Bayesian optimization methods into everyday laboratory practices could facilitate more efficient synthesis of functional chemicals by enabling better-informed, data-driven decisions about which experiments to run.\n[Access through your institution](https://wayf.springernature.com?redirect_uri&#x3D;https://www.nature.com/articles/s41586-021-03213-y)\n[Buy or subscribe](#access-options)\nThis is a preview of subscription content,[access via your institution](https://wayf.springernature.com?redirect_uri&#x3D;https://www.nature.com/articles/s41586-021-03213-y)\n## Access options\n[Access through your institution](https://wayf.springernature.com?redirect_uri&#x3D;https://www.nature.com/articles/s41586-021-03213-y)\nAccess Nature and 54 other Nature Portfolio journals\nGet Nature+, our best-value online-access subscription\n$32.99/\u00a030\u00a0days\ncancel any time\n[Learn more](https://shop-amers.nature.com/products/plus/?region=US)\nSubscribe to this journal\nReceive 51 print issues and online access\n$199.00 per year\nonly $3.90 per issue\n[Learn more](/nature/subscribe)\nBuy this article\n* Purchase on SpringerLink\n* Instant access to the full article PDF.\nUSD 39.95\nPrices may be subject to local taxes which are calculated during checkout\n**Fig. 1: Bayesian reaction optimization.**\n![](//media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41586-021-03213-y/MediaObjects/41586_2021_3213_Fig1_HTML.png)\n**Fig. 2: Training data used to select Bayesian optimizer parameters.**\n![](//media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41586-021-03213-y/MediaObjects/41586_2021_3213_Fig2_HTML.png)\n**Fig. 3: Balancing exploration and exploitation in reaction optimization.**\n![](//media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41586-021-03213-y/MediaObjects/41586_2021_3213_Fig3_HTML.png)\n**Fig. 4: Statistical validation of Bayesian reaction optimization.**\n![](//media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41586-021-03213-y/MediaObjects/41586_2021_3213_Fig4_HTML.png)\n**Fig. 5: Applications of Bayesian reaction optimization.**\n![](//media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41586-021-03213-y/MediaObjects/41586_2021_3213_Fig5_HTML.png)\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-022-00764-7/MediaObjects/42004_2022_764_Fig1_HTML.png)\n### [Bayesian optimization-driven parallel-screening of multiple parameters for the flow synthesis of biaryl compounds](https://www.nature.com/articles/s42004-022-00764-7?fromPaywallRec=true)\nArticleOpen access10 November 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41570-022-00391-9/MediaObjects/41570_2022_391_Figa_HTML.png)\n### [Evaluation guidelines for machine learning tools in the chemical sciences](https://www.nature.com/articles/s41570-022-00391-9?fromPaywallRec=true)\nArticle24 May 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs44160-025-00944-y/MediaObjects/44160_2025_944_Figa_HTML.png)\n### [Heuristic data-driven approach for synergistic cobalt(IV)\u2013enamine catalysis](https://www.nature.com/articles/s44160-025-00944-y?fromPaywallRec=true)\nArticle01 December 2025\n## Data availab...",
      "url": "https://www.nature.com/articles/s41586-021-03213-y?error=cookies_not_supported&code=a9702756-35f5-42e8-a475-e6170f549078"
    },
    {
      "title": "Bayesian reaction optimization as a tool for chemical synthesis",
      "text": "### Subjects\n\n- [Automation](https://www.nature.com/subjects/automation)\n- [Computer science](https://www.nature.com/subjects/computer-science)\n- [Process chemistry](https://www.nature.com/subjects/process-chemistry)\n- [Scientific data](https://www.nature.com/subjects/scientific-data)\n\n## Abstract\n\nReaction optimization is fundamental to synthetic chemistry, from optimizing the yield of industrial processes to selecting conditions for the preparation of medicinal candidates[1](https://www.nature.com/articles/s41586-021-03213-y#ref-CR1). Likewise, parameter optimization is omnipresent in artificial intelligence, from tuning virtual personal assistants to training social media and product recommendation systems[2](https://www.nature.com/articles/s41586-021-03213-y#ref-CR2). Owing to the high cost associated with carrying out experiments, scientists in both areas set numerous (hyper)parameter values by evaluating only a small subset of the possible configurations. Bayesian optimization, an iterative response surface-based global optimization algorithm, has demonstrated exceptional performance in the tuning of machine learning models[3](https://www.nature.com/articles/s41586-021-03213-y#ref-CR3). Bayesian optimization has also been recently applied in chemistry[4](https://www.nature.com/www.nature.com#ref-CR4), [5](https://www.nature.com/www.nature.com#ref-CR5), [6](https://www.nature.com/www.nature.com#ref-CR6), [7](https://www.nature.com/www.nature.com#ref-CR7), [8](https://www.nature.com/www.nature.com#ref-CR8), [9](https://www.nature.com/articles/s41586-021-03213-y#ref-CR9); however, its application and assessment for reaction optimization in synthetic chemistry has not been investigated. Here we report the development of a framework for Bayesian reaction optimization and an open-source software tool that allows chemists to easily integrate state-of-the-art optimization algorithms into their everyday laboratory practices. We collect a large benchmark dataset for a palladium-catalysed direct arylation reaction, perform a systematic study of Bayesian optimization compared to human decision-making in reaction optimization, and apply Bayesian optimization to two real-world optimization efforts (Mitsunobu and deoxyfluorination reactions). Benchmarking is accomplished via an online game that links the decisions made by expert chemists and engineers to real experiments run in the laboratory. Our findings demonstrate that Bayesian optimization\u00a0outperforms human decisionmaking in both average optimization efficiency (number of experiments) and consistency (variance of outcome against initially available data). Overall, our studies suggest that adopting Bayesian optimization methods into everyday laboratory practices could facilitate more efficient synthesis of functional chemicals by enabling better-informed, data-driven decisions about which experiments to run.\n\n[Access through your institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-021-03213-y)\n\n[Buy or subscribe](https://www.nature.com/www.nature.com#access-options)\n\nThis is a preview of subscription content, [access via your institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-021-03213-y)\n\n## Access options\n\n[Access through your institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-021-03213-y)\n\nAccess Nature and 54 other Nature Portfolio journals\n\nGet Nature+, our best-value online-access subscription\n\n$32.99 /\u00a030\u00a0days\n\ncancel any time\n\n[Learn more](https://shop-amers.nature.com/products/plus/?region=US)\n\nSubscribe to this journal\n\nReceive 51 print issues and online access\n\n$199.00 per year\n\nonly $3.90 per issue\n\n[Learn more](https://www.nature.com/nature/subscribe)\n\nBuy this article\n\n- Purchase on SpringerLink\n- Instant access to full article PDF\n\n[Buy now](https://link.springer.com/article/10.1038/s41586-021-03213-y?utm_source=nature&utm_medium=referral&utm_campaign=buyArticle)\n\nPrices may be subject to local taxes which are calculated during checkout\n\n**Fig. 1: Bayesian reaction optimization.**\n\n**Fig. 2: Training data used to select Bayesian optimizer parameters.**\n\n**Fig. 3: Balancing exploration and exploitation in reaction optimization.**\n\n**Fig. 4: Statistical validation of Bayesian reaction optimization.**\n\n**Fig. 5: Applications of Bayesian reaction optimization.**\n\n### Similar content being viewed by others\n\n### [Bayesian optimization-driven parallel-screening of multiple parameters for the flow synthesis of biaryl compounds](https://www.nature.com/articles/s42004-022-00764-7?fromPaywallRec=true)\n\nArticleOpen access10 November 2022\n\n### [Unassisted noise reduction of chemical reaction datasets](https://www.nature.com/articles/s42256-021-00319-w?fromPaywallRec=true)\n\nArticle29 March 2021\n\n### [Evaluation guidelines for machine learning tools in the chemical sciences](https://www.nature.com/articles/s41570-022-00391-9?fromPaywallRec=true)\n\nArticle24 May 2022\n\n## Data availability\n\nQuantum mechanical computation data and Gaussian output files used to parameterize reactions 1\u20135 are available at [https://github.com/b-shields/auto-QChem](https://github.com/b-shields/auto-QChem). Processed reaction outcome data for reactions 1\u20135 are available at [https://github.com/b-shields/edbo](https://github.com/b-shields/edbo) and in our published Code Ocean capsule at [https://doi.org/10.24433/CO.3864629.v1](https://doi.org/10.24433/CO.3864629.v1). Tabulated player data for the reaction optimization game are available at [https://github.com/b-shields/EvML](https://github.com/b-shields/EvML).\n\n## Code availability\n\nTwo software packages and one web application were written to support this work. The first, auto-qchem, was written to facilitate high-throughput computational chemistry and reaction featurization. This package is freely available at [https://github.com/b-shields/auto-QChem](https://github.com/b-shields/auto-QChem). The second, EDBO, was written as a user-friendly implementation of Bayesian optimization. This package is freely available at [https://github.com/b-shields/edbo](https://github.com/b-shields/edbo) and in our published Code Ocean capsule at [https://doi.org/10.24433/CO.3864629.v1](https://doi.org/10.24433/CO.3864629.v1). The web application, EvML, was written to collect user data for comparison of Bayesian optimization with human expert performance. This package is freely available at [https://github.com/b-shields/EvML](https://github.com/b-shields/EvML).\n\n## References\n\n01. Carlson, R. _Design and Optimization in Organic Synthesis_ (Elsevier, 1992).\n\n02. Luo, G. A review of automatic selection methods for machine learning algorithms and hyper-parameter values. _Netw. Model. Anal. Health Inform. Bioinform_. **5**, 18 (2016).\n\n    [Article](https://link.springer.com/doi/10.1007/s13721-016-0125-6) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&bibcode=2019npjQI...5...18L) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20review%20of%20automatic%20selection%20methods%20for%20machine%20learning%20algorithms%20and%20hyper-parameter%20values&journal=Netw.%20Model.%20Anal.%20Health%20Inform.%20Bioinform.&doi=10.1007%2Fs13721-016-0125-6&volume=5&publication_year=2016&author=Luo%2CG)\n\n03. Snoek, J., Larochelle, H. & Adams, R. P. Practical Bayesian optimization of machine learning algorithms. In _Advances in Neural Information Processing Systems_ Vol. 25 (eds Pereira, F. et al.) 2951\u20132959 (Curran Associates Inc., 2012).\n\n04. H\u00e4se, F., Roch, L. M., Kreisbeck, C. & Aspuru-Guzik, A. Phoenics: a Bayesian Optimizer for Chemistry. _ACS Cent. Sci_. **4**, 1134\u20131145 (2018).\n\n    [Article](https://doi.org/10.1021%2Facscentsci.8b00307) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30276246) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles...",
      "url": "https://www.nature.com/articles/s41586-021-03213-y"
    },
    {
      "title": "Bayesian optimisation for additive screening and yield improvements \u2013 beyond one-hot encoding \u2020",
      "text": "Bayesian optimisation for additive screening and yield improvements \u2013beyond one-hot encoding - Digital Discovery (RSC Publishing) DOI:10.1039/D3DD00096F\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2024/dd/d3dd00096f)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/dd/d4dd00021h)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/dd/d3dd00151b)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D3DD00096F](https://doi.org/10.1039/D3DD00096F)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2024,**3**, 654-666\n# Bayesian optimisation for additive screening and yield improvements \u2013beyond one-hot encoding[\u2020](#fn1)\nBojana Rankovi\u0107[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1476-6686)\\*a,Ryan-Rhys Griffiths[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-3117-4559)b,Henry B. MosscandPhilippe Schwaller[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-3046-6576)\\*a\naLaboratory of Artificial Chemical Intelligence (LIAC), National Centre of Competence in Research (NCCR) Catalysis, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland. E-mail:[bojana.rankovic@epfl.ch](mailto:bojana.rankovic@epfl.ch);[philippe.schwaller@epfl.ch](mailto:philippe.schwaller@epfl.ch)\nbDepartment of Physics, University of Cambridge, UK\ncDepartment of Applied Mathematics and Theoretical Physics, University of Cambridge, UK\nReceived 30th May 2023, Accepted 24th October 2023\nFirst published on 2nd November 2023\n## Abstract\nReaction additives are critical in dictating the outcomes of chemical processes making their effective screening vital for research. Conventional high-throughput experimentation tools can screen multiple reaction components rapidly. However, they are prohibitively expensive, which puts them out of reach for many research groups. This work introduces a cost-effective alternative using Bayesian optimisation. We consider a unique reaction screening scenario evaluating a set of 720 additives across four different reactions, aiming to maximise UV210 product area absorption. The complexity of this setup challenges conventional methods for depicting reactions, such as one-hot encoding, rendering them inadequate. This constraint forces us to move towards more suitable reaction representations. We leverage a variety of molecular and reaction descriptors, initialisation strategies and Bayesian optimisation surrogate models and demonstrate convincing improvements over random search-inspired baselines. Importantly, our approach is generalisable and not limited to chemical additives, but can be applied to achieve yield improvements in diverse cross-couplings or other reactions, potentially unlocking access to new chemical spaces that are of interest to the chemical and pharmaceutical industries. The code is available at: https://github.com/schwallergroup/chaos.\n## 1 Introduction\nArtificial intelligence holds great promise to accelerate the chemical sciences.[1\u20134](#cit2)Over the last decade, we have witnessed ground-breaking advances in machine learning forde novomolecular design,[5\u201310](#cit5)synthesis planning,[11\u201317](#cit11)and reaction outcome prediction.[18\u201326](#cit18)Recently, research has focused on sequential model-based optimisation algorithms, particularly Bayesian optimisation (BO), to identify optimal conditions for chemical reactions effectively.[27\u201338](#cit27)As demonstrated in the space of chemical reactions, BO is particularly well suited for trading off exploration and exploitation in the low data regime. Surprisingly, most BO studies report one-hot encoding (OHE), that contains limited chemical information, to perform remarkably well.[31,35](#cit31)This recurring observation raises an important question: why does OHE, with its inherent simplicity manage to deliver competitive results? For instance, Shieldset al.[32](#cit32)compared OHE to more elaborate reaction representations such as quantum mechanical (QM) descriptors. The study found no significant difference in optimisation performance stating these two representations \u201clargely indistinguishable\u201d. This conclusion emerges from evaluating BO across several reaction datasets including the optimisation of Buchwald Hartwig reactions. Consider the case of the Buchwald Hartwig dataset: five distinct reactions with 790 data points each, covering four variable components to optimise over \u2013base, ligand, aryl halide and additive. Our study, while bearing similarities in examining four different Ni-catalysed photoredox decarboxylative arylations[\u2021](#fn2)reactions with 720 data points per reaction,[39](#cit39)has a distinguishing feature: all other reaction components remain fixed, except for the additive being screened. Consequently, the resulting OHE vectors create an orthogonal space where the number of dimensions equals the number of data points, making it difficult for any machine learning method to grasp valuable patterns. This inherent constraint forces us to think beyond OHE and leverage alternative representations to combine with BO and pinpoint the optimal additives for given chemical reactions. Accordingly, we have examined representations that not only address these limitations but also ensure computational efficiency on par with OHE.\nAdditives are critical for altering the reactivity and outcome of chemical reactions.[40,41](#cit40)According to the IUPAC Gold Book definition, additives are \u201csubstances that can be added to a sample for any of a variety of purposes\u201d.[42](#cit42)They are crucial in a range of chemical processes, including polymer synthesis, pharmaceutical development, and materials science.[43\u201345](#cit43)Identifying optimal additives can significantly enhance reaction efficiency, selectivity and yield, leading to cost-effective and sustainable chemical processes.[46,47](#cit46)In this study, we introduce a BO-based approach for efficient exploration of the additive[\u00a7](#fn3)search space. Subsequently, we explore a range of representation methods to determine the most appropriate ones for uncovering additive-induced yield improvements. This approach not only streamlines experimental design and optimisation but also holds immense promise for various applications within the field of chemistry. While Prieto Kullmeret al.[39](#cit39)screened these compounds using high-throughput experimentation (HTE), not all laboratories can access robotic platforms. Synthetic chemists, however, could highly benefit from using BO to discover the optimal additives, allowing them to improve a reaction without the need for exhaustive (and expensive) testing of all possible combinations ([Fig. 1](#imgfig1)). Compared to existing applications of BO to chemical reactions (e.g., Buchwald\u2013Hartwig reactions[48](#cit48)), the additive dataset is substantially more challenging. Firstly, OHE is ill-suited for this task as it results in high-dimensional vectors, with only one active dimension per additive. The resulting extreme sparsity and lack of shared information make it difficult to address the complexities of the dataset. This kind of representation limits the use of machine learning models, which can struggle to extract valuable insights. While seemingly intuitive, we empirically confirm these shortcomings, with details in the results section. As we demonstrate, applying BO in this setting fails to improve over random search. Sec...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2024/dd/d3dd00096f"
    },
    {
      "title": "",
      "text": "GAUCHE: A Library for\nGaussian Processes in Chemistry\nRyan-Rhys Griffiths1\u2217 Leo Klarner2\u2217 Henry Moss3\u2217 Aditya Ravuri3\u2217 Sang Truong4\u2217\nSamuel Stanton5\u2217 Gary Tom6,7\u2217 Bojana Rankovic8,9\u2217 Yuanqi Du10\u2217 Arian Jamasb3\u2217\nAryan Deshwal11 Julius Schwartz3 Austin Tripp3 Gregory Kell12 Simon Frieder2\nAnthony Bourached13 Alex J. Chan3 Jacob Moss3 Chengzhi Guo3\nJohannes Durholt14 Saudamini Chaurasia15 Ji Won Park5 Felix Strieth-Kalthoff6\nAlpha A. Lee3 Bingqing Cheng16 Al\u00e1n Aspuru-Guzik6,7,17 Philippe Schwaller8,9\nJian Tang18,19,17\n1Meta 2University of Oxford 3University of Cambridge 4Stanford University 5Genentech\n6University of Toronto 7Vector Institute 8EPFL 9NCCR Catalysis 10Cornell University\n11Washington State University 12King\u2019s College London 13University College London\n14Evonik Industries AG 15Syracuse University 16IST Austria 17CIFAR AI Research Chair\n18MILA Quebec AI Institute 19HEC Montreal\n\u2217 Equal contributions\n{ryangriff123,leojklarner}@gmail.com\nAbstract\nWe introduce GAUCHE, an open-source library for GAUssian processes in\nCHEmistry. Gaussian processes have long been a cornerstone of probabilistic\nmachine learning, affording particular advantages for uncertainty quantification and\nBayesian optimisation. Extending Gaussian processes to molecular representations,\nhowever, necessitates kernels defined over structured inputs such as graphs, strings\nand bit vectors. By providing such kernels in a modular, robust and easy-to-use\nframework, we seek to enable expert chemists and materials scientists to make\nuse of state-of-the-art black-box optimization techniques. Motivated by scenarios\nfrequently encountered in practice, we showcase applications for GAUCHE in\nmolecular discovery, chemical reaction optimisation and protein design.\nThe codebase is made available at https://github.com/leojklarner/gauche.\n1 Introduction\nEarly-stage scientific discovery is often characterised by the limited availability of high-quality\nexperimental data [1, 2, 3], meaning that there is much knowledge to gain from targeted experiments.\nAs such, machine learning methods that facilitate discovery in low data regimes, such as Bayesian\noptimisation (BO) [4, 5, 6, 7, 8, 9] and active learning (AL) [10, 11], have great potential to expedite\nthe rate at which useful molecules, materials, chemical reactions and proteins can be discovered.\nAt present, Bayesian neural networks (BNNS) and deep ensembles are typically the method of\nchoice to generate uncertainty estimates for molecular BO and AL loops [10, 12, 13, 14]. For small\ndatasets, however, Gaussian processes (GPS) may often be a preferable and more appropriate choice\n[15, 16]. Furthermore, GPS possess particularly advantageous properties for BO; first, they admit\nexact as opposed to approximate Bayesian inference and second, few of their parameters need to be\ndetermined by hand. In the words of Sir David MacKay [17],\n\"Gaussian processes are useful tools for automated tasks where fine tuning for each\nproblem is not possible. We do not appear to sacrifice any performance for this simplicity.\u201d\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nECFP Fingerprint Molecular Graph SMILES String\nNC1=CC=CC=C1\nProteins\nSequence String Structural Graph\nRASRLMQA\nRXN Fingerprint RXN Smarts\n[CH:1][O:2]>>[C:1]=[O:2]\nReactions\nGaussian Processes Bayesian Optimisation\nMolecules\nFigure 1: An overview of the applications and representations available in GAUCHE.\nThe iterative model refitting required in BO makes it a prime example of such an automated task.\nHowever, canonical GPS typically assume continuous input spaces of low and fixed dimensionality,\nhindering their application to standard molecular representations such as SMILES/SELFIES strings\n[18, 19, 20], topological fingerprints [21, 22, 23] and discrete graphs [24, 25].\nWith GAUCHE, we provide a modular, robust and easy-to-use framework to rapidly prototype GPS\nwith 30+ GPU-accelerated string, fingerprint and graph kernels that operate on a range of molecular\nrepresentations (see Figure 1). Furthermore, GAUCHE interfaces with the GPyTorch [26] and\nBoTorch [27] libraries and contains an extensive set of tutorial notebooks to make state-of-the-art\nprobabilistic modelling and black-box optimization techniques more easily accessible to scientific\nexperts in chemistry, materials science and beyond.\n2 Background\nWe briefly recall the fundamentals of Gaussian processes and Bayesian optimisation in Sections 2.1\nand 2.2, respectively, and refer the reader to [28] and [29, 30, 31] for a more comprehensive treatment.\n2.1 Gaussian Processes\nNotation X \u2208 R\nn\u00d7d\nis a design matrix of n training examples of dimension d. A given row i of\nthe design matrix contains a training molecule\u2019s representation xi. A GP is specified by a mean\nfunction, m(x) = E[f(x)] and a covariance function k(x, x\n\u2032\n) = E[(f(x) \u2212 m(x))(f(x\n\u2032\n) \u2212 m(x\n\u2032\n))].\nK\u03b8(X, X) is a kernel matrix, where entries are computed by the kernel function as [K]ij = k(xi, xj )\nand \u03b8 represents the set of kernel hyperparameters. The GP specifies the full distribution over the\nfunction f to be modelled as\nf(x) \u223c GPm(x), k(x, x\n\u2032\n)\n\u0001\n.\nTraining Hyperparameters for GPS comprise kernel hyperparameters, \u03b8, in addition to the likeli\u0002hood noise, \u03c3\n2\ny\n. These hyperparameters are chosen by optimising an objective function known as the\nnegative log marginal likelihood (NLML)\nlog p(y|X, \u03b8) = \u2212\n1\n2\ny\n\u22a4(K\u03b8(X, X) + \u03c32\ny\nI)\n\u22121y\n| {z }\nencourages fit with data\n\u2212\n1\n2\nlog |K\u03b8(X, X) + \u03c3\n2\ny\nI|\n| {z }\ncontrols model capacity\n\u2212\nN\n2\nlog(2\u03c0),\n2\nwhere \u03c3\n2\ny\nI represents the variance of i.i.d. Gaussian noise on the observations y. The NLML\nembodies Occam\u2019s razor for Bayesian model selection [28] in favouring models that fit the data\nwithout being overly complex.\nPrediction At test locations X\u2217, assuming a zero mean function obtained following the standard\u0002ization of the outputs y, the GP returns a predictive mean, f\u00af\u2217 = K(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121y,\nand a predictive uncertainty cov(f\u2217) = K(X\u2217, X\u2217) \u2212 K(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121K(X, X\u2217).\n2.2 Bayesian Optimisation\nIn molecular discovery campaigns, we are typically interested in solving problems of the form\nx\n\u22c6 = arg max\nx\u2208X\nf(x),\nwhere f(\u00b7) : X \u2192 R is an expensive black-box function over a structured input domain X . In our\nsetting the structured input domain consists of a set of molecular representations (graphs, strings, bit\nvectors) and the expensive black-box function is an experimentally determined property of interest\nthat we wish to optimise. Bayesian optimisation (BO) [32, 33, 34, 35, 29, 36] is a data-efficient\nmethodology for determining x\n\u22c6\n. BO operates sequentially by selecting input locations at which\nto query the black-box function f with the aim of identifying the optimum in as few queries as\npossible. Evaluations are focused on promising areas of the input space as well as areas with high\nuncertainty\u2014a balancing act known as the exploration/exploitation trade-off.\nThe two components of a BO scheme are a probabilistic surrogate model and an acquisition function.\nThe surrogate model is typically chosen to be a GP due to its ability to maintain calibrated uncertainty\nestimates through exact Bayesian inference. The uncertainty estimates of the surrogate model are\nthen leveraged by the acquisition function to propose new input locations to query. The acquisition\nfunction is a heuristic that trades off exploration and exploitation, well-known examples of which\ninclude expected improvement (EI) [33, 35] and entropy search [37, 38, 39, 40]. After the acquisition\nfunction proposes an input location, the black-box is evaluated at that location, the surrogate model is\nretrained and the process is repeated until a solution is obtained.\n3 Molecular Representations\nWe review commonly used representations for molecules (Section 3.1), chemical reactions (Sec\u0002tion 3.2) and proteins (Section 3.3), before describing the kernels that operate on them in Section 4.\nAn overview of the representations considered by ...",
      "url": "https://discovery.ucl.ac.uk/10209390/1/NeurIPS-2023-gauche-a-library-for-gaussian-processes-in-chemistry-Paper-Conference.pdf"
    }
  ]
}