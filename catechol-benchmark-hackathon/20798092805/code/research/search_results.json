{
  "query": "What are the best ensemble techniques for neural networks to reduce prediction variance and improve generalization?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensemble learning is the most reliable way to tame the high\u2011variance nature of deep neural networks.  The simplest and often most effective approach is **bagging** \u2013 training many identical networks on different bootstrap samples (or with different random seeds, data\u2011augmentation pipelines, or dropout masks) and averaging their outputs.  Because each model explores a different region of the weight space, the variance of the combined prediction drops roughly in proportion to\u202f1\u2044N, where\u202fN\u202fis the number of members\u202f([machinelearningmastery](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks)).  Variants such as **snapshot ensembles** or **Monte\u2011Carlo dropout** fall under the same principle and are widely used in practice.\n\nWhen variance reduction alone is not enough, **boosting** and **stacking** add complementary strengths.  Boosting (e.g., AdaBoost\u2011style or gradient\u2011boosted neural nets) trains networks sequentially, each one focusing on the errors of its predecessors, which can lower both bias and variance\u202f([interviewnode](https://www.interviewnode.com/post/ensemble-learning-techniques-boosting-bagging-and-stacking-explained)).  **Stacking** trains a meta\u2011learner (often a shallow model) to combine the predictions of diverse base networks, allowing the ensemble to learn optimal weighting and capture non\u2011linear relationships among the members\u202f([rjpn.org](https://rjpn.org/ijcspub/papers/IJCSP24D1098.pdf)).  Recent research also proposes more specialized schemes: a **frequency\u2011distribution\u2011based ensemble** identifies \u201ccore\u201d predictions that appear most often across models and discards outliers, improving accuracy especially on small, highly nonlinear datasets\u202f([arxiv.org](https://arxiv.org/abs/2210.10360)); and **Deep Anti\u2011Regularized Ensembles** explicitly encourage diversity while regularizing against over\u2011confidence, yielding reliable out\u2011of\u2011distribution uncertainty estimates\u202f([hal.science](https://hal.science/hal-04455937/document)).\n\nIn practice, the best strategy combines these ideas: train a set of heterogeneous networks (different architectures, initializations, or data splits), use bagging/averaging for a solid variance reduction baseline, optionally apply boosting or stacking to capture residual errors, and consider frequency\u2011distribution or anti\u2011regularization tricks when data are scarce or robustness to distribution shift is critical.  This multi\u2011pronged ensemble pipeline consistently delivers lower prediction variance and stronger generalization across computer\u2011vision, NLP, and regression tasks\u202f([arxiv.org](https://arxiv.org/abs/2104.02395)).",
      "url": ""
    },
    {
      "title": "Ensemble deep learning: A review",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2104.02395"
    },
    {
      "title": "Adaptive Neural Network Ensemble Using Frequency Distribution",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2210.10360** (cs)\n\n\\[Submitted on 19 Oct 2022\\]\n\n# Title:Adaptive Neural Network Ensemble Using Frequency Distribution\n\nAuthors: [Ungki Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+U), [Namwoo Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+N)\n\nView a PDF of the paper titled Adaptive Neural Network Ensemble Using Frequency Distribution, by Ungki Lee and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2210.10360)\n\n> Abstract:Neural network (NN) ensembles can reduce large prediction variance of NN and improve prediction accuracy. For highly nonlinear problems with insufficient data set, the prediction accuracy of NN models becomes unstable, resulting in a decrease in the accuracy of ensembles. Therefore, this study proposes a frequency distribution-based ensemble that identifies core prediction values, which are expected to be concentrated near the true prediction value. The frequency distribution-based ensemble classifies core prediction values supported by multiple prediction values by conducting statistical analysis with a frequency distribution, which is based on various prediction values obtained from a given prediction point. The frequency distribution-based ensemble can improve predictive performance by excluding prediction values with low accuracy and coping with the uncertainty of the most frequent value. An adaptive sampling strategy that sequentially adds samples based on the core prediction variance calculated as the variance of the core prediction values is proposed to improve the predictive performance of the frequency distribution-based ensemble efficiently. Results of various case studies show that the prediction accuracy of the frequency distribution-based ensemble is higher than that of Kriging and other existing ensemble methods. In addition, the proposed adaptive sampling strategy effectively improves the predictive performance of the frequency distribution-based ensemble compared with the previously developed space-filling and prediction variance-based strategies.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2210.10360](https://arxiv.org/abs/2210.10360) \\[cs.LG\\] |\n|  | (or [arXiv:2210.10360v1](https://arxiv.org/abs/2210.10360v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2210.10360](https://doi.org/10.48550/arXiv.2210.10360)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Namwoo Kang \\[ [view email](https://arxiv.org/show-email/aff62af7/2210.10360)\\]\n\n**\\[v1\\]**\nWed, 19 Oct 2022 08:05:35 UTC (1,184 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Adaptive Neural Network Ensemble Using Frequency Distribution, by Ungki Lee and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2210.10360)\n- [Other Formats](https://arxiv.org/format/2210.10360)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2210.10360&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2210.10360&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-10](https://arxiv.org/list/cs.LG/2022-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2210.10360?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.10360)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.10360)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.10360)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2210.10360&description=Adaptive Neural Network Ensemble Using Frequency Distribution) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2210.10360&title=Adaptive Neural Network Ensemble Using Frequency Distribution)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.10360) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2210.10360"
    },
    {
      "title": "Ensemble Learning Methods for Deep Learning Neural Networks - MachineLearningMastery.com",
      "text": "### [Navigation](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 6, 2019in[Deep Learning Performance](https://machinelearningmastery.com/category/better-deep-learning/)[45](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/#comments)\n\nShare _Tweet_Share\n\n#### How to Improve Performance By Combining Predictions From Multiple Models.\n\nDeep learning neural networks are nonlinear methods.\n\nThey offer increased flexibility and can scale in proportion to the amount of training data available. A downside of this flexibility is that they learn via a stochastic training algorithm which means that they are sensitive to the specifics of the training data and may find a different set of weights each time they are trained, which in turn produce different predictions.\n\nGenerally, this is referred to as neural networks having a high variance and it can be frustrating when trying to develop a final model to use for making predictions.\n\nA successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model.\n\nIn this post, you will discover methods for deep learning neural networks to reduce variance and improve prediction performance.\n\nAfter reading this post, you will know:\n\n- Neural network models are nonlinear and have a high variance, which can be frustrating when preparing a final model for making predictions.\n- Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error.\n- Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.\n\n**Kick-start your project** with my new book [Better Deep Learning](https://machinelearningmastery.com/better-deep-learning/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n![Ensemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks](https://machinelearningmastery.com/wp-content/uploads/2018/12/Ensemble-Methods-to-Reduce-Variance-and-Improve-Performance-of-Deep-Learning-Neural-Networks.jpg)\n\nEnsemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks\n\nPhoto by [University of San Francisco\u2019s Performing Arts](https://www.flickr.com/photos/usfperformingarts/8769648469/), some rights reserved.\n\n## Overview\n\nThis tutorial is divided into four parts; they are:\n\n1. High Variance of Neural Network Models\n2. Reduce Variance Using an Ensemble of Models\n3. How to Ensemble Neural Network Models\n4. Summary of Ensemble Techniques\n\n## High Variance of Neural Network Models\n\nTraining deep neural networks can be very computationally expensive.\n\nVery deep networks trained on millions of examples may take days, weeks, and sometimes months to train.\n\n> Google\u2019s baseline model \\[\u2026\\] was a deep convolutional neural network \\[\u2026\\] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.\n\n\u2014 [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), 2015.\n\nAfter the investment of so much time and resources, there is no guarantee that the final model will have low generalization error, performing well on examples not seen during training.\n\n> \u2026 train many different candidate networks and then to select the best, \\[\u2026\\] and to discard the rest. There are two disadvantages with such an approach. First, all of the effort involved in training the remaining networks is wasted. Second, \\[\u2026\\] the network which had best performance on the validation set might not be the one with the best performance on new test data.\n\n\u2014 Pages 364-365, [Neural Networks for Pattern Recognition](https://amzn.to/2I9gNMP), 1995.\n\nNeural network models are a nonlinear method. This means that they can learn complex nonlinear relationships in the data. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset.\n\nThis stochastic nature of the learning algorithm means that each time a neural network model is trained, it may learn a slightly (or dramatically) different version of the mapping function from inputs to outputs, that in turn will have different performance on the training and holdout datasets.\n\nAs such, we can think of a neural network as a method that has a low bias and high variance. Even when trained on large datasets to satisfy the high variance, having any variance in a final model that is intended to be used to make predictions can be frustrating.\n\n### Want Better Results with Deep Learning?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nDownload Your FREE Mini-Course\n\n## Reduce Variance Using an Ensemble of Models\n\nA solution to the high variance of neural networks is to train multiple models and combine their predictions.\n\nThe idea is to combine the predictions from multiple good but different models.\n\nA good model has skill, meaning that its predictions are better than random chance. Importantly, the models must be good in different ways; they must make different prediction errors.\n\n> The reason that model averaging works is that different models will usually not make all the same errors on the test set.\n\n\u2014 Page 256, [Deep Learning](https://amzn.to/2NJW3gE), 2016.\n\nCombining the predictions from multiple neural networks adds a bias that in turn counters the variance of a single trained neural network model. The results are predictions that are less sensitive to the specifics of the training data, choice of training scheme, and the serendipity of a single training run.\n\nIn addition to reducing the variance in the prediction, the ensemble can also result in better predictions than any single best model.\n\n> \u2026 the performance of a committee can be better than the performance of the best single network used in isolation.\n\n\u2014 Page 365, [Neural Networks for Pattern Recognition](https://amzn.to/2I9gNMP), 1995.\n\nThis approach belongs to a general class of methods called \u201c _ensemble learning_\u201d that describes methods that attempt to make the best use of the predictions from multiple models prepared for the same problem.\n\nGenerally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a prediction before combining the predictions in some way to make a final outcome or prediction.\n\nIn fact, ensembling of models is a standard approach in applied machine learning to ensure that the most stable and best possible prediction is made.\n\nFor example, Alex Krizhevsky, et al. in their famous 2012 paper titled \u201c [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)\u201d that introduced very deep convolutional neural networks for photo classification (i.e. AlexNet) used model averaging across multiple well-performing CNN models to achieve state-of-the-art results at the time. Performance of one model was compared to ensemble predictions averaged over two, five, and seven different models.\n\n> Averaging the predictions of five similar CNNs gives an error rate of 16.4%. \\[\u2026\\] Averaging the predictions of two CNNs that were pre-trained \\[\u2026\\] with the aforementioned five CNNs gives an error rate of 15.3%.\n\nEnsembling is also the approach used by winners in machine learning competitions.\n\n> Another powerful technique...",
      "url": "https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks"
    },
    {
      "title": "",
      "text": "[Back to Blogs](https://www.interviewnode.com/blog/categories/all)\n\n# Ensemble Learning Techniques: Boosting, Bagging, and Stacking Explained\n\nSantosh Rout\n\nOctober 21, 2024\n\n24 min read\n\n### **1\\. Introduction to Ensemble Learning**\n\nOne technique that consistently stands out in ML for enhancing model performance is ensemble learning. Whether you're working on a classification problem, like identifying fraudulent transactions, or a regression problem, such as predicting house prices, ensemble methods can help you achieve superior accuracy and robustness. But what exactly is ensemble learning, and why is it so effective? In this guide, we\u2019ll break down key **ensemble learning techniques** that make these gains possible.\n\n#### **What is Ensemble Learning?**\n\nAt its core, ensemble learning combines multiple [machine learning](https://interviewnode.com/mle-masterclass) models\u2014often called **weak learners**\u2014into a single **strong learner**. The underlying idea is that while individual models may not perform perfectly on their own, when their predictions are aggregated, the combined model often delivers better results. This technique works by reducing errors like **variance** and **bias**, which are the two primary sources of inaccuracy in machine learning models.\n\nLet\u2019s break down these key concepts:\n\n**Variance** refers to how much a model\u2019s predictions fluctuate with changes in the training data. Models like decision trees are prone to high variance, which can lead to overfitting. By averaging multiple models, ensemble methods like **bagging** can reduce variance.\n\n**Bias** is the error introduced when a model is too simplistic, leading to underfitting. Techniques like **boosting** work to reduce bias by sequentially improving weak models.\n\nEnsemble learning is powerful because it addresses these errors, creating models that are more accurate, stable, and generalizable. As a result, it\u2019s no surprise that ensemble methods are widely used in high-stakes applications like credit scoring, fraud detection, healthcare predictions, and more.\n\n#### **Why Use Ensemble Learning?**\n\nThe primary reason to use ensemble learning is to boost **predictive performance**. While a single decision tree or neural network can work well on certain tasks, it might fall short on complex datasets where small errors compound. Ensemble methods help by balancing the strengths and weaknesses of multiple models.\n\nAdditionally, ensemble models can help tackle **class imbalances**\u2014a common challenge in machine learning where one class is overrepresented in the data (for example, detecting fraud in financial transactions, where the vast majority of transactions are legitimate). Boosting algorithms, like **AdaBoost** and **Gradient Boosting**, are particularly effective in handling imbalanced datasets by focusing on hard-to-classify examples\u200b.\n\n#### **Overview of Bagging, Boosting, and Stacking**\n\nThere are several types of ensemble techniques, but the three most widely used in practice are **Bagging**, **Boosting**, and **Stacking**. Each of these methods uses a different approach to model training and prediction:\n\n**Bagging** trains multiple models independently in parallel and averages their predictions. Its goal is to reduce variance by aggregating predictions from multiple weak models trained on different subsets of the data.\n\n**Boosting** trains models sequentially, with each model focusing on correcting the errors made by its predecessor. Boosting is designed to reduce bias by focusing on the hardest-to-predict data points.\n\n**Stacking** combines different models, often of different types, and uses a meta-learner to blend their outputs for improved accuracy.\n\nIn the following sections, we\u2019ll dive deeper into how each of these methods works and when to use them to maximize the performance of your machine learning models.\n\n### **2\\. What is Bagging?**\n\n**Bagging**, short for **Bootstrap Aggregating**, is one of the most popular ensemble learning techniques used to reduce the variance of machine learning models. Developed by **Leo Breiman** in the 1990s, Bagging is particularly useful for models that tend to overfit the data, such as decision trees. Its primary goal is to create more robust and generalized models by averaging predictions from multiple weak learners.\n\n#### **How Bagging Works**\n\nBagging works by generating multiple versions of a dataset through a process called **bootstrapping**, and then training a model on each version. The key idea is to create slightly different training datasets by randomly sampling from the original data **with replacement**. This means that some data points will be used more than once, while others might be left out. By doing this, Bagging creates a diverse set of models, each trained on a different subset of the data, which helps reduce the risk of overfitting.\n\nHere's a step-by-step breakdown of the Bagging process:\n\n**Bootstrap Sampling**: From the original training dataset, multiple random samples are created, each with the same size as the original dataset but generated by random sampling with replacement.\n\n**Training Multiple Models**: A separate model is trained on each bootstrap sample. For example, if Bagging is used with decision trees, each model will be a decision tree trained on a different subset of the data.\n\n**Combining Predictions**: Once the models are trained, their predictions are combined. For classification problems, the final prediction is usually determined by a **majority vote** (i.e., the class that most models predict). For regression tasks, the final prediction is the **average** of the individual model predictions.\n\nThis combination of models leads to a reduction in variance, as the randomness introduced by bootstrapping ensures that the models are less correlated with one another. Bagging excels at creating a stable and reliable model, especially when dealing with high-variance models such as decision trees.\n\n#### **Random Forest: A Bagging Example**\n\nOne of the most famous applications of Bagging is the **Random Forest** algorithm, which is essentially an ensemble of decision trees. In a Random Forest, multiple decision trees are trained on different bootstrapped datasets, and each tree makes predictions independently. These predictions are then aggregated to form the final output.\n\nWhat sets Random Forest apart is that, in addition to bootstrapping the data, it also selects a random subset of features for each tree, further increasing the diversity among the trees and reducing the likelihood of overfitting.\n\nKey steps of Random Forest:\n\n**Random Sampling of Data**: Bootstrapped samples of the data are used to train each decision tree.\n\n**Random Feature Selection**: Instead of considering all features at each split, Random Forest only looks at a random subset of features. This leads to a more diverse set of trees.\n\n**Majority Voting (Classification) or Averaging (Regression)**: The predictions from all the decision trees are combined by voting (for classification) or averaging (for regression) to make the final prediction.\n\nRandom Forest has become a go-to algorithm for many machine learning tasks, particularly when working with tabular data. Its ability to handle large datasets, manage missing values, and reduce overfitting makes it incredibly versatile\u200b.\n\n#### **Advantages of Bagging**\n\n**Reduction in Variance**: By averaging predictions across multiple models, Bagging helps reduce the variance, making the final model more stable and less likely to overfit the training data.\n\n**Robustness**: Since Bagging creates a more generalized model, it performs better on unseen data.\n\n**Parallelization**: Bagging can train models independently, making it easy to parallelize the process and handle large datasets efficiently.\n\n#### **Limitations of Bagging**\n\n**Less Effective for Bias Reduction**: While Bagging is excellent for reducing variance, it doesn\u2019t directly address **bias**. If the base model is highly biased, Ba...",
      "url": "https://www.interviewnode.com/post/ensemble-learning-techniques-boosting-bagging-and-stacking-explained"
    },
    {
      "title": "",
      "text": "www.ijcspub.org \u00a9 2024 IJCSPUB | Volume 14, Issue 4 December 2024 | ISSN: 2250-1770\nIJCSP24D1098 International Journal of Current Science (IJCSPUB) www.ijcspub.org 944\nA Comprehensive Review On Ensemble Deep \nLearning: Opportunities And Challenges\n1Dr. Capt. N. Kumar,2Mr. R. Rajapriyan\n1Director,2Assistant Professor\n1,2School of Maritime Studies, \n1,2 Vels Institute of Science, Technology & Advanced Studies (VISTAS), Chennai, Tamil Nadu, India\nAbstract: Ensemble deep learning (EDL) refers to the combination of multiple deep learning models to \nenhance overall performance in terms of accuracy, robustness, and generalization. The field has gained \nsignificant attention due to its ability to overcome the limitations of individual models by leveraging diversity \nand complementary strengths. This review paper provides a comprehensive overview of ensemble deep \nlearning, exploring its methodologies, opportunities, challenges, and applications. We examine different \nensemble strategies, including bagging, boosting, stacking, and voting mechanisms, and highlight key \ndevelopments and advances in the domain. Moreover, we address the limitations and challenges that hinder \nthe widespread adoption of EDL techniques, such as model diversity, computational cost, and interpretability. \nLastly, we discuss the future research directions that can further enhance the capabilities of ensemble deep \nlearning. This paper aims to serve as a valuable resource for researchers and practitioners seeking to leverage \nensemble methods to solve complex real-world problems.\nIndex Terms - Ensemble deep learning, deep learning, model aggregation, model diversity, boosting, bagging, \nstacking, challenges, opportunities\n1. Introduction\nDeep learning (DL) has revolutionized the field of artificial intelligence (AI) by providing state-of-the-art \nsolutions across a wide range of domains such as computer vision, natural language processing, and speech \nrecognition. Despite its impressive performance, deep learning models often suffer from challenges related to \noverfitting, bias, and poor generalization, especially when dealing with limited data or highly complex \nproblems. Ensemble deep learning (EDL) emerges as a potential solution to these issues, offering a way to \ncombine multiple models to enhance performance.\nThe concept of ensemble learning has been well-established in machine learning, with techniques such as \nrandom forests and gradient boosting widely adopted. However, the integration of ensemble methods into \ndeep learning is relatively new, with research focusing on how to combine neural networks or deep models \neffectively. In this paper, we provide an in-depth exploration of EDL, discuss its opportunities and challenges, \nand present a roadmap for future research.\n2. Ensemble Deep Learning: Background and Methods\nEnsemble learning involves the combination of multiple base models to produce a final prediction. The idea \nis that combining several models can reduce the risk of errors made by individual models. In deep learning, \nthis concept can be extended to create stronger predictive models by aggregating outputs from different neural \nnetwork architectures.\nwww.ijcspub.org \u00a9 2024 IJCSPUB | Volume 14, Issue 4 December 2024 | ISSN: 2250-1770\nIJCSP24D1098 International Journal of Current Science (IJCSPUB) www.ijcspub.org 945\n2.1 Ensemble Techniques in Deep Learning\nSeveral strategies can be employed to build ensembles of deep learning models:\n\uf0b7 Bagging (Bootstrap Aggregating): This technique involves training multiple instances of the same model \non different subsets of the data, created through bootstrapping (sampling with replacement). The \npredictions of individual models are aggregated through averaging (for regression tasks) or majority \nvoting (for classification tasks). Bagging reduces variance and improves generalization.\n\uf0b7 Boosting: Boosting focuses on sequentially training models, where each new model corrects the errors \nmade by its predecessor. The outputs of all models are weighted and combined to make the final \nprediction. Popular boosting methods include AdaBoost, Gradient Boosting, and XGBoost, which have \nbeen adapted for deep learning models.\n\uf0b7 Stacking (Stacked Generalization): In stacking, different models are trained independently, and their \noutputs are combined using a meta-learner (another model). This meta-model learns how to best combine \nthe predictions of the base models. This approach leverages the strengths of each model and can provide \nmore accurate results.\n\uf0b7 Voting Mechanism: In the simplest form of ensemble methods, a voting mechanism can be used to \naggregate the predictions of individual models. This can be in the form of a hard voting (majority voting) \nor soft voting (weighted average).\n2.2 Hybrid and Advanced Ensemble Methods\nBeyond traditional methods, hybrid and advanced techniques have been developed to enhance the diversity \nand performance of ensembles. Some examples include:\n\uf0b7 Neural Architecture Search (NAS) with Ensembles: NAS can be used to automatically discover \noptimal neural network architectures. By combining multiple architectures, ensemble learning can \nbenefit from the strengths of different designs.\n\uf0b7 Adversarially Trained Ensembles: Using adversarial training techniques in the ensemble process can \nimprove robustness by making the ensemble models less susceptible to adversarial attacks.\n\uf0b7 Ensemble Learning in Transfer Learning: When fine-tuning pre-trained models on new datasets, \nensemble learning can improve the generalization ability of the resulting models by leveraging the \ndiversity of pre-trained networks.\n3. Opportunities of Ensemble Deep Learning\nEnsemble deep learning offers several promising opportunities that can further enhance the applicability and \nperformance of AI systems:\n3.1 Improved Generalization and Robustness\nBy combining multiple models, ensemble learning can significantly reduce overfitting and improve the \ngeneralization ability of the final model. This is particularly beneficial in applications where robustness is \ncritical, such as medical diagnosis, autonomous driving, and cybersecurity.\n3.2 Handling Imbalanced Data\nEDL can improve performance on imbalanced datasets by ensuring that different models focus on different \naspects of the data. Each model in the ensemble may specialize in learning different patterns, allowing the \noverall model to perform well across all classes.\nwww.ijcspub.org \u00a9 2024 IJCSPUB | Volume 14, Issue 4 December 2024 | ISSN: 2250-1770\nIJCSP24D1098 International Journal of Current Science (IJCSPUB) www.ijcspub.org 946\n3.3 Increased Accuracy and Performance\nThe aggregation of diverse models typically leads to an increase in accuracy, especially in scenarios where \nthe individual models have complementary strengths. For instance, combining models with different \narchitectures (CNNs, RNNs, and Transformers) can lead to more accurate predictions.\n3.4 Model Interpretability\nThough deep learning models are often criticized for their lack of interpretability, ensemble methods can help \nimprove transparency. By examining the decision-making process of different models in the ensemble, it may \nbecome easier to interpret how the model arrives at a particular decision.\n4. Challenges in Ensemble Deep Learning\nDespite its potential, several challenges hinder the widespread adoption of ensemble deep learning:\n4.1 Computational Complexity and Scalability\nTraining multiple deep learning models can be computationally expensive, requiring significant time and \nhardware resources. The computational complexity increases with the number of models in the ensemble, \nmaking it difficult to scale the approach for large datasets and real-time applications.\n4.2 Lack of Diversity\nThe effectiveness of ensemble methods depends on the diversity of the base models. If the models are highly \nsimilar, combining them will not provide significant improvements over a single model. Achieving \nmeaning...",
      "url": "https://rjpn.org/ijcspub/papers/IJCSP24D1098.pdf"
    },
    {
      "title": "",
      "text": "HAL Id: hal-04455937\nhttps://hal.science/hal-04455937v1\nPreprint submitted on 13 Feb 2024\nHAL is a multi-disciplinary open access\narchive for the deposit and dissemination of sci\u0002entific research documents, whether they are pub\u0002lished or not. The documents may come from\nteaching and research institutions in France or\nabroad, or from public or private research centers.\nL\u2019archive ouverte pluridisciplinaire HAL, est\ndestin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents\nscientifiques de niveau recherche, publi\u00e9s ou non,\n\u00e9manant des \u00e9tablissements d\u2019enseignement et de\nrecherche fran\u00e7ais ou \u00e9trangers, des laboratoires\npublics ou priv\u00e9s.\nDeep Anti-Regularized Ensembles provide reliable\nout-of-distribution uncertainty quantification\nAntoine de Mathelin, Francois Deheeger, Mathilde Mougeot, Nicolas Vayatis\nTo cite this version:\nAntoine de Mathelin, Francois Deheeger, Mathilde Mougeot, Nicolas Vayatis. Deep Anti-Regularized\nEnsembles provide reliable out-of-distribution uncertainty quantification. 2024. ffhal-04455937ff\nDeep Anti-Regularized Ensembles provide reliable\nout-of-distribution uncertainty quantification\nAntoine de Mathelin\n1,2\n, Francois Deheeger\n1\n, Mathilde Mougeot\n2\n, and Nicolas\nVayatis\n2\n1 Manufacture Fran\u00e7aise des Pneumatiques Michelin, Clermont-Ferrand, France\n{antoine.de-mathelin-de-papigny, francois.deheeger}@michelin.com\n2 Universit\u00e9 Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, Gif-sur-Yvette,\nFrance {mathilde.mougeot, nicolas.vayatis}@ens-paris-saclay.fr\nAbstract. We consider the problem of uncertainty quantification in high\ndimensional regression and classification for which deep ensemble have\nproven to be promising methods. Recent observations have shown that\ndeep ensemble often return overconfident estimates outside the training\ndomain, which is a major limitation because shifted distributions are\noften encountered in real-life scenarios. The principal challenge for this\nproblem is to solve the trade-off between increasing the diversity of the\nensemble outputs and making accurate in-distribution predictions. In this\nwork, we show that an ensemble of networks with large weights fitting the\ntraining data are likely to meet these two objectives. We derive a simple\nand practical approach to produce such ensembles, based on an original\nanti-regularization term penalizing small weights and a control process\nof the weight increase which maintains the in-distribution loss under\nan acceptable threshold. The developed approach does not require any\nout-of-distribution training data neither any trade-off hyper-parameter\ncalibration. We derive a theoretical framework for this approach and show\nthat the proposed optimization can be seen as a \"water-filling\" problem.\nSeveral experiments in both regression and classification settings highlight\nthat Deep Anti-Regularized Ensembles (DARE) significantly improve\nuncertainty quantification outside the training domain in comparison to\nrecent deep ensembles and out-of-distribution detection methods. All the\nconducted experiments are reproducible and the source code is available\nat https://github.com/antoinedemathelin/DARE.\nKeywords: Deep Ensemble\u00b7 Uncertainty\u00b7 Out-of-distribution\u00b7 Anti\u0002regularization\n1 Introduction\nWith the adoption of deep learning models in a variety of real-life applications\nsuch as autonomous vehicles [4,9], or industrial product certification [23], pro\u0002viding uncertainty quantification for their predictions becomes critical. Indeed,\nvarious adaptations of classical uncertainty quantification methods to deep learn\u0002ing predictions have been recently introduced as Bayesian neural networks [22,27],\narXiv:2304.04042v1 [cs.LG] 8 Apr 2023\n2 A. de Mathelin et al.\nMC-dropout [11], quantile regression [32] and deep ensembles [17,42,43]. These\nmethods appear to be quite efficient in predicting the uncertainty in the train\u0002ing domain (the domain defined by the training set), called in-distribution or\naleatoric uncertainty [1]. However, when dealing with data outside the training\ndistribution, i.e. out-of-distribution data (OOD), the uncertainty estimation\noften appears to be overconfident [6,20,28]. This is a critical issue, because deep\nmodels are often deployed on shifted distributions [24,34,44]; overconfidence on\nan uncontrolled domain can lead to dramatic consequences in autonomous cars\nor to poor industrial choices in product design.\nOne problem to be solved is to increase the output diversity of the ensemble\nin regions where no data are available during training. This is a very challenging\ntask as neural network outputs are difficult to control outside of the training\ndata regions. In this perspective, contrastive methods make use of real [29,39]\nor synthetic [14,26,36] auxiliary OOD data to constrain the network output\nout-of-distribution. However, these approaches can not guarantee prediction\ndiversity for unseen OOD data as the auxiliary sample may not be representative\nof real OODs encountered by the deployed ensemble. Another set of methods\nassumes that the diversity of the ensemble outputs is linked to the diversity of\nthe networks\u2019 architectures [45], hyper-parameters [43], internal representations\n[31,38] or weights [5,30]. The main difficulty encountered when using these\napproaches is to solve the trade-off between increasing the ensemble diversity\nand providing accurate prediction in-distribution. The current approach to deal\nwith this issue consists in setting a trade-off parameter with hold-out validation\n[13,21,30] which is time consuming and often penalizes the diversity.\nConsidering these difficulties, a question that naturally arises is how to ensure\nimportant output diversity for any unknown data region while maintaining\naccurate in-distribution predictions? In this work, we tackle this question with\nthe following reasoning: an ensemble of large weights networks essentially produces\noutputs with large variance for all data points. Furthermore, to make accurate\nprediction on the training distribution, the output variance for training data\nneeds to be reduced, which requires that some of the network\u2019s weights are also\nreduced. However, to prevent the output variance from being reduced anywhere\nother than the training domain, the network weights should then be kept as large\nas possible. Following this reasoning, we seek an ensemble providing accurate\nprediction in-distribution and keeping the weights as large as possible.\nTo meet these two objectives, deviating from traditional recommendations for\ndeep learning training, we propose an anti-regularization process that consists in\npenalizing small weights during training optimization. To find the right trade-off\nbetween increasing the weights and providing accurate prediction in-distribution,\nwe introduce a control process that activates or deactivates the weight increase\nafter each batch update if the training loss is respectively under or above a\nthreshold. Thus, an increase of the weights induces an increase of the prediction\nvariance while the control on the loss enforces accurate in-distribution predictions.\nSynthetic experiments on toy datasets confirm the efficiency of our proposed\napproach (cf Figure 1). We observe that the uncertainty estimates of our Deep\nDeep Anti-regularized Ensembles 3\nAnti-Regularized Ensembles (DARE) increase for any data point deviating from\nthe training domain, whereas, for the vanilla deep ensemble, the uncertainty\nestimates remain low for some OOD regions.\n(a) \"Two-moons\" classification (b) 1D-regression [13]\nFig. 1: Synthetic datasets uncertainty estimation. White points represent\nthe training data. For each experiment, the ensemble are composed of 20 neural\nnetworks. For classification, darker areas correspond to higher predicted uncer\u0002tainty. For regression, the confidence intervals for \u00b12\u03c3 are represented in light\nblue. The full experiment description is reported in Appendix.\nThe contributions of the present work are the following:\n\u2013 A novel and simple anti-regularization strategy is ...",
      "url": "https://hal.science/hal-04455937/document"
    },
    {
      "title": "Ensemble deep learning: A review - ADS",
      "text": "Ensemble deep learning: A review - ADS\nNow on home page\n## ADS\n## Ensemble deep learning: A review[]()\n* [Ganaie, M. A.](https://ui.adsabs.harvard.edu/search/?q=author:%22Ganaie,+M.+A.%22);\n* [Hu, Minghui](https://ui.adsabs.harvard.edu/search/?q=author:%22Hu,+Minghui%22);\n* [Malik, A. K.](https://ui.adsabs.harvard.edu/search/?q=author:%22Malik,+A.+K.%22);\n* [Tanveer, M.](https://ui.adsabs.harvard.edu/search/?q=author:%22Tanveer,+M.%22);\n* [Suganthan, P. N.](https://ui.adsabs.harvard.edu/search/?q=author:%22Suganthan,+P.+N.%22)\n#### Abstract\nEnsemble learning combines several individual models to obtain better generalization performance. Currently, deep learning architectures are showing better performance compared to the shallow or traditional models. Deep ensemble learning models combine the advantages of both the deep learning models as well as the ensemble learning such that the final model has better generalization performance. This paper reviews the state-of-art deep ensemble models and hence serves as an extensive summary for the researchers. The ensemble models are broadly categorised into bagging, boosting, stacking, negative correlation based deep ensemble models, explicit/implicit ensembles, homogeneous/heterogeneous ensemble, decision fusion strategies based deep ensemble models. Applications of deep ensemble models in different domains are also briefly discussed. Finally, we conclude this paper with some potential future research directions.\nPublication:\narXiv e-prints\nPub Date:April 2021DOI:\n[10.48550/arXiv.2104.02395](https://ui.adsabs.harvard.edu/link_gateway/2021arXiv210402395G/doi:10.48550/arXiv.2104.02395)**\narXiv:[arXiv:2104.02395](https://ui.adsabs.harvard.edu/link_gateway/2021arXiv210402395G/arXiv:2104.02395)**Bibcode:[2021arXiv210402395G](https://ui.adsabs.harvard.edu/abs/2021arXiv210402395G/abstract)**Keywords:\n* Computer Science - Machine Learning;\n* Computer Science - Artificial Intelligence;\n* Computer Science - Computer Vision and Pattern RecognitionE-Print:Engineering Applications of Artificial Intelligence, 2022\n**\nfull text sources\nPreprint\n[**](https://ui.adsabs.harvard.edu/link_gateway/2021arXiv210402395G/EPRINT_PDF)\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/2021arXiv210402395G/EPRINT_HTML)",
      "url": "https://ui.adsabs.harvard.edu/abs/2021arXiv210402395G/abstract"
    },
    {
      "title": "(Machine Learning) Ensemble learning",
      "text": "# (Machine Learning) Ensemble learning\n\nNov 18, 2019Download as pptx, pdf0 likes545 viewsAI-enhanced description\n\n[![Omkar Rane](https://cdn.slidesharecdn.com/profile-photo-OmkarRane15-48x48.jpg?cb=1701802586)\\\nOmkar Rane](https://www.slideshare.net/OmkarRane15)\n\nEnsemble learning combines multiple machine learning models to obtain better predictive performance than could be obtained from any of the constituent models alone. It works by training base models on different subsets of the original data or using different algorithms and then combining their predictions. Two common ensemble methods are bagging and boosting. Bagging generates additional training data by sampling the original data with replacement and trains base models on these samples, while boosting iteratively reweights training examples to focus on those misclassified by previous base models. Both aim to reduce variance and prevent overfitting.\n\nRead more\n\n1 of 14\n\nDownload nowDownloaded 17 times\n\n![-Omkar Rane B.Tech (ENTC) BETB118 Department Elective: Machine Learning (Assignment 4)  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-1-2048.jpg)\n\n![Ensemble learning combines various set of learners (individual models) together which actually improvise on stability and predictive power of model. \u2022 Combining classifier is an ensemble method which increases the accuracy. \u2022 To get improved model M*,combine a series of \u201cn\u201d learned models, M1,M2,M3\u2026..Mn Training Data Data1 Data mData2 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 Learner1 Learner2 Learner m\uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 Model1 Model2 Model m\uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 \uf0d7 Model Combiner Final Model Original training data Step 1: create multiple datasets Step 2: build multiple classifiers Step 3: Combine classifiers [1]  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-2-2048.jpg)\n\n![\u2022 Ensemble methods that minimize variance \u2013 Bagging \u2013 Random Forests \u2022 Ensemble methods that minimize bias \u2013 Functional Gradient Descent \u2013 Boosting \u2013 Ensemble Selection [6]  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-3-2048.jpg)\n\n![(Machine Learning) Ensemble learning ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-4-2048.jpg)\n\n![1.The dataset is too large or small \u2014 If dataset is too large or small we have to use sampling to choose sample to take average of the result. 2.Complex(Non-linear) data \u2014 Real time dataset is mostly in non-linear fashion. so when we train a single model which cannot define the class boundary clearly and model become under-fit. That case we have to take different sub sample and take average of different model. 3.High Confidence \u2014 when we train a model with multiple classes and get high correlated output these situation lead the High Confidence. So, In this case most of the model predict the same class which lead that high confidence 4.Low Bias- It is a measure of how flexible the model is so if the model is very flexible or very powerful then the bias is low. 5.Low variance-Variance is high if you give different subsets of data as training set, the models output are very different then we say variance is high. Low for vice-versa. Reasons to use ensemble learning  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-5-2048.jpg)\n\n![[2]  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-6-2048.jpg)\n\n![Bagging, which stands for bootstrap aggregating, is one of the earliest, most intuitive and perhaps the simplest ensemble based algorithms, with a surprisingly good performance (Breiman 1996). Diversity of classifiers in bagging is obtained by using bootstrapped replicas of the training data. Bagging Steps: 1) Suppose there are N observations and M features in training data set. A sample from training data set is taken randomly with replacement. 2) A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively. 3) The tree is grown to the largest. 4) Above steps are repeated n times and prediction is given based on the aggregation of predictions from n number of trees.  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-7-2048.jpg)\n\n![Advantages: 1) Reduces over-fitting of the model. 2) Handles higher dimensionality data very well. 3) Maintains accuracy for missing data. Disadvantages: 1) Since final prediction is based on the mean predictions from subset trees, it won\u2019t give precise values for the classification and regression model. Python Syntax: rfm = RandomForestClassifier(n_estimators=80, oob_score=True, n_jobs=-1, random_state=101, max_features = 0.50, min_samples_ fit(x_train, y_train) predicted = rfm.predict_proba(x_test) Objectives Achieved by Bagging:  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-8-2048.jpg)\n\n![Similar to bagging, boosting also creates an ensemble of classifiers by resampling the data, which are then combined by majority voting. However, in boosting, resampling is strategically geared to provide the most informative training data for each consecutive classifier. Boosting Steps: 1) Draw a random subset of training samples d1 without replacement from the training set D to train a weak learner C1 2) Draw second random training subset d2 without replacement from the training set and add 50 percent of the samples that were previously falsely classified/misclassified to train a weak learner C2 3) Find the training samples d3 in the training set D on which C1 and C2 disagree to train a third weak learner C3 4) Combine all the weak learners via majority voting.  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-9-2048.jpg)\n\n![Advantages: 1) Supports different loss function (we have used \u2018binary:logistic\u2019 for this example). 2) Works well with interactions. Disadvantages: 1) Prone to over-fitting. 2) Requires careful tuning of different hyper-parameters. Python Syntax: from xgboost import XGBClassifier xgb = XGBClassifier(objective=\u2019binary:logistic\u2019, n_estimators=70, seed=101) fit(x_train, y_train) predicted = xgb.predict_proba(x_test) Objectives Achieved by Boosting:  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-10-2048.jpg)\n\n![Email Spam and Not spam detection e.g.: Feature used for Gmail [4]  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-11-2048.jpg)\n\n![Users rate movies (1,2,3,4,5 stars); Netflix makes suggestions to users based on previous rated movies. \u201cThe Netflix Prize seeks to substantially improve the accuracy of predictions about how much someone is going to love a movie based on their movie preferences. Improve it enough and you win one (or more) Prizes. Winning the Netflix Prize improves our ability to connect people to the movies they love.\u201d  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-12-2048.jpg)\n\n![\u2022 No clear winner; usually depends on the data \u2022 Bagging is computationally more efficient than boosting (note that bagging can train the M models in parallel, boosting can't) \u2022 Both reduce variance (and overfitting) by combining different models \u2022 The resulting model has higher stability as compared to the individual ones \u2022 Bagging usually can't reduce the bias, boosting can (note that in boosting, the training error steadily decreases) \u2022 Bagging usually performs better than boosting if we don't have a high bias and only want to reduce variance (i.e., if we are overfitting)  ](https://image.slidesharecdn.com/mlensemblelearning-191118175056/75/Machine-Learning-Ensemble-learning-13-2048.jpg)\n\n![1.Baldi, P., Frasconi, P., Smyth, P. (20...",
      "url": "https://www.slideshare.net/slideshow/machine-learning-ensemble-learning/194829662"
    }
  ]
}