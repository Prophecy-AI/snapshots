{
  "query": "How to improve neural network generalization when local cross-validation is better than test performance?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving the generalization of a neural network when the local cross\u2011validation (CV) score looks better than the held\u2011out test performance can be tackled by making the CV estimate more reliable and by reducing the model\u2019s tendency to over\u2011fit the CV splits.  Recent research suggests a handful of concrete actions:\n\n1. **Reshuffle and repeat the CV splits during hyper\u2011parameter search** \u2013 instead of a single fixed partition, generate many random train/validation splits and optimise hyper\u2011parameters on the aggregate CV score.  This \u201creshuffling\u201d reduces the variance of the CV estimate and has been shown to improve the final test\u2011set generalisation of tuned models\u202f([arXiv\u202f2405.15393](https://arxiv.org/abs/2405.15393)).  \n\n2. **Add a stability regularisation term to the CV objective** \u2013 use a nested\u202fk\u2011fold CV where the inner loop selects hyper\u2011parameters by minimising a weighted sum of the usual CV loss and an empirical model\u2011stability measure (the weight itself is chosen by an outer CV).  This penalises hyper\u2011parameters that lead to unstable fits and has been demonstrated to raise out\u2011of\u2011sample performance for unstable, interpretable models such as sparse ridge regression and CART\u202f([Stability Regularized CV, arXiv\u202f2505.06927v1](https://arxiv.org/html/2505.06927v1)).  \n\n3. **Restrict the hyper\u2011parameter search space and apply early stopping** \u2013 excessive searching over many configurations can cause the CV estimate to over\u2011fit the validation folds (the \u201cdanger of CV\u201d effect).  Limiting the number of candidates, using early\u2011stopping criteria, and favouring simpler architectures help keep the model from memorising the CV splits\u202f([On the Dangers of Cross\u2011Validation, 2008 PDF](https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf)).  \n\n4. **Control model complexity explicitly** \u2013 incorporate regularisation techniques (weight decay, dropout, batch\u2011norm) and monitor validation loss to stop training before the model begins to over\u2011fit.  The literature on neural\u2011network modelling stresses that careful complexity control is essential for reliable generalisation\u202f([Optimal Data Split for Generalization Estimation, DTU\u202f1999 PDF](https://backend.orbit.dtu.dk/ws/files/5381207/Larsen.pdf)).  \n\n5. **Use bootstrap\u2011based validation or the Deep Bootstrap framework** \u2013 training the network on bootstrap\u2011sampled subsets and evaluating on out\u2011of\u2011bag data aligns online learning behaviour with offline generalisation, often yielding more trustworthy performance estimates\u202f([The Deep Bootstrap Framework, arXiv\u202f2010.08127](https://arxiv.org/abs/2010.08127)).  \n\n6. **Ensemble or stack models** \u2013 combining several independently trained networks (or mixing them with other learners) can reduce variance and improve test\u2011set error beyond what any single CV\u2011optimised model achieves\u202f([Stacked Generalization, Neural Networks\u202f1992](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)).  \n\n7. **Validate the improvements with repeated CV and a fresh hold\u2011out test set** \u2013 after applying the steps above, run multiple reshuffled CV rounds and finally evaluate on an untouched test split to confirm that the observed gains persist beyond the validation folds.  \n\nFollowing these actions\u2014reshuffling CV splits, penalising instability, limiting hyper\u2011parameter search, enforcing explicit regularisation, leveraging bootstrap validation, and optionally ensembling\u2014provides a systematic, research\u2011backed workflow for closing the gap between local CV performance and true test\u2011set generalisation.",
      "url": ""
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[2405.15393] Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2405.15393\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2405.15393**(stat)\n[Submitted on 24 May 2024 ([v1](https://arxiv.org/abs/2405.15393v1)), last revised 7 Nov 2024 (this version, v2)]\n# Title:Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization\nAuthors:[Thomas Nagler](https://arxiv.org/search/stat?searchtype=author&amp;query=Nagler,+T),[Lennart Schneider](https://arxiv.org/search/stat?searchtype=author&amp;query=Schneider,+L),[Bernd Bischl](https://arxiv.org/search/stat?searchtype=author&amp;query=Bischl,+B),[Matthias Feurer](https://arxiv.org/search/stat?searchtype=author&amp;query=Feurer,+M)\nView a PDF of the paper titled Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization, by Thomas Nagler and 3 other authors\n[View PDF](https://arxiv.org/pdf/2405.15393)[HTML (experimental)](https://arxiv.org/html/2405.15393v2)> > Abstract:\n> Hyperparameter optimization is crucial for obtaining peak performance of machine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross-validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model&#39;s generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper. Comments:|Accepted at NeurIPS 2024. 48 pages, 8 tables, 30 figures|\nSubjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:2405.15393](https://arxiv.org/abs/2405.15393)[stat.ML]|\n|(or[arXiv:2405.15393v2](https://arxiv.org/abs/2405.15393v2)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2405.15393](https://doi.org/10.48550/arXiv.2405.15393)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Lennart Schneider [[view email](https://arxiv.org/show-email/82384cda/2405.15393)]\n**[[v1]](https://arxiv.org/abs/2405.15393v1)**Fri, 24 May 2024 09:48:18 UTC (11,071 KB)\n**[v2]**Thu, 7 Nov 2024 17:10:54 UTC (11,798 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization, by Thomas Nagler and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2405.15393)\n* [HTML (experimental)](https://arxiv.org/html/2405.15393v2)\n* [TeX Source](https://arxiv.org/src/2405.15393)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.15393&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.15393&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2024-05](https://arxiv.org/list/stat.ML/2024-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2405.15393?context=cs)\n[cs.LG](https://arxiv.org/abs/2405.15393?context=cs.LG)\n[stat](https://arxiv.org/abs/2405.15393?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.15393)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.15393)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.15393)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2405.15393&amp;description=Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2405.15393&amp;title=Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on ...",
      "url": "https://arxiv.org/abs/2405.15393"
    },
    {
      "title": "Stability Regularized Cross-Validation",
      "text": "Stability Regularized Cross-Validation\n# Stability Regularized Cross-Validation\nRyan Cory-Wright\nDepartment of Analytics, Marketing and Operations\nImperial College Business School\nr.cory-wright@imperial.ac.uk\nAndr\u00e9s G\u00f3mez\nDepartment of Industrial and Systems Engineering\nViterbi School of Engineering, University of Southern California, CA\ngomezand@usc.edu\n###### Abstract\nWe revisit the problem of ensuring strong test-set performance via cross-validation. Motivated by the generalization theory literature, we propose a nested k-fold cross-validation scheme that selects hyperparameters by minimizing a weighted sum of the usual cross-validation metric and an empirical model-stability measure. The weight on the stability term is itself chosen via a nested cross-validation procedure. This reduces the risk of strong validation set performance and poor test set performance due to instability. We benchmark our procedure on a suite of13131313real-world UCI datasets, and find that, compared tok\ud835\udc58kitalic\\_k-fold cross-validation over the same hyperparameters, it improves the out-of-sample MSE for sparse ridge regression and CART by4%percent44\\\\%4 %on average, but has no impact on XGBoost. This suggests that for interpretable and unstable models, such as sparse regression and CART, our approach is a viable and computationally affordable method for improving test-set performance.\n## 1Introduction\nA central problem in machine learning and data-driven optimization involves constructing models that reliably generalize well to unseen data. One of the most popular approaches is cross-validation as introduced by> [\n[> 53\n](https://arxiv.org/html/2505.06927v1#bib.bib53)> , [> 18\n](https://arxiv.org/html/2505.06927v1#bib.bib18)> ]\n, which selects hyperparameters that perform well on a cross-validation set as a proxy for strong test-set performance. Indeed, this model selection pipeline is advocated by most machine learning textbooks> [e.g. [> 28\n](https://arxiv.org/html/2505.06927v1#bib.bib28)> , [> 25\n](https://arxiv.org/html/2505.06927v1#bib.bib25)> ]\n. Moreover, in the statistical learning literature, there is a broad set of conditions under which the cross-validation loss (possibly with mild corrections) is a good estimator of the test-set error> [\n[> 41\n](https://arxiv.org/html/2505.06927v1#bib.bib41)> , [> 32\n](https://arxiv.org/html/2505.06927v1#bib.bib32)> , [> 29\n](https://arxiv.org/html/2505.06927v1#bib.bib29)> , [> 40\n](https://arxiv.org/html/2505.06927v1#bib.bib40)> ]\n, especially when the sample size is large> [\n[> 34\n](https://arxiv.org/html/2505.06927v1#bib.bib34)> , [> 11\n](https://arxiv.org/html/2505.06927v1#bib.bib11)> ]\n.\nAt the same time, both theory> [e.g. [> 48\n](https://arxiv.org/html/2505.06927v1#bib.bib48)> , [> 22\n](https://arxiv.org/html/2505.06927v1#bib.bib22)> ]\nand experiments> [e.g. [> 45\n](https://arxiv.org/html/2505.06927v1#bib.bib45)> , [> 44\n](https://arxiv.org/html/2505.06927v1#bib.bib44)> , [> 42\n](https://arxiv.org/html/2505.06927v1#bib.bib42)> , [> 52\n](https://arxiv.org/html/2505.06927v1#bib.bib52)> , [> 3\n](https://arxiv.org/html/2505.06927v1#bib.bib3)> ]\nhave documented anadaptivity gapbetween validation and test-set performance (although not always observed> [\n[> 46\n](https://arxiv.org/html/2505.06927v1#bib.bib46)> , [> 3\n](https://arxiv.org/html/2505.06927v1#bib.bib3)> ]\n), especially in settings with limited data. Concretely, a positive adaptivity gap arises when the validation error is systematically lower than the test set error of a machine learning model.\nOne explanation for adaptivity gaps is as follows: validation sets give approximately unbiased estimators of test set performance for afixedcombination of hyperparameters. However, validation scores are random variables and subject to some variance. Therefore, the act ofoptimizingthe validation set error risks selecting hyperparameter combinations that disappoint out of sample. In the extreme case with many hyperparameters relative to the number of samples, the act of optimizing the (cross) validation error can be viewed as training on the (cross) validation set. This phenomenon is well documented in different parts of the statistics and optimization literature, where it is variously called \u201cpost-decision surprise\u201d, \u201cout-of-sample disappointment\u201d, \u201cresearcher degree of freedom\u201d or \u201cthe optimizer\u2019s curse\u201d> [\n[> 27\n](https://arxiv.org/html/2505.06927v1#bib.bib27)> , [> 38\n](https://arxiv.org/html/2505.06927v1#bib.bib38)> , [> 39\n](https://arxiv.org/html/2505.06927v1#bib.bib39)> , [> 51\n](https://arxiv.org/html/2505.06927v1#bib.bib51)> , [> 50\n](https://arxiv.org/html/2505.06927v1#bib.bib50)> , [> 56\n](https://arxiv.org/html/2505.06927v1#bib.bib56)> ]\n. This raises the question:is it possible to improve the performance of (cross) validation by combating the adaptivity gap, possibly by selecting models with a higher validation error and a lower score according to a second metric.\nIn this work, we propose a strategy for mitigating out-of-sample disappointment and show that it sometimes improves cross-validation\u2019s performance. Specifically, we propose selecting models according to a weighted sum of their cross-validation error and their empiricalhypothesis stability(see Section[2.2](https://arxiv.org/html/2505.06927v1#S2.SS2)). This is motivated by the observation that both the cross-validation error and the hypothesis stability appear in generalization bounds on the test-set error; thus, minimizing the cross-validation error alone may be vulnerable to selecting high-variance models that perform poorly out-of-sample.\nOur main contributions are threefold. First, we extend a generalization bound on the test set error due to> [\n[> 11\n](https://arxiv.org/html/2505.06927v1#bib.bib11)> ]\nfrom leave-one-out tok\ud835\udc58kitalic\\_k-fold cross-validation. This generalization bound takes the form of the cross-validation error plus a term related to a model\u2019s hypothesis stability. Second, motivated by this (often conservative) bound, we proposeregularizingcross-validation by selecting models that minimize a weighted sum of a validation metric and the hypothesis stability, rather than the validation score alone, to mitigate out-of-sample disappointment without being overly conservative. Indeed, models with a low cross-validation errorthat are stablegeneralize better than models with a low cross-validation error that are unstable. Moreover, to select the weight in this scheme, we embed the entire scheme within a nested cross-validation procedure. Finally, we empirically investigate our proposal using sparse ridge regression, CART, and XGBoost models, and find that it improves the out-of-sample performance of sparse ridge regression and CART by4%percent44\\\\%4 %on average but has no impact on XGBoost, likely because XGBoost is a more stable learner.\n### 1.1Motivating Example: Poor Performance of Cross-Validation for Sparse Linear Regression\nWe illustrate the pitfalls of cross-validation in a sparse ridge regression setting, as studied by> [\n[> 7\n](https://arxiv.org/html/2505.06927v1#bib.bib7)> , [> 31\n](https://arxiv.org/html/2505.06927v1#bib.bib31)> , [> 35\n](https://arxiv.org/html/2505.06927v1#bib.bib35)> ]\n. Suppose we wish to recover a\u03c4true=5subscript\ud835\udf0ftrue5\\\\tau\\_{\\\\text{true}}=5italic\\_\u03c4 start\\_POSTSUBSCRIPT true end\\_POSTSUBSCRIPT = 5sparse regressor which is generated from a stochastic process according to the following setup> [cf. [> 8\n](https://arxiv.org/html/2505.06927v1#bib.bib8)> ]\n: we fix the number of featuresp\ud835\udc5dpitalic\\_p, number of datapointsn\ud835\udc5bnitalic\\_n, correlation parameter\u03c1=0.3\ud835\udf0c0.3\\\\rho=0.3italic\\_\u03c1 = 0.3and signal to noise parameter\u03bd=1\ud835\udf081\\\\nu=1italic\\_\u03bd = 1, and generate\ud835\udc7f,\ud835\udc9a\ud835\udc7f\ud835\udc9a\\\\bm{X},\\\\bm{y}bold\\_italic\\_X , bold\\_italic\\_yaccording to a data generation procedure standard to the literature and stated in Appendix[A](https://arxiv.org/html/2505.06927v1#A1)for brevity.\nFollowing the standardcross-validationparadigm, we then evaluate thecross-validationerror for each\u03c4\ud835\udf0f\\\\tauitalic\\_\u03c4and20...",
      "url": "https://arxiv.org/html/2505.06927v1"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2010.08127] The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2010.08127\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2010.08127**(cs)\n[Submitted on 16 Oct 2020 ([v1](https://arxiv.org/abs/2010.08127v1)), last revised 19 Feb 2021 (this version, v2)]\n# Title:The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers\nAuthors:[Preetum Nakkiran](https://arxiv.org/search/cs?searchtype=author&amp;query=Nakkiran,+P),[Behnam Neyshabur](https://arxiv.org/search/cs?searchtype=author&amp;query=Neyshabur,+B),[Hanie Sedghi](https://arxiv.org/search/cs?searchtype=author&amp;query=Sedghi,+H)\nView a PDF of the paper titled The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers, by Preetum Nakkiran and 2 other authors\n[View PDF](https://arxiv.org/pdf/2010.08127)> > Abstract:\n> We propose a new framework for reasoning about generalization in deep learning. The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in offline learning to the problem of optimization in online learning. We then give empirical evidence that this gap between worlds can be small in realistic deep learning settings, in particular supervised image classification. For example, CNNs generalize better than MLPs on image distributions in the Real World, but this is &#34;because&#34; they optimize faster on the population loss in the Ideal World. This suggests our framework is a useful tool for understanding generalization in deep learning, and lays a foundation for future research in the area. Comments:|Accepted to ICLR 2021|\nSubjects:|Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Statistics Theory (math.ST); Machine Learning (stat.ML)|\nCite as:|[arXiv:2010.08127](https://arxiv.org/abs/2010.08127)[cs.LG]|\n|(or[arXiv:2010.08127v2](https://arxiv.org/abs/2010.08127v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2010.08127](https://doi.org/10.48550/arXiv.2010.08127)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Preetum Nakkiran [[view email](https://arxiv.org/show-email/f2556ae8/2010.08127)]\n**[[v1]](https://arxiv.org/abs/2010.08127v1)**Fri, 16 Oct 2020 03:07:49 UTC (17,486 KB)\n**[v2]**Fri, 19 Feb 2021 03:24:24 UTC (18,002 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers, by Preetum Nakkiran and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2010.08127)\n* [TeX Source](https://arxiv.org/src/2010.08127)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2010.08127&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2010.08127&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-10](https://arxiv.org/list/cs.LG/2020-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2010.08127?context=cs)\n[cs.CV](https://arxiv.org/abs/2010.08127?context=cs.CV)\n[cs.NE](https://arxiv.org/abs/2010.08127?context=cs.NE)\n[math](https://arxiv.org/abs/2010.08127?context=math)\n[math.ST](https://arxiv.org/abs/2010.08127?context=math.ST)\n[stat](https://arxiv.org/abs/2010.08127?context=stat)\n[stat.ML](https://arxiv.org/abs/2010.08127?context=stat.ML)\n[stat.TH](https://arxiv.org/abs/2010.08127?context=stat.TH)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.08127)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.08127)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.08127)\n### [2 blog links](https://arxiv.org/tb/2010.08127)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-08127)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-08127)\n[Preetum Nakkiran](<https://dblp.uni-trier.de/search/author?author=Preetum Nakkiran>)\n[Behnam Neyshabur](<https://dblp.uni-trier.de/search/author?author=Behnam Neyshabur>)\n[Hanie Sedghi](<https://dblp.uni-trier.de/search/author?author=Hanie Sedghi>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2010.08127&amp;description=The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2010.08127&amp;title=The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle...",
      "url": "https://arxiv.org/abs/2010.08127"
    },
    {
      "title": "",
      "text": "On the Dangers of Cross-Validation. An Experimental Evaluation\nR. Bharat Rao\nIKM CKS Siemens Medical Solutions USA\nGlenn Fung\nIKM CKS Siemens Medical Solutions USA\nRomer Rosales\nIKM CKS Siemens Medical Solutions USA\nAbstract\nCross validation allows models to be tested using the\nfull training set by means of repeated resampling; thus,\nmaximizing the total number of points used for testing\nand potentially, helping to protect against overfitting.\nImprovements in computational power, recent reduc\u0002tions in the (computational) cost of classification algo\u0002rithms, and the development of closed-form solutions\n(for performing cross validation in certain classes of\nlearning algorithms) makes it possible to test thousand\nor millions of variants of learning models on the data.\nThus, it is now possible to calculate cross validation per\u0002formance on a much larger number of tuned models than\nwould have been possible otherwise. However, we em\u0002pirically show how under such large number of models\nthe risk for overfitting increases and the performance\nestimated by cross validation is no longer an effective\nestimate of generalization; hence, this paper provides\nan empirical reminder of the dangers of cross valida\u0002tion. We use a closed-form solution that makes this\nevaluation possible for the cross validation problem of\ninterest. In addition, through extensive experiments we\nexpose and discuss the effects of the overuse/misuse of\ncross validation in various aspects, including model se\u0002lection, feature selection, and data dimensionality. This\nis illustrated on synthetic, benchmark, and real-world\ndata sets.\n1 Introduction\nIn a general classification problem, the goal is to learn a\nclassifier that performs well on unseen data drawn from\nthe same distribution as the available data 1; in other\nwords, to learn classifiers with good generalization. One\ncommon way to estimate generalization capabilities is\nto measure the performance of the learned classifier on\ntest data that has not been used to train the classifier.\nWhen a large test data set cannot be held out or easily\n1We concentrate on performance on data drawn from the same\ndistribution but performance on a different distribution is also a\n(less explored) problem of interest.\nacquired, resampling methods, such as cross validation,\nare commonly used to estimate the generalization er\u0002ror. The resulting estimates of generalization can also\nbe used for model selection by choosing from various\npossible classification algorithms (models) the one that\nhas the lowest cross validation error (and hence the low\u0002est expected generalization error).\nA strong argument in favor of using cross validation\nis the potential of using the entire training set for\ntesting (albeit not at once), creating the largest possible\ntest set for a fixed training data set. Essentially,\nthe classifier is trained on a subset of the training\ndata set, and tested on the remainder. This process\nis repeated systematically so that all the points in\nthe training set are tested. There has been much\nstudy on the empirical behavior of cross-validation for\nerror estimation and model selection, and more recently\ntheoretical bounds on the error in the leave-one-out\ncross-validation (loocv) estimate. Much of the focus\nhas been on the expected value of this error over all\ntraining sets of a given sample size and the asymptotic\nbehavior as the sample size increases. In this paper we\nempirically address the pitfalls of using cross validation\nerror to select among a large number of classification\nalgorithms.\nResampling methods, such as bootstrapping or\ncross validation (Stone, 1977; Kohavi, 1995a; Weiss &\nKulikowski, 1991; Efron & Tibshirani, 1993) have typ\u0002ically been used to measure the generalization perfor\u0002mance of a chosen algorithm, or possibly to select be\u0002tween a limited set of algorithms. Until the last decade,\ncross validation experiments could reasonable be per\u0002formed only on a small set of algorithms or possible\nmodels; a k-fold or loocv run for a single algorithm,\neven on a small dataset, typically ran for several hours,\nif not days. As computers have become more power\u0002ful and due to recent advances regarding the compu\u0002tational efficiency of popular classification algorithms\nand techniques (for example: linear training time for\nSVMs (Joachims, 2006) and n log(n) kernel computa\u0002tion (Raykar & Duraiswami, 2005)), cross validation\nperformance can be quickly computed on several thou\u0002sands or even millions of algorithms. Recent develop\u0002ments in grid computing now allow computers distrib\u0002uted in a large geographic area to be harnessed for a spe\u0002cific task, exponentially increasing the computing power\nat hand.\nIt is a commonly held believe that cross validation,\nlike any other tool or metric, can be abused (Ng,\n1997). Some basic heuristic procedures have been\nemployed to avoid these problems. For example, when\npossible a sequestered test set is kept aside. This\nset is finally used only after training to verify that\nthe chosen classifier indeed has superior generalization.\nAny modeling decisions based upon experiments on the\ntraining set, even cross validation estimates, are suspect,\nuntil independently verified.\nDespite certain general knowledge about the draw\u0002backs attached to cross validation, there has not been a\nsufficiently clear experimental (practical) investigation\non the behavior of the estimate of generalization error\nfor a fixed data set.\nIn this paper we provide an empirical reminder\nof a fact that is known but usually underestimated:\nwhen the set of algorithms to be considered becomes\nlarge, cross validation is no longer a good measure\nof generalization performance, and accordingly can no\nlonger be used for algorithm or feature selection. In\naddition we experimentally show the impact of cross\nvalidation as the data dimensionality increases and\nfor feature selection. We provide experimental results\non synthetic, standardized benchmark (from the UCI\nrepository), and a real-world dataset related to clinical\ndiagnosis in virtual colonoscopy.\n2 Related Research\nA fundamental issue in machine learning is to obtain\nan accurate estimate of the generalization error of a\nmodel trained on a finite data set. Precisely estimating\nthe accuracy of a model is not only important to ex\u0002amine the generalization performance of an algorithm,\nbut also for choosing an algorithm from a variety of\nlearning algorithms. Empirical estimators based upon\nresampling, which include bootstrap (Efron & Tibshi\u0002rani, 1993), cross validation (Stone, 1977) estimates are\npopular, and Holdout estimates where a test set is se\u0002questered until the model is frozen are also used.\nA fair amount of research has focused on the\nempirical performance of leave-one-out cross validation\n(loocv) and k-fold CV on synthetic and benchmark\ndata sets. Experiments by (Breiman & Spector, 1992)\nshow that k-fold CV has better empirical performance\nthan loocv for feature selection for linear regression.\n(Kohavi, 1995b) also obtains results in favor of 10-\nfold cross validation using decision trees and Naive\nBayes, and demonstrates the bias-variance trade-off for\ndifferent values of k on multiple benchmark data sets.\n(Kohavi & Wolpert, 1996) discuss the bias-variance\ntrade-off for classifiers using a misclassification loss\nfunction. Our work, while not directly related to the\nbias-variance trade-off is closely related to the notion of\nvariance.\nFrom a theoretical perspective, the most general\ntheoretical results for training error estimates are pro\u0002vided by (Vapnik, 1982) who proved that the training\nerror estimate is less than O(\np\nV C/n) away from the\ntrue test error where V C is the VC dimension of a hy\u0002pothesis space. More recently, the task of developing\nupper bounds on the loocv error for a specific method\u0002ology has drawn the attention in the learning theory\ncommunity. For example, (Zhang, 2003) has derived\nupper bounds on the expected loocv error to show con\u0002sistency for a set of kernel methods. These consistenc...",
      "url": "https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf"
    },
    {
      "title": "",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0893608005800231)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0893608005800231/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#preview-section-abstract)\n- [References (29)](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#preview-section-references)\n- [Cited by (6104)](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/neural-networks)\n\n## [Neural Networks](https://www.sciencedirect.com/journal/neural-networks)\n\n[Volume 5, Issue 2](https://www.sciencedirect.com/journal/neural-networks/vol/5/issue/2), 1992, Pages 241-259\n\n[![Neural Networks](https://ars.els-cdn.com/content/image/1-s2.0-S0893608005X80194-cov150h.gif)](https://www.sciencedirect.com/journal/neural-networks/vol/5/issue/2)\n\n# Original Contribution  Stacked generalization [\\*](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231\\#aep-article-footnote-id1)\n\nAuthor links open overlay panelDavid H.Wolpert\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/S0893-6080(05)80023-1](https://doi.org/10.1016/S0893-6080(05)80023-1) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0893608005800231&orderBeanReset=true)\n\n## Abstract\n\nThis paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.\n\nRecommended articles\n\n- CasdagliM.\n\n### [Non-linear prediction of chaotic time-series](https://www.sciencedirect.com/science/article/pii/0167278989900742)\n\n\n\n\n### Physica D\n\n\n\n(1989)\n\n- WolpertD.\n\n### [Constructing a generalizer superior to NET-talk via a mathematical theory of generalization](https://www.sciencedirect.com/science/article/pii/089360809090027I)\n\n\n\n\n### Neural Networks\n\n\n\n(1990)\n\n- AnshelevichV.V. _et al._\n\n### On the ability of neural networks to perform generalization by induction\n\n\n\n\n### Biological Cybernetics\n\n\n\n(1989)\n\n- CarteretteE.C. _et al._\n\n### Informal speech\n\n\n(1974)\n\n- DietterichT.G.\n\n### Machine learning\n\n\n\n\n### Annual Review of Computer Science\n\n\n\n(1990)\n\n- DeppischJ. _et al._\n\n### Hierarchical training of neural networks and prediction of chaotic time series\n\n\n(1990)\n\n- EfronB.\n\n### Computers and the theory of statistics: thinking the unthinkable\n\n\n\n\n### Siam Review\n\n\n\n(1979)\n\n- FarmerJ.D. _et al._\n\n### Exploiting chaos to predict the future and reduce noise\n\n\n(1988)\n\n- GustafsonS. _et al._\n\n### Neural network for interpolation and extrapolation\n\n- HollandJ.\n\n### Adaptation in natural and artificial systems\n\n\n(1975)\n\n\nLapedesA. _et al._\n\n### How neural nets work, Proceedings of the 187 IEEE Denver conference on neural networks\n\nLiKer-Chau\n\n### From Stein's unbiased risk estimates to the method of generalized cross-validation\n\n### The Annals of Statistics\n\n(1985)\n\nMorozovV.A.\n\n### Methods for solving incorrectly posed problems\n\n(1984)\n\nOmohundroS.\n\n### Efficient algorithms with neural network behavior\n\nView more references\n\n- ### [Development of an automated photolysis rates prediction system based on machine learning](https://www.sciencedirect.com/science/article/pii/S1001074224001748)\n\n\n\n2025, Journal of Environmental Sciences (China)\n\n\n\nShow abstract\n\n\n\n\n\n\n\nBased on observed meteorological elements, photolysis rates (J-values) and pollutant concentrations, an automated J-values predicting system by machine learning (J-ML) has been developed to reproduce and predict the J-values of O1D, NO2, HONO, H2O2, HCHO, and NO3, which are the crucial values for the prediction of the atmospheric oxidation capacity (AOC) and secondary pollutant concentrations such as ozone (O3), secondary organic aerosols (SOA). The J-ML can self-select the optimal \u201cModel + Hyperparameters\u201d without human interference. The evaluated results showed that the J-ML had a good performance to reproduce the J-values where most of the correlation ( _R_) coefficients exceed 0.93 and the accuracy ( _P_) values are in the range of 0.68-0.83, comparing with the J-values from observations and from the tropospheric ultraviolet and visible (TUV) radiation model in Beijing, Chengdu, Guangzhou and Shanghai. The hourly prediction was also well performed with R from 0.78 to 0.81 for next 3-days and from 0.69 to 0.71 for next 7-days, respectively. Compared with O3 concentrations by using J-values from the TUV model, an emission-driven observation-based model (e-OBM) by using the J-values from the J-ML showed a 4%-12% increase in R and 4%-30% decrease in ME, indicating that the J-ML could be used as an excellent supplement to traditional numerical models. The feature importance analysis concluded that the key influential parameter was the surface solar downwards radiation for all J-values, and the other dominant factors for all J-values were 2-m mean temperature, O3, total cloud cover, boundary layer height, relative humidity and surface pressure.\n\n- ### [Cost-sensitive stacking ensemble learning for company financial distress prediction](https://www.sciencedirect.com/science/article/pii/S0957417424013927)\n\n\n\n2024, Expert Systems with Applications\n\n\n\nShow abstract\n\n\n\n\n\n\n\nFinancial distress prediction (FDP) is a topic that has received wide attention in the finance sector and data mining field. Applications of combining cost-sensitive learning with classification models to address the FDP problem have been intensely attracted. However, few combined cost-sensitive learning and Stacking to predict financial distress. In this article, a cost-sensitive learning method for FDP, namely cost-sensitive stacking (CSStacking), is put forward. In this work, a two-phase feature selection method is used to sel...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231"
    },
    {
      "title": "",
      "text": "General rights\nCopyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright \nowners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.\n\uf0b7 Users may download and print one copy of any publication from the public portal for the purpose of private study or research.\n\uf0b7 You may not further distribute the material or use it for any profit-making activity or commercial gain\n\uf0b7 You may freely distribute the URL identifying the publication in the public portal\nIf you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately \nand investigate your claim.\n \nDownloaded from orbit.dtu.dk on: Nov 16, 2025\nOn Optimal Data Split for Generalization Estimation and Model Selection\nLarsen, Jan; Goutte, Cyril\nPublished in:\nProceedings of the IEEE Workshop on Neural Networks for Signal Processing IX\nLink to article, DOI:\n10.1109/NNSP.1999.788141\nPublication date:\n1999\nDocument Version\nPublisher's PDF, also known as Version of record\nLink back to DTU Orbit\nCitation (APA):\nLarsen, J., & Goutte, C. (1999). On Optimal Data Split for Generalization Estimation and Model Selection. In\nProceedings of the IEEE Workshop on Neural Networks for Signal Processing IX (pp. 225-234). IEEE.\nhttps://doi.org/10.1109/NNSP.1999.788141\nON OPTIMAL DATA SPLIT FOR \nGENERALIZATION ESTIMATION AND MODEL \nSELECTION \nJan Larsen and Cyril Goutte \nDepartment of Mathematical Modeling, Building 321 \nTechnical University of Denmark, DK-2800 Lyngby, Denmark \nE-mail: jl,cg@imm.dtu.dk, Web: eivind.imm.dtu.dk \nOVERVIEW \nModeling with flexible models, such as neural networks, requires careful con\u0002trol of the model complexity and generalization ability of the resulting model. \nWhereas general asymptotic estimators of generalization ability have been de\u0002veloped over recent years (e.g., [9]), it is widely acknowledged that in most \nmodeling scenarios there isn't sufficient data available to reliably use these \nestimators for assessing generalization, or select/optimize models. As a con\u0002sequence, one resorts to resampling techniques like cross-validation [3, 8, 141, \njackknife or bootstrap [2]. In this paper, we address a crucial problem of \ncross-validation estimators: how to split the data into various sets. \nThe set V of all available data is usually split into two parts: the design \nset & and the test set F. The test set is exclusively reserved to a final assess\u0002ment of the model which has been designed on & (using e.g., optimization \nand model selection). This usually requires that the design set in turn is \nsplit in two parts: training set 7 and validation set V. The objective of the \ndesign/test split is to both obtain a model with high generalization ability \nand to assess the generalization error reliably. The second split is the train\u0002ing/validation split of the design set. Model parameters are trained on the \ntraining data, while the validation set provides an estimator of generalization \nerror used to e.g., choose between alternative models or optimize additional \n(hyper) parameters such as regularization or robustness parameters [lo, 121. \nThe aim is to select the split so that the generalization ability of the resulting \nmodel is as high as possible. \nThis paper is concerned with studying the very different behavior of the \ntwo data splits using hold-out cross-validation, K-fold cross-validation [3, 141 \nand randomized permutation cross-validation' [l], [13, p. 3091. First we de\u0002scribe the theoretical basics of various cross-validation techniques with the \npurpose of reliably estimating the generalization error and optimizing the \n'Also called monte-carlo cross-validation or repeated learning-testing methods. \n0-7803-5673-X/99/$10.00 0 1999 IEEE 225 \nmodel structure. The next section deals with the simple problem of estimat\u0002ing a single location parameter. This problem is tractable as non-asymptotic \ntheoretical analysis is possible, whereas mainly asymptotic analysis and simu\u0002lation studies are viable for the more complex AR-models and neural networks \ndiscussed in the subsequent sections. \nTRAINING AND GENERALIZATION \nSuppose that our model M (e.g., neural network) is described by the function \nf(x;w) where x is the input vector and 20 is the vector of parameters (or \nweights) with dimensionality m. The objective is to use the model for ap\u0002proximating the true conditional input-output distribution p(ylx), or some \nmoments thereof. For regression and signal processing problems we nor\u0002mally model the conditional expectation E{ylx}. Define the training set \n7 = {z(k);y(k)}f~l of NT input-output examples sampled from the un\u0002known but fixed joint input-output probability density p(x, y). The model \nis trained by minimizing a cost function C~(W), usually the sum of a loss \nfunction (or training error), ST(W), and a regularization term R(~,IE) pa\u0002rameterized by a set of regularization parameters K: \n1 \nCdW) = Ww) + R(w, #) = - (Y(k), y^(k)) + R(w, K) (1) \n\u201cl- k\u20acT \nwhere l(.) measures the cost of estimating the output y(k) with the model \nprediction @(k) = f(x(k); w), e.g., log-likelihood loss or the simple squared \nerror loss function l(y, y^) = lly-y^1I2. Training provides the estimated weight \nvector G = argmin, CT(W). Generalization error is defined as the expected \nloss on a future independent sample (5, y), \nG(G) = J%,,{l(Y, y^)l = 1 [(Y, y^) P(Z, Y) dXdY \nr = &-{G(G)} = / G(G)p(7) d7. \n(2) \nThe average generalization error r is defined by averaging G(G) over all \npossible training sets (see also [ll]): \n(3) \nOptimization of the model structure, including e.g., regularization parame\u0002ters [12], is done by minimizing an empirical estimate of the generalization \nerror based on the validation data. Finally, the test data provides an unbiased \nempirical estimate of the generalization error of the resulting model. \nGeneralization Assessment \nGiven a data set V = {z(k); y(l~)}F=~ of N independent input-output exam\u0002ples, let us first consider the split of the data into the design and test sets, \ndenoted by E and 3 respectively. The purpose is to design a model achieving \nmaximum generalization performance and assess this performance as reliably \nas possible. We consider three methods: \n226 \nHold-Out Cross-Validation (HO). An empirical estimate of (2) is ob\u0002tained by splitting the data once into design and test sets. Define y as the \nsplit ratio leaving NF = yN for testing and NE = (1 - y)N for design2. The \nHO estimate of generalization error for model y^ (with weights G) designed \non & is given by \nThe quality of the estimate is evaluated by considering the mean squared \nerror: \nwhere G(w*) is the minimum achievable error within the model, i.e., \nargminw G(w). The bias term is the excess generalization of our model, and \ndecreases with y. The variance term measures the reliability of the estimator \nand it increases when y decreases. We therefore expect an optimal y to solve \nthe bias/variance trade-off yopt = argmin, MSEHO(~). This optimal choice \nhas been studied asymptotically for non-linear models [ll], using Vapnik\u0002like bounds [7], and in the context of pattern recognition [6]. Surprisingly, \nyopt + 1 as N + CO, indicating that most data should be used for testing. \nFor finite sample sizes, theoretical investigations are limited to simple models \n(see below). \nK-Fold Cross-Validation (KCV). The average over all training sets in (3) \nis simulated by resampling the design and test set. In KCV, the data set is \nsplit into K disjoint subsets Fj of approximately equal sizes, U:, Fj = D. \nFor y < 1/2, the split ratio is the ratio of the size of the subsets to the total \namount of data, i.e., K = Ll/yj. We evaluate on each subset the model \ndesigned on the remaining data &j = 2) \\ 3jj3 The cross-validation estimator \nis obtained by averaging the K estimates of generalization error: ...",
      "url": "https://backend.orbit.dtu.dk/ws/files/5381207/Larsen.pdf"
    },
    {
      "title": "Mathematics > Statistics Theory",
      "text": "[2407.02754] Is Cross-Validation the Gold Standard to Evaluate Model Performance?\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[math](https://arxiv.org/list/math/recent)&gt;arXiv:2407.02754\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Mathematics \\> Statistics Theory\n**arXiv:2407.02754**(math)\n[Submitted on 3 Jul 2024 ([v1](https://arxiv.org/abs/2407.02754v1)), last revised 20 Aug 2024 (this version, v2)]\n# Title:Is Cross-Validation the Gold Standard to Evaluate Model Performance?\nAuthors:[Garud Iyengar](https://arxiv.org/search/math?searchtype=author&amp;query=Iyengar,+G),[Henry Lam](https://arxiv.org/search/math?searchtype=author&amp;query=Lam,+H),[Tianyu Wang](https://arxiv.org/search/math?searchtype=author&amp;query=Wang,+T)\nView a PDF of the paper titled Is Cross-Validation the Gold Standard to Evaluate Model Performance?, by Garud Iyengar and 2 other authors\n[View PDF](https://arxiv.org/pdf/2407.02754)[HTML (experimental)](https://arxiv.org/html/2407.02754v2)> > Abstract:\n> Cross-Validation (CV) is the default choice for evaluating the performance of machine learning models. Despite its wide usage, their statistical benefits have remained half-understood, especially in challenging nonparametric regimes. In this paper we fill in this gap and show that in fact, for a wide spectrum of models, CV does not statistically outperform the simple &#34;plug-in&#34; approach where one reuses training data for testing evaluation. Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge. Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account. We obtain our theoretical comparisons via a novel higher-order Taylor analysis that allows us to derive necessary conditions for limit theorems of testing evaluations, which applies to model classes that are not amenable to previously known sufficient conditions. Our numerical results demonstrate that plug-in performs indeed no worse than CV across a wide range of examples. Subjects:|Statistics Theory (math.ST); Methodology (stat.ME)|\nCite as:|[arXiv:2407.02754](https://arxiv.org/abs/2407.02754)[math.ST]|\n|(or[arXiv:2407.02754v2](https://arxiv.org/abs/2407.02754v2)[math.ST]for this version)|\n|[https://doi.org/10.48550/arXiv.2407.02754](https://doi.org/10.48550/arXiv.2407.02754)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tianyu Wang [[view email](https://arxiv.org/show-email/35b12039/2407.02754)]\n**[[v1]](https://arxiv.org/abs/2407.02754v1)**Wed, 3 Jul 2024 02:10:03 UTC (808 KB)\n**[v2]**Tue, 20 Aug 2024 20:44:47 UTC (800 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Is Cross-Validation the Gold Standard to Evaluate Model Performance?, by Garud Iyengar and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2407.02754)\n* [HTML (experimental)](https://arxiv.org/html/2407.02754v2)\n* [TeX Source](https://arxiv.org/src/2407.02754)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nmath.ST\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2407.02754&amp;function=prev&amp;context=math.ST) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2407.02754&amp;function=next&amp;context=math.ST)\n[new](https://arxiv.org/list/math.ST/new)|[recent](https://arxiv.org/list/math.ST/recent)|[2024-07](https://arxiv.org/list/math.ST/2024-07)\nChange to browse by:\n[math](https://arxiv.org/abs/2407.02754?context=math)\n[stat](https://arxiv.org/abs/2407.02754?context=stat)\n[stat.ME](https://arxiv.org/abs/2407.02754?context=stat.ME)\n[stat.TH](https://arxiv.org/abs/2407.02754?context=stat.TH)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2407.02754)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2407.02754)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2407.02754)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2407.02754&amp;description=Is Cross-Validation the Gold Standard to Evaluate Model Performance?>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2407.02754&amp;title=Is Cross-Validation the Gold Standard to Evaluate Model Performance?>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere t...",
      "url": "https://arxiv.org/abs/2407.02754"
    },
    {
      "title": "(PDF) Inference for the Generalization Error",
      "text": "<div><section><div><div><p>In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.</p></div><div><p><strong>Discover the world's research</strong></p><ul><li>25+ million members</li><li>160+ million publication pages</li><li>2.3+ billion citations</li></ul><p><a href=\"https://www.researchgate.net/publication/signup.SignUp.html\"><span>Join for free</span></a></p></div></div><section><span></span><a href=\"https://www.researchgate.net/publication/publication/227049463_Inference_for_the_Generalization_Error#read-preview\"></a><div>\n \n \n \n \n \n \n <div><p>Inference for the Generalization Error</p><p>Claude Nadeau</p><p>CIRANO</p><p>2020, University<span></span>,</p><p>Montreal, Qc, Canada, H3A 2A5</p><p>jcnadeau@altavista.net</p><p>Y<span></span>oshua Bengio</p><p>CIRANO and <span>Dept. IRO</span></p><p>Universit<span>\u00b4</span></p><p>e de Montr<span>\u00b4</span></p><p>eal</p><p>Montreal, Qc, Canada, H3C 3J7</p><p>bengioy@iro.umontreal.ca</p><p>Abstract</p><p>In order to to compare learning algorithms, experimental results reported</p><p>in the machine learning litterature often use statistical tests of signi\ufb01-</p><p>cance.<span> </span>Unfortunately, most of these tests do not take into account the</p><p>variability due to the choice of training set.<span> </span>We perform a theoretical</p><p>investigation<span> </span>of the variance of the cross-validation<span></span>estimate of the gen-</p><p>eralization error that takes into account the variability due to the choice</p><p>of training sets.<span> </span>This allows us to propose two new ways to estimate</p><p>this variance. We show<span></span>, via simulations, that these new statistics perform</p><p>well relative to the statistics considered by Dietterich (Dietterich,<span></span>1998).</p><p>1 Introduction</p><p>When applying a learning algorithm (or comparing<span> </span>several algorithms), one is typically</p><p>interested in estimating its generalization error<span></span>. Its point estimation is rather trivial through</p><p>cross-validation.<span> </span>Providing a variance estimate of that estimation, so that hypothesis test-</p><p>ing and/or con\ufb01dence intervals are possible, is more dif\ufb01cult, especially<span></span>, as pointed out in</p><p>(Hinton et al., 1995), if one wants to take into account the v<span></span>ariability due to the choice of</p><p>the training sets (Breiman, 1996).<span> </span>A notable effort in that direction is Dietterich\u2019s w<span></span>ork (Di-</p><p>etterich, 1998).<span> </span>Careful investigation of the v<span></span>ariance to be estimated allows us to provide</p><p>new variance estimates, which turn out to perform<span></span>well.</p><p>Let us \ufb01rst lay out the framework<span></span>in which we shall work. We assume that data are av<span></span>ail-</p><p>able in the form<span> </span>.<span> </span>For example, in the case of supervised<span> </span>learning,</p><p>, where<span> </span>and<span> </span>denote the dimensions of the<span> </span>\u2019s (inputs)</p><p>and the<span> </span>\u2019s (outputs).<span> </span>W<span></span>e also assume that the<span> </span>\u2019<span></span>s are independent with<span> </span>.</p><p>Let<span> </span>, where<span> </span>represents a subset of size<span> </span>taken from<span> </span>, be a function</p><p>.<span> </span>For instance, this function could be the<span> </span>loss<span> </span>incurred by the decision</p><p>that a learning algorithm trained on<span> </span>makes on a new example<span> </span>.<span> </span>W<span></span>e are interested in</p><p>estimating<span> </span>where<span> </span>is independent of<span> </span>. Subscript</p><p>stands for the size of the training set (<span> </span>here). The above expectation is taken over</p><p>and<span> </span>, meaning that we are interested in the performance<span></span>of an algorithm rather than</p><p>the performance of the speci\ufb01c decision function it yields on the data at hand.<span> </span>According to</p><p>Dietterich\u2019s taxonomy<span></span>(Dietterich,<span></span>1998),<span></span>we deal with problems<span></span>of type<span></span>5 through<span></span>8, (eval-</p><p>uating learning algorithms) rather then type<span></span>1 through 4 (evaluating<span></span>decision functions). We</p><p>call<span> </span>the generalization error even though it can also represent an error difference:</p><p>Generalization error</p><p>W<span></span>e may take</p><p>(1)</p></div>\n \n \n <div><p>where<span> </span>(<span> </span>) is the decision function obtained when training an</p><p>algorithm on<span> </span><span>,<span> </span>and<span> </span>is a loss function measuring the inaccuracy of a decision.<span> </span>For</span></p><p>instance, we could have<span> </span><span>, where<span> </span>is the indicator function, for</span></p><p>classi\ufb01cation problems and<span> </span>, where is<span> </span>is the Euclidean norm, for</p><p>\u201cregression\u201d problems. In that case<span> </span><span>is what most people call the generalization error.</span></p><p>Comparison of generalization errors</p><p>Sometimes, we are not interested in the performance of algorithms <span>per se</span>, but instead in</p><p>how two algorithms compare with each other<span></span>. In that case we may want to consider<span> </span><span>(2)</span></p><p>where<span> </span>and<span> </span>are decision functions obtained when<span> </span>training two algorithms</p><p>(A and B) on<span> </span>, and<span> </span>is a loss function.<span> </span>In this case<span> </span>would be a difference<span> </span>of</p><p>generalization errors as outlined in<span> </span>the previous example.</p><p>The generalization error is often estimated via some form of cross-validation.<span> </span>Since<span> </span>there</p><p>are various versions of the latter<span></span>, we lay out the speci\ufb01c form we use in this paper.</p><p>Let<span> </span>be<span> </span>a random set of<span> </span>distinct integers from<span> </span>.<span> </span>Here</p><p>represents the size of the training set and we shall let<span> </span>be the size of the test</p><p>set.</p><p>Let<span> </span>be independent such random sets, and let<span> </span>denote the</p><p>complement of<span> </span>.</p><p>Let<span> </span>be the training set obtained by subsampling<span> </span>according to the</p><p>random index set<span> </span>. The corresponding test set is<span> </span>.</p><p>Let<span> </span>.<span> </span>According to (1), this could be the error an algorithm<span> </span>trained</p><p>on the training set<span> </span>makes on example<span> </span...",
      "url": "https://www.researchgate.net/publication/227049463_Inference_for_the_Generalization_Error"
    }
  ]
}