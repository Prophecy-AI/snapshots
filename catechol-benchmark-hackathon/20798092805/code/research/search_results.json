{
  "query": "How to handle sparse molecular fingerprints in machine learning without losing information from PCA?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent reviews of molecular representations and empirical studies of dimensionality\u2011reduction techniques, you can preserve the information in sparse fingerprints while still reducing dimensionality by following these steps\u202f([WIREs Computational Molecular Science](https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.1603); [Springer](https://link.springer.com/article/10.1007/s10822-014-9819-y); [arXiv\u202f2024\u201107\u201113291](https://arxiv.org/abs/2407.13291); [SparseChem GitHub](https://github.com/melloddy/SparseChem); [ML4Molecules\u202f2020](https://ml4molecules.github.io/papers2020/ML4Molecules_2020_paper_65.pdf)):\n\n1. **Keep the fingerprints in a sparse format**  \n   * Compute fingerprints with a library that returns a CSR/COO sparse matrix (e.g.,\u202f*scikit\u2011fingerprints* or RDKit wrappers).  \n   * Store them as `scipy.sparse` objects so that zeros are never materialised, saving memory and keeping the true sparsity of the data\u202f([scikit\u2011fingerprints](https://arxiv.org/abs/2407.13291)).\n\n2. **Avoid vanilla PCA on raw bit vectors**  \n   * Standard PCA treats 0\u2011bits as informative, which distorts distances (it essentially reflects Hamming distance\u2019s square\u2011root and gives equal weight to shared absences)\u202f([Springer\u202f2014](https://link.springer.com/article/10.1007/s10822-014-9819-y)).  \n   * Instead use a *sparse\u2011aware* linear method such as **Truncated SVD / Sparse PCA** that operates directly on the sparse matrix.\n\n3. **Apply a similarity\u2011preserving kernel before reduction (optional)**  \n   * Compute a Tanimoto (Jaccard) similarity matrix from the sparse fingerprints.  \n   * Perform **kernel\u2011PCA** with the Tanimoto kernel; this respects the binary nature of the data and avoids the zero\u2011bias of ordinary PCA.\n\n4. **Use non\u2011linear embeddings that work with sparse distances**  \n   * Convert the sparse fingerprints to pairwise Tanimoto (or Hamming) distances and embed them with **UMAP** or **t\u2011SNE**.  \n   * Studies show that UMAP can improve the quality of ECFP\u2011based descriptors without discarding information\u202f([ML4Molecules\u202f2020](https://ml4molecules.github.io/papers2020/ML4Molecules_2020_paper_65.pdf)).\n\n5. **Leverage models that accept sparse inputs directly**  \n   * Train gradient\u2011boosted trees (e.g., XGBoost) or random forests on the original sparse fingerprint matrix; these traditional non\u2011deep models often outperform deep nets on sparse fingerprint data\u202f([SparseChem GitHub](https://github.com/melloddy/SparseChem)).  \n   * If you need a deep model, feed the sparse matrix into frameworks that support sparse tensors (e.g., PyTorch\u2019s `torch.sparse` or TensorFlow\u2019s `tf.sparse`) or first apply the sparse\u2011aware reduction from step\u202f2.\n\n6. **Validate that information is retained**  \n   * After reduction, compute the correlation between original Tanimoto similarities and distances in the reduced space; a high Spearman/Pearson coefficient indicates minimal loss.  \n   * If the correlation drops, revisit the number of components (for SVD) or the embedding hyper\u2011parameters (for UMAP).\n\nBy keeping fingerprints sparse, using sparse\u2011compatible linear reductions or similarity\u2011preserving kernels, and resorting to non\u2011linear embeddings when visualization is needed, you avoid the information loss that standard PCA introduces while still achieving tractable, lower\u2011dimensional representations for downstream machine\u2011learning models.",
      "url": ""
    },
    {
      "title": "A review of molecular representation in the age of machine learning",
      "text": "[WIREs Computational Molecular Science](https://wires.onlinelibrary.wiley.com/journal/17590884)\n\n[Volume 12, Issue 5](https://wires.onlinelibrary.wiley.com/toc/17590884/2022/12/5) e1603 [![WIREs Computational Molecular Science](https://wires.onlinelibrary.wiley.com/pb-assets/journal-banners/17590884-1529978610983.jpg)](https://wires.onlinelibrary.wiley.com/journal/17590884)\n\nAdvanced Review\n\nOpen Access\n\n# A review of molecular representation in the age of machine learning\n\n[Daniel S. Wigh](https://wires.onlinelibrary.wiley.com/authored-by/Wigh/Daniel+S.),\n\nDaniel S. Wigh\n\n- [orcid.org/0000-0002-0494-643X](https://orcid.org/0000-0002-0494-643X)\n\nDepartment of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK\n\nContribution: Conceptualization (lead), \u200bInvestigation (lead), Writing - original draft (lead)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Wigh/Daniel+S.)\n[Jonathan M. Goodman](https://wires.onlinelibrary.wiley.com/authored-by/Goodman/Jonathan+M.),\n\nJonathan M. Goodman\n\n- [orcid.org/0000-0002-8693-9136](https://orcid.org/0000-0002-8693-9136)\n\nYusuf Hamied Department of Chemistry, University of Cambridge, Cambridge, UK\n\nContribution: Supervision (equal), Writing - review & editing (equal)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Goodman/Jonathan+M.)\n[Alexei A. Lapkin](https://wires.onlinelibrary.wiley.com/authored-by/Lapkin/Alexei+A.),\n\nCorresponding Author\n\nAlexei A. Lapkin\n\n- [aal35@cam.ac.uk](mailto:aal35@cam.ac.uk)\n\n- [orcid.org/0000-0001-7621-0889](https://orcid.org/0000-0001-7621-0889)\n\nDepartment of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK\n\n**Correspondence**\n\nAlexei A. Lapkin, Department of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge CB3 0AS, UK.\n\nEmail: [aal35@cam.ac.uk](mailto:aal35@cam.ac.uk)\n\nContribution: Funding acquisition (lead), Project administration (lead), Supervision (equal), Writing - review & editing (supporting)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Lapkin/Alexei+A.)\n\n[Daniel S. Wigh](https://wires.onlinelibrary.wiley.com/authored-by/Wigh/Daniel+S.),\n\nDaniel S. Wigh\n\n- [orcid.org/0000-0002-0494-643X](https://orcid.org/0000-0002-0494-643X)\n\nDepartment of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK\n\nContribution: Conceptualization (lead), \u200bInvestigation (lead), Writing - original draft (lead)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Wigh/Daniel+S.)\n[Jonathan M. Goodman](https://wires.onlinelibrary.wiley.com/authored-by/Goodman/Jonathan+M.),\n\nJonathan M. Goodman\n\n- [orcid.org/0000-0002-8693-9136](https://orcid.org/0000-0002-8693-9136)\n\nYusuf Hamied Department of Chemistry, University of Cambridge, Cambridge, UK\n\nContribution: Supervision (equal), Writing - review & editing (equal)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Goodman/Jonathan+M.)\n[Alexei A. Lapkin](https://wires.onlinelibrary.wiley.com/authored-by/Lapkin/Alexei+A.),\n\nCorresponding Author\n\nAlexei A. Lapkin\n\n- [aal35@cam.ac.uk](mailto:aal35@cam.ac.uk)\n\n- [orcid.org/0000-0001-7621-0889](https://orcid.org/0000-0001-7621-0889)\n\nDepartment of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK\n\n**Correspondence**\n\nAlexei A. Lapkin, Department of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge CB3 0AS, UK.\n\nEmail: [aal35@cam.ac.uk](mailto:aal35@cam.ac.uk)\n\nContribution: Funding acquisition (lead), Project administration (lead), Supervision (equal), Writing - review & editing (supporting)\n\n[Search for more papers by this author](https://wires.onlinelibrary.wiley.com/authored-by/Lapkin/Alexei+A.)\n\nFirst published: 18 February 2022\n\n[https://doi.org/10.1002/wcms.1603](https://doi.org/10.1002/wcms.1603)\n\nCitations: [40](https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.1603#citedby-section)\n\n**Edited by:** Raghavan Sunoj, Associate Editor\n\n**Funding information:** Engineering and Physical Sciences Research Council, Grant/Award Number: EP/S024220/1; UCB\n\n## Abstract\n\nResearch in chemistry increasingly requires interdisciplinary work prompted by, among other things, advances in computing, machine learning, and artificial intelligence. Everyone working with molecules, whether chemist or not, needs an understanding of the representation of molecules in a machine-readable format, as this is central to computational chemistry. Four classes of representations are introduced: string, connection table, feature-based, and computer-learned representations. Three of the most significant representations are simplified molecular-input line-entry system (SMILES), International Chemical Identifier (InChI), and the MDL molfile, of which SMILES was the first to successfully be used in conjunction with a variational autoencoder (VAE) to yield a continuous representation of molecules. This is noteworthy because a continuous representation allows for efficient navigation of the immensely large chemical space of possible molecules. Since 2018, when the first model of this type was published, considerable effort has been put into developing novel and improved methodologies. Most, if not all, researchers in the community make their work easily accessible on GitHub, though discussion of computation time and domain of applicability is often overlooked. Herein, we present questions for consideration in future work which we believe will make chemical VAEs even more accessible.\n\nThis article is categorized under:\n\n- Data Science > Chemoinformatics\n\n## Graphical Abstract\n\nUnderstanding how to best represent molecules in a machine-readable format is a key challenge.\n\n[![Description unavailable](https://wires.onlinelibrary.wiley.com/cms/asset/e67e8279-e6bc-4da5-b14d-9a94efd4447b/wcms1603-toc-0001-m.png)](https://wires.onlinelibrary.wiley.com/cms/asset/1dbc3565-ec13-42ad-8461-f1d9caadb211/wcms1603-toc-0001-m.jpg)\n\n## 1 INTRODUCTION\n\nRepresenting chemical data in a concise and unambiguous way, understandable by both humans and machines, is not an easy task; this is particularly true for the representation of molecules. While there are numerous methods of adequately representing small and \u201csimple\u201d organic molecules, significant complexity may arise when considering molecules with features such as ring structures, nonstandard valency/bonding, inorganic components, or symmetry. These complexities may lead to issues such as representations being noncanonical (i.e., multiple different representations for the same molecule), being nonunique/clashing (i.e., multiple different molecules that are encoded into the same representation), assuming the wrong number of implicit hydrogen atoms, or failing to capture tautomerism. This can make (sub)structure searching in databases difficult, and even result in representations that refer to the wrong molecules. One way of elucidating the robustness of a representation is with a so-called \u201cround-trip conversion experiment,\u201d which tracks whether the conversion from representation to structure and back is correct for a given molecule. As an example, one could draw a molecule in ChemDraw, read it into ChemDoodle, and then check whether the same structure is obtained when reading the ChemDoodle file back into ChemDraw. Broadly speaking, molecules can be represented in a machine-readable format in four ways: as a string; with a connection table; as a collection of features, for example, a fingerprint or series of physical descriptors; or most recently, with a computer-learned representation using machine learning (ML).\n\nAs the problems that chemists tackle become increasingly complex, interdisciplinary collaboration also becomes more important, particularly with data scientists, with inherent greater ML understanding, and chemical engineers, syst...",
      "url": "https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.1603"
    },
    {
      "title": "GitHub - melloddy/SparseChem: Fast and accurate machine learning models for biochemical applications.",
      "text": "[Skip to content](https://github.com/melloddy/SparseChem#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/melloddy/SparseChem) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/melloddy/SparseChem) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/melloddy/SparseChem) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[melloddy](https://github.com/melloddy)/ **[SparseChem](https://github.com/melloddy/SparseChem)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmelloddy%2FSparseChem) You must be signed in to change notification settings\n- [Fork\\\n11](https://github.com/login?return_to=%2Fmelloddy%2FSparseChem)\n- [Star\\\n52](https://github.com/login?return_to=%2Fmelloddy%2FSparseChem)\n\n\nFast and accurate machine learning models for biochemical applications.\n\n### License\n\n[MIT license](https://github.com/melloddy/SparseChem/blob/master/LICENSE)\n\n[52\\\nstars](https://github.com/melloddy/SparseChem/stargazers) [11\\\nforks](https://github.com/melloddy/SparseChem/forks) [Branches](https://github.com/melloddy/SparseChem/branches) [Tags](https://github.com/melloddy/SparseChem/tags) [Activity](https://github.com/melloddy/SparseChem/activity)\n\n[Star](https://github.com/login?return_to=%2Fmelloddy%2FSparseChem)\n\n[Notifications](https://github.com/login?return_to=%2Fmelloddy%2FSparseChem) You must be signed in to change notification settings\n\n# melloddy/SparseChem\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/melloddy/SparseChem/branches) [Tags](https://github.com/melloddy/SparseChem/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[347 Commits](https://github.com/melloddy/SparseChem/commits/master/) |\n| [docs](https://github.com/melloddy/SparseChem/tree/master/docs) | [docs](https://github.com/melloddy/SparseChem/tree/master/docs) |  |  |\n| [examples/chembl](https://github.com/melloddy/SparseChem/tree/master/examples/chembl) | [examples/chembl](https://github.com/melloddy/SparseChem/tree/master/examples/chembl) |  |  |\n| [sparsechem](https://github.com/melloddy/SparseChem/tree/master/sparsechem) | [sparsechem](https://github.com/melloddy/SparseChem/tree/master/sparsechem) |  |  |\n| [.gitignore](https://github.com/melloddy/SparseChem/blob/master/.gitignore) | [.gitignore](https://github.com/melloddy/SparseChem/blob/master/.gitignore) |  |  |\n| [LICENSE](https://github.com/melloddy/SparseChem/blob/master/LICENSE) | [LICENSE](https://github.com/melloddy/SparseChem/blob/master/LICENSE) |  |  |\n| [README.md](https://github.com/melloddy/SparseChem/blob/master/README.md) | [README.md](https://github.com/melloddy/SparseChem/blob/master/README.md) |  |  |\n| [setup.py](https://github.com/melloddy/SparseChem/blob/master/setup.py) | [setup.py](https://github.com/melloddy/SparseChem/blob/master/setup.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Introduction\n\nThis package provide **fast** and **accurate** machine learning models for biochemical applications.\nEspecially, we support very high-dimensional models with sparse inputs, _e.g._, millions of features and millions of compounds.\n\n- The general documentation can be found [here](https://github.com/melloddy/SparseChem/blob/master/docs/main.md).\n- Documentation about how to retrain a pretrained model can be found [here](https://github.com/melloddy/SparseChem/blob/master/docs/local_trunk.md).\n- Documentation about how to profile GPU memory usage and use mixed precision can be found [here](https://github.com/melloddy/SparseChem/blob/master/docs/memprofile.md).\n- Documentation about how to use Catalogue Fusion can be found [here](https://github.com/melloddy/SparseChem/blob/master/docs/catalogue_fusion.md).\n\n# Reference\n\nIf you use this software in your work, please cite:\n\n```\n@article{arany2022sparsechem,\n  title={SparseChem: Fast and accurate machine learning model for small molecules},\n  author={Arany, Adam and Simm, Jaak and Oldenhof, Martijn and Moreau, Yves},\n  journal={arXiv preprint arXiv:2203.04676},\n  year={2022}\n}\n\n```\n\n## About\n\nFast and accurate machine learning models for biochemical applications.\n\n### Resources\n\n[Readme](https://github.com/melloddy/SparseChem#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/melloddy/SparseChem#MIT-1-ov-file)\n\n[Activity](https://github.com/melloddy/SparseChem/activity)\n\n[Custom properties](https://github.com/melloddy/SparseChem/custom-properties)\n\n### Stars\n\n[**52**\\\nstars](https://github.com/melloddy/SparseChem/stargazers)\n\n### Watchers\n\n[**8**\\\nwatching](https://github.com/melloddy/SparseChem/watchers)\n\n### Forks\n\n[**11**\\\nforks](https://github.com/melloddy/SparseChem/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmelloddy%2FSparseChem&report=melloddy+%28user%29)\n\n## [Releases](https://github.com/melloddy/SparseChem/releases)\n\n[27tags](https://github.com/melloddy/SparseChem/tags)\n\n## [Packages\\ 0](https://github.com/orgs/melloddy/packages?repo_name=SparseChem)\n\nNo packages published\n\n## [Contributors\\ 5](https://github.com/melloddy/SparseChem/graphs/contributors)\n\n## Languages\n\n- [Python100.0%](https://github.com/melloddy/SparseChem/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/melloddy/SparseChem"
    },
    {
      "title": "Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python",
      "text": "# Computer Science > Software Engineering\n\n**arXiv:2407.13291** (cs)\n\n\\[Submitted on 18 Jul 2024 ( [v1](https://arxiv.org/abs/2407.13291v1)), last revised 10 Aug 2025 (this version, v5)\\]\n\n# Title:Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python\n\nAuthors: [Jakub Adamczyk](https://arxiv.org/search/cs?searchtype=author&query=Adamczyk,+J), [Piotr Ludynia](https://arxiv.org/search/cs?searchtype=author&query=Ludynia,+P)\n\nView a PDF of the paper titled Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python, by Jakub Adamczyk and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2407.13291) [HTML (experimental)](https://arxiv.org/html/2407.13291v5)\n\n> Abstract:In this work, we present scikit-fingerprints, a Python package for computation of molecular fingerprints for applications in chemoinformatics. Our library offers an industry-standard scikit-learn interface, allowing intuitive usage and easy integration with machine learning pipelines. It is also highly optimized, featuring parallel computation that enables efficient processing of large molecular datasets. Currently, scikit-fingerprints stands as the most feature-rich library in the open source Python ecosystem, offering over 30 molecular fingerprints. Our library simplifies chemoinformatics tasks based on molecular fingerprints, including molecular property prediction and virtual screening. It is also flexible, highly efficient, and fully open source.\n\n|     |     |\n| --- | --- |\n| Subjects: | Software Engineering (cs.SE); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2407.13291](https://arxiv.org/abs/2407.13291) \\[cs.SE\\] |\n| (or [arXiv:2407.13291v5](https://arxiv.org/abs/2407.13291v5) \\[cs.SE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2407.13291](https://doi.org/10.48550/arXiv.2407.13291) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jakub Adamczyk \\[ [view email](https://arxiv.org/show-email/2d33a276/2407.13291)\\] **[\\[v1\\]](https://arxiv.org/abs/2407.13291v1)**\nThu, 18 Jul 2024 08:45:14 UTC (367 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2407.13291v2)**\nMon, 16 Sep 2024 12:34:52 UTC (106 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2407.13291v3)**\nThu, 24 Oct 2024 17:08:56 UTC (106 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2407.13291v4)**\nThu, 12 Dec 2024 13:35:22 UTC (106 KB)\n**\\[v5\\]**\nSun, 10 Aug 2025 11:05:55 UTC (107 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python, by Jakub Adamczyk and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2407.13291)\n- [HTML (experimental)](https://arxiv.org/html/2407.13291v5)\n- [TeX Source](https://arxiv.org/src/2407.13291)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.SE\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2407.13291&function=prev&context=cs.SE)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2407.13291&function=next&context=cs.SE)\n\n[new](https://arxiv.org/list/cs.SE/new) \\| [recent](https://arxiv.org/list/cs.SE/recent) \\| [2024-07](https://arxiv.org/list/cs.SE/2024-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2407.13291?context=cs) [cs.LG](https://arxiv.org/abs/2407.13291?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2407.13291)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2407.13291)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2407.13291)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2407.13291) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2407.13291"
    },
    {
      "title": "Euclidean chemical spaces from molecular fingerprints: Hamming distance and Hempel\u2019s ravens",
      "text": "<div><div>\n <header>\n <ul>\n \n \n <li>Special Series: Statistics in Molecular Modeling</li>\n \n \n \n <li><a href=\"#article-info\">Published: <time>05 December 2014</time></a></li>\n </ul>\n \n \n \n <p>\n \n <a href=\"/journal/10822\"><i>Journal of Computer-Aided Molecular Design</i></a>\n <b>volume\u00a029</b>,\u00a0pages 387\u2013395 (2015)<a href=\"#citeas\">Cite this article</a>\n </p>\n \n \n <div>\n <ul>\n \n <li>\n <p>952 Accesses</p>\n </li>\n \n \n <li>\n <p>13 Citations</p>\n </li>\n \n \n \n <li>\n <p>1 Altmetric</p>\n </li>\n \n \n <li>\n <p><a href=\"/article/10.1007/s10822-014-9819-y/metrics\">Metrics details</a></p>\n </li>\n </ul>\n </div>\n \n \n \n \n </header>\n </div><div>\n <div><h2>Abstract</h2><p>Molecules are often characterized by sparse binary fingerprints, where 1s represent the presence of substructures and 0s represent their absence. Fingerprints are especially useful for similarity calculations, such as database searching or clustering, generally measuring similarity as the Tanimoto coefficient. In other cases, such as visualization, design of experiments, or latent variable regression, a low-dimensional Euclidian \u201cchemical space\u201d is more useful, where proximity between points reflects chemical similarity. A temptation is to apply principal components analysis (PCA) directly to these fingerprints to obtain a low dimensional continuous chemical space. However, Gower has shown that distances from PCA on bit vectors are proportional to the square root of Hamming distance. Unlike Tanimoto similarity, Hamming similarity (HS) gives equal weight to shared 0s as to shared 1s, that is, HS gives as much weight to substructures that neither molecule contains, as to substructures which both molecules contain. Illustrative examples show that proximity in the corresponding chemical space reflects mainly similar size and complexity rather than shared chemical substructures. These spaces are ill-suited for visualizing and optimizing coverage of chemical space, or as latent variables for regression. A more suitable alternative is shown to be Multi-dimensional scaling on the Tanimoto distance matrix, which produces a space where proximity does reflect structural similarity.</p></div>\n \n \n \n \n \n \n \n \n \n <div>\n \n <h2>Access options</h2>\n <article>\n <h3>Buy single article</h3>\n <div>\n <p>Instant access to the full article PDF.</p>\n <div>\n <p>USD 39.95</p>\n <p>Price excludes VAT (USA)<br />Tax calculation will be finalised during checkout.</p>\n \n </div>\n </div>\n \n </article>\n \n \n \n \n</div>\n \n \n \n \n \n \n \n <div><h2>References</h2><div><ol><li><p>Distances similarity measures for binary data. <a href=\"http://pic.dhe.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fcmd_proximities_sim_measure_binary.htm\">http://pic.dhe.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fcmd_proximities_sim_measure_binary.htm</a>. Accessed 17 June 2014</p></li><li><p>Johnston JW (2014) Similarity indices I: what do they measure. Battelle Pacific Northwest Labs., Richland, Washington. <a href=\"http://www.iaea.org/inis/collection/NCLCollectionStore/_Public/08/337/8337829.pdf\">http://www.iaea.org/inis/collection/NCLCollectionStore/_Public/08/337/8337829.pdf</a>. Accessed 17 July 2014</p></li><li><p>Seung-Seok Choi S-HC, Charles C. Tappert (2014) A survey of binary similarity and distance measures. Department of computer science, Pace University, New York. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.352.6123&amp;rep=rep1&amp;type=pdf\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.352.6123&amp;rep=rep1&amp;type=pdf</a>. Accessed 25 July 2014</p></li><li><p>Rogers DJ, Tanimoto TT (1960) A computer program for classifying plants. Science 132(3434):1115\u20131118</p><p><a href=\"https://doi.org/10.1126%2Fscience.132.3434.1115\">Article</a>\u00a0\n <a href=\"/articles/cas-redirect/1:STN:280:DC%2BC3cvgvFyntw%3D%3D\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20computer%20program%20for%20classifying%20plants&amp;journal=Science&amp;doi=10.1126%2Fscience.132.3434.1115&amp;volume=132&amp;issue=3434&amp;pages=1115-1118&amp;publication_year=1960&amp;author=Rogers%2CDJ&amp;author=Tanimoto%2CTT\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Martin EJ, Blaney JM, Siani MA, Spellmeyer DC, Wong AK, Moos WH (1995) Measuring diversity: experimental design of combinatorial libraries for drug discovery. J Med Chem 38(9):1431\u20131436. doi:<a href=\"https://doi.org/10.1021/jm00009a003\">10.1021/jm00009a003</a>\n </p><p><a href=\"https://doi.org/10.1021%2Fjm00009a003\">Article</a>\u00a0\n <a href=\"/articles/cas-redirect/1:CAS:528:DyaK2MXltVygsrs%3D\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20diversity%3A%20experimental%20design%20of%20combinatorial%20libraries%20for%20drug%20discovery&amp;journal=J%20Med%20Chem&amp;doi=10.1021%2Fjm00009a003&amp;volume=38&amp;issue=9&amp;pages=1431-1436&amp;publication_year=1995&amp;author=Martin%2CEJ&amp;author=Blaney%2CJM&amp;author=Siani%2CMA&amp;author=Spellmeyer%2CDC&amp;author=Wong%2CAK&amp;author=Moos%2CWH\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Martin EJ, Critchlow RE (1999) Beyond mere diversity: tailoring combinatorial libraries for drug discovery. J Comb Chem 1(1):32\u201345. doi:<a href=\"https://doi.org/10.1021/CC9800024\">10.1021/CC9800024</a>\n </p><p><a href=\"https://doi.org/10.1021%2Fcc9800024\">Article</a>\u00a0\n <a href=\"/articles/cas-redirect/1:CAS:528:DyaK1cXnvFGrurs%3D\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Beyond%20mere%20diversity%3A%20tailoring%20combinatorial%20libraries%20for%20drug%20discovery&amp;journal=J%20Comb%20Chem&amp;doi=10.1021%2FCC9800024&amp;volume=1&amp;issue=1&amp;pages=32-45&amp;publication_year=1999&amp;author=Martin%2CEJ&amp;author=Critchlow%2CRE\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Gower JC (1966) Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika 53(3\u20134):325\u2013338</p><p><a href=\"https://doi.org/10.1093%2Fbiomet%2F53.3-4.325\">Article</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Some%20distance%20properties%20of%20latent%20root%20and%20vector%20methods%20used%20in%20multivariate%20analysis&amp;journal=Biometrika&amp;doi=10.1093%2Fbiomet%2F53.3-4.325&amp;volume=53&amp;issue=3%E2%80%934&amp;pages=325-338&amp;publication_year=1966&amp;author=Gower%2CJC\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Young FW (1985) Multidimensional scaling. John wiley &amp; Sons, Inc. <a href=\"http://forrest.psych.unc.edu/teaching/p208a/mds/mds.html\">http://forrest.psych.unc.edu/teaching/p208a/mds/mds.html</a>. Accessed 15 July 2014</p></li><li><p>Todeschini R, Consonni V, Xiang H, Holliday J, Buscema M, Willett P (2012) Similarity coefficients for binary chemoinformatics data: overview and extended comparison using simulated and real data sets. J Chem Inf Model 52(11):2884\u20132901</p><p><a href=\"https://doi.org/10.1021%2Fci300261r\">Article</a>\u00a0\n <a href=\"/articles/cas-redirect/1:CAS:528:DC%2BC38XhsFamt7zP\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Similarity%20coefficients%20for%20binary%20chemoinformatics%20data%3A%20overview%20and%20extended%20comparison%20using%20simulated%20and%20real%20data%20sets&amp;journal=J%20Chem%20Inf%20Model&amp;doi=10.1021%2Fci300261r&amp;volume=52&amp;issue=11&amp;pages=2884-2901&amp;publication_year=2012&amp;author=Todeschini%2CR&amp;author=Consonni%2CV&amp;author=Xiang%2CH&amp;author=Holliday%2CJ&amp;author=Buscema%2CM&amp;author=Willett%2CP\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Lounkine E, Kutchukian P, Petrone P, Davies JW, Glick M (2012) Chemotography for multi-target SAR analysis in the context of biological pathways. Bioorg Med Chem 20(18):5416\u20135427. doi:<a href=\"https://doi.org/10.1016/j.bmc.2012.02.034\">10.1016/j.bmc.2012.02.034</a>\n </p><p><a href=\"https://doi.org/10.1016%2Fj.bmc.2012.02.034\">Article</a>\u00a0\n <a href=\"/articles/cas-redirect/1:CAS:528:DC%2BC38XjsFOitrg%3D\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com...",
      "url": "https://link.springer.com/article/10.1007/s10822-014-9819-y"
    },
    {
      "title": "",
      "text": "Evaluating chemical descriptors with the loss-data\nframework\nWalid Ahmad\nReverie Labs\nwalid@reverielabs.com\nAbstract\nSelecting chemical representations for machine learning models is a challenging\ntask, one which is subject to trial and error. Extended connectivity fingerprints\n(ECFPs), for example, are a classic featurization technique used widely in the\nprediction of properties of molecules, which effectively encodes molecular sub\u0002structures in a bit vector. However, it is not obvious when to use vanilla ECFPs\ninstead of alternative featurizations. We propose using recent progress in the field of\nrepresentation learning to evaluate and improve the quality of chemical descriptors.\nSpecifically, we show that using the loss-data framework with surplus description\nlength and \u03b5 sample complexity can provide insight into which descriptors are ap\u0002propriate for specific tasks. We find that applying simple dimensionality reduction\ntechniques such as UMAP and PCA can improve the quality of ECFP descriptors\nfor certain datasets.\n1 Introduction\nMachine learning (ML) approaches have seen a recent surge in cheminformatics because of their\nutility throughout different stages of the drug discovery pipeline. In particular, supervised machine\nlearning has shown promise for predicting molecular characteristics, such as ADMET properties and\nbinding affinities [11]. However, developing effective ML approaches for supervised tasks requires\nnot only a good model, but also a good representation, or featurization, of chemical compounds.\nThe quality of the featurization affects how well, and how quickly, a machine learning model can\nlearn from the data. Which representation to use, and when, is an open question, particularly in\nsmall molecule design problems, where a wide variety of representations are available [15]. In this\nwork, we investigate the effectiveness of common, one-dimensional featurizations of molecules and\ndemonstrate how to assess different descriptors.\n1.1 Molecular descriptors\nMolecular descriptors are operations that transform a symbolic representation of a molecule into a\nvectorized representation. The vectorized representations can be used as inputs to algorithms, such\nas ML models, to assist in a variety of downstream tasks, including modeling quantitative structure\nactivity relationships (QSAR) for virtual screens [3, 7, 10].\nSimple computed properties A basic class of molecular descriptors includes those constructed\nusing predicted physiochemical properties or experimental measurements. Popular measurements\nused in these representations include molecular weight, logP, number of hydrogen bond donors,\nnumber of hydrogen bond acceptors, polar surface area, and more. The values of interest can\nbe concatenated into a vector and used as a molecular representation. Models trained with such\nrepresentations may be largely invariant to the underlying molecular structures.\nMachine Learning for Molecules Workshop at NeurIPS 2020. https://ml4molecules.github.io\nExtended connectivity fingerprints Extended connectivity fingerprints (ECFPs) are one of the\nmost widely used molecular descriptors. ECFPs are circular fingerprints that utilize a variant of the\nMorgan algorithm [13] to quantize neighborhoods around individual atoms to detect the presence of\nsubstructures. The molecule is represented as a bit-vector where on-bits indicate the presence of a\nparticular substructure. ECFPs can contain varying levels of granularity, based on the number of bits\nthat are used [14].\nML-based descriptors ML models can be used to construct chemical descriptors by extracting the\noutputs at a given layer in the model. Encoder-decoders are the canonical architecture for constructing\ndescriptors in this way. An encoder network compresses an input representation of a molecule into a\nlatent vector, from which a decoder reconstructs a molecular representation. The input and output\nrepresentation can be the same (e.g. SMILES strings [6]) or different (e.g. SMILES to InChI [17]).\n1.2 Dimensionality reduction techniques\nPrincipal component analysis (PCA) PCA [9] is a linear dimensionality reduction technique\nwhich projects data into into a lower-dimensional space using singular value decomposition (SVD).\nPrincipal components are ordered eigenvectors of the covariance matrix, and thus maximize the\nvariance of the projected data.\nUniform manifold approximation and projection (UMAP) UMAP [12] is a relatively recent\ndimensionality reduction technique which claims to preserve both the local and global structure of the\ndata. It has received attention in bioinformatics, for example, for its utility in visualizing single-cell\ndata [2].\n1.3 Evaluating representations\nThe increased interest in representation and self-supervised learning in the machine learning commu\u0002nity has given rise to a family of techniques for analyzing the quality of representations. Notions of\nrobustness for representations normally entail evaluating downstream performance on some relevant\ntask, using simple models that are quick to train \u2013 these models are sometimes referred to as \u201cprobes\"\nin the literature. Probes can be linear [1, 5] or nonlinear [4].\nNonetheless, the question of representation evaluation is complicated. A particular representation\nmay work reasonably well in certain data regimes, and for certain tasks, but not for others. In\nWhitney et al. [16], the authors provide an overview of current state-of-the-art methods for evaluating\nrepresentations, and identify the loss-data framework as a useful analytical tool. The loss-data\ncurve, which plots validation loss versus training set size, elucidates how well probes can learn with\ndifferent numbers of examples. This is in contrast to a traditional loss-curve, which holds the number\nof examples static. Whitney et al. [16] also propose two new, robust methods for representation\nevaluation based on the loss-data curve, which we use here.\nSurplus description length Surplus description length (SDL) is a measure of the extra entropy\nneeded to encode data from a data generating distribution D using a representation \u03c6. On a dataset\nwith i points, the SDL is\nmSDL(\u03c6, D, A) = X\nN\ni=1\n[L(A\u03c6, i) \u2212 \u03b5]+, (1)\nwhere A is the probe algorithm, L is the expected loss, and \u03b5 is a success criterion, i.e. a loss tolerance\nfor which a model is considered successful at the task.\n\u000f sample complexity (SC) \u000fSC measures the smallest number of samples needed for a probe A to\nachieve a loss value of \u03b5 on the dataset,\nm\u000fSC(\u03c6, D, A) = min{n \u2208 N : L(A\u03c6, n) \u2264 \u03b5}. (2)\nBoth of these methods involve selecting a loss threshold, \u03b5, which corresponds to a line on the y-axis\nof the loss-data curve (see Fig. 1a or 2a for an example). \u03b5SC measures the number of data points\nneeded to obtain \u03b5 loss, and SDL integrates the loss curve above \u03b5.\n2\n2 Experiments\nWe conduct an expository analysis of chemical descriptors using SDL and \u03b5SC, by considering a\nsubset of the MoleculeNet [18] benchmark datasets: the Tox21 and Lipophilicity datasets. For each\ntask in each dataset, we sample n \u2264 N datapoints, where n is chosen linearly from [0, N]. Every\nsample is used to train a nonlinear probe with a 90/10 training/validation split. The probe is a simple\nfeed-forward neural network with 2 hidden layers of size 512 each. The validation loss at each\nsampled n is used to compute SDL and \u03b5SC. We repeat the process with 4 different random seeds.\nTwo \u03b5 values are selected by taking 1.5x and 2.0x the lowest loss value found (lmin) for each set of\ndescriptors, to serve as proxies for success criteria.\nThe descriptors evaluated include ECFP descriptors of varying lengths: 1024, 2048, and 4096. For\neach length, we also fit and apply PCA and UMAP to project the fingerprints into a space of dimension\nd \u2208 {2, 16, 128}. Lastly, we also evaluate Continuous Data-Driven Descriptors from Winter et al.\n[17], which are 512-dimensional latent vectors from a SMILES autoencoder architecture, trained on\nZINC12 [8]. For brevity, the results sho...",
      "url": "https://ml4molecules.github.io/papers2020/ML4Molecules_2020_paper_65.pdf"
    },
    {
      "title": "Exposing the Limitations of Molecular Machine Learning with Activity ...",
      "text": "![](https://d.adroll.com/cm/b/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/g/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/index/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/n/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/o/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/outbrain/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/pubmatic/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/r/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/taboola/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/triplelift/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/x/out?adroll_fpc=58256a42f52e68410f6040c82cb77d6b-1720038131478&pv=59488804209.731705&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2F10.1021%2Facs.jcim.2c01073&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)\n\nRecently Viewed [close modal](javascript:void(0))\n\nRecently Viewed\n\n- [Photochemical Studies of CH3C(O)OONO2 (PAN) and CH3CH2C(O)OONO2 (PPN):\u2009 NO3 Quantum Yields](https://pubs.acs.org/doi/full/10.1021/jp0264230)\n- [Structural properties of apocytochrome b5: presence of a stable native core](https://pubs.acs.org/doi/full/10.1021/bi00460a004)\n- [Stereoselective Synthesis of Tilivalline1](https://pubs.acs.org/doi/full/10.1021/jo972158g)\n- [Transition States for Psychrophilic and Mesophilic (R)-3-Hydroxybutyrate Dehydrogenase-Catalyzed Hydride Transfer at Sub-zero Temperatures](https://pubs.acs.org/doi/full/10.1021/acs.biochem.1c00322)\n- [Sesquiterpene Lactones as Allelochemicals](https://pubs.acs.org/doi/full/10.1021/np060056s)\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nYou\u2019ve supercharged your research process with ACS and Mendeley!\n\nContinue\n\n###### STEP 1:\n\nLogin with ACS IDLogged in SuccessClick to create an ACS ID\n\n###### STEP 2:\n\nLogin with MendeleyLogged in Success [Create a Mendeley account](https://id.elsevier.com/as/authorization.oauth2?state=c33c27125763433d4d32a15accaacc18&prompt=login&scope=openid%20email%20profile%20els_auth_info&authType=SINGLE_SIGN_IN&response_type=code&platSite=MDY%2Fmendeley&redirect_uri=https%3A%2F%2Fwww.mendeley.com%2Fcallback%2F&client_id=MENDELEY)\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease login with your ACS ID before connecting to your Mendeley account.\n\nLogin with ACS ID\n\nMENDELEY PAIRING EXPIREDReconnect\n\nYour Mendeley pairing has expired. Please reconnect\n\n![Figure 1](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073)![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n\n[Download Hi-Res Image](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073) [Download to MS-PowerPoint](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073) [**Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acs.jcim.2c01073&href=/doi/10.1021/acs.jcim.2c01073) _J. Chem. Inf. Model._ 2022, 62, 23, 5938-5951\n\n[ADVERTISEMENT](http://acsmediakit.org)\n\n[RETURN TO ISSUE](https://pubs.acs.org/toc/jcisd8/62/23) [PREV](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01060) Machine Learning and...Machine Learning and Deep Learning [NEXT](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01085)\n\n[ADDITION / CORRECTIONThis article has been corrected. View the notice.](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00423)\n\n[ADDITION / CORRECTIONThis article has been corrected. View the notice.](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01576)\n\n[![Journal Logo](https://pubs.acs.org/doi/10.1021/specs/products/achs/releasedAssets/images/loading/loader-128b5db1cc3a83761a15cf2e5c9b452d.gif)](https://pubs.acs.org/journal/jcisd8)\n\n[Get e-Alerts](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073) close\n\n# Exposing the Limitations of Molecular Machine Learning with Activity Cliffs\n\n- Derek van Tilborg\n\n\n\n\nDerek van Tilborg\n\n\n\n\n\nInstitute for Complex Molecular Systems and Dept. Biomedical Engineering, Eindhoven University of Technology, 5612AZEindhoven, The Netherlands\n\n\n\nCentre for Living Technologies, Alliance TU/e, WUR, UU, UMC Utrecht, 3584CBUtrecht, The Netherlands\n\n\n\n\n\nMore by [Derek van Tilborg](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Derek++van+Tilborg)\n\n- ,\n- Alisa Alenicheva\n\n\n\n\nAlisa Alenicheva\n\n\n\n\n\nJetBrains Research, 194100Saint Petersburg, Russia\n\n\n\n\n\nMore by [Alisa Alenicheva](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Alisa++Alenicheva)\n\n- ,\u00a0and\n- Francesca Grisoni **\\***\n\n\n\n\nFrancesca Grisoni\n\n\n\n\n\nInstitute for Complex Molecular Systems and Dept. Biomedical Engineering, Eindhoven University of Technology, 5612AZEindhoven, The Netherlands\n\n\n\nCentre for Living Technologies, Alliance TU/e, WUR, UU, UMC Utrecht, 3584CBUtrecht, The Netherlands\n\n\n\n**\\*** Email: [f.grisoni@tue.nl](mailto:f.grisoni@tue.nl)\n\nMore by [Francesca Grisoni](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Francesca++Grisoni)\n\n\n\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0001-8552-6615](https://orcid.org/0000-0001-8552-6615)\n\n\n[**Cite this:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021%2Facs.jcim.2c01073&href=/doi/10.1021%2Facs.jcim.2c01073) _J. Chem. Inf. Model._2022, 62, 23, 5938\u20135951\n\nPublication Date (Web):December 1, 2022\n\n#### Publication History\n\n- **Received**23 August 2022\n- **Published** online1 December 2022\n- **Published** inissue 12 December 2022\n\n[https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073](https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073)\n\n[https://doi.org/10.1021/acs.jcim.2c01073](https://doi.org/10.1021/acs.jcim.2c01073)\n\nresearch-article\n\nACS Publications**Copyright \u00a9 2022 The Authors. Published by American Chemical Society**. This publication is licensed under\n\n[CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n\n### License Summary\\*\n\nYou are free to share (copy and redistribute) this article in any medium or format and to ...",
      "url": "https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073"
    },
    {
      "title": "Multi-Task Neural Networks and Molecular Fingerprints to Enhance ...",
      "text": "Multi-Task Neural Networks and Molecular Fingerprints to Enhance Compound Identification from LC-MS&#x2F;MS Data | MDPI\nYou are currently viewing a new version of our website. To view the old version clickhere.\nClose\n[![MDPI](https://www.mdpi.com/logo.svg)![MDPI](https://www.mdpi.com/logo.svg)](https://www.mdpi.com/)\nSearch\n[](https://www.mdpi.com/search)Search\n[](https://www.mdpi.com/search)Search all articles\nOpen mobile navigation\n[![](https://www.mdpi.com/_ipx/b_%23fff&amp;f_webp&amp;fit_cover&amp;s_32x32/https://mdpi-res.com/img/journals/molecules-logo-sq.png%3F1f5b1782226f7a70)Molecules](https://www.mdpi.com/journal/molecules)\nOpen CiteCite\nOpen ShareShare\n[Download PDF](https://www.mdpi.com/1420-3049/27/18/5827/pdf)\nMore formats\n[Abstract](#Abstract)[Introduction](#Introduction)[Materials and Methods](#Materials_and_Methods)[Results](#Results)[Conclusions](#Conclusions)[Author Contributions](#Author_Contributions)[Funding](#Funding)[Institutional Review Board Statement](#Institutional_Review_Board_Statement)[Informed Consent Statement](#Informed_Consent_Statement)[Data Availability Statement](#Data_Availability_Statement)[Acknowledgments](#Acknowledgments)[Conflicts of Interest](#Conflicts_of_Interest)[Sample Availability](#Sample_Availability)[References](#References)[Article Metrics](#Article_Metrics)\n* Feature Paper\n* Article\n* ![Open Access](https://www.mdpi.com/_ipx/s_14x14/https://mdpi-res.com/data/open-access.svg)\n8 September 2022\n# Multi-Task Neural Networks and Molecular Fingerprints to Enhance Compound Identification from LC-MS/MS Data\nViviana Consonni,\nFabio Gosetti,\nVeronica Termopoli,\nRoberto Todeschini,\nCecile Valsecchiand\nDavide Ballabio\\*\nDepartment of Earth and Environmental Sciences, University of Milano-Bicocca, Piazza della Scienza 1, 20126 Milano, Italy\n\\*\nAuthor to whom correspondence should be addressed.\n[Molecules](https://www.mdpi.com/journal/molecules)**2022**,*27*(18), 5827;[https://doi.org/10.3390/molecules27185827](https://doi.org/10.3390/molecules27185827)\nThis article belongs to the Special Issue[Chemometrics in Analytical Chemistry](https://www.mdpi.com/journal/molecules/special_issues/chemometrics_analytical)\n[Version Notes](https://www.mdpi.com/1420-3049/27/18/5827/notes)\n[Order Reprints](https://www.mdpi.com/1420-3049/27/18/5827/reprints)\n[Review Reports](https://www.mdpi.com/1420-3049/27/18/5827/review_report)\n## Abstract\nMass spectrometry (MS) is widely used for the identification of chemical compounds by matching the experimentally acquired mass spectrum against a database of reference spectra. However, this approach suffers from a limited coverage of the existing databases causing a failure in the identification of a compound not present in the database. Among the computational approaches for mining metabolite structures based on MS data, one option is to predict molecular fingerprints from the mass spectra by means of chemometric strategies and then use them to screen compound libraries. This can be carried out by calibrating multi-task artificial neural networks from large datasets of mass spectra, used as inputs, and molecular fingerprints as outputs. In this study, we prepared a large LC-MS/MS dataset from an on-line open repository. These data were used to train and evaluate deep-learning-based approaches to predict molecular fingerprints and retrieve the structure of unknown compounds from their LC-MS/MS spectra. Effects of data sparseness and the impact of different strategies of data curing and dimensionality reduction on the output accuracy have been evaluated. Moreover, extensive diagnostics have been carried out to evaluate modelling advantages and drawbacks as a function of the explored chemical space.\nKeywords:\n[LC-MS/MS](https://www.mdpi.com/search?q=LC-MS/MS);[chemometrics](https://www.mdpi.com/search?q=chemometrics);[fingerprints](https://www.mdpi.com/search?q=fingerprints);[similarity matching](https://www.mdpi.com/search?q=similarity+matching);[classification](https://www.mdpi.com/search?q=classification);[neural networks](https://www.mdpi.com/search?q=neural+networks);[multi-task](https://www.mdpi.com/search?q=multi-task)\n## 1. Introduction\nMass spectrometry (MS) is a commonly used detection analytical technique for the identification of compounds in food, environmental, biological and forensic samples [1,2,3]. It is generally coupled with liquid chromatography (LC) or gas chromatography (GC), depending on the type of sample to be analysed. GC-MS uses the most popular hard-ionisation technique (Electron Ionisation, EI), which generates a unique MS spectrum characterised by extensive fragmentation: this is specific for the target compound and useful for its structure elucidation [4]. Since the EI source works in high-vacuum conditions, the EI-based MS spectrum is independent of the gas chromatographic conditions and represents a chemical identifier of the investigated molecule with high accuracy. Conversely, in LC-MS the ionisation process occurs at atmospheric pressure conditions, and this leads to MS spectra that cannot unequivocally characterise the compound, since the acquisition of these spectra depends on several experimental factors, such as source type and mechanism of ionisation, organic solvent, potential matrix effects, presence of additives and their concentration in the mobile phase [5]. In tandem mass spectrometry (MS/MS), a precursor ion is fragmented in a collision cell to generate a product ion spectrum (MS/MS spectrum) to get measurements which are more independent of the chromatographic conditions [6]. Sometimes, these second-level ions can be fragmented even further, giving MS3 spectra and so on. Ad hoc libraries for the recognition of MS/MS spectra can be found on the market. However, the obtained MS/MS spectra are still dependent on the collision gas, collision process, and collision energy involved in the fragmentation process. Therefore, a plethora of different MS spectra can be related to the same compound, from little or no fragmentation, in which the precursor ion is still present, to highly fragmented spectra [7].\nSmall molecules (below 1500 Da), which form as intermediates and products of all chemical reactions within cells of living organisms, are called metabolites. Metabolites cover a wide range of compound classes and their structural diversity is very large, despite their small size. In metabolomics, the high-throughput characterisation of metabolites present in a biological sample is increasingly important across biomedical and life sciences [8]. A commonly employed analytical platform for metabolomic studies includes LC-MS, due to its ability to analyse a sizable number of metabolites with a limited amount of biological material compared to other platforms.\nIdentification of chemical compounds through MS/MS spectra is thus a prerequisite for further data interpretation and it is probably the most time-consuming and laborious step in metabolomics experiments [9,10,11,12]. Metabolite identification requires matching the observed spectrum against a database of reference spectra originating from similar equipment and closely matching operating parameters, a condition that is rarely satisfied in public repositories. Furthermore, the computational support for identification of metabolites not present in reference databases is lacking [13]. The most common routine method implies spectral matching. In particular, it calculates the similarities between the spectrum of an unknown compound and the spectra of standards in the database and the structure of the standard with the highest similarity is predicted as the structure of the unknown. Though widely used, this approach suffers from a coverage problem: if an unknown compound is not in the database, it can never be identified [4]. Despite the intense ongoing efforts to map the metabolome of various organisms, existing databases cover only a small percentage of the actual metabolites that occur in organisms. I...",
      "url": "https://www.mdpi.com/1420-3049/27/18/5827"
    },
    {
      "title": "[PDF] Understanding the Limitations of Deep Models for Molecular ...",
      "text": "Understanding the Limitations of Deep Models for\nMolecular property prediction: Insights and Solutions\nJun Xia\u2217, Lecheng Zhang\u2217, Xiao Zhu\u2217, Yue Liu, Zhangyang Gao,\nBozhen Hu, Cheng Tan, Jiangbin Zheng, Siyuan Li, Stan Z. Li\u2020\nSchool of Engineering, Westlake University\n{xiajun, zhanglecheng, stan.zq.li}@westlake.edu.cn\nAbstract\nMolecular Property Prediction (MPP) is a crucial task in the AI-driven Drug\nDiscovery (AIDD) pipeline, which has recently gained considerable attention\nthanks to advancements in deep learning. However, recent research has revealed\nthat deep models struggle to beat traditional non-deep ones on MPP. In this study,\nwe benchmark 12 representative models (3 non-deep models and 9 deep models)\non 15 molecule datasets. Through the most comprehensive study to date, we\nmake the following key observations: (i) Deep models are generally unable to\noutperform non-deep ones; (ii) The failure of deep models on MPP cannot be\nsolely attributed to the small size of molecular datasets; (iii) In particular, some\ntraditional models including XGB and RF that use molecular fingerprints as inputs\ntend to perform better than other competitors. Furthermore, we conduct extensive\nempirical investigations into the unique patterns of molecule data and inductive\nbiases of various models underlying these phenomena. These findings stimulate us\nto develop a simple-yet-effective feature mapping method for molecule data prior\nto feeding them into deep models. Empirically, deep models equipped with this\nmapping method can beat non-deep ones in most MoleculeNet datasets. Notably,\nthe effectiveness is further corroborated by extensive experiments on cutting-edge\ndataset related to COVID-19 and activity cliff datasets.\n1 Introduction\nMolecular Property Prediction (MPP) is a critical task in computational drug discovery, aimed\nat identifying molecules with desirable pharmacological and ADMET (absorption, distribution,\nmetabolism, excretion, and toxicity) properties. Machine learning models have been widely used\nin this fast-growing field, with two types of models being commonly employed: traditional non\u0002deep models and deep models. In non-deep models, molecules are fed into traditional machine\nlearning models such as random forest and support vector machines in the format of computed or\nhandcrafted molecular fingerprints [64]. The other group utilizes deep models to extract expressive\nrepresentations for molecules in a data-driven manner. Specifically, the Multi-Layer Perceptron\n(MLP) could be applied to computed or handcrafted molecular fingerprints; Sequence-based neural\narchitectures including Recurrent Neural Networks (RNNs) [43], 1D Convolutional Neural Networks\n(1D CNNs) [22], and Transformers [25, 54] are exploited to encode molecules represented in\nSimplified Molecular-Input Line-Entry System (SMILES) strings [71]. Additionally, molecules\ncan be naturally represented as graphs with atoms as nodes and bonds as edges, inspiring a line of\nworks to leverage such structured inductive bias for better molecular representations [20, 76, 79, 58].\nThe key advancements underneath these approaches are Graph Neural Networks (GNNs), which\n\u2217Equal Contribution.\n\u2020Corresponding author\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nNc1ccc(cc1)C(=O)O\n(b) SMILES string\nCNN RNN TRSF\n(a) Fingerprints (FPs)\nSVM RF XGB MLP\n1 0 1 0 0 0 1 0 0\n(c) 2D Graph (d) 3D Graph\nGCN MPNN GAT AFP\n7 0 1 0 0\nAtoms features\nBonds features\n1 0 3\nModels\nDescriptors\nSPN\nFigure 1: Exemplary molecular descriptors and their corresponding models in our benchmark. SVM:\nSupport Vector Machine [84]; RF: Random Forest [61]; XGB: eXtreme Gradient Boosting [8];\nMLP: Multi-Layer Perceptron; CNN: 1D Convolution Neural Network [32]; RNN: Recurrent Neural\nNetwork (GRU) [46]; TRSF: TRanSFormer [67]; GCN: Graph Convolution Network [33]; MPNN:\nMessage-Passing Neural Network [20]; GAT: Graph Attention neTwork [68]; AFP: Attentive FP [77];\nSPN: SPhereNet [38]. The above-mentioned abbreviations are applicable throughout the entire paper.\nconsider graph structures and attributive features simultaneously in the learning process [33, 68, 24].\nMore recently, researchers incorporate 3D conformations of molecules into their representations for\nbetter performance, whereas pragmatic considerations such as calculation cost, alignment invariance,\nuncertainty in conformation generation, and unavailable conformations of target molecules limited the\npractical applicability of these models [5, 17, 57, 16, 38]. We summarize the widely-used molecular\ndescriptors and their corresponding models in our benchmark, as shown in Figure 1. Despite the\nfruitful progress, previous studies [41, 29, 79, 65, 30, 13, 66] have observed that deep models\nstruggled to outperform non-deep ones on molecular datasets. However, these studies neither consider\nthe emerging powerful deep models (e.g., Transformer [25], SphereNet [37]) nor explore various\nmolecular descriptors (e.g., 3D molecular graph). Also, they did not investigate the reasons why deep\nmodels often fail on molecules.\nTo narrow this gap, we present the most comprehensive benchmark study on molecular property\nprediction to date, with a precise methodology for dataset inclusion and hyperparameter tuning. Our\nempirical results confirm the observations of previous studies, namely that deep models generally\nstruggle to outperform traditional non-deep counterparts, even without accounting for the slower\ntraining of deep learning algorithms. Moreover, we observe several interesting phenomena that\nchallenge the prevailing beliefs of the community, which can guide optimal methodology design for\nfuture studies.\nFurthermore, we aim to understand why deep models often underperform non-deep ones in MPP.\nSpecifically, we transform the original molecular data to observe the performance changes of various\nmodels, uncovering the unique patterns of molecular data and the differing inductive biases of various\nmodels. These in-depth empirical studies shed light on the benchmarking results: Deep models\nstruggle to learn non-smooth target functions that map molecular data to labels, while the target\nfunctions are often non-smooth in MPP. This means that small changes in the chemical structure of\na molecule may result in large changes in molecular properties. Additionally, deep models tend to\nattend to molecule features as a whole, especially handling the molecular fingerprints, while partial\nsubstructures known as functional groups are the most informative for molecules. On the other hand,\nXGB and random forest are well-suited for molecules because they make decisions based on each\ndimension of molecular features separately. Based on these phenomena and analyses, we develop a\nnovel feature mapping method for molecule data before feeding them into models. Theoretically, we\nshow that our method can help deep models learn non-smooth target functions that map molecules to\nproperties. Moreover, our method is readily pluggable into various deep methods for performance\nimprovement.\nWe highlight the following contributions: (I) We provide the most comprehensive benchmark on\nMPP tasks to date and expose the limitations of deep models on molecule datasets. Our findings offer\nnew and valuable insights for the fast-growing AIDD community. (II) We empirically investigate the\nunique patterns of molecular data and inductive biases of various models, providing explanations\nfor why deep models often cannot beat non-deep ones on MPP tasks. (III) We develop a simple-yet\u0002effective feature mapping method to help deep models learn the non-smooth target functions with\ntheoretical guarantees. (IV) We verify the effectiveness of our method through extensive experiments\non MoleculeNet datasets, a cutting-edge dataset related to COVID-19 and activity cliff datasets.\n2\n2 Related Work\nIn this section, we elaborate on various molecular descriptors and their respective learning models.\n2.1 Fingerprints-based Molecular Desc...",
      "url": "https://chengtan9907.github.io/assets/publications/nips23_ifm.pdf"
    }
  ]
}