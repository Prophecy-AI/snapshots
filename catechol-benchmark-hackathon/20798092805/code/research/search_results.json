{
  "query": "What are the best techniques for reducing the gap between local cross-validation and leaderboard scores in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the gap between your local cross\u2011validation (CV) score and the private leaderboard score hinges on building a validation scheme that truly mimics the hidden test distribution and on preventing any form of over\u2011fitting to the public data.  The most reliable way to do this is to **use robust, multi\u2011fold CV** (e.g., 5\u2011 or 10\u2011fold, stratified or time\u2011aware when needed) and repeat it several times, averaging the results so that a single lucky split does not bias your estimate\u202f([Chioka](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio))\u202f([DataCamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=8)).  **Adversarial validation**\u2014training a classifier to distinguish train from test rows\u2014helps you spot and correct distribution shifts before they bite the private leaderboard\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).  Pair this with **early stopping and regularization** (L1/L2, dropout for neural nets) so the model does not chase noise in any fold\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).\n\nOnce a solid CV pipeline is in place, **model diversity and ensembling** become the next biggest lever.  Train several fundamentally different algorithms (e.g., gradient\u2011boosted trees, deep nets, linear models) and combine them through **stacking or blending**; the ensemble often smooths out the variance between CV and private scores\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).  Use **Bayesian optimization or AutoML tools** to fine\u2011tune hyper\u2011parameters efficiently, but keep the search constrained to avoid over\u2011optimizing on the CV folds\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).  **Feature selection and engineering**\u2014including target encoding, interaction features, and careful handling of missing values\u2014should be guided by CV performance, not by public leaderboard jumps\u202f([NVIDIA Playbook](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\nFinally, **track everything**: save fold predictions, model checkpoints, and a log of CV scores so you can reproduce and compare runs\u202f([DataCamp Final Tips](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12)).  Regularly scan the **Kaggle forum and public kernels** for hints about hidden test quirks, and incorporate community\u2011tested tricks (e.g., specific data augmentations or leakage checks) into your pipeline\u202f([DataCamp Final Tips](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12)).  By aligning your validation strategy with the true test distribution, regularizing aggressively, and leveraging diverse, well\u2011tuned ensembles, you can dramatically shrink the CV\u2011to\u2011leaderboard gap and achieve more consistent competition performance.",
      "url": ""
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Final tips | Python",
      "text": "Final tips | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12\nFinal tips | Python\nNone\n2022-06-13T00:00:00Z\n# Final tips\n####. Final tips\nAll right, we're almost done. In this lesson, we'll just cover some tips that haven't been mentioned throughout the course.\n####. Save information\nThe first tip is saving all the information we can.\nTo begin, save folds distribution to files. Our goal is to track the validation score during the competition. And of course, this validation score should always be calculated on the same folds.\nAnother data that we'd like to save is model runs. It will allow us to reproduce our experiments or go back if needed. One of the possibilities could be to create a separate git commit for each model run or submission.\nIt is also a good idea to save model predictions as well. If we start saving validation and test predictions from the very beginning of the competition, it will allow us to simply build model ensembles near the end. Because we store predictions for the models blending as well as features for the models stacking.\nFinally, we should keep a log of models' results to track the performance progress. It could be done as comments to the git commits or as notes in a separate document.\n####. Kaggle forum and kernels\nNow let's speak about the Kaggle forum and kernels. It's one of the strongest sources of knowledge on Kaggle.\n####. Kaggle forum and kernels\nEach competition has an open forum where all the participants can start topics sharing their thoughts and ideas, asking questions and so on.\n####. Kaggle forum and kernels\nKaggle kernels is another source of knowledge. It represents scripts and notebooks that participants are sharing during the competition. So, we have an opportunity not only to discuss the competition, but also to look at the code.\nMoreover, kernels represent a computational environment where we have access to an interactive session running in a Docker container with pre-installed packages, the ability to mount data sources, use GPU resources, and more.\n####. Forum and kernels usage\nForum and kernels could bring us lots of benefits during the different competition stages.\nSuppose we decided to join some of the current Kaggle competitions. First of all, it is useful to find similar past competitions on Kaggle. Usually, top teams are sharing their approach on the forum once the competition has finished. It allows us to read through the best performing solutions and get to know what could work for the similar problem types.\nDuring the rest of the competition, we should precisely follow all the topics in the forum and the most popular kernels. It allows us to be up-to-date during the competition and learn lots of new ideas and approaches.\nFinally, even after the end of the competition, it's time to learn from the top participants. Usually, winners share their solutions a couple of days after the competition finish. It's very valuable information that we should utilize to determine what we could have done better during the competition.\n####. Select final submissions\nThe last few words are devoted to the final submissions. Kaggle competitions have different durations, but generally, it's about 2 or 3 months. As we already know, every day we have a limited number of submissions to the Leaderboard.\nSo, if we have a 2-month competition with 5 submissions per day, we could make up to 300 submissions to the Public Leaderboard.\n####. Select final submissions\nHowever, for the final evaluation on the Private Leaderboard, we have to choose only 2 submissions. We mark them in the list of all submissions made.\n####. Select final submissions\nAnd only these are used for the final standings. Our result is the best score out of these two final submissions.\n####. Final submissions\nThe suggested strategy that works pretty well is to select one submission that is the best on the local validation,\nand another submission that is the best on the Public Leaderboard.\n####. Let's practice!\nLet's now review these final tips before saying good-bye!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=12"
    },
    {
      "title": "",
      "text": "Local validation | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=8\nLocal validation | Python\nNone\n2022-06-13T00:00:00Z\n# Local validation\n####. Local validation\nAfter some preliminary steps, we come to one of the crucial parts of the solution process: local validation.\n####. Motivation\nBefore we start, let's discuss the motivation for local validation.\nRecall the plot with possible overfitting to Public test data. The problem we observe here is that we can't detect the moment when our model starts overfitting by looking only at the Public Leaderboard. That's where local validation comes into play.\nUsing only train data, we want to build some kind of an internal, or local, approximation of the model's performance on a Private test data.\n####. Holdout set\nThe question is: how do we build such an approximation of the model's performance? The simplest way is to use a holdout set.\nWe split all train data (in other words, all the observations we know the target variable for) into train and holdout sets.\n####. Holdout set\nWe then build a model using only the train set and make predictions on the holdout set. So, the holdout is similar to the usual test data, but the target variable is known.\n####. Holdout set\nIt allows to compare predictions with the actual values and gives us a fair estimate of the model's performance.\nHowever, such an approach is similar to just looking at the results on the Public Leaderboard. We always use the same data for model evaluation and could potentially overfit to it. A better idea is to use cross-validation.\n####. K-fold cross-validation\nThe process of K-fold cross-validation is presented on the slide. We split the train data into K non-overlapping parts called 'folds' (in this case K is equal to 4).\n####. K-fold cross-validation\nThen train a model K times on all the data except for a single fold. Each time, we also measure the quality on this single fold the model has never seen before.\nK-fold cross-validation gives our model the opportunity to train on multiple train-test splits instead of using a single holdout set. This gives us a better indication of how well our model will perform on unseen data.\n####. K-fold cross-validation\nTo apply K-fold cross-validation with scikit-learn, import it from the model_selection module.\nCreate a KFold object with the following parameters: n_splits is the number of folds, shuffle is whether the data is sorted before splitting. Generally, it's better to always set this parameter to True. And random_state sets a seed to reproduce the same folds in any future run.\nNow, we need to train K models for each cross-validation split. To obtain all the splits we call the split() method of the KFold object with a train data as an argument. It returns a list of training and testing observations for each split. The observations are given as numeric indices in the train data.\nThese indices could be used inside the loop to select training and testing folds for the corresponding cross-validation split.\nFor pandas DataFrame it could be done using the iloc operator, for example.\n####. Stratified K-fold\nAnother approach for cross-validation is stratified K-fold. It is the same as usual K-fold, but creates stratified folds by a target variable. These folds are made by preserving the percentage of samples for each class of this variable. As we see on the image, each fold has the same classes distribution as in the initial data. It is useful when we have a classification problem with high class imbalance in the target variable or our data size is very small.\n####. Stratified K-fold\nStratified K-fold is also located in sklearn's model_selection module.\nIt has the same parameters as the usual KFold: n_splits, shuffle and random_state.\nThe only difference is that on top of the train data, we should also pass the target variable into the split() call in order to make a stratification.\n####. Let's practice!\nAs you can see, there are various validation strategies available. Let's try them out!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=8"
    },
    {
      "title": "How to Select Your Final Models in a Kaggle Competition",
      "text": "Did your rank just drop sharp in the private leaderboard in a Kaggle\u00a0competition?\n[![picard palm](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)\nI\u2019ve been through that, too. We all learn about overfitting when we started machine learning,\u00a0but Kaggle makes\u00a0you really feel the pain of overfitting.\u00a0Should I have been more careful in the [Higgs Boson Machine Learning competition](http://www.kaggle.com/c/higgs-boson/), I would have selected a solution that would gave me a rank 4 than rank 22.\nI vow to come out with some principles systematically select\u00a0final models. Here are the lessons learnt:\n- **Always do cross-validation to get a reliable metric.**\u00a0If you don\u2019t, the validation score you get on a single\u00a0validation set\u00a0is unlikely to reflect the model performance in general. Then, you will likely see\u00a0a model improvement in that single validation set, but actually performs worse in general. **_Keep in mind the CV score can be optimistic, but your model is still overfitting._**\n- **Trust your CV\u00a0score, and not LB\u00a0score.** The leaderboard score\u00a0is scored only on a small percentage of the full test set. In some cases, it\u2019s only a few hundred test cases. Your cross-validation score will be much more reliable in general.\n- If your CV score is not stable (perhaps due to ensembling methods), you can\u00a0run your CV with more folds and multiple times to\u00a0take average.\n- If a single\u00a0CV\u00a0run is very slow, use a subset of the data to run\u00a0the CV. This will help your CV loop to run faster. Of course, the subset should not be too small or else the CV score will not be representative.\n- **For the final 2 models, pick very different models.** Picking two very similar solutions means that your solutions either fail together or win together, effectively meaning that you only pick one model. You should reduce your risk by picking two confident but very different models.\u00a0_**You should not depend on the leaderboard score at all.**_\n- Try to group your solutions by methodologies. Then, pick the best CV score model from each group. Then compare these best candidates of each group, pick two.\n- Example: I have different groups 1)\u00a0Bagging of SVMs 2) RandomForest 3) Neural Networks 4) LinearModels. Then, each group should produce one single best model, then you pick 2 out of these.\n- **Pick a robust methodology.** Here is the tricky part\u00a0which depends on experience, even if you have done cross validation, you can still get burned:\u00a0Sketchy methods of improving the CV score like\u00a0making cubic features, cubic root features, boosting like crazy, magical\u00a0numbers(without understanding it), etc, will likely be a bad model to pick even if the CV score is good. Unfortunately, you will probably have to make this mistake once to know what this means. =\\]\nApplying the above\u00a0principles to the recent competition\u00a0[Africa Soil Property Prediction Challenge](http://www.kaggle.com/c/afsis-soil-properties), plus a bit of luck, I picked the top 1 and top 2 models.\n[![top score](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)\nSorted by private score\nI ended up Top 10% with a rank of\u00a090 by spending just\u00a0a week time and mostly in Mexico in a vacation. I guess,\u00a0not too bad?",
      "url": "https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio"
    },
    {
      "title": "General Tips for participating Kaggle Competitions",
      "text": "# General Tips for participating Kaggle Competitions\n\n\u2022\n\n196 likes\u202286,040 views\n\n![Mark Peng](https://cdn.slidesharecdn.com/profile-photo-markpeng-48x48.jpg?cb=1709552824)\n\n[Mark Peng](https://www.slideshare.net/markpeng?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview) Follow\n\nThe slides of a talk at Spark Taiwan User Group to share my experience and some general tips for participating kaggle competitions. Read less\n\nRead more\n\nReport\n\nShare\n\nReport\n\nShare\n\n1 of 74\n\nDownload nowDownload to read offline\n\n![General Tips for participating Competitions Mark Peng 2015.12.16 @ Spark Taiwan User Group https://tw.linkedin.com/in/markpeng  ](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/75/General-Tips-for-participating-Kaggle-Competitions-1-2048.jpg)\n\n![Kaggle Profile (Master Tier) https://www.kaggle.com/markpeng 2 Big Data Scientist  ](data:image/gif;base64)\n\n![3 Things I Want to Share \u2022 Quick overview to Kaggle rules \u2022 Why cross-validation matters \u2022 Mostly used ML models \u2022 Feature engineering methods \u2022 Ensemble learning \u2022 Team up \u2022 Recommended books, MOOCs and resources  ](data:image/gif;base64)\n\n![Kaggle Competition Dataset and Rules 4 Training Dataset Private LBPublic LB Validation feedback but sometimes misleading Testing Dataset Might be different from public LB (used to determine final prize winners!)  ](data:image/gif;base64)\n\n![5 How to become a Kaggle Master \u2022 To achieve Master tier, you must fulfill 2 criteria \u2022 Consistency: at least 2 Top 10% finishes in public competitions \u2022 Excellence: at least 1 of those finishes in the top 10 positions \u2022 Note that not all competitions count toward earning Master tier! Reference: https://www.kaggle.com/wiki/UserRankingAndTierSystem  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 6 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![Cross Validation The key to avoid overfitting 7  ](data:image/gif;base64)\n\n![Underfitting and Overfitting We want to find a model with lowest generalization error (hopefully) 8 Model Complexity HighLow PredictionError Training Error Validation Error (Local CV) Testing Error (Private LB) High Variance Low Bias High Bias Low Variance Lowest Generalization Error OverfittingUnderfitting  ](data:image/gif;base64)\n\n![9 Big Shake Up on Private LB! Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/leaderboard/private  ](data:image/gif;base64)\n\n![10 Who is the King of Overfitting? Reference: https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission Public LB RMSE: Private LB RMSE: They even wrote a post to show off their perfect overfitting! Num. of Features: 41 Training Data Size: 137 Testing Data Size: 10,000  ](data:image/gif;base64)\n\n![11 K-fold Cross Validation (K = 5) fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 fold 1 fold 2 fold 3 fold 4 fold 5 Round 1 Round 2 Round 3 Round 4 Round 5 Validation Data Training Data score(CV) = the average of evaluation scores from each fold You can also repeat the process many times!  ](data:image/gif;base64)\n\n![12 K-fold Cross Validation Tips \u2022 It is normal to experience big shake up on private LB if not using local CV correctly \u2022 5 or 10 folds are not always the best choices (you need to consider the cost of computation time for training models) \u2022 Which K to choose? \u2022 Depends on your data \u2022 Mimic the ratio of training and testing in validation process \u2022 Find a K with lowest gap between local CV and public LB scores \u2022 Standard deviation of K-fold CV score matters more than mean! \u2022 Stratified K-fold CV is important for imbalanced dataset, especially for classification problems  ](data:image/gif;base64)\n\n![13 Stratified K-fold Cross Validation (K = 5) Round 1 Round 2 Round 3 Round 4 Round 5Class Distributions F M Keep the distribution of classes in each fold Validation Data Training Data  ](data:image/gif;base64)\n\n![14 Should I Trust Public LB? \u2022 Yes if you can find a K-fold CV that follows the same trend with public LB \u2022 High positive correlation between local CV and public LB \u2022 The score increases and decreases in both local CV and public LB \u2022 Trust more in your local CV!  ](data:image/gif;base64)\n\n![Data Cleaning Making data more analyzable 15  ](data:image/gif;base64)\n\n![Recommended Data Science Process (IMHO) 16 Data Cleaning Feature Engineering Single Model Exploratory Data Analysis Diverse Models Ensemble Learning Final Predictions Feature Engineering Ensemble Learning Exploratory Data Analysis Modeling Data Cleaning Relative Time 40%10% 30% 20%  ](data:image/gif;base64)\n\n![17 Data Cleaning Techniques \u2022 Data cleaning is the removal of duplicates, useless data, or fixing of missing data \u2022 Reduce dimensional complexity of dataset \u2022 Make training faster without hurting the performance \u2022 Apply imputation methods to help (hopefully) utilize incomplete rows \u2022 Incomplete rows may contain relevant features (don\u2019t just drop them!) \u2022 In the risk of distorting original data, so be cautious!  ](data:image/gif;base64)\n\n![18 Data Cleaning Techniques (cont.) \u2022 Remove duplicate features \u2022 Columns having the same value distribution or variance \u2022 Only need to keep one of them \u2022 Remove constant features \u2022 Columns with only one unique value \u2022 R: sapply(data, function(x) length(unique(x))) \u2022 Remove features with near-zero variance \u2022 R: nearZeroVar(data, saveMetrics=T) in caret package \u2022 Be sure to know what you are doing before removing any features  ](data:image/gif;base64)\n\n![19 Data Cleaning Techniques (cont.) \u2022 Some machine learning tools cannot accept NAs in the input \u2022 Encode missing values to avoid NAs \u2022 Binary features \u2022 -1 for negatives, 0 for missing values and 1 for positives \u2022 Categorical features \u2022 Encode as an unique category \u2022 \u201cUnknown\u201d, \u201cMissing\u201d, \u2026. \u2022 Numeric features \u2022 Encode as a big positive or negative number \u2022 999, -99999, \u2026.  ](data:image/gif;base64)\n\n![20 Data Cleaning Techniques (cont.) \u2022 Basic ways to impute missing values \u2022 mean, median or most frequent value of feature \u2022 R: impute(x, fun = mean) in Hmisc package \u2022 Python: Imputer(strategy='mean', axis=0) in scikit-learn package  ](data:image/gif;base64)\n\n![21 Data Cleaning Techniques (cont.) \u2022 Advanced: multiple imputation \u2022 Impute incomplete columns based on other columns in the data \u2022 R: mice package (Multivariate Imputation by Chained Equations) \u2022 Imputation would not always give you positive improvements, thus you have to validate it cautiously  ](data:image/gif;base64)\n\n![Mostly Used Models What models to use? 22  ](data:image/gif;base64)\n\n![Model Type Name R Python Regression Linear Regression \u2022 glm, glmnet \u2022 sklearn.linear_model.LinearRegression Ridge Regression \u2022 glmnet \u2022 sklearn.linear_model.Ridge Lasso Regression \u2022 glmnet \u2022 sklearn.linear_model.Lasso Instance-based K-nearest Neighbor (KNN) \u2022 knn \u2022 sklearn.neighbors.KNeighborsClassifier Support Vector Machines (SVM) \u2022 svm {e1071} \u2022 LiblinearR \u2022 sklearn.svm.SVC, sklearn.svm.SVR \u2022 sklearn.svm.LinearSVC, sklearn.svm.LinearSVR Hyperplane-based Naive Bayes \u2022 naiveBayes {e1071} \u2022 sklearn.naive_bayes.GaussianNB \u2022 sklearn.naive_bayes.MultinomialNB \u2022 sklearn.naive_bayes.BernoulliNB Logistic Regression \u2022 glm, glmnet \u2022 LiblinearR \u2022 sklearn.linear_model.LogisticRegression Ensemble Trees Random Forests \u2022 randomForest \u2022 sklearn.ensemble.RandomForestClassifier \u2022 sklearn.ensemble.RandomForestRegressor Extremely Randomized Trees \u2022 extraTrees \u2022 sklearn.ensemble.ExtraTreesClassifier \u2022 sklearn.ensemble.ExtraTreesRegressor Gradient Boosting Machines (GBM) \u2022 gbm \u2022 xgboost \u2022 sklearn.ensemble.GradientBoostingClassifier \u2022 sklearn.ensemble.GradientBoostingRegressor \u2022 xgboost Neural Network Multi-layer Neural Netwo...",
      "url": "https://www.slideshare.net/slideshow/general-tips-for-participating-kaggle-competitions/56209561"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    }
  ]
}