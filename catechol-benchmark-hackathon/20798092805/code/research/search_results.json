{
  "query": "How can neural networks generalize to unseen solvents in chemical reaction prediction using transfer learning or domain adaptation?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Neural\u2011network models can extend their predictive power to solvents that were never seen during training by treating the solvent as a separate, transferable domain and by learning a representation that can be adapted to new conditions.  A common recipe is to **pre\u2011train** a graph\u2011 or transformer\u2011based model on a very large, chemically diverse set of reactions or solvation calculations that include many solvent\u2013solute pairs, then **fine\u2011tune** the same network on the smaller target dataset.  In practice the model learns a solvent embedding (e.g., a vector derived from the solvent SMILES or from physicochemical descriptors) that is shared across all reactions; when a new solvent appears, its embedding can be inferred from its molecular graph and combined with the reaction graph, allowing the network to make predictions without retraining from scratch.  This approach has been shown to work for solvation free\u2011energy and kinetic\u2011rate predictions, where a BERT model pretrained on the USPTO\u2011SMILES reaction corpus achieved R\u00b2\u202f>\u202f0.94 after fine\u2011tuning on a handful of organic\u2011material screening tasks, demonstrating that knowledge transferred from generic reaction data can be reused for new solvent environments ([arXiv\u202f2311.18377](https://arxiv.org/abs/2311.18377)).  Similarly, a graph neural network benchmark on catechol rearrangement used rigorous leave\u2011one\u2011solvent\u2011out and leave\u2011one\u2011mixture\u2011out protocols to confirm that the learned solvent embeddings enable accurate extrapolation to unseen solvents ([arXiv\u202f2512.19530](https://arxiv.org/abs/2512.19530)).\n\nDomain\u2011adaptation techniques such as **attention\u2011based merging of solute\u2011solvent graphs** and **solvent\u2011holdout splits** further improve robustness.  The MMGNN framework builds a merged molecular graph that explicitly models intermolecular hydrogen\u2011bonding and repulsive interactions, then applies an attention aggregator that can re\u2011weight these interactions for a new solvent, achieving reliable solvation free\u2011energy predictions even when the solvent is held out during training ([IJCAI\u202f2024\u202fMMGNN](https://www.ijcai.org/proceedings/2024/0642.pdf)).  Transfer learning from quantum\u2011chemical calculations to experimental data has also been demonstrated: a model trained on COSMO\u2011RS solvation energies for 28\u202f000 reactions and 295 solvents was fine\u2011tuned on a modest experimental set and retained mean absolute errors of ~0.7\u202fkcal\u202fmol\u207b\u00b9 on unseen solvents, enabling rapid screening of kinetic solvent effects ([ChemRxiv\u202f2023](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/6524d5a4bda59ceb9a38ccec/original/machine-learning-from-quantum-chemistry-to-predict-experimental-solvent-effects-on-reaction-rates.pdf)).  Together, these strategies\u2014large\u2011scale pre\u2011training, solvent\u2011specific embeddings, attention\u2011driven graph merging, and systematic hold\u2011out validation\u2014constitute the current best practice for achieving solvent\u2011generalizable chemical reaction prediction with neural networks.",
      "url": ""
    },
    {
      "title": "Learning Continuous Solvent Effects from Transient Flow Data - arXiv",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "[PDF] MMGNN: A Molecular Merged Graph Neural Network for ... - IJCAI",
      "text": "Wenjie Du, Shuai Zhang, Di Wu, Jun Xia, Ziyuan Zhao, Junfeng Fang, Yang Wang MMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation Free Energy Prediction https://ijcai.org/proceedings/2024/0642.pdf\nMMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation Free Energy Prediction\nWenjie Du, Shuai Zhang, Di Wu, Jun Xia, Ziyuan Zhao, Junfeng Fang, Yang Wang\n2024-07-25\nMMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation\nFree Energy Prediction\nWenjie Du1,2, Shuai Zhang1,2, Di Wu1, Jun Xia3, Ziyuan Zhao4, Junfeng Fang1,\u2217,\nYang Wang1,2,\u2217\n1University of Science and Technology of China (USTC), Hefei, China\n2Suzhou Institute for Advanced Research, USTC, Suzhou, China\n3Zhejiang University, Hangzhou, China\n4Agency for Science, Technology and Research (A*STAR), Singapore\n{duwenjie, shuaizhang, fjf, wdcxy}@mail.ustc.edu.cn, zhaoz@i2r.a-star.edu.sg, junxia@zju.edu.cn,\nangyan@ustc.edu.cn\nAbstract\nIn this paper, we address the challenge of ac\u0002curately modeling and predicting Gibbs free en\u0002ergy in solute-solvent interactions, a pivotal yet\ncomplex aspect in the feld of chemical model\u0002ing. Traditional approaches, primarily relying on\ndeep learning models, face limitations in captur\u0002ing the intricate dynamics of these interactions. To\novercome these constraints, we introduce a novel\nframework, Molecular Modeling Graph Neural\nNetwork (MMGNN), which more closely mir\u0002rors real-world chemical processes. Specifcally,\nMMGNN exquisitely models atomic interactions\nsuch as hydrogen bonds by initially forming in\u0002discriminate connections between intermolecular\natoms, which are then refned using an attention\u0002based aggregation method, tailoring to specifc\nsolute-solvent pairs. To address the challenges\nof non-interactive or repulsive atomic interactions,\nMMGNN incorporates interpreters for nodes and\nedges in the merged graph, enhancing explainabil\u0002ity and reducing redundancy. MMGNN stands as\nthe frst framework to exquisitely align with real\nchemical processes, providing a more accurate and\nscientifcally sound approach to modeling solute\u0002solvent interactions. The infusion of explainability\nallows for the extraction of key subgraphs, which\nare pivotal for further research in solute-solvent\ndynamics. Extensive experimental validation con\u0002frms the effcacy and enhanced explainability of\nMMGNN.\n1 Introduction\nUnderstanding solute-solvent interactions in specifc solvents\nis pivotal for areas in physical chemistry, including chemical\nreactions and electrochemistry [Varghese and Mushrif, 2019;\nD\u2019Souza et al., 2011]. A comprehensive grasp of these in\u0002teractions is vital not only for explanatory experimental out\u0002comes but also for guiding the design and control of reac\u0002tions and properties [Chung et al., 2022; Fang et al., 2024b;\n\ud835\udc27 \ud835\udc29\ud835\udc27\n\ud835\udc27\nGraph Merge\n\ud835\udfcf\n\ud835\udfd0\n\ud835\udc29\ud835\udc27\n\u2032\n\u2032\n\n\ud835\udc27\n\ud835\udc27\n\ud835\udfcf\n\ud835\udfd0\n+ \n\ud835\udc27\n\ud835\udc27\nMerge\n\ud835\udfcf\n\ud835\udfd0\n+ \nEthanol\n(Solvent)\nAcetonitrile\n(Solute)\nDissolution\nInitial state\nHomogeneous \nsolution\n(a) Method based on Concatenation\n(b) Method based on Merging\n(c) Chemical Processes\n(d) Our Method based on Merged Graph\nConcatenate Prediction\nPrediction\nPrediction\nFigure 1: Comparison of different paradigms for Gibbs free energy\nprediction. (a) Method by concatenation; (b) method by merging;\n(c) a schematic diagram of the process where acetonitrile (solute) is\ndissolved in ethanol (solvent); and (d) the illustration of our method.\nBest viewed in color.\nXia et al., 2023b]. In this context, the solvation Gibbs free\nenergy \u2206Gsolv emerges as a critical physicochemical prop\u0002erty, dictating a molecule\u2019s behavior in solution [Low et al.,\n2022]. This property is intricately linked to the solute\u2019s parti\u0002tion coeffcient between gas and solvent phases [Chung et al.,\n2022]. However, empirically testing solute-solvent free ener\u0002gies for all combinations is impractical due to high costs and\nextensive time requirements. This challenge necessitates an\nincreased reliance on deep learning models to predict these\nenergies more effciently [Varghese and Mushrif, 2019].\nThe landscape of solute-solvent interaction modeling is\ncurrently dominated by two primary approaches. The frst,\ntermed embedding concatenation, employs two separate\nGraph Neural Networks (GNNs) to represent solute and\nsolvent molecules. The individual embeddings from these\nGNNs are then concatenated for subsequent prediction tasks,\nProceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24)\n5808\nas depicted in Figure 1 (a). While effective, this method may\nnot adequately capture the intricate coupling between solute\nand solvent. In contrast, the embedding merging approach,\nshown in Figure 1 (b), seeks to address this limitation. It\nalso utilizes two Graph Encoders for initial representation\nbut differs in the subsequent processing of these embeddings.\nHere, the embeddings are merged through advanced interac\u0002tion strategies, such as Transformer-based techniques or in\u0002teractive pruning algorithms, to better refect the complex in\u0002teractions before making predictions.\nAlthough the embedding merging approach marks an ad\u0002vancement, it falls short in accurately simulating the real\nchemical processes involved in dissolution, as depicted in\nFigure 1 (c). The dissolution process is characterized by the\nsubstitution of intramolecular forces within solute and solvent\nwith intermolecular forces between them. This is exempli\u0002fed by the interaction of acetonitrile and ethanol, where in\u0002dividual acetonitrile molecules are gradually surrounded by\nethanol molecules. The formation of hydrogen bonds be\u0002tween atoms (nodes) of these molecules leads to a homo\u0002geneous solution. This natural dissolution process suggests\na need for an algorithm that explicitly models these inter\u0002molecular node interactions, rather than relying solely on\nembedding-level interactions. Such a model, by more closely\nmirroring actual chemical processes, is crucial for both scien\u0002tifc accuracy and the effectiveness of the predictions.\nIn response to the limitations of existing models, we\npropose the Molecular Modeling Graph Neural Network\n(MMGNN), a novel framework designed to closely align\nwith the actual chemical processes in solute-solvent interac\u0002tions. This framework signifcantly improves the prediction\naccuracy of Gibbs free energy. Illustrated in Figure 1 (d),\nMMGNN begins by indiscriminately connecting intermolec\u0002ular atoms to enhance interactions, such as hydrogen bonds,\nbetween molecules. After that, MMGNN assigns variable\nweights to these connections, refecting the different con\u0002straints of various chemical bonds. An attention-based ag\u0002gregation method is then employed, enabling the framework\nto adaptively learn from diverse solute-solvent combinations.\nThe result is a merged graph representation for each pair,\nhighlighting the most signifcant atomic interactions.\nMeanwhile, this framework also addresses potential chal\u0002lenges: (1) the presence of non-existent or repulsive atomic\ninteractions, and (2) increased complexity and convergence\ndiffculties in merged complete graphs for large molecular\npairs. Inspired by graph explainability algorithms, MMGNN\nincorporates interpreters for both nodes and solute-solvent\nedges in the merged graph. This approach effectively reduces\nredundancy, focusing only on relevant interactions. The ex\u0002planatory subgraph is then encoded and utilized in regression\nmodels, such as Fully Connected Neural Networks (FCNN),\nto predict Gibbs free energy with greater precision.\nIn conclusion, the main contributions of this paper can be\nsummarized as follows:\n\u2022 Introduction of MMGNN: We present the Molecular\nModeling Graph Neural Network (MMGNN), a pioneering\nframework designed to explicitly align with actual chemi\u0002cal processes, enabling more accurate modeling of solute\u0002solvent interactions.\n\u2022 Advancement in Explainability: Our approach integrates\nexplainability into the model, allowing for the extraction of\nkey subgraphs. This feature not only enhances the under\u0002standing of MMGNN\u2019s predictions but a...",
      "url": "https://www.ijcai.org/proceedings/2024/0642.pdf"
    },
    {
      "title": "Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2311.18377** (physics)\n\n\\[Submitted on 30 Nov 2023 ( [v1](https://arxiv.org/abs/2311.18377v1)), last revised 5 Mar 2024 (this version, v2)\\]\n\n# Title:Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data\n\nAuthors: [Chengwei Zhang](https://arxiv.org/search/physics?searchtype=author&query=Zhang,+C), [Yushuang Zhai](https://arxiv.org/search/physics?searchtype=author&query=Zhai,+Y), [Ziyang Gong](https://arxiv.org/search/physics?searchtype=author&query=Gong,+Z), [Hongliang Duan](https://arxiv.org/search/physics?searchtype=author&query=Duan,+H), [Yuan-Bin She](https://arxiv.org/search/physics?searchtype=author&query=She,+Y), [Yun-Fang Yang](https://arxiv.org/search/physics?searchtype=author&query=Yang,+Y), [An Su](https://arxiv.org/search/physics?searchtype=author&query=Su,+A)\n\nView a PDF of the paper titled Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data, by Chengwei Zhang and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2311.18377)\n\n> Abstract:Machine learning is becoming a preferred method for the virtual screening of organic materials due to its cost-effectiveness over traditional computationally demanding techniques. However, the scarcity of labeled data for organic materials poses a significant challenge for training advanced machine learning models. This study showcases the potential of utilizing databases of drug-like small molecules and chemical reactions to pretrain the BERT model, enhancing its performance in the virtual screening of organic materials. By fine-tuning the BERT models with data from five virtual screening tasks, the version pretrained with the USPTO-SMILES dataset achieved R2 scores exceeding 0.94 for three tasks and over 0.81 for two others. This performance surpasses that of models pretrained on the small molecule or organic materials databases and outperforms three traditional machine learning models trained directly on virtual screening data. The success of the USPTO-SMILES pretrained BERT model can be attributed to the diverse array of organic building blocks in the USPTO database, offering a broader exploration of the chemical space. The study further suggests that accessing a reaction database with a wider range of reactions than the USPTO could further enhance model performance. Overall, this research validates the feasibility of applying transfer learning across different chemical domains for the efficient virtual screening of organic materials.\n\n|     |     |\n| --- | --- |\n| Subjects: | Chemical Physics (physics.chem-ph); Machine Learning (cs.LG); Biomolecules (q-bio.BM) |\n| Cite as: | [arXiv:2311.18377](https://arxiv.org/abs/2311.18377) \\[physics.chem-ph\\] |\n|  | (or [arXiv:2311.18377v2](https://arxiv.org/abs/2311.18377v2) \\[physics.chem-ph\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2311.18377](https://doi.org/10.48550/arXiv.2311.18377)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: An Su \\[ [view email](https://arxiv.org/show-email/b47e7517/2311.18377)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2311.18377v1)**\nThu, 30 Nov 2023 09:20:24 UTC (1,229 KB)\n\n**\\[v2\\]**\nTue, 5 Mar 2024 10:23:33 UTC (1,404 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data, by Chengwei Zhang and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2311.18377)\n- [Other Formats](https://arxiv.org/format/2311.18377)\n\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2311.18377&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2311.18377&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2023-11](https://arxiv.org/list/physics.chem-ph/2023-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2311.18377?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2311.18377?context=cs.LG)\n\n[physics](https://arxiv.org/abs/2311.18377?context=physics)\n\n[q-bio](https://arxiv.org/abs/2311.18377?context=q-bio)\n\n[q-bio.BM](https://arxiv.org/abs/2311.18377?context=q-bio.BM)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.18377)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.18377)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.18377)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2311.18377&description=Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2311.18377&title=Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a pr...",
      "url": "https://arxiv.org/abs/2311.18377"
    },
    {
      "title": "",
      "text": "Machine learning from quantum chemistry to\npredict experimental solvent effects on reaction\nrates\nYunsie Chung and William H. Green\u2217\nDepartment of Chemical Engineering, Massachusetts Institute of Technology, Cambridge,\nMA, 02139, U.S.A\nE-mail: whgreen@mit.edu\nAbstract\nFast and accurate prediction of solvent effects on reaction rates are crucial for kinetic\nmodeling, chemical process design, and high-throughput solvent screening. Despite the\nrecent advance in machine learning, a scarcity of reliable data has hindered the devel\u0002opment of predictive models that are generalizable for diverse reactions and solvents.\nIn this work, we generate a large set of data with the COSMO-RS method for over\n28,000 neutral reactions and 295 solvents and train a machine learning model to pre\u0002dict the solvation free energy and solvation enthalpy of activation (\u2206\u2206G\n\u2021\nsolv, \u2206\u2206H\n\u2021\nsolv)\nfor a solution phase reaction. On unseen reactions, the model achieves mean absolute\nerrors of 0.71 and 1.03 kcal/mol for \u2206\u2206G\n\u2021\nsolv and \u2206\u2206H\n\u2021\nsolv, respectively, relative to the\nCOSMO-RS calculations. The model also provides reliable predictions of relative rate\nconstants within a factor of 4 when tested on experimental data. The presented model\ncan provide nearly instantaneous predictions of kinetic solvent effects or relative rate\nconstants for a broad range of neutral closed-shell or free radical reactions and solvents\nonly based on atom-mapped reaction SMILES and solvent SMILES strings.\n1\nhttps://doi.org/10.26434/chemrxiv-2023-f20bg-v2 ORCID: https://orcid.org/0000-0002-3097-010X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n1 Introduction\nAccurate prediction of reaction rates is essential for modeling a variety of chemical kinetic sys\u0002tems such as pyrolysis, 1,2 polymerization,3 oxidative degradation, 4,5 and atmospheric chem\u0002istry.6 Detailed kinetic models enable one to predict key products, identify major kinetic\npathways, and optimize reaction conditions for complex chemical systems. Kinetic mech\u0002anisms often involve hundreds to tens of thousands of elementary reactions, 7 and a fast,\nhigh-throughput method to estimate reaction rates is thus needed. Ab initio methods like\nquantum mechanics/molecular mechanics (QM/MM) can provide accurate predictions of rate\nconstants, but their high computational cost has been a major limiting factor for large-scale,\nautomated predictions. As more kinetic data become available, data-driven approaches such\nas linear group contribution, 8\u201310 decision tree based rate rules, 11,12 and machine learning\n(ML) models13\u201319 have emerged as more popular choices for estimating kinetic parameters.\nSeveral ML models15\u201317 have successfully predicted barrier heights and rate constants of\ndiverse gas phase reactions only based on readily available 2D information (e.g. SMILES\nstrings) of reactants and products. However, such predictive models for liquid/solution phase\nreactions have been lightly investigated with limited applicability. 20\nSolvents can have significant impacts on reaction rates and outcomes, and it is crucial to\naccurately predict these kinetic solvent effects. Recent research efforts have been devoted\nto employing ML (e.g. deep neural network) for free energy predictions of condensed phase\nreactions.15,18,19,21\u201326 Many of these studies 18,19,21\u201323,26 combine the ML models with semi\u0002empirical or lower-level QM/MM methods to obtain the energy predictions that match the\naccuracy of higher-level QM/MM methods. For example, G\u00b4omez-Flores et al. 19 used a ML\napproach to predict the energy difference between the density functional tight-binding model\nand other higher level QM methods for a thiol-disulfide exchange reaction in water. In a\nstudy by Pan et al.,18 a ML model was trained to reproduce ab initio QM/MM poten\u0002tials in free energy simulations for the aqueous Menshutkin reaction between ammonia and\nchloromethane. Farrar and Grayson26 employed ML models to predict DFT-quality activa\u0002tion barriers for various nitro-Michael addition reactions in toluene based on the features\ngenerated from semi-empirical methods. These approaches, however, require semi-empirical\nQM/MM steps that are less suitable for instantaneous, automatic rate predictions. Fur\u0002thermore, their models are limited to a single solvent and need the 3D coordinates or QM\nfeatures of reactants and transition states as inputs, which are not readily available.\nThe ML models by Jorner et al. 24 and by Heid and Green15 are the few cases that can\npredict reaction properties in multiple solvents only based on the 2D structural information\nof molecules. Jorner et al. 24 employed a Gaussian process regression model and compared\nseveral 2D structural features to predict the barrier height of 443 SNAr reactions in different\nsolvents. In their work, the best accuracy was reached by adopting the BERT27 reaction fin\u0002gerprint. Heid and Green, 15 on the other hand, used the condensed graph of reaction (CGR)\nas an input reaction representation for a graph convolutional neural network (GCNN). They\napplied the CGR GCNN model to the same SNAr data set and were able to achieve better\n2\nhttps://doi.org/10.26434/chemrxiv-2023-f20bg-v2 ORCID: https://orcid.org/0000-0002-3097-010X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nbarrier height predictions compared to the other models that used the BERT fingerprint or\ndifferent reaction representations. While these models can provide fast kinetic estimations\nfor solution-phase reactions at a low computational cost, only one reaction family was con\u0002sidered with a relatively small training set. A larger data set that contains more diverse\ntypes of reactions and solvents is needed in order to train a more generalized model for\nkinetic solvent effect predictions. Moreover, both models used fixed descriptors to represent\nsolvents, but prior studies 15,28,29 revealed that the learned molecular representations based\non a graph convolutional approach outperform fixed molecular descriptors in many property\nprediction tasks.\nIn this study, we present a ML model that can predict kinetic solvent effects for a wide range\nof neutral reactions and solvents only based on atom-mapped reaction SMILES and solvent\nSMILES strings. More precisely, the model predicts the solvation free energy and solvation\nenthalpy of activation (\u2206\u2206G\n\u2021\nsolv, \u2206\u2206H\n\u2021\nsolv) for a reaction-solvent pair, which can be used\nto estimate a relative rate constant between a solution phase and a gas phase reaction or\nbetween the reaction in different solvents. Our model adopts a CGR GCNN architecture with\nseparate GCNN layers for solvent molecular encoding. A large, diverse set of training data\ncontaining over 28,000 reactions and 295 solvents is generated in this work by performing\nab initio COSMO-RS30 calculations. The performance of the model on unseen reactions is\nrigorously assessed by comparing the ML predictions with both COSMO-RS calculations\nand experimental data. A transfer learning approach and various additional features are\nexplored to further improve the model. Our ML model can provide accurate predictions of\nrelative rate constants, and together with the existing predictive models or databases for gas\nphase rate constants (e.g. RMG database 12), it can provide the estimates of absolute rate\nconstants for many different liquid phase reactions.\n2 Background on the prediction targets\nFigure 1: Potential energy diagram of a reaction in a gas phase and a solution phase.\n3\nhttps://doi.org/10.26434/chemrxiv-2023-f20bg-v2 ORCID: https://orcid.org/0000-0002-3097-010X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nOur ML model aims to predict the solvation free energy and solvation enthalpy of activation\n(\u2206\u2206G\n\u2021\nsolv, \u2206\u2206H\n\u2021\nsolv) at 298 K for a reaction in a solvent. Solvation free energy (\u2206Gsolv)\nand solvation enthalpy (\u2206Hsolv) are the changes in Gibbs free energy and enthalpy when a\nmolecule is transferred from an ideal gas to a solvent at a fixed condition. The \u2206\u2206G\n\u2021\nsolv\nand \u2206\u2206...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/6524d5a4bda59ceb9a38ccec/original/machine-learning-from-quantum-chemistry-to-predict-experimental-solvent-effects-on-reaction-rates.pdf"
    },
    {
      "title": "Optimizing Neural Networks for Chemical Reaction Prediction - MDPI",
      "text": "## Article Menu\n\nFont Type:\n\n_Arial__Georgia__Verdana_\n\nOpen AccessArticle\n\nby\n\nIvan Malashin\n\n,\nVadim Tynchenko\n\\*,\nAndrei Gantimurov\n,\nVladimir Nelyub\nand\nAleksei Borodulin\n\nArtificial Intelligence Technology Scientific and Education Center, Bauman Moscow State Technical University, 105005 Moscow, Russia\n\n\\*\n\nAuthor to whom correspondence should be addressed.\n\nSubmission received: 10 March 2024\n/\nRevised: 24 March 2024\n/\nAccepted: 28 March 2024\n/\nPublished: 29 March 2024\n\n## Abstract\n\nThis paper offers a thorough investigation of hyperparameter tuning for neural network architectures using datasets encompassing various combinations of Methylene Blue (MB) Reduction by Ascorbic Acid (AA) reactions with different solvents and concentrations. The aim is to predict coefficients of decay plots for MB absorbance, shedding light on the complex dynamics of chemical reactions. Our findings reveal that the optimal model, determined through our investigation, consists of five hidden layers, each with sixteen neurons and employing the Swish activation function. This model yields an NMSE of 0.05, 0.03, and 0.04 for predicting the coefficients A, B, and C, respectively, in the exponential decay equation A \\+ B \u00b7 e\u2212x/C. These findings contribute to the realm of drug design based on machine learning, providing valuable insights into optimizing chemical reaction predictions.\n\n## 1\\. Introduction\n\nThe kinetic assessment of chemical reactions is pivotal in elucidating reaction mechanisms and optimizing processes across diverse domains of chemistry. Among these, the reduction of Methylene Blue (MB) by ascorbic acid (AA) \\[ [1](http://www.mdpi.com/www.mdpi.com#B1-ijms-25-03860), [2](http://www.mdpi.com/www.mdpi.com#B2-ijms-25-03860)\\] stands as a notable reaction, owing to its widespread applications in analytical and industrial contexts, attributed to its sensitivity and specificity. MB is a water-soluble cationic dye molecule \\[ [3](http://www.mdpi.com/www.mdpi.com#B3-ijms-25-03860)\\], readily reduced to leucomethylene blue, and oxidized back to its blue form. The reduction reaction of MB is actively researched by scientists worldwide \\[ [1](http://www.mdpi.com/www.mdpi.com#B1-ijms-25-03860), [4](http://www.mdpi.com/www.mdpi.com#B4-ijms-25-03860), [5](http://www.mdpi.com/www.mdpi.com#B5-ijms-25-03860)\\], reflecting its significance in various fields of study. Despite challenges in employing it as a clock reaction, its kinetics remain a valuable teaching tool, measured spectrophotometrically at nm, offering insights into rate law dependencies on MB, ascorbic acid, and other concentrations.\n\nThe reduction of MB has been scrutinized and extensively discussed in academic literature. The optimal conditions for decolorizing methylene blue using ascorbic acid were determined by Weiland et al. \\[ [6](http://www.mdpi.com/www.mdpi.com#B6-ijms-25-03860)\\] by investigating the wavelength and intensity of illumination. Apples and potatoes were halved, immersed in a methylene blue solution, and exposed to light. Decolorization was observed, revealing the tissue\u2019s reducing capacity, with apples showing greatest effect around the core and under the skin, while potatoes exhibited less decolorization, primarily in a band near the skin.\n\nSeno et al. \\[ [7](http://www.mdpi.com/www.mdpi.com#B7-ijms-25-03860)\\] proposed that the reduction of MB with L-ascorbic acid in aqueous surfactant solutions is accelerated by hexadecyltrimethylammonium bromide (HTAB) \\[ [8](http://www.mdpi.com/www.mdpi.com#B8-ijms-25-03860)\\] but not by tetrabutylammonium bromide. Micellar effects, affected by pH and inhibited by KCl, suggest changes in substrate dissociation and product binding to micelles.\n\nMB reduction to leucomethylene blue (MBH) by ascorbic acid in acetonitrile is studied by Hallock et al. \\[ [9](http://www.mdpi.com/www.mdpi.com#B9-ijms-25-03860)\\] using cavity ring-down spectroscopy (CRDS) \\[ [10](http://www.mdpi.com/www.mdpi.com#B10-ijms-25-03860)\\]. This technique allows for precise monitoring of concentration changes on a microsecond timescale, which was previously unattainable. CRDS utilizes reflective mirrors to form an optical cavity, and changes in light intensity are recorded as the absorber affects the decay time. This method offers significantly increased sensitivity compared to traditional absorption measurements.\n\nThe impact of the anionic surfactant sodium dodecyl sulphate (SDS) pre-micellar clusters on the electron transfer reaction between MB and AA in dilute acid conditions was explored by Sen et al. \\[ [11](http://www.mdpi.com/www.mdpi.com#B11-ijms-25-03860)\\]. The reaction, exhibiting first-order kinetics with respect to both MB and AA, involves uncatalyzed and H+-catalyzed paths. Iodide ion was found to accelerate the reaction rate.\n\nSimple classroom experiments, like clock reactions, offer accessible avenues to grasp advanced concepts. For instance, Snehalatha et al. \\[ [12](http://www.mdpi.com/www.mdpi.com#B12-ijms-25-03860)\\] demonstrated MB and L-ascorbic acid clock reaction to undergraduates, showcasing kinetics principles with visible color changes. This approach contrasts with the classic \u201cBlue Bottle\u201d experiment and has been expanded to include studies on persulfate-iodide and bromination reactions, elucidating factors like ionic strength and substituent effects on reaction rates.\n\nRecent literature has seen a surge in studies investigating the properties and behaviors of MB through the lens of machine learning (ML) algorithms. Kooh et al. \\[ [13](http://www.mdpi.com/www.mdpi.com#B13-ijms-25-03860)\\] employ supervised ML algorithms to model MB dye adsorption by Azolla \\[ [14](http://www.mdpi.com/www.mdpi.com#B14-ijms-25-03860)\\] pinnata, aiming for accurate predictions of adsorption capacity across various conditions. SVR-RBF \\[ [15](http://www.mdpi.com/www.mdpi.com#B15-ijms-25-03860)\\] emerges as the top-performing algorithm, achieving an R value of 0.994 with minimal error.\n\nAs global regulations tighten on water discharge color, the need for affordable dye removal solutions grows. Marzban et al. \\[ [16](http://www.mdpi.com/www.mdpi.com#B16-ijms-25-03860)\\] investigates MB removal using sodium alginate-kaolin beads, assessing various parameters\u2019 impact on efficiency. Characterization and modeling techniques reveal an artificial neural network (ANN) \\[ [17](http://www.mdpi.com/www.mdpi.com#B17-ijms-25-03860)\\] as the most accurate predictor. The developed model aids in optimizing treatment processes, achieving efficient removal of basic dyes. Adsorption kinetics \\[ [18](http://www.mdpi.com/www.mdpi.com#B18-ijms-25-03860)\\] followed a pseudo-second-order model, and encapsulating kaolin in sodium alginate significantly enhanced removal efficiency.\n\nAsfaram et al. \\[ [19](http://www.mdpi.com/www.mdpi.com#B19-ijms-25-03860)\\] investigates zinc sulfide nanoparticles with activated carbon (ZnSNPs-AC) for MB adsorption. Characterization methods and modeling techniques, including RSM \\[ [20](http://www.mdpi.com/www.mdpi.com#B20-ijms-25-03860)\\], ANN, and least squares-support vector machine (LS-SVM), were employed. LS-SVM \\[ [21](http://www.mdpi.com/www.mdpi.com#B21-ijms-25-03860)\\] and ANN models outperformed central composite design (CCD) \\[ [22](http://www.mdpi.com/www.mdpi.com#B22-ijms-25-03860)\\] in predicting MB adsorption efficiency.\n\nInvestigations into the adsorptive removal of MB dye from water using different parts of an Abelmoschus esculentus (lady\u2019s finger) \\[ [23](http://www.mdpi.com/www.mdpi.com#B23-ijms-25-03860)\\] with processed seed powder (LFSP) are described by Nayak et al. \\[ [24](http://www.mdpi.com/www.mdpi.com#B24-ijms-25-03860)\\]. Batch studies evaluated biosorption performance under varying conditions, revealing pseudo-second-order kinetics and intra-particle diffusion as significant factors. Langmuir and Temkin isotherms provided the best fit. Thermodynamic analysis indicated spontaneous and endothermic processes. RSM combined with ANN showed RSM\u2019...",
      "url": "https://www.mdpi.com/1422-0067/25/7/3860"
    },
    {
      "title": "Deep Learning for Deep Chemistry: Optimizing the Prediction of ...",
      "text": "<div><div>\n<h2>Introduction</h2>\n<p>Patterns are ubiquitous in Chemistry. From the crystalline structures of solid forms to the branched chains of lipids, or the complex combinations of functional groups, chemical patterns determine the underlying properties of molecules and materials, essential to address important issues of societal concern. Artificial Intelligence (AI), and machine learning (ML) in particular, are committed to recognizing and learn from these patterns (<a href=\"#B114\">Mitchell, 2014</a>; <a href=\"#B130\">Rupp, 2015</a>; <a href=\"#B59\">Goh et al., 2017</a>; <a href=\"#B99\">Li et al., 2017</a>; <a href=\"#B21\">Butler et al., 2018</a>; <a href=\"#B52\">Fleming, 2018</a>; <a href=\"#B56\">Gao et al., 2018</a>; <a href=\"#B92\">Kishimoto et al., 2018</a>; <a href=\"#B123\">Popova et al., 2018</a>; <a href=\"#B7\">Aspuru-Guzik et al., 2019</a>; <a href=\"#B48\">Elton et al., 2019</a>; <a href=\"#B65\">Gromski et al., 2019</a>; <a href=\"#B106\">Mater and Coote, 2019</a>; <a href=\"#B139\">Schleder et al., 2019</a>; <a href=\"#B157\">Venkatasubramanian, 2019</a>).</p>\n<p>Recent evidence on the most interesting and challenging prospects for accelerating discoveries in various chemistry fields, reported under \u201cCharting a course for chemistry\u201d (<a href=\"#B7\">Aspuru-Guzik et al., 2019</a>), indicate that the terms often used by the scientific community for describing the future trends in their field of research include \u201cbig data,\u201d \u201cmachine learning,\u201d and \u201cartificial intelligence.\u201d</p>\n<p>It is recognized that ML is already boosting computational chemistry, at different levels. Different aspects have been affected, and it is not easy to summarize developments in a consistent way. In what follows, the main areas in which ML has been employed are enumerated. These are extracted from recent contributions, that can be regarded as complementary and providing an overall perspective of the applications. These include different approaches for (i) understanding and controlling chemical systems and related behavior (<a href=\"#B23\">Chakravarti, 2018</a>; <a href=\"#B54\">Fuchs et al., 2018</a>; <a href=\"#B81\">Janet et al., 2018</a>; <a href=\"#B48\">Elton et al., 2019</a>; <a href=\"#B110\">Mezei and Von Lilienfeld, 2019</a>; <a href=\"#B137\">Sanchez-Lengeling et al., 2019</a>; <a href=\"#B157\">Venkatasubramanian, 2019</a>; <a href=\"#B166\">Xu et al., 2019</a>; <a href=\"#B168\">Zhang et al., 2019</a>), (ii) calculating, optimizing, or predicting structure-property relationships (<a href=\"#B156\">Varnek and Baskin, 2012</a>; <a href=\"#B125\">Ramakrishnan et al., 2014</a>; <a href=\"#B59\">Goh et al., 2017</a>; <a href=\"#B145\">Sim\u00f5es et al., 2018</a>; <a href=\"#B25\">Chandrasekaran et al., 2019</a>), density functional theory (DFT) functionals, and interatomic potentials (<a href=\"#B150\">Snyder et al., 2012</a>; <a href=\"#B126\">Ramakrishnan et al., 2015</a>; <a href=\"#B50\">Faber et al., 2017</a>; <a href=\"#B74\">Hegde and Bowen, 2017</a>; <a href=\"#B147\">Smith et al., 2017</a>; <a href=\"#B124\">Pronobis et al., 2018</a>; <a href=\"#B110\">Mezei and Von Lilienfeld, 2019</a>; <a href=\"#B139\">Schleder et al., 2019</a>), (iii) driving generative models for inverse design (i.e., produce stable molecules from a set of desired properties) (<a href=\"#B162\">White and Wilson, 2010</a>; <a href=\"#B14\">Benjamin et al., 2017</a>; <a href=\"#B87\">Kadurin et al., 2017</a>; <a href=\"#B70\">Harel and Radinsky, 2018</a>; <a href=\"#B86\">J\u00f8rgensen et al., 2018b</a>; <a href=\"#B89\">Kang and Cho, 2018</a>; <a href=\"#B101\">Li et al., 2018b</a>; <a href=\"#B136\">Sanchez-Lengeling and Aspuru-Guzik, 2018</a>; <a href=\"#B140\">Schneider, 2018</a>; <a href=\"#B6\">Ar\u00fas-Pous et al., 2019</a>; <a href=\"#B53\">Freeze et al., 2019</a>; <a href=\"#B83\">Jensen, 2019</a>), (iv) screening, synthesizing, and characterizing new compounds and materials (<a href=\"#B3\">Ahneman et al., 2018</a>; <a href=\"#B35\">Coley et al., 2018a</a>; <a href=\"#B61\">Granda et al., 2018</a>; <a href=\"#B142\">Segler et al., 2018</a>; <a href=\"#B100\">Li and Eastgate, 2019</a>), (v) improving catalytic technologies and analytical tools (<a href=\"#B99\">Li et al., 2017</a>; <a href=\"#B56\">Gao et al., 2018</a>; <a href=\"#B78\">Huang et al., 2018</a>; <a href=\"#B43\">Durand and Fey, 2019</a>; <a href=\"#B53\">Freeze et al., 2019</a>; <a href=\"#B139\">Schleder et al., 2019</a>), (vi) developing quantum algorithms for molecular simulations, and (vii) progressing quantum sensing (<a href=\"#B125\">Ramakrishnan et al., 2014</a>; <a href=\"#B127\">Ramakrishnan and Von Lilienfeld, 2017</a>; <a href=\"#B165\">Xia and Kais, 2018</a>; <a href=\"#B2\">Ahn et al., 2019</a>; <a href=\"#B31\">Christensen et al., 2019</a>; <a href=\"#B110\">Mezei and Von Lilienfeld, 2019</a>; <a href=\"#B167\">Zaspel et al., 2019</a>; <a href=\"#B168\">Zhang et al., 2019</a>), just to name a few examples. In fact, Chemistry is a data-rich area, encompassing complex information which is often unstructured and poorly understood.</p>\n<p>Deep learning (DL) approaches can also be particularly useful to solving a variety of chemical problems, including compound identification and classification, and description of soft matter behavior (<a href=\"#B78\">Huang et al., 2018</a>; <a href=\"#B84\">Jha et al., 2018</a>; <a href=\"#B86\">J\u00f8rgensen et al., 2018b</a>; <a href=\"#B123\">Popova et al., 2018</a>; <a href=\"#B142\">Segler et al., 2018</a>; <a href=\"#B169\">Zhou et al., 2018</a>; <a href=\"#B25\">Chandrasekaran et al., 2019</a>; <a href=\"#B41\">Degiacomi, 2019</a>; <a href=\"#B48\">Elton et al., 2019</a>; <a href=\"#B58\">Ghosh et al., 2019</a>; <a href=\"#B106\">Mater and Coote, 2019</a>; <a href=\"#B107\">Matsuzaka and Uesawa, 2019</a>; <a href=\"#B166\">Xu et al., 2019</a>).</p>\n<p>The design of generalized cause/effect models, and the scaling-up of the contributions that are being made, containing high-dimensional data, and following the open-science basis (i.e., completely accessible, with precise metadata and practical formats) are critical challenges, that may, however, facilitate the routine implementation of data mining in chemistry and expedite new discoveries.</p>\n<p>The amount and quality of chemical data generated by experiments and simulations have been the mainstay of the new data-driven paradigm, that establishes the bridge between theory, experiment, computation, and simulation.</p>\n<p>This review describes, in a critical and comprehensive way, relevant contributions carried out recently and involving the development of chemistry ML approaches. An exhaustive account of the theoretical foundations and applications published in the early years of AI and ML in Chemistry falls beyond the scope of this review. The reader is referred to <a href=\"#B96\">Lecun et al. (2015)</a>, <a href=\"#B38\">Coveney Peter et al. (2016)</a>, <a href=\"#B59\">Goh et al. (2017)</a>, <a href=\"#B48\">Elton et al. (2019)</a>, <a href=\"#B65\">Gromski et al. (2019)</a>, and <a href=\"#B106\">Mater and Coote (2019)</a> for a full description of these efforts.</p>\n<p>Until 10 years ago, only a few 100 studies on the use of ML in Chemistry were published, resulting from the contributions made over four decades. In 2018, ca. 8,000 articles in the Web of Science database included these keywords, corresponding to an increase in ca. 35% for just one decade. In this review, there is room to mention only a small fraction of these applications.</p>\n<p>Despite the increasing number of works on the topic, the models proposed and practices carried out by chemists are entailing serious concerns (<a href=\"#B32\">Chuang and Keiser, 2018a</a>). Several technical challenges, pitfalls, and potentials of ML, and also the reliability of the results, have been discussed by some authors (<a href=\"#B3\">Ahneman et al., 2018</a>; <a href=\"#B32\">Chuang and Keiser, 2018a</a>,<a href=\"#B33\">b</a>; <a href=\"#B49\">Estrada et al., 2018</a>) corroborating some critical remarks on the fragility of purely data-based approaches (<a href=\"#B111\">Microsoft, 2018</a>). \u201cIf data can speak for themselves, th...",
      "url": "https://www.frontiersin.org/journals/chemistry/articles/10.3389/fchem.2019.00809/full"
    },
    {
      "title": "Transfer learning for solvation free energies: From quantum chemistry to experiments",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS1385894721008925)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S1385894721008925/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-snippets)\n- [References (34)](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-references)\n- [Cited by (108)](https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/chemical-engineering-journal)\n\n## [Chemical Engineering Journal](https://www.sciencedirect.com/journal/chemical-engineering-journal)\n\n[Volume 418](https://www.sciencedirect.com/journal/chemical-engineering-journal/vol/418/suppl/C), 15 August 2021, 129307\n\n[![Chemical Engineering Journal](https://ars.els-cdn.com/content/image/1-s2.0-S1385894721X00095-cov150h.gif)](https://www.sciencedirect.com/journal/chemical-engineering-journal/vol/418/suppl/C)\n\n# Transfer learning for solvation free energies: From quantum chemistry to experiments\n\nAuthor links open overlay panelFlorence H.Vermeire, [William H.Green](https://www.sciencedirect.com/author/7402259989/william-h-green)\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.cej.2021.129307](https://doi.org/10.1016/j.cej.2021.129307) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S1385894721008925&orderBeanReset=true)\n\n## Highlights\n\n- \u2022\nMachine learning for solvation free energies.\n\n- \u2022\nDatabase of COSMO-RS calculations and experimental solvation free energies.\n\n- \u2022\nAleatoric uncertainty or test data noise as a limit to model performance.\n\n- \u2022\nTransfer learning to compensate for small dataset sizes.\n\n- \u2022\nTransfer learning for improved out-of-sample predictions.\n\n\n## Abstract\n\nData scarcity, bias, and experimental noise are all frequently encountered problems in the application of deep learning to chemical and material science disciplines. Transfer learning has proven effective in compensating for the lack in data. The use of quantum calculations in machine learning enables the generation of a diverse dataset and ensures that learning is less affected by noise inherent to experimental databases. In this work, we propose a transfer learning approach for the prediction of solvation free energies that combines fundamentals from quantum calculations with the higher accuracy of experimental measurements using two new databases CombiSolv-QM and CombiSolv-Exp. The employed model architecture is based on the directed-message passing neural network for the molecular embedding of solvent and solute molecules. A significant advantage of models pre-trained on quantum calculations is demonstrated for small experimental datasets and for out-of-sample predictions. The improved out-of-sample performance is shown for new solvents, for new solute elements, and for the extension to higher molar mass solutes. The overall performance of the pre-trained models is limited by the noise in the experimental test data, known as the aleatoric uncertainty. On a random test split, a mean absolute error of 0.21 kcal/mol is achieved. This is a significant improvement compared to the mean absolute error of the quantum calculations (0.40 kcal/mol). The error can be further reduced to 0.09 kcal/mol if the model performance is assessed on a more accurate subset of the experimental data.\n\n## Introduction\n\nDeep learning has emerged as an effective technique for property prediction in the field of chemical engineering and material science. In the last decade, many efforts have been made to replace structure-based estimation methods by deep neural networks\u00a0\\[1\\], \\[2\\]. One major problem that is often encountered is data scarcity. Compared to other disciplines like image recognition and natural language processing, the availability and size of datasets in chemical engineering and material science are very limited. Transfer learning has been proposed as a technique to solve the problem of the low data regime\u00a0\\[3\\]. Success has been demonstrated in other disciplines, such as the transfer of knowledge from general image recognition to more specific medical imaging. Data scarcity is not the only problem related to the experimental nature of databases in chemical engineering and material science. They are often biased towards certain groups of components, cover only a limited domain of chemical space, and have an uncertainty associated with the experimental nature of the data. With transfer learning and the use of quantum chemical calculations, one can compensate for this bias and cover a larger chemical space by generating additional and diverse data.\n\nThe advantage of transfer learning with respect to data scarcity in chemical engineering and material science has been demonstrated in recent work. Within quantum machine learning, transfer learning has been used to calculate thermodynamic properties of molecules in vacuum at the coupled cluster level of theory, the gold standard of quantum chemical calculations. This has been done by Grambow et\u00a0al.\u00a0\\[4\\] and Smith et\u00a0al.\u00a0\\[5\\] Large DFT-based datasets have been used to pre-train models that were further fine-tuned on computationally expensive coupled cluster calculations. Ma et\u00a0al.\u00a0\\[6\\] demonstrated the advantage of transfer learning for gas adsorption on metal organic frameworks. Parameters were transferred from a model trained on a large dataset of hydrogen gas adsorption at 100 bar and 243 K to initialize the parameters of a model fine-tuned at 130 K and for methane adsorption with a smaller dataset. This technique has also been effective at transferring knowledge between disciplines of materials. Yamada et\u00a0al.\u00a0\\[7\\] proposed a shot-gun transfer learning approach where models trained on small molecules were used to aid learning of polymer properties, and models trained on organic materials were used to aid learning of inorganic material properties. Jha et\u00a0al.\u00a0\\[8\\] used transfer learning to predict the formation enthalpy of crystal structures starting from the elemental composition. In their approach, a model was pre-trained on a large dataset of DFT calculations. All model parameters were used to initialize a new model that was fine-tuned on two other smaller DFT databases and an experimental database.\n\nIn this work, we introduce an inductive transfer learning approach with the transfer of model parameters from models trained on quantum chemistry calculations to models trained on experimental data, similar to the approach reported by Jha et\u00a0al.\u00a0\\[8\\] The transfer learning algorithm uses inductive biases from the quantum chemistry data to improve learning of small and biased experimental datasets. In the present work, the transfer learning method is applied to the prediction of solvation free energies in a variety of solvents. For the purpose of this work we provide two databases in supplementary material: (i) CombiSolv-QM - a quantum chemistry database with solvation free energies for 1 million solvent/solute combinations, calculated according to the COSMO-RS theory, and (ii) CombiSolv-Exp - an experimental database with 10\u00a0145 solvent/solute combinations c...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1385894721008925"
    },
    {
      "title": "Solvent selection for polymers enabled by generalized chemical fingerprinting and machine learning",
      "text": "This journal is \u00a9 the Owner Societies 2022 Phys. Chem. Chem. Phys., 2022, 24, 26547\u201326555 | 26547\nCite this: Phys. Chem. Chem. Phys.,\n2022, 24, 26547\nSolvent selection for polymers enabled by\ngeneralized chemical fingerprinting and machine\nlearning\u2020\nJoseph Kern, Shruti Venkatram, Manali Banerjee, Blair Brettmann and\nRampi Ramprasad*\nWe present machine learning models trained on experimental data to predict room-temperature\nsolubility for any polymer\u2013solvent pair. The new models are a significant advancement over past data\u0002driven work, in terms of protocol, validity, and versatility. A generalizable fingerprinting method is used\nfor the polymers and solvents, making it possible, in principle, to handle any polymer\u2013solvent combi\u0002nation. Our data-driven approach achieves high accuracy when either both the polymer and solvent or\njust the polymer has been seen during the training phase. Model performance is modest though when a\nsolvent (in a newly queried polymer\u2013solvent pair) is not part of the training set. This is likely because the\nnumber of unique solvents in our data set is small (much smaller than the number of polymers). Never\u0002theless, as the data set increases in size, especially as the solvent set becomes more diverse, the overall\npredictive performance is expected to improve.\n1 Introduction\nSolvent selection is an important component of polymer synth\u0002esis and processing as well as a multitude of polymer applica\u0002tions like microlithography, membrane formation, drug\ndelivery systems, recycling, and waste processing.1 In micro\u0002lithography, polymers are exposed to electromagnetic radiation\nto cause changes to their chemical structure, then immersed in\na solvent to dissolve either the exposed or unexposed region.2\nWhen forming membranes using non-solvent induced phase\nseparation, a polymer is dissolved in a solvent to create a\nhomogeneous dope solution. This solution is cast as a liquid\nfilm on a substrate which is then placed in a coagulation non\u0002solvent bath to remove the solvent and form the membrane.3 In\ndrug delivery systems, water soluble polymers are used to\nincrease the solubility of poorly soluble drugs by dispersing\nthe drug in the polymer structure.4,5 Water soluble polymers\nare also used as stabilisers and mechanical supports for sus\u0002tained release of drugs.6 In efforts to chemically recycle indust\u0002rially relevant polymers like polystyrene and high-density\npolyethylene in a single process stream, solvent selection was\nidentified as the most critical parameter.7 In water treatment,\nwater-soluble polymeric materials are used to remove heavy\nmetal ions and arsenic.8 Water soluble polymers used in\ncosmetics and laundry detergents can also leach into the\nenvironment and need to be removed via sorption.9 When\nusing solvents during the creation of polymers, toxic solvents\ncan remain in the polymers post-processing and come into\ncontact with humans, so ideally safe alternatives to commonly\nused solvents could also be determined for every polymer.10\nGiven how important polymer solubility is in these numerous\nprocesses, it is critical to have a method of estimating what\nsolvents will dissolve polymers.\nTo enable informed solvent identification, several empirical\nmethods have been proposed with varying degrees of success,\nincluding the Hildebrand and Hansen methods. In the Hildeb\u0002rand method, polymers and solvents with similar Hildebrand\nparametric values (which are related to the cohesive energy\ndensity) are predicted as good solvents and those with values\ndiffering by more than a threshold are predicted as bad\nsolvents.1,11 In the Hansen method, the difference between\nthree parameters quantifying dispersion, dipolar, and hydro\u0002gen bonding interactions for the polymer and solvent are used\nto provide an estimate of the solubility of the polymer in the\nsolvent.12 Previous work by Venkatram et al. showed the\nHansen method performed only marginally better than\nthe Hildebrand method despite its greater complexity.13\nSeveral computational approaches have been used to esti\u0002mate these solubility parameters, including group contribution\nmethods14 and a variety of machine learning techniques.\nSchool of Materials Science and Engineering, College of Engineering, Georgia\nInstitute of Technology, 771 Ferst Drive, J. Erskine Love Building, Atlanta, GA\n30332-0245, USA. E-mail: rampi.ramprasad@mse.gatech.edu\n\u2020 Electronic supplementary information (ESI) available. See DOI: https://doi.org/\n10.1039/d2cp03735a\nReceived 13th August 2022,\nAccepted 22nd October 2022\nDOI: 10.1039/d2cp03735a\nrsc.li/pccp\nPCCP\nPAPER\nPublished on 31 October 2022. Downloaded on 11/6/2025 1:28:45 AM. View Article Online View Journal | View Issue\n26548 | Phys. Chem. Chem. Phys., 2022, 24, 26547\u201326555 This journal is \u00a9 the Owner Societies 2022\nSanchez-Lengeling et al. used Gaussian process regression with\nMorgan and MACCS fingerprints to estimate Hansen solubility\nparameters for 31 polymers and 193 solvents, achieving reason\u0002able R2 performance (0.56\u20130.83).15 Kurotani et al. used only four\npieces of analytical data to predict Hansen solubility para\u0002meters for polymers as well, and while their R2 performance\nwas lower than Sanchez-Lengeling et al., they posited that it\nmay be useful for new polymers with unknown SMILES\nstrings.16 Others have used descriptors derived from atomic\nstructure and quantum chemical calculation for small mole\u0002cules representing polymer repeat units to predict the Hil\u0002debrand solubility parameters using kernel ridge regression\nand multi-linear regression models.17 Most recently, Liu\net al. collected data on 81 polymers and 1221 solvents and\ncreated more easily interpretable regression models to pre\u0002dict Chi, Hildebrand, and Hansen parameters.18 They fea\u0002turized their polymers and solvents using RDKit generated\nchemical fingerprints such as the count, density and\nweighted sum for atoms, and 2nd order features generated\nfrom the 3d structures of trimers and solvents such as\nLUMO, HOMO and heat of formation.\nMeanwhile, previous work by Chandrasekaran et al.,\n19\ninspired by the promise of machine learning in the materials\ndomain,20,21 showed that a deep neural network trained on a\ndata set of 4595 polymers and 24 solvents could dramatically\noutperform the Hildebrand approach to estimating\nsolubility.1,11 This method utilized chemical features to repre\u0002sent polymers and a one-hot (label-based) encoding to repre\u0002sent the solvents which were used to train a multilayer\nperceptron neural network that could classify whether a parti\u0002cular polymer\u2013solvent combination was soluble or insoluble.\nThey also found that a Hildebrand Gaussian process model\u2019s\nclassification accuracy was much worse than the neural net\u0002work, correctly classifying good-solvents only 50% of the time\nand bad solvents 70% as opposed to the over 90% accuracy for\nthe neural network. The downside to their neural network\napproach, however, was that solubility estimations could be\nperformed for only the 24 solvents within the training data due\nto the one-hot encoding; i.e., predictions could not be general\u0002ized to cases outside the list of 24 solvents.\nIn the present work, we demonstrate a method to generalize\nmachine learning models to any solvent so predictions can be\nmade on previously unseen solvents. Moreover, we perform a\ncritical analysis of the strengths and weaknesses of any such\ndata-driven approach and identify situations where caution is\nmandated. First, a data set of 3373 polymers and 51 solvents\nwas collected. Next, we structurally fingerprinted the polymers\nand solvents using a hierarchical methodology that is general\u0002izable to any polymer\u2013solvent pair, unlike the one-hot encoding\nmethod. Finally, these fingerprints and solubility data were\nused to train a random forest classifier and a deep neural\nnetwork binary classifier that predicts if a polymer\u2013solvent pair\nis soluble or insoluble. A production version of the random\nforest model has been deployed to our online polymer infor\u0002matics platform, polyme...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/cp/d2cp03735a"
    }
  ]
}