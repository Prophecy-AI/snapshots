# Seed Prompt: Chemical Reaction Yield Prediction - Loop 4

## Current Status
- Best CV score: 0.0105 from exp_003 (Combined Spange + DRFP + Arrhenius)
- Best LB score: 0.0972 from exp_003
- CV-LB gap: ~9x ratio (CV 0.0105 → LB 0.0972)
- Target: 0.0333 (need 66% improvement from current best LB)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The implementation is sound.
- Evaluator's top priority: Submit to Kaggle for LB validation. DONE - LB 0.0972 achieved.
- Key concerns raised:
  1. CV-LB gap uncertainty - CONFIRMED: 9x gap persists
  2. Single solvent performance degraded with combined features - Valid, but overall improved
  3. Notebook template compliance - Must ensure last 3 cells match exactly
- Evaluator recommended task-specific feature sets. This is worth exploring.

## Critical Insight: The Gap is Too Large for Incremental Improvements
- Current best LB: 0.0972
- Target: 0.0333
- Required improvement: 66% (3x better)
- CV improvements translate poorly to LB (~0.2x translation ratio)
- At current rate (1% LB improvement per experiment), would need 66+ experiments
- **NEED FUNDAMENTALLY DIFFERENT APPROACH**

## Data Understanding
**Reference notebooks:**
- `exploration/eda.ipynb` - Data shapes, target distributions, CV structure
- `exploration/evolver_loop4_lb_feedback.ipynb` - CV-LB gap analysis

**Key data characteristics:**
- Single solvent: 656 samples, 24 solvents (leave-one-solvent-out CV)
- Full/mixture: 1227 samples, 13 ramps (leave-one-ramp-out CV)
- Targets: SM, Product 2, Product 3 (yields 0-1)
- Temperature: 175-225°C, Residence Time: ~2-15 min

## Competition Template Constraints (CRITICAL)
The last 3 cells MUST be exactly as in the template. Only the model definition line can change:
```python
model = MLPModel()  # CHANGE THIS LINE ONLY
```
Model class must implement:
- `train_model(X_train, Y_train)` method
- `predict(X_test)` method returning tensor/array of shape [N, 3]

All complexity (featurization, ensembling, etc.) must be inside the model class.

## What's Working
1. **MLP > LightGBM** - Tree models struggle with leave-one-solvent-out generalization
2. **Arrhenius kinetics features** - Physics-informed features are essential
3. **Spange descriptors** - 13 physicochemical features work well
4. **TTA for mixtures** - Averaging predictions from both orderings helps
5. **Combined Spange + DRFP** - Marginal improvement (1% LB)

## What's NOT Working
1. **PCA on DRFP** - Destroys sparse fingerprint information
2. **LightGBM** - Worse than MLP on LB (0.1065 vs 0.0982)
3. **Incremental feature engineering** - Too slow to reach target

## Recommended Approaches (Priority Order)

### 1. DEEPER MLP WITH STRONGER REGULARIZATION (HIGH PRIORITY)
The current MLP may be underfitting. Try:
- Deeper architecture: [512, 256, 128, 64] with residual connections
- More aggressive dropout (0.4-0.5)
- Stronger weight decay (1e-3)
- More epochs (500+) with early stopping
- Cosine annealing learning rate schedule
- **Rationale**: GNN benchmark achieved 0.0039, proving much better performance is possible. A deeper MLP with proper regularization might capture more complex patterns.

### 2. ENSEMBLE OF DIVERSE MODELS (HIGH PRIORITY)
Create an ensemble that combines:
- Multiple MLPs with different architectures
- Different feature subsets (Spange-only, DRFP-only, combined)
- Different random seeds (10+ models)
- **Rationale**: Ensembling reduces variance. The CV-LB gap may be partly due to model variance.

### 3. PER-TARGET MODELS (MEDIUM PRIORITY)
Train separate models for each target (SM, Product 2, Product 3):
- Different targets may have different optimal features/architectures
- Competition allows different hyperparameters for different objectives
- **Rationale**: SM and Products may have different relationships with solvent properties.

### 4. TASK-SPECIFIC MODELS (MEDIUM PRIORITY)
Use different configurations for single vs mixture data:
- Single solvent: Spange + Arrhenius (proven to work well)
- Mixture: Spange + DRFP + Arrhenius (better for mixtures)
- **Rationale**: Evaluator noted single solvent performance degraded with combined features.

### 5. ATTENTION MECHANISM (LOWER PRIORITY)
Add self-attention layer to capture feature interactions:
- Attention over solvent features
- Multi-head attention for different interaction types
- **Rationale**: GNN benchmark used attention (GAT). Simple attention in MLP might help.

## What NOT to Try
- **LightGBM/XGBoost** - Already proven worse on LB (0.1065)
- **PCA on DRFP** - Destroys information (CV 0.017)
- **Simple hyperparameter tuning** - Diminishing returns, won't close 66% gap

## Validation Notes
- CV-LB ratio is ~9x consistently
- To beat target 0.0333, need CV ~0.0037
- Current best CV is 0.0105 - need 64% CV improvement
- Focus on approaches that could give 2-3x improvement, not 5-10%

## Implementation Notes
- All model code must be in a single class that fits the template
- Use GPU (CUDA) for faster training
- Set random seeds for reproducibility
- Use HuberLoss for robustness to outliers
- Batch normalization helps with feature scale differences

## Next Experiment Priority
1. **Try deeper MLP with residual connections** - Could unlock better feature learning
2. **Try 10+ model ensemble** - Reduce variance
3. **Try per-target models** - Different targets may need different approaches

The key insight is that we need a BREAKTHROUGH, not incremental improvement. The GNN benchmark proves 0.0039 is achievable. Focus on approaches that could give 2-3x improvement.
