## Current Status
- Best CV score: 0.0111 from exp_000 (Baseline MLP with Arrhenius Kinetics + TTA)
- Best LB score: 0.0982 (from exp_000)
- CV-LB gap: -0.0871 → **MASSIVE GAP - CV is unreliable for this competition**

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** with one critical compliance issue. I agree with the evaluator's assessment.

**Evaluator's top priority: RESTRUCTURE THE NOTEBOOK TO COMPLY WITH COMPETITION TEMPLATE**

I **DISAGREE** that this is the top priority. Here's why:
1. The reference kernel (arrhenius-kinetics-tta) that achieves LB 0.09831 **ALSO does not follow the template** - it has all code in one cell
2. Our submission got LB 0.0982, which is nearly identical to the reference kernel's 0.09831
3. This proves our submission WAS evaluated correctly despite not following the template exactly

**Key concerns raised:**
1. Notebook structure non-compliance → **Not the actual issue** - our submission was evaluated
2. DRFP features unexplored → **AGREE - this is high priority**
3. Suboptimal hyperparameters (3 models, 200 epochs) → **AGREE - should increase**

**The REAL issue is the massive CV-LB gap (0.0111 vs 0.0982):**
- Our local CV calculation is mathematically correct (verified in analysis)
- The LB runs the ENTIRE notebook from scratch on Kaggle
- Different random seeds, environment, and GPU behavior cause different results
- The model has HIGH VARIANCE between runs

## Data Understanding
- Reference notebooks: `exploration/eda.ipynb`, `exploration/evolver_loop1_lb_feedback.ipynb`
- Single solvent: 656 samples, 24 solvents (leave-one-solvent-out CV)
- Full/mixture: 1227 samples, 13 ramps (leave-one-ramp-out CV)
- Targets: Product 2, Product 3, SM (yields 0-1)
- **CRITICAL**: Local CV is NOT predictive of LB score due to model variance

## Recommended Approaches (Priority Order)

### 1. **REDUCE MODEL VARIANCE** (HIGHEST PRIORITY)
The 9x CV-LB gap indicates our model has extremely high variance. To fix:
- **Set random seeds explicitly** for reproducibility: `torch.manual_seed(42)`, `np.random.seed(42)`
- **Increase bagging** from 3 to 7+ models (reference uses 7)
- **Increase epochs** from 200 to 300 (reference uses 300)
- **Use deterministic operations** in PyTorch: `torch.backends.cudnn.deterministic = True`

### 2. **Try LightGBM as alternative** (HIGH PRIORITY)
LightGBM achieved MSE as low as 0.001 on some folds in the reference kernel:
- More stable/deterministic than neural networks
- Per-target regressors (3 separate models)
- Early stopping with 100 rounds patience
- Hyperparameters: lr=0.03, max_depth=6, regularization
- Should have much lower variance between runs

### 3. **Try DRFP features** (MEDIUM PRIORITY)
2048-dimensional differential reaction fingerprints:
- Achieved MSE ~0.0039 in GNN benchmarks
- May need PCA for dimensionality reduction
- Combine with Arrhenius kinetics features

### 4. **Ensemble MLP + LightGBM** (MEDIUM PRIORITY)
Combine diverse model types for robustness:
- Average predictions from MLP and LightGBM
- Different model families capture different patterns
- Reduces overall variance

## What NOT to Try
- **Don't obsess over template compliance** - our submission was evaluated correctly
- **Don't trust local CV scores** - they are not predictive of LB performance
- **Don't use fewer than 5 models** for bagging - variance is too high

## Validation Notes
- **LOCAL CV IS UNRELIABLE** for this competition
- The LB runs the entire notebook from scratch
- Focus on reducing model variance, not optimizing local CV
- Use deterministic settings and more bagging
- Consider LightGBM for more stable predictions

## Key Insight
The target score is 0.0333. Our LB score is 0.0982. We need to improve by ~3x.
The reference MLP kernel also gets ~0.098, so MLP alone may not be enough.
LightGBM or ensemble approaches may be needed to beat the target.