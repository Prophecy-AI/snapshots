# Seed Prompt: Chemical Reaction Yield Prediction - Loop 2

## Current Status
- Best CV score: 0.0111 from exp_000 (MLP)
- Best LB score: 0.0982 from exp_000 (MLP)
- CV-LB gap: -0.0871 â†’ **MASSIVE GAP - CV does not predict LB performance**
- Target: 0.0333 (need ~66% improvement from current best LB)

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator correctly identified that the LightGBM experiment was sound.

**Evaluator's hypothesis was WRONG:** The evaluator hypothesized that LightGBM's deterministic nature would translate to better LB performance. This was disproven:
- MLP LB: 0.0982
- LightGBM LB: 0.1065 (WORSE, not better)

**Key insight:** Tree models (LightGBM) generalize WORSE to unseen solvents than neural networks. The leave-one-solvent-out CV requires the model to extrapolate to completely unseen chemical environments. Neural networks with continuous embeddings handle this better than tree-based models that rely on discrete splits.

**Evaluator's concern about notebook structure:** Valid. The competition requires EXACT template structure in the last 3 cells. Our notebooks may not comply. This needs to be fixed.

## Critical Realization: CV-LB Gap

The massive CV-LB gap (0.011 vs 0.098) is NOT due to model variance. Both MLP and LightGBM show similar gaps. The gap exists because:

1. **Our local CV calculation may differ from competition evaluation**
2. **The reference kernel achieves the same LB score (0.098)** - our implementation is correct
3. **The target of 0.0333 is achievable** - GNN benchmark achieved 0.0039

## Data Understanding
**Reference notebooks:**
- `exploration/eda.ipynb` - Data shapes, CV structure, target distributions
- `exploration/evolver_loop1_analysis.ipynb` - DRFP feature analysis
- `exploration/evolver_loop2_lb_feedback.ipynb` - CV-LB gap analysis

**Key data facts:**
- Single solvent: 656 samples, 24 solvents (24-fold leave-one-solvent-out CV)
- Full data: 1227 samples, 13 ramps (13-fold leave-one-ramp-out CV)
- Targets: Product 2, Product 3, SM (yields 0-1)
- All 24 solvents have DRFP features (2048-dim, 97.4% sparse)

## What's Working
1. **MLP with Arrhenius kinetics** - Achieves LB 0.0982 (matches reference kernel)
2. **Chemical symmetry handling** - Data augmentation + TTA for mixtures
3. **Spange descriptors** - 13 compact features, good baseline

## What's NOT Working
1. **LightGBM** - Worse LB than MLP (0.1065 vs 0.0982). Tree models don't generalize well to unseen solvents.
2. **Local CV as LB predictor** - Massive gap makes local CV unreliable for model selection

## Recommended Approaches (Priority Order)

### 1. DRFP Features with MLP (HIGH PRIORITY)
**Why:** GNN benchmark achieved MSE 0.0039 using DRFP features. This is 25x better than our current LB.
**How:**
- Load DRFP features (2048-dim) from `drfps_catechol_lookup.csv`
- Combine with Arrhenius kinetics features (5 features)
- Use PCA to reduce dimensionality if needed (try 50, 100, 200 components)
- Train MLP with same architecture as baseline

**Expected improvement:** Significant - DRFP captures molecular structure information that Spange descriptors miss.

### 2. Combined Features: DRFP + Spange + Arrhenius (MEDIUM PRIORITY)
**Why:** Different feature types capture different aspects of chemistry.
**How:**
- Concatenate: DRFP (PCA-reduced) + Spange (13) + Arrhenius (5)
- Total: ~70-220 features depending on PCA components
- May need regularization to prevent overfitting

### 3. Ensemble: MLP + Different Feature Sets (MEDIUM PRIORITY)
**Why:** Diversity in features leads to diversity in predictions.
**How:**
- Train MLP with Spange features (current baseline)
- Train MLP with DRFP features
- Average predictions

### 4. Hyperparameter Tuning for MLP (LOW PRIORITY)
**Why:** Current architecture may not be optimal for DRFP features.
**What to try:**
- Larger hidden layers for high-dimensional DRFP input
- Different dropout rates
- More/fewer bagging models

## What NOT to Try
1. **LightGBM/XGBoost** - Already proven to generalize worse than MLP on this task
2. **GNN from scratch** - Too complex, requires molecular graph construction
3. **Chasing local CV** - Local CV doesn't predict LB; focus on LB-validated approaches

## Validation Notes
- **DO NOT trust local CV for model selection** - Use LB submissions for validation
- **Template compliance is CRITICAL** - Last 3 cells must match template exactly
- **MLP is the baseline to beat** - Any new approach must beat LB 0.0982

## Competition Template Requirements
The model class MUST implement:
- `train_model(X_train, Y_train)` - Training method
- `predict(X_test)` - Returns predictions (numpy array or tensor)

The last 3 cells must be EXACTLY as in the template:
1. Single solvent CV loop with `model = YourModel()` 
2. Full data CV loop with `model = YourModel(data='full')`
3. Submission file creation

## Next Steps
1. **Implement DRFP-based MLP** - This is the highest-leverage improvement
2. **Ensure template compliance** - Critical for valid submission
3. **Submit to validate** - Only LB scores matter for model selection

## Target Path
- Current best LB: 0.0982
- Target: 0.0333
- GNN benchmark: 0.0039

The path to beating the target is through better features (DRFP), not better models. The MLP architecture is sufficient; the bottleneck is feature representation.