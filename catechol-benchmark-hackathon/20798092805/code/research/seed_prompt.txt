## Current Status
- Best CV score: 0.0093 from exp_007 (Even Simpler [32,16])
- Best LB score: 0.0932 from exp_007 (JUST SUBMITTED)
- CV-LB gap: 10.02x ratio (slightly higher than avg 9.31x)
- Target: 0.0333 (need 64% LB improvement from current best)
- Submissions: 6/5 used, 1 remaining

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The even simpler model experiment executed correctly and achieved the best CV score.

**Evaluator's top priority: SUBMIT TO LB AND CONTINUE SIMPLIFYING IF SUCCESSFUL.**
COMPLETED. exp_007 was submitted and achieved LB 0.0932 - NEW BEST LB!

**Key insights from submission:**
1. **CV-LB correlation is 0.97** (p=0.0013) - VERY STRONG
2. **Simplification trend VALIDATED on LB:**
   - exp_003 [256,128,64]: LB 0.0972
   - exp_006 [64,32]: LB 0.0946 (2.7% better)
   - exp_007 [32,16]: LB 0.0932 (1.5% better)
3. **Each simplification step improves BOTH CV and LB**
4. **The ratio is increasing slightly** (9.26x → 9.75x → 10.02x) - simpler models may have slightly worse CV-LB ratio

**Evaluator's concerns addressed:**
1. **LB validation** - DONE. exp_007 achieved LB 0.0932 (best so far)
2. **Diminishing returns** - Improvement rate slowed (2.7% → 1.5%), but still improving
3. **Linear model not tried** - NEXT PRIORITY

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop8_lb_feedback.ipynb` - CV-LB correlation analysis
- `experiments/008_even_simpler/even_simpler.ipynb` - Best CV experiment

Key patterns:
1. **Simpler models generalize better** - STRONGLY VALIDATED across 3 experiments AND on LB
2. **CV-LB ratio is ~9.31x** (std: 0.52) - highly consistent
3. **Best features**: Spange (13) + DRFP high-variance (122) + Arrhenius (5) = 140 features
4. **To beat target 0.0333, need CV ≤ 0.00358** (61.5% improvement from current 0.0093)
5. **Optimal model capacity is MUCH lower than expected** - 5K params beats 77K params

## Recommended Approaches

### Priority 1: Continue Simplification - Try Linear Model
**Why:** The simplification trend is VALIDATED on LB. Each step improves both CV and LB.
**How:**
- Try Ridge Regression (linear model with L2 regularization)
- Same features (Spange + DRFP + Arrhenius = 140 features)
- Per-target regressors (3 separate models)
- Tune alpha (regularization strength) via nested CV
- Very fast to train, deterministic
**Expected outcome:** May find optimal simplicity level. If linear works, confirms overfitting hypothesis.

### Priority 2: Try Single Hidden Layer [16]
**Why:** If linear doesn't work, try minimal non-linearity
**How:**
- MLP with single hidden layer [16]
- Same features (Spange + DRFP + Arrhenius)
- Minimal dropout (0.0 or 0.05)
- 5 models bagged
**Expected outcome:** May be the sweet spot between linear and [32,16]

### Priority 3: Try [16,8] Architecture
**Why:** Continue the simplification trend
**How:**
- MLP [16,8] (2.4K params vs 5K for [32,16])
- Same features
- Minimal dropout
- 5 models bagged
**Expected outcome:** May improve further if [32,16] wasn't optimal

### Priority 4: Feature Selection for Linear Model
**Why:** With linear model, feature selection becomes more important
**How:**
- Use Lasso (L1 regularization) to identify important features
- Try ElasticNet (L1 + L2)
- Remove low-importance features
**Expected outcome:** May improve linear model performance

## What NOT to Try

1. **Larger ensembles** - Diminishing returns proven (15 models only 0.7% better than 5)
2. **Deep architectures** - exp_004 proved this hurts badly (5x worse)
3. **Residual connections** - Not appropriate for this tabular data
4. **More epochs** - Already at 200, diminishing returns
5. **Higher dropout** - Simpler model with lower dropout works better
6. **Complex feature engineering** - Current features are working well
7. **GNN/Transformer** - Would require significant code changes, unlikely to fit template
8. **LightGBM** - Already tried (exp_001), worse than MLP

## Validation Notes

- CV scheme: Leave-one-solvent-out (24 folds for single, 13 folds for mixtures)
- CV-LB ratio: ~9.31x consistently (std: 0.52)
- **CRITICAL:** Simpler models have BETTER CV AND BETTER LB. Continue simplifying.
- To beat target 0.0333, need CV ≤ 0.00358 (61.5% improvement from current 0.0093)

## Competition Constraints (CRITICAL)

The competition template requires:
1. **Last 3 cells must match template exactly**
2. Only the model definition line can change
3. Model must have `train_model(X_train, Y_train)` and `predict(X_test)` methods
4. Same hyperparameters across all folds (unless explainable rationale)

## Final Submission Strategy

With 1 submission remaining:
1. **Try linear model (Ridge Regression)** - fastest to test
2. **If linear doesn't improve, try [16] or [16,8]**
3. **Submit whichever has best CV**
4. **Accept that beating target (0.0333) may require fundamentally different approach**

## Reality Check

The target (0.0333) is very challenging:
- Current best LB: 0.0932
- Target: 0.0333
- Gap: 2.8x (0.0932 → 0.0333)

**Key insight from web research:**
- Few-shot learning, meta-learning, and transfer learning are recommended for extrapolating to unseen solvents
- Gaussian processes with uncertainty quantification are recommended
- Simple, regularized models work better for small datasets
- The GNN benchmark (0.0039) suggests graph-based approaches are needed for breakthrough performance

**However, given constraints:**
- Template requires specific model interface
- Only 1 submission remaining
- The simplification trend is VALIDATED and continues to improve

**Strategic priority:**
1. Try linear model (Ridge Regression) - may be optimal simplicity
2. If not, try [16] or [16,8]
3. Submit best CV performer
4. Accept that beating target may require fundamentally different approach (GNN)

## Key Metrics to Track

| Metric | Current | Target | Gap |
|--------|---------|--------|-----|
| Best CV | 0.0093 | 0.00358 | 61.5% |
| Best LB | 0.0932 | 0.0333 | 64.3% |

The simplification strategy is working. Continue until plateau is reached.

## Implementation Notes for Linear Model

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

class RidgeModel:
    def __init__(self, data='single', alpha=1.0):
        self.data_type = data
        self.featurizer = CombinedFeaturizer(mixed=(data=='full'))
        self.models = []  # 3 models, one per target
        self.scaler = StandardScaler()
        self.alpha = alpha
    
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train).numpy()
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_train, flip=True).numpy()
            X_feat = np.vstack([X_feat, X_flip])
            y_train = np.vstack([y_train.values, y_train.values])
        else:
            y_train = y_train.values
        
        X_scaled = self.scaler.fit_transform(X_feat)
        
        for i in range(3):  # 3 targets
            model = Ridge(alpha=self.alpha)
            model.fit(X_scaled, y_train[:, i])
            self.models.append(model)
    
    def predict(self, X):
        X_feat = self.featurizer.featurize(X).numpy()
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X, flip=True).numpy()
            X_scaled = self.scaler.transform(X_feat)
            X_flip_scaled = self.scaler.transform(X_flip)
            preds = []
            for i, model in enumerate(self.models):
                pred = (model.predict(X_scaled) + model.predict(X_flip_scaled)) / 2
                preds.append(pred)
            return torch.tensor(np.column_stack(preds))
        else:
            X_scaled = self.scaler.transform(X_feat)
            preds = []
            for model in self.models:
                preds.append(model.predict(X_scaled))
            return torch.tensor(np.column_stack(preds))
```

This is the simplest possible model that could work. If it improves CV, submit it.
