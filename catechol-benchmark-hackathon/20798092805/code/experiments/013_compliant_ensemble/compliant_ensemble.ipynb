{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c789fb7",
   "metadata": {},
   "source": [
    "# Competition-Compliant Ensemble Notebook\n",
    "\n",
    "**CRITICAL**: This notebook follows the EXACT template structure required by the competition.\n",
    "- Last 3 cells are IDENTICAL to the template\n",
    "- Only the model definition line is changed\n",
    "- Model class has `train_model(X_train, y_train)` and `predict(X)` methods\n",
    "\n",
    "**Model**: SimpleEnsemble = [32,16] MLP (0.6) + LightGBM (0.4)\n",
    "**Best CV**: 0.008785 from exp_011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import tqdm\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions (matching template)\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41676cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Featurizer with Arrhenius kinetics\n",
    "class CombinedFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Feature dimension: {CombinedFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model [32,16] with BatchNorm\n",
    "class MLPModelInternal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super(MLPModelInternal, self).__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print('MLPModelInternal defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Ensemble (5 models with different seeds)\n",
    "class MLPEnsemble:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single'):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.featurizer = CombinedFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "            y_all = y_vals\n",
    "            \n",
    "        input_dim = X_all.shape[1]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            torch.manual_seed(42 + i * 13)\n",
    "            np.random.seed(42 + i * 13)\n",
    "            \n",
    "            model = MLPModelInternal(input_dim, hidden_dims=self.hidden_dims, dropout=0.05).to(device)\n",
    "            model.train()\n",
    "            self.models.append(model)\n",
    "            \n",
    "            dataset = TensorDataset(X_all, y_all)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            criterion = nn.HuberLoss()\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                scheduler.step(epoch_loss / len(dataset))\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.models[0].parameters()).device\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_std = self.featurizer.featurize_torch(X, flip=False).to(device)\n",
    "            X_flip = self.featurizer.featurize_torch(X, flip=True).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    pred_sum += (model(X_std) + model(X_flip)) * 0.5\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        else:\n",
    "            X_std = self.featurizer.featurize_torch(X).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    pred_sum += model(X_std)\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        return avg_pred.cpu()\n",
    "\n",
    "print('MLPEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82448561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Model\n",
    "class LGBMModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = CombinedFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "\n",
    "    def train_model(self, X_train, y_train, num_boost_round=200):\n",
    "        X_feat = self.featurizer.featurize(X_train)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_feat, X_flip])\n",
    "            y_all = np.vstack([y_train.values, y_train.values])\n",
    "        else:\n",
    "            X_all = X_feat\n",
    "            y_all = y_train.values\n",
    "        \n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            train_data = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            model = lgb.train(self.params, train_data, num_boost_round=num_boost_round)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.data_type == 'full':\n",
    "            X_std = self.featurizer.featurize(X, flip=False)\n",
    "            X_flip = self.featurizer.featurize(X, flip=True)\n",
    "            preds = np.zeros((len(X), 3))\n",
    "            for i, model in enumerate(self.models):\n",
    "                preds[:, i] = (model.predict(X_std) + model.predict(X_flip)) / 2\n",
    "        else:\n",
    "            X_feat = self.featurizer.featurize(X)\n",
    "            preds = np.zeros((len(X), 3))\n",
    "            for i, model in enumerate(self.models):\n",
    "                preds[:, i] = model.predict(X_feat)\n",
    "        return torch.tensor(np.clip(preds, 0, 1))\n",
    "\n",
    "print('LGBMModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleEnsemble: [32,16] MLP (0.6) + LightGBM (0.4)\n",
    "# This is the model class that will be used in the template cells\n",
    "class SimpleEnsemble:\n",
    "    \"\"\"Competition-compliant ensemble model.\n",
    "    \n",
    "    Has train_model(X_train, y_train) and predict(X) methods.\n",
    "    Accepts data='single' or data='full' parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, data='single', mlp_weight=0.6, lgbm_weight=0.4):\n",
    "        self.data_type = data\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.mlp = None\n",
    "        self.lgbm = None\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"Train both MLP and LightGBM models.\"\"\"\n",
    "        # Train MLP [32,16] - best LB model\n",
    "        self.mlp = MLPEnsemble(hidden_dims=[32, 16], n_models=5, data=self.data_type)\n",
    "        self.mlp.train_model(X_train, y_train, epochs=200)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm = LGBMModel(data=self.data_type)\n",
    "        self.lgbm.train_model(X_train, y_train, num_boost_round=200)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return weighted average of MLP and LightGBM predictions.\"\"\"\n",
    "        pred_mlp = self.mlp.predict(X)\n",
    "        pred_lgbm = self.lgbm.predict(X)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_pred = self.mlp_weight * pred_mlp + self.lgbm_weight * pred_lgbm\n",
    "        return torch.clamp(ensemble_pred, 0, 1)\n",
    "\n",
    "print('SimpleEnsemble defined: MLP[32,16] (0.6) + LightGBM (0.4)')\n",
    "print('Model interface: train_model(X_train, y_train), predict(X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ced87",
   "metadata": {},
   "source": [
    "---\n",
    "## COMPETITION TEMPLATE CELLS BELOW\n",
    "\n",
    "**The following 3 cells are EXACTLY as required by the competition template.**\n",
    "**Only the model definition line is changed.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = SimpleEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51054dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = SimpleEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
