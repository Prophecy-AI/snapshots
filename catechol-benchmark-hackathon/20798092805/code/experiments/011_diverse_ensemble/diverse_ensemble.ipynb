{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9165789",
   "metadata": {},
   "source": [
    "# Diverse Model Ensemble: [32,16] MLP + LightGBM + [64,32] MLP\n",
    "\n",
    "**Rationale**: Single-model optimization has plateaued. CV-LB correlation has broken down:\n",
    "- exp_007 ([32,16]): CV 0.0093, LB 0.0932 (BEST LB)\n",
    "- exp_009 ([16]): CV 0.0092, LB 0.0936 (WORSE LB despite better CV)\n",
    "\n",
    "**Hypothesis**: Ensembling diverse models may improve generalization by:\n",
    "1. Reducing variance through averaging\n",
    "2. Capturing different patterns with different model types\n",
    "3. Providing more robust predictions on unseen solvents\n",
    "\n",
    "**Ensemble composition**:\n",
    "- [32,16] MLP (best LB model)\n",
    "- LightGBM (different model family, captures non-linearities differently)\n",
    "- [64,32] MLP (slightly more complex, may capture additional patterns)\n",
    "\n",
    "**Weighting**: Start with equal weights, can tune based on CV performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a96f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:04.147309Z",
     "iopub.status.busy": "2026-01-08T18:01:04.146508Z",
     "iopub.status.idle": "2026-01-08T18:01:07.059131Z",
     "shell.execute_reply": "2026-01-08T18:01:07.058292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a261cbe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:07.062136Z",
     "iopub.status.busy": "2026-01-08T18:01:07.061259Z",
     "iopub.status.idle": "2026-01-08T18:01:07.117180Z",
     "shell.execute_reply": "2026-01-08T18:01:07.116580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122)\n"
     ]
    }
   ],
   "source": [
    "# Load data and features\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f9cb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:07.119471Z",
     "iopub.status.busy": "2026-01-08T18:01:07.118964Z",
     "iopub.status.idle": "2026-01-08T18:01:07.127390Z",
     "shell.execute_reply": "2026-01-08T18:01:07.126783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension: 140\n"
     ]
    }
   ],
   "source": [
    "# Combined Featurizer for both MLP and LightGBM\n",
    "class CombinedFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[[\"Residence Time\", \"Temperature\"]].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Feature dimension: {CombinedFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25781528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:07.129476Z",
     "iopub.status.busy": "2026-01-08T18:01:07.129240Z",
     "iopub.status.idle": "2026-01-08T18:01:07.143679Z",
     "shell.execute_reply": "2026-01-08T18:01:07.143112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model with configurable architecture\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# MLP Ensemble\n",
    "class MLPEnsemble:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single'):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.featurizer = CombinedFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "            y_all = y_vals\n",
    "            \n",
    "        input_dim = X_all.shape[1]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            torch.manual_seed(42 + i * 13)\n",
    "            np.random.seed(42 + i * 13)\n",
    "            \n",
    "            model = MLPModel(input_dim, hidden_dims=self.hidden_dims, dropout=0.05).to(device)\n",
    "            model.train()\n",
    "            self.models.append(model)\n",
    "            \n",
    "            dataset = TensorDataset(X_all, y_all)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            criterion = nn.HuberLoss()\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                scheduler.step(epoch_loss / len(dataset))\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.models[0].parameters()).device\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_std = self.featurizer.featurize_torch(X, flip=False).to(device)\n",
    "            X_flip = self.featurizer.featurize_torch(X, flip=True).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    pred_sum += (model(X_std) + model(X_flip)) * 0.5\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        else:\n",
    "            X_std = self.featurizer.featurize_torch(X).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    pred_sum += model(X_std)\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        return avg_pred.cpu().numpy()\n",
    "\n",
    "print('MLPEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23612d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:07.145443Z",
     "iopub.status.busy": "2026-01-08T18:01:07.145208Z",
     "iopub.status.idle": "2026-01-08T18:01:07.153772Z",
     "shell.execute_reply": "2026-01-08T18:01:07.153167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMModel defined\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Model\n",
    "class LGBMModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = CombinedFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []  # One model per target\n",
    "        self.params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "\n",
    "    def train_model(self, X_train, y_train, num_boost_round=200):\n",
    "        X_feat = self.featurizer.featurize(X_train)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_feat, X_flip])\n",
    "            y_all = np.vstack([y_train.values, y_train.values])\n",
    "        else:\n",
    "            X_all = X_feat\n",
    "            y_all = y_train.values\n",
    "        \n",
    "        self.models = []\n",
    "        for i in range(3):  # 3 targets\n",
    "            train_data = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            model = lgb.train(self.params, train_data, num_boost_round=num_boost_round)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.data_type == 'full':\n",
    "            X_std = self.featurizer.featurize(X, flip=False)\n",
    "            X_flip = self.featurizer.featurize(X, flip=True)\n",
    "            preds = np.zeros((len(X), 3))\n",
    "            for i, model in enumerate(self.models):\n",
    "                preds[:, i] = (model.predict(X_std) + model.predict(X_flip)) / 2\n",
    "        else:\n",
    "            X_feat = self.featurizer.featurize(X)\n",
    "            preds = np.zeros((len(X), 3))\n",
    "            for i, model in enumerate(self.models):\n",
    "                preds[:, i] = model.predict(X_feat)\n",
    "        return np.clip(preds, 0, 1)\n",
    "\n",
    "print('LGBMModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7e475b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T18:01:07.155883Z",
     "iopub.status.busy": "2026-01-08T18:01:07.155374Z",
     "iopub.status.idle": "2026-01-08T18:01:07.165087Z",
     "shell.execute_reply": "2026-01-08T18:01:07.164515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiverseEnsemble defined: MLP[32,16] + LightGBM + MLP[64,32]\n",
      "Weights: MLP[32,16]=0.5, LGBM=0.25, MLP[64,32]=0.25\n"
     ]
    }
   ],
   "source": [
    "# Diverse Ensemble combining MLP [32,16] + LightGBM + MLP [64,32]\n",
    "class DiverseEnsemble:\n",
    "    def __init__(self, data='single', weights=[0.5, 0.25, 0.25]):\n",
    "        self.data_type = data\n",
    "        self.weights = weights  # [MLP_32_16, LGBM, MLP_64_32]\n",
    "        self.mlp_32_16 = None\n",
    "        self.lgbm = None\n",
    "        self.mlp_64_32 = None\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        # Train MLP [32,16] - best LB model\n",
    "        self.mlp_32_16 = MLPEnsemble(hidden_dims=[32, 16], n_models=5, data=self.data_type)\n",
    "        self.mlp_32_16.train_model(X_train, y_train, epochs=200)\n",
    "        \n",
    "        # Train LightGBM - different model family\n",
    "        self.lgbm = LGBMModel(data=self.data_type)\n",
    "        self.lgbm.train_model(X_train, y_train, num_boost_round=200)\n",
    "        \n",
    "        # Train MLP [64,32] - slightly more complex\n",
    "        self.mlp_64_32 = MLPEnsemble(hidden_dims=[64, 32], n_models=5, data=self.data_type)\n",
    "        self.mlp_64_32.train_model(X_train, y_train, epochs=200)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_mlp_32_16 = self.mlp_32_16.predict(X)\n",
    "        pred_lgbm = self.lgbm.predict(X)\n",
    "        pred_mlp_64_32 = self.mlp_64_32.predict(X)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_pred = (\n",
    "            self.weights[0] * pred_mlp_32_16 +\n",
    "            self.weights[1] * pred_lgbm +\n",
    "            self.weights[2] * pred_mlp_64_32\n",
    "        )\n",
    "        return np.clip(ensemble_pred, 0, 1)\n",
    "\n",
    "print('DiverseEnsemble defined: MLP[32,16] + LightGBM + MLP[64,32]')\n",
    "print(f'Weights: MLP[32,16]=0.5, LGBM=0.25, MLP[64,32]=0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvent\n",
    "print(\"\\n--- TASK 0: Single Solvent (24 folds) ---\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    model = DiverseEnsemble(data='single', weights=[0.5, 0.25, 0.25])\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    for row_idx, row in enumerate(predictions):\n",
    "        all_predictions.append({\"task\": 0, \"fold\": fold_idx, \"row\": row_idx,\n",
    "            \"target_1\": row[0], \"target_2\": row[1], \"target_3\": row[2]})\n",
    "    all_actuals.append(test_Y.values)\n",
    "\n",
    "submission_single = pd.DataFrame(all_predictions)\n",
    "actuals_single = np.vstack(all_actuals)\n",
    "preds_single = submission_single[['target_1', 'target_2', 'target_3']].values\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "print(f'\\nSingle Solvent MSE: {mse_single:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb22e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for full data\n",
    "print(\"\\n--- TASK 1: Full Data (13 folds) ---\")\n",
    "X, Y = load_data(\"full\")\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    model = DiverseEnsemble(data='full', weights=[0.5, 0.25, 0.25])\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    for row_idx, row in enumerate(predictions):\n",
    "        all_predictions_full.append({\"task\": 1, \"fold\": fold_idx, \"row\": row_idx,\n",
    "            \"target_1\": row[0], \"target_2\": row[1], \"target_3\": row[2]})\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "submission_full = pd.DataFrame(all_predictions_full)\n",
    "actuals_full = np.vstack(all_actuals_full)\n",
    "preds_full = submission_full[['target_1', 'target_2', 'target_3']].values\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "print(f'\\nFull Data MSE: {mse_full:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== FINAL RESULTS ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\n=== COMPARISON ===')\n",
    "print(f'  exp_007 [32,16] alone: CV 0.009262, LB 0.0932 (BEST LB)')\n",
    "print(f'  exp_009 [16] alone: CV 0.009192, LB 0.0936')\n",
    "print(f'  This ensemble: CV {overall_mse:.6f}')\n",
    "print(f'\\nExpected LB (using 10x ratio): {overall_mse * 10:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532db451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission = pd.concat([submission_single, submission_full])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "print(f'Submission saved: {submission.shape}')\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
