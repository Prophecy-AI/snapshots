{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T09:52:55.102117",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics feature engineering: Use 1/Temperature (inverse temp in Kelvin), ln(Time), and their interaction term. This physics-informed approach improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical symmetry TTA: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped, then average. This respects physical symmetry of mixtures. Score: 0.09831",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors are the best solvent features for linear mixing. Use linear interpolation: A*(1-pct) + B*pct for mixed solvents.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "MLP architecture: BatchNorm -> Linear(128) -> BatchNorm -> ReLU -> Dropout(0.2) -> repeat -> Linear(64) -> Linear(3) -> Sigmoid. Use HuberLoss for robustness.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble: Train 7 models with different seeds and average predictions. Combined with symmetry augmentation during training.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Numeric feature engineering: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Use StandardScaler for normalization.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV procedure: Task 0 = single solvent (leave-one-solvent-out), Task 1 = full data with mixtures (leave-one-ramp-out). Must use exact template format with last 3 cells unchanged except model definition.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available precomputed features: drfps_catechol, fragprints, smiles, acs_pca_descriptors, spange_descriptors. Spange descriptors commonly used as best for linear mixing.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data sizes: Full data 1227 rows, Single solvent 656 rows. 24 unique solvents. Temperature: 175-225\u00b0C, Residence Time: 2-15 min. Targets: SM, Product 2, Product 3 (yields 0-1).",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: Spange descriptors (13 features), ACS PCA (5 features), DRFPS (2048 features), Fragprints (2133 features). Spange is most compact and interpretable.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Gaussian Processes work well for small chemistry datasets. Best kernels: Tanimoto kernel on fingerprints, graph kernels (Weisfeiler-Lehman), string kernels on SMILES. GAUCHE library provides GPU-accelerated implementations.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "Deep Kernel Learning (DKL) combines neural network embeddings with GP uncertainty quantification. Outperforms standard GPs and provides comparable performance to GNNs with uncertainty estimates.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid models combining DFT/mechanistic modeling with ML achieve 0.77 kcal/mol MAE for reaction barriers. ML corrects for DFT deficiencies and solvation model errors.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with small datasets?",
      "finding": "Predicting reaction yields when only a few hundred (or fewer) experiments are available calls for models that can extract maximal information from limited data and that can quantify their confidence.  Recent work shows that **transfer\u2011learning with reaction\u2011transformer architectures** is especially effective: a pretrained natural\u2011language\u2011processing\u2011style transformer is fine\u2011tuned on an augmented version of the small training set, and test\u2011time augmentation is used to produce uncertainty estimates that correlate with actual errors.  Even when the augmented set contains only ~2.5\u202f% of the original high\u2011throughput data, these models outperform traditional fingerprint\u2011 or physics\u2011based descriptors ([IBM\u202f2021](https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches)).  A closely related study demonstrates that an encoder\u2011transformer plus a regression head can achieve \u201coutstanding\u201d performance on high\u2011throughput reac",
      "agent": "explorer"
    },
    {
      "query": "How do Gaussian Processes work for chemical reaction yield prediction and what are the best kernel choices?",
      "finding": "Gaussian\u202fProcess (GP) regression treats the mapping from a reaction\u2019s descriptors (e.g., substrate, catalyst, solvent, temperature) to its experimental yield as a distribution over functions.  A GP is defined by a mean function (often taken as zero) and a covariance (kernel) function that encodes how similar two reactions are expected to behave; the kernel therefore determines the shape of the prior and, after observing a few data points, yields a posterior mean (the predicted yield) together with a variance that quantifies uncertainty\u202f([acs.org](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022)).  Because the posterior can be computed analytically, GPs work especially well in the \u201csmall\u2011data\u201d regime typical of early\u2011stage reaction optimisation, providing calibrated error bars that are useful for Bayesian optimisation and active learning\u202f([rsc.org](https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc04896h)).\n\nFor reaction\u2011yield modelling the choice of kernel is driven by the ne",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}