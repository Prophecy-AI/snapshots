{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T09:52:55.102117",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics feature engineering: Use 1/Temperature (inverse temp in Kelvin), ln(Time), and their interaction term. This physics-informed approach improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical symmetry TTA: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped, then average. This respects physical symmetry of mixtures. Score: 0.09831",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors are the best solvent features for linear mixing. Use linear interpolation: A*(1-pct) + B*pct for mixed solvents.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "MLP architecture: BatchNorm -> Linear(128) -> BatchNorm -> ReLU -> Dropout(0.2) -> repeat -> Linear(64) -> Linear(3) -> Sigmoid. Use HuberLoss for robustness.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble: Train 7 models with different seeds and average predictions. Combined with symmetry augmentation during training.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Numeric feature engineering: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Use StandardScaler for normalization.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV procedure: Task 0 = single solvent (leave-one-solvent-out), Task 1 = full data with mixtures (leave-one-ramp-out). Must use exact template format with last 3 cells unchanged except model definition.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available precomputed features: drfps_catechol, fragprints, smiles, acs_pca_descriptors, spange_descriptors. Spange descriptors commonly used as best for linear mixing.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data sizes: Full data 1227 rows, Single solvent 656 rows. 24 unique solvents. Temperature: 175-225\u00b0C, Residence Time: 2-15 min. Targets: SM, Product 2, Product 3 (yields 0-1).",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: Spange descriptors (13 features), ACS PCA (5 features), DRFPS (2048 features), Fragprints (2133 features). Spange is most compact and interpretable.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with small datasets?",
      "finding": "Predicting reaction yields when only a few hundred (or fewer) experiments are available calls for models that can extract maximal information from limited data and that can quantify their confidence.  Recent work shows that **transfer\u2011learning with reaction\u2011transformer architectures** is especially effective: a pretrained natural\u2011language\u2011processing\u2011style transformer is fine\u2011tuned on an augmented version of the small training set, and test\u2011time augmentation is used to produce uncertainty estimates that correlate with actual errors.  Even when the augmented set contains only ~2.5\u202f% of the original high\u2011throughput data, these models outperform traditional fingerprint\u2011 or physics\u2011based descriptors ([IBM\u202f2021](https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches)).  A closely related study demonstrates that an encoder\u2011transformer plus a regression head can achieve \u201coutstanding\u201d performance on high\u2011throughput reac",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}