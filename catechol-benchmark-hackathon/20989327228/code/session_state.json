{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T09:52:55.102117",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Chemical Reaction Yield Prediction - Techniques Guide\n\n## Problem Overview\nThis is a multi-output regression problem predicting 3 reaction yields (SM, Product 2, Product 3) for chemical reactions under different solvent and process conditions. The evaluation uses a custom cross-validation procedure with leave-one-solvent-out (single solvent data) and leave-one-ramp-out (mixture data).\n\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: data shapes, feature distributions, target distributions\n\n## Critical Challenge: Leave-One-Group-Out CV\nThe CV structure tests generalization to **completely unseen solvents**. This is harder than random splits because:\n- The model must generalize to new chemical entities\n- Solvent descriptors (Spange, ACS PCA) are crucial for this\n- Models that overfit to specific solvents will fail\n\n**Key insight:** The model needs to learn the relationship between solvent properties and reaction outcomes, not memorize specific solvents.\n\n## Key Techniques from Top Solutions\n\n### 1. Physics-Informed Feature Engineering (CRITICAL)\n**Arrhenius Kinetics Features** - The best performing solutions use physics-informed features:\n- `1/Temperature` (inverse temperature in Kelvin: 1000/(T + 273.15))\n- `ln(Time)` (log of residence time)\n- `inv_temp * log_time` (interaction term)\n\nThis captures the Arrhenius equation relationship between temperature and reaction rates.\n\n**Additional Numeric Features:**\n- `rt^2`, `temp^2` (polynomial features)\n- `log1p(rt)`, `log1p(temp)` (log transforms)\n- `rt * temp` (interaction)\n\n### 2. Chemical Symmetry Exploitation (KEY INSIGHT)\nFor mixed solvents, a mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\".\n\n**Test Time Augmentation (TTA):**\n- Predict twice: once with (A, B) and once with (B, A) flipped\n- Average the predictions\n- This respects physical symmetry and reduces variance\n\n**Training Augmentation:**\n- Also train on both (A, B) and (B, A) versions of the data\n- Doubles training data effectively\n\n### 3. Solvent Featurization\n**Spange descriptors** are the best for linear mixing (13 features):\n- dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, n, f(n), delta\n\n**Linear Mixing for Mixed Solvents:**\n```\nmixed_features = A_features * (1 - pct) + B_features * pct\n```\nwhere pct is SolventB%.\n\n**Other available features:**\n- ACS PCA descriptors (5 features) - compact, green chemistry focused\n- DRFPS (2048 features) - high-dimensional reaction fingerprints\n- Fragprints (2133 features) - high-dimensional molecular fingerprints\n\n**Feature Combination Strategy:**\n- Concatenate Spange + ACS PCA for richer representation\n- Or use Spange alone for simplicity\n\n### 4. Model Architecture Options\n\n#### Option A: MLP (proven effective, score ~0.09831)\n```\nBatchNorm1d(input_dim)\nLinear(input_dim, 128) -> BatchNorm1d -> ReLU -> Dropout(0.2)\nLinear(128, 128) -> BatchNorm1d -> ReLU -> Dropout(0.2)\nLinear(128, 64) -> BatchNorm1d -> ReLU -> Dropout(0.2)\nLinear(64, 3) -> Sigmoid()\n```\n\n**Training Configuration:**\n- Loss: HuberLoss (robust to outliers) or SmoothL1Loss\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Gradient clipping: max_norm=1.0\n- Epochs: 250-300\n- Batch size: 32\n\n#### Option B: Gaussian Processes (excellent for small data)\nGPs are particularly well-suited for this problem due to:\n- Small dataset size (~1200 full, ~650 single solvent)\n- Need for uncertainty quantification\n- Leave-one-out CV structure\n\n**Best GP Kernels for Chemistry:**\n- Tanimoto kernel on fingerprints\n- RBF/Mat\u00e9rn kernel on Spange descriptors\n- Additive kernels combining solvent + process features\n\n**GP Libraries:** GPyTorch, GAUCHE (specialized for chemistry)\n\n#### Option C: Gradient Boosting (XGBoost/LightGBM/CatBoost)\n- Works well with tabular data\n- Can handle mixed feature types\n- Consider MultiOutputRegressor wrapper\n- CatBoost handles categorical features natively\n\n### 5. Ensemble Strategies\n**Bagging:**\n- Train 5-7 models with different random seeds\n- Average predictions\n- Reduces variance significantly\n\n**Multi-seed ensemble:**\n- Use seeds like [42, 77, 2025]\n- Train separate models per seed per fold\n- Average all predictions\n\n**Model Diversity:**\n- Combine MLP + GP + XGBoost predictions\n- Weight by validation performance\n- Stacking can further improve\n\n### 6. Post-Processing\n- Clip predictions to [0, 1] range\n- Consider normalizing so SM + Product2 + Product3 \u2248 1 (chemical constraint)\n- The three yields should approximately sum to 1 (mass balance)\n\n### 7. CV Procedure (MUST FOLLOW)\n**Task 0 - Single Solvent:**\n- Leave-one-solvent-out cross-validation\n- 24 folds (one per solvent)\n\n**Task 1 - Full Data (Mixtures):**\n- Leave-one-ramp-out cross-validation\n- 13 folds (one per solvent pair)\n\n**Submission format:**\n- Must use exact template with last 3 cells unchanged\n- Only modify model definition line\n\n## Advanced Techniques to Consider\n\n### Deep Kernel Learning (DKL)\nCombines neural network feature learning with GP uncertainty:\n- NN learns embeddings from molecular representations\n- GP provides calibrated uncertainty estimates\n- Outperforms standard GPs on chemistry tasks\n\n### Feature Concatenation\nCombine multiple descriptor types:\n- Spange descriptors (13 features) - physicochemical\n- ACS PCA descriptors (5 features) - green chemistry\n- Numeric features with Arrhenius engineering\n\n### Multi-Task Learning\nSince predicting 3 correlated outputs:\n- Consider shared hidden layers with separate output heads\n- Or multi-output GP with correlated outputs\n- The three outputs are chemically related (mass balance)\n\n### Regularization for Generalization\nSince testing on unseen solvents:\n- Use stronger regularization (higher dropout, weight decay)\n- Early stopping based on validation loss\n- Avoid overfitting to training solvents\n\n## Recommended Approach (Priority Order)\n\n1. **Start with Spange descriptors** - best for linear mixing\n2. **Add Arrhenius kinetics features** - physics-informed\n3. **Use MLP with BatchNorm and Dropout** - proven architecture\n4. **Implement symmetry TTA** for mixed solvents - key improvement\n5. **Ensemble 5-7 models** with different seeds\n6. **Use HuberLoss** for robustness\n7. **Try GP as alternative** - may work better for small data\n8. **Consider stacking** MLP + GP + XGBoost\n\n## Target Score\nBeat 0.017270 (lower is better). Top kernels achieve ~0.09831 with Arrhenius + TTA approach.\n\nNote: The target score of 0.017270 is significantly lower than the 0.09831 achieved by top kernels, suggesting there may be room for improvement through:\n- Better feature engineering (more physics-informed features)\n- More sophisticated models (GPs, deep kernel learning)\n- Better handling of the leave-one-out CV structure\n- Exploiting chemical constraints more effectively\n- Larger ensembles with more diverse models\n\n## Data Characteristics\n- Full data: 1227 rows, 13 solvent pairs\n- Single solvent: 656 rows, 24 solvents\n- Temperature: 175-225\u00b0C\n- Residence Time: 2-15 minutes\n- Targets: SM, Product 2, Product 3 (yields 0-1)\n- Spange descriptors: 13 features per solvent\n- ACS PCA: 5 features per solvent\n- DRFPS: 2048 features per reaction\n- Fragprints: 2133 features per molecule\n\n## Code Template Requirements\nThe submission notebook must follow the exact template structure:\n- Third-to-last cell: Single solvent CV loop (only change model definition)\n- Second-to-last cell: Full data CV loop (only change model definition)\n- Last cell: Combine and save submission (DO NOT CHANGE)\n\nThe model class must implement:\n- `train_model(X_train, y_train)` - training method\n- `predict(X_test)` - prediction method returning tensor/array of shape [N, 3]\n",
      "agent": "explorer",
      "timestamp": "2026-01-14T09:59:55.609267"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics feature engineering: Use 1/Temperature (inverse temp in Kelvin), ln(Time), and their interaction term. This physics-informed approach improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical symmetry TTA: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped, then average. This respects physical symmetry of mixtures. Score: 0.09831",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors are the best solvent features for linear mixing. Use linear interpolation: A*(1-pct) + B*pct for mixed solvents.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "MLP architecture: BatchNorm -> Linear(128) -> BatchNorm -> ReLU -> Dropout(0.2) -> repeat -> Linear(64) -> Linear(3) -> Sigmoid. Use HuberLoss for robustness.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble: Train 7 models with different seeds and average predictions. Combined with symmetry augmentation during training.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Numeric feature engineering: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Use StandardScaler for normalization.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV procedure: Task 0 = single solvent (leave-one-solvent-out), Task 1 = full data with mixtures (leave-one-ramp-out). Must use exact template format with last 3 cells unchanged except model definition.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available precomputed features: drfps_catechol, fragprints, smiles, acs_pca_descriptors, spange_descriptors. Spange descriptors commonly used as best for linear mixing.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data sizes: Full data 1227 rows, Single solvent 656 rows. 24 unique solvents. Temperature: 175-225\u00b0C, Residence Time: 2-15 min. Targets: SM, Product 2, Product 3 (yields 0-1).",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: Spange descriptors (13 features), ACS PCA (5 features), DRFPS (2048 features), Fragprints (2133 features). Spange is most compact and interpretable.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Gaussian Processes work well for small chemistry datasets. Best kernels: Tanimoto kernel on fingerprints, graph kernels (Weisfeiler-Lehman), string kernels on SMILES. GAUCHE library provides GPU-accelerated implementations.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "Deep Kernel Learning (DKL) combines neural network embeddings with GP uncertainty quantification. Outperforms standard GPs and provides comparable performance to GNNs with uncertainty estimates.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid models combining DFT/mechanistic modeling with ML achieve 0.77 kcal/mol MAE for reaction barriers. ML corrects for DFT deficiencies and solvation model errors.",
      "source": "WebSearch: Gaussian Processes for chemistry",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with small datasets?",
      "finding": "Predicting reaction yields when only a few hundred (or fewer) experiments are available calls for models that can extract maximal information from limited data and that can quantify their confidence.  Recent work shows that **transfer\u2011learning with reaction\u2011transformer architectures** is especially effective: a pretrained natural\u2011language\u2011processing\u2011style transformer is fine\u2011tuned on an augmented version of the small training set, and test\u2011time augmentation is used to produce uncertainty estimates that correlate with actual errors.  Even when the augmented set contains only ~2.5\u202f% of the original high\u2011throughput data, these models outperform traditional fingerprint\u2011 or physics\u2011based descriptors ([IBM\u202f2021](https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches)).  A closely related study demonstrates that an encoder\u2011transformer plus a regression head can achieve \u201coutstanding\u201d performance on high\u2011throughput reac",
      "agent": "explorer"
    },
    {
      "query": "How do Gaussian Processes work for chemical reaction yield prediction and what are the best kernel choices?",
      "finding": "Gaussian\u202fProcess (GP) regression treats the mapping from a reaction\u2019s descriptors (e.g., substrate, catalyst, solvent, temperature) to its experimental yield as a distribution over functions.  A GP is defined by a mean function (often taken as zero) and a covariance (kernel) function that encodes how similar two reactions are expected to behave; the kernel therefore determines the shape of the prior and, after observing a few data points, yields a posterior mean (the predicted yield) together with a variance that quantifies uncertainty\u202f([acs.org](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022)).  Because the posterior can be computed analytically, GPs work especially well in the \u201csmall\u2011data\u201d regime typical of early\u2011stage reaction optimisation, providing calibrated error bars that are useful for Bayesian optimisation and active learning\u202f([rsc.org](https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc04896h)).\n\nFor reaction\u2011yield modelling the choice of kernel is driven by the ne",
      "agent": "explorer"
    },
    {
      "query": "How to handle leave-one-group-out cross-validation in machine learning for better generalization?",
      "finding": "Based on the scikit\u2011learn documentation and a practical Stack\u202fOverflow example, you can apply Leave\u2011One\u2011Group\u2011Out (LOGO) cross\u2011validation as follows to obtain a more realistic estimate of generalization when samples share a natural grouping (e.g., subjects, time periods, devices)\u202f([LeaveOneGroupOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html); [Stack\u202fOverflow](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn)):\n\n1. **Prepare the data and a group label array**  \n   - `X` \u2013 feature matrix, `y` \u2013 target vector.  \n   - `groups` \u2013 1\u2011D array where each entry is an integer (or hashable) identifier for the group to which the corresponding sample belongs (e.g., subject ID, year, device).  \n\n2. **Instantiate the LOGO splitter**  \n   ```python\n   from sklearn.model_selection import LeaveOneGroupOut\n   logo = LeaveOneGroupOut()\n   n_splits = logo.get_n_splits(groups=groups)   # number of d",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}