{
  "query": "How do Gaussian Processes work for chemical reaction yield prediction and what are the best kernel choices?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Gaussian\u202fProcess (GP) regression treats the mapping from a reaction\u2019s descriptors (e.g., substrate, catalyst, solvent, temperature) to its experimental yield as a distribution over functions.  A GP is defined by a mean function (often taken as zero) and a covariance (kernel) function that encodes how similar two reactions are expected to behave; the kernel therefore determines the shape of the prior and, after observing a few data points, yields a posterior mean (the predicted yield) together with a variance that quantifies uncertainty\u202f([acs.org](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022)).  Because the posterior can be computed analytically, GPs work especially well in the \u201csmall\u2011data\u201d regime typical of early\u2011stage reaction optimisation, providing calibrated error bars that are useful for Bayesian optimisation and active learning\u202f([rsc.org](https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc04896h)).\n\nFor reaction\u2011yield modelling the choice of kernel is driven by the need to compare structured chemical inputs.  The GAUCHE library demonstrates that the most effective kernels are those defined on molecular representations such as (i) **graph kernels** (e.g., Weisfeiler\u2011Lehman or shortest\u2011path kernels) that operate on reaction\u2011graph encodings, (ii) **string kernels** on SMILES sequences, and (iii) **bit\u2011vector (fingerprint) kernels** using Tanimoto or linear kernels on binary fingerprints\u202f([liner.com](https://liner.com/review/gauche-a-library-for-gaussian-processes-in-chemistry);\u202f[neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2023/file/f2b1b2e974fa5ea622dd87f22815f423-Paper-Conference.pdf)).  In practice, these base kernels are often combined additively or multiplicatively to capture the contributions of each reaction component (substrate\u202f+\u202fcatalyst\u202f+\u202fsolvent) and to allow hyper\u2011parameter optimisation via marginal\u2011likelihood maximisation.\n\nWhen richer representations are desired, **deep\u2011kernel learning** couples a neural\u2011network embedding (trained on large reaction datasets) with a conventional GP kernel, preserving the GP\u2019s uncertainty quantification while letting the network learn expressive features\u202f([nature.com](https://www.nature.com/articles/s42004-024-01219-x)).  Empirically, the best-performing setups for yield prediction combine a graph\u2011based kernel (or a Tanimoto kernel on fingerprints) with a smooth stationary kernel such as the Mat\u00e9rn\u20113/2 or RBF kernel for the learned embedding, and they are tuned on the marginal likelihood of the limited experimental data.  This hybrid approach has been shown to outperform pure kernel methods and to provide reliable confidence intervals for reaction\u2011yield forecasts.",
      "url": ""
    },
    {
      "title": "Deep Kernel learning for reaction outcome prediction and ...",
      "text": "Deep Kernel learning for reaction outcome prediction and optimization | Communications Chemistry\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Communications Chemistry](https://media.springernature.com/full/nature-cms/uploads/product/commschem/header-3dc28429486e0d2c8f49fd9baf5afa40.svg)](https://www.nature.com/commschem)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42004-024-01219-x?error=cookies_not_supported&code=339c684d-2df4-42bb-a10b-c18beba44b3d)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42004)\n* [RSS feed](https://www.nature.com/commschem.rss)\nDeep Kernel learning for reaction outcome prediction and optimization\n[Download PDF](https://www.nature.com/articles/s42004-024-01219-x.pdf)\n[Download PDF](https://www.nature.com/articles/s42004-024-01219-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:14 June 2024# Deep Kernel learning for reaction outcome prediction and optimization\n* [Sukriti Singh](#auth-Sukriti-Singh-Aff1)[ORCID:orcid.org/0000-0003-2286-2974](https://orcid.org/0000-0003-2286-2974)[1](#Aff1)&amp;\n* [Jos\u00e9 Miguel Hern\u00e1ndez-Lobato](#auth-Jos__Miguel-Hern_ndez_Lobato-Aff1)[ORCID:orcid.org/0000-0001-7610-949X](https://orcid.org/0000-0001-7610-949X)[1](#Aff1)\n[*Communications Chemistry*](https://www.nature.com/commschem)**volume7**, Article\u00a0number:136(2024)[Cite this article](#citeas)\n* 7809Accesses\n* 14Citations\n* 8Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42004-024-01219-x/metrics)\n### Subjects\n* [Catalysis](https://www.nature.com/subjects/catalysis)\n* [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n* [Method development](https://www.nature.com/subjects/method-development)\n* [Synthetic chemistry methodology](https://www.nature.com/subjects/methodology)\n* [Structure prediction](https://www.nature.com/subjects/structure-prediction)\n## Abstract\nRecent years have seen a rapid growth in the application of various machine learning methods for reaction outcome prediction. Deep learning models have gained popularity due to their ability to learn representations directly from the molecular structure. Gaussian processes (GPs), on the other hand, provide reliable uncertainty estimates but are unable to learn representations from the data. We combine the feature learning ability of neural networks (NNs) with uncertainty quantification of GPs in a deep kernel learning (DKL) framework to predict the reaction outcome. The DKL model is observed to obtain very good predictive performance across different input representations. It significantly outperforms standard GPs and provides comparable performance to graph neural networks, but with uncertainty estimation. Additionally, the uncertainty estimates on predictions provided by the DKL model facilitated its incorporation as a surrogate model for Bayesian optimization (BO). The proposed method, therefore, has a great potential towards accelerating reaction discovery by integrating accurate predictive models that provide reliable uncertainty estimates with BO.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-021-00144-6/MediaObjects/41598_2021_144_Fig1_HTML.png)\n### [Deep Bayesian Gaussian processes for uncertainty estimation in electronic health records](https://www.nature.com/articles/s41598-021-00144-6?fromPaywallRec=false)\nArticleOpen access19 October 2021\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-57135-6/MediaObjects/41598_2024_57135_Fig1_HTML.png)\n### [Relationship between prediction accuracy and uncertainty in compound potency prediction using deep neural networks and control models](https://www.nature.com/articles/s41598-024-57135-6?fromPaywallRec=false)\nArticleOpen access19 March 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-021-88939-5/MediaObjects/41598_2021_88939_Fig1_HTML.png)\n### [GPCR\\_LigandClassify.py; a rigorous machine learning classifier for GPCR targeting compounds](https://www.nature.com/articles/s41598-021-88939-5?fromPaywallRec=false)\nArticleOpen access04 May 2021\n## Introduction\nChemical reaction optimization is central to organic synthesis and has largely been based on chemical intuition[1](https://www.nature.com/articles/s42004-024-01219-x#ref-CR1). During optimization, the aim is to maximize the reaction outcome (e.g., yield and/or enantiomeric excess) by identifying suitable experimental conditions[2](https://www.nature.com/articles/s42004-024-01219-x#ref-CR2). This involves evaluating a multidimensional chemical space comprising of reaction variables such as catalyst, solvent, substrate, additive, time, temperature, concentration, etc.[3](https://www.nature.com/articles/s42004-024-01219-x#ref-CR3),[4](https://www.nature.com/articles/s42004-024-01219-x#ref-CR4). Owing to the complexity of this problem, several data-driven approaches have been employed for efficient exploration of the chemical space[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s42004-024-01219-x#ref-CR7).\nThe estimation of reaction outcome is of great importance in reaction development. It could enable chemists to identify, for instance, low-yield reactions prior to wet-lab experiments, thereby saving time and resources. Machine learning (ML) has shown an impressive degree of success in many areas of chemistry[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42004-024-01219-x#ref-CR11). Earlier efforts toward reaction outcome prediction use hand-crafted features such as physical organic descriptors and molecular fingerprints[12](https://www.nature.com/articles/s42004-024-01219-x#ref-CR12),[13](https://www.nature.com/articles/s42004-024-01219-x#ref-CR13). Conventional ML methods, particularly random forests, perform extremely well with these non-learned representations. Recently, the advances in deep learning (DL) have led to the development of new molecular representations[14](https://www.nature.com/articles/s42004-024-01219-x#ref-CR14). These are learned directly from molecular structures like simplified molecular input line entry specifications (SMILES) and molecular graphs. The chemical language models (LMs) and graph neural networks (GNNs) trained using these string or graph-based representations have displayed great potential in reaction outcome prediction[15](#ref-CR15),[16](#ref-CR16),[17](#ref-CR17),[18](https://www.nature.com/articles/s42004-024-01219-x#ref-CR18).\nThe reaction outcome prediction augmented with uncertainty quantification is expected to find superior utility during reaction optimization[19](https://www.nature.com/articles/s42004-024-01219-x#ref-CR19). As an example, Bayesian optimization (BO) works with the uncertainty estimates to suggest new experiments in a search for optimal reaction conditions[20](https://www.nature.com/articles/s42004-024-01219-x#ref-CR20). While the quantification of uncertainty using the above-mentioned ML methods might not be straightforward, the uncertainty-awareness of Gaussian processes (GPs) is well-known[21](https://www.nature.com/articles/s42004-024-01219-x#ref-CR21),[22](https://www.nature.com/articles/s42004-024-01219-x#ref-CR22). G...",
      "url": "https://www.nature.com/articles/s42004-024-01219-x"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "<div><div><header><a href=\"https://liner.com/\"></a></header><main><div><p>Ryan-Rhys Griffiths</p><p>Leo Klarner</p><p>Henry B. Moss</p><p>Aditya Ravuri</p><p>Sang T. Truong</p><p>Bojana Rankovi\u0107</p><p>Yuanqi Du</p><p>Arian R. Jamasb</p><p>Julius Schwartz</p><p>Austin Tripp</p><p>Gregory Kell</p><p>Anthony Bourached</p><p>A. Chan</p><p>Jacob Moss</p><p>Chengzhi Guo</p><p>A. Lee</p><p>P. Schwaller</p><p>Jian Tang</p></div><div><h2><p>Proposed method</p><p>Proposed method</p></h2><div><div><div><p></p></div><div><h4><p>Figure 1: An overview of the applications and representations available in GAUCHE.</p></h4></div></div><div><div><h3><p>Motivation &amp; Objective</p></h3><ul><li><p>Early-stage scientific discovery is hindered by limited high-quality experimental data, necessitating machine learning methods that thrive in small data regimes.</p></li><li><p>Current molecular machine learning often uses Bayesian neural networks or deep ensembles for uncertainty quantification in Bayesian optimization and active learning, but these are suboptimal for small datasets.</p></li><li><p>Gaussian processes offer advantages in small data settings and for Bayesian optimization due to exact Bayesian inference and fewer parameters requiring manual tuning.</p></li></ul></div><div><h3><p>Key contributions</p></h3><ul><li><p>GAUCHE introduces a comprehensive Gaussian Process (GP) framework for molecules and chemical reactions, overcoming limitations of traditional GPs with structured inputs.</p></li><li><p>The library, GAUCHE, provides GPU-based implementations of kernels for various molecular representations including strings, fingerprints, and graphs.</p></li><li><p>GAUCHE extends black-box graph kernels from GraKel to GP regression via a GPyTorch interface, enabling optimization of graph kernel hyperparameters through marginal likelihood.</p></li><li><p>GAUCHE includes support for protein and chemical reaction representations and integrates with GPyTorch and BoTorch for advanced probabilistic modeling and Bayesian optimization.</p></li><li><p>The GAUCHE framework is benchmarked on regression, uncertainty quantification, and Bayesian optimization tasks across diverse chemical datasets.</p></li></ul></div></div></div></div><div><h2><p>Experimental results</p><p>Experimental results</p></h2><div><div><div><p></p></div><div><h4><p>Figure 3: BO performance reporting the standard error from 50 randomly initialised trials (20 for Buchwald-Hartwig). A kernel density estimate over the trials is shown on the right axis. EI fragprints results use the Tanimoto kernel.</p></h4></div></div><div><div><p><strong>RQ1</strong>: Does GAUCHE improve BO performance over baseline approaches?</p><ul><li>The BO benchmarks in Figure 3 (50 trials, 20 for Buchwald\u2013Hartwig) show GAUCHE yields competitive and often improved BO performance with low standard error across trials, especially when using EI fragprints with the Tanimoto kernel, indicating reliable surrogate modelling for chemistry tasks.</li></ul><p><strong>RQ2</strong>: Can regression and uncertainty quantification performance be used as a proxy for BO performance?</p><ul><li>The experiments indicate that strong regression and UQ metrics generally correlate with better BO outcomes, but the discussion cautions that surrogate misspecification can still degrade real-world BO, so direct BO evaluation remains necessary.</li></ul><p><strong>RQ3</strong>: How consistent are BO outcomes across random initialisations and datasets?</p><ul><li>Figure 3's standard errors and kernel density estimates demonstrate consistent BO outcomes across random initialisations (50 trials, 20 for Buchwald\u2013Hartwig) with relatively low variance, though the authors highlight future work to incorporate more sophisticated GP-based optimisation and active learning to further improve robustness.</li></ul></div><div><h3><p>Analysis</p></h3><ul><li><p>The authors found that the best choice of representation and kernel is task-dependent for regression on chemical reaction yield prediction.</p></li><li><p>Uncertainty estimate quality was roughly correlated with regression performance.</p></li><li><p>For Bayesian optimization, performance was tightly coupled to regression and uncertainty quantification performance.</p></li><li><p>In ultra-low data settings, both the Tanimoto-fragprint kernel and SMILES string kernel models outperformed random search for molecular discovery tasks.</p></li><li><p>The differential reaction fingerprint in conjunction with the Tanimoto kernel was the best-performing reaction representation.</p></li></ul></div></div></div></div><div><h2><p>Related Quick Review</p><p>Related Quick Review</p></h2></div></main><div><div><p>This review was generated by AI. Complete your research faster and more accurately with Liner.</p></div><div><p>Research without delay\nwith Liner\u2019s research agents</p><p>Research without delay\nwith Liner\u2019s research agents</p></div><p>\u00a92025 Liner. All Rights Reserved.</p></div></div></div>",
      "url": "https://liner.com/review/gauche-a-library-for-gaussian-processes-in-chemistry"
    },
    {
      "title": "Gaussian Process Regression for Materials and Molecules",
      "text": "<div><div><div><h4>SPECIAL ISSUE</h4><p>This article is part of the <a href=\"https://pubs.acs.org/toc/chreay/121/16\"></a> special issue.</p></div><div><div><p><h2>1. Introduction</h2></p><div><p></p><div><p><a href=\"#\"><i></i></a></p><div><ul><li><a href=\"#Abstract\"></a></li><li><a href=\"#_i2\"></a></li><li><a href=\"#_i4\"></a></li><li><a href=\"#_i76\"></a></li><li><a href=\"#_i114\"></a></li><li><a href=\"#_i140\"></a></li><li><a href=\"#_i167\"></a></li><li><a href=\"#_i183\"></a></li><li><a href=\"#_i205\"></a></li><li><a href=\"#authorInformationSection\"></a></li><li><a href=\"#biographies\"></a></li><li><a href=\"#_i213\"></a></li><li><a href=\"#_i214\"></a></li><li><a href=\"#citeThis\"></a></li></ul></div></div></div></div><hr/><p>At the heart of chemistry is the need to understand the nature, transformations, and macroscopic effects of atomistic structure. This is true for <i>materials</i>\u2014crystals, glasses, nanostructures, composites\u2014as well as for <i>molecules</i>, from the simplest industrial feedstocks to entire proteins. And with the often-quoted role of chemistry as the \u201ccentral science\u201d, (1,2) its emphasis on atomistic understanding has a bearing on many neighboring disciplines: candidate drug molecules are made by synthetic chemists based on an atomic-level knowledge of reaction mechanisms; functional materials for technological applications are characterized on a range of length scales, which begins with increasingly accurate information about where exactly the atoms are located relative to one another in three-dimensional space.</p><p>Research progress in structural chemistry has largely been driven by advances in experimental characterization techniques, from landmark studies in X-ray and neutron crystallography to novel electron microscopy techniques which make it possible to visualize individual atoms directly. Complementing these new developments, detailed and realistic structural insight is also increasingly gained from computer simulations. Today, chemists (together with materials scientists) are heavy users of large-scale supercomputing facilities, and the computationally guided discovery of previously unknown molecules and materials has come within reach. (3\u22128)</p><p>Computations based on the quantum mechanics of electronic structure, currently most commonly within the framework of density-functional theory (DFT), are widely used to study structures of molecules and materials and to predict a range of atomic-scale properties. (9\u221211) Two approaches are of note here. One is the prediction of atomically resolved physical quantities, e.g., isotropic chemical shifts, \u03b4<sub>iso</sub>, that can be used to simulate NMR spectra with a large degree of realism (12)\u2014thereby making it possible to corroborate or falsify a candidate structural model or to deconvolute experimentally measured spectra. The other central task is the determination of atomistic structure itself, achieved through molecular dynamics (MD), structural optimization, and other quantum-mechanically driven techniques. Many implementations of DFT exist and are widely used, and their consistency has been demonstrated in a comprehensive community-wide exercise. (13)</p><p>Electronic-structure computations are expensive, in terms of both their absolute resource requirements and their scaling behavior with the number of atoms, <i>N</i>. For DFT, the scaling is typically in the most common implementations; see ref (14) for the current status of a linear-scaling implementation. Routine use is therefore limited to a few thousand atoms at most for DFT single-point evaluations, to a few hundred atoms for DFT-driven \u201cab initio\u201d MD, and to even fewer for high-level wave function theory methods such as coupled cluster (CC) theory or quantum Monte Carlo (QMC). The latter techniques offer an accuracy far beyond standard DFT, and they are beginning to become accessible not only for isolated molecules but also for condensed phases. However, running MD with these methods requires substantial effort and is currently largely limited to proof-of-principle simulations. (15\u221217) For studies that predict atomistic properties, such as NMR shifts, derived from the wave function, a new electronic-structure computation has to be carried out every time a new structure is considered, again incurring large computational expense.</p><p>In the past decade, machine learning (ML) techniques have become a popular alternative, aiming to make the same type of predictions using an approximate or surrogate model, while requiring only a small fraction of the computational costs. There is practical interest in being able to access much more realistic descriptions of structurally complex systems (e.g., disordered and amorphous phases) than currently feasible, as well as a wider chemical space (e.g., scanning large databases of candidate materials rather than just a few selected ones). There is also a fundamental interest in the question of how one might \u201cteach\u201d chemical and physical properties to a computer algorithm which is inherently chemically agnostic and in the relationship of established chemical rules with the outcome of purely data-driven techniques. (19) We may direct the reader to high-level overviews of ML methods in the physical sciences by Butler et al., (20) Himanen et al., (21) and Batra et al., (22) to more detailed discussions of various technical aspects, (23\u221226) and to a physics-oriented review that places materials science in the context of many other topics for which ML is currently being used. (27)</p><p>The use of ML in computational chemistry, materials science, and also condensed-matter physics is often focused on the regression (fitting) of atomic properties, that is, the functional dependence of a given quantity on the local structural environment. For the case of force fields and interatomic potentials, there are a number of general overview articles (28\u221231) and examples of recent benchmark studies. (32,33) There are also specialized articles that offer more detailed introductions. (34\u221238)</p><p>In the present work, we review the application of Gaussian process regression (GPR) to computational chemistry, with an emphasis on the development of the methodology over the past decade. <a href=\"#fig1\">Figure </a><a href=\"#fig1\">1</a> provides an overview of the central concepts. Given early successes, there is significant emphasis on the construction of accurate, linear-scaling force-field models and the new chemical and physical insights that can be gained by using them. We also survey, more broadly, methodology and emerging applications concerning the \u201clearning\u201d of general atomistic properties that are of interest for chemical and materials research. Quantum-mechanical properties, including the eletronic energy, are inherently nonlocal, but the degree to which local approximations, taking account of the immediate neighborhood of an atom, can be used will be of central importance. It is hoped that the present work\u2014indeed the entire thematic issue in which it appears\u2014will provide guidance and inspiration for research in this quickly evolving field and that it will help advance the transition of the methodology from relatively specialized to much more widely used.</p></div><div><div><p><h2>2. Gaussian Process Regression</h2></p><div><p></p><div><p><a href=\"#\"><i></i></a></p><div><ul><li><a href=\"#Abstract\"></a></li><li><a href=\"#_i2\"></a></li><li><a href=\"#_i4\"></a></li><li><a href=\"#_i76\"></a></li><li><a href=\"#_i114\"></a></li><li><a href=\"#_i140\"></a></li><li><a href=\"#_i167\"></a></li><li><a href=\"#_i183\"></a></li><li><a href=\"#_i205\"></a></li><li><a href=\"#authorInformationSection\"></a></li><li><a href=\"#biographies\"></a></li><li><a href=\"#_i213\"></a></li><li><a href=\"#_i214\"></a></li><li><a href=\"#citeThis\"></a></li></ul></div></div></div></div><hr/><p>We begin this review article with a brief general introduction to the basic principles of GPR. The present section is not yet concerned with applications, b...",
      "url": "https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "GAUCHE: A Library for\nGaussian Processes in Chemistry\nRyan-Rhys Griffiths1\u2217 Leo Klarner2\u2217 Henry Moss3\u2217 Aditya Ravuri3\u2217 Sang Truong4\u2217\nSamuel Stanton5\u2217 Gary Tom6,7\u2217 Bojana Rankovic8,9\u2217 Yuanqi Du10\u2217 Arian Jamasb3\u2217\nAryan Deshwal11 Julius Schwartz3 Austin Tripp3 Gregory Kell12 Simon Frieder2\nAnthony Bourached13 Alex J. Chan3 Jacob Moss3 Chengzhi Guo3\nJohannes Durholt14 Saudamini Chaurasia15 Ji Won Park5 Felix Strieth-Kalthoff6\nAlpha A. Lee3 Bingqing Cheng16 Al\u00e1n Aspuru-Guzik6,7,17 Philippe Schwaller8,9\nJian Tang18,19,17\n1Meta 2University of Oxford 3University of Cambridge 4Stanford University 5Genentech\n6University of Toronto 7Vector Institute 8EPFL 9NCCR Catalysis 10Cornell University\n11Washington State University 12King\u2019s College London 13University College London\n14Evonik Industries AG 15Syracuse University 16IST Austria 17CIFAR AI Research Chair\n18MILA Quebec AI Institute 19HEC Montreal\n\u2217 Equal contributions\n{ryangriff123,leojklarner}@gmail.com\nAbstract\nWe introduce GAUCHE, an open-source library for GAUssian processes in\nCHEmistry. Gaussian processes have long been a cornerstone of probabilistic\nmachine learning, affording particular advantages for uncertainty quantification and\nBayesian optimisation. Extending Gaussian processes to molecular representations,\nhowever, necessitates kernels defined over structured inputs such as graphs, strings\nand bit vectors. By providing such kernels in a modular, robust and easy-to-use\nframework, we seek to enable expert chemists and materials scientists to make\nuse of state-of-the-art black-box optimization techniques. Motivated by scenarios\nfrequently encountered in practice, we showcase applications for GAUCHE in\nmolecular discovery, chemical reaction optimisation and protein design.\nThe codebase is made available at https://github.com/leojklarner/gauche.\n1 Introduction\nEarly-stage scientific discovery is often characterised by the limited availability of high-quality\nexperimental data [1, 2, 3], meaning that there is much knowledge to gain from targeted experiments.\nAs such, machine learning methods that facilitate discovery in low data regimes, such as Bayesian\noptimisation (BO) [4, 5, 6, 7, 8, 9] and active learning (AL) [10, 11], have great potential to expedite\nthe rate at which useful molecules, materials, chemical reactions and proteins can be discovered.\nAt present, Bayesian neural networks (BNNS) and deep ensembles are typically the method of\nchoice to generate uncertainty estimates for molecular BO and AL loops [10, 12, 13, 14]. For small\ndatasets, however, Gaussian processes (GPS) may often be a preferable and more appropriate choice\n[15, 16]. Furthermore, GPS possess particularly advantageous properties for BO; first, they admit\nexact as opposed to approximate Bayesian inference and second, few of their parameters need to be\ndetermined by hand. In the words of Sir David MacKay [17],\n\"Gaussian processes are useful tools for automated tasks where fine tuning for each\nproblem is not possible. We do not appear to sacrifice any performance for this simplicity.\u201d\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nECFP Fingerprint Molecular Graph SMILES String\nNC1=CC=CC=C1\nProteins\nSequence String Structural Graph\nRASRLMQA\nRXN Fingerprint RXN Smarts\n[CH:1][O:2]>>[C:1]=[O:2]\nReactions\nGaussian Processes Bayesian Optimisation\nMolecules\nFigure 1: An overview of the applications and representations available in GAUCHE.\nThe iterative model refitting required in BO makes it a prime example of such an automated task.\nHowever, canonical GPS typically assume continuous input spaces of low and fixed dimensionality,\nhindering their application to standard molecular representations such as SMILES/SELFIES strings\n[18, 19, 20], topological fingerprints [21, 22, 23] and discrete graphs [24, 25].\nWith GAUCHE, we provide a modular, robust and easy-to-use framework to rapidly prototype GPS\nwith 30+ GPU-accelerated string, fingerprint and graph kernels that operate on a range of molecular\nrepresentations (see Figure 1). Furthermore, GAUCHE interfaces with the GPyTorch [26] and\nBoTorch [27] libraries and contains an extensive set of tutorial notebooks to make state-of-the-art\nprobabilistic modelling and black-box optimization techniques more easily accessible to scientific\nexperts in chemistry, materials science and beyond.\n2 Background\nWe briefly recall the fundamentals of Gaussian processes and Bayesian optimisation in Sections 2.1\nand 2.2, respectively, and refer the reader to [28] and [29, 30, 31] for a more comprehensive treatment.\n2.1 Gaussian Processes\nNotation X \u2208 R\nn\u00d7d\nis a design matrix of n training examples of dimension d. A given row i of\nthe design matrix contains a training molecule\u2019s representation xi. A GP is specified by a mean\nfunction, m(x) = E[f(x)] and a covariance function k(x, x\n\u2032\n) = E[(f(x) \u2212 m(x))(f(x\n\u2032\n) \u2212 m(x\n\u2032\n))].\nK\u03b8(X, X) is a kernel matrix, where entries are computed by the kernel function as [K]ij = k(xi, xj )\nand \u03b8 represents the set of kernel hyperparameters. The GP specifies the full distribution over the\nfunction f to be modelled as\nf(x) \u223c GPm(x), k(x, x\n\u2032\n)\n\u0001\n.\nTraining Hyperparameters for GPS comprise kernel hyperparameters, \u03b8, in addition to the likeli\u0002hood noise, \u03c3\n2\ny\n. These hyperparameters are chosen by optimising an objective function known as the\nnegative log marginal likelihood (NLML)\nlog p(y|X, \u03b8) = \u2212\n1\n2\ny\n\u22a4(K\u03b8(X, X) + \u03c32\ny\nI)\n\u22121y\n| {z }\nencourages fit with data\n\u2212\n1\n2\nlog |K\u03b8(X, X) + \u03c3\n2\ny\nI|\n| {z }\ncontrols model capacity\n\u2212\nN\n2\nlog(2\u03c0),\n2\nwhere \u03c3\n2\ny\nI represents the variance of i.i.d. Gaussian noise on the observations y. The NLML\nembodies Occam\u2019s razor for Bayesian model selection [28] in favouring models that fit the data\nwithout being overly complex.\nPrediction At test locations X\u2217, assuming a zero mean function obtained following the standard\u0002ization of the outputs y, the GP returns a predictive mean, f\u00af\u2217 = K(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121y,\nand a predictive uncertainty cov(f\u2217) = K(X\u2217, X\u2217) \u2212 K(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121K(X, X\u2217).\n2.2 Bayesian Optimisation\nIn molecular discovery campaigns, we are typically interested in solving problems of the form\nx\n\u22c6 = arg max\nx\u2208X\nf(x),\nwhere f(\u00b7) : X \u2192 R is an expensive black-box function over a structured input domain X . In our\nsetting the structured input domain consists of a set of molecular representations (graphs, strings, bit\nvectors) and the expensive black-box function is an experimentally determined property of interest\nthat we wish to optimise. Bayesian optimisation (BO) [32, 33, 34, 35, 29, 36] is a data-efficient\nmethodology for determining x\n\u22c6\n. BO operates sequentially by selecting input locations at which\nto query the black-box function f with the aim of identifying the optimum in as few queries as\npossible. Evaluations are focused on promising areas of the input space as well as areas with high\nuncertainty\u2014a balancing act known as the exploration/exploitation trade-off.\nThe two components of a BO scheme are a probabilistic surrogate model and an acquisition function.\nThe surrogate model is typically chosen to be a GP due to its ability to maintain calibrated uncertainty\nestimates through exact Bayesian inference. The uncertainty estimates of the surrogate model are\nthen leveraged by the acquisition function to propose new input locations to query. The acquisition\nfunction is a heuristic that trades off exploration and exploitation, well-known examples of which\ninclude expected improvement (EI) [33, 35] and entropy search [37, 38, 39, 40]. After the acquisition\nfunction proposes an input location, the black-box is evaluated at that location, the surrogate model is\nretrained and the process is repeated until a solution is obtained.\n3 Molecular Representations\nWe review commonly used representations for molecules (Section 3.1), chemical reactions (Sec\u0002tion 3.2) and proteins (Section 3.3), before describing the kernels that operate on them in Section 4.\nAn overview of the representations considered by ...",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/f2b1b2e974fa5ea622dd87f22815f423-Paper-Conference.pdf"
    },
    {
      "title": "Machine learning meets mechanistic modelling for accurate prediction of experimental activation energies",
      "text": "Kjell Jorner \nEarly Chemical Development\nPharmaceutical Sciences\nR&D, AstraZeneca\nMacclesfieldUnited Kingdom\n\nTore Brinck \nApplied Physical Chemistry\nDepartment of Chemistry\nCBH\nKTH Royal Institute of Technology\nStockholmSweden\n\nPer-Ola Norrby \nData Science & Modelling\nPharmaceutical Sciences\nR&D, AstraZeneca\nGothenburgSweden\n\nDavid Buttar \nEarly Chemical Development\nPharmaceutical Sciences\nR&D, AstraZeneca\nMacclesfieldUnited Kingdom\n\nMachine learning meets mechanistic modelling for accurate prediction of experimental activation energies\n1\nAccurate prediction of chemical reactions in solution is challenging for current state-of-the-art approaches based on transition state modelling with density functional theory. Models based on machine learning have emerged as a promising alternative to address these problems, but these models currently lack the precision to give crucial information on the magnitude of barrier heights, influence of solvents and catalysts and extent of regio-and chemoselectivity. Here, we construct hybrid models which combine the traditional transition state modelling and machine learning to accurately predict reaction barriers. We train a Gaussian Process Regression model to reproduce high-quality experimental kinetic data for the nucleophilic aromatic substitution reaction and use it to predict barriers with a mean absolute error of 0.77 kcal/mol for an external test set. The model was further validated on regio-and chemoselectivity prediction on patent reaction data and achieved a competitive top-1 accuracy of 86%, despite not being trained explicitly for this task. Importantly, the model gives error bars for its predictions that can be used for risk assessment by the end user. Hybrid models emerge as the preferred alternative for accurate reaction prediction in the very common low-data situation where only 100-150 rate constants are available for a reaction class. With recent advances in deep learning for quickly predicting barriers and transition state geometries from density functional theory, we envision that hybrid models will soon become a standard alternative to complement current machine learning approaches based on ground-state physical organic descriptors or structural information such as molecular graphs or fingerprints.\n\nIntroduction\n\nAccurate prediction of chemical reactions is an important goal both in academic and industrial research. [1][2][3] Recently, machine learning approaches have had tremendous success in quantitative prediction of reaction yields based on data from high-throughput experimentation 4,5 and enantioselectivities based on carefully selected universal training sets. 6 At the same time, traditional quantitative structure-reactivity relationship (QSRR) methods based on linear regression have seen a renaissance with interpretable, holistic models that can generalize across reaction types. 7 In parallel with these developments of quantitative prediction methods, deep learning models trained on reaction databases containing millions of patent and literature data have made quick qualitative yes/no feasibility prediction routine for almost any reaction type. 8 In the pharmaceutical industry, prediction tools have great potential to accelerate synthesis of prospective drugs (Figure 1a). 9 Quick prediction is essential in the discovery phase, especially within the context of automation and rapid synthesis of a multitude of candidates for initial activity screening. 3,10,11 In these circumstances, a simple yes/no as provided by classification models is usually sufficient. More accurate prediction is necessary in the later drug development process, where the synthesis route and formulation of one or a few promising drug candidates is optimized. Here, regression models that give the reaction activation energy can be used to predict both absolute reactivity and selectivity (Figure 1b). Prediction of absolute reactivity can be used to assess feasibility under process-relevant conditions, while prediction of selectivity is key to reducing purification steps. Predictive tools therefore hold great promise for accelerating route and process development, ultimately delivering medicines to patients both faster and at lower costs. The current workhorse for prediction of organic reactions is density functional theory (DFT, Figure 2a). Since rising to prominence in the early 90s, DFT has enjoyed extraordinary success in rationalizing reactivity and selectivity across the reaction spectrum by modelling the full reaction mechanism. 12 The success of DFT can be traced in part due to a fortuitous cancellation of errors, which makes it particularly suited for properties such as enantioselectivity, which depends on the relative energies of two structurally very similar transition states (TSs). However, this cancellation of errors does not generally extend to the prediction of the absolute magnitude of reactions barriers (activation free energies, \u0394G \u2021 ). In particular, DFT struggles with one very important class of reactions: ionic reactions in solution. Plata and Singleton even suggested that computed mechanisms of this type can be so flawed that they are \"not even wrong\". 13 Similarly, Maseras and co-workers only achieved agreement with experiment for the simple condensation of an imine and an aldehyde in water by introducing an ad-hoc correction factor, even when using more accurate methods than DFT. 14 These results point to the fact that the largest error in the DFT simulations is often due to the poor performance of the solvation model. Machine learning represents a potential solution to the problems of DFT. Based on reaction data in different solvents, machine learning models could in principle learn to compensate for both the deficiencies in the DFT energies and the solvation model. Accurate QSRR machine learning models ( Figure 2b) have been constructed for, e.g., cycloaddition, 15,16 SN2 substitution, 17 and E2 elimination. 18 While these models are highly encouraging, they treat simple reactions that occur in a single mechanistic step and they are based on an amount of kinetic data (>500 samples) that is only available for very few reaction classes. It is also not clear how they would incorporate the effects of more complex reaction conditions, such as the use of catalysts or reagents. Another promising line of research uses machine learning to predict DFT barrier heights and then use these barrier heights to predict experimental outcomes. A recent study from Hong and co-workers used the ratio of predicted DFT barriers to predict regioselectivity in radical C\u2212H functionalization reactions. 19 While these models can show good performance, the predicted barriers still suffer from the shortcomings of the underlying DFT method and solvation model. We therefore believe that for models to be broadly applicable in guiding experiments, they should be trained to reproduce experimental rather than computed barrier heights. Based on the recent success of machine learning for modelling reaction barriers, we wondered if we could combine the traditional mechanistic modelling using DFT with machine learning in a hybrid method (Figure 2d). Machine learning would here be used to correct for the deficiencies in the mechanistic modelling. Hybrid models could potentially reach useful chemical accuracy (error below 1 kcal/mol) 20,21 with fewer training data than QSRR models, be able to treat more complicated multi-step reactions, and naturally incorporate the effect of catalysts directly in the DFT calculations. Mechanistic models are also chemically understandable and the results can be presented to the chemist with both a view of the computed mechanism and a value for the associated barrier. As a prototype application for a hybrid model, we study the nucleophilic aromatic substitution (SNAr) reaction (Figure 1c), one of the most important reactions in chemistry in general and the pharmaceutical chemistry in particular. The SNAr rea...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc04896h"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "GAUCHE: A Library for Gaussian Processes in Chemistry\nRyan-Rhys Griffiths * 1 Leo Klarner * 2 Henry B. Moss * 3 Aditya Ravuri * 1 Sang Truong * 4 Bojana Rankovic * 5\nYuanqi Du * 6 Arian Jamasb 1 Julius Schwartz 1 Austin Tripp 1 Gregory Kell 7 Anthony Bourached 8\nAlex J. Chan 1 Jacob Moss 1 Chengzhi Guo 1 Alpha A. Lee 1 Philippe Schwaller 5 Jian Tang 9 10 11\nAbstract\nWe introduce GAUCHE, a library for GAUssian\nprocesses in CHEmistry. Gaussian processes have\nlong been a cornerstone of probabilistic machine\nlearning, affording particular advantages for un\u0002certainty quantification and Bayesian optimisa\u0002tion. Extending Gaussian processes to chemical\nrepresentations however is nontrivial, necessitat\u0002ing kernels defined over structured inputs such as\ngraphs, strings and bit vectors. By defining such\nkernels in GAUCHE, we seek to open the door\nto powerful tools for uncertainty quantification\nand Bayesian optimisation in chemistry. Moti\u0002vated by scenarios frequently encountered in ex\u0002perimental chemistry, we showcase applications\nfor GAUCHE in molecular discovery and chemi\u0002cal reaction optimisation. The codebase is made\navailable at https://github.com/leojklarner/gauche\n1. Introduction\nEarly-stage scientific discovery is typically characterised by\nthe small data regime due to the limited availability of high\u0002quality experimental data (Zhang & Ling, 2018; Thawani\net al., 2020). Much of the novelty of discovery relies on\nthe fact that there is much knowledge to gain in the small\ndata regime. By contrast, in the big data regime, discov\u0002ery offers diminishing returns as much of the knowledge\nabout the space of interest has already been acquired. As\nsuch, machine learning methodologies that facilitate search\nin small data regimes such as Bayesian optimisation (BO)\n(Gomez-Bombarelli et al. \u00b4 , 2018; Griffiths & Hernandez- \u00b4\nLobato, 2020; Shields et al., 2021; Du et al., 2022) and\n*Equal contribution 1University of Cambridge 2University of\nOxford 3Secondmind Labs 4Stanford University 5EPFL 6Cornell\nUniversity 7King\u2019s College London 8University College London\n9Mila Quebec AI Institute 10CIFAR AI Research Chair 11Chair\nHEC Montreal National Research Council Canada. Correspon\u0002dence to: Ryan-Rhys Griffiths <rrg27@cam.ac.uk>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy\u0002right 2022 by the author(s).\nactive learning (AL) (Zhang et al., 2019; Jablonka et al.,\n2021) have great potential to expedite the rate at which per\u0002formant molecules, molecular materials, chemical reactions\nand proteins are discovered.\nTo date in molecular machine learning, Bayesian neural\nnetworks (BNNs) have been the surrogate of choice to pro\u0002duce the uncertainty estimates that underpin BO and AL\n(Ryu et al., 2019; Zhang et al., 2019; Hwang et al., 2020;\nScalia et al., 2020). For small datasets, however, deep neu\u0002ral networks are often not the model of choice. Notably,\ncertain deep learning experts have voiced a preference for\nGaussian processes (GPs) in the small data regime (Bengio,\n2011). Furthermore, for BO, GPs possess particularly ad\u0002vantageous properties; first, they admit exact as opposed to\napproximate Bayesian inference and second, few of their\nparameters need to be determined by hand. In the words of\nSir David MacKay (MacKay et al., 2003),\n\u201dGaussian processes are useful tools for auto\u0002mated tasks where fine tuning for each problem\nis not possible. We do not appear to sacrifice any\nperformance for this simplicity.\u201d\nThe iterative model refitting required in BO makes it a prime\nexample of such an automated task. Although BNN surro\u0002gates have been trialled for BO (Snoek et al., 2015; Sprin\u0002genberg et al., 2016), GPs remain the model of choice as\nevidenced by the results of the recent NeurIPS Black-Box\nOptimisation Competition (Turner et al., 2021).\nTraining GPs on molecular inputs is non-trivial however.\nCanonical applications of GPs assume continuous input\nspaces of low and fixed dimensionality. The most popu\u0002lar molecular input representations are SMILES/SELFIES\nstrings (Anderson et al., 1987; Weininger, 1988; Krenn et al.,\n2020), fingerprints (Rogers & Hahn, 2010; Probst & Rey\u0002mond, 2018; Capecchi et al., 2020) and graphs (Duvenaud\net al., 2015; Kearnes et al., 2016). Each of these input\nrepresentations poses problems for GPs. SMILES strings\nhave variable length, fingerprints are high-dimensional\nand sparse bit vectors, while graphs are also a form of\nnon-continuous input. To construct a GP framework over\nmolecules, GAUCHE provides GPU-based implementations\nGAUCHE: A Software Library for Gaussian Processes in Chemistry\nof kernels that operate on molecular inputs, including string,\nfingerprint and graph kernels. Furthermore, GAUCHE in\u0002cludes support for protein and chemical reaction representa\u0002tions and interfaces with the GPyTorch (Gardner et al., 2018)\nand BoTorch (Balandat et al., 2020) libraries to facilitate\nusage for advanced probabilistic modelling and BO.\nConcretely, our contributions may be summarised as:\n1. We propose a GP framework for molecules and chemi\u0002cal reactions.\n2. We provide an open-source, GPU-enabled library build\u0002ing on GPyTorch (Gardner et al., 2018), BoTorch (Ba\u0002landat et al., 2020) and RDKit (Landrum, 2013).\n3. We extend the use of black box graph kernels (from\nGraKel, Siglidis et al. (2020)) to GP regression via a\nGPyTorch interface, along with a limited set of graph\nkernels implemented in native GPyTorch to enable\noptimisation of the graph kernel hyperparameters under\nthe marginal likelihood.\n4. We conduct benchmark experiments evaluating the\nutility of the GP framework on regression, uncertainty\nquantification and BO tasks.\nGAUCHE includes tutorials to guide users through the\ntasks considered in this paper and is made available at\nhttps://github.com/leojklarner/gauche\n2. Background\nWe summarise the background on Gaussian processes,\nBayesian optimisation, common molecular representations\nand how GP kernels may be extended to cater for them.\n2.1. Gaussian Processes\nNotation: X \u2208 R\nn\u00d7d\nis a design matrix of n training ex\u0002amples of dimension d. A given row i of the design matrix\ncontains a training molecule\u2019s representation xi. A GP is\nspecified by a mean function, m(x) = E[f(x)] and a covari\u0002ance function k(x, x\n0\n) = E[(f(x)\u2212m(x))(f(x\n0\n)\u2212m(x))].\nK\u03b8(X, X) is a kernel matrix where entries are computed\nby the kernel function as [K]ij = k(xi, xj ). \u03b8 represents\nthe set of kernel hyperparameters. The GP specifies the full\ndistribution over the function f to be modelled as\nf(x) \u223c GPm(x), k(x, x\n0\n)\n\u0001\n.\nPrediction: At test locations X\u2217 the GP returns a pre\u0002dictive mean, f\u00af\u2217 = K(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121y\nand a predictive uncertainty cov(f\u2217) = K(X\u2217, X\u2217) \u2212\nK(X\u2217, X)[K(X, X) + \u03c3\n2\ny\nI]\n\u22121K(X, X\u2217).\nKernel Functions: The choice of kernel function is an\nimportant inductive bias for the properties of the function\nbeing modelled. A common choice for continuous input\ndomains is the radial basis function kernel\nkRBF(x, x\n0\n) = \u03c3\n2\nf\nexp \u0012\n\u2212||x \u2212 x\n0\n||2\n2\n2`\n2\n\u0013\n,\nwhere \u03c3\n2\nf\nis the signal amplitude hyperparameter (vertical\nlengthscale) and ` is the (horizontal) lengthscale hyperpa\u0002rameter. The symbol \u03b8, introduced previously, is used to\nrepresent the set of kernel hyperparameters. For molecules,\nbespoke kernel functions will need to be defined for struc\u0002tured input spaces.\nGP Training: Hyperparameters for Gaussian processes\ncomprise kernel hyperparameters, \u03b8 in addition to the like\u0002lihood noise, \u03c3\n2\ny\n. These hyperparameters are chosen by\noptimising an objective function known as the negative log\nmarginal likelihood (NLML)\nlog p(y|X, \u03b8) = \u2212\n1\n2\ny\n>(K\u03b8(X, X) + \u03c32\ny\nI)\n\u22121y\n| {z }\nencourages fit with data\n\u2212\n1\n2\nlog |K\u03b8(X, X) + \u03c3\n2\ny\nI|\n| {z }\ncontrols model capacity\n\u2212\nN\n2\nlog(2\u03c0).\nI\u03c32\ny\nrepresents the variance of i.i.d. Gaussian noise on\nthe observations y. The NLML embodies Occam\u2019s razor\nfor Bayesian model selection (Rasmussen & Ghahramani,\n2001) in favouring models that fit the data without being\noverly co...",
      "url": "https://henrymoss.github.io/files/gauche.pdf"
    },
    {
      "title": "Kernel Methods for Predicting Yields of Chemical Reactions",
      "text": "Kernel Methods for Predicting Yields of\nChemical Reactions\nAlexe L. Haywood,a Joseph Redshaw,a Magnus W. D.\nHanson-Heine,a Adam Taylor,b Alex Brown,b Andy M. Mason,b\nThomas G\u00e4rtnerc and Jonathan D. Hirsta\u2217\na School of Chemistry, University of Nottingham, University Park, Nottingham,\nNG7 2RD, UK. b GlaxoSmithKline, Gunnels Wood Rd, Stevenage, SG1 2NY, UK.\nc Machine Learning Group, TU Wien Informatics, Vienna, Austria\nAbstract\nThe use of machine learning methods for the prediction of reaction yield is an emerging area.\nWe demonstrate the applicability of support vector regression (SVR) for predicting reaction yields,\nusing combinatorial data. Molecular descriptors used in regression tasks related to chemical reac\u0002tivity have often been based on time-consuming, computationally demanding quantum chemical\ncalculations, usually density functional theory. Structure-based descriptors (molecular fingerprints\nand molecular graphs) are quicker and easier to calculate, and are applicable to any molecule.\nIn this study, SVR models built on structure-based descriptors were compared to models built on\nquantum chemical descriptors. The models were evaluated along the dimension of each reaction\ncomponent in a set of Buchwald-Hartwig amination reactions. The structure-based SVR models\nout-performed the quantum chemical SVR models, along the dimension of each reaction compo\u0002nent. The applicability of the models was assessed with respect to similarity to training. Prospec\u0002tive predictions of unseen Buchwald-Hartwig reactions are presented for synthetic assessment, to\nvalidate the generalisability of the models, with particular interest along the aryl halide dimension.\n1\n1 Introduction\nAdvances in medicinal chemistry rely on the discovery and synthesis of novel molecules. Time,\ncost and efficiency pressures in the pharmaceutical industry are key drivers in accelerating drug\ndesign and development. The success of artificial intelligence and machine learning in other fields,\nsuch as image recognition and text processing, has sparked increased interest in their application\nto drug discovery.1\u20133 This attention includes the design and optimisation of small molecules. The\navailability of large reaction datasets and high-performance computing have been key in the devel\u0002opment of computer-aided chemistry,4\nfor example in: molecular design,5retrosynthetic planning\ntools,6\u201310 reaction prediction10\u201312 and the optimisation of reaction conditions.13\u201315\nWhilst the prediction of biological activities and molecular properties using quantitative structure\u0002activity or structure-property relationship (QSAR/QSPR) models has been well-studied,1,16 reac\u0002tivity prediction, has been explored much less. This is largely due to a lack of appropriately curated\ndata, for example, on reaction yield and enantiomeric excess (%ee). Performing a large number of\nexperimental reactions is expensive, time-consuming, resource-consuming and requires synthetic\nchemists. High-throughput chemistry, along with batch and flow systems, have recently opened up\nopportunities to generate reaction data for use in machine learning.17\u201319\nSupport vector machines (SVM) are a supervised learning technique that use labelled training\ndata to predict the label of unlabelled data.20 It can be applied to classification and regression prob\u0002lems, whereby the label is either a class/category or continuous value, respectively. For non-linear\nrelationships, SVMs use a kernel function to map data from an input space to a high-dimensional\nfeature space, where classification or regression is performed linearly. The kernel function com\u0002putes the inner product in the feature space directly, without applying the non-linear transfor\u0002mations at a higher computational cost. Different types of kernels have been assessed for both\nclassification and regression problems related to chemo-21 and bioinformatics.22\u201326 Applications\nof SVMs in chemistry include bioactivity prediction, toxicity-related properties and physicochemi\u0002cal property prediction.1,26\u201329\nA dataset consisting of chemical structures or reactions must converted to a machine readable\nformat before presented to a machine learning algorithm. Molecular descriptors are based on the\nstructural, physiochemical, electronic, or topological nature of molecules. Quantum chemical de\u0002scriptors are common for the prediction of chemical reactivity.19,30\u201332 They have also been used to\n2\nbuild kernel-based QSAR and QSPR models, employing the Gaussian radial basis function (RBF)\nkernel.33\u201335 Site-specific, atomic properties including NMR shifts, vibrational frequencies, vibra\u0002tional intensities and partial atomic charges have been used, along with global descriptors such as\nHOMO (Highest Occupied Molecular Orbital) energies, LUMO (Lowest Unoccupied Molecular Or\u0002bital) energies, dipole moment and polar surface area. Three-dimensional steric descriptors have\nbeen included in models of catalyst selectivity to improve predictions, by capturing important con\u0002formational information.31,32 Quantum chemical descriptors are typically calculated using density\nfunctional theory (DFT), which can be computationally demanding. Therefore, quantum chemical\ndescriptors may not always be appropriate for large datasets, particularly if the dataset contains\nlarge molecules. Site-specific descriptors require overlapping, common structural features within\nthe molecules.19,30,31 Reaction components that consist of a large variety of molecules with no\nkey shared atoms between them all, require alternative representations such as structure-based\ndescriptors.\nA chemical hashed fingerprint defines the two-dimensional topology of a molecule in the form\nof a vector of binary bits. For example, MACCS Keys36 depict the presence or absence of a set\nof predefined structural fragments, while other fingerprints consider each atom and its local en\u0002vironment. Morgan circular fingerprints37 encode the neighbourhood within a particular radius\nof each atom, whereas RDK fingerprints38 encode topological paths up to a specified path length.\nMolecular fingerprints are fast and easy to calculate, making them a popular choice for represent\u0002ing molecules. They are established in machine learning for virtual screening39 and have emerged\nin the prediction of reaction conditions.13,14 Sandfort et al.40 have shown that two-dimensional,\nstructure-based molecular fingerprints can achieve similar accuracy to quantum chemical descrip\u0002tors in the prediction of chemical reactivity. Reactions were represented by a concatenation of mul\u0002tiple fingerprint features (MFFs) and were used to build random forest models to predict reaction\nyields and %ee.40 Fingerprints have also been utilised in kernel-based QSAR/QSPR relationship\nmodels, using the Tanimoto or RBF kernel.27\u201329\nMolecular graphs are another two-dimensional representation that depict the atoms and bonds\nwithin molecules as a set of nodes and edges. The global molecular structure is considered, in\ncontrast to the local environments in fingerprints. The kernel trick can be applied to molecular\ngraphs to build machine learning models based on kernel methods, including SVMs.41 Kriege et\nal.23 give a detailed overview of graph kernels and provide guidelines to aid researchers in the\n3\nidentification of successful kernels for different applications. Molecular graphs have been used\nin combination with deep learning to generate graph convolutional network models for reaction\nprediction,12 retrosynthetic route design7 and the prediction of reaction conditions.42\nThe prediction of reaction yields and enantiomeric excess are multidimensional problems as\nreaction outcomes depend on multiple reaction parameters, including both categorical and contin\u0002uous variables. Small changes in the reaction conditions such as catalyst(s), reagent(s), solvent(s),\nas well as temperature and pressure can result in radically different reaction outcomes or possibly\nfailed reactions. Even with the chemical intuition and experienc...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60c756f1f96a000435288ba1/original/kernel-methods-for-predicting-yields-of-chemical-reactions.pdf"
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "# GAUCHE: A Library for Gaussian Processes in Chemistry\nRyan-Rhys Griffiths, Leo Klarner, Henry Moss, Aditya Ravuri, Sang Truong, Yuanqi Du, Samuel Stanton, Gary Tom, Bojana Rankovic, Arian Jamasb, Aryan Deshwal, Julius Schwartz, Austin Tripp, Gregory Kell, Simon Frieder, Anthony Bourached, Jacob Moss, Chengzhi Guo, Johannes P. D\u00fcrholt, Saudamini Chaurasia, Ji Won Park, Felix Strieth-Kalthoff, Alpha Lee, Bingquing Cheng, Al\u00e1n Aspuru-Guzik, Philippe Schwaller, Jian Tang\nDecember 2022\n[DOI](https://doi.org/https://doi.org/10.48550/arXiv.2212.04450)\n### Abstract\nWe introduce GAUCHE, an open-source library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to molecular representations, however, necessitates kernels defined over structured inputs such as graphs, strings and bit vectors. By providing such kernels in a modular, robust and easy-to-use framework, we seek to enable expert chemists and materials scientists to make use of state-of-the-art black-box optimization techniques. Motivated by scenarios frequently encountered in practice, we showcase applications for GAUCHE in molecular discovery, chemical reaction optimisation and protein design. The codebase is made available at [https://github.com/leojklarner/gauche](https://github.com/leojklarner/gauche).",
      "url": "https://fsk-lab.github.io/proceedings.neurips.cc/paper_files/paper/2023/hash/f2b1b2e974fa5ea622dd87f22815f423-Abstract-Conference.html"
    }
  ]
}