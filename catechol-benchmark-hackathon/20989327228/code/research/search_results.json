{
  "query": "How to handle leave-one-group-out cross-validation in machine learning for better generalization?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the scikit\u2011learn documentation and a practical Stack\u202fOverflow example, you can apply Leave\u2011One\u2011Group\u2011Out (LOGO) cross\u2011validation as follows to obtain a more realistic estimate of generalization when samples share a natural grouping (e.g., subjects, time periods, devices)\u202f([LeaveOneGroupOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html); [Stack\u202fOverflow](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn)):\n\n1. **Prepare the data and a group label array**  \n   - `X` \u2013 feature matrix, `y` \u2013 target vector.  \n   - `groups` \u2013 1\u2011D array where each entry is an integer (or hashable) identifier for the group to which the corresponding sample belongs (e.g., subject ID, year, device).  \n\n2. **Instantiate the LOGO splitter**  \n   ```python\n   from sklearn.model_selection import LeaveOneGroupOut\n   logo = LeaveOneGroupOut()\n   n_splits = logo.get_n_splits(groups=groups)   # number of distinct groups\n   ```  \n   This creates train/test splits that leave out **all** samples of one group at a time\u202f([LeaveOneGroupOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html)).\n\n3. **Loop over the splits, training and evaluating the model**  \n   ```python\n   scores = []\n   for train_idx, test_idx in logo.split(X, y, groups=groups):\n       X_train, X_test = X[train_idx], X[test_idx]\n       y_train, y_test = y[train_idx], y[test_idx]\n\n       model = YourEstimator()          # e.g., RandomForestClassifier()\n       model.fit(X_train, y_train)       # train on all groups except the left\u2011out one\n       preds = model.predict(X_test)     # evaluate on the held\u2011out group\n       scores.append(metric(y_test, preds))   # e.g., accuracy, f1, mse\n   ```\n   Each iteration treats an entire group as the test set, preventing information leakage across related samples and thus yielding a stricter generalization estimate.\n\n4. **Aggregate the results**  \n   - Compute the mean (and optionally the standard deviation) of `scores` to report overall performance.  \n   - Because every group is used once as a test set, the aggregated metric reflects how the model would perform on **unseen groups**\u2014the scenario you care about for real\u2011world deployment.\n\n5. **(Optional) Hyper\u2011parameter tuning with nested LOGO**  \n   - Wrap the outer LOGO loop (steps\u202f2\u20114) around an inner cross\u2011validation (e.g., `GridSearchCV` with `GroupKFold`) to select hyper\u2011parameters without contaminating the outer test groups.  \n   - This nested approach further guards against over\u2011optimistic estimates.\n\n**Key points for better generalization**\n\n- **Group\u2011level splitting** ensures that correlated samples (same subject, same time slice, etc.) never appear simultaneously in training and test folds, avoiding optimistic bias.  \n- **Leave\u2011One\u2011Group\u2011Out** uses the maximum possible test size for each group, which is especially useful when the number of groups is small or when each group represents a distinct domain.  \n- Combine LOGO with **nested cross\u2011validation** if you need to tune models while preserving the group\u2011wise separation.\n\nFollowing these steps will let you implement LOGO cross\u2011validation correctly and obtain performance metrics that better reflect how your model will generalize to new, unseen groups.",
      "url": ""
    },
    {
      "title": "LeaveOneGroupOut #",
      "text": "LeaveOneGroupOut &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# LeaveOneGroupOut[#](#leaveonegroupout)\n*class*sklearn.model\\_selection.LeaveOneGroupOut[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1323)[#](#sklearn.model_selection.LeaveOneGroupOut)\nLeave One Group Out cross-validator.\nProvides train/test indices to split data such that each training set is\ncomprised of all samples except ones belonging to one specific group.\nArbitrary domain specific group information is provided as an array of integers\nthat encodes the group of each sample.\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\nRead more in the[User Guide](../cross_validation.html#leave-one-group-out).\nSee also\n[`GroupKFold`](sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)\nK-fold iterator variant with non-overlapping groups.\nNotes\nSplits are ordered according to the index of the group left out. The first\nsplit has testing set consisting of the group whose index in`groups`is\nlowest, and so on.\nExamples\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.model\\_selectionimportLeaveOneGroupOut&gt;&gt;&gt;X=np.array([[1,2],[3,4],[5,6],[7,8]])&gt;&gt;&gt;y=np.array([1,2,1,2])&gt;&gt;&gt;groups=np.array([1,1,2,2])&gt;&gt;&gt;logo=LeaveOneGroupOut()&gt;&gt;&gt;logo.get\\_n\\_splits(groups=groups)2&gt;&gt;&gt;print(logo)LeaveOneGroupOut()&gt;&gt;&gt;fori,(train\\_index,test\\_index)inenumerate(logo.split(X,y,groups)):...print(f&quot;Fold{i}:&quot;)...print(f&quot; Train: index={train\\_index}, group={groups[train\\_index]}&quot;)...print(f&quot; Test: index={test\\_index}, group={groups[test\\_index]}&quot;)Fold 0:Train: index=[2 3], group=[2 2]Test: index=[0 1], group=[1 1]Fold 1:Train: index=[0 1], group=[1 1]Test: index=[2 3], group=[2 2]\n```\nget\\_metadata\\_routing()[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1550)[#](#sklearn.model_selection.LeaveOneGroupOut.get_metadata_routing)\nGet metadata routing of this object.\nPlease check[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nReturns:**routing**MetadataRequest\nA[`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest)encapsulating\nrouting information.\nget\\_n\\_splits(*X=None*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1386)[#](#sklearn.model_selection.LeaveOneGroupOut.get_n_splits)\nReturns the number of splitting iterations in the cross-validator.\nParameters:**X**array-like of shape (n\\_samples, n\\_features), default=None\nAlways ignored, exists for API compatibility.\n**y**array-like of shape (n\\_samples,), default=None\nAlways ignored, exists for API compatibility.\n**groups**array-like of shape (n\\_samples,), default=None\nGroup labels for the samples used while splitting the dataset into\ntrain/test set. This \u2018groups\u2019 parameter must always be specified to\ncalculate the number of splits, though the other parameters can be\nomitted.\nReturns:**n\\_splits**int\nReturns the number of splitting iterations in the cross-validator.\nset\\_split\\_request(*\\**,*groups:[bool](https://docs.python.org/3/library/functions.html#bool)|[None](https://docs.python.org/3/library/constants.html#None)|[str](https://docs.python.org/3/library/stdtypes.html#str)='$UNCHANGED$'*)&#x2192;[LeaveOneGroupOut](#sklearn.model_selection.LeaveOneGroupOut)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1315)[#](#sklearn.model_selection.LeaveOneGroupOut.set_split_request)\nConfigure whether metadata should be requested to be passed to the`split`method.\nNote that this method is only relevant when this estimator is used as a\nsub-estimator within a[meta-estimator](../../glossary.html#term-meta-estimator)and metadata routing is enabled\nwith`enable\\_metadata\\_routing=True`(see[`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config)).\nPlease check the[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nThe options for each parameter are:\n* `True`: metadata is requested, and passed to`split`if provided. The request is ignored if metadata is not provided.\n* `False`: metadata is not requested and the meta-estimator will not pass it to`split`.\n* `None`: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n* `str`: metadata should be passed to the meta-estimator with this given alias instead of the original name.\nThe default (`sklearn.utils.metadata\\_routing.UNCHANGED`) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\nAdded in version 1.3.\nParameters:**groups**str, True, False, or None, default=sklearn.utils.metadata\\_routing.UNCHANGED\nMetadata routing for`groups`parameter in`split`.\nReturns:**self**object\nThe updated object.\nsplit(*X*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1413)[#](#sklearn.model_selection.LeaveOneGroupOut.split)\nGenerate indices to split data into training and test set.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nTraining data, where`n\\_samples`is the number of samples\nand`n\\_features`is the number of features.\n**y**array-like of shape (n\\_samples,), default=None\nThe target variable for supervised learning problems.\n**groups**array-like of shape (n\\_samples,)\nGroup labels for the samples used while splitting the dataset into\ntrain/test set.\nYields:**train**ndarray\nThe training set indices for that split.\n**test**ndarray\nThe testing set indices for that split.\n**On this page\n### This Page\n* [Show Source](../../_sources/modules/generated/sklearn.model_selection.LeaveOneGroupOut.rst.txt)",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html"
    },
    {
      "title": "How to apply Leave-one-Group-out cross validation in sklearn?",
      "text": "##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [How to apply Leave-one-Group-out cross validation in sklearn?](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked3 years, 3 months ago\n\nModified [3 years, 3 months ago](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn?lastactivity)\n\nViewed\n5k times\n\n5\n\nI am building a Naive Bayes classifier (nb) using sklearn.\n\nThe dataset consists of 4 subjects with each a different amount of labeled data.\n\nI want to apply leave-one-subject-out cross validation, but I do not find a comparable example on the internet.\n\nMy data consists of the following:\n\n```\nx = [[2,0],[3,1],[2,1],[3,2]], [[4,2],[5,3],[5,2],[5,3]], [[7,3],[6,2],[7,1],[6,2]], [[2,3],[2,4],[3,4],[2,3]]]\ny = [[0,1,3,2],[1,2,3,2],[0,1,1,1],[0,1,2,1]]\n\n```\n\nSo the data of each subject is a subarray in x with the corresponding subarray of y. The input features consists of 2 elements each (for example the mean and std of an accelerometer).\n\nI found on the internet the example of\n\n> sklearn.model\\_selection.LeaveOneOut\n\nBut this does not work in my example since I want to take data of a whole subject as a test set.\n\nIs there a good equivalent for my needs?\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [python-3.x](https://stackoverflow.com/questions/tagged/python-3.x)\n- [validation](https://stackoverflow.com/questions/tagged/validation)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n\n[Share](https://stackoverflow.com/q/66734121)\n\n[Improve this question](https://stackoverflow.com/posts/66734121/edit)\n\nFollow\n\n[edited Mar 23, 2021 at 20:33](https://stackoverflow.com/posts/66734121/revisions)\n\n[![Miguel Trejo's user avatar](https://www.gravatar.com/avatar/c4b84c32bf7371540723f44839b3456e?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/12483346/miguel-trejo)\n\n[Miguel Trejo](https://stackoverflow.com/users/12483346/miguel-trejo)\n\n6,50955 gold badges2727 silver badges5252 bronze badges\n\nasked Mar 21, 2021 at 15:22\n\n[![Jan D.M.'s user avatar](https://www.gravatar.com/avatar/aa651c69537c4436c25bb3eebeb98856?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/7657749/jan-d-m)\n\n[Jan D.M.](https://stackoverflow.com/users/7657749/jan-d-m) Jan D.M.\n\n2,43233 gold badges2222 silver badges3333 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n6\n\nThe sklearn's method `LeaveOneGroupOut` is what you're looking for, just pass a `group` parameter that will define each subject to leave out from the train set. From the [docs](https://scikit-learn.org/stable/modules/cross_validation.html#leave-one-group-out):\n\n> Each training set is thus constituted by all the samples except the ones related to a specific group.\n\nto adapt it to your data, just concatenate the list of lists.\n\n```\nimport itertools\nfrom sklearn.model_selection import LeaveOneGroupOut\n\njoined_x = list(itertools.chain.from_iterable(x))\njoined_y = list(itertools.chain.from_iterable(y))\n\nlogo = LeaveOneGroupOut()\nfor train, test in logo.split(joined_x, joined_y, groups=joined_y):\n    print(\"%s %s\" % (train, test))\n>>>\n[ 1  2  3  4  5  6  7  9 10 11 13 14 15] [ 0  8 12]\n[ 0  2  3  5  6  7  8 12 14] [ 1  4  9 10 11 13 15]\n[ 0  1  2  4  6  8  9 10 11 12 13 15] [ 3  5  7 14]\n[ 0  1  3  4  5  7  8  9 10 11 12 13 14 15] [2 6]\n\n```\n\nIn the first training set group 0 is the test, the second group 1 and so on.\n\n## EDIT\n\nAs @JanDM requested, to use it with `cross_val_score` the `groups` parameter should be passed as it is [pass to](https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/model_selection/_validation.py#L255) the `split()` method of the crossvalidator `cv`\n\n```\nimport itertools\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(estimator, joined_x, joined_y, cv=logo, groups=joined_y)\n\n```\n\n[Share](https://stackoverflow.com/a/66734528)\n\n[Improve this answer](https://stackoverflow.com/posts/66734528/edit)\n\nFollow\n\n[edited Mar 23, 2021 at 14:27](https://stackoverflow.com/posts/66734528/revisions)\n\nanswered Mar 21, 2021 at 16:02\n\n[![Miguel Trejo's user avatar](https://www.gravatar.com/avatar/c4b84c32bf7371540723f44839b3456e?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/12483346/miguel-trejo)\n\n[Miguel Trejo](https://stackoverflow.com/users/12483346/miguel-trejo) Miguel Trejo\n\n6,50955 gold badges2727 silver badges5252 bronze badges\n\n3\n\n- Could you update your answer on how to use this in combination with the sklearn.model\\_selection.cross\\_val\\_score()? I don't see how it would work together\n\n\u2013\u00a0[Jan D.M.](https://stackoverflow.com/users/7657749/jan-d-m)\n\nCommentedMar 23, 2021 at 9:11\n\n- 1\n\n\n\n\n\n@JanD.M. please see the edit, just need to pass a `groups` parameter to `cross_val_score`\n\n\u2013\u00a0[Miguel Trejo](https://stackoverflow.com/users/12483346/miguel-trejo)\n\nCommentedMar 23, 2021 at 14:29\n\n- So this means the Groups are my labels y? I am not sure\n\n\u2013\u00a0[Paul](https://stackoverflow.com/users/5817580/paul)\n\nCommentedAug 20, 2022 at 18:47\n\n\n[Add a comment](https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f66734121%2fhow-to-apply-leave-one-group-out-cross-validation-in-sklearn%23new-answer)\n\nSign up using Google\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [python](https://stackoverflow.com/questions/tagged/python) - [python-3.x](https://stackoverflow.com/questions/tagged/python-3.x) - [validation](https://stackoverflow.com/questions/tagged/validation) - [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)   or [ask your own question](https://stackoverflow.com/questions/ask).\n\n- Featured on Meta\n- [We spent a sprint addressing your requests \u2014 here\u2019s how it went](https://meta.stackexchange.com/questions/401060/we-spent-a-sprint-addressing-your-requests-here-s-how-it-went)\n\n- [Upcoming initiatives on Stack Overflow and across the Stack Exchange network...](https://meta.stackexchange.com/questions/401061/upcoming-initiatives-on-stack-overflow-and-across-the-stack-exchange-network-ju)\n\n- [The \\[lib\\] tag is being burninated](https://meta.stackoverflow.com/questions/411327/the-lib-tag-is-being-burninated)\n\n- [What makes a homepage useful for logged-in users](https://meta.stackoverflow.com/qu...",
      "url": "https://stackoverflow.com/questions/66734121/how-to-apply-leave-one-group-out-cross-validation-in-sklearn"
    },
    {
      "title": "sklearn.model_selection .LeaveOneGroupOut \u00b6",
      "text": "sklearn.model\\_selection.LeaveOneGroupOut &#8212;&#8212; scikit-learn 0.18.2 documentation\n[![Logo](../../_static/scikit-learn-logo-small.png)](../../index.html)\n* [Home](../../index.html)\n* [Installation](../../install.html)\n* [Documentation](../../documentation.html)\n* Scikit-learn 0.18 (stable)\n* [Tutorials](../../tutorial/index.html)\n* [User guide](../../user_guide.html)\n* [API](../classes.html)\n* [FAQ](../../faq.html)\n* [Contributing](../../developers/contributing.html)\n* * [Scikit-learn 0.19-dev (development)](http://scikit-learn.org/dev/documentation.html)\n* [Scikit-learn 0.17](http://scikit-learn.org/0.17/documentation.html)\n* [Scikit-learn 0.16](http://scikit-learn.org/0.16/documentation.html)\n* [Scikit-learn 0.15](http://scikit-learn.org/0.15/documentation.html)\n* [PDF documentation](../../_downloads/scikit-learn-docs.pdf)\n* [Examples](../../auto_examples/index.html)[\n](javascript:void(0);)\n[![Fork me on GitHub](../../_static/img/forkme.png)](https://github.com/scikit-learn/scikit-learn)\n[Previous\nsklearn.model...sklearn.model\\_selection.StratifiedKFold](sklearn.model_selection.StratifiedKFold.html)\n[Next\nsklearn.model...sklearn.model\\_selection.LeavePGroupsOut](sklearn.model_selection.LeavePGroupsOut.html)\n[Up\nAPI ReferenceAPI Reference](../classes.html)\nThis documentation is for scikit-learn**version 0.18.2**&mdash;[Other versions](http://scikit-learn.org/stable/support.html#documentation-resources)\nIf you use the software, please consider[citing scikit-learn](../../about.html#citing-scikit-learn).\n* [`sklearn.model\\_selection`.LeaveOneGroupOut](#)\n# [`sklearn.model\\_selection`](../classes.html#module-sklearn.model_selection).LeaveOneGroupOut[\u00b6](#sklearn-model-selection-leaveonegroupout)\n*class*`sklearn.model\\_selection.``LeaveOneGroupOut`[[source]](https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L737)[\u00b6](#sklearn.model_selection.LeaveOneGroupOut)\nLeave One Group Out cross-validator\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\nRead more in the[User Guide](../cross_validation.html#cross-validation).\nExamples\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimportLeaveOneGroupOut&gt;&gt;&gt;X=np.array([[1,2],[3,4],[5,6],[7,8]])&gt;&gt;&gt;y=np.array([1,2,1,2])&gt;&gt;&gt;groups=np.array([1,1,2,2])&gt;&gt;&gt;logo=LeaveOneGroupOut()&gt;&gt;&gt;logo.get\\_n\\_splits(X,y,groups)2&gt;&gt;&gt;print(logo)LeaveOneGroupOut()&gt;&gt;&gt;fortrain\\_index,test\\_indexinlogo.split(X,y,groups):...print(&quot;TRAIN:&quot;,train\\_index,&quot;TEST:&quot;,test\\_index)...X\\_train,X\\_test=X[train\\_index],X[test\\_index]...y\\_train,y\\_test=y[train\\_index],y[test\\_index]...print(X\\_train,X\\_test,y\\_train,y\\_test)TRAIN: [2 3] TEST: [0 1][[5 6][7 8]] [[1 2][3 4]] [1 2] [1 2]TRAIN: [0 1] TEST: [2 3][[1 2][3 4]] [[5 6][7 8]] [1 2] [1 2]\n```\nMethods\n[`get\\_n\\_splits`](#sklearn.model_selection.LeaveOneGroupOut.get_n_splits)(X,y,groups)|Returns the number of splitting iterations in the cross-validator|\n[`split`](#sklearn.model_selection.LeaveOneGroupOut.split)(X[,y,groups])|Generate indices to split data into training and test set.|\n`\\_\\_init\\_\\_`()[[source]](https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L60)[\u00b6](#sklearn.model_selection.LeaveOneGroupOut.__init__)`get\\_n\\_splits`(*X*,*y*,*groups*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L789)[\u00b6](#sklearn.model_selection.LeaveOneGroupOut.get_n_splits)\nReturns the number of splitting iterations in the cross-validator\n|Parameters:|\n**X**: object\n> > Always ignored, exists for compatibility.\n> **y**: object\n> > Always ignored, exists for compatibility.\n> **groups**: array-like, with shape (n\\_samples,), optional\n> > Group labels for the samples used while splitting the dataset intotrain/test set.\n> |\nReturns:|\n**n\\_splits**: int\n> > Returns the number of splitting iterations in the cross-validator.\n> |\n`split`(*X*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L65)[\u00b6](#sklearn.model_selection.LeaveOneGroupOut.split)\nGenerate indices to split data into training and test set.\n|Parameters:|\n**X**: array-like, shape (n\\_samples, n\\_features)\n> > Training data, where n_samples is the number of samplesand n_features is the number of features.\n> **y**: array-like, of length n\\_samples\n> > The target variable for supervised learning problems.\n> **groups**: array-like, with shape (n\\_samples,), optional\n> > Group labels for the samples used while splitting the dataset intotrain/test set.\n> |\nReturns:|\n**train**: ndarray\n> > The training set indices for that split.\n> **test**: ndarray\n> > The testing set indices for that split.\n> |\n&copy; 2010 - 2016, scikit-learn developers (BSD License).[Show this page source](../../_sources/modules/generated/sklearn.model_selection.LeaveOneGroupOut.txt)\n[Previous](sklearn.model_selection.StratifiedKFold.html)\n[Next](sklearn.model_selection.LeavePGroupsOut.html)",
      "url": "https://scikit-learn.org/0.18/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html"
    },
    {
      "title": "LeaveOneOut #",
      "text": "LeaveOneOut &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# LeaveOneOut[#](#leaveoneout)\n*class*sklearn.model\\_selection.LeaveOneOut[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L172)[#](#sklearn.model_selection.LeaveOneOut)\nLeave-One-Out cross-validator.\nProvides train/test indices to split data in train/test sets. Each\nsample is used once as a test set (singleton) while the remaining\nsamples form the training set.\nNote:`LeaveOneOut()`is equivalent to`KFold(n\\_splits=n)`and`LeavePOut(p=1)`where`n`is the number of samples.\nDue to the high number of test sets (which is the same as the\nnumber of samples) this cross-validation method can be very costly.\nFor large datasets one should favor[`KFold`](sklearn.model_selection.KFold.html#sklearn.model_selection.KFold),[`ShuffleSplit`](sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)or[`StratifiedKFold`](sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold).\nRead more in the[User Guide](../cross_validation.html#leave-one-out).\nSee also\n[`LeaveOneGroupOut`](sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut)\nFor splitting the data according to explicit, domain-specific stratification of the dataset.\n[`GroupKFold`](sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)\nK-fold iterator variant with non-overlapping groups.\nExamples\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.model\\_selectionimportLeaveOneOut&gt;&gt;&gt;X=np.array([[1,2],[3,4]])&gt;&gt;&gt;y=np.array([1,2])&gt;&gt;&gt;loo=LeaveOneOut()&gt;&gt;&gt;loo.get\\_n\\_splits(X)2&gt;&gt;&gt;print(loo)LeaveOneOut()&gt;&gt;&gt;fori,(train\\_index,test\\_index)inenumerate(loo.split(X)):...print(f&quot;Fold{i}:&quot;)...print(f&quot; Train: index={train\\_index}&quot;)...print(f&quot; Test: index={test\\_index}&quot;)Fold 0:Train: index=[1]Test: index=[0]Fold 1:Train: index=[0]Test: index=[1]\n```\nget\\_metadata\\_routing()[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1550)[#](#sklearn.model_selection.LeaveOneOut.get_metadata_routing)\nGet metadata routing of this object.\nPlease check[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nReturns:**routing**MetadataRequest\nA[`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest)encapsulating\nrouting information.\nget\\_n\\_splits(*X*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L226)[#](#sklearn.model_selection.LeaveOneOut.get_n_splits)\nReturns the number of splitting iterations in the cross-validator.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nTraining data, where`n\\_samples`is the number of samples\nand`n\\_features`is the number of features.\n**y**array-like of shape (n\\_samples,), default=None\nAlways ignored, exists for API compatibility.\n**groups**array-like of shape (n\\_samples,), default=None\nAlways ignored, exists for API compatibility.\nReturns:**n\\_splits**int\nReturns the number of splitting iterations in the cross-validator.\nsplit(*X*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L63)[#](#sklearn.model_selection.LeaveOneOut.split)\nGenerate indices to split data into training and test set.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nTraining data, where`n\\_samples`is the number of samples\nand`n\\_features`is the number of features.\n**y**array-like of shape (n\\_samples,), default=None\nThe target variable for supervised learning problems.\n**groups**array-like of shape (n\\_samples,), default=None\nAlways ignored, exists for API compatibility.\nYields:**train**ndarray\nThe training set indices for that split.\n**test**ndarray\nThe testing set indices for that split.\n**On this page\n### This Page\n* [Show Source](../../_sources/modules/generated/sklearn.model_selection.LeaveOneOut.rst.txt)",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html"
    },
    {
      "title": "LeavePGroupsOut #",
      "text": "LeavePGroupsOut &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# LeavePGroupsOut[#](#leavepgroupsout)\n*class*sklearn.model\\_selection.LeavePGroupsOut(*n\\_groups*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1440)[#](#sklearn.model_selection.LeavePGroupsOut)\nLeave P Group(s) Out cross-validator.\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\nThe difference between LeavePGroupsOut and LeaveOneGroupOut is that\nthe former builds the test sets with all the samples assigned to`p`different values of the groups while the latter uses samples\nall assigned the same groups.\nRead more in the[User Guide](../cross_validation.html#leave-p-groups-out).\nParameters:**n\\_groups**int\nNumber of groups (`p`) to leave out in the test split.\nSee also\n[`GroupKFold`](sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)\nK-fold iterator variant with non-overlapping groups.\nExamples\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.model\\_selectionimportLeavePGroupsOut&gt;&gt;&gt;X=np.array([[1,2],[3,4],[5,6]])&gt;&gt;&gt;y=np.array([1,2,1])&gt;&gt;&gt;groups=np.array([1,2,3])&gt;&gt;&gt;lpgo=LeavePGroupsOut(n\\_groups=2)&gt;&gt;&gt;lpgo.get\\_n\\_splits(groups=groups)3&gt;&gt;&gt;print(lpgo)LeavePGroupsOut(n\\_groups=2)&gt;&gt;&gt;fori,(train\\_index,test\\_index)inenumerate(lpgo.split(X,y,groups)):...print(f&quot;Fold{i}:&quot;)...print(f&quot; Train: index={train\\_index}, group={groups[train\\_index]}&quot;)...print(f&quot; Test: index={test\\_index}, group={groups[test\\_index]}&quot;)Fold 0:Train: index=[2], group=[3]Test: index=[0 1], group=[1 2]Fold 1:Train: index=[1], group=[2]Test: index=[0 2], group=[1 3]Fold 2:Train: index=[0], group=[1]Test: index=[1 2], group=[2 3]\n```\nget\\_metadata\\_routing()[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1550)[#](#sklearn.model_selection.LeavePGroupsOut.get_metadata_routing)\nGet metadata routing of this object.\nPlease check[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nReturns:**routing**MetadataRequest\nA[`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest)encapsulating\nrouting information.\nget\\_n\\_splits(*X=None*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1517)[#](#sklearn.model_selection.LeavePGroupsOut.get_n_splits)\nReturns the number of splitting iterations in the cross-validator.\nParameters:**X**array-like of shape (n\\_samples, n\\_features), default=None\nAlways ignored, exists for API compatibility.\n**y**array-like of shape (n\\_samples,), default=None\nAlways ignored, exists for API compatibility.\n**groups**array-like of shape (n\\_samples,), default=None\nGroup labels for the samples used while splitting the dataset into\ntrain/test set. This \u2018groups\u2019 parameter must always be specified to\ncalculate the number of splits, though the other parameters can be\nomitted.\nReturns:**n\\_splits**int\nReturns the number of splitting iterations in the cross-validator.\nset\\_split\\_request(*\\**,*groups:[bool](https://docs.python.org/3/library/functions.html#bool)|[None](https://docs.python.org/3/library/constants.html#None)|[str](https://docs.python.org/3/library/stdtypes.html#str)='$UNCHANGED$'*)&#x2192;[LeavePGroupsOut](#sklearn.model_selection.LeavePGroupsOut)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1315)[#](#sklearn.model_selection.LeavePGroupsOut.set_split_request)\nConfigure whether metadata should be requested to be passed to the`split`method.\nNote that this method is only relevant when this estimator is used as a\nsub-estimator within a[meta-estimator](../../glossary.html#term-meta-estimator)and metadata routing is enabled\nwith`enable\\_metadata\\_routing=True`(see[`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config)).\nPlease check the[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nThe options for each parameter are:\n* `True`: metadata is requested, and passed to`split`if provided. The request is ignored if metadata is not provided.\n* `False`: metadata is not requested and the meta-estimator will not pass it to`split`.\n* `None`: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n* `str`: metadata should be passed to the meta-estimator with this given alias instead of the original name.\nThe default (`sklearn.utils.metadata\\_routing.UNCHANGED`) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\nAdded in version 1.3.\nParameters:**groups**str, True, False, or None, default=sklearn.utils.metadata\\_routing.UNCHANGED\nMetadata routing for`groups`parameter in`split`.\nReturns:**self**object\nThe updated object.\nsplit(*X*,*y=None*,*groups=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/model_selection/_split.py#L1544)[#](#sklearn.model_selection.LeavePGroupsOut.split)\nGenerate indices to split data into training and test set.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nTraining data, where`n\\_samples`is the number of samples\nand`n\\_features`is the number of features.\n**y**array-like of shape (n\\_samples,), default=None\nThe target variable for supervised learning problems.\n**groups**array-like of shape (n\\_samples,)\nGroup labels for the samples used while splitting the dataset into\ntrain/test set.\nYields:**train**ndarray\nThe training set indices for that split.\n**test**ndarray\nThe testing set indices for that split.\n**On this page\n### This Page\n* [Show Source](../../_sources/modules/generated/sklearn.model_selection.LeavePGroupsOut.rst.txt)",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html"
    },
    {
      "title": "Statistics > Methodology",
      "text": "[2311.17100] Automatic cross-validation in structured models: Is it time to leave out leave-one-out?\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2311.17100\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Methodology\n**arXiv:2311.17100**(stat)\n[Submitted on 28 Nov 2023 ([v1](https://arxiv.org/abs/2311.17100v1)), last revised 7 Mar 2024 (this version, v2)]\n# Title:Automatic cross-validation in structured models: Is it time to leave out leave-one-out?\nAuthors:[A. Adin](https://arxiv.org/search/stat?searchtype=author&amp;query=Adin,+A),[E. Krainski](https://arxiv.org/search/stat?searchtype=author&amp;query=Krainski,+E),[A. Lenzi](https://arxiv.org/search/stat?searchtype=author&amp;query=Lenzi,+A),[Z. Liu](https://arxiv.org/search/stat?searchtype=author&amp;query=Liu,+Z),[J. Mart\u00ednez-Minaya](https://arxiv.org/search/stat?searchtype=author&amp;query=Mart\u00ednez-Minaya,+J),[H. Rue](https://arxiv.org/search/stat?searchtype=author&amp;query=Rue,+H)\nView a PDF of the paper titled Automatic cross-validation in structured models: Is it time to leave out leave-one-out?, by A. Adin and 5 other authors\n[View PDF](https://arxiv.org/pdf/2311.17100)[HTML (experimental)](https://arxiv.org/html/2311.17100v2)> > Abstract:\n> Standard techniques such as leave-one-out cross-validation (LOOCV) might not be suitable for evaluating the predictive performance of models incorporating structured random effects. In such cases, the correlation between the training and test sets could have a notable impact on the model&#39;s prediction error. To overcome this issue, an automatic group construction procedure for leave-group-out cross validation (LGOCV) has recently emerged as a valuable tool for enhancing predictive performance measurement in structured models. The purpose of this paper is (i) to compare LOOCV and LGOCV within structured models, emphasizing model selection and predictive performance, and (ii) to provide real data applications in spatial statistics using complex structured models fitted with INLA, showcasing the utility of the automatic LGOCV method. First, we briefly review the key aspects of the recently proposed LGOCV method for automatic group construction in latent Gaussian models. We also demonstrate the effectiveness of this method for selecting the model with the highest predictive performance by simulating extrapolation tasks in both temporal and spatial data analyses. Finally, we provide insights into the effectiveness of the LGOCV method in modelling complex structured data, encompassing spatio-temporal multivariate count data, spatial compositional data, and spatio-temporal geospatial data. Subjects:|Methodology (stat.ME); Applications (stat.AP); Computation (stat.CO)|\nCite as:|[arXiv:2311.17100](https://arxiv.org/abs/2311.17100)[stat.ME]|\n|(or[arXiv:2311.17100v2](https://arxiv.org/abs/2311.17100v2)[stat.ME]for this version)|\n|[https://doi.org/10.48550/arXiv.2311.17100](https://doi.org/10.48550/arXiv.2311.17100)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|Spatial Statistics (2024)|\nRelated DOI:|[https://doi.org/10.1016/j.spasta.2024.100843](https://doi.org/10.1016/j.spasta.2024.100843)\nFocus to learn more\nDOI(s) linking to related resources\n|\n## Submission history\nFrom: Aritz Adin [[view email](https://arxiv.org/show-email/15dba0a1/2311.17100)]\n**[[v1]](https://arxiv.org/abs/2311.17100v1)**Tue, 28 Nov 2023 08:01:35 UTC (11,136 KB)\n**[v2]**Thu, 7 Mar 2024 15:14:42 UTC (10,897 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Automatic cross-validation in structured models: Is it time to leave out leave-one-out?, by A. Adin and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2311.17100)\n* [HTML (experimental)](https://arxiv.org/html/2311.17100v2)\n* [TeX Source](https://arxiv.org/src/2311.17100)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ME\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2311.17100&amp;function=prev&amp;context=stat.ME) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2311.17100&amp;function=next&amp;context=stat.ME)\n[new](https://arxiv.org/list/stat.ME/new)|[recent](https://arxiv.org/list/stat.ME/recent)|[2023-11](https://arxiv.org/list/stat.ME/2023-11)\nChange to browse by:\n[stat](https://arxiv.org/abs/2311.17100?context=stat)\n[stat.AP](https://arxiv.org/abs/2311.17100?context=stat.AP)\n[stat.CO](https://arxiv.org/abs/2311.17100?context=stat.CO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.17100)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.17100)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.17100)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2311.17100&amp;description=Automatic cross-validation in structured models: Is it time to leave out leave-one-out?>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2311.17100&amp;title=Automatic cross-validation in structured models: Is it time to leave out leave-one-out?>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore rec...",
      "url": "https://arxiv.org/abs/2311.17100"
    },
    {
      "title": "",
      "text": "On the Dangers of Cross-Validation. An Experimental Evaluation\nR. Bharat Rao\nIKM CKS Siemens Medical Solutions USA\nGlenn Fung\nIKM CKS Siemens Medical Solutions USA\nRomer Rosales\nIKM CKS Siemens Medical Solutions USA\nAbstract\nCross validation allows models to be tested using the\nfull training set by means of repeated resampling; thus,\nmaximizing the total number of points used for testing\nand potentially, helping to protect against overfitting.\nImprovements in computational power, recent reduc\u0002tions in the (computational) cost of classification algo\u0002rithms, and the development of closed-form solutions\n(for performing cross validation in certain classes of\nlearning algorithms) makes it possible to test thousand\nor millions of variants of learning models on the data.\nThus, it is now possible to calculate cross validation per\u0002formance on a much larger number of tuned models than\nwould have been possible otherwise. However, we em\u0002pirically show how under such large number of models\nthe risk for overfitting increases and the performance\nestimated by cross validation is no longer an effective\nestimate of generalization; hence, this paper provides\nan empirical reminder of the dangers of cross valida\u0002tion. We use a closed-form solution that makes this\nevaluation possible for the cross validation problem of\ninterest. In addition, through extensive experiments we\nexpose and discuss the effects of the overuse/misuse of\ncross validation in various aspects, including model se\u0002lection, feature selection, and data dimensionality. This\nis illustrated on synthetic, benchmark, and real-world\ndata sets.\n1 Introduction\nIn a general classification problem, the goal is to learn a\nclassifier that performs well on unseen data drawn from\nthe same distribution as the available data 1; in other\nwords, to learn classifiers with good generalization. One\ncommon way to estimate generalization capabilities is\nto measure the performance of the learned classifier on\ntest data that has not been used to train the classifier.\nWhen a large test data set cannot be held out or easily\n1We concentrate on performance on data drawn from the same\ndistribution but performance on a different distribution is also a\n(less explored) problem of interest.\nacquired, resampling methods, such as cross validation,\nare commonly used to estimate the generalization er\u0002ror. The resulting estimates of generalization can also\nbe used for model selection by choosing from various\npossible classification algorithms (models) the one that\nhas the lowest cross validation error (and hence the low\u0002est expected generalization error).\nA strong argument in favor of using cross validation\nis the potential of using the entire training set for\ntesting (albeit not at once), creating the largest possible\ntest set for a fixed training data set. Essentially,\nthe classifier is trained on a subset of the training\ndata set, and tested on the remainder. This process\nis repeated systematically so that all the points in\nthe training set are tested. There has been much\nstudy on the empirical behavior of cross-validation for\nerror estimation and model selection, and more recently\ntheoretical bounds on the error in the leave-one-out\ncross-validation (loocv) estimate. Much of the focus\nhas been on the expected value of this error over all\ntraining sets of a given sample size and the asymptotic\nbehavior as the sample size increases. In this paper we\nempirically address the pitfalls of using cross validation\nerror to select among a large number of classification\nalgorithms.\nResampling methods, such as bootstrapping or\ncross validation (Stone, 1977; Kohavi, 1995a; Weiss &\nKulikowski, 1991; Efron & Tibshirani, 1993) have typ\u0002ically been used to measure the generalization perfor\u0002mance of a chosen algorithm, or possibly to select be\u0002tween a limited set of algorithms. Until the last decade,\ncross validation experiments could reasonable be per\u0002formed only on a small set of algorithms or possible\nmodels; a k-fold or loocv run for a single algorithm,\neven on a small dataset, typically ran for several hours,\nif not days. As computers have become more power\u0002ful and due to recent advances regarding the compu\u0002tational efficiency of popular classification algorithms\nand techniques (for example: linear training time for\nSVMs (Joachims, 2006) and n log(n) kernel computa\u0002tion (Raykar & Duraiswami, 2005)), cross validation\nperformance can be quickly computed on several thou\u0002sands or even millions of algorithms. Recent develop\u0002ments in grid computing now allow computers distrib\u0002uted in a large geographic area to be harnessed for a spe\u0002cific task, exponentially increasing the computing power\nat hand.\nIt is a commonly held believe that cross validation,\nlike any other tool or metric, can be abused (Ng,\n1997). Some basic heuristic procedures have been\nemployed to avoid these problems. For example, when\npossible a sequestered test set is kept aside. This\nset is finally used only after training to verify that\nthe chosen classifier indeed has superior generalization.\nAny modeling decisions based upon experiments on the\ntraining set, even cross validation estimates, are suspect,\nuntil independently verified.\nDespite certain general knowledge about the draw\u0002backs attached to cross validation, there has not been a\nsufficiently clear experimental (practical) investigation\non the behavior of the estimate of generalization error\nfor a fixed data set.\nIn this paper we provide an empirical reminder\nof a fact that is known but usually underestimated:\nwhen the set of algorithms to be considered becomes\nlarge, cross validation is no longer a good measure\nof generalization performance, and accordingly can no\nlonger be used for algorithm or feature selection. In\naddition we experimentally show the impact of cross\nvalidation as the data dimensionality increases and\nfor feature selection. We provide experimental results\non synthetic, standardized benchmark (from the UCI\nrepository), and a real-world dataset related to clinical\ndiagnosis in virtual colonoscopy.\n2 Related Research\nA fundamental issue in machine learning is to obtain\nan accurate estimate of the generalization error of a\nmodel trained on a finite data set. Precisely estimating\nthe accuracy of a model is not only important to ex\u0002amine the generalization performance of an algorithm,\nbut also for choosing an algorithm from a variety of\nlearning algorithms. Empirical estimators based upon\nresampling, which include bootstrap (Efron & Tibshi\u0002rani, 1993), cross validation (Stone, 1977) estimates are\npopular, and Holdout estimates where a test set is se\u0002questered until the model is frozen are also used.\nA fair amount of research has focused on the\nempirical performance of leave-one-out cross validation\n(loocv) and k-fold CV on synthetic and benchmark\ndata sets. Experiments by (Breiman & Spector, 1992)\nshow that k-fold CV has better empirical performance\nthan loocv for feature selection for linear regression.\n(Kohavi, 1995b) also obtains results in favor of 10-\nfold cross validation using decision trees and Naive\nBayes, and demonstrates the bias-variance trade-off for\ndifferent values of k on multiple benchmark data sets.\n(Kohavi & Wolpert, 1996) discuss the bias-variance\ntrade-off for classifiers using a misclassification loss\nfunction. Our work, while not directly related to the\nbias-variance trade-off is closely related to the notion of\nvariance.\nFrom a theoretical perspective, the most general\ntheoretical results for training error estimates are pro\u0002vided by (Vapnik, 1982) who proved that the training\nerror estimate is less than O(\np\nV C/n) away from the\ntrue test error where V C is the VC dimension of a hy\u0002pothesis space. More recently, the task of developing\nupper bounds on the loocv error for a specific method\u0002ology has drawn the attention in the learning theory\ncommunity. For example, (Zhang, 2003) has derived\nupper bounds on the expected loocv error to show con\u0002sistency for a set of kernel methods. These consistenc...",
      "url": "https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf"
    },
    {
      "title": "Cross-Validation: K-Fold vs. Leave-One-Out | Baeldung on Computer Science",
      "text": "```\n\n```\n\n## 1\\. Overview\n\nIn this tutorial, **we\u2019ll talk about two cross-validation techniques in machine learning: the k-fold and leave-one-out methods**. To do so, we\u2019ll start with the train-test splits and explain why we need cross-validation in the first place. Then, we\u2019ll describe the two cross-validation techniques and compare them to illustrate their pros and cons.\n\n## 2\\. Train-Test Split Method\n\nAn important decision when developing any [machine learning](https://www.baeldung.com/cs/machine-learning-how-to-start) model is how to evaluate its final performance. **To get an unbiased estimate of the model\u2019s performance, we need to evaluate it on the data we didn\u2019t use for training.**\n\n**The simplest way to [split the data](https://www.baeldung.com/cs/train-test-datasets-ratio) is to use the train-test split method.** It randomly partitions the dataset into two subsets (called training and test sets) so that the predefined percentage of the entire dataset is in the training set.\n\nThen, we train our machine learning model on the training set and evaluate its performance on the test set. In this way, we are always sure that **the samples used for training are not used for [evaluation](https://www.baeldung.com/cs/ml-train-validate-test) and vice versa**.\n\nVisually, this is how the train-test split method works:\n\n![train-test split](https://www.baeldung.com/wp-content/uploads/sites/4/2022/05/train_test.png)\n\n## 3\\. Introduction to Cross-Validation\n\nHowever, the train-split method has certain limitations. **When the dataset is small, the method is prone to high variance.** Due to the random partition, the results can be entirely different for different test sets. Why? Because in some partitions, [samples](https://www.baeldung.com/cs/ml-stratified-sampling) that are easy to classify get into the test set, while in others, the test set receives the \u2018difficult\u2019 ones.\n\n**To deal with this issue, we use [cross-validation](https://www.baeldung.com/cs/cross-validation-decision-trees) to evaluate the performance of a machine learning model.** In cross-validation, we don\u2019t divide the dataset into training and test sets only once. Instead, we repeatedly partition the dataset into smaller groups and then average the performance in each group. That way, we reduce the impact of partition randomness on the results.\n\nMany cross-validation techniques define different ways to divide the dataset at hand. We\u2019ll focus on the two most frequently used: the k-fold and the leave-one-out methods.\n\n## 4\\. K-Fold Cross-Validation\n\nIn k-fold cross-validation, we first divide our dataset into k equally sized subsets. **Then, we repeat the train-test method k times such that each time one of the k subsets is used as a test set and the rest k-1 subsets are used together as a training set.** Finally, we compute the estimate of the model\u2019s performance estimate by averaging the scores over the k trials.\n\nFor example, let\u2019s suppose that we have a dataset ![S = \\{x_1, x_2, x_3, x_4, x_5, x_6\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-34a93d38642ea9cdbb3af18e61560a1d_l3.svg) containing 6 samples and that we want to perform a 3-fold cross-validation.\n\nFirst, we divide ![S](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-52fd2a0fc27878e7dfce68d4632b4ffb_l3.svg) into 3 subsets randomly. For instance:\n\n![S_1 = \\{x_1, x_2\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-9d0cd6c7c640eb9fc76fc803608317c2_l3.svg)\n\n![S_2 = \\{x_3, x_4\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-10c13d4d996abb7ded2207d081d9cfb6_l3.svg)\n\n![S_3 = \\{x_5, x_6\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-a8aee302fcfb1a71fc13a211d7a0e9c4_l3.svg)\n\nThen, we train and evaluate our machine-learning model 3 times. Each time, two subsets form the training set, while the remaining one acts as the test set. In our example:\n\n![k-fold cross validation](https://www.baeldung.com/wp-content/uploads/sites/4/2022/05/kfold.png)\n\nFinally, the overall performance is the average of the model\u2019s performance scores on those three test sets:\n\n![\\[\\text{overall score} = \\frac{score_1 + score_2 + score_3}{3}\\]](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-9f801ec979cb31d043ab99f8784fc1f1_l3.svg)\n\n## 5\\. Leave-One-Out Cross-Validation\n\nIn the leave-one-out (LOO) cross-validation, we train our machine-learning model ![n](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ec4217f4fa5fcd92a9edceba0e708cf7_l3.svg) times where ![n](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ec4217f4fa5fcd92a9edceba0e708cf7_l3.svg) is to our dataset\u2019s size. **Each time, only one sample is used as a test set while the rest are used to train our model.**\n\nWe\u2019ll show that **LOO is an extreme case of k-fold where ![\\mathbf{k=n}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-6665418095b07379628c03f3648c3c45_l3.svg).** If we apply LOO to the previous example, we\u2019ll have 6 test subsets:\n\n![S_1 = \\{x_1\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-68d8b416f7098c6e075db77bf8d008c0_l3.svg)\n\n![S_2 = \\{x_2\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-6c1d427966f4cc357d655107b2824f5d_l3.svg)\n\n![S_3 = \\{x_3\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-eb624b7078b653ca3af508315ece4352_l3.svg)\n\n![S_4 = \\{x_4\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-d66e4e2988fe61931098fde3ef8237b7_l3.svg)\n\n![S_5 = \\{x_5\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-44c63908eba95d73c7a4c5415064a153_l3.svg)\n\n![S_6 = \\{x_6\\}](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-08bf47432769f31b339d2c533d9adf33_l3.svg)\n\nIterating over them, we use ![S \\setminus S_i](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-00b71631bb9b6a63341acfe2f584179c_l3.svg) as the training data in iteration ![i=1,2,\\ldots, 6](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-f3832077e55dd129814ac393734f1b72_l3.svg), and evaluate the model on ![S_i](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ab31408519664b6114c4d5b687b47eb7_l3.svg):\n\n![leave-one-out (LOO) cross-validation](https://www.baeldung.com/wp-content/uploads/sites/4/2022/05/loso.png)\n\nThe final performance estimate is the average of the six individual scores:\n\n![\\[\\text{overall score} = \\frac{score_1 + score_2 + score_3 + score_4 + score_5 + score_6}{6}\\]](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-01d42c626feb31650e5d6f5a65b5d7fe_l3.svg)\n\n## 6\\. Comparison\n\n**An important factor when choosing between the k-fold and the LOO cross-validation methods is the size of the dataset.**\n\n**When the size is small, LOO is more appropriate** since it will use more training samples in each iteration. That will enable our model to learn better representations.\n\nConversely, **we use k-fold cross-validation to train a model on a large dataset** since LOO trains ![n](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ec4217f4fa5fcd92a9edceba0e708cf7_l3.svg) models, one per sample in the data. When our dataset contains a lot of samples, training so many models will take too long. So, the k-fold cross-validation is more appropriate.\n\nAlso, in a large dataset, it is sufficient to use less than ![n](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ec4217f4fa5fcd92a9edceba0e708cf7_l3.svg) folds since the test folds are large enough for the estimates to be sufficiently precise.\n\n## 7\\. Conclusion\n\nIn this article, we presented two cross-validation techniques: the k-fold and leave-one-out (LOO) methods. The latter validates our machine learning model more times giving us very precise metrics. However, in the case of large datasets, the k-fold cross-validation will give us sufficiently precise estimates but will spend less time training the model.\n\nWe rely on ads to keep creating quality content for you to enjoy for free.\n\nPlease support our site by disabling your ad blocker or, use...",
      "url": "https://www.baeldung.com/cs/cross-validation-k-fold-loo"
    }
  ]
}