{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5403a0d9",
   "metadata": {},
   "source": [
    "# Uncertainty-Weighted Ensemble with Conservative Predictions\n",
    "\n",
    "**Hypothesis**: The high CV-LB intercept (0.0533 > target 0.0347) suggests the model is overconfident on OOD samples. By using GP uncertainty to identify high-uncertainty predictions and making them more conservative (closer to training mean), we might reduce the intercept.\n",
    "\n",
    "**Key Insight**:\n",
    "- CV-LB relationship: LB = 4.23 × CV + 0.0533 (R² = 0.98)\n",
    "- Intercept (0.0533) > Target (0.0347) - even with CV=0, predicted LB would be 0.0533\n",
    "- The problem is OOD generalization - we need to be more conservative on uncertain predictions\n",
    "\n",
    "**Implementation**:\n",
    "1. Get GP predictions and uncertainty (std)\n",
    "2. Get ensemble predictions (GP + MLP + LGBM)\n",
    "3. For high-uncertainty samples, blend toward training mean\n",
    "4. High uncertainty → more weight to training mean (conservative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15414e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:52.381653Z",
     "iopub.status.busy": "2026-01-16T01:12:52.381130Z",
     "iopub.status.idle": "2026-01-16T01:12:53.995205Z",
     "shell.execute_reply": "2026-01-16T01:12:53.994795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f46397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:53.996489Z",
     "iopub.status.busy": "2026-01-16T01:12:53.996341Z",
     "iopub.status.idle": "2026-01-16T01:12:54.000895Z",
     "shell.execute_reply": "2026-01-16T01:12:54.000547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e55e27a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:54.001797Z",
     "iopub.status.busy": "2026-01-16T01:12:54.001701Z",
     "iopub.status.idle": "2026-01-16T01:12:54.030918Z",
     "shell.execute_reply": "2026-01-16T01:12:54.030586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76709e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:54.032180Z",
     "iopub.status.busy": "2026-01-16T01:12:54.031849Z",
     "iopub.status.idle": "2026-01-16T01:12:54.039014Z",
     "shell.execute_reply": "2026-01-16T01:12:54.038667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n",
      "Simple feature dimension: 18\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "\n",
    "# Simple Featurizer (for GP) - 18 features\n",
    "class SimpleFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange])\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')\n",
    "print(f'Simple feature dimension: {SimpleFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2832805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:54.039925Z",
     "iopub.status.busy": "2026-01-16T01:12:54.039832Z",
     "iopub.status.idle": "2026-01-16T01:12:54.043479Z",
     "shell.execute_reply": "2026-01-16T01:12:54.043153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim=145, hidden_dim=64, output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "print('MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6516175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:54.044298Z",
     "iopub.status.busy": "2026-01-16T01:12:54.044190Z",
     "iopub.status.idle": "2026-01-16T01:12:54.053738Z",
     "shell.execute_reply": "2026-01-16T01:12:54.053417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty-Weighted Ensemble defined\n"
     ]
    }
   ],
   "source": [
    "# Uncertainty-Weighted Ensemble Model\n",
    "class UncertaintyWeightedEnsemble:\n",
    "    \"\"\"\n",
    "    GP + MLP + LGBM ensemble with uncertainty-weighted conservative predictions.\n",
    "    \n",
    "    For high-uncertainty (OOD) samples, blend predictions toward training mean\n",
    "    to reduce overconfident extrapolation errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.30, \n",
    "                 uncertainty_blend=0.3):\n",
    "        self.data = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.uncertainty_blend = uncertainty_blend  # How much to blend toward mean for high uncertainty\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        \n",
    "        self.scaler_full = StandardScaler()\n",
    "        self.scaler_simple = StandardScaler()\n",
    "        self.gp_models = []\n",
    "        self.mlp_model = None\n",
    "        self.lgbm_models = []\n",
    "        self.train_mean = None  # Store training mean for conservative predictions\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        # Store training mean for conservative predictions\n",
    "        self.train_mean = Y_np.mean(axis=0)\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.fit_transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.fit_transform(X_simple)\n",
    "        \n",
    "        # Train GP for each target (with return_std capability)\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(\n",
    "                kernel=kernel, alpha=0.01, n_restarts_optimizer=3, random_state=42\n",
    "            )\n",
    "            gp.fit(X_simple_scaled, Y_np[:, i])\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        # Train MLP\n",
    "        self.mlp_model = MLPModel(\n",
    "            input_dim=self.full_featurizer.feats_dim,\n",
    "            hidden_dim=64,\n",
    "            output_dim=3,\n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        Y_tensor = torch.tensor(Y_np, dtype=torch.double).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp_model.train()\n",
    "        for epoch in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.mlp_model(X_tensor)\n",
    "            loss = criterion(pred, Y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train LGBM for each target\n",
    "        for i in range(3):\n",
    "            lgbm = lgb.LGBMRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, min_child_samples=5, random_state=42, verbose=-1\n",
    "            )\n",
    "            lgbm.fit(X_full_scaled, Y_np[:, i])\n",
    "            self.lgbm_models.append(lgbm)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.transform(X_simple)\n",
    "        \n",
    "        # GP predictions with uncertainty\n",
    "        gp_preds = []\n",
    "        gp_stds = []\n",
    "        for i in range(3):\n",
    "            pred, std = self.gp_models[i].predict(X_simple_scaled, return_std=True)\n",
    "            gp_preds.append(pred)\n",
    "            gp_stds.append(std)\n",
    "        gp_preds = np.column_stack(gp_preds)\n",
    "        gp_stds = np.column_stack(gp_stds)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp_model.eval()\n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        with torch.no_grad():\n",
    "            mlp_preds = self.mlp_model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgbm_preds = np.column_stack([self.lgbm_models[i].predict(X_full_scaled) for i in range(3)])\n",
    "        \n",
    "        # Standard ensemble prediction\n",
    "        ensemble_preds = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "        \n",
    "        # Compute uncertainty weight (normalized std across all targets)\n",
    "        avg_std = gp_stds.mean(axis=1, keepdims=True)\n",
    "        max_std = avg_std.max() if avg_std.max() > 0 else 1.0\n",
    "        uncertainty_weight = np.clip(avg_std / max_std, 0, 1) * self.uncertainty_blend\n",
    "        \n",
    "        # Blend toward training mean for high-uncertainty samples\n",
    "        predictions = (1 - uncertainty_weight) * ensemble_preds + uncertainty_weight * self.train_mean\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            X_full_scaled_flip = self.scaler_full.transform(X_full_flip)\n",
    "            X_simple_scaled_flip = self.scaler_simple.transform(X_simple_flip)\n",
    "            \n",
    "            gp_preds_flip = []\n",
    "            gp_stds_flip = []\n",
    "            for i in range(3):\n",
    "                pred, std = self.gp_models[i].predict(X_simple_scaled_flip, return_std=True)\n",
    "                gp_preds_flip.append(pred)\n",
    "                gp_stds_flip.append(std)\n",
    "            gp_preds_flip = np.column_stack(gp_preds_flip)\n",
    "            gp_stds_flip = np.column_stack(gp_stds_flip)\n",
    "            \n",
    "            X_tensor_flip = torch.tensor(X_full_scaled_flip, dtype=torch.double).to(device)\n",
    "            with torch.no_grad():\n",
    "                mlp_preds_flip = self.mlp_model(X_tensor_flip).cpu().numpy()\n",
    "            lgbm_preds_flip = np.column_stack([self.lgbm_models[i].predict(X_full_scaled_flip) for i in range(3)])\n",
    "            \n",
    "            ensemble_preds_flip = self.gp_weight * gp_preds_flip + self.mlp_weight * mlp_preds_flip + self.lgbm_weight * lgbm_preds_flip\n",
    "            \n",
    "            avg_std_flip = gp_stds_flip.mean(axis=1, keepdims=True)\n",
    "            uncertainty_weight_flip = np.clip(avg_std_flip / max_std, 0, 1) * self.uncertainty_blend\n",
    "            predictions_flip = (1 - uncertainty_weight_flip) * ensemble_preds_flip + uncertainty_weight_flip * self.train_mean\n",
    "            \n",
    "            predictions = (predictions + predictions_flip) / 2\n",
    "        \n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('Uncertainty-Weighted Ensemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab845d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:12:54.054549Z",
     "iopub.status.busy": "2026-01-16T01:12:54.054446Z",
     "iopub.status.idle": "2026-01-16T01:12:55.740614Z",
     "shell.execute_reply": "2026-01-16T01:12:55.740307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([20, 3])\n",
      "Test predictions range: [0.1733, 0.5232]\n",
      "Training mean: [0.18109954 0.19224699 0.48085667]\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: X={X_single.shape}, Y={Y_single.shape}')\n",
    "\n",
    "# Test on a small subset\n",
    "X_train = X_single.iloc[:80]\n",
    "Y_train = Y_single.iloc[:80]\n",
    "X_test = X_single.iloc[80:100]\n",
    "Y_test = Y_single.iloc[80:100]\n",
    "\n",
    "model = UncertaintyWeightedEnsemble(data='single', uncertainty_blend=0.3)\n",
    "model.train_model(X_train, Y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(f'Test predictions shape: {preds.shape}')\n",
    "print(f'Test predictions range: [{preds.min():.4f}, {preds.max():.4f}]')\n",
    "print(f'Training mean: {model.train_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6d2514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:44:57.390410Z",
     "iopub.status.busy": "2026-01-16T01:44:57.389875Z",
     "iopub.status.idle": "2026-01-16T01:52:56.537799Z",
     "shell.execute_reply": "2026-01-16T01:52:56.534175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Solvent CV (Uncertainty-Weighted, blend=0.05) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:20<07:53, 20.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:40<07:23, 20.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [00:58<06:44, 19.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [01:17<06:23, 19.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [01:38<06:16, 19.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [01:58<05:57, 19.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [02:16<05:26, 19.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [02:36<05:09, 19.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [02:55<04:50, 19.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [03:15<04:34, 19.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [03:37<04:22, 20.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [03:57<04:03, 20.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [04:17<03:40, 20.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [04:36<03:18, 19.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [04:58<03:03, 20.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [05:19<02:45, 20.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [05:41<02:28, 21.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [06:00<02:02, 20.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [06:19<01:39, 19.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [06:40<01:21, 20.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [06:58<00:59, 19.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [07:18<00:39, 19.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [07:40<00:20, 20.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [07:59<00:00, 19.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [07:59<00:00, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent MSE: 0.010652 (n=656)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on single solvent data - Try lower blend factor\n",
    "print('\\n=== Single Solvent CV (Uncertainty-Weighted, blend=0.05) ===')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single = []\n",
    "all_actuals_single = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = UncertaintyWeightedEnsemble(data='single', uncertainty_blend=0.05)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single.append(predictions.numpy())\n",
    "    all_actuals_single.append(test_Y.values)\n",
    "\n",
    "preds_single = np.vstack(all_predictions_single)\n",
    "actuals_single = np.vstack(all_actuals_single)\n",
    "mse_single = np.mean((preds_single - actuals_single) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={len(preds_single)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c6cf37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:52:56.539335Z",
     "iopub.status.busy": "2026-01-16T01:52:56.539210Z",
     "iopub.status.idle": "2026-01-16T02:12:51.530560Z",
     "shell.execute_reply": "2026-01-16T02:12:51.530108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Data CV (Uncertainty-Weighted, blend=0.05) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/13 [01:40<20:03, 100.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [03:15<17:49, 97.24s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 3/13 [04:47<15:51, 95.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [06:09<13:26, 89.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 5/13 [07:39<11:58, 89.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 6/13 [08:51<09:46, 83.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [10:18<08:28, 84.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 8/13 [11:54<07:21, 88.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 9/13 [13:19<05:49, 87.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 10/13 [15:00<04:34, 91.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [16:39<03:07, 93.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 12/13 [18:22<01:36, 96.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:54<00:00, 95.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:54<00:00, 91.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data MSE: 0.010029 (n=1227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on full data - Try lower blend factor\n",
    "print('\\n=== Full Data CV (Uncertainty-Weighted, blend=0.05) ===')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = UncertaintyWeightedEnsemble(data='full', uncertainty_blend=0.05)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full.append(predictions.numpy())\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "preds_full = np.vstack(all_predictions_full)\n",
    "actuals_full = np.vstack(all_actuals_full)\n",
    "mse_full = np.mean((preds_full - actuals_full) ** 2)\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={len(preds_full)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d354e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:12:51.536455Z",
     "iopub.status.busy": "2026-01-16T02:12:51.536343Z",
     "iopub.status.idle": "2026-01-16T02:12:51.601788Z",
     "shell.execute_reply": "2026-01-16T02:12:51.601303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV SCORE SUMMARY (Uncertainty-Weighted, blend=0.3) ===\n",
      "Single Solvent MSE: 0.010652 (n=656)\n",
      "Full Data MSE: 0.010029 (n=1227)\n",
      "Overall MSE: 0.010246\n",
      "\n",
      "Best CV (exp_032): 0.008194\n",
      "\n",
      "✗ WORSE: 25.04% worse than best CV\n",
      "\n",
      "Predicted LB (using LB = 4.23*CV + 0.0533): 0.0966\n",
      "Best LB achieved: 0.0877\n",
      "Target: 0.0347\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(preds_single)\n",
    "n_full = len(preds_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (Uncertainty-Weighted, blend=0.3) ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')\n",
    "\n",
    "# Predict LB using CV-LB relationship\n",
    "predicted_lb = 4.23 * overall_mse + 0.0533\n",
    "print(f'\\nPredicted LB (using LB = 4.23*CV + 0.0533): {predicted_lb:.4f}')\n",
    "print(f'Best LB achieved: 0.0877')\n",
    "print(f'Target: 0.0347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = UncertaintyWeightedEnsemble(data='single', uncertainty_blend=0.3)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f134b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = UncertaintyWeightedEnsemble(data='full', uncertainty_blend=0.3)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(f'\\n=== FINAL CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "print(f'Predicted LB: {predicted_lb:.4f}')\n",
    "print(f'Target: 0.0347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a374444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:14:37.363162Z",
     "iopub.status.busy": "2026-01-16T02:14:37.362666Z",
     "iopub.status.idle": "2026-01-16T02:14:37.398567Z",
     "shell.execute_reply": "2026-01-16T02:14:37.398168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest-Neighbor Blending Ensemble defined\n"
     ]
    }
   ],
   "source": [
    "# Try Nearest-Neighbor Blending approach\n",
    "# For OOD samples, predictions from the most similar training solvents might be more reliable\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class NearestNeighborBlendingEnsemble:\n",
    "    \"\"\"\n",
    "    GP + MLP + LGBM ensemble with nearest-neighbor blending.\n",
    "    \n",
    "    For samples far from training data, blend predictions with nearest neighbor predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.30, \n",
    "                 nn_blend_temp=1.0, k_neighbors=3):\n",
    "        self.data = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.nn_blend_temp = nn_blend_temp  # Temperature for distance-based blending\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        \n",
    "        self.scaler_full = StandardScaler()\n",
    "        self.scaler_simple = StandardScaler()\n",
    "        self.gp_models = []\n",
    "        self.mlp_model = None\n",
    "        self.lgbm_models = []\n",
    "        self.nn_model = None\n",
    "        self.train_Y = None\n",
    "        self.train_X_scaled = None\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        self.train_Y = Y_np\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.fit_transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.fit_transform(X_simple)\n",
    "        \n",
    "        self.train_X_scaled = X_full_scaled\n",
    "        \n",
    "        # Train nearest neighbor model\n",
    "        self.nn_model = NearestNeighbors(n_neighbors=self.k_neighbors, metric='euclidean')\n",
    "        self.nn_model.fit(X_full_scaled)\n",
    "        \n",
    "        # Train GP for each target\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(\n",
    "                kernel=kernel, alpha=0.01, n_restarts_optimizer=3, random_state=42\n",
    "            )\n",
    "            gp.fit(X_simple_scaled, Y_np[:, i])\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        # Train MLP\n",
    "        self.mlp_model = MLPModel(\n",
    "            input_dim=self.full_featurizer.feats_dim,\n",
    "            hidden_dim=64,\n",
    "            output_dim=3,\n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        Y_tensor = torch.tensor(Y_np, dtype=torch.double).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp_model.train()\n",
    "        for epoch in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.mlp_model(X_tensor)\n",
    "            loss = criterion(pred, Y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train LGBM for each target\n",
    "        for i in range(3):\n",
    "            lgbm = lgb.LGBMRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, min_child_samples=5, random_state=42, verbose=-1\n",
    "            )\n",
    "            lgbm.fit(X_full_scaled, Y_np[:, i])\n",
    "            self.lgbm_models.append(lgbm)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.transform(X_simple)\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_preds = np.column_stack([self.gp_models[i].predict(X_simple_scaled) for i in range(3)])\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp_model.eval()\n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        with torch.no_grad():\n",
    "            mlp_preds = self.mlp_model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgbm_preds = np.column_stack([self.lgbm_models[i].predict(X_full_scaled) for i in range(3)])\n",
    "        \n",
    "        # Standard ensemble prediction\n",
    "        ensemble_preds = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "        \n",
    "        # Get nearest neighbor predictions\n",
    "        distances, indices = self.nn_model.kneighbors(X_full_scaled)\n",
    "        nn_preds = self.train_Y[indices].mean(axis=1)  # Average of k nearest neighbors\n",
    "        \n",
    "        # Blend based on distance (closer = more weight to model, farther = more weight to NN)\n",
    "        avg_distance = distances.mean(axis=1, keepdims=True)\n",
    "        # Normalize by training set distances\n",
    "        train_distances, _ = self.nn_model.kneighbors(self.train_X_scaled)\n",
    "        train_avg_dist = train_distances.mean()\n",
    "        \n",
    "        # Blend weight: 0 = use model, 1 = use NN\n",
    "        blend_weight = 1 - np.exp(-avg_distance / (self.nn_blend_temp * train_avg_dist))\n",
    "        \n",
    "        predictions = (1 - blend_weight) * ensemble_preds + blend_weight * nn_preds\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            X_full_scaled_flip = self.scaler_full.transform(X_full_flip)\n",
    "            X_simple_scaled_flip = self.scaler_simple.transform(X_simple_flip)\n",
    "            \n",
    "            gp_preds_flip = np.column_stack([self.gp_models[i].predict(X_simple_scaled_flip) for i in range(3)])\n",
    "            \n",
    "            X_tensor_flip = torch.tensor(X_full_scaled_flip, dtype=torch.double).to(device)\n",
    "            with torch.no_grad():\n",
    "                mlp_preds_flip = self.mlp_model(X_tensor_flip).cpu().numpy()\n",
    "            lgbm_preds_flip = np.column_stack([self.lgbm_models[i].predict(X_full_scaled_flip) for i in range(3)])\n",
    "            \n",
    "            ensemble_preds_flip = self.gp_weight * gp_preds_flip + self.mlp_weight * mlp_preds_flip + self.lgbm_weight * lgbm_preds_flip\n",
    "            \n",
    "            distances_flip, indices_flip = self.nn_model.kneighbors(X_full_scaled_flip)\n",
    "            nn_preds_flip = self.train_Y[indices_flip].mean(axis=1)\n",
    "            avg_distance_flip = distances_flip.mean(axis=1, keepdims=True)\n",
    "            blend_weight_flip = 1 - np.exp(-avg_distance_flip / (self.nn_blend_temp * train_avg_dist))\n",
    "            \n",
    "            predictions_flip = (1 - blend_weight_flip) * ensemble_preds_flip + blend_weight_flip * nn_preds_flip\n",
    "            \n",
    "            predictions = (predictions + predictions_flip) / 2\n",
    "        \n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('Nearest-Neighbor Blending Ensemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8554ae64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:14:37.399619Z",
     "iopub.status.busy": "2026-01-16T02:14:37.399437Z",
     "iopub.status.idle": "2026-01-16T02:14:43.400094Z",
     "shell.execute_reply": "2026-01-16T02:14:43.399716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NN Blending Ensemble...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([20, 3])\n"
     ]
    }
   ],
   "source": [
    "# Quick test of NN blending\n",
    "print('Testing NN Blending Ensemble...')\n",
    "X_train = X_single.iloc[:80]\n",
    "Y_train = Y_single.iloc[:80]\n",
    "X_test = X_single.iloc[80:100]\n",
    "\n",
    "model = NearestNeighborBlendingEnsemble(data='single', nn_blend_temp=1.0, k_neighbors=3)\n",
    "model.train_model(X_train, Y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(f'Test predictions shape: {preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b059073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:14:43.401398Z",
     "iopub.status.busy": "2026-01-16T02:14:43.401038Z",
     "iopub.status.idle": "2026-01-16T02:22:48.403639Z",
     "shell.execute_reply": "2026-01-16T02:22:48.403213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Solvent CV (NN Blending, temp=1.0) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:24<09:34, 24.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:45<08:07, 22.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [01:03<07:08, 20.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [01:22<06:37, 19.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [01:43<06:22, 20.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [02:03<06:02, 20.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [02:21<05:30, 19.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [02:41<05:13, 19.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [03:00<04:52, 19.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [03:19<04:32, 19.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [03:41<04:23, 20.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [04:02<04:03, 20.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [04:21<03:38, 19.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [04:40<03:17, 19.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [05:02<03:03, 20.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [05:23<02:45, 20.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [05:46<02:28, 21.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [06:05<02:03, 20.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [06:23<01:39, 19.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [06:45<01:21, 20.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [07:03<00:59, 19.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [07:23<00:39, 19.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [07:45<00:20, 20.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [08:04<00:00, 20.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [08:04<00:00, 20.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent MSE: 0.033500 (n=656)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on single solvent data with NN blending\n",
    "print('\\n=== Single Solvent CV (NN Blending, temp=1.0) ===')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single_nn = []\n",
    "all_actuals_single_nn = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = NearestNeighborBlendingEnsemble(data='single', nn_blend_temp=1.0, k_neighbors=3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single_nn.append(predictions.numpy())\n",
    "    all_actuals_single_nn.append(test_Y.values)\n",
    "\n",
    "preds_single_nn = np.vstack(all_predictions_single_nn)\n",
    "actuals_single_nn = np.vstack(all_actuals_single_nn)\n",
    "mse_single_nn = np.mean((preds_single_nn - actuals_single_nn) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single_nn:.6f} (n={len(preds_single_nn)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1b0bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:22:48.405183Z",
     "iopub.status.busy": "2026-01-16T02:22:48.404886Z",
     "iopub.status.idle": "2026-01-16T02:42:45.100613Z",
     "shell.execute_reply": "2026-01-16T02:42:45.100172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Data CV (NN Blending, temp=1.0) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/13 [01:40<20:04, 100.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [03:15<17:53, 97.57s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 3/13 [04:48<15:54, 95.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [06:10<13:29, 89.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 5/13 [07:40<12:00, 90.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 6/13 [08:53<09:48, 84.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [10:20<08:30, 85.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 8/13 [11:55<07:21, 88.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 9/13 [13:20<05:49, 87.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 10/13 [15:02<04:35, 91.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [16:41<03:07, 93.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 12/13 [18:24<01:36, 96.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:56<00:00, 95.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:56<00:00, 92.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data MSE: 0.024093 (n=1227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on full data with NN blending\n",
    "print('\\n=== Full Data CV (NN Blending, temp=1.0) ===')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full_nn = []\n",
    "all_actuals_full_nn = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = NearestNeighborBlendingEnsemble(data='full', nn_blend_temp=1.0, k_neighbors=3)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full_nn.append(predictions.numpy())\n",
    "    all_actuals_full_nn.append(test_Y.values)\n",
    "\n",
    "preds_full_nn = np.vstack(all_predictions_full_nn)\n",
    "actuals_full_nn = np.vstack(all_actuals_full_nn)\n",
    "mse_full_nn = np.mean((preds_full_nn - actuals_full_nn) ** 2)\n",
    "print(f'Full Data MSE: {mse_full_nn:.6f} (n={len(preds_full_nn)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a70ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:42:45.101824Z",
     "iopub.status.busy": "2026-01-16T02:42:45.101529Z",
     "iopub.status.idle": "2026-01-16T02:42:45.105216Z",
     "shell.execute_reply": "2026-01-16T02:42:45.104847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV SCORE SUMMARY (NN Blending, temp=1.0) ===\n",
      "Single Solvent MSE: 0.033500 (n=656)\n",
      "Full Data MSE: 0.024093 (n=1227)\n",
      "Overall MSE: 0.027371\n",
      "\n",
      "Best CV (exp_032): 0.008194\n",
      "\n",
      "✗ WORSE: 234.03% worse than best CV\n",
      "\n",
      "Predicted LB (using LB = 4.23*CV + 0.0533): 0.1691\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall MSE for NN blending\n",
    "n_single_nn = len(preds_single_nn)\n",
    "n_full_nn = len(preds_full_nn)\n",
    "overall_mse_nn = (mse_single_nn * n_single_nn + mse_full_nn * n_full_nn) / (n_single_nn + n_full_nn)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (NN Blending, temp=1.0) ===')\n",
    "print(f'Single Solvent MSE: {mse_single_nn:.6f} (n={n_single_nn})')\n",
    "print(f'Full Data MSE: {mse_full_nn:.6f} (n={n_full_nn})')\n",
    "print(f'Overall MSE: {overall_mse_nn:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse_nn < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse_nn) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse_nn - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')\n",
    "\n",
    "# Predict LB using CV-LB relationship\n",
    "predicted_lb_nn = 4.23 * overall_mse_nn + 0.0533\n",
    "print(f'\\nPredicted LB (using LB = 4.23*CV + 0.0533): {predicted_lb_nn:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff1601f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:43:55.855174Z",
     "iopub.status.busy": "2026-01-16T02:43:55.854697Z",
     "iopub.status.idle": "2026-01-16T02:43:55.860020Z",
     "shell.execute_reply": "2026-01-16T02:43:55.859635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure GP Ensemble defined\n"
     ]
    }
   ],
   "source": [
    "# Try pure GP model to see if it has different CV-LB relationship\n",
    "# GP might have better OOD generalization due to its uncertainty-aware nature\n",
    "\n",
    "class PureGPEnsemble:\n",
    "    \"\"\"\n",
    "    Pure GP model with multi-output prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, data='single'):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.gp_models = []\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_simple)\n",
    "        \n",
    "        # Train GP for each target\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(\n",
    "                kernel=kernel, alpha=0.01, n_restarts_optimizer=3, random_state=42\n",
    "            )\n",
    "            gp.fit(X_scaled, Y_np[:, i])\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        X_scaled = self.scaler.transform(X_simple)\n",
    "        \n",
    "        gp_preds = np.column_stack([self.gp_models[i].predict(X_scaled) for i in range(3)])\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            X_scaled_flip = self.scaler.transform(X_simple_flip)\n",
    "            gp_preds_flip = np.column_stack([self.gp_models[i].predict(X_scaled_flip) for i in range(3)])\n",
    "            gp_preds = (gp_preds + gp_preds_flip) / 2\n",
    "        \n",
    "        predictions = np.clip(gp_preds, 0, 1)\n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('Pure GP Ensemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0bb08a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:43:55.860980Z",
     "iopub.status.busy": "2026-01-16T02:43:55.860888Z",
     "iopub.status.idle": "2026-01-16T02:51:52.038605Z",
     "shell.execute_reply": "2026-01-16T02:51:52.038299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Solvent CV (Pure GP) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:20<07:54, 20.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [00:40<07:20, 20.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [00:58<06:39, 19.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [01:16<06:17, 18.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [01:37<06:08, 19.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [01:56<05:49, 19.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [02:14<05:20, 18.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [02:33<05:02, 18.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [02:52<04:44, 18.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [03:11<04:26, 19.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [03:32<04:15, 19.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [03:52<03:57, 19.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [04:11<03:34, 19.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [04:30<03:13, 19.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [04:51<02:59, 19.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [05:13<02:42, 20.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [05:35<02:27, 21.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [05:54<02:01, 20.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [06:12<01:38, 19.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [06:33<01:19, 20.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [06:51<00:58, 19.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [07:14<00:41, 20.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [07:36<00:21, 21.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [07:56<00:00, 20.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [07:56<00:00, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent MSE: 0.011722 (n=656)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on single solvent data with pure GP\n",
    "print('\\n=== Single Solvent CV (Pure GP) ===')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single_gp = []\n",
    "all_actuals_single_gp = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PureGPEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single_gp.append(predictions.numpy())\n",
    "    all_actuals_single_gp.append(test_Y.values)\n",
    "\n",
    "preds_single_gp = np.vstack(all_predictions_single_gp)\n",
    "actuals_single_gp = np.vstack(all_actuals_single_gp)\n",
    "mse_single_gp = np.mean((preds_single_gp - actuals_single_gp) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single_gp:.6f} (n={len(preds_single_gp)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f428e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:51:52.039862Z",
     "iopub.status.busy": "2026-01-16T02:51:52.039757Z",
     "iopub.status.idle": "2026-01-16T03:11:39.629251Z",
     "shell.execute_reply": "2026-01-16T03:11:39.628672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Data CV (Pure GP) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/13 [01:39<19:56, 99.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [03:14<17:45, 96.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 3/13 [04:46<15:46, 94.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [06:07<13:22, 89.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 5/13 [07:37<11:54, 89.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 6/13 [08:48<09:43, 83.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [10:15<08:26, 84.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 8/13 [11:50<07:18, 87.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 9/13 [13:14<05:46, 86.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 10/13 [14:55<04:33, 91.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [16:33<03:06, 93.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 12/13 [18:15<01:36, 96.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:47<00:00, 94.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [19:47<00:00, 91.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data MSE: 0.010991 (n=1227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CV on full data with pure GP\n",
    "print('\\n=== Full Data CV (Pure GP) ===')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full_gp = []\n",
    "all_actuals_full_gp = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PureGPEnsemble(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full_gp.append(predictions.numpy())\n",
    "    all_actuals_full_gp.append(test_Y.values)\n",
    "\n",
    "preds_full_gp = np.vstack(all_predictions_full_gp)\n",
    "actuals_full_gp = np.vstack(all_actuals_full_gp)\n",
    "mse_full_gp = np.mean((preds_full_gp - actuals_full_gp) ** 2)\n",
    "print(f'Full Data MSE: {mse_full_gp:.6f} (n={len(preds_full_gp)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b8fb6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T03:11:39.630891Z",
     "iopub.status.busy": "2026-01-16T03:11:39.630303Z",
     "iopub.status.idle": "2026-01-16T03:11:39.696930Z",
     "shell.execute_reply": "2026-01-16T03:11:39.639964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV SCORE SUMMARY (Pure GP) ===\n",
      "Single Solvent MSE: 0.011722 (n=656)\n",
      "Full Data MSE: 0.010991 (n=1227)\n",
      "Overall MSE: 0.011246\n",
      "\n",
      "Best CV (exp_032): 0.008194\n",
      "\n",
      "✗ WORSE: 37.24% worse than best CV\n",
      "\n",
      "Predicted LB (using LB = 4.23*CV + 0.0533): 0.1009\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall MSE for pure GP\n",
    "n_single_gp = len(preds_single_gp)\n",
    "n_full_gp = len(preds_full_gp)\n",
    "overall_mse_gp = (mse_single_gp * n_single_gp + mse_full_gp * n_full_gp) / (n_single_gp + n_full_gp)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (Pure GP) ===')\n",
    "print(f'Single Solvent MSE: {mse_single_gp:.6f} (n={n_single_gp})')\n",
    "print(f'Full Data MSE: {mse_full_gp:.6f} (n={n_full_gp})')\n",
    "print(f'Overall MSE: {overall_mse_gp:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse_gp < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse_gp) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse_gp - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')\n",
    "\n",
    "# Predict LB using CV-LB relationship\n",
    "predicted_lb_gp = 4.23 * overall_mse_gp + 0.0533\n",
    "print(f'\\nPredicted LB (using LB = 4.23*CV + 0.0533): {predicted_lb_gp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07524fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T03:12:07.015256Z",
     "iopub.status.busy": "2026-01-16T03:12:07.014774Z",
     "iopub.status.idle": "2026-01-16T03:12:07.018646Z",
     "shell.execute_reply": "2026-01-16T03:12:07.018268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXPERIMENT 065 SUMMARY ===\n",
      "\\nApproach 1: Uncertainty-Weighted (blend=0.3)\n",
      "  Overall MSE: 0.012241 (49.39% worse than baseline)\n",
      "\\nApproach 2: Uncertainty-Weighted (blend=0.05)\n",
      "  Overall MSE: 0.010246 (25.04% worse than baseline)\n",
      "\\nApproach 3: NN Blending (temp=1.0)\n",
      "  Overall MSE: 0.027371 (234.03% worse than baseline)\n",
      "\\nApproach 4: Pure GP\n",
      "  Overall MSE: 0.011246 (37.24% worse than baseline)\n",
      "\\n=== CONCLUSION ===\n",
      "All approaches tried in this experiment performed WORSE than baseline.\n",
      "The baseline GP+MLP+LGBM ensemble (CV=0.008194) remains the best.\n",
      "\\nKey insights:\n",
      "1. Conservative predictions (blending toward mean) hurt CV performance\n",
      "2. NN blending hurts performance significantly\n",
      "3. Pure GP is worse than the ensemble\n",
      "4. The ensemble diversity is valuable - removing components hurts performance\n",
      "\\nBest CV from this experiment: 0.010246\n"
     ]
    }
   ],
   "source": [
    "# Summary of all approaches tried in this experiment\n",
    "print('\\\\n=== EXPERIMENT 065 SUMMARY ===')\n",
    "print('\\\\nApproach 1: Uncertainty-Weighted (blend=0.3)')\n",
    "print(f'  Overall MSE: 0.012241 (49.39% worse than baseline)')\n",
    "\n",
    "print('\\\\nApproach 2: Uncertainty-Weighted (blend=0.05)')\n",
    "print(f'  Overall MSE: 0.010246 (25.04% worse than baseline)')\n",
    "\n",
    "print('\\\\nApproach 3: NN Blending (temp=1.0)')\n",
    "print(f'  Overall MSE: {overall_mse_nn:.6f} (234.03% worse than baseline)')\n",
    "\n",
    "print('\\\\nApproach 4: Pure GP')\n",
    "print(f'  Overall MSE: {overall_mse_gp:.6f} (37.24% worse than baseline)')\n",
    "\n",
    "print('\\\\n=== CONCLUSION ===')\n",
    "print('All approaches tried in this experiment performed WORSE than baseline.')\n",
    "print('The baseline GP+MLP+LGBM ensemble (CV=0.008194) remains the best.')\n",
    "print('\\\\nKey insights:')\n",
    "print('1. Conservative predictions (blending toward mean) hurt CV performance')\n",
    "print('2. NN blending hurts performance significantly')\n",
    "print('3. Pure GP is worse than the ensemble')\n",
    "print('4. The ensemble diversity is valuable - removing components hurts performance')\n",
    "\n",
    "# Best score from this experiment\n",
    "best_cv_this_exp = min(0.012241, 0.010246, overall_mse_nn, overall_mse_gp)\n",
    "print(f'\\\\nBest CV from this experiment: {best_cv_this_exp:.6f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
