{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7943b549",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization of Best Model\n",
    "\n",
    "**Hypothesis**: The best model (GP+MLP+LGBM) has fixed hyperparameters. Systematic optimization could improve CV significantly.\n",
    "\n",
    "**Target**: Reduce CV from 0.008194 to ~0.006 (20-30% improvement)\n",
    "\n",
    "**Parameters to optimize**:\n",
    "1. MLP: learning rate, dropout, hidden dims, epochs\n",
    "2. LGBM: n_estimators, learning_rate, max_depth, num_leaves\n",
    "3. GP: kernel parameters, alpha\n",
    "4. Ensemble: weights for GP, MLP, LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27fae82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:42.872467Z",
     "iopub.status.busy": "2026-01-15T10:53:42.871958Z",
     "iopub.status.idle": "2026-01-15T10:53:44.641132Z",
     "shell.execute_reply": "2026-01-15T10:53:44.640649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel, RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cdf8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.642651Z",
     "iopub.status.busy": "2026-01-15T10:53:44.642445Z",
     "iopub.status.idle": "2026-01-15T10:53:44.647070Z",
     "shell.execute_reply": "2026-01-15T10:53:44.646705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da29408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.648013Z",
     "iopub.status.busy": "2026-01-15T10:53:44.647891Z",
     "iopub.status.idle": "2026-01-15T10:53:44.681905Z",
     "shell.execute_reply": "2026-01-15T10:53:44.681535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f516e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.683095Z",
     "iopub.status.busy": "2026-01-15T10:53:44.682990Z",
     "iopub.status.idle": "2026-01-15T10:53:44.691407Z",
     "shell.execute_reply": "2026-01-15T10:53:44.691009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n",
      "Simple feature dimension: 18\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "\n",
    "# Simple Featurizer (for GP) - 18 features\n",
    "class SimpleFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange])\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')\n",
    "print(f'Simple feature dimension: {SimpleFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88aeae95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.692504Z",
     "iopub.status.busy": "2026-01-15T10:53:44.692394Z",
     "iopub.status.idle": "2026-01-15T10:53:44.696042Z",
     "shell.execute_reply": "2026-01-15T10:53:44.695664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model with configurable hyperparameters\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim=145, hidden_dim=64, output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "print('MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1392788b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.697179Z",
     "iopub.status.busy": "2026-01-15T10:53:44.697072Z",
     "iopub.status.idle": "2026-01-15T10:53:44.708217Z",
     "shell.execute_reply": "2026-01-15T10:53:44.707823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Ensemble defined\n"
     ]
    }
   ],
   "source": [
    "# Optimized GP + MLP + LGBM Ensemble with configurable hyperparameters\n",
    "class OptimizedEnsemble:\n",
    "    def __init__(self, data='single', \n",
    "                 gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.30,\n",
    "                 mlp_hidden=64, mlp_dropout=0.2, mlp_lr=0.001, mlp_epochs=200,\n",
    "                 lgbm_n_estimators=100, lgbm_lr=0.05, lgbm_max_depth=5,\n",
    "                 gp_alpha=0.01, gp_length_scale=1.0):\n",
    "        self.data = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        # MLP hyperparameters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.mlp_lr = mlp_lr\n",
    "        self.mlp_epochs = mlp_epochs\n",
    "        \n",
    "        # LGBM hyperparameters\n",
    "        self.lgbm_n_estimators = lgbm_n_estimators\n",
    "        self.lgbm_lr = lgbm_lr\n",
    "        self.lgbm_max_depth = lgbm_max_depth\n",
    "        \n",
    "        # GP hyperparameters\n",
    "        self.gp_alpha = gp_alpha\n",
    "        self.gp_length_scale = gp_length_scale\n",
    "        \n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        \n",
    "        self.scaler_full = StandardScaler()\n",
    "        self.scaler_simple = StandardScaler()\n",
    "        \n",
    "        self.gp_models = [None, None, None]\n",
    "        self.mlp_model = None\n",
    "        self.lgbm_models = [None, None, None]\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.fit_transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.fit_transform(X_simple)\n",
    "        \n",
    "        # Train GP for each target\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=self.gp_length_scale, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            self.gp_models[i] = GaussianProcessRegressor(\n",
    "                kernel=kernel, alpha=self.gp_alpha, n_restarts_optimizer=3, random_state=42\n",
    "            )\n",
    "            self.gp_models[i].fit(X_simple_scaled, Y_np[:, i])\n",
    "        \n",
    "        # Train MLP\n",
    "        self.mlp_model = MLPModel(\n",
    "            input_dim=self.full_featurizer.feats_dim,\n",
    "            hidden_dim=self.mlp_hidden,\n",
    "            output_dim=3,\n",
    "            dropout=self.mlp_dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        Y_tensor = torch.tensor(Y_np, dtype=torch.double).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp_model.parameters(), lr=self.mlp_lr, weight_decay=1e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp_model.train()\n",
    "        for epoch in range(self.mlp_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.mlp_model(X_tensor)\n",
    "            loss = criterion(pred, Y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train LGBM for each target\n",
    "        for i in range(3):\n",
    "            self.lgbm_models[i] = lgb.LGBMRegressor(\n",
    "                n_estimators=self.lgbm_n_estimators, \n",
    "                learning_rate=self.lgbm_lr, \n",
    "                max_depth=self.lgbm_max_depth,\n",
    "                num_leaves=31, min_child_samples=5, random_state=42, verbose=-1\n",
    "            )\n",
    "            self.lgbm_models[i].fit(X_full_scaled, Y_np[:, i])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        \n",
    "        X_full_scaled = self.scaler_full.transform(X_full)\n",
    "        X_simple_scaled = self.scaler_simple.transform(X_simple)\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_preds = np.column_stack([self.gp_models[i].predict(X_simple_scaled) for i in range(3)])\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp_model.eval()\n",
    "        X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "        with torch.no_grad():\n",
    "            mlp_preds = self.mlp_model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgbm_preds = np.column_stack([self.lgbm_models[i].predict(X_full_scaled) for i in range(3)])\n",
    "        \n",
    "        # Ensemble\n",
    "        predictions = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            X_full_scaled_flip = self.scaler_full.transform(X_full_flip)\n",
    "            X_simple_scaled_flip = self.scaler_simple.transform(X_simple_flip)\n",
    "            \n",
    "            gp_preds_flip = np.column_stack([self.gp_models[i].predict(X_simple_scaled_flip) for i in range(3)])\n",
    "            X_tensor_flip = torch.tensor(X_full_scaled_flip, dtype=torch.double).to(device)\n",
    "            with torch.no_grad():\n",
    "                mlp_preds_flip = self.mlp_model(X_tensor_flip).cpu().numpy()\n",
    "            lgbm_preds_flip = np.column_stack([self.lgbm_models[i].predict(X_full_scaled_flip) for i in range(3)])\n",
    "            \n",
    "            predictions_flip = self.gp_weight * gp_preds_flip + self.mlp_weight * mlp_preds_flip + self.lgbm_weight * lgbm_preds_flip\n",
    "            predictions = (predictions + predictions_flip) / 2\n",
    "        \n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('Optimized Ensemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b7b1ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:44.709172Z",
     "iopub.status.busy": "2026-01-15T10:53:44.709066Z",
     "iopub.status.idle": "2026-01-15T10:53:44.713710Z",
     "shell.execute_reply": "2026-01-15T10:53:44.713349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Fast CV evaluation function (only on single solvent data for speed)\n",
    "def evaluate_hyperparams(params, n_folds=5):\n",
    "    \"\"\"Evaluate hyperparameters using a subset of CV folds for speed.\"\"\"\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    \n",
    "    # Use only first n_folds for speed\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        if fold_idx >= n_folds:\n",
    "            break\n",
    "            \n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = OptimizedEnsemble(\n",
    "            data='single',\n",
    "            gp_weight=params['gp_weight'],\n",
    "            mlp_weight=params['mlp_weight'],\n",
    "            lgbm_weight=params['lgbm_weight'],\n",
    "            mlp_hidden=params['mlp_hidden'],\n",
    "            mlp_dropout=params['mlp_dropout'],\n",
    "            mlp_lr=params['mlp_lr'],\n",
    "            mlp_epochs=params['mlp_epochs'],\n",
    "            lgbm_n_estimators=params['lgbm_n_estimators'],\n",
    "            lgbm_lr=params['lgbm_lr'],\n",
    "            lgbm_max_depth=params['lgbm_max_depth'],\n",
    "            gp_alpha=params['gp_alpha'],\n",
    "            gp_length_scale=params['gp_length_scale']\n",
    "        )\n",
    "        model.train_model(train_X, train_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        \n",
    "        all_predictions.append(predictions.numpy())\n",
    "        all_actuals.append(test_Y.values)\n",
    "    \n",
    "    preds = np.vstack(all_predictions)\n",
    "    actuals = np.vstack(all_actuals)\n",
    "    mse = np.mean((preds - actuals) ** 2)\n",
    "    return mse\n",
    "\n",
    "print('Evaluation function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7d2910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:53:59.855922Z",
     "iopub.status.busy": "2026-01-15T10:53:59.855412Z",
     "iopub.status.idle": "2026-01-15T10:53:59.859863Z",
     "shell.execute_reply": "2026-01-15T10:53:59.859459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna objective function defined\n"
     ]
    }
   ],
   "source": [
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    params = {\n",
    "        # Ensemble weights (must sum to 1)\n",
    "        'gp_weight': trial.suggest_float('gp_weight', 0.05, 0.30),\n",
    "        'mlp_weight': trial.suggest_float('mlp_weight', 0.40, 0.70),\n",
    "        'lgbm_weight': None,  # Will be computed\n",
    "        \n",
    "        # MLP hyperparameters\n",
    "        'mlp_hidden': trial.suggest_categorical('mlp_hidden', [32, 64, 128]),\n",
    "        'mlp_dropout': trial.suggest_float('mlp_dropout', 0.1, 0.4),\n",
    "        'mlp_lr': trial.suggest_float('mlp_lr', 0.0005, 0.005, log=True),\n",
    "        'mlp_epochs': trial.suggest_int('mlp_epochs', 150, 300),\n",
    "        \n",
    "        # LGBM hyperparameters\n",
    "        'lgbm_n_estimators': trial.suggest_int('lgbm_n_estimators', 50, 200),\n",
    "        'lgbm_lr': trial.suggest_float('lgbm_lr', 0.01, 0.1, log=True),\n",
    "        'lgbm_max_depth': trial.suggest_int('lgbm_max_depth', 3, 7),\n",
    "        \n",
    "        # GP hyperparameters\n",
    "        'gp_alpha': trial.suggest_float('gp_alpha', 0.001, 0.1, log=True),\n",
    "        'gp_length_scale': trial.suggest_float('gp_length_scale', 0.5, 2.0),\n",
    "    }\n",
    "    \n",
    "    # Compute lgbm_weight to ensure weights sum to 1\n",
    "    params['lgbm_weight'] = 1.0 - params['gp_weight'] - params['mlp_weight']\n",
    "    if params['lgbm_weight'] < 0.05:\n",
    "        return float('inf')  # Invalid configuration\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = evaluate_hyperparams(params, n_folds=5)\n",
    "    return mse\n",
    "\n",
    "print('Optuna objective function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb05246d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:54:10.772936Z",
     "iopub.status.busy": "2026-01-15T10:54:10.772451Z",
     "iopub.status.idle": "2026-01-15T11:31:42.719373Z",
     "shell.execute_reply": "2026-01-15T11:31:42.719028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 10:54:10,774] A new study created in memory with name: no-name-4cc94585-c69c-442c-9722-45d7935379cc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n",
      "Using 5 folds for speed (will validate best params on full CV later)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34bf52007ab4fd6b3a478d2404cf9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 10:56:01,605] Trial 0 finished with value: 0.012707327520873038 and parameters: {'gp_weight': 0.1436350297118406, 'mlp_weight': 0.6852142919229748, 'mlp_hidden': 32, 'mlp_dropout': 0.1467983561008608, 'mlp_lr': 0.000571549193815661, 'mlp_epochs': 280, 'lgbm_n_estimators': 140, 'lgbm_lr': 0.051059032093947576, 'lgbm_max_depth': 3, 'gp_alpha': 0.08706020878304858, 'gp_length_scale': 1.7486639612006325}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 10:58:04,035] Trial 1 finished with value: 0.016655073376011516 and parameters: {'gp_weight': 0.10308477766956904, 'mlp_weight': 0.4545474901621302, 'mlp_hidden': 128, 'mlp_dropout': 0.22958350559263474, 'mlp_lr': 0.0009776854331372624, 'mlp_epochs': 242, 'lgbm_n_estimators': 71, 'lgbm_lr': 0.019594972058679168, 'lgbm_max_depth': 4, 'gp_alpha': 0.008168455894760165, 'gp_length_scale': 1.6777639420895203}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:00:15,938] Trial 2 finished with value: 0.018160067301852215 and parameters: {'gp_weight': 0.09991844553958994, 'mlp_weight': 0.5542703315240834, 'mlp_hidden': 128, 'mlp_dropout': 0.15115723710618748, 'mlp_lr': 0.0005807932994623225, 'mlp_epochs': 293, 'lgbm_n_estimators': 195, 'lgbm_lr': 0.06432759992849894, 'lgbm_max_depth': 4, 'gp_alpha': 0.0015679933916723015, 'gp_length_scale': 1.5263495397682354}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:01:47,820] Trial 3 finished with value: 0.015239343661077551 and parameters: {'gp_weight': 0.16003812343490031, 'mlp_weight': 0.4366114704534337, 'mlp_hidden': 128, 'mlp_dropout': 0.1776339944800051, 'mlp_lr': 0.002298752892366083, 'mlp_epochs': 197, 'lgbm_n_estimators': 128, 'lgbm_lr': 0.03521358805467869, 'lgbm_max_depth': 3, 'gp_alpha': 0.08692991511139551, 'gp_length_scale': 1.6626992350416718}. Best is trial 0 with value: 0.012707327520873038.\n",
      "[I 2026-01-15 11:01:47,824] Trial 4 finished with value: inf and parameters: {'gp_weight': 0.2848747353910473, 'mlp_weight': 0.6684482051282946, 'mlp_hidden': 64, 'mlp_dropout': 0.15879485872574356, 'mlp_lr': 0.0005548777280551552, 'mlp_epochs': 199, 'lgbm_n_estimators': 108, 'lgbm_lr': 0.01867880257107068, 'lgbm_max_depth': 7, 'gp_alpha': 0.005170191786366992, 'gp_length_scale': 0.9214017645310711}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:03:35,130] Trial 5 finished with value: 0.014768118153472929 and parameters: {'gp_weight': 0.18567402078956213, 'mlp_weight': 0.4422772674924288, 'mlp_hidden': 128, 'mlp_dropout': 0.33167343078899725, 'mlp_lr': 0.0007901065932051942, 'mlp_epochs': 150, 'lgbm_n_estimators': 173, 'lgbm_lr': 0.05091635945818555, 'lgbm_max_depth': 6, 'gp_alpha': 0.034877126245459314, 'gp_length_scale': 0.6110669776011355}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:05:48,132] Trial 6 finished with value: 0.013687278951242809 and parameters: {'gp_weight': 0.13961643213606817, 'mlp_weight': 0.43476071785753895, 'mlp_hidden': 32, 'mlp_dropout': 0.1190675050858071, 'mlp_lr': 0.0010231806681740801, 'mlp_epochs': 199, 'lgbm_n_estimators': 160, 'lgbm_lr': 0.043406770118894, 'lgbm_max_depth': 7, 'gp_alpha': 0.008798929749689027, 'gp_length_scale': 0.6793913689074526}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:07:47,235] Trial 7 finished with value: 0.017599744785782852 and parameters: {'gp_weight': 0.22831119680574874, 'mlp_weight': 0.6282355145850692, 'mlp_hidden': 64, 'mlp_dropout': 0.25681984881459824, 'mlp_lr': 0.0013381691783830377, 'mlp_epochs': 153, 'lgbm_n_estimators': 66, 'lgbm_lr': 0.010750512925563078, 'lgbm_max_depth': 6, 'gp_alpha': 0.00425316236379087, 'gp_length_scale': 1.262856036747054}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:09:36,235] Trial 8 finished with value: 0.015838957566300157 and parameters: {'gp_weight': 0.27689161848152327, 'mlp_weight': 0.4747876687446625, 'mlp_hidden': 64, 'mlp_dropout': 0.12309397294863791, 'mlp_lr': 0.0009743645106784236, 'mlp_epochs': 174, 'lgbm_n_estimators': 190, 'lgbm_lr': 0.06428658848831817, 'lgbm_max_depth': 6, 'gp_alpha': 0.05532496914298508, 'gp_length_scale': 1.7055081153486717}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:11:52,839] Trial 9 finished with value: 0.01675695328501775 and parameters: {'gp_weight': 0.09664251472150896, 'mlp_weight': 0.6677676995469932, 'mlp_hidden': 128, 'mlp_dropout': 0.19540104249155918, 'mlp_lr': 0.0006442017924231741, 'mlp_epochs': 184, 'lgbm_n_estimators': 114, 'lgbm_lr': 0.06576801979658928, 'lgbm_max_depth': 7, 'gp_alpha': 0.0010325337616482041, 'gp_length_scale': 1.2661209538663485}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:13:48,318] Trial 10 finished with value: 0.014597557063036323 and parameters: {'gp_weight': 0.06057250231679878, 'mlp_weight': 0.5676552694772402, 'mlp_hidden': 32, 'mlp_dropout': 0.3796871025735363, 'mlp_lr': 0.0038426102582429945, 'mlp_epochs': 287, 'lgbm_n_estimators': 141, 'lgbm_lr': 0.09203870791207931, 'lgbm_max_depth': 3, 'gp_alpha': 0.02533691514684049, 'gp_length_scale': 1.987660095293712}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:15:53,802] Trial 11 finished with value: 0.013330786323687074 and parameters: {'gp_weight': 0.15837618880879983, 'mlp_weight': 0.5166992822502666, 'mlp_hidden': 32, 'mlp_dropout': 0.10598545731786826, 'mlp_lr': 0.0015719693957202945, 'mlp_epochs': 241, 'lgbm_n_estimators': 155, 'lgbm_lr': 0.03382472854820698, 'lgbm_max_depth': 5, 'gp_alpha': 0.01765603430879867, 'gp_length_scale': 0.5194940257656061}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:17:48,825] Trial 12 finished with value: 0.016004449686705603 and parameters: {'gp_weight': 0.20226846585843616, 'mlp_weight': 0.5060188414980882, 'mlp_hidden': 32, 'mlp_dropout': 0.11007576548034162, 'mlp_lr': 0.002334167208904365, 'mlp_epochs': 257, 'lgbm_n_estimators': 148, 'lgbm_lr': 0.02577598268486535, 'lgbm_max_depth': 4, 'gp_alpha': 0.02070699744102167, 'gp_length_scale': 0.9246073616106361}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:19:58,038] Trial 13 finished with value: 0.015549606059562945 and parameters: {'gp_weight': 0.13829138218955295, 'mlp_weight': 0.6064688535850599, 'mlp_hidden': 32, 'mlp_dropout': 0.272822496431662, 'mlp_lr': 0.0046164480255257365, 'mlp_epochs': 263, 'lgbm_n_estimators': 104, 'lgbm_lr': 0.030051039478538218, 'lgbm_max_depth': 5, 'gp_alpha': 0.016604496710763958, 'gp_length_scale': 1.9659427975068455}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:21:30,722] Trial 14 finished with value: 0.01501186974641698 and parameters: {'gp_weight': 0.23101569651236467, 'mlp_weight': 0.5154847305503363, 'mlp_hidden': 32, 'mlp_dropout': 0.10076831144073116, 'mlp_lr': 0.001945323335177902, 'mlp_epochs': 232, 'lgbm_n_estimators': 170, 'lgbm_lr': 0.03971108897784325, 'lgbm_max_depth': 5, 'gp_alpha': 0.09006020499939704, 'gp_length_scale': 0.9798088603953311}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:23:15,529] Trial 15 finished with value: 0.015084681686563198 and parameters: {'gp_weight': 0.1542536837307999, 'mlp_weight': 0.40079005467029827, 'mlp_hidden': 32, 'mlp_dropout': 0.2173586379013956, 'mlp_lr': 0.0031850761055011697, 'mlp_epochs': 273, 'lgbm_n_estimators': 92, 'lgbm_lr': 0.02282392254267503, 'lgbm_max_depth': 3, 'gp_alpha': 0.0551132475128409, 'gp_length_scale': 1.438017956788638}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:25:36,435] Trial 16 finished with value: 0.015597172347583716 and parameters: {'gp_weight': 0.20672022953861277, 'mlp_weight': 0.5910628193857126, 'mlp_hidden': 32, 'mlp_dropout': 0.29611288475324316, 'mlp_lr': 0.0018735249103612151, 'mlp_epochs': 247, 'lgbm_n_estimators': 136, 'lgbm_lr': 0.013943122920045648, 'lgbm_max_depth': 5, 'gp_alpha': 0.002566444181212709, 'gp_length_scale': 0.5171647819942123}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:28:02,330] Trial 17 finished with value: 0.015791076130767624 and parameters: {'gp_weight': 0.06975273113465091, 'mlp_weight': 0.6965189504750028, 'mlp_hidden': 32, 'mlp_dropout': 0.14931902128299568, 'mlp_lr': 0.0013454350674068922, 'mlp_epochs': 214, 'lgbm_n_estimators': 160, 'lgbm_lr': 0.09801153617943263, 'lgbm_max_depth': 4, 'gp_alpha': 0.014476141427033188, 'gp_length_scale': 1.0872528356115083}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:29:56,919] Trial 18 finished with value: 0.012749729976071978 and parameters: {'gp_weight': 0.12712042801934542, 'mlp_weight': 0.5198395796937997, 'mlp_hidden': 32, 'mlp_dropout': 0.20939732083913032, 'mlp_lr': 0.0030915840031187553, 'mlp_epochs': 279, 'lgbm_n_estimators': 177, 'lgbm_lr': 0.05151747744709276, 'lgbm_max_depth': 3, 'gp_alpha': 0.036601666089861117, 'gp_length_scale': 0.7106769058866473}. Best is trial 0 with value: 0.012707327520873038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 11:31:42,711] Trial 19 finished with value: 0.015128375868137717 and parameters: {'gp_weight': 0.12390589527146749, 'mlp_weight': 0.6356815165329587, 'mlp_hidden': 32, 'mlp_dropout': 0.20417858180646106, 'mlp_lr': 0.0027084980921912817, 'mlp_epochs': 278, 'lgbm_n_estimators': 181, 'lgbm_lr': 0.05081579618860371, 'lgbm_max_depth': 3, 'gp_alpha': 0.04149498598423083, 'gp_length_scale': 0.7644677511433781}. Best is trial 0 with value: 0.012707327520873038.\n",
      "\n",
      "Best trial:\n",
      "  Value (MSE): 0.012707\n",
      "  Params: {'gp_weight': 0.1436350297118406, 'mlp_weight': 0.6852142919229748, 'mlp_hidden': 32, 'mlp_dropout': 0.1467983561008608, 'mlp_lr': 0.000571549193815661, 'mlp_epochs': 280, 'lgbm_n_estimators': 140, 'lgbm_lr': 0.051059032093947576, 'lgbm_max_depth': 3, 'gp_alpha': 0.08706020878304858, 'gp_length_scale': 1.7486639612006325}\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter optimization\n",
    "print('Starting hyperparameter optimization...')\n",
    "print('Using 5 folds for speed (will validate best params on full CV later)')\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest trial:')\n",
    "print(f'  Value (MSE): {study.best_trial.value:.6f}')\n",
    "print(f'  Params: {study.best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_params['lgbm_weight'] = 1.0 - best_params['gp_weight'] - best_params['mlp_weight']\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "for k, v in best_params.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62518d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full CV evaluation with best hyperparameters\n",
    "print('\\n=== Full CV Evaluation with Best Hyperparameters ===')\n",
    "\n",
    "# Single solvent CV\n",
    "print('\\n--- Single Solvent CV ---')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single = []\n",
    "all_actuals_single = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = OptimizedEnsemble(\n",
    "        data='single',\n",
    "        gp_weight=best_params['gp_weight'],\n",
    "        mlp_weight=best_params['mlp_weight'],\n",
    "        lgbm_weight=best_params['lgbm_weight'],\n",
    "        mlp_hidden=best_params['mlp_hidden'],\n",
    "        mlp_dropout=best_params['mlp_dropout'],\n",
    "        mlp_lr=best_params['mlp_lr'],\n",
    "        mlp_epochs=best_params['mlp_epochs'],\n",
    "        lgbm_n_estimators=best_params['lgbm_n_estimators'],\n",
    "        lgbm_lr=best_params['lgbm_lr'],\n",
    "        lgbm_max_depth=best_params['lgbm_max_depth'],\n",
    "        gp_alpha=best_params['gp_alpha'],\n",
    "        gp_length_scale=best_params['gp_length_scale']\n",
    "    )\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single.append(predictions.numpy())\n",
    "    all_actuals_single.append(test_Y.values)\n",
    "\n",
    "preds_single = np.vstack(all_predictions_single)\n",
    "actuals_single = np.vstack(all_actuals_single)\n",
    "mse_single = np.mean((preds_single - actuals_single) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={len(preds_single)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data CV\n",
    "print('\\n--- Full Data CV ---')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = OptimizedEnsemble(\n",
    "        data='full',\n",
    "        gp_weight=best_params['gp_weight'],\n",
    "        mlp_weight=best_params['mlp_weight'],\n",
    "        lgbm_weight=best_params['lgbm_weight'],\n",
    "        mlp_hidden=best_params['mlp_hidden'],\n",
    "        mlp_dropout=best_params['mlp_dropout'],\n",
    "        mlp_lr=best_params['mlp_lr'],\n",
    "        mlp_epochs=best_params['mlp_epochs'],\n",
    "        lgbm_n_estimators=best_params['lgbm_n_estimators'],\n",
    "        lgbm_lr=best_params['lgbm_lr'],\n",
    "        lgbm_max_depth=best_params['lgbm_max_depth'],\n",
    "        gp_alpha=best_params['gp_alpha'],\n",
    "        gp_length_scale=best_params['gp_length_scale']\n",
    "    )\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full.append(predictions.numpy())\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "preds_full = np.vstack(all_predictions_full)\n",
    "actuals_full = np.vstack(all_actuals_full)\n",
    "mse_full = np.mean((preds_full - actuals_full) ** 2)\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={len(preds_full)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(preds_single)\n",
    "n_full = len(preds_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (Optimized Hyperparameters) ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model class with best hyperparameters\n",
    "class BestOptimizedEnsemble:\n",
    "    def __init__(self, data='single'):\n",
    "        self.model = OptimizedEnsemble(\n",
    "            data=data,\n",
    "            gp_weight=best_params['gp_weight'],\n",
    "            mlp_weight=best_params['mlp_weight'],\n",
    "            lgbm_weight=best_params['lgbm_weight'],\n",
    "            mlp_hidden=best_params['mlp_hidden'],\n",
    "            mlp_dropout=best_params['mlp_dropout'],\n",
    "            mlp_lr=best_params['mlp_lr'],\n",
    "            mlp_epochs=best_params['mlp_epochs'],\n",
    "            lgbm_n_estimators=best_params['lgbm_n_estimators'],\n",
    "            lgbm_lr=best_params['lgbm_lr'],\n",
    "            lgbm_max_depth=best_params['lgbm_max_depth'],\n",
    "            gp_alpha=best_params['gp_alpha'],\n",
    "            gp_length_scale=best_params['gp_length_scale']\n",
    "        )\n",
    "    \n",
    "    def train_model(self, X, Y):\n",
    "        self.model.train_model(X, Y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "print('BestOptimizedEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = BestOptimizedEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = BestOptimizedEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13dc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad769b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(f'\\n=== FINAL CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "print(f'\\nBest hyperparameters:')\n",
    "for k, v in best_params.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
