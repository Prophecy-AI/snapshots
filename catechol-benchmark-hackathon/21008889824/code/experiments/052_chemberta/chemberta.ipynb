{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8a52cf",
   "metadata": {},
   "source": [
    "# ChemBERTa Pre-trained Molecular Embeddings\n",
    "\n",
    "**Hypothesis**: Pre-trained molecular embeddings may generalize better to unseen solvents and potentially break the CV-LB relationship.\n",
    "\n",
    "**Rationale**:\n",
    "- The GNN benchmark achieved CV 0.0039 (5x better than our best CV 0.008194)\n",
    "- The key difference is likely pre-training on large molecular datasets\n",
    "- ChemBERTa was trained on 77M molecules from ZINC database\n",
    "- Pre-trained embeddings capture chemical knowledge that may transfer to unseen solvents\n",
    "\n",
    "**Implementation**:\n",
    "1. Use ChemBERTa to generate 768-dim embeddings for each solvent SMILES\n",
    "2. Replace Spange/DRFP/ACS features with ChemBERTa embeddings\n",
    "3. Add Arrhenius kinetics features (1/T, ln(t), interaction)\n",
    "4. Train MLP on ChemBERTa embeddings + kinetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ChemBERTa\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = 'seyonec/ChemBERTa-zinc-base-v1'\n",
    "print(f'Loading {model_name}...')\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "chemberta_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "chemberta_model.eval()\n",
    "print('ChemBERTa loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f61520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SMILES lookup and generate ChemBERTa embeddings\n",
    "SMILES_DF = pd.read_csv(f'{DATA_PATH}/smiles_lookup.csv', index_col=0)\n",
    "print(f'SMILES lookup: {SMILES_DF.shape}')\n",
    "\n",
    "# Generate embeddings for all solvents\n",
    "CHEMBERTA_EMBEDDINGS = {}\n",
    "\n",
    "for solvent in SMILES_DF.index:\n",
    "    smiles = SMILES_DF.loc[solvent, 'solvent smiles']\n",
    "    \n",
    "    # Handle mixture solvents (e.g., \"Water.Acetonitrile\")\n",
    "    if '.' in smiles:\n",
    "        # For mixtures, average the embeddings of components\n",
    "        parts = smiles.split('.')\n",
    "        embeddings = []\n",
    "        for part in parts:\n",
    "            inputs = chemberta_tokenizer(part, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = chemberta_model(**inputs)\n",
    "                emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n",
    "            embeddings.append(emb)\n",
    "        # Average the embeddings\n",
    "        CHEMBERTA_EMBEDDINGS[solvent] = np.mean(embeddings, axis=0).squeeze()\n",
    "    else:\n",
    "        inputs = chemberta_tokenizer(smiles, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = chemberta_model(**inputs)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n",
    "        CHEMBERTA_EMBEDDINGS[solvent] = emb.squeeze()\n",
    "\n",
    "print(f'Generated ChemBERTa embeddings for {len(CHEMBERTA_EMBEDDINGS)} solvents')\n",
    "print(f'Embedding dimension: {CHEMBERTA_EMBEDDINGS[\"Methanol\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for ChemBERTa embeddings\n",
    "CHEMBERTA_DF = pd.DataFrame(CHEMBERTA_EMBEDDINGS).T\n",
    "CHEMBERTA_DF.index.name = 'SOLVENT NAME'\n",
    "print(f'ChemBERTa DataFrame: {CHEMBERTA_DF.shape}')\n",
    "print(CHEMBERTA_DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemBERTa Featurizer\n",
    "class ChemBERTaFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.chemberta_df = CHEMBERTA_DF\n",
    "        # 5 kinetics features + 768 ChemBERTa features = 773 total\n",
    "        self.feats_dim = 5 + self.chemberta_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        # Kinetics features\n",
    "        temp_c = X[\"Temperature\"].values.reshape(-1, 1)\n",
    "        time_m = X[\"Residence Time\"].values.reshape(-1, 1)\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([temp_c, time_m, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_emb = self.chemberta_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_emb = self.chemberta_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_emb = B_emb * (1 - (1-pct)) + A_emb * (1-pct)\n",
    "            else:\n",
    "                X_emb = A_emb * (1 - pct) + B_emb * pct\n",
    "        else:\n",
    "            X_emb = self.chemberta_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_emb])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip), dtype=torch.float32)\n",
    "\n",
    "print(f'ChemBERTa feature dimension: {ChemBERTaFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model for ChemBERTa embeddings\n",
    "class ChemBERTaMLP(nn.Module):\n",
    "    def __init__(self, input_dim=773, hidden_dim=256, output_dim=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "print('ChemBERTa MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeacd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemBERTa Model Wrapper\n",
    "class ChemBERTaModel:\n",
    "    def __init__(self, data='single', epochs=300, lr=0.001, hidden_dim=256, dropout=0.3):\n",
    "        self.data = data\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.featurizer = ChemBERTaFeaturizer(mixed=(data == 'full'))\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        # Get features\n",
    "        features = self.featurizer.featurize(X)\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)\n",
    "        Y_tensor = torch.tensor(Y.values, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = ChemBERTaMLP(\n",
    "            input_dim=self.featurizer.feats_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=3,\n",
    "            dropout=self.dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=30, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = self.model(X_tensor)\n",
    "            loss = criterion(predictions, Y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 50:\n",
    "                    break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get features\n",
    "        features = self.featurizer.featurize(X)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        X_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        \n",
    "        # TTA for mixed solvents\n",
    "        if self.data == 'full':\n",
    "            features_flip = self.featurizer.featurize(X, flip=True)\n",
    "            features_flip_scaled = self.scaler.transform(features_flip)\n",
    "            X_flip_tensor = torch.tensor(features_flip_scaled, dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions_flip = self.model(X_flip_tensor)\n",
    "            predictions = (predictions + predictions_flip) / 2\n",
    "        \n",
    "        predictions = torch.clamp(predictions, 0, 1)\n",
    "        return predictions\n",
    "\n",
    "print('ChemBERTa model wrapper defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: X={X_single.shape}, Y={Y_single.shape}')\n",
    "\n",
    "# Test on a small subset\n",
    "X_test = X_single.iloc[:50]\n",
    "Y_test = Y_single.iloc[:50]\n",
    "\n",
    "model = ChemBERTaModel(data='single', epochs=50)\n",
    "model.train_model(X_test, Y_test)\n",
    "preds = model.predict(X_test)\n",
    "print(f'Test predictions shape: {preds.shape}')\n",
    "print(f'Test predictions range: [{preds.min():.4f}, {preds.max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c781b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on single solvent data\n",
    "print('\\n=== Single Solvent CV (ChemBERTa) ===')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single = []\n",
    "all_actuals_single = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ChemBERTaModel(data='single', epochs=300, lr=0.001)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single.append(predictions.cpu().numpy())\n",
    "    all_actuals_single.append(test_Y.values)\n",
    "\n",
    "preds_single = np.vstack(all_predictions_single)\n",
    "actuals_single = np.vstack(all_actuals_single)\n",
    "mse_single = np.mean((preds_single - actuals_single) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={len(preds_single)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on full data\n",
    "print('\\n=== Full Data CV (ChemBERTa) ===')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = ChemBERTaModel(data='full', epochs=300, lr=0.001)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full.append(predictions.cpu().numpy())\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "preds_full = np.vstack(all_predictions_full)\n",
    "actuals_full = np.vstack(all_actuals_full)\n",
    "mse_full = np.mean((preds_full - actuals_full) ** 2)\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={len(preds_full)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(preds_single)\n",
    "n_full = len(preds_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (ChemBERTa) ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "print(f'GNN Benchmark: 0.0039')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31291aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ChemBERTaModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ChemBERTaModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246abaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb624572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(f'\\n=== FINAL CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "print(f'GNN Benchmark: 0.0039')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
