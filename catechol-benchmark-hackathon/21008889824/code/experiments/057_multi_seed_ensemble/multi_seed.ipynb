{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d74b2ba",
   "metadata": {},
   "source": [
    "# Multi-Seed Deep Ensemble\n",
    "\n",
    "**Hypothesis**: Variance reduction through averaging many models with different random seeds could improve CV.\n",
    "\n",
    "**Rationale**:\n",
    "- The best CV (0.008194) may have benefited from a lucky seed\n",
    "- Averaging many models reduces variance\n",
    "- This is a proven technique in Kaggle competitions\n",
    "- Low implementation risk, high potential reward\n",
    "\n",
    "**Implementation**:\n",
    "1. Train GP+MLP+LGBM ensemble with 10 different random seeds\n",
    "2. Average predictions across ALL seeds\n",
    "3. Expected improvement: 5-15% CV reduction through variance reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97047098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:40.123226Z",
     "iopub.status.busy": "2026-01-15T13:36:40.122693Z",
     "iopub.status.idle": "2026-01-15T13:36:41.818680Z",
     "shell.execute_reply": "2026-01-15T13:36:41.818251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c433e550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:41.820058Z",
     "iopub.status.busy": "2026-01-15T13:36:41.819871Z",
     "iopub.status.idle": "2026-01-15T13:36:41.824518Z",
     "shell.execute_reply": "2026-01-15T13:36:41.824126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06bed4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:41.825534Z",
     "iopub.status.busy": "2026-01-15T13:36:41.825411Z",
     "iopub.status.idle": "2026-01-15T13:36:41.854522Z",
     "shell.execute_reply": "2026-01-15T13:36:41.854190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e74cda7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:41.855676Z",
     "iopub.status.busy": "2026-01-15T13:36:41.855384Z",
     "iopub.status.idle": "2026-01-15T13:36:41.862779Z",
     "shell.execute_reply": "2026-01-15T13:36:41.862438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n",
      "Simple feature dimension: 18\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "\n",
    "# Simple Featurizer (for GP) - 18 features\n",
    "class SimpleFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange])\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')\n",
    "print(f'Simple feature dimension: {SimpleFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4608fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:41.864263Z",
     "iopub.status.busy": "2026-01-15T13:36:41.863938Z",
     "iopub.status.idle": "2026-01-15T13:36:41.867602Z",
     "shell.execute_reply": "2026-01-15T13:36:41.867227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim=145, hidden_dim=64, output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "print('MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7f9fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:41.868550Z",
     "iopub.status.busy": "2026-01-15T13:36:41.868442Z",
     "iopub.status.idle": "2026-01-15T13:36:41.878113Z",
     "shell.execute_reply": "2026-01-15T13:36:41.877760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Seed Ensemble defined\n"
     ]
    }
   ],
   "source": [
    "# Multi-Seed Ensemble Model\n",
    "class MultiSeedEnsemble:\n",
    "    \"\"\"Ensemble that trains multiple models with different seeds and averages predictions.\"\"\"\n",
    "    def __init__(self, data='single', n_seeds=10, gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.30):\n",
    "        self.data = data\n",
    "        self.n_seeds = n_seeds\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        \n",
    "        # Store models for each seed\n",
    "        self.models = []  # List of (scaler_full, scaler_simple, gp_models, mlp_model, lgbm_models)\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        for seed in range(self.n_seeds):\n",
    "            # Set seeds\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            # Create scalers\n",
    "            scaler_full = StandardScaler()\n",
    "            scaler_simple = StandardScaler()\n",
    "            \n",
    "            X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "            X_simple_scaled = scaler_simple.fit_transform(X_simple)\n",
    "            \n",
    "            # Train GP for each target\n",
    "            gp_models = []\n",
    "            for i in range(3):\n",
    "                kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "                gp = GaussianProcessRegressor(\n",
    "                    kernel=kernel, alpha=0.01, n_restarts_optimizer=3, random_state=seed\n",
    "                )\n",
    "                gp.fit(X_simple_scaled, Y_np[:, i])\n",
    "                gp_models.append(gp)\n",
    "            \n",
    "            # Train MLP\n",
    "            mlp_model = MLPModel(\n",
    "                input_dim=self.full_featurizer.feats_dim,\n",
    "                hidden_dim=64,\n",
    "                output_dim=3,\n",
    "                dropout=0.2\n",
    "            ).to(device)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "            Y_tensor = torch.tensor(Y_np, dtype=torch.double).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            mlp_model.train()\n",
    "            for epoch in range(200):\n",
    "                optimizer.zero_grad()\n",
    "                pred = mlp_model(X_tensor)\n",
    "                loss = criterion(pred, Y_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Train LGBM for each target\n",
    "            lgbm_models = []\n",
    "            for i in range(3):\n",
    "                lgbm = lgb.LGBMRegressor(\n",
    "                    n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                    num_leaves=31, min_child_samples=5, random_state=seed, verbose=-1\n",
    "                )\n",
    "                lgbm.fit(X_full_scaled, Y_np[:, i])\n",
    "                lgbm_models.append(lgbm)\n",
    "            \n",
    "            # Store models\n",
    "            self.models.append((scaler_full, scaler_simple, gp_models, mlp_model, lgbm_models))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        for scaler_full, scaler_simple, gp_models, mlp_model, lgbm_models in self.models:\n",
    "            X_full_scaled = scaler_full.transform(X_full)\n",
    "            X_simple_scaled = scaler_simple.transform(X_simple)\n",
    "            \n",
    "            # GP predictions\n",
    "            gp_preds = np.column_stack([gp_models[i].predict(X_simple_scaled) for i in range(3)])\n",
    "            \n",
    "            # MLP predictions\n",
    "            mlp_model.eval()\n",
    "            X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "            with torch.no_grad():\n",
    "                mlp_preds = mlp_model(X_tensor).cpu().numpy()\n",
    "            \n",
    "            # LGBM predictions\n",
    "            lgbm_preds = np.column_stack([lgbm_models[i].predict(X_full_scaled) for i in range(3)])\n",
    "            \n",
    "            # Ensemble for this seed\n",
    "            predictions = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "        # Average across all seeds\n",
    "        avg_predictions = np.mean(all_predictions, axis=0)\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            all_predictions_flip = []\n",
    "            \n",
    "            for scaler_full, scaler_simple, gp_models, mlp_model, lgbm_models in self.models:\n",
    "                X_full_scaled = scaler_full.transform(X_full_flip)\n",
    "                X_simple_scaled = scaler_simple.transform(X_simple_flip)\n",
    "                \n",
    "                gp_preds = np.column_stack([gp_models[i].predict(X_simple_scaled) for i in range(3)])\n",
    "                X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "                with torch.no_grad():\n",
    "                    mlp_preds = mlp_model(X_tensor).cpu().numpy()\n",
    "                lgbm_preds = np.column_stack([lgbm_models[i].predict(X_full_scaled) for i in range(3)])\n",
    "                \n",
    "                predictions_flip = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "                all_predictions_flip.append(predictions_flip)\n",
    "            \n",
    "            avg_predictions_flip = np.mean(all_predictions_flip, axis=0)\n",
    "            avg_predictions = (avg_predictions + avg_predictions_flip) / 2\n",
    "        \n",
    "        avg_predictions = np.clip(avg_predictions, 0, 1)\n",
    "        return torch.tensor(avg_predictions)\n",
    "\n",
    "print('Multi-Seed Ensemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fe20fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:36:50.183658Z",
     "iopub.status.busy": "2026-01-15T13:36:50.183100Z",
     "iopub.status.idle": "2026-01-15T13:36:53.411664Z",
     "shell.execute_reply": "2026-01-15T13:36:53.411303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([50, 3])\n",
      "Test predictions range: [0.0000, 0.9650]\n"
     ]
    }
   ],
   "source": [
    "# Quick test with 3 seeds\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: X={X_single.shape}, Y={Y_single.shape}')\n",
    "\n",
    "# Test on a small subset\n",
    "X_test = X_single.iloc[:50]\n",
    "Y_test = Y_single.iloc[:50]\n",
    "\n",
    "model = MultiSeedEnsemble(data='single', n_seeds=3)\n",
    "model.train_model(X_test, Y_test)\n",
    "preds = model.predict(X_test)\n",
    "print(f'Test predictions shape: {preds.shape}')\n",
    "print(f'Test predictions range: [{preds.min():.4f}, {preds.max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf5cfd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:11:13.167256Z",
     "iopub.status.busy": "2026-01-15T15:11:13.166906Z",
     "iopub.status.idle": "2026-01-15T15:11:13.169858Z",
     "shell.execute_reply": "2026-01-15T15:11:13.169479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent MSE: 0.009149 (n=656)\n"
     ]
    }
   ],
   "source": [
    "# Single solvent results already computed\n",
    "mse_single = 0.009149\n",
    "n_single = 656\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb06721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:40:27.725884Z",
     "iopub.status.busy": "2026-01-15T17:40:27.725352Z",
     "iopub.status.idle": "2026-01-15T17:40:27.728240Z",
     "shell.execute_reply": "2026-01-15T17:40:27.727912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data MSE: 0.009539 (n=1227)\n"
     ]
    }
   ],
   "source": [
    "# Full data results already computed\n",
    "mse_full = 0.009539\n",
    "n_full = 1227\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a861dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:40:27.729079Z",
     "iopub.status.busy": "2026-01-15T17:40:27.728984Z",
     "iopub.status.idle": "2026-01-15T17:40:27.732448Z",
     "shell.execute_reply": "2026-01-15T17:40:27.732120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV SCORE SUMMARY (Multi-Seed Ensemble, 10 seeds) ===\n",
      "Single Solvent MSE: 0.009149 (n=656)\n",
      "Full Data MSE: 0.009539 (n=1227)\n",
      "Overall MSE: 0.009403\n",
      "\n",
      "Best CV (exp_032): 0.008194\n",
      "\n",
      "✗ WORSE: 14.76% worse than best CV\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(preds_single)\n",
    "n_full = len(preds_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (Multi-Seed Ensemble, 10 seeds) ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10121d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MultiSeedEnsemble(data='single', n_seeds=10)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MultiSeedEnsemble(data='full', n_seeds=10)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(f'\\n=== FINAL CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f36dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different approach - Per-Target Weight Optimization\n",
    "# The current weights (GP 0.15, MLP 0.55, LGBM 0.30) may not be optimal for all targets\n",
    "\n",
    "# First, let's understand the individual model performance per target\n",
    "# by running a quick analysis\n",
    "\n",
    "print(\"Multi-seed ensemble didn't improve CV.\")\n",
    "print(\"The issue may be that averaging introduces bias, not just reduces variance.\")\n",
    "print(\"\\nLet's try Per-Target Weight Optimization instead.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
