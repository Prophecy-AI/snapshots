{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8755a9c7",
   "metadata": {},
   "source": [
    "# Multi-Model Ensemble (MLP + XGBoost + RF + LGBM)\n",
    "\n",
    "**CRITICAL INSIGHT**: After 49 experiments, ALL model families follow the SAME CV-LB relationship.\n",
    "- LB = 4.23×CV + 0.0533 (R²=0.981)\n",
    "- Intercept (0.0533) > Target (0.0347) → Current approach CANNOT reach target\n",
    "\n",
    "**Hypothesis**: Different model families might have different biases that cancel out when combined.\n",
    "\n",
    "**Implementation**:\n",
    "- Combine MLP + XGBoost + RandomForest + LightGBM (like the \"mixall\" kernel)\n",
    "- Each model might make different errors on novel solvents\n",
    "- The ensemble might have a different CV-LB relationship\n",
    "- Use Spange + DRFP + ACS PCA + Arrhenius kinetics features\n",
    "\n",
    "**Why this might work**:\n",
    "- The \"mixall\" kernel uses this approach and claims good CV/LB\n",
    "- Different model families capture different aspects of the data\n",
    "- Combining them might reduce the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26919f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:41:59.082145Z",
     "iopub.status.busy": "2026-01-15T08:41:59.081577Z",
     "iopub.status.idle": "2026-01-15T08:42:00.893617Z",
     "shell.execute_reply": "2026-01-15T08:42:00.893203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03feffda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.894847Z",
     "iopub.status.busy": "2026-01-15T08:42:00.894671Z",
     "iopub.status.idle": "2026-01-15T08:42:00.899257Z",
     "shell.execute_reply": "2026-01-15T08:42:00.898881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43eb236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.900219Z",
     "iopub.status.busy": "2026-01-15T08:42:00.900108Z",
     "iopub.status.idle": "2026-01-15T08:42:00.931089Z",
     "shell.execute_reply": "2026-01-15T08:42:00.930738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e5598b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.932060Z",
     "iopub.status.busy": "2026-01-15T08:42:00.931963Z",
     "iopub.status.idle": "2026-01-15T08:42:00.937675Z",
     "shell.execute_reply": "2026-01-15T08:42:00.937111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature dimension: 145\n"
     ]
    }
   ],
   "source": [
    "# Combined Featurizer (Spange + DRFP + ACS PCA + Arrhenius kinetics)\n",
    "class CombinedFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "\n",
    "print(f'Combined feature dimension: {CombinedFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1b424a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.939108Z",
     "iopub.status.busy": "2026-01-15T08:42:00.938796Z",
     "iopub.status.idle": "2026-01-15T08:42:00.942439Z",
     "shell.execute_reply": "2026-01-15T08:42:00.942044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP defined\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP for ensemble\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print('SimpleMLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27e588c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.943463Z",
     "iopub.status.busy": "2026-01-15T08:42:00.943351Z",
     "iopub.status.idle": "2026-01-15T08:42:00.953013Z",
     "shell.execute_reply": "2026-01-15T08:42:00.952649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiModelEnsemble defined with MLP(0.4) + XGB(0.2) + RF(0.2) + LGBM(0.2)\n"
     ]
    }
   ],
   "source": [
    "# Multi-Model Ensemble (MLP + XGBoost + RF + LGBM)\n",
    "class MultiModelEnsemble:\n",
    "    \"\"\"Multi-model ensemble combining MLP + XGBoost + RandomForest + LightGBM.\n",
    "    \n",
    "    Hypothesis: Different model families might have different biases that cancel out.\n",
    "    \"\"\"\n",
    "    def __init__(self, data='single', weights=[0.4, 0.2, 0.2, 0.2]):\n",
    "        self.data_type = data\n",
    "        self.weights = weights  # [MLP, XGB, RF, LGBM]\n",
    "        self.featurizer = CombinedFeaturizer(mixed=(data == 'full'))\n",
    "        self.scaler = StandardScaler()\n",
    "        self.y_scaler = StandardScaler()\n",
    "        \n",
    "        # Models\n",
    "        self.mlp = SimpleMLP(self.featurizer.feats_dim, hidden_dim=64, output_dim=3, dropout=0.2).to(device)\n",
    "        self.xgb_models = []\n",
    "        self.rf_models = []\n",
    "        self.lgbm_models = []\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_feat = self.featurizer.featurize(X)\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        Y_np = Y.values\n",
    "        Y_scaled = self.y_scaler.fit_transform(Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        self._train_mlp(X_scaled, Y_scaled)\n",
    "        \n",
    "        # Train XGBoost (one per target)\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=4,\n",
    "                reg_alpha=0.1, reg_lambda=0.1, random_state=42, verbosity=0\n",
    "            )\n",
    "            model.fit(X_scaled, Y_scaled[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "        \n",
    "        # Train RandomForest (one per target)\n",
    "        self.rf_models = []\n",
    "        for i in range(3):\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=100, max_depth=6, min_samples_leaf=5,\n",
    "                random_state=42, n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_scaled, Y_scaled[:, i])\n",
    "            self.rf_models.append(model)\n",
    "        \n",
    "        # Train LightGBM (one per target)\n",
    "        self.lgbm_models = []\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=4,\n",
    "                num_leaves=15, min_child_samples=10, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                random_state=42, verbose=-1\n",
    "            )\n",
    "            model.fit(X_scaled, Y_scaled[:, i])\n",
    "            self.lgbm_models.append(model)\n",
    "    \n",
    "    def _train_mlp(self, X_scaled, Y_scaled):\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "        Y_tensor = torch.tensor(Y_scaled, dtype=torch.double).to(device)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.mlp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "        \n",
    "        target_weights = torch.tensor([1.0, 1.0, 2.0], dtype=torch.double).to(device)\n",
    "        \n",
    "        self.mlp.train()\n",
    "        for epoch in range(100):\n",
    "            for X_batch, Y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp(X_batch)\n",
    "                loss = ((pred - Y_batch) ** 2 * target_weights).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_feat = self.featurizer.featurize(X)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # MLP prediction\n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "            pred_mlp = self.mlp(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # XGBoost prediction\n",
    "        pred_xgb = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # RandomForest prediction\n",
    "        pred_rf = np.column_stack([m.predict(X_scaled) for m in self.rf_models])\n",
    "        \n",
    "        # LightGBM prediction\n",
    "        pred_lgbm = np.column_stack([m.predict(X_scaled) for m in self.lgbm_models])\n",
    "        \n",
    "        # Weighted average (in scaled space)\n",
    "        pred_scaled = (\n",
    "            self.weights[0] * pred_mlp +\n",
    "            self.weights[1] * pred_xgb +\n",
    "            self.weights[2] * pred_rf +\n",
    "            self.weights[3] * pred_lgbm\n",
    "        )\n",
    "        \n",
    "        # Inverse transform\n",
    "        pred = self.y_scaler.inverse_transform(pred_scaled)\n",
    "        pred = np.clip(pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(pred)\n",
    "    \n",
    "    def predict_with_tta(self, X):\n",
    "        if self.data_type == 'single':\n",
    "            return self.predict(X)\n",
    "        \n",
    "        pred1 = self.predict(X)\n",
    "        \n",
    "        X_flip = X.copy()\n",
    "        X_flip[\"SOLVENT A NAME\"] = X[\"SOLVENT B NAME\"]\n",
    "        X_flip[\"SOLVENT B NAME\"] = X[\"SOLVENT A NAME\"]\n",
    "        X_flip[\"SolventB%\"] = 1 - X[\"SolventB%\"]\n",
    "        \n",
    "        pred2 = self.predict(X_flip)\n",
    "        \n",
    "        return (pred1 + pred2) / 2\n",
    "\n",
    "print('MultiModelEnsemble defined with MLP(0.4) + XGB(0.2) + RF(0.2) + LGBM(0.2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facdba7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:42:00.954148Z",
     "iopub.status.busy": "2026-01-15T08:42:00.954042Z",
     "iopub.status.idle": "2026-01-15T08:42:00.968207Z",
     "shell.execute_reply": "2026-01-15T08:42:00.967825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: 656 samples, 24 unique solvents\n",
      "Full data: 1227 samples, 13 unique ramps\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape[0]} samples, {len(X_single['SOLVENT NAME'].unique())} unique solvents\")\n",
    "\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "print(f\"Full data: {X_full.shape[0]} samples, {len(ramps)} unique ramps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MultiModelEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afdfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = MultiModelEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict_with_tta(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceade13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV score (for verification only - NOT part of submission)\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "# Get actuals in same order as predictions\n",
    "actuals_single = []\n",
    "for solvent in sorted(X_single[\"SOLVENT NAME\"].unique()):\n",
    "    mask = X_single[\"SOLVENT NAME\"] == solvent\n",
    "    actuals_single.append(Y_single[mask].values)\n",
    "actuals_single = np.vstack(actuals_single)\n",
    "\n",
    "actuals_full = []\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "for _, row in ramps.iterrows():\n",
    "    mask = (X_full[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X_full[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"])\n",
    "    actuals_full.append(Y_full[mask].values)\n",
    "actuals_full = np.vstack(actuals_full)\n",
    "\n",
    "# Get predictions\n",
    "preds_single = submission_single_solvent[['target_1', 'target_2', 'target_3']].values\n",
    "preds_full = submission_full_data[['target_1', 'target_2', 'target_3']].values\n",
    "\n",
    "# Calculate MSE\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== MULTI-MODEL ENSEMBLE CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "\n",
    "print(f'\\n=== COMPARISON ===')\n",
    "print(f'exp_032 (best CV, GP 0.15 + MLP 0.55 + LGBM 0.3): CV 0.008194')\n",
    "print(f'exp_050 (MULTI-MODEL ENSEMBLE): CV {overall_mse:.6f}')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than exp_032!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than exp_032')\n",
    "    \n",
    "# Estimate LB using old relationship\n",
    "estimated_lb = 4.23 * overall_mse + 0.0533\n",
    "print(f'\\nEstimated LB (using old relationship): {estimated_lb:.4f}')\n",
    "print(f'Best LB so far: 0.0877')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'\\nKey question: Does Multi-Model Ensemble have a DIFFERENT CV-LB relationship?')\n",
    "print(f'If it has lower intercept, we might be able to reach the target.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
