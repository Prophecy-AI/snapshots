{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0feb23ea",
   "metadata": {},
   "source": [
    "# Per-Target Optimization\n",
    "\n",
    "**Hypothesis**: The server may weight targets differently. Our current approach treats all 3 targets equally.\n",
    "\n",
    "**Rationale**:\n",
    "- SM (starting material) may behave differently than products\n",
    "- Different targets may have different optimal feature sets\n",
    "- The CV-LB gap (intercept 0.0533 > target 0.0347) might be reduced by optimizing per-target\n",
    "\n",
    "**Implementation**:\n",
    "1. Train separate models for each target (Product 2, Product 3, SM)\n",
    "2. Use the best ensemble (GP + MLP + LGBM) but with per-target hyperparameters\n",
    "3. Compare per-target CV with overall CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44b761c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:05.927463Z",
     "iopub.status.busy": "2026-01-15T09:22:05.926966Z",
     "iopub.status.idle": "2026-01-15T09:22:07.650335Z",
     "shell.execute_reply": "2026-01-15T09:22:07.649895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03003d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.651597Z",
     "iopub.status.busy": "2026-01-15T09:22:07.651417Z",
     "iopub.status.idle": "2026-01-15T09:22:07.656154Z",
     "shell.execute_reply": "2026-01-15T09:22:07.655781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46920c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.657197Z",
     "iopub.status.busy": "2026-01-15T09:22:07.657097Z",
     "iopub.status.idle": "2026-01-15T09:22:07.690111Z",
     "shell.execute_reply": "2026-01-15T09:22:07.689718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcfd457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.691158Z",
     "iopub.status.busy": "2026-01-15T09:22:07.691053Z",
     "iopub.status.idle": "2026-01-15T09:22:07.697625Z",
     "shell.execute_reply": "2026-01-15T09:22:07.697216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer (for MLP and LGBM) - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e11731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.698595Z",
     "iopub.status.busy": "2026-01-15T09:22:07.698487Z",
     "iopub.status.idle": "2026-01-15T09:22:07.703154Z",
     "shell.execute_reply": "2026-01-15T09:22:07.702777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple feature dimension: 18\n"
     ]
    }
   ],
   "source": [
    "# Simple Featurizer (for GP) - 18 features (Spange + Arrhenius kinetics)\n",
    "class SimpleFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1]  # 18 features\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange])\n",
    "\n",
    "print(f'Simple feature dimension: {SimpleFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b170a1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.704182Z",
     "iopub.status.busy": "2026-01-15T09:22:07.704078Z",
     "iopub.status.idle": "2026-01-15T09:22:07.707726Z",
     "shell.execute_reply": "2026-01-15T09:22:07.707347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim=145, hidden_dim=64, output_dim=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "print('MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33690d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.708690Z",
     "iopub.status.busy": "2026-01-15T09:22:07.708582Z",
     "iopub.status.idle": "2026-01-15T09:22:07.718816Z",
     "shell.execute_reply": "2026-01-15T09:22:07.718421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Target Ensemble model defined\n"
     ]
    }
   ],
   "source": [
    "# Per-Target Ensemble Model\n",
    "class PerTargetEnsemble:\n",
    "    \"\"\"Ensemble with separate models for each target.\"\"\"\n",
    "    def __init__(self, data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.30):\n",
    "        self.data = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        self.mixed = (data == 'full')\n",
    "        \n",
    "        # Featurizers\n",
    "        self.full_featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.simple_featurizer = SimpleFeaturizer(mixed=self.mixed)\n",
    "        \n",
    "        # Scalers (one per target)\n",
    "        self.scalers_full = [StandardScaler() for _ in range(3)]\n",
    "        self.scalers_simple = [StandardScaler() for _ in range(3)]\n",
    "        \n",
    "        # Models (one per target)\n",
    "        self.gp_models = [None, None, None]\n",
    "        self.mlp_models = [None, None, None]\n",
    "        self.lgbm_models = [None, None, None]\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        # Get features\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        Y_np = Y.values\n",
    "        \n",
    "        # Train separate models for each target\n",
    "        for target_idx in range(3):\n",
    "            y_target = Y_np[:, target_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            X_full_scaled = self.scalers_full[target_idx].fit_transform(X_full)\n",
    "            X_simple_scaled = self.scalers_simple[target_idx].fit_transform(X_simple)\n",
    "            \n",
    "            # Train GP\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            self.gp_models[target_idx] = GaussianProcessRegressor(\n",
    "                kernel=kernel, alpha=0.01, n_restarts_optimizer=3, random_state=42\n",
    "            )\n",
    "            self.gp_models[target_idx].fit(X_simple_scaled, y_target)\n",
    "            \n",
    "            # Train MLP\n",
    "            self.mlp_models[target_idx] = MLPModel(\n",
    "                input_dim=self.full_featurizer.feats_dim,\n",
    "                hidden_dim=64,\n",
    "                output_dim=1,\n",
    "                dropout=0.2\n",
    "            ).to(device)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "            y_tensor = torch.tensor(y_target.reshape(-1, 1), dtype=torch.double).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(self.mlp_models[target_idx].parameters(), lr=0.001, weight_decay=1e-4)\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            self.mlp_models[target_idx].train()\n",
    "            for epoch in range(200):\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp_models[target_idx](X_tensor)\n",
    "                loss = criterion(pred, y_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Train LGBM\n",
    "            self.lgbm_models[target_idx] = lgb.LGBMRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, min_child_samples=5, random_state=42, verbose=-1\n",
    "            )\n",
    "            self.lgbm_models[target_idx].fit(X_full_scaled, y_target)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get features\n",
    "        X_full = self.full_featurizer.featurize(X)\n",
    "        X_simple = self.simple_featurizer.featurize(X)\n",
    "        \n",
    "        predictions = np.zeros((len(X), 3))\n",
    "        \n",
    "        for target_idx in range(3):\n",
    "            X_full_scaled = self.scalers_full[target_idx].transform(X_full)\n",
    "            X_simple_scaled = self.scalers_simple[target_idx].transform(X_simple)\n",
    "            \n",
    "            # GP prediction\n",
    "            gp_pred = self.gp_models[target_idx].predict(X_simple_scaled)\n",
    "            \n",
    "            # MLP prediction\n",
    "            self.mlp_models[target_idx].eval()\n",
    "            X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "            with torch.no_grad():\n",
    "                mlp_pred = self.mlp_models[target_idx](X_tensor).cpu().numpy().squeeze()\n",
    "            \n",
    "            # LGBM prediction\n",
    "            lgbm_pred = self.lgbm_models[target_idx].predict(X_full_scaled)\n",
    "            \n",
    "            # Ensemble\n",
    "            predictions[:, target_idx] = (\n",
    "                self.gp_weight * gp_pred +\n",
    "                self.mlp_weight * mlp_pred +\n",
    "                self.lgbm_weight * lgbm_pred\n",
    "            )\n",
    "        \n",
    "        # TTA for mixtures\n",
    "        if self.mixed:\n",
    "            X_full_flip = self.full_featurizer.featurize(X, flip=True)\n",
    "            X_simple_flip = self.simple_featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            predictions_flip = np.zeros((len(X), 3))\n",
    "            \n",
    "            for target_idx in range(3):\n",
    "                X_full_scaled = self.scalers_full[target_idx].transform(X_full_flip)\n",
    "                X_simple_scaled = self.scalers_simple[target_idx].transform(X_simple_flip)\n",
    "                \n",
    "                gp_pred = self.gp_models[target_idx].predict(X_simple_scaled)\n",
    "                \n",
    "                X_tensor = torch.tensor(X_full_scaled, dtype=torch.double).to(device)\n",
    "                with torch.no_grad():\n",
    "                    mlp_pred = self.mlp_models[target_idx](X_tensor).cpu().numpy().squeeze()\n",
    "                \n",
    "                lgbm_pred = self.lgbm_models[target_idx].predict(X_full_scaled)\n",
    "                \n",
    "                predictions_flip[:, target_idx] = (\n",
    "                    self.gp_weight * gp_pred +\n",
    "                    self.mlp_weight * mlp_pred +\n",
    "                    self.lgbm_weight * lgbm_pred\n",
    "                )\n",
    "            \n",
    "            predictions = (predictions + predictions_flip) / 2\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        \n",
    "        return torch.tensor(predictions)\n",
    "\n",
    "print('Per-Target Ensemble model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a057c448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:22:07.719744Z",
     "iopub.status.busy": "2026-01-15T09:22:07.719639Z",
     "iopub.status.idle": "2026-01-15T09:22:09.964481Z",
     "shell.execute_reply": "2026-01-15T09:22:09.964145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([50, 3])\n",
      "Test predictions range: [0.0000, 0.9493]\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: X={X_single.shape}, Y={Y_single.shape}')\n",
    "\n",
    "# Test on a small subset\n",
    "X_test = X_single.iloc[:50]\n",
    "Y_test = Y_single.iloc[:50]\n",
    "\n",
    "model = PerTargetEnsemble(data='single')\n",
    "model.train_model(X_test, Y_test)\n",
    "preds = model.predict(X_test)\n",
    "print(f'Test predictions shape: {preds.shape}')\n",
    "print(f'Test predictions range: [{preds.min():.4f}, {preds.max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on single solvent data\n",
    "print('\\n=== Single Solvent CV (Per-Target Ensemble) ===')\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "all_predictions_single = []\n",
    "all_actuals_single = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PerTargetEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_single.append(predictions.numpy())\n",
    "    all_actuals_single.append(test_Y.values)\n",
    "\n",
    "preds_single = np.vstack(all_predictions_single)\n",
    "actuals_single = np.vstack(all_actuals_single)\n",
    "mse_single = np.mean((preds_single - actuals_single) ** 2)\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={len(preds_single)})')\n",
    "\n",
    "# Per-target MSE\n",
    "for i, target in enumerate(['Product 2', 'Product 3', 'SM']):\n",
    "    mse_target = np.mean((preds_single[:, i] - actuals_single[:, i]) ** 2)\n",
    "    print(f'  {target} MSE: {mse_target:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd21923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on full data\n",
    "print('\\n=== Full Data CV (Per-Target Ensemble) ===')\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = PerTargetEnsemble(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    all_predictions_full.append(predictions.numpy())\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "preds_full = np.vstack(all_predictions_full)\n",
    "actuals_full = np.vstack(all_actuals_full)\n",
    "mse_full = np.mean((preds_full - actuals_full) ** 2)\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={len(preds_full)})')\n",
    "\n",
    "# Per-target MSE\n",
    "for i, target in enumerate(['Product 2', 'Product 3', 'SM']):\n",
    "    mse_target = np.mean((preds_full[:, i] - actuals_full[:, i]) ** 2)\n",
    "    print(f'  {target} MSE: {mse_target:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall MSE\n",
    "n_single = len(preds_single)\n",
    "n_full = len(preds_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== CV SCORE SUMMARY (Per-Target Ensemble) ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')\n",
    "\n",
    "if overall_mse < 0.008194:\n",
    "    improvement = (0.008194 - overall_mse) / 0.008194 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than best CV!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008194) / 0.008194 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than best CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efe073",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b91f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe384343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(f'\\n=== FINAL CV SCORE ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest CV (exp_032): 0.008194')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
