{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c3abc3",
   "metadata": {},
   "source": [
    "# Loop 60 Analysis: Post-TabNet Strategy Review\n",
    "\n",
    "**Key Finding**: TabNet failed dramatically (CV 0.036642 vs best 0.008194 - 347% worse)\n",
    "\n",
    "**Critical Question**: What approaches can CHANGE the CV-LB relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_041', 'cv': 0.0090, 'lb': 0.0932},\n",
    "    {'exp': 'exp_042', 'cv': 0.0145, 'lb': 0.1147},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df.to_string())\n",
    "print(f'\\nBest CV: {df[\"cv\"].min():.4f} ({df.loc[df[\"cv\"].idxmin(), \"exp\"]})')\n",
    "print(f'Best LB: {df[\"lb\"].min():.4f} ({df.loc[df[\"lb\"].idxmin(), \"exp\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'CV-LB Relationship: LB = {slope:.2f}×CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nTarget LB: 0.0347')\n",
    "print(f'Intercept: {intercept:.4f}')\n",
    "print(f'Gap: Intercept - Target = {intercept - 0.0347:.4f}')\n",
    "\n",
    "# Required CV to hit target\n",
    "required_cv = (0.0347 - intercept) / slope\n",
    "print(f'\\nRequired CV to hit target: {required_cv:.6f}')\n",
    "if required_cv < 0:\n",
    "    print('IMPOSSIBLE: Required CV is negative!')\n",
    "else:\n",
    "    print(f'Required CV improvement: {(df[\"cv\"].min() - required_cv) / df[\"cv\"].min() * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddeec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residuals from CV-LB relationship\n",
    "df['predicted_lb'] = slope * df['cv'] + intercept\n",
    "df['residual'] = df['lb'] - df['predicted_lb']\n",
    "df['residual_pct'] = df['residual'] / df['predicted_lb'] * 100\n",
    "\n",
    "print('Residual Analysis (negative = better than predicted):')\n",
    "print(df[['exp', 'cv', 'lb', 'predicted_lb', 'residual', 'residual_pct']].sort_values('residual').to_string())\n",
    "\n",
    "print(f'\\nBest generalization (lowest residual): {df.loc[df[\"residual\"].idxmin(), \"exp\"]}')\n",
    "print(f'Worst generalization (highest residual): {df.loc[df[\"residual\"].idxmax(), \"exp\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would we need to change the CV-LB relationship?\n",
    "print('=== STRATEGIC ANALYSIS ===')\n",
    "print('\\nCurrent CV-LB relationship: LB = 4.23×CV + 0.0533')\n",
    "print('Target: 0.0347')\n",
    "print('\\nTo reach target, we need ONE of:')\n",
    "print('1. Reduce intercept from 0.0533 to < 0.0347 (35% reduction)')\n",
    "print('2. Reduce slope from 4.23 to < 0 (impossible - would mean better CV = worse LB)')\n",
    "print('3. Find an approach that BREAKS the current CV-LB relationship')\n",
    "\n",
    "print('\\n=== APPROACHES TRIED (60 experiments) ===')\n",
    "approaches_tried = [\n",
    "    ('MLP variants', 'exp_000, exp_004, exp_005, exp_006, exp_007, exp_008, exp_010, exp_017'),\n",
    "    ('LightGBM', 'exp_001, exp_002'),\n",
    "    ('XGBoost', 'exp_041'),\n",
    "    ('CatBoost', 'exp_047'),\n",
    "    ('GP', 'exp_030, exp_031, exp_032, exp_044'),\n",
    "    ('Ridge', 'exp_009, exp_033, exp_034, exp_049'),\n",
    "    ('KNN', 'exp_040'),\n",
    "    ('GNN', 'exp_051, exp_056'),\n",
    "    ('ChemBERTa', 'exp_052'),\n",
    "    ('TabNet', 'exp_061'),\n",
    "    ('Ensembles', 'exp_011, exp_012, exp_013, exp_028, exp_030, exp_041, exp_045, exp_050'),\n",
    "    ('Feature engineering', 'exp_003, exp_018, exp_019, exp_023, exp_024, exp_039, exp_044, exp_046, exp_048'),\n",
    "    ('Per-target models', 'exp_025, exp_053'),\n",
    "    ('Per-solvent-type models', 'exp_054'),\n",
    "    ('Hyperparameter optimization', 'exp_055'),\n",
    "    ('Physical constraints', 'exp_059'),\n",
    "    ('GroupKFold CV', 'exp_042'),\n",
    "]\n",
    "for approach, exps in approaches_tried:\n",
    "    print(f'- {approach}: {exps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e958b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What HASN'T been tried?\n",
    "print('\\n=== APPROACHES NOT YET TRIED ===')\n",
    "print('1. Quantile regression (predict distribution, not point estimate)')\n",
    "print('2. Conformal prediction (uncertainty quantification)')\n",
    "print('3. Domain adaptation (explicitly model train-test shift)')\n",
    "print('4. Adversarial training (make model robust to distribution shift)')\n",
    "print('5. Bayesian neural networks (uncertainty-aware predictions)')\n",
    "print('6. Mixture of experts (different models for different regions)')\n",
    "print('7. Gradient-based meta-learning (MAML for few-shot generalization)')\n",
    "print('8. Self-training / pseudo-labeling (use test predictions to improve)')\n",
    "print('9. Temperature scaling (calibrate predictions)')\n",
    "print('10. Ensemble selection (select best subset of models)')\n",
    "\n",
    "print('\\n=== MOST PROMISING APPROACHES ===')\n",
    "print('1. Quantile regression - could provide better uncertainty estimates')\n",
    "print('2. Temperature scaling - simple calibration method')\n",
    "print('3. Ensemble selection - find optimal subset of diverse models')\n",
    "print('4. Bayesian optimization of ensemble weights - find optimal weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what makes exp_030 the best\n",
    "print('=== ANALYSIS OF BEST MODEL (exp_030) ===')\n",
    "print('Model: GP (0.15) + MLP (0.55) + LGBM (0.30)')\n",
    "print('CV: 0.0083')\n",
    "print('LB: 0.0877')\n",
    "print('LB/CV ratio: 10.57x')\n",
    "print('\\nKey characteristics:')\n",
    "print('- GP provides uncertainty-aware predictions')\n",
    "print('- MLP captures non-linear patterns')\n",
    "print('- LGBM provides tree-based diversity')\n",
    "print('- Weighted ensemble combines strengths')\n",
    "\n",
    "print('\\n=== WHAT COULD IMPROVE IT? ===')\n",
    "print('1. Better ensemble weights (Bayesian optimization)')\n",
    "print('2. More diverse base models')\n",
    "print('3. Stacking with meta-learner')\n",
    "print('4. Post-hoc calibration')\n",
    "print('5. Uncertainty-based weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what LB we could achieve with different intercepts\n",
    "print('=== SENSITIVITY ANALYSIS ===')\n",
    "print('\\nIf we could reduce the intercept:')\n",
    "for new_intercept in [0.05, 0.045, 0.04, 0.035, 0.03]:\n",
    "    new_lb = slope * df['cv'].min() + new_intercept\n",
    "    print(f'Intercept {new_intercept:.3f} -> LB {new_lb:.4f} (target: 0.0347)')\n",
    "\n",
    "print('\\nIf we could reduce the slope:')\n",
    "for new_slope in [4.0, 3.5, 3.0, 2.5, 2.0]:\n",
    "    new_lb = new_slope * df['cv'].min() + intercept\n",
    "    print(f'Slope {new_slope:.1f} -> LB {new_lb:.4f} (target: 0.0347)')\n",
    "\n",
    "print('\\nTo hit target 0.0347 with current best CV (0.0083):')\n",
    "required_intercept = 0.0347 - slope * df['cv'].min()\n",
    "print(f'Required intercept: {required_intercept:.4f} (current: {intercept:.4f})')\n",
    "print(f'Intercept reduction needed: {(intercept - required_intercept) / intercept * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b57a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations\n",
    "print('=== FINAL RECOMMENDATIONS ===')\n",
    "print('\\n1. DO NOT submit TabNet (CV 0.036642 is 347% worse)')\n",
    "print('\\n2. Focus on approaches that could CHANGE the CV-LB relationship:')\n",
    "print('   - Quantile regression (different loss function)')\n",
    "print('   - Temperature scaling (post-hoc calibration)')\n",
    "print('   - Bayesian optimization of ensemble weights')\n",
    "print('\\n3. With only 3 submissions remaining:')\n",
    "print('   - Save at least 1 for final attempt')\n",
    "print('   - Only submit if we find a fundamentally different approach')\n",
    "print('   - The goal is to reduce the intercept, not just minimize CV')\n",
    "\n",
    "print('\\n=== CRITICAL INSIGHT ===')\n",
    "print('The CV-LB relationship has intercept 0.0533 > target 0.0347')\n",
    "print('This means CV minimization alone CANNOT reach the target')\n",
    "print('We need to find an approach that BREAKS this relationship')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
