{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330f5939",
   "metadata": {},
   "source": [
    "# Loop 41 Analysis: GroupKFold(5) Hypothesis Disproven\n",
    "\n",
    "**Key Result**: GroupKFold(5) CV (0.009237) is only 1.13x higher than Leave-One-Out CV (0.008199).\n",
    "\n",
    "This is NOT the dramatic increase expected (3-5x). The CV-LB gap is NOT due to the CV procedure - it's STRUCTURAL.\n",
    "\n",
    "**Implications**:\n",
    "1. The \"mixall\" kernel's claim of \"good CV-LB\" may be misleading\n",
    "2. The CV-LB gap is due to something else (model variance, hidden test data, etc.)\n",
    "3. We need a fundamentally different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b46ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(\"Submission History:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6cb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship\n",
    "from scipy import stats\n",
    "\n",
    "cv_values = df['cv'].values\n",
    "lb_values = df['lb'].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_values, lb_values)\n",
    "\n",
    "print(f\"\\nCV-LB Relationship:\")\n",
    "print(f\"  LB = {slope:.2f} × CV + {intercept:.4f}\")\n",
    "print(f\"  R² = {r_value**2:.4f}\")\n",
    "print(f\"  Intercept = {intercept:.4f}\")\n",
    "print(f\"  Target = 0.0347\")\n",
    "print(f\"  Gap: Intercept is {intercept/0.0347:.2f}x higher than target\")\n",
    "\n",
    "# What CV would be needed to reach target?\n",
    "required_cv = (0.0347 - intercept) / slope\n",
    "print(f\"\\nTo reach target with current relationship:\")\n",
    "print(f\"  Required CV = {required_cv:.6f}\")\n",
    "if required_cv < 0:\n",
    "    print(f\"  IMPOSSIBLE - would require negative CV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupKFold(5) experiment results\n",
    "print(\"\\n=== GroupKFold(5) Experiment Results ===\")\n",
    "print(f\"Leave-One-Out CV: 0.008199\")\n",
    "print(f\"GroupKFold(5) CV: 0.009237\")\n",
    "print(f\"Ratio: {0.009237/0.008199:.2f}x\")\n",
    "print(f\"\\nExpected ratio if CV-LB gap was due to CV procedure: 3-5x\")\n",
    "print(f\"Actual ratio: 1.13x\")\n",
    "print(f\"\\nCONCLUSION: The CV-LB gap is NOT due to the CV procedure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b16ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this mean for our strategy?\n",
    "print(\"\\n=== Strategic Implications ===\")\n",
    "print(\"\\n1. The CV-LB gap is STRUCTURAL, not procedural\")\n",
    "print(\"   - Changing CV procedure doesn't help\")\n",
    "print(\"   - The gap is due to model behavior, not evaluation\")\n",
    "\n",
    "print(\"\\n2. Possible causes of the structural gap:\")\n",
    "print(\"   a) Model variance between runs (different random seeds on Kaggle)\")\n",
    "print(\"   b) Hidden test data with different distribution\")\n",
    "print(\"   c) Systematic overfitting to training data distribution\")\n",
    "print(\"   d) Kaggle's evaluation using different metric weighting\")\n",
    "\n",
    "print(\"\\n3. What to try next:\")\n",
    "print(\"   a) AGGRESSIVE REGULARIZATION - reduce overfitting\")\n",
    "print(\"   b) SIMPLER MODELS - less variance, better generalization\")\n",
    "print(\"   c) ENSEMBLE DIVERSITY - reduce variance through diversity\")\n",
    "print(\"   d) DOMAIN ADAPTATION - address distribution shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what LB we might get with aggressive regularization\n",
    "print(\"\\n=== Aggressive Regularization Hypothesis ===\")\n",
    "print(\"\\nIf the CV-LB gap is due to overfitting:\")\n",
    "print(\"  - Stronger regularization should INCREASE CV (worse locally)\")\n",
    "print(\"  - But DECREASE LB (better generalization)\")\n",
    "print(\"  - The CV-LB ratio should decrease\")\n",
    "\n",
    "print(\"\\nCurrent best:\")\n",
    "print(f\"  CV: 0.0083, LB: 0.0877\")\n",
    "print(f\"  Ratio: {0.0877/0.0083:.1f}x\")\n",
    "\n",
    "print(\"\\nIf regularization works:\")\n",
    "print(\"  CV might increase to 0.012-0.015\")\n",
    "print(\"  But LB might decrease to 0.06-0.07\")\n",
    "print(f\"  New ratio: {0.065/0.013:.1f}x (better!)\")\n",
    "\n",
    "print(\"\\nTarget: 0.0347\")\n",
    "print(\"Gap to close: 2.53x (0.0877 → 0.0347)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b19b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What regularization changes to try?\n",
    "print(\"\\n=== Aggressive Regularization Experiment Design ===\")\n",
    "print(\"\\nCurrent model (GP 0.15 + MLP 0.55 + LGBM 0.3):\")\n",
    "print(\"  MLP: Dropout 0.2, weight decay 1e-4, hidden [128, 128]\")\n",
    "print(\"  LGBM: max_depth=6, min_child_samples=10\")\n",
    "print(\"  GP: Matern kernel, length_scale=1.0\")\n",
    "\n",
    "print(\"\\nAggressive regularization:\")\n",
    "print(\"  MLP: Dropout 0.5, weight decay 1e-3, hidden [32, 16]\")\n",
    "print(\"  LGBM: max_depth=3, min_child_samples=20, reg_alpha=1.0\")\n",
    "print(\"  GP: Larger length scale (2.0), more noise\")\n",
    "\n",
    "print(\"\\nExpected outcome:\")\n",
    "print(\"  CV will get WORSE (maybe 0.012-0.015)\")\n",
    "print(\"  But LB might IMPROVE if gap is due to overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Try the exact \"mixall\" ensemble\n",
    "print(\"\\n=== Alternative: Exact 'mixall' Ensemble ===\")\n",
    "print(\"\\nThe 'mixall' kernel uses:\")\n",
    "print(\"  - MLP + XGBoost + RF + LightGBM\")\n",
    "print(\"  - Optuna-optimized weights\")\n",
    "print(\"  - Spange features only (no DRFP, no ACS PCA)\")\n",
    "\n",
    "print(\"\\nWe tested XGBoost in exp_039 but it made CV worse.\")\n",
    "print(\"However, we used different weights and features.\")\n",
    "\n",
    "print(\"\\nTo fully test the 'mixall' approach:\")\n",
    "print(\"  1. Use Spange features only (18 features)\")\n",
    "print(\"  2. Use MLP + XGB + RF + LGBM ensemble\")\n",
    "print(\"  3. Use Optuna to optimize weights\")\n",
    "print(\"  4. Use GroupKFold(5) CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d847bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we know\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: What We Know After 41 Experiments\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. BEST RESULTS:\")\n",
    "print(f\"   Best CV: 0.008199 (exp_038: GP 0.15 + MLP 0.55 + LGBM 0.3)\")\n",
    "print(f\"   Best LB: 0.0877 (exp_030)\")\n",
    "print(f\"   Target: 0.0347\")\n",
    "print(f\"   Gap: 2.53x\")\n",
    "\n",
    "print(\"\\n2. CV-LB RELATIONSHIP:\")\n",
    "print(f\"   LB = 4.27 × CV + 0.0527 (R² = 0.967)\")\n",
    "print(f\"   Intercept (0.0527) > Target (0.0347)\")\n",
    "print(f\"   Current approach CANNOT reach target!\")\n",
    "\n",
    "print(\"\\n3. GROUPKFOLD(5) HYPOTHESIS: DISPROVEN\")\n",
    "print(f\"   GroupKFold CV: 0.009237 (only 1.13x higher than LOO)\")\n",
    "print(f\"   The gap is STRUCTURAL, not procedural\")\n",
    "\n",
    "print(\"\\n4. WHAT WORKS:\")\n",
    "print(\"   - GP + MLP + LGBM ensemble\")\n",
    "print(\"   - Spange + DRFP + ACS PCA features\")\n",
    "print(\"   - Arrhenius kinetics features\")\n",
    "print(\"   - TTA for mixtures\")\n",
    "\n",
    "print(\"\\n5. WHAT DOESN'T WORK:\")\n",
    "print(\"   - k-NN (222% worse)\")\n",
    "print(\"   - Deep residual MLP (5x worse)\")\n",
    "print(\"   - Higher GP weight (10.61% worse)\")\n",
    "print(\"   - Similarity weighting (169% worse)\")\n",
    "print(\"   - Feature selection alone (16.83% worse)\")\n",
    "print(\"   - XGBoost addition (6.51% worse)\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS:\")\n",
    "print(\"   PRIORITY 1: Aggressive regularization\")\n",
    "print(\"   PRIORITY 2: Simpler models (reduce variance)\")\n",
    "print(\"   PRIORITY 3: Submit to verify new CV-LB relationship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef23343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n**AGGRESSIVE REGULARIZATION EXPERIMENT**\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"  The CV-LB gap is structural (not procedural).\")\n",
    "print(\"  This suggests overfitting to training distribution.\")\n",
    "print(\"  Stronger regularization should reduce the gap.\")\n",
    "\n",
    "print(\"\\nImplementation:\")\n",
    "print(\"  1. Keep GP + MLP + LGBM ensemble\")\n",
    "print(\"  2. Increase MLP dropout: 0.2 → 0.5\")\n",
    "print(\"  3. Increase MLP weight decay: 1e-4 → 1e-3\")\n",
    "print(\"  4. Reduce MLP hidden: [128, 128] → [32, 16]\")\n",
    "print(\"  5. Reduce LGBM max_depth: 6 → 3\")\n",
    "print(\"  6. Increase LGBM min_child_samples: 10 → 20\")\n",
    "print(\"  7. Increase LGBM reg_alpha: 0.1 → 1.0\")\n",
    "\n",
    "print(\"\\nExpected outcome:\")\n",
    "print(\"  CV will get WORSE (maybe 0.012-0.015)\")\n",
    "print(\"  But LB might IMPROVE if gap is due to overfitting\")\n",
    "print(\"  If LB improves, this confirms the overfitting hypothesis\")\n",
    "\n",
    "print(\"\\n**THE TARGET IS REACHABLE.**\")\n",
    "print(\"We just need to find the right regularization level.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
