{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b229c2f",
   "metadata": {},
   "source": [
    "# Loop 64 Analysis: Understanding the CV-LB Gap\n",
    "\n",
    "## Key Insight\n",
    "The CV-LB relationship is: LB = 4.21 × CV + 0.0535 (R² = 0.98)\n",
    "\n",
    "The intercept (0.0535) is HIGHER than the target (0.0347). This means:\n",
    "- Even with CV = 0, the predicted LB would be 0.0535 > target\n",
    "- The current approach CANNOT reach the target by minimizing CV alone\n",
    "\n",
    "## Questions to Investigate\n",
    "1. What is the distribution of errors across different solvents?\n",
    "2. Are there specific solvents that contribute more to the error?\n",
    "3. Is there a pattern in which solvents are harder to predict?\n",
    "4. Can we identify features that correlate with prediction difficulty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f037294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "# Load data\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Single solvent data: {df_single.shape}')\n",
    "print(f'Full data: {df_full.shape}')\n",
    "print(f'Spange descriptors: {spange_df.shape}')\n",
    "print(f'\\nSolvents in single: {df_single[\"SOLVENT NAME\"].nunique()}')\n",
    "print(f'Unique solvents: {sorted(df_single[\"SOLVENT NAME\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of target values\n",
    "print('=== Target Distribution ===')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'{col}: mean={df_single[col].mean():.4f}, std={df_single[col].std():.4f}, min={df_single[col].min():.4f}, max={df_single[col].max():.4f}')\n",
    "\n",
    "print('\\n=== Per-Solvent Target Means ===')\n",
    "solvent_means = df_single.groupby('SOLVENT NAME')[['Product 2', 'Product 3', 'SM']].mean()\n",
    "print(solvent_means.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadcb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the variance of targets per solvent\n",
    "print('=== Per-Solvent Target Variance ===')\n",
    "solvent_vars = df_single.groupby('SOLVENT NAME')[['Product 2', 'Product 3', 'SM']].var()\n",
    "print(solvent_vars.round(6))\n",
    "\n",
    "print('\\n=== Solvents with Highest Variance ===')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    top_var = solvent_vars[col].nlargest(5)\n",
    "    print(f'{col}: {list(top_var.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Spange descriptors\n",
    "print('=== Spange Descriptors ===')\n",
    "print(spange_df.describe().round(4))\n",
    "\n",
    "print('\\n=== Descriptor Correlations with Target Means ===')\n",
    "solvent_means_with_spange = solvent_means.join(spange_df)\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    correlations = solvent_means_with_spange.corr()[col].drop(['Product 2', 'Product 3', 'SM'])\n",
    "    top_corr = correlations.abs().nlargest(5)\n",
    "    print(f'\\n{col}:')\n",
    "    for desc in top_corr.index:\n",
    "        print(f'  {desc}: {correlations[desc]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the CV-LB relationship more carefully\n",
    "# Load submission history\n",
    "import json\n",
    "with open('/home/code/session_state.json') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "submissions = state.get('submissions', [])\n",
    "cvs = [s['cv_score'] for s in submissions if s.get('cv_score') and s.get('lb_score')]\n",
    "lbs = [s['lb_score'] for s in submissions if s.get('cv_score') and s.get('lb_score')]\n",
    "exp_ids = [s['experiment_id'] for s in submissions if s.get('cv_score') and s.get('lb_score')]\n",
    "\n",
    "print('=== CV-LB Relationship ===')\n",
    "for exp_id, cv, lb in zip(exp_ids, cvs, lbs):\n",
    "    ratio = lb / cv if cv > 0 else 0\n",
    "    print(f'{exp_id}: CV={cv:.6f}, LB={lb:.5f}, LB/CV={ratio:.2f}')\n",
    "\n",
    "# Linear regression\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cvs, lbs)\n",
    "print(f'\\nLinear fit: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'Intercept = {intercept:.4f}')\n",
    "print(f'Target = 0.0347')\n",
    "print(f'Gap = {intercept - 0.0347:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV vs LB\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cvs, lbs, s=100, alpha=0.7)\n",
    "for i, exp_id in enumerate(exp_ids):\n",
    "    plt.annotate(exp_id, (cvs[i], lbs[i]), fontsize=8)\n",
    "\n",
    "# Fit line\n",
    "x_line = np.linspace(min(cvs), max(cvs), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "plt.plot(x_line, y_line, 'r--', label=f'LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.0347, color='g', linestyle=':', label='Target (0.0347)')\n",
    "\n",
    "# Intercept line\n",
    "plt.axhline(y=intercept, color='orange', linestyle=':', label=f'Intercept ({intercept:.4f})')\n",
    "\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to /home/code/exploration/cv_lb_relationship.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what the intercept means\n",
    "print('=== Intercept Analysis ===')\n",
    "print(f'Intercept: {intercept:.4f}')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'Gap: {intercept - 0.0347:.4f}')\n",
    "print()\n",
    "print('The intercept represents the \"baseline\" LB error that exists even with perfect CV.')\n",
    "print('This suggests there is a SYSTEMATIC BIAS in how the model generalizes to the test set.')\n",
    "print()\n",
    "print('Possible causes:')\n",
    "print('1. The test set contains solvents that are fundamentally different from training solvents')\n",
    "print('2. The model is overconfident on OOD samples')\n",
    "print('3. There is a distribution shift between CV and LB evaluation')\n",
    "print()\n",
    "print('To reach the target (0.0347), we need to either:')\n",
    "print('1. Find an approach that reduces the intercept (changes the CV-LB relationship)')\n",
    "print('2. Find an approach with a different CV-LB relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fe11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the residuals from the CV-LB fit\n",
    "residuals = np.array(lbs) - (slope * np.array(cvs) + intercept)\n",
    "print('=== Residuals from CV-LB Fit ===')\n",
    "for exp_id, cv, lb, res in zip(exp_ids, cvs, lbs, residuals):\n",
    "    print(f'{exp_id}: CV={cv:.6f}, LB={lb:.5f}, Residual={res:+.5f}')\n",
    "\n",
    "print(f'\\nMean residual: {residuals.mean():.6f}')\n",
    "print(f'Std residual: {residuals.std():.6f}')\n",
    "print(f'Max positive residual: {residuals.max():.6f} ({exp_ids[np.argmax(residuals)]})')\n",
    "print(f'Max negative residual: {residuals.min():.6f} ({exp_ids[np.argmin(residuals)]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f146628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any experiments with significantly different CV-LB relationship\n",
    "print('=== Experiments with Unusual CV-LB Relationship ===')\n",
    "for exp_id, cv, lb, res in zip(exp_ids, cvs, lbs, residuals):\n",
    "    if abs(res) > 0.005:  # More than 0.5% deviation\n",
    "        print(f'{exp_id}: CV={cv:.6f}, LB={lb:.5f}, Residual={res:+.5f} (UNUSUAL)')\n",
    "\n",
    "print('\\nNote: Experiments with negative residuals performed BETTER on LB than expected from CV.')\n",
    "print('These might have approaches that generalize better to OOD data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0da784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print('=== SUMMARY ===')\n",
    "print(f'Best CV: {min(cvs):.6f} ({exp_ids[np.argmin(cvs)]})')\n",
    "print(f'Best LB: {min(lbs):.5f} ({exp_ids[np.argmin(lbs)]})')\n",
    "print(f'CV-LB relationship: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'Intercept: {intercept:.4f} > Target: 0.0347')\n",
    "print()\n",
    "print('=== KEY INSIGHT ===')\n",
    "print('The intercept (0.0535) is HIGHER than the target (0.0347).')\n",
    "print('This means the current approach CANNOT reach the target by minimizing CV alone.')\n",
    "print('We need an approach that CHANGES the CV-LB relationship itself.')\n",
    "print()\n",
    "print('=== RECOMMENDATIONS ===')\n",
    "print('1. Try approaches that specifically improve OOD generalization')\n",
    "print('2. Try approaches that reduce model confidence on uncertain predictions')\n",
    "print('3. Try approaches that leverage domain knowledge (solvent similarity, mass balance)')\n",
    "print('4. Consider that the test set might have fundamentally different solvents')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
