{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f609fc",
   "metadata": {},
   "source": [
    "# Loop 63 Analysis: Critical Assessment\n",
    "\n",
    "## Key Questions:\n",
    "1. What is the true CV-LB relationship?\n",
    "2. What approaches haven't been tried?\n",
    "3. Is there a fundamentally different approach that could work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission data\n",
    "submissions = [\n",
    "    ('exp_000', 0.0111, 0.0982),\n",
    "    ('exp_001', 0.0123, 0.1065),\n",
    "    ('exp_003', 0.0105, 0.0972),\n",
    "    ('exp_005', 0.0104, 0.0969),\n",
    "    ('exp_006', 0.0097, 0.0946),\n",
    "    ('exp_007', 0.0093, 0.0932),\n",
    "    ('exp_009', 0.0092, 0.0936),\n",
    "    ('exp_012', 0.0090, 0.0913),\n",
    "    ('exp_024', 0.0087, 0.0893),\n",
    "    ('exp_026', 0.0085, 0.0887),\n",
    "    ('exp_030', 0.0083, 0.0877),\n",
    "    ('exp_041', 0.0090, 0.0932),\n",
    "    ('exp_042', 0.0145, 0.1147),\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions, columns=['exp', 'cv', 'lb'])\n",
    "print('Submission History:')\n",
    "print(df.to_string(index=False))\n",
    "print(f'\\nBest CV: {df[\"cv\"].min():.6f}')\n",
    "print(f'Best LB: {df[\"lb\"].min():.6f}')\n",
    "print(f'Target: 0.0347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression analysis\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'CV-LB Relationship: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nInterpretation:')\n",
    "print(f'- Intercept ({intercept:.4f}) > Target (0.0347): {intercept > 0.0347}')\n",
    "print(f'- Even with CV=0, predicted LB would be {intercept:.4f}')\n",
    "print(f'\\nRequired CV to hit target (using linear model):')\n",
    "required_cv = (0.0347 - intercept) / slope\n",
    "print(f'- Required CV: {required_cv:.6f}')\n",
    "if required_cv < 0:\n",
    "    print(f'- IMPOSSIBLE: Required CV is negative!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But wait - let's check if the relationship is truly linear\n",
    "# Or if there's a different pattern at lower CV values\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['cv'], df['lb'], s=100, alpha=0.7)\n",
    "plt.plot(df['cv'], slope * df['cv'] + intercept, 'r--', label=f'Linear: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "plt.axhline(y=0.0347, color='g', linestyle=':', label='Target LB = 0.0347')\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to /home/code/exploration/cv_lb_relationship.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The intercept is high because the LB test set is fundamentally different\n",
    "# from the CV test set. The CV uses leave-one-solvent-out, but the LB might have:\n",
    "# 1. Different solvents entirely\n",
    "# 2. Different temperature/time ranges\n",
    "# 3. Different mixture compositions\n",
    "\n",
    "# Let's analyze what we know about the data\n",
    "DATA_PATH = '/home/data'\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('=== Data Overview ===')\n",
    "print(f'Single solvent: {len(df_single)} samples, {df_single[\"SOLVENT NAME\"].nunique()} solvents')\n",
    "print(f'Full data: {len(df_full)} samples, {df_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates().shape[0]} ramps')\n",
    "print(f'\\nSingle solvent solvents: {sorted(df_single[\"SOLVENT NAME\"].unique())}')\n",
    "print(f'\\nFull data solvent A: {sorted(df_full[\"SOLVENT A NAME\"].unique())}')\n",
    "print(f'Full data solvent B: {sorted(df_full[\"SOLVENT B NAME\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature and time ranges\n",
    "print('=== Temperature/Time Ranges ===')\n",
    "print(f'Single solvent:')\n",
    "print(f'  Temperature: {df_single[\"Temperature\"].min():.1f} - {df_single[\"Temperature\"].max():.1f}')\n",
    "print(f'  Residence Time: {df_single[\"Residence Time\"].min():.1f} - {df_single[\"Residence Time\"].max():.1f}')\n",
    "print(f'\\nFull data:')\n",
    "print(f'  Temperature: {df_full[\"Temperature\"].min():.1f} - {df_full[\"Temperature\"].max():.1f}')\n",
    "print(f'  Residence Time: {df_full[\"Residence Time\"].min():.1f} - {df_full[\"Residence Time\"].max():.1f}')\n",
    "print(f'  SolventB%: {df_full[\"SolventB%\"].min():.1f} - {df_full[\"SolventB%\"].max():.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1109cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution analysis\n",
    "print('=== Target Distribution ===')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'\\n{col}:')\n",
    "    print(f'  Single: mean={df_single[col].mean():.4f}, std={df_single[col].std():.4f}, range=[{df_single[col].min():.4f}, {df_single[col].max():.4f}]')\n",
    "    print(f'  Full:   mean={df_full[col].mean():.4f}, std={df_full[col].std():.4f}, range=[{df_full[col].min():.4f}, {df_full[col].max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f39efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches have been tried?\n",
    "approaches_tried = [\n",
    "    ('MLP architectures', 'Various depths [16] to [512,256,128,64], residual connections'),\n",
    "    ('Ensemble methods', 'Bagging (3-15 models), weighted averaging, stacking'),\n",
    "    ('Tree models', 'LightGBM, XGBoost, CatBoost, Random Forest'),\n",
    "    ('Gaussian Process', 'GP alone and in ensemble'),\n",
    "    ('GNN', 'Basic GNN and GAT - both failed'),\n",
    "    ('Pre-trained', 'ChemBERTa embeddings - failed'),\n",
    "    ('TabNet', 'Attention-based - failed'),\n",
    "    ('Features', 'Spange, DRFP, ACS PCA, RDKit descriptors, fragprints'),\n",
    "    ('Regularization', 'Dropout, weight decay, aggressive regularization'),\n",
    "    ('Loss functions', 'MSE, Huber, Quantile'),\n",
    "    ('Data augmentation', 'TTA for mixtures, Mixup - failed'),\n",
    "    ('CV-LB gap reduction', 'GroupKFold, importance weighting - failed'),\n",
    "    ('Physical constraints', 'Mass balance normalization - marginal'),\n",
    "    ('Per-target optimization', 'Different weights per target'),\n",
    "    ('Per-solvent-type models', 'Different models for different solvent types - failed'),\n",
    "]\n",
    "\n",
    "print('=== Approaches Tried ===')\n",
    "for approach, details in approaches_tried:\n",
    "    print(f'\\n{approach}:')\n",
    "    print(f'  {details}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42254834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What HASN'T been tried that could fundamentally change the CV-LB relationship?\n",
    "print('=== Potential Unexplored Approaches ===')\n",
    "unexplored = [\n",
    "    ('1. Solvent-specific calibration', 'Learn a calibration factor for each solvent type'),\n",
    "    ('2. Uncertainty-based prediction adjustment', 'Use GP uncertainty to adjust predictions'),\n",
    "    ('3. Nearest-neighbor blending', 'Blend with predictions from most similar training solvents'),\n",
    "    ('4. Multi-fidelity learning', 'Use single solvent data to inform mixture predictions'),\n",
    "    ('5. Domain adaptation', 'Explicitly model the distribution shift'),\n",
    "    ('6. Bayesian model averaging', 'Weight models by their uncertainty'),\n",
    "    ('7. Conformal prediction with coverage guarantee', 'Ensure predictions are well-calibrated'),\n",
    "    ('8. Ensemble selection', 'Select best model per sample based on similarity'),\n",
    "]\n",
    "\n",
    "for approach, details in unexplored:\n",
    "    print(f'{approach}:')\n",
    "    print(f'  {details}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL INSIGHT: The CV-LB gap is ~10x, not ~4x as previously calculated\n",
    "# Let's recalculate\n",
    "\n",
    "print('=== CV-LB Gap Analysis ===')\n",
    "for _, row in df.iterrows():\n",
    "    ratio = row['lb'] / row['cv']\n",
    "    print(f'{row[\"exp\"]}: CV={row[\"cv\"]:.4f}, LB={row[\"lb\"]:.4f}, Ratio={ratio:.2f}x')\n",
    "\n",
    "print(f'\\nAverage ratio: {(df[\"lb\"] / df[\"cv\"]).mean():.2f}x')\n",
    "print(f'\\nIf we achieve CV=0.008194 (best), expected LB = {0.008194 * 10:.4f}')\n",
    "print(f'But actual best LB = 0.0877 (from exp_030 with CV=0.0083)')\n",
    "print(f'\\nThe relationship is: LB ≈ 10 * CV + 0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a64bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify with a different regression model\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "X = df['cv'].values.reshape(-1, 1)\n",
    "y = df['lb'].values\n",
    "\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X, y)\n",
    "print(f'Huber Regression: LB = {huber.coef_[0]:.2f} * CV + {huber.intercept_:.4f}')\n",
    "\n",
    "# Predict LB for best CV\n",
    "best_cv = 0.008194\n",
    "predicted_lb = huber.predict([[best_cv]])[0]\n",
    "print(f'\\nPredicted LB for best CV ({best_cv}): {predicted_lb:.4f}')\n",
    "print(f'Target: 0.0347')\n",
    "print(f'Gap to target: {predicted_lb - 0.0347:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCLUSION: The target (0.0347) requires a fundamentally different approach\n",
    "# Current best: CV=0.008194 → LB=0.0877 (predicted ~0.087)\n",
    "# Target: LB=0.0347\n",
    "# Gap: 0.053 (60% reduction needed)\n",
    "\n",
    "print('=== CRITICAL CONCLUSION ===')\n",
    "print(f'Current best LB: 0.0877')\n",
    "print(f'Target LB: 0.0347')\n",
    "print(f'Required improvement: {(0.0877 - 0.0347) / 0.0877 * 100:.1f}%')\n",
    "print(f'\\nThe CV-LB relationship has a high intercept (~0.05) that cannot be reduced')\n",
    "print(f'by simply improving CV. We need an approach that:')\n",
    "print(f'1. Changes the CV-LB relationship itself')\n",
    "print(f'2. Reduces the intercept, not just the slope')\n",
    "print(f'3. Improves extrapolation to unseen solvents')\n",
    "print(f'\\nPotential approaches:')\n",
    "print(f'- Ensemble of diverse models with different CV-LB relationships')\n",
    "print(f'- Post-hoc calibration based on solvent similarity')\n",
    "print(f'- Domain adaptation techniques')\n",
    "print(f'- Meta-learning for quick adaptation to new solvents')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
