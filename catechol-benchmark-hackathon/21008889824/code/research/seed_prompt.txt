## Current Status
- Best CV score: 0.008194 from exp_032 (GP 0.15 + MLP 0.55 + LGBM 0.3)
- Best LB score: 0.0877 from exp_030
- CV-LB gap: LB = 4.23*CV + 0.0533 (R²=0.98)
- Target: 0.0347 (2.53x away from best LB)
- Submissions remaining: 3

## CRITICAL INSIGHT: THE INTERCEPT PROBLEM
The CV-LB relationship has an intercept of 0.0533, which is HIGHER than the target (0.0347).
This means: **Even with CV=0, LB would be 0.0533 > 0.0347**
We CANNOT reach the target by improving CV alone. We need to CHANGE THE RELATIONSHIP.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The GNN experiment was executed correctly.
- Evaluator's top priority: Do NOT submit the GNN model (CV 0.014080, 71.84% worse).
- I AGREE with the evaluator - the GNN experiment confirms that simple GCNConv doesn't work.
- Key concerns raised: The CV-LB gap is structural, not just a matter of improving CV.
- The evaluator correctly identified that we need a fundamentally different approach.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop50_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. CV-LB relationship is highly linear (R²=0.98) with intercept 0.0533
  2. All model types (MLP, LGBM, GP, XGBoost, GNN) fall on the SAME LINE
  3. The intercept represents a "floor" that we cannot break through with current approaches
  4. The GNN benchmark (CV 0.0039) likely used sophisticated pre-training or architecture

## What's Working
1. GP + MLP + LGBM ensemble - Best CV (0.008194) and best LB (0.0877)
2. Spange + DRFP + ACS PCA features - Consistently outperform other feature sets
3. Arrhenius kinetics features (1/T, ln(t), interaction) - Physically meaningful
4. TTA for mixtures - Reduces variance

## What's NOT Working
1. Simple GNN (GCNConv) - CV 0.014080 (71.84% worse)
2. Pure GP - CV 0.0145 (77% worse)
3. Aggressive regularization - Did not reduce CV-LB gap
4. Stacking - CV 0.0100 (22% worse)

## Recommended Approaches (Priority Order)

### PRIORITY 1: Pre-trained Molecular Embeddings (ChemBERTa)
**Rationale**: The GNN benchmark achieved CV 0.0039. The key difference is likely pre-training on large molecular datasets. Pre-trained embeddings may generalize better to unseen solvents.

**ChemBERTa is AVAILABLE**: Verified that `seyonec/ChemBERTa-zinc-base-v1` can be loaded via transformers library.

**Specific actions**:
1. Use ChemBERTa to generate 768-dim embeddings for each solvent SMILES
2. Replace Spange/DRFP/ACS features with ChemBERTa embeddings
3. Add Arrhenius kinetics features (1/T, ln(t), interaction)
4. Train MLP on ChemBERTa embeddings + kinetics
5. The hypothesis is that pre-trained embeddings capture more generalizable molecular information

**Implementation notes**:
- Load SMILES from `/home/data/smiles_lookup.csv`
- Use ChemBERTa tokenizer and model to get embeddings
- Use [CLS] token embedding as the molecule representation
- For mixtures, use weighted average of embeddings (like current approach)

### PRIORITY 2: Domain Adaptation / Distribution Shift Handling
**Rationale**: The CV-LB gap suggests distribution shift between train/test solvents.

**Specific actions**:
1. Adversarial validation to identify which features cause the shift
2. Remove or down-weight features that distinguish train/test
3. Try domain-invariant feature learning

### PRIORITY 3: Uncertainty-Weighted Predictions
**Rationale**: GP provides uncertainty estimates. We could weight predictions by confidence.

**Specific actions**:
1. Use GP uncertainty to weight ensemble predictions
2. For high-uncertainty samples, fall back to simpler predictions

## What NOT to Try
- Simple GNN (GCNConv) - Already tried, 71.84% worse
- Pure GP - Already tried, 77% worse
- Aggressive regularization - Already tried, did not help
- Stacking - Already tried, 22% worse

## Validation Notes
- CV scheme: Leave-One-Solvent-Out for single solvent (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- CV-LB relationship: LB = 4.23*CV + 0.0533 (R²=0.98)
- To reach target 0.0347, we need to reduce the intercept from 0.0533 to ~0.03

## Strategic Recommendation
With only 3 submissions remaining:

1. **DO NOT SUBMIT** the GNN model (exp_051) - it's 71.84% worse
2. **TRY** ChemBERTa embeddings first - this has the highest potential to break the CV-LB relationship
3. **IF** ChemBERTa doesn't work, try domain adaptation techniques
4. **SAVE** at least 1 submission for the final model

The target IS reachable. The GNN benchmark achieved CV 0.0039. We need to find what makes that benchmark work - likely sophisticated pre-training.

## IMPORTANT: Competition Template Compliance
The submission must follow the competition template structure:
- Third-to-last cell: Single solvent CV with `model = YourModel(data='single')`
- Second-to-last cell: Full data CV with `model = YourModel(data='full')`
- Last cell: Combine and save submission

The model must have:
- `train_model(X, Y)` method
- `predict(X)` method returning tensor of shape [N, 3]