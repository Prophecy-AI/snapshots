## Current Status
- Best CV score: 0.008194 from exp_030 (GP 0.15 + MLP 0.55 + LGBM 0.30)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347
- CV-LB relationship: LB = 4.23×CV + 0.0533 (R²=0.98)
- **CRITICAL**: Intercept (0.0533) > Target (0.0347)
- Required CV to hit target: -0.0044 (IMPOSSIBLE - negative)
- Remaining submissions: 3
- Latest experiment: TabNet FAILED (CV 0.036642 is 347% worse)

## Response to Evaluator

**AGREEMENT with Evaluator's Key Findings:**
1. TabNet performed extremely poorly (CV 0.036642 vs best 0.008194 - 347% worse)
2. Attention-based architectures don't work for this small dataset
3. The CV-LB gap is the REAL problem, not CV minimization
4. Architecture exploration is exhausted after 60 experiments

**Evaluator's recommendation I AGREE with:**
- DO NOT submit TabNet (CV 0.036642 is much worse than best)
- Focus on fundamentally different approaches that could change the CV-LB relationship
- The target IS reachable but requires a different approach

**Key insight from Loop 60 analysis:**
- CV-LB relationship: LB = 4.23×CV + 0.0533 (R²=0.98)
- Intercept (0.0533) > Target (0.0347) means CV minimization CANNOT reach target
- Best generalization: exp_000 (residual -2.1%)
- We need to CHANGE the CV-LB relationship, not just minimize CV

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop60_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. CV-LB relationship: LB = 4.23×CV + 0.0533 (R²=0.98)
  2. **CRITICAL**: Intercept > Target means CV minimization alone CANNOT reach target
  3. Best generalization: exp_000 (Spange only) with residual -2.1%
  4. Best LB: exp_030 (GP ensemble) with LB 0.0877
  5. 60 experiments have been run, all following the same CV-LB relationship

## CRITICAL STRATEGIC INSIGHT

The target (0.0347) is NOT reachable with the current approach because:
1. The CV-LB relationship has intercept 0.0533 > target 0.0347
2. Even with CV=0, predicted LB would be 0.0533
3. All 60 experiments follow this same relationship (R²=0.98)

**We need a fundamentally different approach that:**
1. Changes the CV-LB relationship (reduces intercept or slope)
2. OR uses a completely different loss function
3. OR applies post-hoc calibration to improve generalization

## Recommended Approaches (Priority Order)

### PRIORITY 1: Conformalized Quantile Regression (CQR)
**Rationale**: CQR combines conformal prediction with quantile regression to provide distribution-free, finite-sample valid prediction intervals that are adaptive to heteroscedasticity. This could fundamentally change the CV-LB relationship.
**Implementation**:
1. Train quantile regressors at α/2 and 1-α/2 levels using LightGBM with quantile loss
2. Use a calibration set to conformalize the predictions
3. The conformalized predictions have guaranteed coverage
4. Use the median of the conformalized interval as the point prediction
**Why this could work**:
- CQR is designed to correct for overfitting bias in quantile regression
- It provides calibrated predictions that generalize better to OOD data
- Different loss function (pinball loss) may have different CV-LB relationship

### PRIORITY 2: Engression (Distributional Regression for Extrapolation)
**Rationale**: Engression is a neural network-based distributional regression method that is specifically designed for extrapolation. It models the full conditional distribution and can constrain predictions outside the training support.
**Implementation**:
1. Use the engression package (available in Python)
2. Model the conditional distribution of yields given features
3. Use the mean or median of the fitted distribution as the prediction
**Why this could work**:
- Engression is specifically designed for extrapolation
- It can handle "pre-additive noise" models where noise is added before transformation
- Different approach to modeling may have different CV-LB relationship

### PRIORITY 3: Two-Stage Bias Correction
**Rationale**: Fit a second model on the residuals of the first model to correct for systematic bias.
**Implementation**:
1. Train the best model (GP+MLP+LGBM ensemble)
2. Compute residuals on a held-out calibration set
3. Fit a second model (e.g., quantile regression forest) on the residuals
4. Use the second model to adjust predictions
**Why this could work**:
- Can correct for systematic biases in the first model
- Has been shown to reduce bias in high-dimensional settings
- Simple post-hoc method that doesn't change the base model

### PRIORITY 4: Bayesian Optimization of Ensemble Weights with OOD Objective
**Rationale**: Instead of optimizing weights for CV, optimize for a proxy of OOD performance.
**Implementation**:
1. Use Bayesian optimization to find ensemble weights
2. Objective: minimize CV + λ * (variance of predictions across folds)
3. Higher variance penalty encourages more conservative predictions
**Why this could work**:
- Current weights are optimized for CV, not LB
- Adding variance penalty could improve generalization
- Simple modification to existing approach

## What NOT to Try (Exhausted Approaches)
- **TabNet**: exp_061 was 347% worse CV - CONFIRMED DEAD END
- **GNN (ANY variant)**: exp_051 (72% worse), exp_056 (266% worse) - CONFIRMED DEAD END
- **ChemBERTa embeddings**: exp_052 (137-309% worse)
- **Hyperparameter optimization**: exp_055 (54% worse)
- **Per-solvent-type models**: exp_054 (138% worse)
- **Per-target models**: exp_053 (21% worse)
- **Physical constraint normalization**: exp_059 (17% worse)
- **Multi-seed ensemble**: exp_057 (15% worse)
- **Deep residual networks**: exp_004 (failed)
- **Simple Ridge regression**: exp_049 (99% worse)
- **Simpler features (Spange only)**: exp_058 was 37.5% worse CV
- **GroupKFold CV**: exp_042 was 77% worse CV, 31% worse LB

## Validation Notes
- CV scheme: Leave-One-Solvent-Out (24 folds) + Leave-One-Ramp-Out (13 folds)
- CV-LB relationship: LB = 4.23×CV + 0.0533 (R²=0.98)
- **CRITICAL**: Intercept > Target, so CV minimization alone CANNOT reach target
- Need to focus on approaches that CHANGE the CV-LB relationship

## Submission Strategy (3 remaining)
1. **DO NOT submit unless there's a fundamentally different approach**
2. Save at least 1 submission for final attempt
3. Priority: Submit if we find an approach that could change the CV-LB relationship
4. The goal is to reduce the intercept, not just minimize CV

## Next Experiment Recommendation
**Implement Conformalized Quantile Regression (CQR)**

Focus on:
1. Use LightGBM with quantile loss (alpha=0.1 and alpha=0.9)
2. Train quantile regressors for lower and upper bounds
3. Use a calibration set to conformalize predictions
4. Use the median of the conformalized interval as point prediction

**Why CQR is the best next step:**
1. It's a fundamentally different approach (quantile regression + conformal calibration)
2. It's designed to correct for overfitting bias
3. It provides calibrated predictions that generalize better to OOD data
4. Different loss function may have different CV-LB relationship

**Alternative if CQR doesn't work:**
- Try Engression (distributional regression for extrapolation)
- Try two-stage bias correction

## IMPORTANT: Target IS Reachable
The target (0.0347) IS reachable because:
1. The GNN benchmark achieved MSE 0.0039 (much better than target)
2. Other competitors may have found approaches we haven't tried
3. The CV-LB relationship is based on OUR experiments, not the true relationship

We need to find an approach that BREAKS the current CV-LB relationship.
The current intercept (0.0533) is 53.6% above the target.
We need approaches that improve generalization, not just minimize CV.

CQR is the most promising approach because it's specifically designed to provide calibrated predictions that generalize better to OOD data.