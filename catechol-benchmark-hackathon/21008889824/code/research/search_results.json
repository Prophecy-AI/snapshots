{
  "query": "What is the state-of-the-art MSE score for catechol reaction yield prediction using graph neural networks?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most recent source (the\u202farXiv\u202fv2 version of \u201cThe Catechol Benchmark\u201d dated\u202f27\u202fNov\u202f2025) does not disclose the exact mean\u2011squared\u2011error achieved by the best graph\u2011neural\u2011network model, so a specific state\u2011of\u2011the\u2011art MSE value cannot be provided from the available results. ([arxiv.org](https://arxiv.org/html/2506.07619v2))",
      "url": ""
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n> jumper2021highly\n> ]\n, to fusion reactor control> [\n> degrave2022magnetic\n> ]\n, disease detection> [\n> zhou2023foundation\n> ]\n, battery design> [\n> folch2023combining\n> ]\n, and material discovery> [\n> raccuglia2016machine\n> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n> constable2007perspective\n> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n> karpov2019transformer\n> , > tetko2020state\n> ]\n, condition recommendations> [\n> gao2018using\n> ]\n, product predictions> [\n> coley2019graph\n> , > tu2022permutation\n> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n> wigh2024orderly\n> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n> schwaller2021prediction\n> , > griffiths2023gauche\n> , > rankovic2024bayesian\n> , > rankovic2025gollum\n> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v2#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We answer the call for more flow chemistry reaction data> [\n> deadman2025wanted\n> ]\n, further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify potential solutions.\n![Refer to caption](figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n> kearnes2021open\n> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n> lowe2012extraction\n> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n> wigh2024orderly\n> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> schwaller2021prediction\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive models for yield, and highlighting the importance of creating datasets with full (including potentially sub-optimal) reaction conditions.\nMore relevant to our work,> perera2018platform\nintroduced a dataset of 5760 Suzuki-Miyaura cross-coupling reactions,> ahneman2018predicting\nintroduced a dataset of 3956 Buchwald\u2013Hartwig aminations, and> prieto2022accelerating\ninvestigated screening additives for Ni-catalysed reactions, all for the purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes and Bayesian neural networks> [\n> gr...",
      "url": "https://arxiv.org/html/2506.07619v2"
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Graph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.</p>\n<section><h3>Supplementary Information</h3>\n<p>The online version contains supplementary material available at 10.1186/s13321-024-00818-z.</p></section><section><p><strong>Keywords:</strong> Chemical reaction yield prediction, Graph neural network, Pre-training, Deep learning</p></section></section><section><h2>Introduction</h2>\n<p>A chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner [<a href=\"#CR1\">1</a>\u2013<a href=\"#CR8\">8</a>].</p>\n<p>Recently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. [<a href=\"#CR6\">6</a>, <a href=\"#CR7\">7</a>] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. [<a href=\"#CR8\">8</a>] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs [<a href=\"#CR9\">9</a>, <a href=\"#CR10\">10</a>].</p>\n<p>Despite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.</p>\n<p>To alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches [<a href=\"#CR11\">11</a>, <a href=\"#CR12\">12</a>]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart [<a href=\"#CR13\">13</a>\u2013<a href=\"#CR18\">18</a>]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented [<a href=\"#CR19\">19</a>, <a href=\"#CR20\">20</a>]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them [<a href=\"#CR21\">21</a>\u2013<a href=\"#CR25\">25</a>]. Existing methods have attempted to define appropriate pre-text tasks in various ways to effectively learn molecular representations. The process of acquiring pseudo-labels can be costly and time-consuming depending on how the pre-text task is defined. Since both approaches have their own advantages and drawbacks, it is important to choose the most suitable pre-training method that best aligns with the objective of a specific downstream task that needs to be addressed.</p>\n<p>In this study, we propose a novel pre-training method, <strong>MolDescPred</strong>, to improve the performance in predicting chemical reaction yields. <strong>MolDescPred</strong> is based on the pre-text task approach to pre-train a GNN. Given a molecular database containing a substantial number of molecules, we calculate the molecular descriptors for the molecules and reduce their dimensionality by applying principal component analysis (PCA). Each molecule is then pseudo-labeled with a vector of its principal component scores. The GNN is then pre-trained to predict the pseudo-label of its input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then is fine-tuned with a training dataset composed of chemical reactions and their corresponding yields. Through experiments on benchmark datasets, we demonstrate the effectiveness of the proposed method compared to existing methods, especially when the training dataset is insufficient.</p></section><section><h2>Method</h2>\n<section><h3>Problem definition</h3>\n<p>For chemical reaction yield prediction, we aim to build an accurate prediction model <em>f</em> which takes a chemical reaction <span></span> as the input to predict the yield <em>y</em> by learning from the training dataset <span></span>. Given a query chemical reaction <span></span>, the prediction model <em>f</em> can be used to make a prediction for the yield <span></span> as:</p>\n<p>It should be noted that additional information, such as the operating conditions for chemical reactions, can be utilized as extra input for the model <em>f</em>. If we denote this additional information by <span></span>, the problem can be formulated as learning the model <em>f</em> from the dataset <span></span>. The input and output of the model <em>f</em> can be described as:</p>\n<p>The data representation used for the prediction model <em>f</em> is as follows. In a chemical reaction <span></span>, <span></span> and <span></span> denote the sets of reactants and products, respectively. The set <span></span> contains <em>m</em> reactant molecules represented as molecular graphs, where <em>m</em> can vary for each reaction. The set <span></span> contains a single molecular graph representing a product molecule. Each molecular graph <span></span> represents the topology of a molecule. Here, <span></span> and <span></span> are the sets of nodes and edges associated with heavy atoms and their chemical bonds within the molecule. Hydrogen atoms are implicitly handled as node features of their neighboring heavy atoms. ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10905905"
    },
    {
      "title": "Chemical Reaction Prediction - CatalyzeX",
      "text": "Get our free extension to see links to code for papers anywhere online!Free add-on: code for papers everywhere!Free add-on: See code for papers anywhere!\n\n[![Chrome logo](https://www.catalyzex.com/static/images/google-chrome.svg)Add to Chrome - It's Free](https://chrome.google.com/webstore/detail/%F0%9F%92%BB-catalyzex-link-all-aim/aikkeehnlfpamidigaffhfmgbkdeheil)\n\nSearch Icon![Alert button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffilter.cf288982.png&w=1080&q=75)\n\n# Topic: **Chemical Reaction Prediction**\n\n![Alert button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Falert_light_mode_icon.b8fca154.png&w=1080&q=75)\n\nWhat is Chemical Reaction Prediction? Chemical reaction prediction is the process of predicting the outcome of chemical reactions using machine learning models.\n\n### Papers and Code\n\n## [**Interpretable Deep Learning for Polar Mechanistic Reaction Prediction**](https://www.catalyzex.com/paper/interpretable-deep-learning-for-polar)\n\n[Github IconRequest Code](https://www.catalyzex.com/s/Chemical%20Reaction%20Prediction) Code for Similar Papers:![Code for Similar Papers](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frelated_icon_transparent.98f57b13.png&w=96&q=75) [![Add code](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Faddcode_white.6afb879f.png&w=96&q=75)](https://www.catalyzex.com/add_code?title=Interpretable Deep Learning for Polar Mechanistic Reaction Prediction&paper_url=http://arxiv.org/abs/2504.15539)\n\n![Bookmark button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fbookmark_outline.3a3e1c2c.png&w=828&q=75)\n\n![Alert button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Falert_light_mode_icon.b8fca154.png&w=1080&q=75)\n\nApr 22, 2025\n\nAuthors:[Ryan J. Miller](https://www.catalyzex.com/author/Ryan%20J.%20Miller), [Alexander E. Dashuta](https://www.catalyzex.com/author/Alexander%20E.%20Dashuta), [Brayden Rudisill](https://www.catalyzex.com/author/Brayden%20Rudisill), [David Van Vranken](https://www.catalyzex.com/author/David%20Van%20Vranken), [Pierre Baldi](https://www.catalyzex.com/author/Pierre%20Baldi)\n\nAbstract:Accurately predicting chemical reactions is essential for driving innovation in synthetic chemistry, with broad applications in medicine, manufacturing, and agriculture. At the same time, reaction prediction is a complex problem which can be both time-consuming and resource-intensive for chemists to solve. Deep learning methods offer an appealing solution by enabling high-throughput reaction prediction. However, many existing models are trained on the US Patent Office dataset and treat reactions as overall transformations: mapping reactants directly to products with limited interpretability or mechanistic insight. To address this, we introduce PMechRP (Polar Mechanistic Reaction Predictor), a system that trains machine learning models on the PMechDB dataset, which represents reactions as polar elementary steps that capture electron flow and mechanistic detail. To further expand model coverage and improve generalization, we augment PMechDB with a diverse set of combinatorially generated reactions. We train and compare a range of machine learning models, including transformer-based, graph-based, and two-step siamese architectures. Our best-performing approach was a hybrid model, which combines a 5-ensemble of Chemformer models with a two-step Siamese framework to leverage the accuracy of transformer architectures, while filtering away \"alchemical\" products using the two-step network predictions. For evaluation, we use a test split of the PMechDB dataset and additionally curate a human benchmark dataset consisting of complete mechanistic pathways extracted from an organic chemistry textbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB test set and a target recovery rate of 84.9% on the pathway dataset.\n\nVia![arxiv icon](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farxiv.41e50dc5.png&w=128&q=75)\n\nGithub Icon [Access Paper or Ask Questions](https://www.catalyzex.com/paper/interpretable-deep-learning-for-polar)\n\n## [**Transferable Learning of Reaction Pathways from Geometric Priors**](https://www.catalyzex.com/paper/transferable-learning-of-reaction-pathways)\n\n[Github IconView Code](https://www.catalyzex.com/s/Chemical%20Reaction%20Prediction) Play IconNotebookCode for Similar Papers:![Code for Similar Papers](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frelated_icon_transparent.98f57b13.png&w=96&q=75) [![Add code](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Faddcode_white.6afb879f.png&w=96&q=75)](https://www.catalyzex.com/add_code?title=Transferable Learning of Reaction Pathways from Geometric Priors&paper_url=http://arxiv.org/abs/2504.15370)\n\n![Bookmark button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fbookmark_outline.3a3e1c2c.png&w=828&q=75)\n\n![Alert button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Falert_light_mode_icon.b8fca154.png&w=1080&q=75)\n\nApr 21, 2025\n\nAuthors:[Juno Nam](https://www.catalyzex.com/author/Juno%20Nam), [Miguel Steiner](https://www.catalyzex.com/author/Miguel%20Steiner), [Max Misterka](https://www.catalyzex.com/author/Max%20Misterka), [Soojung Yang](https://www.catalyzex.com/author/Soojung%20Yang), [Avni Singhal](https://www.catalyzex.com/author/Avni%20Singhal), [Rafael G\u00f3mez-Bombarelli](https://www.catalyzex.com/author/Rafael%20G%C3%B3mez-Bombarelli)\n\nAbstract:Identifying minimum-energy paths (MEPs) is crucial for understanding chemical reaction mechanisms but remains computationally demanding. We introduce MEPIN, a scalable machine-learning method for efficiently predicting MEPs from reactant and product configurations, without relying on transition-state geometries or pre-optimized reaction paths during training. The task is defined as predicting deviations from geometric interpolations along reaction coordinates. We address this task with a continuous reaction path model based on a symmetry-broken equivariant neural network that generates a flexible number of intermediate structures. The model is trained using an energy-based objective, with efficiency enhanced by incorporating geometric priors from geodesic interpolation as initial interpolations or pre-training objectives. Our approach generalizes across diverse chemical reactions and achieves accurate alignment with reference intrinsic reaction coordinates, as demonstrated on various small molecule reactions and \\[3+2\\] cycloadditions. Our method enables the exploration of large chemical reaction spaces with efficient, data-driven predictions of reaction pathways.\n\n_\\\\* 14 pages, 6 figures; Supporting Information in ancillary files_\n\nVia![arxiv icon](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farxiv.41e50dc5.png&w=128&q=75)\n\nGithub Icon [Access Paper or Ask Questions](https://www.catalyzex.com/paper/transferable-learning-of-reaction-pathways)\n\n## [**Predicting Chemical Reaction Outcomes Based on Electron Movements Using Machine Learning**](https://www.catalyzex.com/paper/predicting-chemical-reaction-outcomes-based)\n\n[Github IconRequest Code](https://www.catalyzex.com/s/Chemical%20Reaction%20Prediction) Code for Similar Papers:![Code for Similar Papers](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frelated_icon_transparent.98f57b13.png&w=96&q=75) [![Add code](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Faddcode_white.6afb879f.png&w=96&q=75)](https://www.catalyzex.com/add_code?title=Predicting Chemical Reaction Outcomes Based on Electron Movements Using Machine Learning&paper_url=http://arxiv.org/abs/2503.10197)\n\n![Bookmark button](https://www.catalyzex.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fbookmark_outline.3a3e1c2c.png&w=828&q=75)\n\n![Alert button](https://www.catalyz...",
      "url": "https://www.catalyzex.com/s/Chemical%20Reaction%20Prediction"
    },
    {
      "title": "Learning Condensed Graph via Differentiable Atom Mapping for ...",
      "text": "<div><div>\n<p><span>Abstract:</span></p><p>Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets. In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges. Here, we first approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR), which is a supergraph encompassing both the reactants and products. This CGR serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder.Our experiments show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping.</p>\n</div></div>",
      "url": "https://icml.cc/virtual/2025/poster/43812"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph neural networks",
      "text": "Han\u00a0et\u00a0al. Journal of Cheminformatics (2024) 16:25 \nhttps://doi.org/10.1186/s13321-024-00818-z\nRESEARCH Open Access\n\u00a9 The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativeco\nmmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\nImproving chemical reaction yield prediction \nusing pre-trained graph neural networks\nJongmin Han1, Youngchun Kwon2, Youn\u2011Suk Choi2* and Seokho Kang1*\nAbstract\nGraph neural networks (GNNs) have proven to be efective in the prediction of chemical reaction yields. However, \ntheir performance tends to deteriorate when they are trained using an insufcient training dataset in terms \nof quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecula\ndatabase. In this study, we investigate the efectiveness of GNN pre-training in chemical reaction yield prediction.\nWe present a novel GNN pre-training method for performance improvement.Given a molecular database consisting\nof a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality \nof these descriptors by applying principal component analysis. We defne a pre-text task by assigning a vector\nof principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-traine\nto perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yiel\nprediction, a prediction model is initialized using the pre-trained GNN and then fne-tuned with the trainin\ndataset containing chemical reactions and their yields. We demonstrate the efectiveness of the proposed method \nthrough experimental evaluation on benchmark datasets.\nKeywords Chemical reaction yield prediction, Graph neural network, Pre-training, Deep learnin\nIntroduction\nA chemical reaction is a process in which reactants are \nchanged into products through chemical transforma\u0002tions. Te percentage of products obtained relative to \nthe reactants consumed is referred to as the chemical \nreaction yield. Te prediction of the chemical reaction \nyields provides clues for exploring high-yield chemical \nreactions without the need for conducting direct experi\u0002ments. Tis is crucial for accelerating synthesis planning \nin organic chemistry by signifcantly reducing time and \ncost. Machine learning has been actively utilized for the \nfast and accurate prediction of chemical reaction yields \nin a data-driven manner [1\u20138].\nRecently, deep learning has shown remarkable per\u0002formance in predicting chemical reaction yields by \nefectively modeling the intricate relationships between \nchemical reactions and their yields using neural net\u0002works. Schwaller et\u00a0 al. [6, 7] represented a chemical \nreaction as a series of simplifed molecular-input line\u0002entry system (SMILES) strings and built a bidirectional \nencoder representations from transformers (BERT) \nas the prediction model. Kwon et\u00a0 al. [8] represented a \nchemical reaction as a set of molecular graphs and built \na graph neural network (GNN) that operates directly on \nthe molecular graphs as the prediction model. Te use of \nGNNs led to a signifcant improvement in the predictive \nperformance owing to their high expressive power on \nmolecular graphs [9, 10].\nDespite its efectiveness, the predictive performance \nof a GNN can sufer when it is trained on an insufcient \n*Correspondence:\nYoun\u2011Suk Choi\nysuk.choi@samsung.com\nSeokho Kang\ns.kang@skku.edu\n1\n Department of Industrial Engineering, Sungkyunkwan University, 2066 \nSeobu\u2011ro, Jangan\u2011gu, Suwon, Republic of Korea 2\n Samsung Advanced Institute of Technology, Samsung Electronics Co. \nLtd., 130 Samsung\u2011ro, Yeongtong\u2011gu, Suwon, Republic of Korea\nHan\u00a0et\u00a0al. Journal of Cheminformatics (2024) 16:25 Page 2 of 15\ntraining dataset in terms of quantity or diversity. For \nexample, a GNN may not generalize well to query reac\u0002tions involving substances that are not considered in \nthe training dataset. Although the performance can be \nsignifcantly improved by securing a large-scale train\u0002ing dataset, this is difcult in practice because of the \nhigh cost associated with conducting direct experi\u0002ments to acquire the yields for a large number of chem\u0002ical reactions.\nTo alleviate this issue, a promising solution is to pre\u0002train a GNN on a large-scale molecular database and \nuse it to adapt to chemical reaction yield prediction. \nVarious pre-training methods have been studied in \nthe literature, which can be categorized into contras\u0002tive learning and pre-text task approaches [11, 12]. Te \ncontrastive learning approach pre-trains a GNN by \nlearning molecular representations such that diferent \nviews of the same molecule are mapped close together, \nand views of diferent molecules are mapped far apart \n[13\u201318]. Most existing methods based on this approach \nhave utilized data augmentation techniques to generate \ndiferent views of each molecule. Data augmentation \nmay potentially alter the properties of the molecules \nbeing represented [19, 20]. Te pre-text task approach \nacquires the pseudo-labels of molecules and pre-trains \na GNN to predict them [21\u201325]. Existing methods have \nattempted to defne appropriate pre-text tasks in vari\u0002ous ways to efectively learn molecular representations. \nTe process of acquiring pseudo-labels can be costly \nand time-consuming depending on how the pre-text \ntask is defned. Since both approaches have their own \nadvantages and drawbacks, it is important to choose \nthe most suitable pre-training method that best aligns \nwith the objective of a specifc downstream task that \nneeds to be addressed.\nIn this study, we propose a novel pre-training method, \nMolDescPred, to improve the performance in predict\u0002ing chemical reaction yields. MolDescPred is based on \nthe pre-text task approach to pre-train a GNN. Given a \nmolecular database containing a substantial number of \nmolecules, we calculate the molecular descriptors for the \nmolecules and reduce their dimensionality by applying \nprincipal component analysis (PCA). Each molecule is \nthen pseudo-labeled with a vector of its principal compo\u0002nent scores. Te GNN is then pre-trained to predict the \npseudo-label of its input molecule. For chemical reaction \nyield prediction, a prediction model is initialized using \nthe pre-trained GNN and then is fne-tuned with a train\u0002ing dataset composed of chemical reactions and their \ncorresponding yields. Trough experiments on bench\u0002mark datasets, we demonstrate the efectiveness of the \nproposed method compared to existing methods, espe\u0002cially when the training dataset is insufcient.\nMethod\nProblem defnition\nFor chemical reaction yield prediction, we aim to build an \naccurate prediction model f which takes a chemical reac\u0002tion (R,P) as the input to predict the yield y by learning \nfrom the training dataset D = {(Ri,Pi, yi)}\nN\ni=1. Given a \nquery chemical reaction (R\u2217,P\u2217), the prediction model f\ncan be used to make a prediction for the yield y\u2217 as:\nIt should be noted that additional information, such as \nthe operating condition...",
      "url": "https://link.springer.com/content/pdf/10.1186/s13321-024-00818-z.pdf"
    },
    {
      "title": "GitHub - hjm9702/reaction_yield_pretrained_gnn: Source code for \"Improving Chemical Reaction Yield Prediction Using Pre-Trained Graph Neural Networks\"",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[hjm9702](https://github.com/hjm9702)/ **[reaction\\_yield\\_pretrained\\_gnn](https://github.com/hjm9702/reaction_yield_pretrained_gnn)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fhjm9702%2Freaction_yield_pretrained_gnn) You must be signed in to change notification settings\n- [Fork\\\n6](https://github.com/login?return_to=%2Fhjm9702%2Freaction_yield_pretrained_gnn)\n- [Star\\\n16](https://github.com/login?return_to=%2Fhjm9702%2Freaction_yield_pretrained_gnn)\n\n\nSource code for \"Improving Chemical Reaction Yield Prediction Using Pre-Trained Graph Neural Networks\"\n\n[16\\\nstars](https://github.com/hjm9702/reaction_yield_pretrained_gnn/stargazers) [6\\\nforks](https://github.com/hjm9702/reaction_yield_pretrained_gnn/forks) [Branches](https://github.com/hjm9702/reaction_yield_pretrained_gnn/branches) [Tags](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tags) [Activity](https://github.com/hjm9702/reaction_yield_pretrained_gnn/activity)\n\n[Star](https://github.com/login?return_to=%2Fhjm9702%2Freaction_yield_pretrained_gnn)\n\n[Notifications](https://github.com/login?return_to=%2Fhjm9702%2Freaction_yield_pretrained_gnn) You must be signed in to change notification settings\n\n# hjm9702/reaction\\_yield\\_pretrained\\_gnn\n\nmain\n\n[Branches](https://github.com/hjm9702/reaction_yield_pretrained_gnn/branches) [Tags](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[22 Commits](https://github.com/hjm9702/reaction_yield_pretrained_gnn/commits/main/) |\n| [data](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/data) | [data](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/data) |\n| [model](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/model) | [model](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/model) |\n| [src](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/src) | [src](https://github.com/hjm9702/reaction_yield_pretrained_gnn/tree/main/src) |\n| [README.md](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/README.md) | [README.md](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/README.md) |\n| [main\\_finetune.py](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/main_finetune.py) | [main\\_finetune.py](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/main_finetune.py) |\n| [main\\_pretrain.py](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/main_pretrain.py) | [main\\_pretrain.py](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/main_pretrain.py) |\n| [run.sh](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/run.sh) | [run.sh](https://github.com/hjm9702/reaction_yield_pretrained_gnn/blob/main/run.sh) |\n| View all files |\n\n## Repository files navigation\n\n# reaction\\_yield\\_pretrained\\_gnn\n\nSource code for the paper: Improving Chemical Reaction Yield Prediction Using Pre-Trained Graph Neural Networks\n\n## Data\n\n- The datasets used in the paper\n  - Pre-Training Dataset (10M mols collected from Pubchem) - [https://arxiv.org/pdf/2010.09885.pdf](https://arxiv.org/pdf/2010.09885.pdf)\n  - Chemical Reaction Yield Benchmark Datasets - [https://github.com/rxn4chemistry/rxn\\_yields/](https://github.com/rxn4chemistry/rxn_yields/)\n\n## Components\n\n- **data/get\\_pretraining\\_data.py** \\- pre-training dataset preprocessing functions\n- **data/get\\_reaction\\_yield\\_data.py** \\- chemical reaction yield benchmark dataset preprocessing functions\n- **src/dataset.py** \\- data structure & functions\n- **src/model.py** \\- model architecture & training / inference functions\n- **src/pretrain.py** \\- pre-training functions\n- **src/finetune.py** \\- fine-tuning functions\n- **src/preprocess\\_util.py** \\- util functions for data preprocessing\n- **src/util.py** \\- util functions for model training / inference\n- **main\\_pretrain.py** \\- script for pre-training\n- **main\\_finetune.py** \\- script for fine-tuning\n- **run.sh** \\- run code example\n\n## Dependencies\n\n- **Python**\n- **Pytorch**\n- **DGL**\n- **RDKit**\n- **Mordred**\n\n## Citation\n\n```\n@Article{reaction_yield_pretrained_gnn,\n  title={Improving chemical reaction yield prediction using pre-trained graph neural networks},\n  author={Han, Jongmin and Kwon, Youngchun and Choi, Youn-Suk and Kang, Seokho},\n  journal={Journal of Cheminformatics},\n  volume={16},\n  number={25}\n  year={2024},\n  doi={10.1186/s13321-024-00818-z}\n}\n\n```\n\n## About\n\nSource code for \"Improving Chemical Reaction Yield Prediction Using Pre-Trained Graph Neural Networks\"\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/hjm9702/reaction_yield_pretrained_gnn/activity)\n\n### Stars\n\n[**16**\\\nstars](https://github.com/hjm9702/reaction_yield_pretrained_gnn/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/hjm9702/reaction_yield_pretrained_gnn/watchers)\n\n### Forks\n\n[**6**\\\nforks](https://github.com/hjm9702/reaction_yield_pretrained_gnn/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fhjm9702%2Freaction_yield_pretrained_gnn&report=hjm9702+%28user%29)\n\n## [Releases](https://github.com/hjm9702/reaction_yield_pretrained_gnn/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/hjm9702/packages?repo_name=reaction_yield_pretrained_gnn)\n\nNo packages published\n\n## Languages\n\n- [Python99.4%](https://github.com/hjm9702/reaction_yield_pretrained_gnn/search?l=python)\n- [Shell0.6%](https://github.com/hjm9702/reaction_yield_pretrained_gnn/search?l=shell)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/hjm9702/reaction_yield_pretrained_gnn"
    },
    {
      "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
      "text": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features\nJiayuan Chen1, Kehan Guo2, Zhen Liu3, Olexandr Isayev3, Xiangliang Zhang2*\n1The Ohio State University\n2 Department of Computer Science and Engineering, University of Notre Dame\n3Department of Chemistry, Carnegie Mellon University\nchen.12930@osu.edu, kguo2@nd.edu, liu5@andrew.cmu.edu, olexandr@olexandrisayev.com, xzhang33@nd.edu\nAbstract\nPredicting chemical reaction yields is pivotal for effcient\nchemical synthesis, an area that focuses on the creation of\nnovel compounds for diverse uses. Yield prediction demands\naccurate representations of reactions for forecasting practical\ntransformation rates. Yet, the uncertainty issues broadcasting\nin real-world situations prohibit current models to excel in\nthis task owing to the high sensitivity of yield activities and\nthe uncertainty in yield measurements. Existing models often\nutilize single-modal feature representations, such as molec\u0002ular fngerprints, SMILES sequences, or molecular graphs,\nwhich is not suffcient to capture the complex interactions\nand dynamic behavior of molecules in reactions. In this pa\u0002per, we present an advanced Uncertainty-Aware Multimodal\nmodel (UAM) to tackle these challenges. Our approach seam\u0002lessly integrates data sources from multiple modalities by\nencompassing sequence representations, molecular graphs,\nand expert-defned chemical reaction features for a com\u0002prehensive representation of reactions. Additionally, we ad\u0002dress both the model and data-based uncertainty, refning the\nmodel\u2019s predictive capability. Extensive experiments on three\ndatasets, including two high throughput experiment (HTE)\ndatasets and one chemist-constructed Amide coupling reac\u0002tion dataset, demonstrate that UAM outperforms the state\u0002of-the-art methods. The code and used datasets are available\nat https://github.com/jychen229/Multimodal-reaction-yield\u0002prediction.\nIntroduction\nComputer-Assisted Synthesis Prediction (CASP) has\nemerged as a key area of focus in the intersection of artif\u0002cial intelligence in scientifc domains. The goal of CASP\nrevolves around tackling a diverse array of chemical chal\u0002lenges, including the prediction of reaction products (Coley\net al. 2017) and the intricacies of retro-synthesis (Ishida\net al. 2019). Yield prediction, among the spectrum of CASP\ntasks, is particularly crucial. The target of yield prediction\nis to accurately estimate the practical conversion rates in\nchemical reactions, illustrating the transition from reactants\nto products. In this context, yield prediction lays the foun\u0002dation for reaction-related predictions, thereby supporting\nthe advancements in CASP (Ahneman et al. 2018).\n*The corresponding author.\nCopyright \u00a9 2024, Association for the Advancement of Artifcial\nIntelligence (www.aaai.org). All rights reserved.\nWhen conceptualized as a machine learning problem,\nyield prediction is essentially a regression task. The devel\u0002opment of an effective yield prediction model depends crit\u0002ically on obtaining high-quality representations of the re\u0002actants and products involved in chemical reactions. Early,\nmolecular fngerprints were employed to depict chemical\nstructures, yet their effcacy in handling complex structures\nwas limited. Deep learning-based methods can automati\u0002cally learn intricate patterns and features from data. For\ninstance, (Schwaller et al. 2020) employ BERT (Devlin\net al. 2018), a bidirectional transformer language model,\nfor learning the representation of molecules involved in\nchemical reactions based on their sequential SMILES ex\u0002pressions. This learned representation is then utilized in a\nsubsequent regression model to predict yields. Similarly,\n(Kwon et al. 2022) employ molecular graphs to represent\nmolecules within chemical reactions and utilize graph neural\nnetworks to learn useful features for yield prediction. These\ncurrent yield prediction models exhibit strong performance\non specially curated reaction datasets, such as the High\u0002Throughput (HTE) datasets (Ahneman et al. 2018; Perera\net al. 2018). However, when applied to real-world tasks, their\neffcacy diminishes signifcantly (Saebi et al. 2023). One pri\u0002mary reason for this decline is the pervasive issue of uncer\u0002tainty in real-world yield prediction datasets, manifesting in\ntwo major aspects.\nHigh sensitivity of yield. In chemical reactions, structural\nisomers\u2014compounds with identical molecular formulas but\ndifferent arrangements of atoms\u2014can signifcantly impact\nthe yield. Even minor structural variations within the reac\u0002tants themselves can lead to pronounced discrepancies in\nthe resulting yields. For example, the addition of a methoxy\ngroup that is far from the reaction center can lower the reac\u0002tion center by as much as 55% (Schierle et al. 2020). This\nhighlights how real-world reactions can be extremely sen\u0002sitive to slight variations in the reactants and products in\u0002volved. Existing models, as referenced by (Schwaller et al.\n2021), primarily utilize single-modal data such as graphs or\nsequences, and thus may not adequately capture the subtle\nstructural variations in molecules. These subtle yet critical\nvariations include minor differences in stereochemistry and\nthe presence of specifc functional groups, both of which can\nhave a signifcant impact on reaction pathways and yields.\nUncertainty in the yield measurement. The yield from\nThe Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n8274\nthe reaction process depends on many factors in the reaction\ncycle, including the properties of the molecules, the envi\u0002ronmental condition, and human operations. As a result, the\nsame reaction can exhibit signifcant yield variations. For ex\u0002ample, (Liu, Moroz, and Isayev 2023) pointed out that the\nyield standardized deviation can be as large as 23.7% when\nthe same reaction was reported by different research groups.\nAlthough (Kwon et al. 2022) considered yield prediction un\u0002certainty and introduced an uncertainty-related loss for train\u0002ing the prediction model, the inherent intricacies of data un\u0002certainty hinder a precise prediction.\nTo address the aforementioned challenges, we propose\nan advanced Uncertainty-Aware Multimodal model (UAM)\nfor yield prediction by taking into account multi-modal fea\u0002tures to combat the prediction uncertainty. Specifcally, we\nintroduce a multi-modal feature extractor that integrates\nsequence features, graph structural features, and human\u0002defned reaction condition features to acquire a more com\u0002prehensive representation of reactants and products. More\u0002over, aided by cross-modal contrastive learning, we facil\u0002itate modal fusion to capture the shared information and\ndistinctive features across modalities to alleviate discrep\u0002ancies induced by the high sensitivity of yield. Addition\u0002ally, we incorporate a Mixture-of-Experts (MoE) module to\nenhance model expressiveness without additional computa\u0002tional costs. This facilitates a dynamic equilibrium between\nthe model\u2019s sensitivity to variations and its ability to discern\nreaction types. Last, we introduce an uncertainty quantif\u0002cation module, which mitigates the inherent training uncer\u0002tainty of the model while focusing on quantifying the uncer\u0002tainty presented in the data itself, thereby enhancing predic\u0002tive accuracy.\nOur contributions in this work are summarized as follows:\n\u2022 We study the reaction yield prediction problem and pro\u0002posed a novel model called UAM to tackle the uncer\u0002tainty issue by fusing multi-modal molecular features;\n\u2022 We explore an innovative and effective way to utilize\ncross-modal contrastive learning and an additional MoE\nmodule is added to enhance the reaction representation;\n\u2022 Experimental results on three real-world datasets demon\u0002strate the effectiveness of UAM in comparison to the\nstate-of-the-art approaches.\nRelated Work\nMolecular Representation Learning\nMolecular representation learning is a crucial link between\nmachine learning and chemistry and is gaining rising aware\u0002ness in computational chemistry. Early tech...",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28668/29297"
    }
  ]
}