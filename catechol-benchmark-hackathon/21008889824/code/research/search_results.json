{
  "query": "How can I use isotonic regression or Platt scaling to calibrate regression predictions and improve out-of-distribution generalization?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Isotonic regression and Platt scaling are post\u2011hoc calibrators that can be applied to the raw scores of any regression model to turn them into well\u2011behaved predictive distributions, which in turn makes the model\u2019s uncertainty estimates more reliable when it encounters data that differ from the training set.\u202fThe idea is to treat the regression output (or a derived quantity such as the absolute residual) as a \u201cscore\u201d\u202fs and then learn a monotonic mapping\u202ff(s)\u202fthat aligns the predicted cumulative distribution function (CDF) with the empirical CDF observed on a held\u2011out calibration set\u202f([Song\u202fet\u202fal., 2019](https://proceedings.mlr.press/v97/song19a/song19a.pdf)).\u202fIn practice, you first fit your base regressor, collect its point predictions\u202f\u0177\u202fand the corresponding true targets\u202fy\u202fon a validation split, compute the empirical quantiles\u202fq\u202f=\u202frank(y)/N, and then fit an isotonic regression model\u202ff\u202fthat minimizes a proper scoring loss (e.g., the Brier or log\u2011loss) under the monotonicity constraint\u202ff(\u0177\u2081)\u202f\u2264\u202ff(\u0177\u2082)\u202fwhenever\u202f\u0177\u2081\u202f\u2264\u202f\u0177\u2082.\u202fBecause isotonic regression makes no parametric assumptions, it can adapt to arbitrary score\u2011to\u2011probability shapes and often yields near\u2011perfect calibration even on out\u2011of\u2011distribution (OOD) samples, as shown by the modular conformal calibration framework that recovers isotonic recalibration with finite\u2011sample guarantees\u202f([Marx\u202fet\u202fal., 2022](https://proceedings.mlr.press/v162/marx22a.html)).\n\nPlatt scaling offers a parametric alternative: you fit a logistic (sigmoid) function\u202f\u03c3(A\u00b7s\u202f+\u202fB)\u202fto the same calibration data, learning\u202fA\u202fand\u202fB\u202fby minimizing log\u2011loss.\u202fSince the sigmoid has only two degrees of freedom, it requires far fewer calibration points and is less prone to over\u2011fitting when the calibration set is small, but it assumes that the score distribution roughly follows a symmetric, Gaussian\u2011like shape\u202f([Abzu, 2022](https://www.abzu.ai/data-science/calibration-introduction-part-2)).\u202fTo use Platt scaling for regression, treat the residual magnitude\u202f|y\u202f\u2212\u202f\u0177|\u202fas a binary \u201clarge\u2011error\u201d indicator (e.g., above a chosen quantile) and calibrate the probability of a large error; the resulting calibrated probability can be interpreted as a confidence measure that generalizes better to OOD inputs because the sigmoid smooths extreme scores.\n\nIn both cases the workflow is identical: (1) split your data into training, validation (for calibration), and test; (2) train the regression model on the training split; (3) on the validation split compute scores and empirical targets (quantiles or error indicators); (4) fit either isotonic regression (non\u2011parametric, flexible) or Platt scaling (parametric, data\u2011efficient); (5) apply the learned mapping to future predictions.\u202fBecause the calibrator is learned on data that may include distribution shifts, the calibrated predictions tend to retain their reliability on OOD samples, improving downstream decisions that depend on well\u2011quantified uncertainty.",
      "url": ""
    },
    {
      "title": "An introduction to calibration (part II): Platt scaling, isotonic regression, and beta calibration.",
      "text": "[Skip to content](https://www.abzu.ai/www.abzu.ai#content)\n\n- [Data science](https://www.abzu.ai/category/data-science/)\n- [QLattice](https://www.abzu.ai/tag/qlattice/)\n\n# An introduction to calibration (part II): Platt scaling, isotonic regression, and beta calibration.\n\n## Calibrators are tools used to transform the scores generated by your models into (almost) real mathematical probabilities.\n\nIn the [first part of this blog series](https://www.abzu.ai/data-science/calibration-introduction-part-1/) we learned the basics of how to evaluate the calibration of a model, but we didn\u2019t address what you can do to improve it in this regard.\n\nThe goal of this blog post is to introduce you to _calibrators_, which are tools that you can use to transform the scores generated by your models into something that is as close as possible to real mathematical probabilities.\n\n## Platt scaling.\n\nOne of the most popular calibrators is Platt scaling (or _logistic regression_). It was originally proposed by John C. Platt in 2000 [1](https://www.abzu.ai/www.abzu.ai#reference), and it consists in fitting the scores _s\\_i_ given by the uncalibrated model for the positive class to the sigmoid\n\nwhere the parameters A > 0 and B are obtained by minimising the log-loss for a given training set.\n\nSome of the calibration maps that can be obtained through Platt scaling.\n\nBeing a parametric model (which makes assumptions about the score distributions), Platt scaling has the advantage that it does not require a lot of data to be trained. But in some cases this has significant drawbacks.\n\nAs discussed in _Beyond sigmoids_ [2](https://www.abzu.ai/www.abzu.ai#reference), this method can lead to quite good calibrations as long as the ratio of the score distributions for both binary classes is similar to the ratio of two Gaussian distributions with equal variance. Unfortunately, if this is not the case, Platt scaling can actually lead to even worse calibrated scores, as the identity function is not present in the family of functions that can be obtained from (1), which means it would leave the scores untouched if there is no better calibration.\n\n## Isotonic regression.\n\n_Isotonic regression_ [3](https://www.abzu.ai/www.abzu.ai#reference) is a calibration method that provides a discrete, step-wise, monotonically increasing calibration map consisting of a set of points arranged in a staircase-like way. The calibration map for isotonic regression is obtained by minimising\n\nsubject to _s-bar\\_i_ \u2265 _s-bar\\_j_ whenever _s-bar\\_i_ \u2265 _s-bar\\_j_, where _s_ and _s-bar_ refer to the scores given by the uncalibrated model and calibrator, respectively.\n\nExamples of isotonic calibration maps (linear interpolations added to obtain a continuous calibration map).\n\nThanks to not assuming anything about the score distribution, a calibration of this kind should be able to correct any monotonic distortion coming from an uncalibrated model. However, it also has two major disadvantages: it cannot provide good calibrations when non-monotonic corrections are needed (this is shared with Platt scaling), and it usually requires a vast amount of data compared to parametrics methods as it is prone to overfitting.\n\n## Beta calibration.\n\nAnother interesting calibration method that has been proposed by Meelis Kull, Telmo M. Silva Filho, and Peter Flach in recent years is _beta calibration_ [2](https://www.abzu.ai/www.abzu.ai#reference).\n\nAs the authors argue in the paper, this kind of calibration aims to solve some of the problems that the previously seen calibrators have. First of all, beta calibration is a parametric method, so having a small data set is not as problematic as for isotonic regression. Second, in this case, instead of assuming that the ratio of the score distributions for both binary classes has to be similar to the ratio of two Gaussian distributions with equal variance, it assumes that they are similar to the ratio of two beta distributions.\n\nThe main advantage with respect to logistic regression is that this allows to obtain a broader family of calibration maps where sigmoids are included, but also inverse sigmoids and the identity function. Thus, beta calibration not only provides a wider set of maps, but it also guarantees up to some point that it will not worsen the calibration of the original model if it is already calibrated or if no optimal calibration maps are found\n\nwhere A, B (both \u2265 0) and C are again obtained by minimising log-loss.\n\nSome of the calibration maps that can be obtained through beta calibration: inverse sigmoid (dark blue), quasi-identity (purple) and sigmoid (light blue).\n\nIn principle, beta calibration is a very nice alternative for model calibration, but it still has some limitations. The most obvious is that although more flexible, it is still a parametric method, so it will not be able to correct all kinds of distortions. Moreover, as the other two methods, it has been designed to provide monotonic calibration maps. Although this is usually a good idea to avoid overfitting to the training set, if the original model is ill-calibrated with non-monotonic distortions, beta calibration will not be capable of obtaining well-calibrated scores.\n\n## In the next episode\u2026\n\nAll of this will hopefully have been interesting to you, but we have not put any of this knowledge to practice. Lucky for you, in our [next (and last) blog post about calibration](https://www.abzu.ai/data-science/calibration-introduction-part-3/), we will show you some examples of the outcomes that you might find when evaluating the calibration of the models coming from [the QLattice](https://www.abzu.ai/qlattice/). Stay tuned!\n\n### References:\n\n1\\. Platt, J. (2000). Probabilities for SV machines. In Advances in Large Margin Classifiers (A. Smola, P. Bartlett, B. Sch\u00f6lkopf and D. Schuurmans, eds.) 61\u201374. MIT Press.\n\n2\\. Kull, M., Silva Filho, T., Flach, P. (2017). Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. Electron. J. Statist. 11, no. 2, 5052\u20135080. [doi: 10.1214/17-EJS1338SI](https://doi.org/10.1214/17-EJS1338SI).\n\n3\\. Zadrozny, B. and Elkan, C. (2002). Transforming classifier scores into accurate multiclass probability estimates, In Proc. 8th Int. Conf on Knowledge Discovery and Data Mining (KDD\u201902) 694-699. ACM.\n\n## Try the QLattice.\n\n### Experience the future of AI, where accuracy meets simplicity and explainability.\n\nModels developed by [the QLattice](https://www.abzu.ai/qlattice/) have unparalleled accuracy, even with very little data, and are uniquely simple to understand.\n\n[What is the QLattice?](https://www.abzu.ai/qlattice/)\n\n[Get started now](https://docs.abzu.ai/docs/guides/getting_started/quick_start.html)\n\n## Share some data science.\n\n## More data science.\n\n### Basic concepts and ideas in data science and bioinformatics.\n\n- [Data science](https://www.abzu.ai/category/data-science/)\n- [Kevin](https://www.abzu.ai/tag/kevin/), [QLattice](https://www.abzu.ai/tag/qlattice/)\n\n## [Feyn Dev Diary: Stratified data splitting, part 2.](https://www.abzu.ai/data-science/stratified-data-splitting-part-2/)\n\nPart one of a two-part developer diary on building a stratified data splitter for Abzu's python library Feyn and the QLattice.\n\n[Read more](https://www.abzu.ai/data-science/stratified-data-splitting-part-2/)\n\n- [Data science](https://www.abzu.ai/category/data-science/)\n- [Kevin](https://www.abzu.ai/tag/kevin/), [QLattice](https://www.abzu.ai/tag/qlattice/)\n\n## [Feyn Dev Diary: Stratified data splitting, part 1.](https://www.abzu.ai/data-science/stratified-data-splitting-part-1/)\n\nPart one of a two-part developer diary on building a stratified data splitter for Abzu's python library Feyn and the QLattice.\n\n[Read more](https://www.abzu.ai/data-science/stratified-data-splitting-part-1/)\n\n- [Data science](https://www.abzu.ai/category/data-science/)\n- [Kevin](https://www.abzu.ai/tag/kevin/), [QLattice](https://www.abzu.ai/tag/qlattice/)\n\n## [Binary Cl...",
      "url": "https://www.abzu.ai/data-science/calibration-introduction-part-2"
    },
    {
      "title": "",
      "text": "Distribution Calibration for Regression\nHao Song 1 Tom Diethe 2 Meelis Kull 3 Peter Flach 1 4\nAbstract\nWe are concerned with obtaining well-calibrated\noutput distributions from regression models. Such\ndistributions allow us to quantify the uncertainty\nthat the model has regarding the predicted tar\u0002get value. We introduce the novel concept of\ndistribution calibration, and demonstrate its ad\u0002vantages over the existing definition of quantile\ncalibration. We further propose a post-hoc ap\u0002proach to improving the predictions from pre\u0002viously trained regression models, using multi\u0002output Gaussian Processes with a novel Beta link\nfunction. The proposed method is experimentally\nverified on a set of common regression models\nand shows improvements for both distribution\u0002level and quantile-level calibration.\n1 Introduction\nWith recent progress in predictive machine learning, many\nmodels are now capable of providing outstanding perfor\u0002mance with respect to certain metrics, such as accuracy in\nclassification or mean squared error in regression. While\nsuch models are suitable for some tasks, they often cannot\nprovide well-quantified uncertainties on the target variables.\nIn this paper we focus on this problem in the regression\nsetting, extending concepts from the well-established frame\u0002work of probability calibration for classification.\nIn a classification task, a probabilistic prediction s \u2208 [0, 1]\nfor the positive class is calibrated if the following condition\nholds: among all the instances receiving this same predic\u0002tion value s, the probability of observing a positive label\nis s. Having such calibrated outputs is important as they\ncan be interpreted as degrees of uncertainty on the class,\nhence enabling quantitative approaches towards decision\n1University of Bristol, Bristol, United Kingdom 2Amazon Re\u0002search, Cambridge, United Kingdom 3University of Tartu, Tartu,\nEstonia 4The Alan Turing Institute, London, United Kingdom.\nCorrespondence to: Hao Song <hao.song@bristol.ac.uk>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nmaking, such as cost-sensitive classification [33]. However,\nfrom simple models (e.g. Na\u00a8\u0131ve Bayes) to complex ones\n(e.g. (deep) Neural Networks (NNs)), poor calibration is\noften observed, irrespective of the model\u2019s complexity or its\nprobabilistic nature [9, 16]. To mitigate this, several tech\u0002niques have been proposed to apply post-hoc corrections\nto the outputs from trained classifiers such as Platt scaling\n[23], Isotonic regression [33], and Beta calibration [16].\nIn the setting of regression, calibration has been tradition\u0002ally defined through predicted credible intervals [7, 3, 14],\nwhere a 0.95 predicted credible level (e.g. conditional quan\u0002tile) is calibrated if marginally 95% of the true target values\nare below or equal to it. Having a quantile-calibrated re\u0002gressor is particularly useful for certain forecasting tasks\nsuch as energy usage [7] and supply chain optimisation\n[12]. Existing approaches which aim to obtain calibrated\npredictions as part of the training, can be loosely divided\ninto two categories: (i) quantile regression [13], where it\nhas shown that generalised additive models can be applied\nto yield better-calibrated quantiles [3]; (ii) direct application\nof conditional density estimators [2, 30, 22] to obtain an es\u0002timated cumulative distribution function (CDF), which can\nthen be used to generate corresponding credible intervals.\nWhile such approaches can be good options when simple\nmodels will suffice, they are less suitable when employing\n(possibly extant) specialised models, such as (pre-trained)\ndeep NN models. Unlike the case of classification, the post\u0002hoc calibration approach in regression has been left largely\nunexplored. Recently, [14] proposed a post-hoc approach\nthat applies isotonic regression to match the predicted CDF\nand empirical frequency, so that the final results are better\ncalibrated in the quantile sense.\nWhile quantile-level calibration is useful in certain scenar\u0002ios, it is defined uniformly over the entire input space and\ndoes not ensure calibration for a particular prediction, unlike\nthe classification setting. For instance, a quantile-calibrated\nregressor cannot always guarantee that all instances receiv\u0002ing an estimated mean of \u00b5 and standard deviation \u03c3 are\nindeed distributed as a Gaussian distribution with the same\nmoments. In this paper, we focus on a post-hoc method\nthat aims to achieve distribution-level calibration and hence\ngives more accurate uncertainty information for a continu\u0002ous target variable.\nDistribution Calibration for Regression\nFigure 1. Applying quantile and distribution calibration on a synthetic dataset. The column on the left shows the true conditional PDF /\nCDF of the dataset, with the yellow points being the observed data. An ordinary linear regression is fitted to the data and the predictions\nare shown in the second column. An isotonic regression and a GP-BETA model are trained to calibrate the OLS outputs on quantile and\ndistribution level respectively, giving the right two columns.\nFigure 2. (Left Three) The latent Beta parameters modelled by the GP, with red points representing the inducing points. The red shade\nshows the area for one standard deviation at each side. Notice here the x-axis represent the mean value predicted by the OLS, and hence\ncorresponds to the y-axis in Fig. 1. (Right) Calibration maps from GP-BETA and isotonic regression. GP-BETA is capable of predicting\ndifferent calibration maps for different model output, the example shows three calibration maps at 0, 5, 7 respectively. Isotonic regression\nonly gives a single calibration map for all model outputs, and it only aims to calibrate the marginal quantiles.\nOur contributions are as follows:\n1. We introduce the concept of distribution calibration,\nand demonstrate that being calibrated on a distribution\u0002level will naturally lead to calibrated quantiles.\n2. We propose a multi-output Gaussian Process (GP)-\nbased approach [1] to solve the task of post-hoc den\u0002sity calibration. This approach models the distribution\nover calibration parameters, and uses a novel Beta link\nfunction to calibrate any given regression outputs. An\nexample is demonstrated in Fig. 2.\n3. To ensure the scalability of the model, we further pro\u0002vide a solution based on stochastic variational inference\ntogether with induced pseudo-points.\n4. Finally, the proposed approach is experimentally anal\u0002ysed on different regression models including GP re\u0002gression [24] and Bayesian NN (BNN) regression [5].\nThe rest of the paper is organised as follows. In Sec. 2 we\nintroduce calibration in the context of both classification\nand regression, together with some related post-hoc cali\u0002bration approaches. Sec. 3 defines density-level calibration\nand discusses some theoretical properties. The proposed\ncalibration approach is described in Sec. 4. Experimental\nanalysis is shown in Sec. 5 and Sec. 6 concludes.\n2 Background and Definitions\nThroughout this paper, X and Y are random variables over\nspaces X and Y, where X represents the input features of an\ninstance and Y is the corresponding target value. In k-class\nclassification Y is categorical with Y = {1, . . . , k}.\nPost-hoc calibration applies if there is a (pre-trained) prob\u0002abilistic model, which inputs the feature values and out\u0002puts a probability distribution over the target value. We\nuse the notation f : X \u2192 SY to denote such a probabilis\u0002tic model, where SY is a space of probability distributions\nover Y. For the classification task, SY consists in vectors\ns = [s1, . . . , sk], where si denotes the probability of class i.\nHence, s1, . . . , sk \u2208 [0, 1] and Pk\nj=1 si = 1.\nThe idea of post-hoc calibration is to learn a transformation,\nwhich takes in the probability distribution as output by the\nmodel, and transforms it so that the resulting probability\ndistribution would be better calibrated. Intuitive...",
      "url": "https://proceedings.mlr.press/v97/song19a/song19a.pdf"
    },
    {
      "title": "Modular Conformal Calibration",
      "text": "Modular Conformal Calibration\n[![[International Conference on Machine Learning Logo]](https://proceedings.mlr.press/v162/assets/images/logo-pmlr.svg)](https://proceedings.mlr.press/)Proceedings of Machine Learning Research\n[[edit](https://github.com/mlresearch/v162/edit/gh-pages/_posts/2022-06-28-marx22a.md)]\n# Modular Conformal Calibration\nCharles Marx,Shengjia Zhao,Willie Neiswanger,Stefano Ermon\n*Proceedings of the 39th International Conference on Machine Learning*,PMLR 162:15180-15195,2022.\n#### Abstract\nUncertainty estimates must be calibrated (i.e., accurate) and sharp (i.e., informative) in order to be useful. This has motivated a variety of methods for*recalibration*, which use held-out data to turn an uncalibrated model into a calibrated model. However, the applicability of existing methods is limited due to their assumption that the original model is also a probabilistic model. We introduce a versatile class of algorithms for recalibration in regression that we call*modular conformal calibration*(MCC). This framework allows one to transform any regression model into a calibrated probabilistic model. The modular design of MCC allows us to make simple adjustments to existing algorithms that enable well-behaved distribution predictions. We also provide finite-sample calibration guarantees for MCC algorithms. Our framework recovers isotonic recalibration, conformal calibration, and conformal interval prediction, implying that our theoretical results apply to those methods as well. Finally, we conduct an empirical study of MCC on 17 regression datasets. Our results show that new algorithms designed in our framework achieve near-perfect calibration and improve sharpness relative to existing methods.\n#### Cite this Paper\nBibTeX\n`@InProceedings{pmlr-v162-marx22a,\ntitle = {Modular Conformal Calibration},\nauthor = {Marx, Charles and Zhao, Shengjia and Neiswanger, Willie and Ermon, Stefano},\nbooktitle = {Proceedings of the 39th International Conference on Machine Learning},\npages = {15180--15195},\nyear = {2022},\neditor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},\nvolume = {162},\nseries = {Proceedings of Machine Learning Research},\nmonth = {17--23 Jul},\npublisher = {PMLR},\npdf = {https://proceedings.mlr.press/v162/marx22a/marx22a.pdf},\nurl = {https://proceedings.mlr.press/v162/marx22a.html},\nabstract = {Uncertainty estimates must be calibrated (i.e., accurate) and sharp (i.e., informative) in order to be useful. This has motivated a variety of methods for*recalibration*, which use held-out data to turn an uncalibrated model into a calibrated model. However, the applicability of existing methods is limited due to their assumption that the original model is also a probabilistic model. We introduce a versatile class of algorithms for recalibration in regression that we call*modular conformal calibration*(MCC). This framework allows one to transform any regression model into a calibrated probabilistic model. The modular design of MCC allows us to make simple adjustments to existing algorithms that enable well-behaved distribution predictions. We also provide finite-sample calibration guarantees for MCC algorithms. Our framework recovers isotonic recalibration, conformal calibration, and conformal interval prediction, implying that our theoretical results apply to those methods as well. Finally, we conduct an empirical study of MCC on 17 regression datasets. Our results show that new algorithms designed in our framework achieve near-perfect calibration and improve sharpness relative to existing methods.}\n}`\nCopy to ClipboardDownload\nEndnote\n`%0 Conference Paper\n%T Modular Conformal Calibration\n%A Charles Marx\n%A Shengjia Zhao\n%A Willie Neiswanger\n%A Stefano Ermon\n%B Proceedings of the 39th International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2022\n%E Kamalika Chaudhuri\n%E Stefanie Jegelka\n%E Le Song\n%E Csaba Szepesvari\n%E Gang Niu\n%E Sivan Sabato\t%F pmlr-v162-marx22a\n%I PMLR\n%P 15180--15195\n%U https://proceedings.mlr.press/v162/marx22a.html\n%V 162\n%X Uncertainty estimates must be calibrated (i.e., accurate) and sharp (i.e., informative) in order to be useful. This has motivated a variety of methods for*recalibration*, which use held-out data to turn an uncalibrated model into a calibrated model. However, the applicability of existing methods is limited due to their assumption that the original model is also a probabilistic model. We introduce a versatile class of algorithms for recalibration in regression that we call*modular conformal calibration*(MCC). This framework allows one to transform any regression model into a calibrated probabilistic model. The modular design of MCC allows us to make simple adjustments to existing algorithms that enable well-behaved distribution predictions. We also provide finite-sample calibration guarantees for MCC algorithms. Our framework recovers isotonic recalibration, conformal calibration, and conformal interval prediction, implying that our theoretical results apply to those methods as well. Finally, we conduct an empirical study of MCC on 17 regression datasets. Our results show that new algorithms designed in our framework achieve near-perfect calibration and improve sharpness relative to existing methods.`\nCopy to ClipboardDownload\nAPA\n`Marx, C., Zhao, S., Neiswanger, W. & Ermon, S.. (2022). Modular Conformal Calibration.*Proceedings of the 39th International Conference on Machine Learning*, in*Proceedings of Machine Learning Research*162:15180-15195 Available from https://proceedings.mlr.press/v162/marx22a.html.`\nCopy to ClipboardDownload\n#### Related Material\n* [Download PDF](https://proceedings.mlr.press/v162/marx22a/marx22a.pdf)",
      "url": "https://proceedings.mlr.press/v162/marx22a.html"
    },
    {
      "title": "",
      "text": "Classifier Calibration with ROC-Regularized Isotonic Regression\nEug`ene Berta Francis Bach Michael I. Jordan\nInria, Ecole Normale Sup\u00b4erieure,\nPSL Research University\nInria, Ecole Normale Sup\u00b4erieure,\nPSL Research University\nInria, Ecole Normale Sup\u00b4erieure,\nPSL Research University,\nUniversity of California, Berkeley\nAbstract\nCalibration of machine learning classifiers is\nnecessary to obtain reliable and interpretable\npredictions, bridging the gap between model\noutputs and actual probabilities. One promi\u0002nent technique, isotonic regression (IR), aims\nat calibrating binary classifiers by minimiz\u0002ing the cross entropy with respect to mono\u0002tone transformations. IR acts as an adaptive\nbinning procedure that is able to achieve a\ncalibration error of zero but leaves open the\nissue of the effect on performance. We first\nprove that IR preserves the convex hull of the\nROC curve\u2014an essential performance met\u0002ric for binary classifiers. This ensures that\na classifier is calibrated while controlling for\nover-fitting of the calibration set. We then\npresent a novel generalization of isotonic re\u0002gression to accommodate classifiers with K\u0002classes. Our method constructs a multidi\u0002mensional adaptive binning scheme on the\nprobability simplex, again achieving a multi\u0002class calibration error equal to zero. We reg\u0002ularize this algorithm by imposing a form of\nmonotony that preserves the K-dimensional\nROC surface of the classifier. We show em\u0002pirically that this general monotony criterion\nis effective in striking a balance between re\u0002ducing cross entropy loss and avoiding over\u0002fitting of the calibration set.\n1 INTRODUCTION\nCalibration is a natural requirement for probabilistic\npredictions. It aligns the outputs of a classifier with\ntrue probabilities, according with the intuition that\nProceedings of the 27th International Conference on Artifi\u0002cial Intelligence and Statistics (AISTATS) 2024, Valencia,\nSpain. PMLR: Volume 238. Copyright 2024 by the au\u0002thor(s).\nthe predictions of our models should match observed\nfrequencies. Several papers have demonstrated em\u0002pirically that simple machine learning classifiers can\nexhibit poor calibration, even on very simple datasets\n(Zadrozny and Elkan, 2001, 2002; Niculescu-Mizil and\nCaruana, 2005). More recently Guo et al. (2017)\nshowed that deep neural networks suffer from the same\nproblem, due to their tendency to over-fit the training\ndata, reviving the community\u2019s interest in calibration.\nThe interpretation of the predictions of machine learn\u0002ing classifiers as probabilities is not possible without\ncalibration. Calibration is desirable in that it provides\na lingua franca for multiple users to assess the outputs\nof a learning system. It also permits the use of learning\nsystems as modules in complex prediction pipelines\u2014a\nsingle module can be updated independently of others\nif its outputs can be assumed to be calibrated.\n1.1 Calibration\nWe let X and Y denote the feature space and the\noutput space of a numerical classification problem, re\u0002spectively, with Y = {0, 1} in the binary classifica\u0002tion setting and Y = {1, . . . , K} in the general K\u0002class classification setting. We consider a probability\ndistribution for a random variable (X, Y ) \u2208 X \u00d7 Y,\nand a probabilistic classifier f : X \u2192 P making pre\u0002dictions p = f(x) in the prediction space P. In the\nbinary case we take P = [0, 1] and in the multi-class\ncase P = \u2206K, with \u2206K the K-dimensional simplex\n{p \u2208 R\nK\n+ |\nPK\ni=1 pi = 1}.\nDefinition 1.1 (Calibration, Foster and Vohra, 1998;\nZadrozny and Elkan, 2002). A binary classifier f :\nX \u2192 [0, 1] is said to be calibrated if P[Y = 1|f(X)] =\nf(X), or equivalently E[Y |f(X)] = f(X). For a\nmulti-class classifier f : X \u2192 \u2206K, the definition is\nE[Y |f(X)] = f(X).\nThe concept of calibration has been useful in a va\u0002riety of applied contexts, notably including weather\nforecasting (Murphy and Winkler, 1977).\nEvaluating calibration. We define a criterion that\nClassifier Calibration with ROC-Regularized Isotonic Regression\nassesses the calibration of a classifier.\nDefinition 1.2 (Calibration error). For a classifier f,\nthe calibration error is K(f) = E\n\u0002\n|E[Y |f(X)]\u2212f(X)|\n\u0003\n.\nThis error is usually referred to as the expected cali\u0002bration error (ECE) (Pakdaman Naeini et al., 2015).\nFor a discrete set of observed data points,\n(xi, yi)1\u2264i\u2264n, if the classifier f takes continuous\nvalues, the expectation E[Y |f(X)] needs to be es\u0002timated. If the predictions live on a discrete grid\nP = [\u03bb1, . . . , \u03bbm], we can readily approximate this\nexpectation. For any index i, we have f(xi) = \u03bbj for\nsome \u03bbj in the grid. We can use all the points for which\nthe prediction was \u03bbj (Sj = {k \u2208 J1, nK | f(xk) = \u03bbj})\nto compute the empirical expectation:\nE[yi|f(xi)] \u2243\n1\n#Sj\nP\nk\u2208Sj\nyk.\nPlugging in such estimates, the calibration error can\nbe approximated. Predictions on grids have been ubiq\u0002uitous in the literature on calibration. In particular,\nin weather forecasting, the predictions usually live on\nthe grid [0%, 10%, . . . , 100%]. In the continuous case\nof machine learning classifiers, however, it is not clear\nthat such discretizations make sense; in particular, it\nis not clear how they interact with performance.\nCalibration and model performance. There is\na significant literature establishing theoretical bounds\nfor calibration (see Foster and Hart, 2021, for a re\u0002view). A central result is that one can always produce\na calibrated sequence of predictions, even if the out\u0002comes are generated by an adversarial player. This\nsurprising result is a consequence of the minimax the\u0002orem (Hart, 2022), and it leads to simple strategies to\ngenerate a sequence of forecasts that is asymptotically\ncalibrated against any possible sequence of outcomes.\nThis is a positive result, but it also reflects the fact\nthat calibration is a weak constraint. Consider a lo\u0002cale where it rains every other day. Predicting a 50%\nchance of precipitation every day is enough to achieve\ncalibration even if this forecast is quite poor. This\nsuggests that while calibration is useful, it should be\nconsidered in the overall context of the accuracy of the\nforecasts (Foster and Hart, 2022).\nCalibration and proper scoring rules. Br\u00a8ocker\n(2009) proved that any proper score can be decom\u0002posed into the calibration error and a second refine\u0002ment term. In particular, for the cross-entropy loss:\nH(Y, f(X)) = E[KL(f(X)||P(Y |f(X))]\n+E[H(P(Y |f(X)))],\n(1)\nwith H(., .) the cross entropy and H(.) the entropy.\nHere, we see that the calibration error is expressed in\nterms of the Kullback-Leibler (KL) divergence; other\ncriteria can arise depending on the specific proper scor\u0002ing rule that is chosen. This confirms that a zero\ncalibration error does not necessarily guarantee good\nforecasts. Indeed, calibration can be achieved inde\u0002pendently of the performance of the classifier. The\nintuition is that aligning model confidence with prob\u0002abilities can be done whatever the performance of the\nmodel, and the lower the model\u2019s accuracy, the less\nconfident it should be in its predictions. Machine\nlearning classifiers are usually able to generate fore\u0002casts with good accuracy, but these forecasts are gen\u0002erally not calibrated. The decomposition above shows\nthat calibrating our classifiers might help in reducing\nthe cross-entropy loss even further.\n1.2 Calibrating machine learning classifiers\nThe machine learning literature has generally em\u0002ployed the following simple data-splitting heuristic\nto calibrate classifiers. Given n i.i.d data points\n(xi, yi)1\u2264i\u2264n \u2208 (X , Y), a portion of this available data\nis reserved for calibration (calibration set) and the\nclassifier is trained on the rest of the data (training\nset). After the classifier is trained, the held-out cali\u0002bration set is used to evaluate and correct its calibra\u0002tion error. This paradigm separates the calibration\nprocedure from model fitting, resulting in calibration\nmethods that can be applied to any model. How\u0002ever, holding out a portion of the data for calibration\ncan be problematic in d...",
      "url": "https://proceedings.mlr.press/v238/berta24a/berta24a.pdf"
    },
    {
      "title": "Classifier calibration: a survey on how to assess and improve predicted class probabilities",
      "text": "Classifier calibration: a survey on how to assess and improve predicted class probabilities | Machine Learning\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10994-023-06336-7?)\n# Classifier calibration: a survey on how to assess and improve predicted class probabilities\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:16 May 2023\n* Volume\u00a0112,\u00a0pages 3211\u20133260, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10994-023-06336-7.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/10994?as=webp)Machine Learning](https://link.springer.com/journal/10994)[Aims and scope](https://link.springer.com/journal/10994/aims-and-scope)[Submit manuscript](https://submission.springernature.com/new-submission/10994/3)\nClassifier calibration: a survey on how to assess and improve predicted class probabilities\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10994-023-06336-7.pdf)\n* [Telmo Silva Filho](#auth-Telmo-Silva_Filho-Aff1-Aff2)[ORCID:orcid.org/0000-0003-0826-6885](https://orcid.org/0000-0003-0826-6885)[1](#Aff1),[2](#Aff2)[na1](#na1),\n* [Hao Song](#auth-Hao-Song-Aff2)[ORCID:orcid.org/0000-0003-0876-485X](https://orcid.org/0000-0003-0876-485X)[2](#Aff2)[na1](#na1),\n* [Miquel Perello-Nieto](#auth-Miquel-Perello_Nieto-Aff2)[ORCID:orcid.org/0000-0001-8925-424X](https://orcid.org/0000-0001-8925-424X)[2](#Aff2)[na1](#na1),\n* [Raul Santos-Rodriguez](#auth-Raul-Santos_Rodriguez-Aff2)[ORCID:orcid.org/0000-0001-9576-3905](https://orcid.org/0000-0001-9576-3905)[2](#Aff2),\n* [Meelis Kull](#auth-Meelis-Kull-Aff3)[ORCID:orcid.org/0000-0001-9257-595X](https://orcid.org/0000-0001-9257-595X)[3](#Aff3)&amp;\n* \u2026* [Peter Flach](#auth-Peter-Flach-Aff2)[ORCID:orcid.org/0000-0001-6857-5810](https://orcid.org/0000-0001-6857-5810)[2](#Aff2)Show authors\n* 22kAccesses\n* 105Citations\n* 9Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1007/s10994-023-06336-7/metrics)\n## Abstract\nThis paper provides both an introduction to and a detailed overview of the principles and practice of classifier calibration. A well-calibrated classifier correctly quantifies the level of uncertainty or confidence associated with its instance-wise predictions. This is essential for critical applications, optimal decision making, cost-sensitive classification, and for some types of context change. Calibration research has a rich history which predates the birth of machine learning as an academic field by decades. However, a recent increase in the interest on calibration has led to new methods and the extension from binary to the multiclass setting. The space of options and issues to consider is large, and navigating it requires the right set of concepts and tools. We provide both introductory material and up-to-date technical details of the main concepts and methods, including proper scoring rules and other evaluation metrics, visualisation approaches, a comprehensive account of post-hoc calibration methods for binary and multiclass classification, and several advanced topics.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-08974-9?as&#x3D;webp)\n### [Classifier Probability Calibration Through Uncertain Information Revision](https://link.springer.com/10.1007/978-3-031-08974-9_48?fromPaywallRec=false)\nChapter\u00a9 2022\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-63800-8?as&#x3D;webp)\n### [Investigating Calibrated Classification Scores Through the\u00a0Lens of\u00a0Interpretability](https://link.springer.com/10.1007/978-3-031-63800-8_11?fromPaywallRec=false)\nChapter\u00a9 2024\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-86380-7?as&#x3D;webp)\n### [Estimating Expected Calibration Errors](https://link.springer.com/10.1007/978-3-030-86380-7_12?fromPaywallRec=false)\nChapter\u00a9 2021\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Categorization](https://link.springer.com/subjects/categorization)\n* [Data Mining](https://link.springer.com/subjects/data-mining)\n* [Learning algorithms](https://link.springer.com/subjects/learning-algorithms)\n* [Machine Learning](https://link.springer.com/subjects/machine-learning)\n* [Probability and Statistics in Computer Science](https://link.springer.com/subjects/probability-and-statistics-in-computer-science)\n* [Statistical Learning](https://link.springer.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10994)\nAvoid common mistakes on your manuscript.\n## 1Introduction and motivation\nA*K*-class probabilistic classifier is*well-calibrated*if among test instances receiving a predicted*K*-dimensional probability vector\\\\({\\\\textbf{s}}\\\\), the class distribution is (approximately) distributed as\\\\({\\\\textbf{s}}\\\\). This property is of fundamental importance when using a classifier for cost-sensitive classification, for human decision making, or within an autonomous system. It means that the classifier correctly quantifies the level of uncertainty or confidence associated with its predictions. In a binary setting, scores given by a sufficiently calibrated classifier can be simply thresholded to minimise expected misclassification cost. Thresholds can also be derived to optimally adapt to a change in class prior, or to a combination of both. In contrast, for a poorly calibrated classifier the optimal thresholds cannot be obtained without optimisation.\nMany machine learning algorithms are known to produce over-confident models, unless dedicated procedures are applied during training. The goal of*(post-hoc) calibration methods*is to use hold-out validation data to learn a*calibration map*for a previously trained model that transforms the model\u2019s predictions to be better calibrated. Many calibration methods for binary classifiers have been introduced, including logistic calibration (also known as \u2018Platt scaling\u2019), various binning methods including isotonic calibration (also known as the ROC convex hull method), as well as more recent methods including beta calibration and Bayesian methods.\nWhen we have more than two classes, calibration is generally more involved, as is often the case with multiclass classification. Multiclass calibration has mostly been approached by decomposing the problem into*K*one-vs-rest binary calibration tasks, one for each class. The predictions of these*K*calibration models form unnormalised probability vectors, which, after normalisation, may not be calibrated in a multiclass sense. Native multiclass calibration methods were introduced recently focusing on neural networks. These methods constitute various multiclass extensions of Platt scaling, adding a calibration layer between the logits of the neural network and the softmax layer.\nThe literature on post-hoc classifier calibration in machine learning is now sufficiently rich that it is no longer straightforward to obtain or maintain a good overview of the area, which was the main motivation for writing this survey. It grew out of a tutorial we presented at the 2020 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (see[https://classifier-calibration.github.io](https://classifier-calibration.github.io)). Our aim then and now is to provide both int...",
      "url": "https://link.springer.com/article/10.1007/s10994-023-06336-7?error=cookies_not_supported&code=b7fe0596-fdf3-4b29-ae16-0f2b2b76f39a"
    },
    {
      "title": "",
      "text": "Binary Classifier Calibration using an Ensemble of Near Isotonic \nRegression Models\nMahdi Pakdaman Naeini and\nIntelligent Systems Program, University of Pittsburgh, Pittsburgh, USA\nGregory F. Cooper\nDepartment of Biomedical Informatics, University of Pittsburgh, Pittsburgh, USA\nAbstract\nLearning accurate probabilistic models from data is crucial in many practical tasks in data mining. \nIn this paper we present a new non-parametric calibration method called ensemble of near isotonic \nregression (ENIR). The method can be considered as an extension of BBQ [20], a recently \nproposed calibration method, as well as the commonly used calibration method based on isotonic \nregression (IsoRegC) [27]. ENIR is designed to address the key limitation of IsoRegC which is the \nmonotonicity assumption of the predictions. Similar to BBQ, the method post-processes the output \nof a binary classifier to obtain calibrated probabilities. Thus it can be used with many existing \nclassification models to generate accurate probabilistic predictions.\nWe demonstrate the performance of ENIR on synthetic and real datasets for commonly applied \nbinary classification models. Experimental results show that the method outperforms several \ncommon binary classifier calibration methods. In particular on the real data, ENIR commonly \nperforms statistically significantly better than the other methods, and never worse. It is able to \nimprove the calibration power of classifiers, while retaining their discrimination power. The \nmethod is also computationally tractable for large scale datasets, as it is O(N log N) time, where N \nis the number of samples.\nI. Introduction\nIn many real world data mining applications, intelligent agents often must make decisions \nunder considerable uncertainty due to noisy observations, physical randomness, incomplete \ndata, and incomplete knowledge. Decision theory provides a normative basis for intelligent \nagents to make rational decisions under such uncertainty. To do so, decision theory \ncombines utilities and probabilities to determine the optimal actions that maximize expected \nutility [23]. The output in many of the machine learning models that are used in data mining \napplications is designed to discriminate the patterns in data. However, such output should \nalso provide accurate (calibrated) probabilities in order to be practically useful for rational \ndecision making in many real world applications.\nThis paper focuses on developing a new non-parametric calibration method for post\u0002processing the output of commonly used binary classification models to generate accurate \nHHS Public Access\nAuthor manuscript\nProc IEEE Int Conf Data Min. Author manuscript; available in PMC 2017 March 15.\nPublished in final edited form as:\nProc IEEE Int Conf Data Min. 2016 December ; 2016: 360\u2013369. doi:10.1109/ICDM.2016.0047.\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript\nprobabilities. Informally, we say that a classification model is well-calibrated if events \npredicted to occur with probability p do occur about p fraction of the time, for all p. This \nconcept applies to binary as well as multi-class classification problems. Figure 1 illustrates \nthe binary calibration problem using a reliability curve [6], [19]. The curve shows the \nprobability predicted by the classification model versus the actual fraction of positive \noutcomes for a hypothetical binary classification problem, where Z is the binary event being \npredicted. The curve shows that when the model predicts Z = 1 to have probability 0.2, the \noutcome Z = 1 occurs in about 0.3 fraction of the time. The curve shows that the model is \nfairly well calibrated, but it tends to underestimate the actual probabilities. In general, the \nstraight dashed line connecting (0, 0) to (1, 1) represents a perfectly calibrated model. The \ncloser a calibration curve is to this line, the better calibrated is the associated prediction \nmodel. Deviations from perfect calibration are very common in practice and may vary \nwidely depending on the binary classification model that is used [20].\nProducing well-calibrated probabilistic predictions is critical in many areas of science (e.g., \ndetermining which experiments to perform), medicine (e.g., deciding which therapy to give \na patient), business (e.g., making investment decisions), and many others. In data mining \nproblems, obtaining well-calibrated classification models is crucial not only for decision\u0002making, but also for combining output of different classification models [3]. It is also useful \nwhen we aim to use the output of a classifier not only to discriminate the instances but also \nto rank them [28], [16], [11]. Research on learning well calibrated models has not been \nexplored in the data mining literature as extensively as, for example, learning models that \nhave high discrimination (e.g., high accuracy).\nThere are two main approaches to obtaining well-calibrated classification models. The first \napproach is to build a classification model that is intrinsically well-calibrated ab initio. This \napproach will restrict the designer of the data mining model by requiring major changes in \nthe objective function (e.g, using a different type of loss function) and could potentially \nincrease the complexity and computational cost of the associated optimization program to \nlearn the model. The other approach is to rely on the existing discriminative data mining \nmodels and then calibrate their output using post-processing methods. This approach has the \nadvantage that it is general, flexible, and it frees the designer of a data mining algorithm \nfrom modifying the learning procedure and the associated optimization method [20]. \nHowever, this approach has the potential to decrease discrimination while increasing \ncalibration, if care is not taken. The method we describe in this paper is shown empirically \nto improve calibration of different types of classifiers (e.g., LR, SVM, and NB) while \nmaintaining their discrimination performance.\nExisting post-processing binary classifier calibration methods include Platt scaling [22], \nhistogram binning [26], isotonic regression [27], and a recently proposed method BBQ \nwhich is a Bayesian extension of histogram binning [20]. In all these methods, the post\u0002processing step can be seen as a function that maps the outputs of a prediction model to \nprobabilities that are intended to be well-calibrated. Figure 1 shows an example of such a \nmapping.\nNaeini and Cooper Page 2\nProc IEEE Int Conf Data Min. Author manuscript; available in PMC 2017 March 15.\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript\nIn general, there are two main applications of post-processing calibration methods. First, \nthey can be used to convert the outputs of discriminative classification methods with no \napparent probabilistic interpretation to posterior class probabilities [22]. An example is an \nSVM model that learns a discriminative model that does not have a direct probabilistic \ninterpretation. In this paper, we show this use of calibration to map SVM outputs to well\u0002calibrated probabilities. Second, calibration methods can be applied to improve the \ncalibration of predictions of a probabilistic model that is miscalibrated. For example, a na\u00efve \nBayes (NB) model is a probabilistic model, but its class posteriors are often miscalibrated \ndue to unrealistic independence assumptions [19]. The method we describe is shown \nempirically to improve the calibration of NB models without reducing their discrimination. \nThe method can also work well on calibrating models that are less egregiously miscalibrated \nthan are NB models.\nII. Related work\nExisting post-processing binary classifier calibration models can be divided into parametric \nand non-parametric methods. Platt's method is an example of the former; it uses a sigmoid \ntransformation to map the output of a classifier into a calibrated probability [22]. The two \nparameters ...",
      "url": "https://www.dbmi.pitt.edu/wp-content/uploads/2022/10/Binary-classifier-calibration-using-an-ensemble-of-near-isotonic-regression-models.pdf"
    },
    {
      "title": "Smooth Isotonic Regression: A New Method to Calibrate Predictive Models",
      "text": "Smooth Isotonic Regression: A New Method to Calibrate Predictive Models - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](pdf/16-cri_summit_2011.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![AMIA Summits on Translational Science Proceedings logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-amiasummtsp.gif)\nAMIA Jt Summits Transl Sci Proc\n. 2011 Mar 7;2011:16\u201320.\n# Smooth Isotonic Regression: A New Method to Calibrate Predictive Models\n[Xiaoqian Jiang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Jiang X\"[Author]>)\n### Xiaoqian Jiang,PhD\n1Division of Biomedical Informatics, Department of Medicine University of California, San Diego\nFind articles by[Xiaoqian Jiang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Jiang X\"[Author]>)\n1,[Melanie Osl](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Osl M\"[Author]>)\n### Melanie Osl,PhD\n1Division of Biomedical Informatics, Department of Medicine University of California, San Diego\nFind articles by[Melanie Osl](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Osl M\"[Author]>)\n1,[Jihoon Kim](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kim J\"[Author]>)\n### Jihoon Kim,MSc\n1Division of Biomedical Informatics, Department of Medicine University of California, San Diego\nFind articles by[Jihoon Kim](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kim J\"[Author]>)\n1,[Lucila Ohno-Machado](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ohno-Machado L\"[Author]>)\n### Lucila Ohno-Machado,MD, PhD\n1Division of Biomedical Informatics, Department of Medicine University of California, San Diego\nFind articles by[Lucila Ohno-Machado](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ohno-Machado L\"[Author]>)\n1\n* Author information\n* Article notes\n* Copyright and License information\n1Division of Biomedical Informatics, Department of Medicine University of California, San Diego\nCollection date 2011.\n\u00a92011 AMIA - All rights reserved.\nThis is an Open Access article: verbatim copying and redistribution of this article are permitted in all media for any purpose\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC3248752\u00a0\u00a0PMID:[22211175](https://pubmed.ncbi.nlm.nih.gov/22211175/)\n## Abstract\nPredictive models are critical for risk adjustment in clinical research. Evaluation of supervised learning models often focuses on predictive model discrimination, sometimes neglecting the assessment of their calibration. Recent research in machine learning has shown the benefits of calibrating predictive models, which becomes especially important when probability estimates are used for clinical decision making. By extending the isotonic regression method for recalibration to obtain a smoother fit in reliability diagrams, we introduce a novel method that combines parametric and non-parametric approaches. The method calibrates probabilistic outputs smoothly and shows better generalization ability than its ancestors in simulated as well as real world biomedical data sets.\n## Introduction\nRisk assessment tools such as the Cox proportional hazard model, the logistic regression model, and other machine-learning based predictive models are widely used in patient diagnosis, prognosis and clinical studies. Accurate calibration of these models is important if the outputs are going to be applied to new cohorts [[3](#b3-16-cri_summit_2011)]. For example, the Gail model, a predictive model of a woman\u2019s risk of developing breast cancer, was reported to underestimate the risk among a specific subgroup of patients [[8](#b8-16-cri_summit_2011)]. After recalibration, the model identified more patients who would benefit from chemoprevention than the original model [[1](#b1-16-cri_summit_2011)]. Another example is derived from the Framingham Heart Study model, in which gender-specific coronary heart disease (CHD) prediction functions can be used for assessing the risk of developing CHD. While the original model overestimated the risk of 5-year CHD events among Japanese American men, Hispanic men and Native American women, the recalibrated risk score based on the new cohort\u2019s own average incidence rate, performed well [[4](#b4-16-cri_summit_2011)].\nA well calibrated predictive model provides risk estimates that reflect the underlying probabilities for an disease. This means that the proportion of positive events (*c*= 1 from*c*\u2208 {0, 1}) in a group of cases that have according to the model a risk of e.g.*p*= 0.8 is exactly 0.8. Needless to say, this notion of calibration depends on an sufficient number of cases with the same risk to be evaluated reliably. In practice, when there are not many cases with the same estimated probability, cases with similar values for*p*are grouped for evaluation.\n### Calibration Assessment\nA simple way of assessing the calibration of a predictive model is a calibration plot or reliability diagram. This visual tool plots expected versus observed events as follows: all estimated probabilities*p*are grouped according to the fixed cutoff points 0.1, 0.2, . . ., 1.0. The*x*-coordinates of the points in the plot are the mean values of the estimated probabilities in each group. The*y*-coordinates are the observed fraction of cases with*c*= 1. If the predictive model is well calibrated, the points fall near the diagonal line. An example of a calibration plot is given in[Figure 1](#f1-16-cri_summit_2011). The points of the plot are connected by a line for better visualization. The meaning of the dotted red lines is explained in the next paragraph.\n#### Figure 1:\n![Figure 1:](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/60d5/3248752/fe6fba4400f5/16-cri_summit_2011f1.jpg)\n[Open in a new tab](figure/f1-16-cri_summit_2011/)\nCalibration plot with fitted probabilities by (a) sigmoid fitting and (b) isotonic regression.\nA quantitative measure of calibration is given by a goodness-of-fit test like the well-known Hosmer-Lemeshow test [[9](#b9-16-cri_summit_2011)]. The Homer-Lemeshow*C*-statistic is given by:\nHLc=\u2211c=01\u2211i=1G(Oic\u2212Ei)2Ei(1\u2212E1ni),|\nwhere*G*denotes the number of groups (usually 10),Oicis the sum of cases with*c*= 0 or*c*= 1,*Ei*is the sum of estimated probabilities, and*ni*denotes the number of cases in group*i*. This value is then compared to a chi-square distribution with G-2 degrees of freedom.\nTo improve the calibration of binary classification models, different calibration methods have been proposed.\n### Calibration Improvement\nTwo popular methods to improve the calibration of predictive models are the methods proposed by Platt [[13](#b13-16-cri...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3248752"
    },
    {
      "title": "",
      "text": "Obtaining Well Calibrated Probabilities Using Bayesian Binning\nMahdi Pakdaman Naeini1, Gregory F. Cooper1,2, and Milos Hauskrecht1,3\n1\nIntelligent Systems Program, University of Pittsburgh, PA, USA\n2Department of Biomedical Informatics, University of Pittsburgh, PA, USA\n3Computer Science Department, University of Pittsburgh, PA, USA\nAbstract\nLearning probabilistic predictive models that are well cali\u0002brated is critical for many prediction and decision-making\ntasks in artificial intelligence. In this paper we present a new\nnon-parametric calibration method called Bayesian Binning\ninto Quantiles (BBQ) which addresses key limitations of ex\u0002isting calibration methods. The method post processes the\noutput of a binary classification algorithm; thus, it can be\nreadily combined with many existing classification algo\u0002rithms. The method is computationally tractable, and em\u0002pirically accurate, as evidenced by the set of experiments\nreported here on both real and simulated datasets.\nIntroduction\nA rational problem solving agent aims to maximize its util\u0002ity subject to the existing constraints (Russell and Norvig\n1995). To be able to maximize the utility function for many\npractical prediction and decision-making tasks, it is cru\u0002cial to develop an accurate probabilistic prediction model\nfrom data. Unfortunately, many existing machine learning\nand data mining models and algorithms are not optimized\nfor obtaining accurate probabilities and the predictions they\nproduce may be miscalibrated. Generally, a set of predic\u0002tions of a binary outcome is well calibrated if the outcomes\npredicted to occur with probability p do occur about p frac\u0002tion of the time, for each probability p that is predicted.\nThis concept can be readily generalized to outcomes with\nmore than two values. Figure 1 shows a hypothetical ex\u0002ample of a reliability curve (DeGroot and Fienberg 1983;\nNiculescu-Mizil and Caruana 2005), which displays the cal\u0002ibration performance of a prediction method. The curve\nshows, for example, that when the method predicts Z = 1\nto have probability 0.5, the outcome Z = 1 occurs in about\n0.57 fraction of the instances (cases). The curve indicates\nthat the method is fairly well calibrated, but it tends to assign\nprobabilities that are too low. In general, perfect calibration\ncorresponds to a straight line from (0, 0) to (1, 1). The closer\na calibration curve is to this line, the better calibrated is the\nassociated prediction method.\nProducing well-calibrated probabilistic predictions is crit\u0002ical in many areas of science (e.g., determining which exper\u0002Copyright \rc 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\niments to perform), medicine (e.g., deciding which therapy\nto give a patient), business (e.g., making investment deci\u0002sions), and others. However, model calibration and the learn\u0002ing of well-calibrated probabilistic models have not been\nstudied in the machine learning literature as extensively as\nfor example discriminative machine learning models that\nare built to achieve the best possible discrimination among\nclasses of objects. One way to achieve a high level of model\ncalibration is to develop methods for learning probabilistic\nmodels that are well-calibrated, ab initio. However, this ap\u0002proach would require one to modify the objective function\nused for learning the model and it may increase the com\u0002putational cost of the associated optimization task. An al\u0002ternative approach is to construct well-calibrated models by\nrelying on the existing machine learning methods and by\nmodifying their outputs in a post-processing step to obtain\nthe desired model. This approach is often preferred because\nof its generality, flexibility, and the fact that it frees the de\u0002signer of the machine learning model from the need to add\nadditional calibration measures into the objective function\nused to learn the model. The existing approaches developed\nfor this purpose include histogram binning, Platt scaling, or\nisotonic regression (Platt 1999; Zadrozny and Elkan 2001;\n2002). In all these the post-processing step can be seen as\na function that maps the output of a prediction model to\nprobabilities that are intended to be well-calibrated. Figure\n1 shows an example of such a mapping.\nExisting post-processing calibration methods can be di\u0002vided into two groups: parametric and nonparametric meth\u0002ods. An example of a parametric method is Platt\u2019s method\nthat applies a sigmoidal transformation that maps the output\nof a predictive model (Platt 1999) to a calibrated probabil\u0002ity output. The parameters of the sigmoidal transformation\nfunction are learned using a maximum likelihood estima\u0002tion framework. The key limitation of the approach is the\n(sigmoidal) form of the transformation function, which only\nrarely fits the true distribution of predictions. The most com\u0002mon non-parametric methods are based either on binning\n(Zadrozny and Elkan 2001) or isotonic regression (Zadrozny\nand Elkan 2002). In the histogram binning approach, also\nknown as quantile binning, the raw predictions of a binary\nclassifier are sorted first, and then they are partitioned into\nB subsets of equal size, called bins. Given a prediction y,\nthe method finds the bin containing that prediction and re-\nFigure 1: The solid line shows a calibration (reliability)\ncurve for predicting Z = 1. The dotted line is the ideal cali\u0002bration curve.\nturns as y\u02c6 the fraction of positive outcomes (Z = 1) in the\nbin. Histogram binning has several limitations, including the\nneed to define the number of bins and the fact that the bins\nand their associated boundaries remain fixed over all predic\u0002tions (Zadrozny and Elkan 2002).\nThe other non-parametric calibration method is based on\nisotonic regression (Zadrozny and Elkan 2002). This method\nonly requires that the mapping function be isotonic (mono\u0002tonically increasing) (Niculescu-Mizil and Caruana 2005).\nA commonly used method for computing the isotonic re\u0002gression is pool adjacent violators (PAV) algorithm (Bar\u0002low et al. 1972). The isotonic calibration method based on\nthe (PAV) algorithm can be viewed as a binning algorithm\nwhere the position of boundaries and the size of bins are\nselected according to how well the classifier ranks the ex\u0002amples in the training data (Zadrozny and Elkan 2002).\nRecently a variation of the isotonic-regression-based cali\u0002bration method for predicting accurate probabilities with a\nranking loss was proposed (Menon et al. 2012). Although\nisotonic regression based calibration yields good perfor\u0002mance in many real data applications, the violation of iso\u0002tonicity assumption in practice is quite frequent, so the re\u0002laxation of the isotonicity constraints may be appropriate.\nThis paper presents a new binary classifier calibration\nmethod called Bayesian Binning into Quantiles (BBQ) that\nis applied as a post-processing step1. The approach can be\nviewed as a refinement of the histogram-binning calibration\nmethod in that it considers multiple different binnings and\ntheir combination to yield more robust calibrated predic\u0002tions. Briefly, by considering only one fixed bin discretiza\u0002tion, one may not be able to guess correctly the optimal bin\nwidth. In addition, the data may not be distributed equally\nacross the output space after applying the discriminative pro\u0002jection learned in the first step, and various biases in the dis\u0002tribution may occur. Both of these problems can be resolved\nby considering multiple different binnings and their com\u0002bination. The experimental results presented below indicate\nthat the proposed method works well in practice.\n1An implementation of BBQ method can be found at the fol\u0002lowing address: https://github.com/pakdaman/calibration.git\nMethods\nBBQ extends the simple histogram-binning calibration\nmethod (Zadrozny and Elkan 2001) by considering multiple\nbinning models and their combination. The main challenge\nhere is to decide on how to pick the models and how to com\u0002bine them. BBQ considers multipl...",
      "url": "https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf"
    }
  ]
}