{
  "query": "How can I improve model generalization when extrapolating to new chemical entities in molecular property prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving the ability of a molecular property model to extrapolate to truly new chemical entities hinges on reducing covariate\u2011shift between the training set and the out\u2011of\u2011distribution (OOD) compounds that appear in drug\u2011discovery campaigns.  Recent work shows that standard end\u2011to\u2011end training of graph neural networks (GNNs) often over\u2011fits the structural motifs present in the training data, leading to unstable predictions when the scaffold or similarity distribution changes\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  A practical way to mitigate this is to start from a large\u2011scale pretrained GNN or foundation model that has already learned a broad chemical vocabulary, and then adapt it to the target task with techniques that preserve the generic knowledge while injecting task\u2011specific signals.  For example, the MolE foundation model uses disentangled attention to capture diverse graph patterns and can be fine\u2011tuned with auxiliary learning or bi\u2011level task\u2011weight optimization, which has been shown to improve generalization by up to\u202f8\u202f% over na\u00efve fine\u2011tuning\u202f([nature.com](https://www.nature.com/articles/s41467-024-53751-y)) and by jointly training on related auxiliary tasks to learn both general and task\u2011specific features\u202f([openreview.net](https://openreview.net/pdf/41e1a95e6fc281b863d44eb407ff5a07d8f83bef.pdf)).  \n\nComplementary strategies focus on leveraging additional data and more robust training objectives.  Transfer\u2011learning pipelines that first train on high\u2011fidelity (expensive) measurements and then adapt to low\u2011fidelity (cheap) data can dramatically reduce the amount of labeled chemistry needed while preserving performance in the ultra\u2011low\u2011data regime\u202f([nature.com](https://www.nature.com/articles/s41467-024-45566-8)).  Meta\u2011learning approaches that treat unlabeled molecules as a bridge between in\u2011distribution and OOD space further densify the scarce labeled set, enabling the model to \u201cinterpolate\u201d across covariate shift and achieve significant gains on real\u2011world benchmarks\u202f([arxiv.org](https://arxiv.org/abs/2506.11877)).  Finally, calibrating probabilistic outputs (e.g., with the DIONYSUS framework) and explicitly evaluating models with scaffold\u2011 and cluster\u2011based splits helps identify over\u2011optimistic ID performance and guides the selection of architectures that remain reliable on novel chemotypes\u202f([rsc.org](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  Combining pretrained, attention\u2011rich GNNs, auxiliary\u2011task adaptation, transfer/meta\u2011learning, and rigorous OOD evaluation provides a comprehensive recipe for stronger generalization when predicting properties of new chemical entities.",
      "url": ""
    },
    {
      "title": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting",
      "text": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=8370ae3e-f2c3-44c8-9e8b-b5e96c227834)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nTransfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:26 February 2024# Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n* [David Buterez](#auth-David-Buterez-Aff1)[ORCID:orcid.org/0000-0001-6558-0833](https://orcid.org/0000-0001-6558-0833)[1](#Aff1),\n* [Jon Paul Janet](#auth-Jon_Paul-Janet-Aff2)[ORCID:orcid.org/0000-0001-7825-4797](https://orcid.org/0000-0001-7825-4797)[2](#Aff2),\n* [Steven J. Kiddle](#auth-Steven_J_-Kiddle-Aff3)[3](#Aff3),\n* [Dino Oglic](#auth-Dino-Oglic-Aff4)[4](#Aff4)&amp;\n* \u2026* [Pietro Li\u00f3](#auth-Pietro-Li_-Aff1)[ORCID:orcid.org/0000-0002-0540-5053](https://orcid.org/0000-0002-0540-5053)[1](#Aff1)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:1517(2024)[Cite this article](#citeas)\n* 32kAccesses\n* 77Citations\n* 5Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-45566-8/metrics)\n### Subjects\n* [Computational biology and bioinformatics](https://www.nature.com/subjects/computational-biology-and-bioinformatics)\n* [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n* [Quantum mechanics](https://www.nature.com/subjects/quantum-mechanics)\n## Abstract\nWe investigate the potential of graph neural networks for transfer learning and improving molecular property prediction on sparse and expensive to acquire high-fidelity data by leveraging low-fidelity measurements as an inexpensive proxy for a targeted property of interest. This problem arises in discovery processes that rely on screening funnels for trading off the overall costs against throughput and accuracy. Typically, individual stages in these processes are loosely connected and each one generates data at different scale and fidelity. We consider this setup holistically and demonstrate empirically that existing transfer learning techniques for graph neural networks are generally unable to harness the information from multi-fidelity cascades. Here, we propose several effective transfer learning strategies and study them in transductive and inductive settings. Our analysis involves a collection of more than 28 million unique experimental protein-ligand interactions across 37 targets from drug discovery by high-throughput screening and 12 quantum properties from the dataset QMugs. The results indicate that transfer learning can improve the performance on sparse tasks by up to eight times while using an order of magnitude less high-fidelity training data. Moreover, the proposed methods consistently outperform existing transfer learning strategies for graph-structured data on drug discovery and quantum mechanics datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-022-00501-8/MediaObjects/42256_2022_501_Fig1_HTML.png)\n### [An adaptive graph learning method for automated molecular interactions and properties predictions](https://www.nature.com/articles/s42256-022-00501-8?fromPaywallRec=false)\nArticle23 June 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-023-45269-y/MediaObjects/41598_2023_45269_Fig1_HTML.png)\n### [Binding affinity predictions with hybrid quantum-classical convolutional neural networks](https://www.nature.com/articles/s41598-023-45269-y?fromPaywallRec=false)\nArticleOpen access20 October 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-05905-z/MediaObjects/41586_2023_5905_Fig1_HTML.png)\n### [Computational approaches streamlining drug discovery](https://www.nature.com/articles/s41586-023-05905-z?fromPaywallRec=false)\nArticle26 April 2023\n## Introduction\nWe investigate the potential of graph neural networks (GNNs) for transfer learning and improved molecular property prediction in the context of funnels or screening cascades characteristic of drug discovery and/or molecular design. GNNs have emerged as a powerful and widely-used class of algorithms for molecular property prediction thanks to their natural ability to learn from molecular structures represented as atoms and bonds[1](#ref-CR1),[2](#ref-CR2),[3](https://www.nature.com/articles/s41467-024-45566-8#ref-CR3), as well as in the life sciences in general[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s41467-024-45566-8#ref-CR6). However, their potential for transfer learning is yet to be established. The screening cascade refers to a multi-stage approach where one starts with cheap and relatively noisy methods (high-throughput screening, molecular mechanics calculations, etc.) that allow for screening a large number of molecules. This is followed by increasingly accurate and more expensive evaluations that come with much lower throughput, up to the experimental characterisation of compounds. Individual stages or tiers in the screening funnel are, thus, used to make a reduction of the search space and focus the evaluation of more expensive properties on the promising regions. In this way, the funnel maintains a careful trade-off between the scale, cost, and accuracy. The progression from one tier to another is typically done manually by selecting subsets of molecules from the library screened at the previous stage or via a surrogate model that focuses the screening budget of the next step on the part of the chemical space around the potential hits. Such surrogate models are typically built using the data originating from a single tier and, thus, without leveraging measurements of different fidelity.\nFor efficient use of experimental resources, it is beneficial to have good predictive models operating on sparse datasets and guiding the high-fidelity evaluations relative to properties of interest. The latter is the most expensive part of the funnel and to efficiently support it, we consider it in a transfer learning setting designed to leverage low-fidelity observations to improve the effectiveness of predictive models on sparse and high-fidelity experimental data. In drug discovery applications of this setup, low-fidelity measurements can be seen as ground truth values that have been corrupted by noise, experimental or reading artefacts, or are simply performed using less precise but cheaper experiments. For quantum mechanics...",
      "url": "https://www.nature.com/articles/s41467-024-45566-8"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation",
      "text": "Enhancing Molecular Property Prediction with Auxiliary\nLearning and Task-Specific Adaptation\nVishal Dey\nThe Ohio State University, Columbus\ndey.78@osu.edu\nXia Ning\nThe Ohio State University, Columbus\nning.104@osu.edu\nAbstract\nPretrained Graph Neural Networks (GNNs) have been widely adopted for various\nmolecular property prediction tasks. Despite their ability to capture rich chemical\nknowledge, traditional finetuning of such pretrained GNNs on the target task\ncan lead to poor generalization. To address this, we explore the adaptation of\npretrained GNNs to the target task by jointly training them with multiple auxiliary\ntasks. This could enable the GNNs to learn both general and task-specific features,\nwhich may benefit the target task. However, an effective adaptation strategy\nneeds to determine the relevance of auxiliary tasks with the target task, which\nposes a major challenge. In this regard, we investigate multiple strategies to\nadaptively combine task gradients or learn task weights via bi-level optimization.\nOur experiments with state-of-the-art pretrained GNNs demonstrate the efficacy\nof our proposed methods, with improvements of up to 8.45% over finetuning.\nOverall, this suggests that incorporating auxiliary tasks along with target task\nfinetuning can be an effective way to improve the generalizability of pretrained\nGNNs for molecular property prediction tasks, and thus inspires future research.\n1 Introduction\nAccurate prediction of molecular properties is pivotal in drug discovery[1], as it accelerates the\nidentification of potential molecules with desired characteristics. Developing computational models\nfor property prediction relies on learning effective representations of molecules[2]. In this regard,\nGraph Neural Networks (GNNs) have shown impressive results in learning effective representations\nfor molecular property prediction tasks[3\u20137]. Inspired by the paradigm of pretraining followed by\nfinetuning in large language models (LLMs)[8, 9], molecular GNNs are often pretrained[10] on a\nlarge corpus of molecules (for literature review, refer to Appendix A.1), which might encompass\nirrelevant data for the target property prediction task (e.g., toxicity). This can lead the GNNs to learn\nfeatures that do not benefit the target task. Consequently, pretrained GNNs are finetuned with the\ntarget task to encode task-specific features. However, vanilla finetuning can potentially lead to poor\ngeneralization, particularly when dealing with diverse downstream tasks, limited data, and the need\nto generalize across varying scaffold distributions[11].\nTo improve generalization, auxiliary learning has recently garnered attention[12, 13], notably in\nthe domains of natural language processing (NLP)[14] and computer vision[15, 16]. Auxiliary\nlearning leverages informative signals from self-supervised tasks on unlabeled data, to improve the\nperformance on the target tasks (for a brief literature review, refer to Appendix A.2). Following\nthis line of work, we explore how to adapt pretrained molecular GNNs by combining widely-used\nself-supervised tasks with the target task using respective task-specific data (with self-supervised and\ntarget task labels). Our contribution lies in a preliminary investigation of multiple adaptation strategies\nfor pretrained molecular GNNs in molecular property prediction using well-established concepts\nof auxiliary learning. The significance of our contribution lies in addressing the limited benefit of\npretrained GNNs[17], and in improving generalizability across a diverse set of downstream tasks with\nlimited data. Overall, our proposed adaptation strategies improved the target task performance by as\nmuch as 8.45% over vanilla finetuning. Moreover, our findings indicate that the proposed adaptation\nDey et al., Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\n(Extended Abstract). Presented at the Second Learning on Graphs Conference (LoG 2023), Virtual Event,\nNovember 27\u201330, 2023.\nEnhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\nGNN\nPretraining\nTask 1\nPretraining\nTask \nPretraining\nTask \nRandomly Initialized GNN:\nPretrained GNN:\n(a) Pretraining Stage\nGNN\nTarget Auxiliary Auxiliary \nAdaptation Strategy\nTask-specific\nparamaeters\nPretrained GNN\n(b) Adaptation Stage\nFigure 1: Off-the-shelf available pretrained GNNs are transferred for target task-specific adaptation.\nstrategies are particularly effective in tasks with limited labeled data, which is a common challenge in\nmolecular property prediction tasks.\n2 Methods\nMotivated by the success of continued pretraining and adaptation in pretrained Large Language\nModels (LLMs) [18\u201320], we investigate adaptation of off-the-shelf pretrained molecular GNNs (e.g.,\nSupervised+ContextPred[10] denoted as Sup-C) to target molecular property prediction tasks. Via\nsuch an adaptation, we aim to leverage existing self-supervised (SSL) tasks designed for molecular\nGNNs and transfer learned knowledge from such tasks to the target task. We employ the existing SSL\ntasks typically used in molecular pretraining such as masked atom prediction (AM), edge prediction\n(EP), context prediction (CP), graph infomax (IG), and motif prediction (MP) (detailed in Appendix\nB.1). We refer to these tasks as auxiliary tasks. Intuitively, these auxiliary tasks can potentially\ncapture diverse chemical semantics and rich structural patterns at varying granularities. By utilizing\nSSL objectives on target task-specific data, auxiliary tasks augment the pretrained GNNs with richer\nrepresentations. Such representations, in turn, can improve the generalizability of the target property\nprediction task. Henceforth, the term \u201cGNN\u201d refers to off-the-shelf pretrained molecular GNN.\nFigure 1 presents an overview of the adaptation setup. Formally, we adapt a GNN with parameters \u0398\nto optimize the performance on the target task Tt. We achieve this by jointly training Tt with auxiliary\ntasks {Ta}\nk\ni=1:\nmin\n\u0398,\u03a8,\u03a6i\u2208{1..k}\nLt +\nX\nk\ni=1\nwiLa,i,\nwhere Lt and La,i denote the target task loss and i-th auxiliary task loss, respectively, and \u03a8 and\n\u03a6i\u2208{1,...,k} denotes task-specific learnable parameters for the target and k-auxiliary tasks, respec\u0002tively, and w indicates the influence of the auxiliary tasks on the target task. Through the above\noptimization, all the parameters are simultaneously updated in an end-to-end manner. Note that the\nabove optimization does not optimize w\u2013 we will introduce an approach that can additionally learn\nw. In fact, the key to effective adaptation lies in accurately determining w, such that the combined\ntask gradients can backpropagate relevant training signals to the shared GNN as follows:\n\u0398\n(t+1) := \u0398(t) \u2212 \u03b1\n\u0012\ngt +\nXk\ni=1\nwiga,i\u0013,\nwhere gt = \u2207\u0398Lt, and ga,i = \u2207\u0398La,i denote the gradients updating \u0398 from the target and i-th\nauxiliary task, respectively, and \u03b1 denotes the learning rate. We experiment with multiple strategies\nto adaptively combine task gradients, or learn adaptive w, as opposed to using fixed weights or\nconducting expensive grid-search to explore all possible w.\n2.1 Gradient Cosine Similarity (GCS)\nOne such strategy to meaningfully combine task gradients is based on gradient cosine similarity\n(GCS). Intuitively, GCS measures the alignment between task gradients during training, providing\ninsights into the relatedness of auxiliary tasks with the target task. High GCS indicates that the\nauxiliary tasks provide complementary information and thus, can benefit the target task. Conversely,\nlow GCS indicates potential orthogonality or even conflict between tasks. Thus, GCS can naturally\nbe used to quantify the relatedness of auxiliary tasks with the target task over the course of training.\nWe compute GCS and update \u0398 as:\n\u0398\n(t+1) := \u0398(t) \u2212 \u03b1\n\u0012\ngt +\nXk\ni=1\nmax (0, cos (gt, ga,i)) ga,i)\n\u0013\n,\n2\nEnhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\nwhere, in essence, we drop the tasks with ...",
      "url": "https://openreview.net/pdf/41e1a95e6fc281b863d44eb407ff5a07d8f83bef.pdf"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Evaluating generalizability of artificial intelligence models for molecular datasets",
      "text": "## References\n\n001. Green, A. G. et al. A convolutional neural network highlights mutations relevant to antimicrobial resistance in mycobacterium tuberculosis. _Nat. Commun._ **13**, 3817 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41467-022-31236-0) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20convolutional%20neural%20network%20highlights%20mutations%20relevant%20to%20antimicrobial%20resistance%20in%20mycobacterium%20tuberculosis&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-022-31236-0&volume=13&publication_year=2022&author=Green%2CAG)\n\n002. Kumar, V., Deepak, A., Ranjan, A. & Prakash, A. Lite-SeqCNN: a light-weight deep CNN architecture for protein function prediction. _IEEE/ACM Trans. Comput. Biol. Bioinform._ **20**, 2242\u20132253 (2023).\n\n     [Article](https://doi.org/10.1109%2FTCBB.2023.3240169) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Lite-SeqCNN%3A%20a%20light-weight%20deep%20CNN%20architecture%20for%20protein%20function%20prediction&journal=IEEE%2FACM%20Trans.%20Comput.%20Biol.%20Bioinform.&doi=10.1109%2FTCBB.2023.3240169&volume=20&pages=2242-2253&publication_year=2023&author=Kumar%2CV&author=Deepak%2CA&author=Ranjan%2CA&author=Prakash%2CA)\n\n003. Sanderson, T., Bileschi, M. L., Belanger, D. & Colwell, L. J. Proteinfer, deep neural networks for protein functional inference. _eLife_ **12**, e80942 (2023).\n\n     [Article](https://doi.org/10.7554%2FeLife.80942) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Proteinfer%2C%20deep%20neural%20networks%20for%20protein%20functional%20inference&journal=eLife&doi=10.7554%2FeLife.80942&volume=12&publication_year=2023&author=Sanderson%2CT&author=Bileschi%2CML&author=Belanger%2CD&author=Colwell%2CLJ)\n\n004. Bileschi, M. L. et al. Using deep learning to annotate the protein universe. _Nat. Biotechnol._ **40**, 932\u2013937 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41587-021-01179-w) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Using%20deep%20learning%20to%20annotate%20the%20protein%20universe&journal=Nat.%20Biotechnol.&doi=10.1038%2Fs41587-021-01179-w&volume=40&pages=932-937&publication_year=2022&author=Bileschi%2CML)\n\n005. Griffith, D. & Holehouse, A. S. Parrot is a flexible recurrent neural network framework for analysis of large protein datasets. _eLife_ **10**, e70576 (2021).\n\n     [Article](https://doi.org/10.7554%2FeLife.70576) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Parrot%20is%20a%20flexible%20recurrent%20neural%20network%20framework%20for%20analysis%20of%20large%20protein%20datasets&journal=eLife&doi=10.7554%2FeLife.70576&volume=10&publication_year=2021&author=Griffith%2CD&author=Holehouse%2CAS)\n\n006. Liu, X. Deep recurrent neural network for protein function prediction from sequence. Preprint at [https://arxiv.org/abs/1701.08318](https://arxiv.org/abs/1701.08318) (2017).\n\n007. Hill, S. T. et al. A deep recurrent neural network discovers complex biological rules to decipher RNA protein-coding potential. _Nucleic Acids Res._ **46**, 8105\u20138113 (2018).\n\n     [Article](https://doi.org/10.1093%2Fnar%2Fgky567) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20deep%20recurrent%20neural%20network%20discovers%20complex%20biological%20rules%20to%20decipher%20RNA%20protein-coding%20potential&journal=Nucleic%20Acids%20Res.&doi=10.1093%2Fnar%2Fgky567&volume=46&pages=8105-8113&publication_year=2018&author=Hill%2CST)\n\n008. Zhang, Z., Xu, M., Jamasb, A. R., Chenthamarakshan, V., Lozano, A., Das, P. & Tang, J. Protein representation learning by geometric structure pretraining. In _Proc. Eleventh International Conference on Learning Representations_ (2023).\n\n009. Somnath, V. R., Bunne, C. & Krause, A. Multi-scale representation learning on proteins. In _Advances in Neural Information Processing Systems_ Vol. 34 (eds Ranzato, M. et al.) 25244\u201325255 (Curran Associates, 2021).\n\n010. Jha, K., Saha, S. & Singh, H. Prediction of protein\u2013protein interaction using graph neural networks. _Sci. Rep._ **12**, 8360 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41598-022-12201-9) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Prediction%20of%20protein%E2%80%93protein%20interaction%20using%20graph%20neural%20networks&journal=Sci.%20Rep.&doi=10.1038%2Fs41598-022-12201-9&volume=12&publication_year=2022&author=Jha%2CK&author=Saha%2CS&author=Singh%2CH)\n\n011. Gao, Z. et al. Hierarchical graph learning for protein\u2013protein interaction. _Nat. Commun._ **14**, 1093 (2023).\n\n     [Article](https://doi.org/10.1038%2Fs41467-023-36736-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hierarchical%20graph%20learning%20for%20protein%E2%80%93protein%20interaction&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-023-36736-1&volume=14&publication_year=2023&author=Gao%2CZ)\n\n012. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_ **379**, 1123\u20131130 (2023).\n\n     [Article](https://doi.org/10.1126%2Fscience.ade2574) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4567681) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Evolutionary-scale%20prediction%20of%20atomic-level%20protein%20structure%20with%20a%20language%20model&journal=Science&doi=10.1126%2Fscience.ade2574&volume=379&pages=1123-1130&publication_year=2023&author=Lin%2CZ)\n\n013. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational protein engineering with sequence-based deep representation learning. _Nat. Methods_ **16**, 1315\u20131322 (2019).\n\n     [Article](https://doi.org/10.1038%2Fs41592-019-0598-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Unified%20rational%20protein%20engineering%20with%20sequence-based%20deep%20representation%20learning&journal=Nat.%20Methods&doi=10.1038%2Fs41592-019-0598-1&volume=16&pages=1315-1322&publication_year=2019&author=Alley%2CEC&author=Khimulya%2CG&author=Biswas%2CS&author=AlQuraishi%2CM&author=Church%2CGM)\n\n014. Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _Proc. Natl Acad. Sci. USA_ **118**, e2016239118 (2021).\n\n     [Article](https://doi.org/10.1073%2Fpnas.2016239118) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Biological%20structure%20and%20function%20emerge%20from%20scaling%20unsupervised%20learning%20to%20250%20million%20protein%20sequences&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2016239118&volume=118&publication_year=2021&author=Rives%2CA)\n\n015. Madani, A. et al. Large language models generate functional protein sequences across diverse families. _Nat. Biotechnol._ **41**, 1099\u20131106 (2023).\n\n     [Article](https://doi.org/10.1038%2Fs41587-022-01618-2) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Large%20language%20models%20generate%20functional%20protein%20sequences%20across%20diverse%20families&journal=Nat.%20Biotechnol.&doi=10.1038%2Fs41587-022-01618-2&volume=41&pages=1099-1106&publication_year=2023&author=Madani%2CA)\n\n016. Notin, P. et al. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In _Proc. 39th International Conference on Machine Learning_, Vol. 162 (eds Chaudhuri, K. et al.) 16990\u201317017 (PMLR, 2022).\n\n017. Wright, B. W., Yi, Z., Weissman, J. S. & Chen, J. The dark proteome: translation from noncanonical open reading frames. _Trends Cell Biol._ **32**, 243\u2013258 (2022).\n\n     [Article](https://doi.org/10.1016%2Fj.tcb.2021.10.010) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20dark%20proteome%3A%20translation%20from%20noncanonical%20open%20reading%20frames&journal=Trends%20Cell%20Biol.&doi=10.1016%2Fj.tcb.2021.10.010&volume=32&pages=243-258&publication_year=2022&author=Wright%2CBW&author=Yi%2CZ&author=Weissman%2CJS&author=Chen%2CJ)\n\n018. Liu, J. et al. Towards out-of-distribution generalization: a survey. Preprint at [htt...",
      "url": "https://www.nature.com/articles/s42256-024-00931-6"
    },
    {
      "title": "Molecular property prediction in the ultra\u2010low data regime",
      "text": "Molecular property prediction in the ultra\u2010low data regime | Communications Chemistry\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Communications Chemistry](https://media.springernature.com/full/nature-cms/uploads/product/commschem/header-3dc28429486e0d2c8f49fd9baf5afa40.svg)](https://www.nature.com/commschem)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42004-025-01592-1?error=cookies_not_supported&code=080dccb6-b4aa-47ba-840b-aba438a0f70a)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42004)\n* [RSS feed](https://www.nature.com/commschem.rss)\nMolecular property prediction in the ultra\u2010low data regime\n[Download PDF](https://www.nature.com/articles/s42004-025-01592-1.pdf)\n[Download PDF](https://www.nature.com/articles/s42004-025-01592-1.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:08 July 2025# Molecular property prediction in the ultra\u2010low data regime\n* [Basem A. Eraqi](#auth-Basem_A_-Eraqi-Aff1)[ORCID:orcid.org/0000-0003-2911-221X](https://orcid.org/0000-0003-2911-221X)[1](#Aff1),\n* [Dmitrii Khizbullin](#auth-Dmitrii-Khizbullin-Aff2)[2](#Aff2),\n* [Shashank S. Nagaraja](#auth-Shashank_S_-Nagaraja-Aff1)[ORCID:orcid.org/0000-0003-4930-6513](https://orcid.org/0000-0003-4930-6513)[1](#Aff1)&amp;\n* \u2026* [S. Mani Sarathy](#auth-S__Mani-Sarathy-Aff1)[ORCID:orcid.org/0000-0002-3975-6206](https://orcid.org/0000-0002-3975-6206)[1](#Aff1)Show authors\n[*Communications Chemistry*](https://www.nature.com/commschem)**volume8**, Article\u00a0number:201(2025)[Cite this article](#citeas)\n* 8813Accesses\n* 1Citations\n* 21Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42004-025-01592-1/metrics)\n### Subjects\n* [Chemical engineering](https://www.nature.com/subjects/chemical-engineering)\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n## Abstract\nData scarcity remains a major obstacle to effective machine learning in molecular property prediction and design, affecting diverse domains such as pharmaceuticals, solvents, polymers, and energy carriers. Although multi-task learning (MTL) can leverage correlations among properties to improve predictive performance, imbalanced training datasets often degrade its efficacy through negative transfer. Here, we present adaptive checkpointing with specialization (ACS), a training scheme for multi-task graph neural networks that mitigates detrimental inter-task interference while preserving the benefits of MTL. We validate ACS on multiple molecular property benchmarks, where it consistently surpasses or matches the performance of recent supervised methods. To illustrate its practical utility, we deploy ACS in a real-world scenario of predicting sustainable aviation fuel properties, showing that it can learn accurate models with as few as 29 labeled samples. By enabling reliable property prediction in low-data regimes, ACS broadens the scope and accelerates the pace of artificial intelligence-driven materials discovery and design.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-55082-4/MediaObjects/41467_2024_55082_Fig1_HTML.png)\n### [Multi-channel learning for integrating structural hierarchies into context-dependent molecular representation](https://www.nature.com/articles/s41467-024-55082-4?fromPaywallRec=false)\nArticleOpen access06 January 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-022-00790-5/MediaObjects/42004_2022_790_Fig1_HTML.png)\n### [Transferring chemical and energetic knowledge between molecular systems with machine learning](https://www.nature.com/articles/s42004-022-00790-5?fromPaywallRec=false)\nArticleOpen access13 January 2023\n## Introduction\nMachine learning (ML)-based molecular property prediction models can significantly accelerate the de novo design of high-performance molecules and mixtures by providing accurate property predictions. This data-driven approach explores the chemical space defined by learned model representations, enabling the discovery of materials that fulfill specific application requirements. However, the efficacy of such models relies heavily on predictive accuracy, which is constrained by the availability and quality of training data[1](https://www.nature.com/articles/s42004-025-01592-1#ref-CR1),[2](https://www.nature.com/articles/s42004-025-01592-1#ref-CR2). Across many practical domains\u2014including pharmaceutical drugs[3](https://www.nature.com/articles/s42004-025-01592-1#ref-CR3), chemical solvents[2](https://www.nature.com/articles/s42004-025-01592-1#ref-CR2), polymers[4](https://www.nature.com/articles/s42004-025-01592-1#ref-CR4), and green energy carriers[5](https://www.nature.com/articles/s42004-025-01592-1#ref-CR5)\u2014the scarcity of reliable, high-quality labels impedes the development of robust molecular property predictors.\nMulti-task learning (MTL) has been proposed to alleviate data bottlenecks by exploiting correlations among related molecular properties (hereafter termed*tasks*)[6](#ref-CR6),[7](#ref-CR7),[8](https://www.nature.com/articles/s42004-025-01592-1#ref-CR8). Through inductive transfer, MTL leverages the training signals or learned representations from one task to improve another, allowing the model to discover and utilize shared structures for more accurate predictions across all tasks. In practice, however, MTL is frequently undermined by negative transfer (NT)[7](https://www.nature.com/articles/s42004-025-01592-1#ref-CR7): performance drops that occur when updates driven by one task are detrimental to another. Prior studies linked NT primarily to*low task relatedness*and the associated*gradient conflicts*in shared parameters[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42004-025-01592-1#ref-CR11). The resulting gradient conflicts can reduce the overall benefits of MTL or even degrade performance.\nBeyond task dissimilarity, NT can also arise from architectural or optimization mismatches[12](https://www.nature.com/articles/s42004-025-01592-1#ref-CR12). Capacity mismatch occurs when the shared backbone lacks sufficient flexibility to support divergent task demands, leading to overfitting on some tasks and underfitting on others. Similarly, when tasks exhibit different optimal learning rates, shared training may update parameters at incompatible magnitudes, destabilizing convergence[11](https://www.nature.com/articles/s42004-025-01592-1#ref-CR11). Additionally, data distribution differences, such as temporal and spatial disparities, can impede effective knowledge transfer[13](https://www.nature.com/articles/s42004-025-01592-1#ref-CR13),[14](https://www.nature.com/articles/s42004-025-01592-1#ref-CR14). Temporal differences\u2014such as variations in the measurement years of molecular data\u2014can lead to inflated performance estimates if not properly accounted for. This inflation has been shown to re...",
      "url": "https://www.nature.com/articles/s42004-025-01592-1"
    },
    {
      "title": "MolE: a foundation model for molecular graphs using disentangled attention",
      "text": "MolE: a foundation model for molecular graphs using disentangled attention | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-53751-y?error=cookies_not_supported&code=bbab3c64-c482-4ea2-8fa0-7cf631045701)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nMolE: a foundation model for molecular graphs using disentangled attention\n[Download PDF](https://www.nature.com/articles/s41467-024-53751-y.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-53751-y.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:12 November 2024# MolE: a foundation model for molecular graphs using disentangled attention\n* [Oscar M\u00e9ndez-Lucio](#auth-Oscar-M_ndez_Lucio-Aff1)[ORCID:orcid.org/0000-0003-0345-1168](https://orcid.org/0000-0003-0345-1168)[1](#Aff1),\n* [Christos A. Nicolaou](#auth-Christos_A_-Nicolaou-Aff1-Aff2)[ORCID:orcid.org/0000-0002-1466-6992](https://orcid.org/0000-0002-1466-6992)[1](#Aff1)[nAff2](#nAff2)&amp;\n* [Berton Earnshaw](#auth-Berton-Earnshaw-Aff1)[ORCID:orcid.org/0000-0002-9728-2408](https://orcid.org/0000-0002-9728-2408)[1](#Aff1)\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:9431(2024)[Cite this article](#citeas)\n* 25kAccesses\n* 30Citations\n* 10Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-53751-y/metrics)\n### Subjects\n* [Computational models](https://www.nature.com/subjects/computational-models)\n* [Machine learning](https://www.nature.com/subjects/machine-learning)\n## Abstract\nModels that accurately predict properties based on chemical structure are valuable tools in the chemical sciences. However, for many properties, public and private training sets are typically small, making it difficult for models to generalize well outside of the training data. Recently, this lack of generalization has been mitigated by using self-supervised pretraining on large unlabeled datasets, followed by finetuning on smaller, labeled datasets. Inspired by these advances, we report MolE, a Transformer architecture adapted for molecular graphs together with a two-step pretraining strategy. The first step of pretraining is a self-supervised approach focused on learning chemical structures trained on \\~842 million molecular graphs, and the second step is a massive multi-task approach to learn biological information. We show that finetuning models that were pretrained in this way perform better than the best published results on 10 of the 22 ADMET (absorption, distribution, metabolism, excretion and toxicity) tasks included in the Therapeutic Data Commons leaderboard (c. September 2023).\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-55082-4/MediaObjects/41467_2024_55082_Fig1_HTML.png)\n### [Multi-channel learning for integrating structural hierarchies into context-dependent molecular representation](https://www.nature.com/articles/s41467-024-55082-4?fromPaywallRec=false)\nArticleOpen access06 January 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-022-00580-7/MediaObjects/42256_2022_580_Fig1_HTML.png)\n### [Large-scale chemical language representations capture molecular structure and properties](https://www.nature.com/articles/s42256-022-00580-7?fromPaywallRec=false)\nArticle21 December 2022\n## Introduction\nMachine learning has been successfully applied to chemical sciences for many decades[1](https://www.nature.com/articles/s41467-024-53751-y#ref-CR1). In particular, molecular property prediction has been critical in successfully advancing material and drug discovery projects[2](https://www.nature.com/articles/s41467-024-53751-y#ref-CR2). Nonetheless, a major challenge in this area still is to represent a molecule in a way that is compatible with machine learning algorithms with minimum information loss. Initially, molecules were represented in terms of their physicochemical properties (e.g., partition coefficient) or information that can be obtained from the molecular formula such as molecular weight or number of heteroatoms[3](https://www.nature.com/articles/s41467-024-53751-y#ref-CR3). While this approach was successful for the first quantitative structure-activity relationship (QSAR) studies[4](https://www.nature.com/articles/s41467-024-53751-y#ref-CR4), it used only global properties of the molecule, and not the chemical structure itself. With time, molecules were described in more sophisticated ways using molecular fingerprints such as MACCS keys[5](https://www.nature.com/articles/s41467-024-53751-y#ref-CR5)and Extended Connectivity Fingerprints (ECFPs)[6](https://www.nature.com/articles/s41467-024-53751-y#ref-CR6)among others. These molecular fingerprints encode substructures of the molecules either in the form of preset chemical groups or as atom environments. Despite their successful use in numerous QSAR applications, molecular fingerprints fail to preserve the complete molecular graph topology especially when using a small fingerprint length[6](https://www.nature.com/articles/s41467-024-53751-y#ref-CR6).\nFollowing recent advances in natural language modeling, it was noted that molecules could be used directly as input for predictive models in the form of SMILES[7](https://www.nature.com/articles/s41467-024-53751-y#ref-CR7),[8](https://www.nature.com/articles/s41467-024-53751-y#ref-CR8), a string-based representation developed to store and search molecular structures in a fast and easy way. SMILES have been used as inputs for deep learning architectures such as recurrent neural networks (RNNs)[9](https://www.nature.com/articles/s41467-024-53751-y#ref-CR9)and Transformers[10](#ref-CR10),[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](https://www.nature.com/articles/s41467-024-53751-y#ref-CR14), though they suffer from the fact that molecules do not have unique SMILES representations. Other types of string-based representations have been proposed[15](https://www.nature.com/articles/s41467-024-53751-y#ref-CR15), e.g., Self-Referencing Embedded Strings (SELFIES)[16](https://www.nature.com/articles/s41467-024-53751-y#ref-CR16), which encode the molecular graph in the form of a Chomsky type-2 free context grammar appropriate for deep learning applications. An alternative is to use a graph representation of the molecule where nodes represent atoms and edges represent bonds. Such an approach is compatible with graph neural networks (GNNs), which have been extensively used for molecular property prediction[17](https://www.nature.com/articles/s41467-024-53751-y#ref-CR17),[18](http...",
      "url": "https://www.nature.com/articles/s41467-024-53751-y?error=cookies_not_supported&code=bfab6ef9-5afc-4b79-8449-99eeca160291"
    }
  ]
}