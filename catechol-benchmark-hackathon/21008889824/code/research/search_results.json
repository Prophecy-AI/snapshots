{
  "query": "How can ensemble diversity and model stacking reduce generalization gap in chemical property prediction with distribution shift?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensemble diversity and model\u2011stacking tackle the **generalization gap** that appears when a chemical\u2011property model is applied to compounds that lie outside the training distribution (OOD).  By training a set of base learners that capture complementary aspects of molecular information\u2014e.g., random\u2011forest, XGBoost, gradient\u2011boosting, graph\u2011neural\u2011networks, or transformer\u2011based language models\u2014each model makes different errors on OOD samples.  When these heterogeneous predictions are combined, the errors tend to cancel, yielding a more stable and accurate overall forecast\u202f([NVIDIA\u202f2022](https://ogb.stanford.edu/paper/neurips2022/pcqm4mv2_NVIDIA-PCQM4Mv2.pdf)).  The heterogeneity also expands the **applicability domain** of the system, because the ensemble can reliably extrapolate to regions of chemical space that any single model would consider out\u2011of\u2011range\u202f([PMC\u202f2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC12290627)).\n\nStacking formalises this diversity by training a **meta\u2011learner** on the predictions of the base models.  The meta\u2011model learns how to weight each base predictor according to the input\u2019s characteristics, effectively performing a data\u2011dependent \u201cblending\u201d that corrects systematic biases of individual learners.  In practice, stacking ensembles have been shown to outperform each constituent model on both in\u2011distribution and OOD test sets, reducing the mean absolute error for mechanical\u2011property prediction of high\u2011entropy alloys and for quantum\u2011chemical property prediction of small molecules\u202f([Frontiers\u202f2025](https://doi.org/10.3389/fmats.2025.1601874);\u202f[OGB\u202f2022](https://ogb.stanford.edu/paper/neurips2022/pcqm4mv2_NVIDIA-PCQM4Mv2.pdf)).  Moreover, k\u2011fold cross\u2011validation ensembles used for uncertainty estimation provide calibrated confidence intervals that further narrow the gap between predicted and true performance on unseen chemistry\u202f([J.\u202fCheminformatics\u202f2023](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9)).\n\nTogether, **diverse base models** supply a rich set of perspectives on molecular structure, while **stacking** learns to exploit those perspectives adaptively.  This combination mitigates covariate shift, improves robustness, and narrows the generalization gap that typically hampers chemical\u2011property prediction under distribution shift.",
      "url": ""
    },
    {
      "title": "Stacking Ensemble Neural Network for Chemical Safety Assessment",
      "text": "Stacking Ensemble Neural Network for Chemical Safety Assessment: A Case Study of Thyroid Peroxidase and Natural Product Screening - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1021/acsomega.5c02188)\n* [](pdf/ao5c02188.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![ACS Omega logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-acsomega.png)\nACS Omega\n. 2025 Jul 10;10(28):30450\u201330466. doi:[10.1021/acsomega.5c02188](https://doi.org/10.1021/acsomega.5c02188)\n# Stacking Ensemble\nNeural Network for Chemical Safety\nAssessment: A Case Study of Thyroid Peroxidase and Natural Product\nScreening\n[Darlene Nabila Zetta](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Zetta DN\"[Author]>)\n### Darlene Nabila Zetta\n\u2020Graduate\nSchool in the Program of Pharmaceutical Sciences, Faculty of Pharmaceutical\nSciences, Khon Kaen University, Khon Kaen\n40002, Thailand\nFind articles by[Darlene Nabila Zetta](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Zetta DN\"[Author]>)\n\u2020,[Tarapong Srisongkram](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Srisongkram T\"[Author]>)\n### Tarapong Srisongkram\n\u2021Division\nof Pharmaceutical Chemistry, Faculty of Pharmaceutical Sciences, Khon Kaen University, Khon Kaen\n40002, Thailand\nFind articles by[Tarapong Srisongkram](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Srisongkram T\"[Author]>)\n\u2021,\\*\n* Author information\n* Article notes\n* Copyright and License information\n\u2020Graduate\nSchool in the Program of Pharmaceutical Sciences, Faculty of Pharmaceutical\nSciences, Khon Kaen University, Khon Kaen\n40002, Thailand\n\u2021Division\nof Pharmaceutical Chemistry, Faculty of Pharmaceutical Sciences, Khon Kaen University, Khon Kaen\n40002, Thailand\n\\*\nE-mail:tarasri@kku.ac.th.\nReceived 2025 Mar 8; Accepted 2025 Jun 27; Revised 2025 May 16; Collection date 2025 Jul 22.\n\u00a92025 The Authors. Published by American Chemical Society\nThis article is licensed under CC-BY-NC-ND 4.0\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC12290627\u00a0\u00a0PMID:[40727784](https://pubmed.ncbi.nlm.nih.gov/40727784/)\n## Abstract\nStacking ensemble\nlearning is a method to improve model\ngeneralization\nand robustness. Deep neural networks have demonstrated significant\npotential for predicting chemical properties due to their effectiveness\nin learning complex patterns within the chemical space. Nevertheless,\nan individual model may rely on a single molecular feature set that\nmight not explicitly explain all of the relationships between drugs\nand targets. Integrating a stacking ensemble with deep learning (DL)\nand various molecular features could potentially enhance the learning\nprocess and improve the ability to capture complex relationships between\nmolecular structures and bioactivities. Chemicals binding to thyroid\nperoxidase (TPO) are associated with thyroid dysfunction, highlighting\nthe importance of assessing their potential risks to human health\nand the environment. In this study, we developed a novel stacking\nensemble neural network model to predict TPO inhibitory activity.\nThis model integrates convolutional neural networks, bidirectional\nlong short-term memory networks, and attention mechanisms combined\nwith top-performing molecular fingerprints to generate three probability\nfeatures. These features were used as inputs in a meta-decision model,\nenhancing learning probability. The meta-model was validated through\ny-randomization, ensuring that the model does not produce outputs\nrandomly. The applicability domain of this model was also assessed\nto affirm the reliability and trustworthiness of each prediction.\nThe final attention-based meta-model achieved a recall of 0.55, specificity\nof 0.95, Matthews correlation coefficient of 0.56, area under the\ncurve of 0.85, balanced accuracy of 0.75, and precision of 0.70. Furthermore,\nthe developed model was generalized to other external test sets, effectively\npredicting TPO inhibition and identifying potentially toxic compounds\nfrom a selected Thai indigenous vegetable. These findings will contribute\nto the application of stacking ensemble neural networks in the toxicity\nscreening of chemical compounds, enhancing their learning ability\nto capture more diverse chemical risk assessments.\n![graphic file with name ao5c02188_0016.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/a52d/12290627/f379643ac25b/ao5c02188_0016.jpg)\n![graphic file with name ao5c02188_0014.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/a52d/12290627/f379643ac25b/ao5c02188_0014.jpg)\n## 1. Introduction\nStacking ensemble learning\nis an advanced machine learning (ML)\ntechnique that combines multiple ML models into a single framework.*\u2212*The key idea behind this approach is to take advantage of the strengths\nof each model, allowing them to contribute to better overall performance. This approach mirrors how we often learn new\nskills by building on previous knowledge and experiences, making the\nprocess more efficient. This concept, also known as meta-model, focuses\non solving problems more efficiently by building upon prior knowledge.\nIn a meta-model, extracting knowledge from different base models and\ntheir features guides model development and decision-making with greater\nrobustness and accuracy. As a result, stacking builds on the individual\nstrengths of each base model to improve accuracy and reliability.\nIn essence, the concept of the meta-modeling framework aims to improve\nlearning efficiency by leveraging prior model probability outputs,\nthereby reducing the need for manual fine-tuning and minimizing trial-and-error.\nRecently, stacking ensemble learning has\ngained attention as a\npromising concept that helps models learn better to a variety of tasks\nmore efficiently.*,\u2212*Advancing this\nconcept, stacked neural networks have demonstrated higher accuracy\nin previous studies. By mimicking brain\nfunction, neural networks consist of interconnected and interacting\nneurons that work together to produce outputs. Specialized architectures like convolutional neural networks\n(CNNs) are especially effective for computer vision tasks and are\nconsidered among the best techniques for learning spatial information.*,*This architecture is also capable of identifying spatial patterns\nin molecular feature representation.*,*Meanwhile,\nbidirectional long short-term memory (BiLSTM) networks are particularly\nuseful for capturing relationships in sequential data, as they process\ninformation in both forward and backward directions with short-term\nmemory gates. This mode...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12290627"
    },
    {
      "title": "Heterogenous Ensemble Of Models For Molecular Property Prediction",
      "text": "Sajad Darabi, Shayan Fazeli, Jiwei Liu, Alexandre Milesi, Jean-Fran\u00e7ois Puget, Gilberto Titericz Heterogenous Ensemble Of Models For Molecular Property Prediction https://ogb.stanford.edu/paper/neurips2022/pcqm4mv2_NVIDIA-PCQM4Mv2.pdf\nHeterogenous Ensemble Of Models For Molecular Property Prediction\nSajad Darabi, Shayan Fazeli, Jiwei Liu, Alexandre Milesi, Jean-Fran\u00e7ois Puget, Gilberto Titericz\n2022-11-18\nHETEROGENOUS ENSEMBLE OF MODELS FOR MOLECULAR\nPROPERTY PREDICTION\nTECHNICAL REPORT\nSajad Darabi\nNVIDIA\nUSA\nsdarabi@nvidia.com\nShayan Fazeli\nUCLA\nUSA\nshayan@cs.ucla.edu\nJiwei Liu\nNVIDIA\nUSA\njiweil@nvidia.com\nAlexandre Milesi\nNVIDIA\nFrance\nalexandrem@nvidia.com\nPawel Morkisz\nNVIDIA\nUSA\npmorkisz@nvidia.com\nJean-Fran\u00e7ois Puget\nNVIDIA\nFrance\njpuget@nvidia.com\nGilberto Titericz\nNVIDIA\nBrazil\ngtitericz@nvidia.com\nABSTRACT\nPrevious works have demonstrated the importance of considering different modalities on molecules,\neach of which provide a varied granularity of information for downstream property prediction tasks.\nOur method combines variants of the recent TransformerM architecture with Transformer, GNN, and\nResNet backbone architectures. Models are trained on the 2D data, 3D data, and image modalities of\nmolecular graphs. We ensemble these models with a HuberRegressor. The models are trained on 4\ndifferent train/validation splits of the original train + valid datasets. This yields a winning solution to\nthe 2nd edition of the OGB Large-Scale Challenge (2022) on the PCQM4Mv2 molecular property\nprediction dataset. Our proposed method achieves a test-challenge MAE of 0.0723 and a validation\nMAE of 0.07145. Total inference time for our solution is less than 2 hours. We open-source our code\nat github.com/jfpuget/NVIDIA-PCQM4Mv2.\nKeywords GNN \u00b7 Transformer \u00b7 OGB-LSC \u00b7 PCQM4Mv2 \u00b7 DFT \u00b7 Drug Discovery\n1 Introduction\nThe OGB Large-Scale Challenge (LSC) [Hu et al., 2021] is a Machine Learning (ML) challenge to predict a quantum\nchemical property, the HUMO-LUMO gap of small molecules. This ground truth is obtained via a density-functional\ntheory (DFT) computation which is known to be time-consuming and could take several hours, even for small molecules.\nWith the rapid advancement of machine learning technology, it is promising to use fast, GPU-accelerated and accurate\nML models to replace this expensive DFT optimization process.\nThe PCQM4Mv2 dataset, based on the PubChemQC project Nakata and Shimazaki [2017], provides us with a well\u0002defined ML task of predicting the HOMO-LUMO gap of molecules given their 2D molecular graphs. Each molecule\nhas two natural views. The 2D graph incorporates topological structures defined by bonds, and the 3D view provides\nspatial information that better reflects the geometry and spatial relation of the different bonds in the molecule. In the\nPCQM4Mv2 dataset, additional 3D structures are also provided for training molecules. Further, in multiple domains,\nthere has been a tendency to convert data modalities to images such that high-capacity and expressive vision models\ncould be used to learn the underlying patterns. Although images do not correspond to the data\u2019s natural view, they\nhave shown promising results kag [2021]. We also consider images of the molecule as an alternative view for training\nvision-based models used in our ensemble.\nOur solution is an ensemble of 39 checkpoints from 10 different models. Before training these models, we performed\nexploratory data analysis and found a significant distribution shift between the train dataset and the other datasets (valid,\ntest-dev, and test-challenge). The train dataset contains molecules containing at most 20 heavy atoms, while the other\nHeterogenous Ensemble of of Models for Molecular Property Prediction TECHNICAL REPORT\n3 datasets contain molecules up to 51 heavy atoms. We made the hypothesis that training on the proposed train split\nwould lead to mediocre performance on large molecules (more than 20 atoms).\nWe decided to use different train/validation splits by creating 24 folds from the combined train+valid datasets. We then\ntrained a model on 23 folds and used the last fold for validation. Ideally, one would train 24 models that way, using\neach fold as a validation fold in turn. This is the usual k-folds cross-validation, with k = 24. However, for practical\nrunning time considerations, we decided to only use 4 folds as validation folds, i.e. training 4 fold models. Each model\nis trained by selecting one of the first 4 folds as validation fold, and using the remaining 23 folds as training data.\nWhenever we discuss folds in the remainder of this report we refer to the aforementioned 4 validation folds.\nIn the following sections of this report, we describe the model variants trained for each data modality, followed by a a\nsection describing how these models were ensembled for the final test-challenge prediction.\n2 Transformer-M variants\nFollowing prior work incorporating 2D and 3D information via bias terms in the Transformer self-attention [Ying et al.,\n2021], we use the recently proposed random structural channels between 2D/3D in Transformer-M [Luo et al., 2022] as\nour backbone architecture. In this method, the same Graphormer [Ying et al., 2021] backbone architecture is used for\nboth channels.\nGiven the input molecule G2D/3D, our encoder outputs a latent representation z = f(G2D/3D). We leverage the provided\n3D SDF file to extract the corresponding 2D & 3D spatial atom positions for this model. In subsequent subsections, we\ndescribe the variants used.\nAll hyperparameters not mentioned in the following sections (dropout, gradient clipping, weight decay, learning rate\nschedule, number of kernels) were kept the same as described in the Transformer-M work.\n2.1 Transformer-Mbase\nwithout_denoising\nThe first variant is a 12-layers, 768-wide model that includes randomly-activated structural channels with probabilities\n1\n4\nfor 2D, 1\n2\nfor 3D, and 1\n4\nfor 2D+3D. In this variant, the denoising task was not included, but the input atom positions\nprovided by the organizers were perturbed with gaussian noise of magnitude 0.2 in the 3D channel. This variant was\ntrained once for each of the 4 folds, for 400 epochs with batch size 1024, on an NVIDIA DGX-1 (8 V100-SXM2-16GB\n160W).\n2.2 Transformer-Mlarge\nwith_denoising\nThe second variant is similar to the first one, except for its 18 layers, and the inclusion of the atom position denoising\ntask, using a cosine similarity loss weighted equally to the graph prediction loss. Stochastic depth was enabled with a\nprobability of 10%. This variant was trained once for each of the 4 folds, for 400 epochs with batch size 1024, on an\nNVIDIA DGX-2H (16 V100-SXM3-32GB 300W).\n2.3 Transformer-Mlarge\nbaseline\nIn this variant, we train the Transformer-M model without any modifications. The random channels probabilities are set\nto the original weights 1\n5\n,\n1\n5\n,\n3\n5\nfor 2D, 3D, and 2D+3D respectively. The training setup is the same as previous variants,\nexcept the training lasts 454 epochs. This variant was trained twice with different seeds on the 4 folds.\n2.4 Transformer-Mlarge\nDirichlet\nIn this variant, we set the random channels probabilities to follow a Dirichlet distribution with weights 1\n5\n,\n1\n5\n,\n3\n5\nfor 2D,\n3D, and 2D+3D respectively. The training setup is the same as previous variants, except the training lasts 454 epochs.\nThis variant was trained once on the 4 folds.\n2.5 Transformer-M with Knowledge Guided Regularization\nResearch in the domain of molecular property prediction has shown that incorporating additional knowledge-based\ninformation could lead to considerable performance improvements.\n2\nHeterogenous Ensemble of of Models for Molecular Property Prediction TECHNICAL REPORT\nSpecifically, the observations in Li et al. [2022] show that pre-training a model on molecule fingerprints and descriptors\nconfigured as a mask-and-predict pretext task can result in significant improvements, and with merely relying on 2D\ninformation even surpassing most o...",
      "url": "https://ogb.stanford.edu/paper/neurips2022/pcqm4mv2_NVIDIA-PCQM4Mv2.pdf"
    },
    {
      "title": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation",
      "text": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00709-9?)\n# Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 April 2023\n* Volume\u00a015, article\u00a0number49, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nLarge-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n* [Thomas-Martin Dutschmann](#auth-Thomas_Martin-Dutschmann-Aff1)[1](#Aff1),\n* [Lennart Kinzel](#auth-Lennart-Kinzel-Aff1)[1](#Aff1),\n* [Antonius ter Laak](#auth-Antonius-Laak-Aff2)[2](#Aff2)&amp;\n* \u2026* [Knut Baumann](#auth-Knut-Baumann-Aff1)[1](#Aff1)Show authors\n* 7359Accesses\n* 50Citations\n* 3Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9/metrics)\n## Abstract\nIt is insightful to report an estimator that describes how certain a model is in a prediction, additionally to the prediction alone. For regression tasks, most approaches implement a variation of the ensemble method, apart from few exceptions. Instead of a single estimator, a group of estimators yields several predictions for an input. The uncertainty can then be quantified by measuring the disagreement between the predictions, for example by the standard deviation. In theory, ensembles should not only provide uncertainties, they also boost the predictive performance by reducing errors arising from variance. Despite the development of novel methods, they are still considered the \u201cgolden-standard\u201d to quantify the uncertainty of regression models. Subsampling-based methods to obtain ensembles can be applied to all models, regardless whether they are related to deep learning or traditional machine learning. However, little attention has been given to the question whether the ensemble method is applicable to virtually all scenarios occurring in the field of cheminformatics. In a widespread and diversified attempt, ensembles are evaluated for 32 datasets of different sizes and modeling difficulty, ranging from physicochemical properties to biological activities. For increasing ensemble sizes with up to 200 members, the predictive performance as well as the applicability as uncertainty estimator are shown for all combinations of five modeling techniques and four molecular featurizations. Useful recommendations were derived for practitioners regarding the success and minimum size of ensembles, depending on whether predictive performance or uncertainty quantification is of more importance for the task at hand.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-45630-5?as&#x3D;webp)\n### [Ensemble Models](https://link.springer.com/10.1007/978-3-031-45630-5_12?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-48317-7?as&#x3D;webp)\n### [Ensemble Methods for Time Series Forecasting](https://link.springer.com/10.1007/978-3-319-48317-7_13?fromPaywallRec=false)\nChapter\u00a9 2017\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-12240-8?as&#x3D;webp)\n### [Ensemble Models Using Symbolic Regression and Genetic Programming for Uncertainty Estimation in ESG and Alternative Investments](https://link.springer.com/10.1007/978-3-031-12240-8_5?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://jcheminf.biomedcentral.com/subjects/applied-probability)\n* [Applied Statistics](https://jcheminf.biomedcentral.com/subjects/applied-statistics)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://jcheminf.biomedcentral.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Statistical Theory and Methods](https://jcheminf.biomedcentral.com/subjects/statistical-theory-and-methods)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nMachine learning (ML) for drug design purposes holds a long tradition\u00a0[[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR1)], but has recently started to gain further attention due to the success of deep learning (DL)\u00a0[[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR2)]. Yet, the prediction of chemical properties and activities is only one step in a long and resource-intense process of drug design, discovery, and development. When developing ML models, predictions alone are not sufficient and require further analysis\u00a0[[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR3)]. During model construction and testing, errors made by the model can easily be evaluated since the true target values are known. The error distribution allows the estimation of the quality of the model, but cannot be applied when predicting values for new compounds with unknown target values. In this case, it is good practice to provide an estimate of the uncertainty associated with the prediction. Measures quantifying the predictive uncertainty can be used to set a threshold which defines the model\u2019s applicability domain. The latter is defined as follows: \u201cThe applicability domain of a (Q)SAR model is the response and chemical structure space in which the model makes predictions with a given reliability.\u201d\u00a0[[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR4)]. The reliability of a model can either be addressed by quantifying its confidence, or, conversely, its uncertainty. Recent studies make use of the term uncertainty quantification (UQ)\u00a0[[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR5)]. Uncertainty can be of aleatoric nature, relating to the random process that generates the target values, or epistemic nature, implying model-related uncertainty\u00a0[[6](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR6)]. Usually, these two types cannot be fully distinguished\u00a0[[7](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR7)].\nClassification algorithms often provide built-in mechanisms or augmentations to measure their uncertainty\u00a0[[8](https://jcheminf.biomedcentral.com/...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9"
    },
    {
      "title": "What is Stack Ensemble? | H20.ai - H2O.ai",
      "text": "What is Stack Ensemble? | H20.ai\nXReturn to page\n[![h2o_logo](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/logo.coreimg.svg/1764980469868/h2o-logo.svg)![h2o_logo](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/logo.coreimg.svg/1764980469868/h2o-logo.svg \"h2o_logo\")](https://h2o.ai/)\n![]()![]()\n* [SOLUTIONS](https://h2o.ai/solutions/use-case/)\nSOLUTIONS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/pie_chart.svg)\nFinancial Services\n](https://h2o.ai/solutions/industry/financial-services/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/mobile_phone.svg)\nTelecommunications\n](https://h2o.ai/solutions/industry/telecommunications/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/family.svg)\nPublic Sector\n](https://h2o.ai/solutions/industry/public-sector/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/white_house.svg)\nUS Federal\n](https://h2o.ai/solutions/industry/government/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/folder_gallery.svg)\nUse Cases\n](https://h2o.ai/solutions/use-case/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/solutions/books.svg)\nCase Studies\n](https://h2o.ai/case-studies/)\nCUSTOMER STORIES\n[](https://h2o.ai/case-studies/cba/)\nCommonwealth Bank of Australiareduced scam losses by 70%usingreal-time GenAIand predictive AI from H2O.ai.\n[LEARN MORE](https://h2o.ai/case-studies/cba/)\n[](https://h2o.ai/case-studies/att-call-center/)\nAT&amp;T is transforming call center operations andcutting costs by 90%with H2O.ai\u2019s GenAI.\n[LEARN MORE](https://h2o.ai/case-studies/att-call-center/)\n* [PLATFORM](https://h2o.ai/platform/ai-cloud/)\nAI AGENTS &amp; DIGITAL ASSISTANTS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/chat_bot.svg)\nEnterprise AI Agent\n**h2oGPTe:**Enterprise GenAI with multi-model support, cost controls, and app integrations\n](https://h2o.ai/platform/enterprise-h2ogpte/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/setup_tools.svg)\nFine-Tune Your Agent\n**H2O LLM Studio:**No-code training and tuning for efficient, enterprise-ready LLMs and SLMs\n](https://h2o.ai/platform/llm-studio/)\nBUSINESS INSIGHTS &amp; PREDICTIVE AI\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/decentralize.svg)\nAutoML Platform\n**H2O Driverless AI:**Accelerate model development with automatic feature engineering and explainability\n](https://h2o.ai/platform/ai-cloud/make/h2o-driverless-ai/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/cyborg.svg)\nNo-Code Deep Learning\n**H2O Hydrogen Torch:**Train image, text, and time-series models with prebuilt templates\n](https://h2o.ai/platform/ai-cloud/make/hydrogen-torch/)\n### FOR MODEL BUILDERS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/floors.svg)\nOpenweight SLMs\n**H2O Danube3:**Lightweight, offline-capable small language models\n](https://h2o.ai/platform/danube/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/view.svg)\nOpenweight Vision-Language Models\n**H2OVL Mississippi:**OCR and Document AI with open multimodal models\n](https://h2o.ai/platform/mississippi/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/no_data.svg)\nNo-Code Fine-Tuning Studio\n**H2O LLM Studio:**Train custom LLMs and SLMs without code\n](https://h2o.ai/platform/llm-studio/)\n### FOR DATA SCIENTISTS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/shopping_tag.svg)\nAI-Powered Labeling\n**Label Genie:**Faster annotations with smart suggestions\n](https://h2o.ai/platform/ai-cloud/make/h2o-label-genie/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/decentralize.svg)\nFeature Engineering at Scale\n**H2O Feature Store:**Centralized and shareable feature repository\n](https://h2o.ai/platform/ai-cloud/make/feature-store/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/chat_bot.svg)\nOpen-Source ML Platform\n**H2O-3:**Distributed machine learning for Python, R, and Spark\n](https://h2o.ai/platform/ai-cloud/make/h2o/)\n### FOR ENTERPRISE DEVELOPERS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/pc_monitor.svg)\nModel Deployment &amp; Monitoring\n**H2O MLOps:**Manage the full ML lifecycle from training to production\n](https://h2o.ai/platform/ai-cloud/operate/h2o-mlops/)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/app_store.svg)\nReady-to-Use AI Apps\n**H2O GenAI App Store:**Prebuilt apps for industry-specific GenAI use cases\n](https://genai.h2o.ai/appstore)\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/platform/code.svg)\nLow-Code App Framework\n**H2O Wave:**Build custom AI apps with Python and minimal code\n](https://h2o.ai/platform/ai-cloud/make/h2o-wave/)\n* [ABOUT](https://h2o.ai/platform/why-h2o/)\nCOMPANY\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/info.svg)\nWhy H2O\n](https://h2o.ai/platform/why-h2o/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/team.svg)\nOur Team\n](https://h2o.ai/company/team/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/partnership.svg)\nPartners\n](https://h2o.ai/partner-network/find-a-partner/)\nOUR VISION\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/language.svg)\nAbout Us\n](https://h2o.ai/company/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/plants.svg)\nAI4Conservation\n](https://h2o.ai/company/ai-4-conservation/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/ecology.svg)\nAI4Good\n](https://h2o.ai/company/ai-4-good/)\nPRESS\n[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/news.svg)\nNews\n](https://h2o.ai/company/news/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/collection.svg)\nPress Releases\n](https://h2o.ai/company/press-releases/)[![](https://h2o.ai/content/dam/h2o/images/marketing/icons/navigation/about/medal.svg)\nAwards\n](https://h2o.ai/company/awards/)\n[![View Dell Partnership](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/navigation/item_1754849308528/par/advancedcolumncontro/columns0/advancedcolumncontro_412232897/columns0/cta/nav-cta-image.coreimg.png/1764980472305/dell-technologies-small.png)![View Dell Partnership](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/navigation/item_1754849308528/par/advancedcolumncontro/columns0/advancedcolumncontro_412232897/columns0/cta/nav-cta-image.coreimg.png/1764980472305/dell-technologies-small.png \"View Dell Partnership\")](https://h2o.ai/partner-network/find-a-partner/dell/)\n[ALL PARTNERS](https://h2o.ai/partner-network/find-a-partner/)\n[![View more about H2O and Nvidia](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/navigation/item_1754849308528/par/advancedcolumncontro/columns0/advancedcolumncontro_412232897/columns1/cta_copy/nav-cta-image.coreimg.png/1764980472447/nvidia-small.png)![View more about H2O and Nvidia](https://h2o.ai/content/experience-fragments/h2o/us/en/site/header/master/_jcr_content/root/container/header_copy/navigation/item_1754849308528/par/advancedcolumncontro/columns0/advancedcolumncontro_412232897/columns1/cta_copy/nav-cta-image.coreimg.png/1764980472447/nvidia-small.png \"View more about H2O and Nvidia\")](https://h2o.ai/partner-network/find-a-partner/nvidia/)\n[![View 2024 AI Winners](https://h2o.ai/content/experience-fragments/h2o/us/en/s...",
      "url": "https://h2o.ai/wiki/stack-ensemble"
    },
    {
      "title": "Stacking in Machine Learning - Applied AI Course",
      "text": "[Skip to content](https://www.appliedaicourse.com/www.appliedaicourse.com#content)\n\n# Stacking in Machine Learning\n\nOctober 15, 2024\n\n[Mohit Uniyal](https://www.appliedaicourse.com/blog/author/mohit-uniyal/)\n\n[Machine Learning](https://www.appliedaicourse.com/blog/category/machine-learning/)\n\n### Latest articles\n\n#### [Hadoop Distributed File System (HDFS) \u2014 A Complete Guide](https://www.appliedaicourse.com/blog/hadoop-distributed-file-system-hdfs/)\n\n#### [Ordinal Encoding \u2014 A Brief Guide](https://www.appliedaicourse.com/blog/ordinal-encoding/)\n\n#### [What is NoSQL? Guide to NoSQL Databases](https://www.appliedaicourse.com/blog/nosql-database/)\n\n#### [Hadoop YARN Architecture](https://www.appliedaicourse.com/blog/hadoop-yarn-architecture/)\n\n#### [Healthcare Analytics: A Comprehensive Guide](https://www.appliedaicourse.com/blog/healthcare-analytics/)\n\n#### [What is Apache Hive?](https://www.appliedaicourse.com/blog/what-is-apache-hive/)\n\n#### [Big Data Engineer Salary 2025](https://www.appliedaicourse.com/blog/big-data-engineer-salary/)\n\n#### [What is Spark Streaming?](https://www.appliedaicourse.com/blog/what-is-spark-streaming/)\n\nEnsemble learning is a popular approach in [machine learning](https://www.appliedaicourse.com/blog/machine-learning/) where multiple models are combined to improve the accuracy and robustness of predictions. Often, individual models may have limitations, such as overfitting or underfitting. By combining several models, ensemble methods can reduce these issues and produce better results.\n\n**Stacking** (stacked generalization) is an ensemble technique that uses several base models and combines their outputs through a meta-model to enhance overall performance. It integrates predictions from diverse models, making it effective when a single model doesn\u2019t perform well. Stacking is applied in areas like customer behavior prediction and image classification for improved results.\n\n## What is Stacking?\n\n**Stacking**, or **stacked generalization**, is an ensemble learning technique that combines the predictions of multiple models (base models) to create a more accurate final prediction. The idea is to utilize the strengths of different models by blending their outputs using another model called the **meta-model**.\n\nStacking differs from other ensemble methods like **bagging** and **boosting**:\n\n- **Bagging** (Bootstrap Aggregating) involves training multiple versions of the same model on different subsets of the data and averaging their predictions to reduce variance and prevent overfitting.\n- **Boosting** trains models sequentially, where each new model focuses on correcting the mistakes of the previous ones, improving the overall accuracy but increasing the risk of overfitting.\n\nUnlike bagging and boosting, stacking combines different types of models rather than the same type. The meta-model in stacking learns the best way to integrate their outputs, leading to a more balanced and accurate result.\n\n## How Stacking Works?\n\nStacking involves combining the outputs of different models to enhance predictive performance. Here\u2019s how the process typically works:\n\n1. **Data Preparation**: The dataset is divided into training and testing sets. The training data is used to build the models, while the testing data evaluates the performance.\n2. **Training the Base Models**: Multiple base models (e.g., decision trees, logistic regression) are trained independently using the training set. These models are diverse, ensuring varied perspectives on the data.\n3. **Generating Base Model Predictions**: The trained base models make predictions on the same training data. These predictions form a new set of features, representing each model\u2019s view of the data.\n4. **Training the Meta-Model**: A meta-model is trained using these new features (base model predictions) and the original features. The meta-model, often a simpler algorithm like linear regression, learns to find patterns in the base models\u2019 outputs.\n5. **Final Prediction**: For the test set, base models generate predictions, which are then used by the meta-model to produce the final output. This way, the meta-model combines the strengths of all base models.\n\nThis multi-layered structure allows stacking to achieve better performance by leveraging different models\u2019 strengths.\n\n## Architecture of Stacking\n\nStacking consists of two main levels in its architecture:\n\n### **1\\. Primary Level (Base Models)**:\n\n- At this level, multiple base models are trained using the training dataset. These models can vary in type, such as decision trees, support vector machines, or logistic regression. The goal is to create diverse models that offer different perspectives on the data.\n- The base models generate predictions on the training data, which are then used as inputs for the next level.\n\n### **2\\. Secondary Level (Meta-Model)**:\n\n- The meta-model, also called the secondary model, is trained using the predictions from the base models and the original dataset features. This model aims to learn how to best combine the outputs of the base models for improved accuracy.\n- Common meta-model algorithms include logistic regression, random forest, or even neural networks, depending on the complexity and nature of the data.\n\n### **3\\. Final Prediction**:\n\n- The meta-model makes the final prediction by leveraging the combined insights from the base models and the original features. This output is expected to be more accurate than any of the base models alone, as it integrates their strengths.\n\nThis architecture allows stacking to achieve higher predictive performance by combining diverse models and optimizing their collective output.\n\n## What is Blending and How Does it Work?\n\nBlending is another ensemble learning technique, similar to stacking, but with a simpler approach. It combines predictions from multiple models, often averaging them, to achieve better performance. Here\u2019s how blending differs from stacking and how it works:\n\n### **1\\. Blending vs. Stacking**:\n\n- While **stacking** uses predictions from base models as input for a meta-model, **blending** generally averages the predictions from base models directly without the need for a meta-model.\n- Blending typically splits the training data into two parts. The first part is used to train the base models, and the second part is used to generate predictions, which are averaged to produce the final output.\n\n### **2\\. How Blending Works**:\n\n- The training dataset is divided into two subsets: one for training the base models and another (a holdout set) for making predictions.\n- Base models are trained using the first subset, and their predictions are collected on the holdout set.\n- These predictions are then averaged or combined using a simple model to produce the final prediction. This straightforward approach reduces the risk of overfitting that sometimes occurs in stacking.\n\n### **3\\. Advantages and Disadvantages of Blending**:\n\n- **Advantages**: Blending is simpler to implement and requires less computation since it avoids training an additional meta-model.\n- **Disadvantages**: It may not achieve the same level of performance improvement as stacking because it doesn\u2019t learn how to best combine the predictions from the base models.\n\nBlending is often used as a quick way to boost model performance without the complexity of stacking, but stacking usually yields more refined and accurate results when properly implemented.\n\n## Conclusion\n\nStacking is a powerful ensemble technique in machine learning that enhances model performance by combining predictions from multiple models through a meta-model. By leveraging the strengths of diverse models, stacking produces a more accurate and reliable final output than any single model alone.\n\nWhile stacking offers significant benefits, such as improved accuracy and robustness, it also has some drawbacks, including increased complexity and the need for more computational resources. In contrast, blending provides a simpler alternati...",
      "url": "https://www.appliedaicourse.com/blog/stacking-in-machine-learning"
    },
    {
      "title": "An interpretable stacking ensemble model for high-entropy alloy ...",
      "text": "Your new experience awaits. Try the new design now and help us make it even better\n\nSwitch to the new experience\n\nORIGINAL RESEARCH article\n\nFront. Mater., 18 June 2025\n\nSec. Structural Materials\n\nVolume 12 - 2025 \\| [https://doi.org/10.3389/fmats.2025.1601874](https://doi.org/10.3389/fmats.2025.1601874)\n\n# An interpretable stacking ensemble model for high-entropy alloy mechanical property prediction\n\n[Songpeng Zhao](https://loop.frontiersin.org/people/3013788) 1Zeyuan Li2Changshuai Yin1Zhaofu Zhang1Teng Long3Jingjing Yang1\\*[Ruyue Cao](https://loop.frontiersin.org/people/3019085) 3\\*Yuzheng Guo2\\*\n\n- 1The Institute of Technological Sciences, Wuhan University, Wuhan, China\n- 2School of Power and Mechanical Engineering, Wuhan University, Wuhan, China\n- 3Department of Engineering, Cambridge University, Cambridge, United Kingdom\n\nHigh-entropy alloys (HEAs) have attracted significant attention due to their excellent mechanical properties and broad application prospects. However, accurately predicting their mechanical behavior remains challenging because of the vast compositional design space and complex multi-element interactions. In this study, we propose a stacking learning-based machine learning framework to improve the accuracy and robustness of HEA mechanical property predictions. Key physicochemical features were extracted, and a hierarchical clustering model-driven hybrid feature selection strategy (HC-MDHFS) was employed to identify the most relevant descriptors. Three machine learning algorithms-Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Gradient Boosting (Gradient Boosting)-were integrated into a multi-level stacking ensemble, with Support Vector Regression serving as the meta-learner. To improve model interpretability, the SHapley Additive Explanations (SHAP) method was applied to assess feature importance. The results demonstrate that the proposed stacking framework outperforms individual models in predicting yield strength and elongation, showing improved generalization ability and predictive accuracy.\n\n## 1 Introduction\n\nHigh-entropy alloys (HEAs), also referred to as multi-principal element alloys (MPEAs), were first introduced by [Yeh et al. (2004)](https://www.frontiersin.org/www.frontiersin.org#B50) and [Cantor et al. (2004)](https://www.frontiersin.org/www.frontiersin.org#B4). Initially, HEAs were defined as alloys comprising five or more principal elements, with atomic fractions between 5% and 35% ( [Yeh et al., 2004](https://www.frontiersin.org/www.frontiersin.org#B50)). This classification was based on the hypothesis that when multiple elements coexist in near-equiatomic proportions, the configurational entropy significantly increases, reducing the likelihood of intermetallic compound formation and promoting the stabilization of a single-phase solid solution. Over time, the definition of HEAs has expanded, and contemporary research generally considers any alloy composed of three or more principal elements in near-equiatomic ratios as a high-entropy alloy.\n\nBenefiting from the combined effects of high-entropy, lattice distortion, sluggish diffusion, and the cocktail effect, HEAs have drawn increasing attention due to their unique physicochemical properties, which differ significantly from those of conventional alloys ( [Pickering and Jones, 2016](https://www.frontiersin.org/www.frontiersin.org#B35)). For instance, CrMnFeCoNi HEAs exhibit an excellent combination of strength and ductility at room temperature and maintain outstanding mechanical properties in low and ultra-low temperature environments ( [Liu et al., 2022](https://www.frontiersin.org/www.frontiersin.org#B26)). Similarly, refractory HEAs such as CrMoNbV demonstrate high yield strength and exceptional thermal softening resistance at elevated temperatures (1273 K), outperforming traditional Ni-based superalloys ( [Lee et al., 2021](https://www.frontiersin.org/www.frontiersin.org#B22)). These properties make HEAs highly suitable for demanding applications in aerospace, nu-clear energy, and extreme environmental conditions.\n\nAnother important aspect of HEAs is their remarkable stability, which can be attributed to the fact that in a certain macrostate, a sample can have many variants of states at the atomic level, without affecting the material\u2019s external characteristics. This contributes to their exceptional stability, as described in previous studies on HEA mechanical properties ( [Maruschak and Maruschak, 2024](https://www.frontiersin.org/www.frontiersin.org#B29)).\n\nDespite their promising mechanical characteristics, the vast compositional design space of HEAs and the complex nonlinear interactions between multiple elements pose significant challenges to their efficient design and optimization. Traditional experimental approaches are costly and time-intensive, limiting their ability to explore the full range of possible HEA compositions. Additionally, computational modeling methods, including Density Functional Theory (DFT) ( [Zhang et al., 2014](https://www.frontiersin.org/www.frontiersin.org#B54)), _Ab Initio_ Molecular Dynamics (AIMD) ( [Gao and Alman, 2013](https://www.frontiersin.org/www.frontiersin.org#B9)), Molecular Dynamics (MD) ( [Wang Q. et al., 2024](https://www.frontiersin.org/www.frontiersin.org#B40)), and Finite Element Analysis (FEA), have been employed to predict HEA properties. While these techniques provide valuable theoretical insights, they are computationally expensive and often struggle to handle the high-dimensional compositional space characteristic of HEAs ( [Guo et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B13)).\n\nMachine learning (ML) has recently emerged as a powerful data-driven approach to overcoming these limitations. By leveraging large datasets of experimental and computational results, ML models can identify complex relationships between alloy composition, processing parameters, and mechanical properties. This enables faster and more accurate property predictions, accelerating the HEA design process. In HEA research, ML has been widely used for phase prediction ( [Chen et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B6); [Ye et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B49); [Zhang W. et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B52); [He et al., 2024](https://www.frontiersin.org/www.frontiersin.org#B18)) and mechanical property estimation ( [Pan et al., 2025](https://www.frontiersin.org/www.frontiersin.org#B34); [Li S. et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B23); [Yang et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B47); [Jain et al., 2023](https://www.frontiersin.org/www.frontiersin.org#B20)).\n\nFor instance, [O\u00f1ate et al. (2023)](https://www.frontiersin.org/www.frontiersin.org#B33) investigated HEA phase classification using four different ML models and found that the Random Forest model performed best, achieving an accuracy of 72.8%. [Mandal et al. (2023)](https://www.frontiersin.org/www.frontiersin.org#B28) employed multiple ML algorithms incorporating atomic size difference, electronegativity difference, and three other parameters for phase and crystal structure classification, with decision tree and support vector machine (SVM) achieving a phase prediction accuracy of 93.84% and SVM attaining the highest classification accuracy of 84.32% for crystal structures. [Zhao et al. (2024)](https://www.frontiersin.org/www.frontiersin.org#B56) utilized interpretable ML models combined with empirical descriptors to con-struct two-dimensional (2D) phase diagrams for HEAs. Compared to commonly used descriptors, their newly proposed descriptors improved prediction accuracy, reaching approximately 95% for distinguishing between crystalline and amorphous phases, as well as between BCC and FCC structures. [Gao et al. (2024)](https://www.frontiersin.org/www.frontiersin.org#B10) applied ML combined with multi-objective optimizatio...",
      "url": "https://www.frontiersin.org/journals/materials/articles/10.3389/fmats.2025.1601874/full"
    },
    {
      "title": "Ensemble Model With Bert,Roberta and Xlnet For Molecular ... - arXiv",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2406.06553v1"
    },
    {
      "title": "Stacking classifier - ensemble for great results - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=65e25d68eb562b179c3e:1:10718)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/marcinrutecki/stacking-classifier-ensemble-for-great-results"
    }
  ]
}