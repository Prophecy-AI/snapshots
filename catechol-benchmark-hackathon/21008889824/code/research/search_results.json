{
  "query": "How can I use quantile regression to improve model generalization and reduce prediction bias on out-of-distribution data?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent research on quantile regression bias, regularization, and ensemble\u2011based correction methods, you can strengthen generalization and curb out\u2011of\u2011distribution (OOD) prediction bias by following these core steps\u202f([Correcting the Coverage Bias of Quantile Regression](https://arxiv.org/html/2511.00820v1); [Regularization Strategies for Quantile Regression](https://arxiv.org/abs/2102.05135); [Conformalized Quantile Regression](https://papers.neurips.cc/paper/8613-conformalized-quantile-regression.pdf); [Flexible Model Aggregation for Quantile Regression](https://jmlr.org/papers/v24/22-0799.html); [Two\u2011level Quantile Regression Forests for Bias Correction](https://link.springer.com/article/10.1007/s10994-014-5452-1)):\n\n1. **Fit a regularized quantile\u2011regression model**  \n   - Choose a loss (e.g., pinball) and add a regularizer (L1, L2, or group\u2011lasso) to control model complexity, as recommended for high\u2011dimensional settings\u202f([Regularization Strategies for Quantile Regression](https://arxiv.org/abs/2102.05135)).  \n   - Perform K\u2011fold cross\u2011validation to select the regularization strength that minimizes out\u2011of\u2011sample quantile loss.\n\n2. **Assess and correct coverage bias**  \n   - Compute the empirical coverage of each fitted quantile on a held\u2011out validation set.  \n   - Apply the leave\u2011one\u2011out coverage adjustment described in the \u201ccoverage bias correction\u201d framework, which uses the dual formulation of quantile regression to obtain calibrated quantile predictions with minimal extra computation\u202f([Correcting the Coverage Bias of Quantile Regression](https://arxiv.org/html/2511.00820v1)).\n\n3. **Apply conformal calibration for OOD robustness**  \n   - Use conformalized quantile regression (CQR): after obtaining the regularized quantile estimates, compute non\u2011conformity scores on a calibration set and adjust the interval bounds to guarantee finite\u2011sample coverage even under distribution shift\u202f([Conformalized Quantile Regression](https://papers.neurips.cc/paper/8613-conformalized-quantile-regression.pdf)).  \n   - This step yields adaptive, heteroscedastic prediction intervals that remain valid on OOD data.\n\n4. **Ensemble multiple quantile models**  \n   - Train several diverse quantile regressors (e.g., linear, tree\u2011based, neural\u2011network) and aggregate them with quantile\u2011level\u2011specific weights that may vary across feature space.  \n   - Weighted aggregation improves both accuracy and robustness, especially when individual models capture different aspects of the conditional distribution\u202f([Flexible Model Aggregation for Quantile Regression](https://jmlr.org/papers/v24/22-0799.html)).\n\n5. **Add a second\u2011level bias\u2011correction stage (optional)**  \n   - Fit a second quantile\u2011regression forest on the residuals of the first\u2011level model (the \u201cbcQRF\u201d approach).  \n   - Use the second\u2011level predictions to adjust the original quantile estimates, further reducing systematic bias in high\u2011dimensional or noisy regimes\u202f([Two\u2011level Quantile Regression Forests for Bias Correction](https://link.springer.com/article/10.1007/s10994-014-5452-1)).\n\n6. **Validate on simulated OOD scenarios**  \n   - Create synthetic shifts (e.g., covariate distribution changes) and evaluate calibrated coverage and interval length.  \n   - Iterate the regularization strength, calibration set size, or ensemble weights until the model maintains target coverage (e.g., 90\u202f%) and minimal bias across these tests.\n\nBy regularizing the base quantile model, calibrating its coverage with leave\u2011one\u2011out or conformal adjustments, and optionally boosting robustness through ensembles or a second\u2011level bias\u2011correction forest, you can achieve better generalization and substantially lower prediction bias when the model encounters out\u2011of\u2011distribution data.",
      "url": ""
    },
    {
      "title": "Correcting the Coverage Bias of Quantile Regression",
      "text": "Correcting the Coverage Bias of Quantile Regression\n\\\\setkeys\nGinwidth=\\\\Gin@nat@width,height=\\\\Gin@nat@height,keepaspectratio\n# Correcting the Coverage Bias of Quantile Regression\nIsaac Gibbs\\*\\*\\*The first two authors contributed equally to this work.\u2020\u2020\u2020Department of Statistics, University of California, Berkeley. Corresponding author:[igibbs@berkeley.edu](mailto:igibbs@berkeley.edu).John J. Cherian\u2020\u2020footnotemark:\u2021\u2021\u2021Department of Statistics, Stanford University.Emmanuel J. Cand\u00e8s\u00a7\u00a7\u00a7Departments of Mathematics and Statistics, Stanford University.\n###### Abstract\nWe develop a collection of methods for adjusting the predictions of quantile regression to ensure coverage. Our methods are model agnostic and can be used to correct for high-dimensional overfitting bias with only minimal assumptions. Theoretical results show that the estimates we develop are consistent and facilitate accurate calibration in the proportional asymptotic regime where the ratio of the dimension of the data and the sample size converges to a constant. This is further confirmed by experiments on both simulated and real data. A key component of our work is a new connection between the leave-one-out coverage and the fitted values of variables appearing in a dual formulation of the quantile regression problem. This facilitates the use of cross-validation in a variety of settings at significantly reduced computational costs.\n## 1Introduction\nQuantile regression is a popular tool for bounding the tail of a target outcome. This method has a long history dating back to the foundational work of> Koenker &amp; Bassett (\n[> 1978\n](https://arxiv.org/html/2511.00820v1#bib.bib27)> )\nand has found widespread applications across a variety of areas> (Koenker &amp; Hallock [> 2001\n](https://arxiv.org/html/2511.00820v1#bib.bib28)> , Koenker [> 2017\n](https://arxiv.org/html/2511.00820v1#bib.bib26)> )\n. Classical results demonstrate that as the sample size increases quantile regression estimates are consistent, normally distributed around their population analogs> (Koenker &amp; Bassett [> 1978\n](https://arxiv.org/html/2511.00820v1#bib.bib27)> , Angrist et\u00a0al. [> 2006\n](https://arxiv.org/html/2511.00820v1#bib.bib3)> )\n, and, perhaps most critically, achieve their target coverage level> (Jung et\u00a0al. [> 2023\n](https://arxiv.org/html/2511.00820v1#bib.bib23)> , Duchi [> 2025\n](https://arxiv.org/html/2511.00820v1#bib.bib14)> )\n.\n![Refer to caption](x1.png)Figure 1:Miscoverage of (unregularized) quantile regression fit with modelYi\u223c\u03b20+Xi\u22a4\u200b\u03b2Y\\_{i}\\\\sim\\\\beta\\_{0}+X\\_{i}^{\\\\top}\\\\betaon i.i.d. data{(Xi,Yi)}i=1n\\\\{(X\\_{i},Y\\_{i})\\\\}\\_{i=1}^{n}sampled asYi=Xi\u22a4\u200b\u03b2\\~+\u03f5iY\\_{i}=X\\_{i}^{\\\\top}\\\\tilde{\\\\beta}+\\\\epsilon\\_{i}forXi\u223c\ud835\udca9\u200b(0,Id)X\\_{i}\\\\sim\\\\mathcal{N}(0,I\\_{d}),\u03f5\u223c\ud835\udca9\u200b(0,1)\\\\epsilon\\\\sim\\\\mathcal{N}(0,1), and\u03f5i\u27c2\u27c2Xi\\\\epsilon\\_{i}\\\\mathchoice{\\\\mathrel{\\\\hbox to0.0pt{$\\\\displaystyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\displaystyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\textstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\textstyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\scriptstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\scriptstyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\scriptscriptstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\scriptscriptstyle\\\\perp}}}X\\_{i}. Boxplots in the figure show the empirical distribution of the training-conditional coverage,\u2119\u200b(Yn+1\u2264\u03b2^0+Xn+1\u22a4\u200b\u03b2^\u2223{(Xi,Yi)}i=1n)\\\\mathbb{P}(Y\\_{n+1}\\\\leq\\\\hat{\\\\beta}\\_{0}+X\\_{n+1}^{\\\\top}\\\\hat{\\\\beta}\\\\mid\\\\{(X\\_{i},Y\\_{i})\\\\}\\_{i=1}^{n})where(\u03b2^0,\u03b2^)(\\\\hat{\\\\beta}\\_{0},\\\\hat{\\\\beta})denote the estimated coefficients at quantile level\u03c4=0.9\\\\tau=0.9and(Xn+1,Yn+1)(X\\_{n+1},Y\\_{n+1})is an independent sample from the same model. The results come from 100 trials where in each trial the coverage is evaluated over a test set of size 2000 and the population coefficients are sampled as\u03b2\\~\u223c\ud835\udca9\u200b(0,Id/d)\\\\tilde{\\\\beta}\\\\sim\\\\mathcal{N}(0,I\\_{d}/d). The red line shows the target miscoverage level of1\u2212\u03c4=0.11-\\\\tau=0.1.\nAlthough the classical theory can be accurate for large sample sizes, it is often insufficient to characterize the realities of finite datasets. Figure[1](https://arxiv.org/html/2511.00820v1#S1.F1)shows the realized miscoverage of quantile estimates fit at target level\u03c4=0.9\\\\tau=0.9in a well specified linear modelYi=Xi\u22a4\u200b\u03b2\\~+\u03f5iY\\_{i}=X\\_{i}^{\\\\top}\\\\tilde{\\\\beta}+\\\\epsilon\\_{i}with\u03f5i\u27c2\u27c2Xi\\\\epsilon\\_{i}\\\\mathchoice{\\\\mathrel{\\\\hbox to0.0pt{$\\\\displaystyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\displaystyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\textstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\textstyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\scriptstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\scriptstyle\\\\perp}}}{\\\\mathrel{\\\\hbox to0.0pt{$\\\\scriptscriptstyle\\\\perp$\\\\hss}\\\\mkern 2.0mu{\\\\scriptscriptstyle\\\\perp}}}X\\_{i}andXi\u2208\u211ddX\\_{i}\\\\in\\\\mathbb{R}^{d}. In agreement with the classical theory, we see that whenXiX\\_{i}has very low dimension (e.g.,d=1d=1) quantile regression reliably obtains the target miscoverage rate of1\u2212\u03c4=0.11-\\\\tau=0.1. However, the scope of this result is limited, and the coverage shows visible bias in what might be typically considered to be small or moderate dimensions (e.g.,d\u2208{15,30}d\\\\in\\\\{15,30\\\\}compared to a sample size ofn=300n=300). Perhaps unsurprisingly, this issue only worsens as the dimension increases and quantile regression exhibits over two times the target error rate whend=90d=90.\nFormal characterization of the coverage bias of quantile regression was first given in> Bai et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2511.00820v1#bib.bib5)> )\n. They eschew classical theory and instead work under a proportional asymptotic framework in which the ratio of the dimension of the data and the sample size converges to a constant. Under a stylized linear model, they show that in this regime the coverage of quantile regression converges to a value different from the target level and provide an exact formula for quantifying this bias. Interestingly, while both under- and overcoverage are possible, they demonstrate that in most settings quantile regression will tend to undercover.\u00b6\u00b6\u00b6As a matter of terminology, ifq^\u03c4\\\\hat{q}\\_{\\\\tau}is an estimate of the\u03c4\u2208[1/2,1]\\\\tau\\\\in[1/2,1]quantile ofYYwe say thatq^\u03c4\\\\hat{q}\\_{\\\\tau}undercovers if\u2119\u200b(Y\u2264q^\u03c4)&lt;\u03c4\\\\mathbb{P}(Y\\\\leq\\\\hat{q}\\_{\\\\tau})&lt;&lt;\\\\tauand overcovers if\u2119\u200b(Y\u2264q^\u03c4)&gt;\u03c4\\\\mathbb{P}(Y\\\\leq\\\\hat{q}\\_{\\\\tau})&gt;&gt;\\\\tau. For\u03c4&lt;1/2\\\\tau&lt;&lt;1/2this terminology is reversed and we say thatq^\u03c4\\\\hat{q}\\_{\\\\tau}undercovers if\u2119\u200b(Y\u2264q^\u03c4)&gt;\u03c4\\\\mathbb{P}(Y\\\\leq\\\\hat{q}\\_{\\\\tau})&gt;&gt;\\\\tauand overcovers otherwise. This is motivated by the fact that for\u03c4&gt;1/2\\\\tau&gt;&gt;1/2(resp.\u03c4&lt;1/2\\\\tau&lt;&lt;1/2) the\u03c4\\\\tau-quantile is designed to be a high probability upper (resp. lower) bound onYY. We use the terms undercoverage and overcoverage to reflect these goals.This is consistent with the results in Figure[1](https://arxiv.org/html/2511.00820v1#S1.F1)as well as additional empirical evaluations that we will present in Section[5](https://arxiv.org/html/2511.00820v1#S5).\nTwo proposals have been made in the literature for correcting quantile regression\u2019s bias. Under the same linear model assumptions,> Bai et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2511.00820v1#bib.bib5)> )\nderive a simple method for adjusting the nominal level to account for overfitting. While quite effective, this procedure is limited in scope to small aspect ratios and a restrictive model for the data. A more generic procedure that does not require any such modeling assumptions was given in> Gibbs et\u00a0al. (\n[> 2025\n](https://arxiv.org/html/2511.00820v1#bib.bib16)> )\n. They employ a technique known as full conformal inference, which augments the regression fit with a guess of the unseen test point. This mimics the effect of overfitting the training data on the test point, thereby eliminating the resulting bias. In general, this approach has two main drawbacks. First, it requires randomization in order to obtain the desired coverage level. As we will show in Section[2.3](https://arxiv.org/html/2511.00820v1#S2.SS3), thi...",
      "url": "https://arxiv.org/html/2511.00820v1"
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[2102.05135] Regularization Strategies for Quantile Regression\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2102.05135\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2102.05135**(stat)\n[Submitted on 9 Feb 2021]\n# Title:Regularization Strategies for Quantile Regression\nAuthors:[Taman Narayan](https://arxiv.org/search/stat?searchtype=author&amp;query=Narayan,+T),[Serena Wang](https://arxiv.org/search/stat?searchtype=author&amp;query=Wang,+S),[Kevin Canini](https://arxiv.org/search/stat?searchtype=author&amp;query=Canini,+K),[Maya Gupta](https://arxiv.org/search/stat?searchtype=author&amp;query=Gupta,+M)\nView a PDF of the paper titled Regularization Strategies for Quantile Regression, by Taman Narayan and 3 other authors\n[View PDF](https://arxiv.org/pdf/2102.05135)> > Abstract:\n> We investigate different methods for regularizing quantile regression when predicting either a subset of quantiles or the full inverse CDF. We show that minimizing an expected pinball loss over a continuous distribution of quantiles is a good regularizer even when only predicting a specific quantile. For predicting multiple quantiles, we propose achieving the classic goal of non-crossing quantiles by using deep lattice networks that treat the quantile as a monotonic input feature, and we discuss why monotonicity on other features is an apt regularizer for quantile regression. We show that lattice models enable regularizing the predicted distribution to a location-scale family. Lastly, we propose applying rate constraints to improve the calibration of the quantile predictions on specific subsets of interest and improve fairness metrics. We demonstrate our contributions on simulations, benchmark datasets, and real quantile regression problems. Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)|\nCite as:|[arXiv:2102.05135](https://arxiv.org/abs/2102.05135)[stat.ML]|\n|(or[arXiv:2102.05135v1](https://arxiv.org/abs/2102.05135v1)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2102.05135](https://doi.org/10.48550/arXiv.2102.05135)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Taman Narayan [[view email](https://arxiv.org/show-email/28778729/2102.05135)]\n**[v1]**Tue, 9 Feb 2021 21:10:35 UTC (390 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Regularization Strategies for Quantile Regression, by Taman Narayan and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2102.05135)\n* [TeX Source](https://arxiv.org/src/2102.05135)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2102.05135&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2102.05135&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2021-02](https://arxiv.org/list/stat.ML/2021-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2102.05135?context=cs)\n[cs.LG](https://arxiv.org/abs/2102.05135?context=cs.LG)\n[stat](https://arxiv.org/abs/2102.05135?context=stat)\n[stat.ME](https://arxiv.org/abs/2102.05135?context=stat.ME)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2102.05135)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2102.05135)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2102.05135)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2102.05135&amp;description=Regularization Strategies for Quantile Regression>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2102.05135&amp;title=Regularization Strategies for Quantile Regression>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2102.05135)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2102.05135"
    },
    {
      "title": "Flexible Model Aggregation for Quantile Regression",
      "text": "Flexible Model Aggregation for Quantile Regression\n[![](https://jmlr.org/img/jmlr.jpg)](https://jmlr.org/)\n[Home Page](https://jmlr.org/)\n[Papers](https://jmlr.org/papers)\n[Submissions](https://jmlr.org/author-info.html)\n[News](https://jmlr.org/news.html)\n[Editorial Board](https://jmlr.org/editorial-board.html)\n[Special Issues](https://jmlr.org/special_issues/)\n[Open Source Software](https://jmlr.org/mloss)\n[Proceedings (PMLR)](https://proceedings.mlr.press/)\n[Data (DMLR)](https://data.mlr.press/)\n[Transactions (TMLR)](https://jmlr.org/tmlr)\n[Search](https://jmlr.org/search-jmlr.html)\n[Statistics](https://jmlr.org/stats.html)\n[Login](https://jmlr.org/manudb)\n[Frequently Asked Questions](https://jmlr.org/faq.html)\n[Contact Us](https://jmlr.org/contact.html)\n[![RSS Feed](https://jmlr.org/img/RSS.gif)](https://jmlr.org/jmlr.xml)\n## Flexible Model Aggregation for Quantile Regression\n***Rasool Fakoor, Taesup Kim, Jonas Mueller, Alexander J. Smola, Ryan J. Tibshirani***; 24(162):1&minus;45, 2023.\n### Abstract\nQuantile regression is a fundamental problem in statistical learning motivated by a need to quantify uncertainty in predictions, or to model a diverse population without being overly reductive. For instance, epidemiological forecasts, cost estimates, and revenue predictions all benefit from being able to quantify the range of possible values accurately. As such, many models have been developed for this problem over many years of research in statistics, machine learning, and related fields. Rather than proposing yet another (new) algorithm for quantile regression we adopt a meta viewpoint: we investigate methods for aggregating any number of conditional quantile models, in order to improve accuracy and robustness. We consider weighted ensembles where weights may vary over not only individual models, but also over quantile levels, and feature values. All of the models we consider in this paper can be fit using modern deep learning toolkits, and hence are widely accessible (from an implementation point of view) and scalable. To improve the accuracy of the predicted quantiles (or equivalently, prediction intervals), we develop tools for ensuring that quantiles remain monotonically ordered, and apply conformal calibration methods. These can be used without any modification of the original library of base models. We also review some basic theory surrounding quantile aggregation and related scoring rules, and contribute a few new results to this literature (for example, the fact that post sorting or post isotonic regression can only improve the weighted interval score). Finally, we provide an extensive suite of empirical comparisons across 34 data sets from two different benchmark repositories.\n[abs][[pdf](https://jmlr.org/papers/volume24/22-0799/22-0799.pdf)][[bib](https://jmlr.org/papers/v24/22-0799.bib)]\n[[code](https://github.com/amazon-research/quantile-aggregation)]&copy[JMLR](https://www.jmlr.org)2023. ([edit](https://github.com/JmlrOrg/v24/tree/main/22-0799),[beta](http://jmlr.org/beta/papers/v24/22-0799.html))|\n[Mastodon](https://sigmoid.social/@jmlr)",
      "url": "https://jmlr.org/papers/v24/22-0799.html"
    },
    {
      "title": "",
      "text": "Conformalized Quantile Regression\nYaniv Romano\nDepartment of Statistics\nStanford University\nEvan Patterson\nDepartment of Statistics\nStanford University\nEmmanuel J. Cand\u00e8s\nDepartments of Mathematics and of Statistics\nStanford University\nAbstract\nConformal prediction is a technique for constructing prediction intervals that at\u0002tain valid coverage in finite samples, without making distributional assumptions.\nDespite this appeal, existing conformal methods can be unnecessarily conserva\u0002tive because they form intervals of constant or weakly varying length across the\ninput space. In this paper we propose a new method that is fully adaptive to het\u0002eroscedasticity. It combines conformal prediction with classical quantile regression,\ninheriting the advantages of both. We establish a theoretical guarantee of valid\ncoverage, supplemented by extensive experiments on popular regression datasets.\nWe compare the efficiency of conformalized quantile regression to other conformal\nmethods, showing that our method tends to produce shorter intervals.\n1 Introduction\nIn many applications of regression modeling, it is important not only to predict accurately but also to\nquantify the accuracy of the predictions. This is especially true in situations involving high-stakes\ndecision making, such as estimating the efficacy of a drug or the risk of a credit default. The\nuncertainty in a prediction can be quantified using a prediction interval, giving lower and upper\nbounds between which the response variable lies with high probability. An ideal procedure for\ngenerating prediction intervals should satisfy two properties. First, it should provide valid coverage\nin finite samples, without making strong distributional assumptions, such as Gaussianity. Second, its\nintervals should be as short as possible at each point in the input space, so that the predictions will be\ninformative. When the data is heteroscedastic, getting valid but short prediction intervals requires\nadjusting the lengths of the intervals according to the local variability at each query point in predictor\nspace. This paper introduces a procedure that performs well on both criteria, being distribution-free\nand adaptive to heteroscedasticity.\nOur work is heavily inspired by conformal prediction, a general methodology for constructing\nprediction intervals [1\u20136]. Conformal prediction has the virtue of providing a nonasymptotic,\ndistribution-free coverage guarantee. The main idea is to fit a regression model on the training\nsamples, then use the residuals on a held-out validation set to quantify the uncertainty in future\npredictions. The effect of the underlying model on the length of the prediction intervals, and attempts\nto construct intervals with locally varying length, have been studied in numerous recent works [6\u201316].\nNevertheless, existing methods yield conformal intervals of either fixed length or length depending\nonly weakly on the predictors, as argued in [6, 15, 17].\nIn conformal prediction to date, there has been a mismatch between the primary inferential focus\u2014\nconditional mean estimation\u2014and the ultimate inferential goal\u2014prediction interval estimation.\nStatistical efficiency is lost by estimating a mean when an interval is needed. A more direct approach\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nto interval estimation is offered by quantile regression [18]. Take any algorithm for quantile regression,\ni.e., for estimating conditional quantile functions from data. To obtain prediction intervals with, say,\nnominal 90% coverage, simply fit the conditional quantile function at the 5% and 95% levels and\nform the corresponding intervals. Even for highly heteroscedastic data, this methodology has been\nshown to be adaptive to local variability [19\u201325]. However, the validity of the estimated intervals is\nguaranteed only for specific models, under certain regularity and asymptotic conditions [22\u201324].\nIn this work, we combine conformal prediction with quantile regression. The resulting method, which\nwe call conformalized quantile regression (CQR), inherits both the finite sample, distribution-free\nvalidity of conformal prediction and the statistical efficiency of quantile regression.1 On one hand,\nCQR is flexible in that it can wrap around any algorithm for quantile regression, including random\nforests and deep neural networks [26\u201329]. On the other hand, a key strength of CQR is its rigorous\ncontrol of the miscoverage rate, independent of the underlying regression algorithm.\nSummary and outline\nSuppose we are given n training samples {(Xi, Yi)}\nn\ni=1 and we must now predict the unknown\nvalue of Yn+1 at a test point Xn+1. We assume that all the samples {(Xi, Yi)}\nn+1\ni=1 are drawn\nexchangeably\u2014for instance, they may be drawn i.i.d.\u2014from an arbitrary joint distribution PXY\nover the feature vectors X \u2208 R\np\nand response variables Y \u2208 R. We aim to construct a marginal\ndistribution-free prediction interval C(Xn+1) \u2286 R that is likely to contain the unknown response\nYn+1. That is, given a desired miscoverage rate \u03b1, we ask that\nP{Yn+1 \u2208 C(Xn+1)} \u2265 1 \u2212 \u03b1 (1)\nfor any joint distribution PXY and any sample size n. The probability in this statement is marginal,\nbeing taken over all the samples {(Xi, Yi)}\nn+1\ni=1 .\nTo accomplish this, we build on the method of conformal prediction [2, 3, 8]. We first split the\ntraining data into two disjoint subsets, a proper training set and a calibration set.2 We fit two quantile\nregressors on the proper training set to obtain initial estimates of the lower and upper bounds of the\nprediction interval, as explained in Section 2. Then, using the calibration set, we conformalize and, if\nnecessary, correct this prediction interval. Unlike the original interval, the conformalized prediction\ninterval is guaranteed to satisfy the coverage requirement (1) regardless of the choice or accuracy of\nthe quantile regression estimator. We prove this in Section 4.\nOur method differs from the standard method of conformal prediction [3, 15], recalled in Section 3,\nin that we calibrate the prediction interval using conditional quantile regression, while the standard\nmethod uses only classical, conditional mean regression. The result is that our intervals are adaptive\nto heteroscedasticity whereas the standard intervals are not. We evaluate the statistical efficiency of\nour framework by comparing its miscoverage rate and average interval length with those of other\nmethods. We review existing state-of-the-art schemes for conformal prediction in Section 5 and we\ncompare them with our method in Section 6. Based on extensive experiments across eleven datasets,\nwe conclude that conformal quantile regression yields shorter intervals than the competing methods.\n2 Quantile regression\nThe aim of conditional quantile regression [18] is to estimate a given quantile, such as the median, of\nY conditional on X. Recall that the conditional distribution function of Y given X = x is\nF(y | X = x) := P{Y \u2264 y | X = x},\nand that the \u03b1th conditional quantile function is\nq\u03b1(x) := inf{y \u2208 R : F(y | X = x) \u2265 \u03b1}.\nFix the lower and upper quantiles to be equal to \u03b1lo = \u03b1/2 and \u03b1hi = 1 \u2212 \u03b1/2, say. Given the\npair q\u03b1lo (x) and q\u03b1hi(x) of lower and upper conditional quantile functions, we obtain a conditional\nprediction interval for Y given X = x, with miscoverage rate \u03b1, as\nC(x) = [q\u03b1lo (x), q\u03b1hi(x)]. (2)\n1An implementation of CQR is available online at https://github.com/yromano/cqr.\n2Like conformal regression, CQR has a variant that does not require data splitting.\n2\nBy construction, this interval satisfies\nP{Y \u2208 C(X)|X = x} \u2265 1 \u2212 \u03b1. (3)\nNotice that the length of the interval C(X) can vary greatly depending on the value of X. The\nuncertainty in the prediction of Y is naturally reflected in the length of the interval. In practice we\ncannot know this ideal prediction interval, but we can try to estimate it from the data.\nEstimating quantiles from data\nClassical regression analysis estimates the co...",
      "url": "https://papers.neurips.cc/paper/8613-conformalized-quantile-regression.pdf"
    },
    {
      "title": "Statistics > Methodology",
      "text": "[2307.00835] Engression: Extrapolation through the Lens of Distributional Regression\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2307.00835\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Methodology\n**arXiv:2307.00835**(stat)\n[Submitted on 3 Jul 2023 ([v1](https://arxiv.org/abs/2307.00835v1)), last revised 5 Jul 2024 (this version, v3)]\n# Title:Engression: Extrapolation through the Lens of Distributional Regression\nAuthors:[Xinwei Shen](https://arxiv.org/search/stat?searchtype=author&amp;query=Shen,+X),[Nicolai Meinshausen](https://arxiv.org/search/stat?searchtype=author&amp;query=Meinshausen,+N)\nView a PDF of the paper titled Engression: Extrapolation through the Lens of Distributional Regression, by Xinwei Shen and Nicolai Meinshausen\n[View PDF](https://arxiv.org/pdf/2307.00835)[HTML (experimental)](https://arxiv.org/html/2307.00835v3)> > Abstract:\n> Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree-ensemble based quantile regression. We propose a neural network-based distributional regression methodology called `engression&#39;. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for `pre-additive noise&#39; models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method and indicate that the pre-additive noise model is typically suitable for many real-world scenarios. The software implementations of engression are available in both R and Python. Subjects:|Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2307.00835](https://arxiv.org/abs/2307.00835)[stat.ME]|\n|(or[arXiv:2307.00835v3](https://arxiv.org/abs/2307.00835v3)[stat.ME]for this version)|\n|[https://doi.org/10.48550/arXiv.2307.00835](https://doi.org/10.48550/arXiv.2307.00835)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Xinwei Shen [[view email](https://arxiv.org/show-email/c5cf6202/2307.00835)]\n**[[v1]](https://arxiv.org/abs/2307.00835v1)**Mon, 3 Jul 2023 08:19:00 UTC (9,378 KB)\n**[[v2]](https://arxiv.org/abs/2307.00835v2)**Fri, 15 Sep 2023 13:38:09 UTC (9,353 KB)\n**[v3]**Fri, 5 Jul 2024 04:06:23 UTC (9,392 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Engression: Extrapolation through the Lens of Distributional Regression, by Xinwei Shen and Nicolai Meinshausen\n* [View PDF](https://arxiv.org/pdf/2307.00835)\n* [HTML (experimental)](https://arxiv.org/html/2307.00835v3)\n* [TeX Source](https://arxiv.org/src/2307.00835)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ME\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2307.00835&amp;function=prev&amp;context=stat.ME) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2307.00835&amp;function=next&amp;context=stat.ME)\n[new](https://arxiv.org/list/stat.ME/new)|[recent](https://arxiv.org/list/stat.ME/recent)|[2023-07](https://arxiv.org/list/stat.ME/2023-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2307.00835?context=cs)\n[cs.LG](https://arxiv.org/abs/2307.00835?context=cs.LG)\n[stat](https://arxiv.org/abs/2307.00835?context=stat)\n[stat.ML](https://arxiv.org/abs/2307.00835?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2307.00835)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2307.00835)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2307.00835)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2307.00835&amp;description=Engression: Extrapolation through the Lens of Distributional Regression>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2307.00835&amp;title=Engression: Extrapolation through the Lens of Distributional Regression>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHav...",
      "url": "https://arxiv.org/abs/2307.00835"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2106.05515] Understanding the Under-Coverage Bias in Uncertainty Estimation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2106.05515\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2106.05515**(cs)\n[Submitted on 10 Jun 2021]\n# Title:Understanding the Under-Coverage Bias in Uncertainty Estimation\nAuthors:[Yu Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+Y),[Song Mei](https://arxiv.org/search/cs?searchtype=author&amp;query=Mei,+S),[Huan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[Caiming Xiong](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+C)\nView a PDF of the paper titled Understanding the Under-Coverage Bias in Uncertainty Estimation, by Yu Bai and 3 other authors\n[View PDF](https://arxiv.org/pdf/2106.05515)> > Abstract:\n> Estimating the data uncertainty in regression tasks is often done by learning a quantile function or a prediction interval of the true label conditioned on the input. It is frequently observed that quantile regression -- a vanilla algorithm for learning quantiles with asymptotic guarantees -- tends to \\emph{under-cover} than the desired coverage level in reality. While various fixes have been proposed, a more fundamental understanding of why this under-coverage bias happens in the first place remains elusive.\n> In this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation algorithms in learning quantiles. We prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for $\\alpha&gt;0.5$ and small $d/n$, the $\\alpha$-quantile learned by quantile regression roughly achieves coverage $\\alpha - (\\alpha-1/2)\\cdot d/n$ regardless of the noise distribution, where $d$ is the input dimension and $n$ is the number of training data. Our theory reveals that this under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regression. Experiments on simulated and real data verify our theory and further illustrate the effect of various factors such as sample size and model capacity on the under-coverage bias in more practical setups. Subjects:|Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)|\nCite as:|[arXiv:2106.05515](https://arxiv.org/abs/2106.05515)[cs.LG]|\n|(or[arXiv:2106.05515v1](https://arxiv.org/abs/2106.05515v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2106.05515](https://doi.org/10.48550/arXiv.2106.05515)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yu Bai [[view email](https://arxiv.org/show-email/1204e07e/2106.05515)]\n**[v1]**Thu, 10 Jun 2021 06:11:55 UTC (79 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Understanding the Under-Coverage Bias in Uncertainty Estimation, by Yu Bai and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2106.05515)\n* [TeX Source](https://arxiv.org/src/2106.05515)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2106.05515&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2106.05515&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-06](https://arxiv.org/list/cs.LG/2021-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2106.05515?context=cs)\n[math](https://arxiv.org/abs/2106.05515?context=math)\n[math.ST](https://arxiv.org/abs/2106.05515?context=math.ST)\n[stat](https://arxiv.org/abs/2106.05515?context=stat)\n[stat.ML](https://arxiv.org/abs/2106.05515?context=stat.ML)\n[stat.TH](https://arxiv.org/abs/2106.05515?context=stat.TH)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2106.05515)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2106.05515)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2106.05515)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2106.html#abs-2106-05515)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2106-05515)\n[Yu Bai](<https://dblp.uni-trier.de/search/author?author=Yu Bai>)\n[Song Mei](<https://dblp.uni-trier.de/search/author?author=Song Mei>)\n[Huan Wang](<https://dblp.uni-trier.de/search/author?author=Huan Wang>)\n[Caiming Xiong](<https://dblp.uni-trier.de/search/author?author=Caiming Xiong>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2106.05515&amp;description=Understanding the Under-Coverage Bias in Uncertainty Estimation>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2106.05515&amp;title=Understanding the Under-Coverage Bias in Uncertainty Estimation>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Re...",
      "url": "https://arxiv.org/abs/2106.05515"
    },
    {
      "title": "",
      "text": "Understanding the Under-Coverage Bias\nin Uncertainty Estimation\nYu Bai\u2217 Song Mei\u2020 Huan Wang\u2021 Caiming Xiong\u2021\nJune 11, 2021\nAbstract\nEstimating the data uncertainty in regression tasks is often done by learning a quantile\nfunction or a prediction interval of the true label conditioned on the input. It is frequently\nobserved that quantile regression\u2014a vanilla algorithm for learning quantiles with asymptotic\nguarantees\u2014tends to under-cover than the desired coverage level in reality. While various fixes\nhave been proposed, a more fundamental understanding of why this under-coverage bias happens\nin the first place remains elusive.\nIn this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation\nalgorithms in learning quantiles. We prove that quantile regression suffers from an inherent\nunder-coverage bias, in a vanilla setting where we learn a realizable linear quantile function\nand there is more data than parameters. More quantitatively, for \u03b1 > 0.5 and small d/n, the\n\u03b1-quantile learned by quantile regression roughly achieves coverage \u03b1 \u2212 (\u03b1 \u2212 1/2)\u00b7 d/n regardless\nof the noise distribution, where d is the input dimension and n is the number of training data.\nOur theory reveals that this under-coverage bias stems from a certain high-dimensional parameter\nestimation error that is not implied by existing theories on quantile regression. Experiments on\nsimulated and real data verify our theory and further illustrate the effect of various factors such\nas sample size and model capacity on the under-coverage bias in more practical setups.\n1 Introduction\nThis paper is concerned with the problem of uncertainty estimation in regression problems. Uncer\u0002tainty estimation is an increasingly important task in modern machine learning applications\u2014Models\nshould not only make high-accuracy predictions, but also have a sense of how much the true label\nmay deviate from the prediction. This capability is crucial for deploying machine learning in the real\nworld, in particular in risk-sensitive domains such as medical AI [15, 29], self-driving cars [47], and\nso on. A common approach for uncertainty estimation in regression is to learn a quantile function or\na prediction interval of the true label conditioned on the input, which provides useful distributional\ninformation about the label. Such learned quantiles are typically evaluated by their coverage, i.e.,\nprobability that it covers the true label on a new test example. For example, a learned 90% upper\nquantile function should be an actual upper bound of the true label at least 90% of the time.\nAlgorithms for learning quantiles date back to the classical quantile regression [35], which\nestimates the quantile function by solving an empirical risk minimization problem with a suitable loss\nfunction that depends on the desired quantile level \u03b1. Quantile regression is conceptually simple, and\n\u2217Salesforce Research. E-mail: yu.bai@salesforce.com\n\u2020University of California, Berkeley. E-mail: songmei@berkeley.edu\n\u2021Salesforce Research. E-mail: {huan.wang, cxiong}@salesforce.com\n1\narXiv:2106.05515v1 [cs.LG] 10 Jun 2021\nis theoretically shown to achieve asymptotically correct coverage as the sample size goes to infinity [34]\nor approximately correct coverage in finite samples under specific modeling assumptions [46, 60, 56].\nHowever, it is observed that quantile regression often under-covers than the desired coverage level\nin practice [53]. Various alternative approaches for constructing quantiles and confidence intervals\nare proposed in more recent work, for example by aggregating multiple predictions using Bayesian\nneural networks or ensembles [24, 37], or by building on the conformal prediction technique to\nconstruct prediction intervals with finite-sample coverage guarantees [68, 66, 39, 53]. However, despite\nthese advances, a more fundamental understanding on why vanilla quantile regression exhibits this\nunder-coverage bias is still lacking.\nThis paper revisits quantile regression and presents a first precise theoretical study on its coverage,\nin a new regime where the number of samples n is proportional to the dimension d, and the ratio d/n\nis small (so that the problem is under-parametrized). Our main result shows that quantile regression\nexhibits an inherent under-cover bias under this regime, even in the well-specified setting of learning\na linear quantile function when the true data distribution follows a Gaussian linear model. To the\nbest of our knowledge, this is the first rigorous theoretical justification of the under-coverage bias.\nOur main contributions are summarized as follows.\n\u2022 We prove that linear quantile regression exhibits an inherent under-coverage bias in the well\u0002specified setting where the data is generated from a Gaussian linear model, and the number\nof samples n is proportional to the feature dimension d with a small d/n (Section 3). More\nquantitatively, quantile regression at nominal level \u03b1 \u2208 (0.5, 1) roughly achieves coverage\n\u03b1 \u2212 (\u03b1 \u2212 1/2)d/n regardless of the noise distribution. To the best of our knowledge, this is the\nfirst rigorous characterization of the under-coverage bias in quantile regression.\n\u2022 Towards understanding the source of this under-coverage bias, we disentangle the effect of\nestimating the bias and estimating the linear coefficient on the coverage of the learned linear\nquantile (Section 4). We show that the estimation error in the bias can have either an under\u0002coverage or over-coverage effect, depending on the noise distribution. In contrast, the estimation\nerror in the linear coefficient always drives the learned quantile to under-cover, and we show this\neffect is present even on broader classes of data distributions beyond the Gaussian linear model.\n\u2022 We perform experiments on simulated and real data to test our theory (Section 5). Our\nsimulations show that the coverage of quantile regression in Gaussian linear models agrees well\nwith our precise theoretical formula as well as the \u03b1\u2212(\u03b1\u22121/2)d/n approximation. On real data,\nwe find quantile regression using high-capacity models (such as neural networks) exhibits severe\nunder-coverage biases, while linear quantile regression can also have a mild but non-negligible\namount of under-coverage, even after we remove the potential effect of model misspecification.\n\u2022 On the technical end, our analysis builds on recent understandings of empirical risk minimization\nproblems in the high-dimensional proportional limit with a small d/n, and develops new techniques\nsuch as a novel concentration argument to deal with an additional learnable variable in learning\nlinear models with biases, which we believe could be of further interest (Section 6).\n1.1 Related work\nAlgorithms for uncertainty estimation in regression The earliest methods for uncertainty\nestimation in regression adopted subsampling methods (bootstrap) or leave-one-out methods (Jack\u0002knife) for assessing or calibrating prediction uncertainty [52, 63, 58, 25]. More recently, a growing line\nof work builds on the idea of conformal prediction [55] to design uncertainty estimation algorithms\nfor regression. These algorithms provide confidence bounds or prediction intervals by post-processing\n2\nany predictor, and can achieve distribution-free finite-sample marginal coverage guarantees uti\u0002lizing exchangeability of the data [50, 66, 39, 53, 33, 67, 69, 68, 11]. Further modifications of\nthe conformal prediction technique can yield stronger guarantees such as group coverage [10] or\ncoverage under distribution shift [9] under additional assumptions. Our under-coverage results\nadvocate the necessity of such post-processing techniques, and are complementary in the sense\nthat we provide understandings on the more vanilla quantile regression algorithm. Quantiles and\nprediction intervals can also be obtained by aggregating multiple predictors, such as using Bayesian\nneural networks [41, 24, 32, 44, 42] or ensembles [37, 49, 28, 45]. These methods offer an alternative\napproac...",
      "url": "https://arxiv.org/pdf/2106.05515"
    },
    {
      "title": "Two-level quantile regression forests for bias correction in range prediction",
      "text": "Advertisement\n\n# Two-level quantile regression forests for bias correction in range prediction\n\n- Published: 19 July 2014\n\n- Volume\u00a0101,\u00a0pages 325\u2013343, (2015)\n- [Cite this article](https://link.springer.com/article/10.1007/s10994-014-5452-1?error=cookies_not_supported&code=2e6e9dc1-9d9b-4e74-a0b1-7d30e4d298dd#citeas)\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10994-014-5452-1.pdf)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/10994?as=webp)Machine Learning](https://link.springer.com/journal/10994) [Aims and scope](https://link.springer.com/journal/10994/aims-and-scope) [Submit manuscript](https://submission.springernature.com/new-submission/10994/3)\n\nTwo-level quantile regression forests for bias correction in range prediction\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10994-014-5452-1.pdf)\n\n## Abstract\n\nQuantile regression forests (QRF), a tree-based ensemble method for estimation of conditional quantiles, has been proven to perform well in terms of prediction accuracy, especially for range prediction. However, the model may have bias and suffer from working with high dimensional data (thousands of features). In this paper, we propose a new bias correction method, called bcQRF that uses bias correction in QRF for range prediction. In bcQRF, a new feature weighting subspace sampling method is used to build the first level QRF model. The residual term of the first level QRF model is then used as the response feature to train the second level QRF model for bias correction. The two-level models are used to compute bias-corrected predictions. Extensive experiments on both synthetic and real world data sets have demonstrated that the bcQRF method significantly reduced prediction errors and outperformed most existing regression random forests. The new method performed especially well on high dimensional data.\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-62362-3?as=webp)\n\n### [On Random-Forest-Based Prediction Intervals](https://link.springer.com/10.1007/978-3-030-62362-3_16?fromPaywallRec=false)\n\nChapter\u00a9 2020\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs11634-018-0318-1/MediaObjects/11634_2018_318_Figa_HTML.gif)\n\n### [An efficient random forests algorithm for high dimensional data classification](https://link.springer.com/10.1007/s11634-018-0318-1?fromPaywallRec=false)\n\nArticle21 March 2018\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs40484-017-0121-6/MediaObjects/40484_2017_121_Fig1_HTML.jpg)\n\n### [Variable importance-weighted random forests](https://link.springer.com/10.1007/s40484-017-0121-6?fromPaywallRec=false)\n\nArticle06 November 2017\n\n### Explore related subjects\n\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\n\n- [Quantitative Finance](https://link.springer.com/subjects/quantitative-finance)\n- [Quantitative trait](https://link.springer.com/subjects/quantitative-trait)\n- [Quantum Cascade Lasers](https://link.springer.com/subjects/quantum-cascade-lasers)\n- [Quantitative Psychology](https://link.springer.com/subjects/quantitative-psychology)\n- [Statistical Learning](https://link.springer.com/subjects/statistical-learning)\n- [Predictive markers](https://link.springer.com/subjects/predictive-markers)\n\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10994)\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nRandom forests (RF) (Breiman [2001](https://link.springer.com/article/10.1007/s10994-014-5452-1#ref-CR3)) is a non-parametric regression method that builds an ensemble model of regression trees from random subsets of features and bagged samples of the training data. Given a training data set:\n\n$$\\\\begin{aligned} \\\\mathcal {L}=\\\\left\\\\{ (X\\_i, Y\\_i)\\_{i=1}^N \\\\,\\|\\\\, X\\_i \\\\in \\\\mathbb {R}^M, Y \\\\in \\\\mathbb {R}^1 \\\\right\\\\} , \\\\end{aligned}$$\n\nwhere \\\\(N\\\\) is the number of training samples (also called objects) and \\\\(M\\\\) is the number of features, a regression RF independently and uniformly samples with replacement the training data \\\\(\\\\mathcal {L}\\\\) to draw a bootstrap data set \\\\(\\\\mathcal {L}^\\*\\_k\\\\) from which a regression tree \\\\(T^\\*\\_k\\\\) is grown. Repeating this process for \\\\(K\\\\) replicates produces \\\\(K\\\\) bootstrap data sets and \\\\(K\\\\) corresponding regression trees \\\\(T^\\*\\_1, T^\\*\\_2,\\\\ldots ,T^\\*\\_K\\\\) which form a regression RF.\n\nGiven an input \\\\(X=x\\\\), a regression RF is used as a function \\\\(f: \\\\mathbb {R}^M \\\\rightarrow \\\\mathbb {R}^1\\\\) to estimate the unknown value \\\\(y\\\\) of input \\\\(x \\\\in \\\\mathbb {R}^M\\\\), denoted as \\\\(\\\\hat{f}(x)\\\\). Write the regression RF in the common regression form\n\n$$\\\\begin{aligned} Y=f(X)+\\\\varepsilon , \\\\end{aligned}$$\n\n(1)\n\nwhere \\\\(E(\\\\varepsilon )=0, Var(\\\\varepsilon )=\\\\sigma ^2\\_\\\\varepsilon \\\\). The function \\\\(f(\\\\cdot )\\\\) is estimated from \\\\(\\\\mathcal {L}\\\\) and the prediction \\\\(\\\\hat{f}(x)\\\\) is obtained from an independent test case \\\\(x\\\\).\n\nFor point regression with a regression RF, each tree \\\\(T\\_k\\\\) gives a prediction \\\\(\\\\hat{f}\\_k (x)\\\\) and the predictions of all trees are averaged to produce the final RF prediction\n\n$$\\\\begin{aligned} \\\\hat{f}(x)=\\\\frac{1}{K}\\\\sum \\_{k=1}^K \\\\hat{f}\\_k(x). \\\\end{aligned}$$\n\n(2)\n\nThis is the estimation of \\\\(f(x)=E(Y\\|X=x)\\\\). The mean-squared error of the prediction measures the effectiveness of \\\\(\\\\hat{f}\\\\), defined as (Hastie et al. [2009](https://link.springer.com/article/10.1007/s10994-014-5452-1#ref-CR6))\n\n$$\\\\begin{aligned} Err(x)&= E\\\\left\\[ (Y-\\\\hat{f}(x))^2 \\| X=x\\\\right\\] \\\\nonumber \\\\\\&= \\\\sigma ^2\\_\\\\varepsilon + \\\\left\\[ E\\\\hat{f}(x) -f(x)\\\\right\\] ^2 +E\\\\left\\[ \\\\hat{f}(x)-E\\\\hat{f}(x)\\\\right\\] ^2 \\\\nonumber \\\\\\&= \\\\sigma ^2\\_\\\\varepsilon +\\\\textit{Bias}^2(\\\\hat{f}(x)) + \\\\textit{Var}(\\\\hat{f}(x)) \\\\nonumber \\\\\\&= \\\\textit{Irreducible Error} + \\\\textit{Bias}^2 + \\\\textit{Variance}. \\\\end{aligned}$$\n\n(3)\n\nThe first term is the variance of the target around its true mean \\\\(f(x)\\\\). This cannot be avoided no matter how well \\\\(\\\\hat{f}(x)\\\\) is estimated, unless \\\\(\\\\sigma ^2\\_\\\\varepsilon =0\\\\). The second term is the squared bias and the last term is the variance. The last two terms need to be addressed for a good performance of the prediction model.\n\nGiven an input object \\\\(x\\\\), a regression RF predicts a value in each leaf node which is the mean of \\\\(Y\\\\) values of the objects in that leaf node. This value can be biased because large and small values in the objects of the leaf node are often underestimated or overestimated. The prediction accuracy can be improved if the median is used (instead of the mean) as the prediction and the median surpasses the mean in robustness towards extreme values/outliers.\n\nMeinshausen ( [2006](https://link.springer.com/article/10.1007/s10994-014-5452-1#ref-CR10)) proposed quantile regression forests (QRF) for both point and range prediction. QRF uses the median in point regression. For range prediction, QRF requires the estimated distribution of \\\\(F(y\\|X = x) = P(Y < y\\|X = x)\\\\) at each leaf node, not only the mean. Given two quantile probabilities \\\\(\\\\alpha \\_l\\\\) and \\\\(\\\\alpha \\_h\\\\), QRF predicts the range \\\\(\\[Q\\_{\\\\alpha \\_l}(x), Q\\_{\\\\alpha \\_h}(x)\\]\\\\) of \\\\(Y\\\\) with a given probability \\\\(\\\\tau \\\\) that\n\n$$\\\\begin{aligned} P(Q\\_{\\\\alpha \\_l}(x) < Y < Q\\_{\\\\alpha \\_h}(x) \\|X = x) = \\\\tau . \\\\end{aligned}$$\n\nBesides range prediction, QRF also performs well in situations where the conditional distributions are not Gaussian. However, similar to regression RF, QRF can still be biased in point prediction even though the median is used instead of the mean in prediction.\n\nTo illustrate this kind of bias, we generated 200 objects as a training data set and 1000 objects as the testing data set using the following model:\n\n...",
      "url": "https://link.springer.com/article/10.1007/s10994-014-5452-1?error=cookies_not_supported&code=2e6e9dc1-9d9b-4e74-a0b1-7d30e4d298dd"
    }
  ]
}