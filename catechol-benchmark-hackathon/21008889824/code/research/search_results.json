{
  "query": "How can I reduce the generalization gap between cross-validation and test performance when predicting chemical properties for unseen molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the gap between cross\u2011validation (CV) scores and the performance on truly unseen molecules can be achieved by adopting validation schemes and model\u2011calibration practices that better reflect out\u2011of\u2011distribution (OOD) chemistry\u202f([Fooladi\u202fet\u202fal., 2025](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866); [Rosseinsky\u202fet\u202fal., 2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)):\n\n1. **Use OOD\u2011focused splits instead of random k\u2011fold** \u2013 generate test folds by scaffold\u2011based or chemical\u2011similarity clustering (e.g., Bemis\u2011Murcko scaffolds or K\u2011means on ECFP4). Scaffold splits retain a strong ID\u2011OOD correlation (Pearson\u202f\u2248\u202f0.9), while cluster\u2011based splits expose the hardest cases (Pearson\u202f\u2248\u202f0.4) and give a more realistic estimate of generalization\u202f([Fooladi\u202fet\u202fal., 2025](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).\n\n2. **Apply \u201cleave\u2011one\u2011cluster\u2011out\u201d CV (LOCO\u2011CV)** \u2013 treat each chemical cluster (e.g., scaffold or similarity group) as a single unit and iteratively hold it out as the validation set. This directly measures how the model performs on entirely unseen groups of molecules\u202f([Rosseinsky\u202fet\u202fal., 2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)).\n\n3. **Adopt k\u2011fold n\u2011step forward (step\u2011forward) CV** \u2013 order the data chronologically or by similarity, then progressively expand the training set while keeping the most recent (or most dissimilar) compounds as the validation fold. This mimics prospective deployment and improves OOD prediction accuracy\u202f([Schindler\u202fet\u202fal., 2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\n4. **Ensemble multiple CV models** \u2013 train several models on different CV folds and average their predictions. Ensembles not only boost predictive performance but also provide a basis for reliable uncertainty estimates\u202f([Baumann\u202fet\u202fal., 2023](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9)).\n\n5. **Quantify and calibrate predictive uncertainty** \u2013 use probabilistic models (e.g., DIONYSUS) or Bayesian GNNs to obtain calibrated confidence intervals. Well\u2011calibrated uncertainties help identify molecules that lie outside the model\u2019s applicability domain, reducing over\u2011optimistic CV scores\u202f([Aspuru\u2011Guzik\u202fet\u202fal., 2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).\n\n6. **Validate on an external hold\u2011out set** \u2013 after selecting hyper\u2011parameters with the OOD\u2011aware CV scheme, evaluate the final model on a completely independent dataset (different source, assay, or time period). This final check confirms that the CV protocol has indeed narrowed the generalization gap.\n\n7. **Iterate the data split design** \u2013 if the external test still underperforms, refine the clustering criteria (e.g., increase fingerprint radius, incorporate physicochemical descriptors) and repeat steps\u202f1\u20135 until CV and external metrics converge.\n\nFollowing these steps aligns the validation process with the chemical diversity encountered in real\u2011world screening, thereby minimizing the discrepancy between cross\u2011validation estimates and true test performance.",
      "url": ""
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/dd/d2dd00039c)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00046f)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00056c)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D2DD00039C](https://doi.org/10.1039/D2DD00039C)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2022, **1**, 763-778\n\n# Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties [\u2020](https://pubs.rsc.org/pubs.rsc.org\\#fn1)\n\nSamantha\nDurdy\n\\*ab,\nMichael W.\nGaultois\nbc,\nVladimir V.\nGusev\nbc,\nDanushka\nBollegala\nab and Matthew J.\nRosseinsky\nbcaDepartment of Computer Science, University of Liverpool, Ashton Street, Liverpool, L69 3BX, UK. E-mail: [samantha.durdy@liverpool.ac.uk](mailto:samantha.durdy@liverpool.ac.uk)bLeverhulme Research Centre for Functional Materials Design, University of Liverpool, 51 Oxford Street, Liverpool, L7 3NY, UKcDepartment of Chemistry, University of Liverpool, Crown St, Liverpool, L69 7ZD, UK\n\nReceived\n9th May 2022\n, Accepted 31st August 2022\n\nFirst published on 2nd September 2022\n\n## Abstract\n\nWith machine learning being a popular topic in current computational materials science literature, creating representations for compounds has become common place. These representations are rarely compared, as evaluating their performance \u2013 and the performance of the algorithms that they are used with \u2013 is non-trivial. With many materials datasets containing bias and skew caused by the research process, leave one cluster out cross validation (LOCO-CV) has been introduced as a way of measuring the performance of an algorithm in predicting previously unseen groups of materials. This raises the question of the impact, and control, of the range of cluster sizes on the LOCO-CV measurement outcomes. We present a thorough comparison between composition-based representations, and investigate how kernel approximation functions can be used to better separate data to enhance LOCO-CV applications. We find that domain knowledge does not improve machine learning performance in most tasks tested, with band gap prediction being the notable exception. We also find that the radial basis function improves the linear separability of chemical datasets in all 10 datasets tested and provides a framework for the application of this function in the LOCO-CV process to improve the outcome of LOCO-CV measurements regardless of machine learning algorithm, choice of metric, and choice of compound representation. We recommend kernelised LOCO-CV as a training paradigm for those looking to measure the extrapolatory power of an algorithm on materials data.\n\n## 1 Introduction\n\nRecent advances in materials science have seen a plethora of research into application of machine learning (ML) algorithms. Much of this research has focused on supervised ML methods, such as random forests (RFs) and neural networks. More recently, authors have laid out the best practices to help unify and progress this field. [1\u20134](https://pubs.rsc.org/pubs.rsc.org#cit1)\n\nData representation can play a large role in the performance of ML algorithms; however, optimum choice of representation is not always apparent. In materials science it is often difficult to choose an appropriate representation due to variability in the ML task and in the nature of the chemistry, composition and structures of the materials studied. Additionally, some properties of a material, such its crystal structure in the case of crystalline materials, may not be known until its synthesis. Accordingly, many studies derive representations from either the ratios of elements in the chemical composition, or from domain knowledge-based properties (referred to as features) of these elements, or both, in a process called \u201cfeaturisation\u201d.\n\nGiven the ubiquity of featurisation methods such as those presented here in materials applications, it is important to evaluate the statistical advantage of specific feature sets. [5](https://pubs.rsc.org/pubs.rsc.org#cit5) Section 2.1 overviews different featurisation techniques and how their effectiveness has been previously reported. We expand on this evaluation in Section 3.1, in which seven representations are investigated across five case studies from the literature to explore how these representations perform in published ML tasks. These cases thus represent practical applications, rather than constructed tasks. Each of these representations is also compared to a random projection of equal size to establish the performance benefit of domain knowledge over random noise.\n\nEvaluating the generalisability of ML models is a known challenge across data science, and is of particular concern in materials science, where data sets are of limited size compared with other application areas for ML, and often biased towards historically interesting materials or those closely related to known high-performance materials for certain performance metrics. Typically, models are evaluated on test sets separate from their training data, through a consistent train:test split or N-fold cross validation. However, this does not consider skew in a dataset. In chemical datasets, families of promising materials are often explored more thoroughly than the domain as a whole, which introduces bias and reduces the generalisability of ML models because the data they are trained and tested on are not sampled in a way representative of the domain of target chemistries to be screened with these models. Investigations into how such skew can affect ML models has seen that this skew can result in overfitting [6](https://pubs.rsc.org/pubs.rsc.org#cit6) and that more skewed datasets require more data points in order to train models to achieve similar predictive performance when compared to models trained on less skewed datasets. [7](https://pubs.rsc.org/pubs.rsc.org#cit7)\n\nLeave one cluster out cross validation (LOCO-CV) was suggested to combat this, [8](https://pubs.rsc.org/pubs.rsc.org#cit8) using K-means clustering to exclude similar families of materials from the training set to measure the extrapolatory power of an ML algorithm (its ability to predict the performance of materials with chemistries qualitatively different from the training set). The value of such an approach can be seen in the case of predicting new classes of superconductors. One may choose to remove cuprate superconductors from the training set, and if an ML model can then successfully predict the existence of cuprate superconductors without prior knowledge of them, we can conclude that model is likely to perform better at predicting new classes of superconductors than a model which could not predict the existence of cuprate superconductors. LOCO-CV provides an algorithmic framework to measure the performance of models on predicting new classes of materials by defining these classes as clusters found by the K-means clustering algorithm. Application and implementation of this algorithm is discussed further in Section 2.2.1.\n\nWhile differences in cluster sizes in this domain are expected, it has been observed that clusters found with K-means can differ in size by orders of magnitude, [9](https://pubs.rsc.org/pubs.rsc.org#cit9) which can pose a practical challenge to adoption of this method. With such differences in cluster size, LOCO-CV measurements can represent the performance of an algorithm on a small training set rather than the performance of an algorithm in extrapolation. As representation plays a role in clustering, it is pertinent to investigate the issues of representation and clustering together, even though the representation used in clustering does not need to be the same as that used to train the model ( [F...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation",
      "text": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00709-9?)\n# Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 April 2023\n* Volume\u00a015, article\u00a0number49, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nLarge-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n* [Thomas-Martin Dutschmann](#auth-Thomas_Martin-Dutschmann-Aff1)[1](#Aff1),\n* [Lennart Kinzel](#auth-Lennart-Kinzel-Aff1)[1](#Aff1),\n* [Antonius ter Laak](#auth-Antonius-Laak-Aff2)[2](#Aff2)&amp;\n* \u2026* [Knut Baumann](#auth-Knut-Baumann-Aff1)[1](#Aff1)Show authors\n* 7359Accesses\n* 50Citations\n* 3Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9/metrics)\n## Abstract\nIt is insightful to report an estimator that describes how certain a model is in a prediction, additionally to the prediction alone. For regression tasks, most approaches implement a variation of the ensemble method, apart from few exceptions. Instead of a single estimator, a group of estimators yields several predictions for an input. The uncertainty can then be quantified by measuring the disagreement between the predictions, for example by the standard deviation. In theory, ensembles should not only provide uncertainties, they also boost the predictive performance by reducing errors arising from variance. Despite the development of novel methods, they are still considered the \u201cgolden-standard\u201d to quantify the uncertainty of regression models. Subsampling-based methods to obtain ensembles can be applied to all models, regardless whether they are related to deep learning or traditional machine learning. However, little attention has been given to the question whether the ensemble method is applicable to virtually all scenarios occurring in the field of cheminformatics. In a widespread and diversified attempt, ensembles are evaluated for 32 datasets of different sizes and modeling difficulty, ranging from physicochemical properties to biological activities. For increasing ensemble sizes with up to 200 members, the predictive performance as well as the applicability as uncertainty estimator are shown for all combinations of five modeling techniques and four molecular featurizations. Useful recommendations were derived for practitioners regarding the success and minimum size of ensembles, depending on whether predictive performance or uncertainty quantification is of more importance for the task at hand.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-45630-5?as&#x3D;webp)\n### [Ensemble Models](https://link.springer.com/10.1007/978-3-031-45630-5_12?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-48317-7?as&#x3D;webp)\n### [Ensemble Methods for Time Series Forecasting](https://link.springer.com/10.1007/978-3-319-48317-7_13?fromPaywallRec=false)\nChapter\u00a9 2017\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-12240-8?as&#x3D;webp)\n### [Ensemble Models Using Symbolic Regression and Genetic Programming for Uncertainty Estimation in ESG and Alternative Investments](https://link.springer.com/10.1007/978-3-031-12240-8_5?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://jcheminf.biomedcentral.com/subjects/applied-probability)\n* [Applied Statistics](https://jcheminf.biomedcentral.com/subjects/applied-statistics)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://jcheminf.biomedcentral.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Statistical Theory and Methods](https://jcheminf.biomedcentral.com/subjects/statistical-theory-and-methods)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nMachine learning (ML) for drug design purposes holds a long tradition\u00a0[[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR1)], but has recently started to gain further attention due to the success of deep learning (DL)\u00a0[[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR2)]. Yet, the prediction of chemical properties and activities is only one step in a long and resource-intense process of drug design, discovery, and development. When developing ML models, predictions alone are not sufficient and require further analysis\u00a0[[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR3)]. During model construction and testing, errors made by the model can easily be evaluated since the true target values are known. The error distribution allows the estimation of the quality of the model, but cannot be applied when predicting values for new compounds with unknown target values. In this case, it is good practice to provide an estimate of the uncertainty associated with the prediction. Measures quantifying the predictive uncertainty can be used to set a threshold which defines the model\u2019s applicability domain. The latter is defined as follows: \u201cThe applicability domain of a (Q)SAR model is the response and chemical structure space in which the model makes predictions with a given reliability.\u201d\u00a0[[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR4)]. The reliability of a model can either be addressed by quantifying its confidence, or, conversely, its uncertainty. Recent studies make use of the term uncertainty quantification (UQ)\u00a0[[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR5)]. Uncertainty can be of aleatoric nature, relating to the random process that generates the target values, or epistemic nature, implying model-related uncertainty\u00a0[[6](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR6)]. Usually, these two types cannot be fully distinguished\u00a0[[7](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR7)].\nClassification algorithms often provide built-in mechanisms or augmentations to measure their uncertainty\u00a0[[8](https://jcheminf.biomedcentral.com/...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Evaluating generalizability of artificial intelligence models for molecular datasets",
      "text": "## References\n\n001. Green, A. G. et al. A convolutional neural network highlights mutations relevant to antimicrobial resistance in mycobacterium tuberculosis. _Nat. Commun._ **13**, 3817 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41467-022-31236-0) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20convolutional%20neural%20network%20highlights%20mutations%20relevant%20to%20antimicrobial%20resistance%20in%20mycobacterium%20tuberculosis&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-022-31236-0&volume=13&publication_year=2022&author=Green%2CAG)\n\n002. Kumar, V., Deepak, A., Ranjan, A. & Prakash, A. Lite-SeqCNN: a light-weight deep CNN architecture for protein function prediction. _IEEE/ACM Trans. Comput. Biol. Bioinform._ **20**, 2242\u20132253 (2023).\n\n     [Article](https://doi.org/10.1109%2FTCBB.2023.3240169) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Lite-SeqCNN%3A%20a%20light-weight%20deep%20CNN%20architecture%20for%20protein%20function%20prediction&journal=IEEE%2FACM%20Trans.%20Comput.%20Biol.%20Bioinform.&doi=10.1109%2FTCBB.2023.3240169&volume=20&pages=2242-2253&publication_year=2023&author=Kumar%2CV&author=Deepak%2CA&author=Ranjan%2CA&author=Prakash%2CA)\n\n003. Sanderson, T., Bileschi, M. L., Belanger, D. & Colwell, L. J. Proteinfer, deep neural networks for protein functional inference. _eLife_ **12**, e80942 (2023).\n\n     [Article](https://doi.org/10.7554%2FeLife.80942) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Proteinfer%2C%20deep%20neural%20networks%20for%20protein%20functional%20inference&journal=eLife&doi=10.7554%2FeLife.80942&volume=12&publication_year=2023&author=Sanderson%2CT&author=Bileschi%2CML&author=Belanger%2CD&author=Colwell%2CLJ)\n\n004. Bileschi, M. L. et al. Using deep learning to annotate the protein universe. _Nat. Biotechnol._ **40**, 932\u2013937 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41587-021-01179-w) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Using%20deep%20learning%20to%20annotate%20the%20protein%20universe&journal=Nat.%20Biotechnol.&doi=10.1038%2Fs41587-021-01179-w&volume=40&pages=932-937&publication_year=2022&author=Bileschi%2CML)\n\n005. Griffith, D. & Holehouse, A. S. Parrot is a flexible recurrent neural network framework for analysis of large protein datasets. _eLife_ **10**, e70576 (2021).\n\n     [Article](https://doi.org/10.7554%2FeLife.70576) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Parrot%20is%20a%20flexible%20recurrent%20neural%20network%20framework%20for%20analysis%20of%20large%20protein%20datasets&journal=eLife&doi=10.7554%2FeLife.70576&volume=10&publication_year=2021&author=Griffith%2CD&author=Holehouse%2CAS)\n\n006. Liu, X. Deep recurrent neural network for protein function prediction from sequence. Preprint at [https://arxiv.org/abs/1701.08318](https://arxiv.org/abs/1701.08318) (2017).\n\n007. Hill, S. T. et al. A deep recurrent neural network discovers complex biological rules to decipher RNA protein-coding potential. _Nucleic Acids Res._ **46**, 8105\u20138113 (2018).\n\n     [Article](https://doi.org/10.1093%2Fnar%2Fgky567) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20deep%20recurrent%20neural%20network%20discovers%20complex%20biological%20rules%20to%20decipher%20RNA%20protein-coding%20potential&journal=Nucleic%20Acids%20Res.&doi=10.1093%2Fnar%2Fgky567&volume=46&pages=8105-8113&publication_year=2018&author=Hill%2CST)\n\n008. Zhang, Z., Xu, M., Jamasb, A. R., Chenthamarakshan, V., Lozano, A., Das, P. & Tang, J. Protein representation learning by geometric structure pretraining. In _Proc. Eleventh International Conference on Learning Representations_ (2023).\n\n009. Somnath, V. R., Bunne, C. & Krause, A. Multi-scale representation learning on proteins. In _Advances in Neural Information Processing Systems_ Vol. 34 (eds Ranzato, M. et al.) 25244\u201325255 (Curran Associates, 2021).\n\n010. Jha, K., Saha, S. & Singh, H. Prediction of protein\u2013protein interaction using graph neural networks. _Sci. Rep._ **12**, 8360 (2022).\n\n     [Article](https://doi.org/10.1038%2Fs41598-022-12201-9) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Prediction%20of%20protein%E2%80%93protein%20interaction%20using%20graph%20neural%20networks&journal=Sci.%20Rep.&doi=10.1038%2Fs41598-022-12201-9&volume=12&publication_year=2022&author=Jha%2CK&author=Saha%2CS&author=Singh%2CH)\n\n011. Gao, Z. et al. Hierarchical graph learning for protein\u2013protein interaction. _Nat. Commun._ **14**, 1093 (2023).\n\n     [Article](https://doi.org/10.1038%2Fs41467-023-36736-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hierarchical%20graph%20learning%20for%20protein%E2%80%93protein%20interaction&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-023-36736-1&volume=14&publication_year=2023&author=Gao%2CZ)\n\n012. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_ **379**, 1123\u20131130 (2023).\n\n     [Article](https://doi.org/10.1126%2Fscience.ade2574) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4567681) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Evolutionary-scale%20prediction%20of%20atomic-level%20protein%20structure%20with%20a%20language%20model&journal=Science&doi=10.1126%2Fscience.ade2574&volume=379&pages=1123-1130&publication_year=2023&author=Lin%2CZ)\n\n013. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational protein engineering with sequence-based deep representation learning. _Nat. Methods_ **16**, 1315\u20131322 (2019).\n\n     [Article](https://doi.org/10.1038%2Fs41592-019-0598-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Unified%20rational%20protein%20engineering%20with%20sequence-based%20deep%20representation%20learning&journal=Nat.%20Methods&doi=10.1038%2Fs41592-019-0598-1&volume=16&pages=1315-1322&publication_year=2019&author=Alley%2CEC&author=Khimulya%2CG&author=Biswas%2CS&author=AlQuraishi%2CM&author=Church%2CGM)\n\n014. Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _Proc. Natl Acad. Sci. USA_ **118**, e2016239118 (2021).\n\n     [Article](https://doi.org/10.1073%2Fpnas.2016239118) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Biological%20structure%20and%20function%20emerge%20from%20scaling%20unsupervised%20learning%20to%20250%20million%20protein%20sequences&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2016239118&volume=118&publication_year=2021&author=Rives%2CA)\n\n015. Madani, A. et al. Large language models generate functional protein sequences across diverse families. _Nat. Biotechnol._ **41**, 1099\u20131106 (2023).\n\n     [Article](https://doi.org/10.1038%2Fs41587-022-01618-2) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Large%20language%20models%20generate%20functional%20protein%20sequences%20across%20diverse%20families&journal=Nat.%20Biotechnol.&doi=10.1038%2Fs41587-022-01618-2&volume=41&pages=1099-1106&publication_year=2023&author=Madani%2CA)\n\n016. Notin, P. et al. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In _Proc. 39th International Conference on Machine Learning_, Vol. 162 (eds Chaudhuri, K. et al.) 16990\u201317017 (PMLR, 2022).\n\n017. Wright, B. W., Yi, Z., Weissman, J. S. & Chen, J. The dark proteome: translation from noncanonical open reading frames. _Trends Cell Biol._ **32**, 243\u2013258 (2022).\n\n     [Article](https://doi.org/10.1016%2Fj.tcb.2021.10.010) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20dark%20proteome%3A%20translation%20from%20noncanonical%20open%20reading%20frames&journal=Trends%20Cell%20Biol.&doi=10.1016%2Fj.tcb.2021.10.010&volume=32&pages=243-258&publication_year=2022&author=Wright%2CBW&author=Yi%2CZ&author=Weissman%2CJS&author=Chen%2CJ)\n\n018. Liu, J. et al. Towards out-of-distribution generalization: a survey. Preprint at [htt...",
      "url": "https://www.nature.com/articles/s42256-024-00931-6"
    },
    {
      "title": "Characterizing Uncertainty in Machine Learning for Chemistry",
      "text": "Characterizing Uncertainty in Machine Learning for Chemistry | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Characterizing Uncertainty in Machine Learning for Chemistry\n08 February 2023, Version 1\nThis is not the most recent version. There is a[\nnewer version\n](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)of this content available\nWorking Paper\n## Authors\n* [Esther Heid](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Esther%20Heid)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-8404-6596),\n* [Charles J. McGill](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Charles%20J.%20McGill),\n* [Florence H. Vermeire](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Florence%20H.%20Vermeire),\n* [William H. Green](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=William%20H.%20Green)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nCharacterizing uncertainty in machine learning models has recently gained interest in the context of machine learning reliability, robustness, safety, and active learning. Here, we separate the total uncertainty into contributions from noise in the data (aleatoric) and shortcomings of the model (epistemic), further dividing epistemic uncertainty into model bias and variance contributions. We systematically address the influence of noise, model bias, and model variance in the context of chemical property predictions, where the diverse nature of target properties and the vast chemical chemical space give rise to many different distinct sources of prediction error. We demonstrate that different sources of error can each be significant in different contexts and must be individually addressed during model development. Through controlled experiments on datasets of molecular properties, we show important trends in model performance associated with the level of noise in the dataset, size of the dataset, model architecture, molecule representation, ensemble size, and dataset splitting. In particular, we show that 1) noise in the test set can limit a model's observed performance when the actual performance is much better, 2) using size-extensive model aggregation structures is crucial for extensive property prediction, 3) ensembling is a reliable tool for uncertainty quantification and improvement specifically for the contribution of model variance, and 4) evaluations of cross-validation models understate their performance. We develop general guidelines on how to improve an underperforming model when falling into different uncertainty contexts.\n## Keywords\n[Uncertainty](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Uncertainty)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[Chemical property prediction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Chemical%20property%20prediction)\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nCharacterizing Uncertainty in Machine Learning for Chemistry Scripts\n**Description**\nPython scripts to reproduce the results of the manuscript.\n**Actions**\n[**View**](https://github.com/cjmcgill/characterizing_uncertainty_scripts)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\n[May 17, 2023 Version\n3](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)\n[Feb 16, 2023 Version\n2](https://chemrxiv.org/engage/chemrxiv/article-details/63ecf74d1d2d1840638a8b75)\nFeb 08, 2023 Version 1\n## Metrics\n3,215\n1,200\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2023-00vcg\nD O I: 10.26434/chemrxiv-2023-00vcg [opens in a new tab]](https://doi.org/10.26434/chemrxiv-2023-00vcg)\n## Funding\n**Austrian Science Fund**\nJ-4415\n**Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS)**\n## Author\u2019s competing interest statement\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n## Ethics\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/63e2b82b6d032916bb650914"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    }
  ]
}