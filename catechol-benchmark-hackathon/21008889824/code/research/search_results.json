{
  "query": "What is the best approach to reduce the gap between local cross-validation score and leaderboard score when the intercept of the CV-LB relationship is too high?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most effective way to shrink a large, positive intercept in the CV\u2011vs\u2011leaderboard (LB) relationship is to treat the CV score as a biased, overly\u2011optimistic estimate and explicitly correct that bias before using it to predict LB performance.  Tibshirani\u202f&\u202fTibshirani show that the minimum CV error is systematically downward\u2011biased and propose a simple, virtually cost\u2011free bias\u2011estimate that can be subtracted from the CV score, yielding a more realistic prediction of test\u2011set error\u3010https://www.stat.berkeley.edu/~ryantibs/papers/cvbias.pdf\u3011.  Applying this correction (or, equivalently, using a nested\u2011CV or a held\u2011out \u201cvalidation\u2011set\u201d that mimics the competition\u2019s hold\u2011out) removes the inflated intercept and brings the local CV metric into line with the leaderboard metric.  \n\nIn practice you should (1) reserve a true out\u2011of\u2011sample hold\u2011out that matches the competition\u2019s evaluation split, (2) compute the bias\u2011corrected CV estimate (or use the 1\u2011standard\u2011error rule to pick a less\u2011optimistic model), and (3) optionally recalibrate the final predictions on that hold\u2011out before submission.  This workflow directly addresses the gap reported by practitioners who see CV AUC\u202f\u2248\u202f0.88 but leaderboard AUC\u202f\u2248\u202f0.67, a discrepancy that typically stems from optimistic CV bias and over\u2011fitting to the internal folds\u3010https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition\u3011.  By correcting the CV bias and validating on an untouched hold\u2011out, the intercept of the CV\u2011LB regression drops toward zero, narrowing the performance gap.",
      "url": ""
    },
    {
      "title": "",
      "text": "A Bias Correction for the Minimum Error Rate in\nCross-validation\nRyan J. Tibshirani\u2217 Robert Tibshirani\u2020\nAbstract\nTuning parameters in supervised learning problems are often esti\u0002mated by cross-validation. The minimum value of the cross-validation\nerror can be biased downward as an estimate of the test error at that\nsame value of the tuning parameter. We propose a simple method\nfor the estimation of this bias that uses information from the cross\u0002validation process. As a result, it requires essentially no additional\ncomputation. We apply our bias estimate to a number of popular clas\u0002sifiers in various settings, and examine its performance.\nKey words: cross-validation, prediction error estimation, optimism es\u0002timation\n1 Introduction\nCross-validation is widely used in regression and classification problems to\nchoose the value of a \u201ctuning parameter\u201d in a prediction model. By training\nand testing the model on separate subsets of the data, we get an idea of the\nmodel\u2019s prediction strength as a function of the tuning parameter, and we\nchoose the parameter value to minimize the CV error curve. This estimate\nadmits many nice properties (see Stone 1997 for a discussion of asymptotic\nconsistency and efficiency) and works well in practice.\nHowever, the minimum CV error itself tends to be too optimistic as an\nestimate of true prediction error. Many have noticed this downward bias\nin the minimum error rate. Breiman, Friedman, Olshen & Stone (1984)\n\u2217Dept. of Statistics, Stanford University, ryantibs@stanford.edu. Supported by a Na\u0002tional Science Foundation Vertical Integration of Graduate Research and Education fel\u0002lowship.\n\u2020Dept. of Statistics, Stanford University, tibs@stanford.edu. Supported in part by Na\u0002tional Science Foundation Grant DMS-9971405 and National Institutes of Health Contract\nN01-HV-28183.\n1\nacknowledge this bias in the context of classification and regression trees.\nEfron (2008) discusses this problem in the setting p \u226b n, and employs\nan empirical Bayes method, which does not involve cross-validation in the\nchoice of tuning parameters, to avoid such a bias. However, the proposed al\u0002gorithm requires an initial choice for a \u201ctarget error rate\u201d, which complicates\nmatters by introducing another tuning parameter. Varma & Simon (2006)\nsuggest a method using \u201cnested\u201d cross-validation to estimate the true error\nrate. This essentially amounts to doing a cross-validation procedure for ev\u0002ery data point, and is hence impractical in settings where cross-validation\nis computationally expensive.\nWe propose a bias correction for the minimum CV error rate in K-fold\ncross-validation. It is computed directly from the individual error curves\nfrom each fold, and hence does not require a significant amount of additional\ncomputation.\nFigure 1 shows an example. The data come from the laboratory of Dr.\nPat Brown of Stanford, consisting of gene expression measurements over\n4718 genes on 128 patient samples, 88 from healthy tissues and 30 from\nCNS tumors. We randomly divided the data in half, into training and test\nsamples, and applied the nearest shrunken centroids classifier (Tibshirani,\nHastie, Narasimhan & Chu 2001) with 10-fold cross-validation, using the\npamr package in the R language. The figure shows the CV curve, with its\nminimum at 23 genes, achieving a CV error rate of 4.7%. The test error at\n23 genes is 8%. The estimate of the CV bias, using the method described\nin this in paper, is 2.7%, yielding an adjusted error of 4.7 + 2.7 = 7.4%.\nOver 100 repeats of this experiment, the average test error was 7.8%, and\nthe average adjusted CV error was 7.3%.\nIn this paper we study the CV bias problem and examine the accuracy of\nour proposed adjustment on simulated data. These examples suggest that\nthe bias is larger when the signal-to-noise ratio is lower, a fact also noted\nby Efron (2008). We also provide a short theoretical section examining the\nexpectation of the bias when there is no signal at all.\n2 Model Selection Using Cross-Validation\nSuppose we observe n independent and identically distributed points (xi, yi),\nwhere xi = (xi1, . . . , xip) is a vector of predictors, and yiis a response (this\ncan be real-valued or discrete). From this \u201ctraining\u201d data we estimate a\nprediction model \u02c6f(x) for y, and we have a loss function L(y, \u02c6f(x)) that\n2\n2 5 10 20 50 200 500 2000\n0.05 0.10 0.15 0.20 0.25 0.30 0.35\nNumber of genes\nCV error\nCV error curve\nTest error\nAdjusted min of CV\nFigure 1: Brown microarray cancer data: the CV error curve is minimized\nat 23 genes, achieving a CV error of 0.047. Meanwhile, the test error at 23\ngenes is 0.08, drawn as a dashed line. The proposed bias estimate is 0.027,\ngiving an adjusted error of 0.047 + 0.027 = 0.074, drawn as a dotted line.\n3\nmeasures the error between y and \u02c6f(x). Typically this is\nL(y, \u02c6f(x)) = (y \u2212 \u02c6f(x))2squared error\nfor regression, and\nL(y, \u02c6f(x)) = 1{y 6= \u02c6f(x)} 0-1 loss\nfor classification.\nAn important quantity is the expected prediction error E[L(y0,\n\u02c6f(x0))]\n(also called expected test error). This is the expected value of the loss\nwhen predicting an independent data point (x0, y0), drawn from the same\ndistribution as our training data. The expectation is over all that is random\n(namely, the model \u02c6f and the test point (x0, y0)).\nSuppose that our prediction model depends on a parameter \u03b8, that is\n\u02c6f(x) = \u02c6f(x, \u03b8). We want to select \u03b8 based on the training set (xi\n, yi), i =\n1, . . . , n, in order to minimize the expected prediction error.\nOne of the simplest and most popular methods for doing this is K-fold\ncross-validation. We first split our data (xi, yi) into K equal parts. Then\nfor each k = 1, . . . , K, we remove the kth part from our data set and fit a\nmodel \u02c6f\n\u2212k\n(x, \u03b8). Let Ck be the indices of observations in the kth fold. The\ncross-validation estimate of expected test error is\nCV(\u03b8) = 1\nn\nX\nK\nk=1\nX\ni\u2208Ck\nL(yi,\n\u02c6f\n\u2212k\n(xi, \u03b8)). (1)\nRecall that \u02c6f\n\u2212k\n(xi, \u03b8) is a function of \u03b8, so we compute CV(\u03b8) over a grid of\nparameter values \u03b81, . . . , \u03b8t, and choose the minimizer \u02c6\u03b8 to be our parameter\nestimate. We call CV(\u03b8) the \u201cCV error curve\u201d.\n3 Bias Correction\nWe would like to estimate the expected test error using \u02c6f(x, \u02c6\u03b8), namely\nErr = E[L(y0,\n\u02c6f(x0,\u02c6\u03b8))].\nThe naive estimate is CV(\u02c6\u03b8), having bias\nBias = Err \u2212 CV(\u02c6\u03b8). (2)\nThis is likely to be positive, since \u02c6\u03b8 was chosen because it minimizes CV(\u03b8).\n4\nLet nk be the number of observations in the kth fold, and define\nek(\u03b8) = 1\nnk\nX\ni\u2208Ck\nL(yi,\n\u02c6f\n\u2212k\n(xi, \u03b8))\nThis is the error curve computed from the predictions in the kth fold.\nOur estimate uses the difference between the value of ek at \u02c6\u03b8 and its\nminimum to mimic the bias in cross-validation. Specifically, we propose the\nfollowing estimate:\nBias = d 1\nK\nX\nK\nk=1\n[ek(\n\u02c6\u03b8) \u2212 ek(\u02c6\u03b8k)] (3)\nwhere \u02c6\u03b8k is the minimizer of ek(\u03b8). Note that this estimate uses only quanti\u0002ties that have already been computed for the CV estimate (1), and requires\nno new model fitting. Since Bias is a mean over d K folds, we can also use\nthe standard error of the mean as an approximate estimate for its standard\ndeviation.\nThe adjusted estimate of test error is CV(\u02c6\u03b8)+Bias. Note that if the fold d\nsizes are equal, then CV(\u02c6\u03b8) = 1\nK\nPK\nk=1 ek(\n\u02c6\u03b8) and the adjusted estimate of\ntest error is\nCV(\u02c6\u03b8) + Bias = 2CV( d \u02c6\u03b8) \u2212\n1\nK\nX\nK\nk=1\nek(\n\u02c6\u03b8k).\nThe intuitive motivation for the estimate Bias is as follows: first, d ek(\n\u02c6\u03b8k) \u2248\nCV(\u02c6\u03b8) since both are error curves evaluated at their minima; the latter\nuses all K folds while the former uses just fold k. Second, for fixed \u03b8,\ncross-validation error estimates the expected test error, so that ek(\u03b8) \u2248\nE[L(y, \u02c6f(x, \u03b8))]. Thus ek(\n\u02c6\u03b8) \u2248 Err.\nThe second analogy is not perfect: Err = E[L(y, \u02c6f(x, \u02c6\u03b8))], where (x, y)\nis stochastically independent of the training data, and hence of \u02c6\u03b8. In con\u0002trast, the terms in ek(\n\u02c6\u03b8) are L(yi\n,\n\u02c6f\n\u2212k\n(xi,\n\u02c6\u03b8)), i \u2208 Ck; here (xi\n, yi) has some\ndependence on \u02c6\u03b8 since \u02c6\u03b8 is chosen to minimize the validation error across all\nfolds, including the kth one. To remove this dependence, one...",
      "url": "https://www.stat.berkeley.edu/~ryantibs/papers/cvbias.pdf"
    },
    {
      "title": "Gap leaderboard score and model scoring on a Competition",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Gap leaderboard score and model scoring on a Competition](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 4 months ago\n\nModified [6 years, 4 months ago](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?lastactivity)\n\nViewed\n694 times\n\n3\n\n$\\\\begingroup$\n\nI'm working on a Veolia challenge on Ens Data Challenge [ens-data](https://challengedata.ens.fr/en/home) (equivalent to Kaggle) the goal is to classify very rare binary events (the failure of a pipeline ) for 2014 and 2015 (y={2014,2015}). In input we have 5 features 3 categorical features (which I turned into dummy variable) and two continuous. The score is average AUC, $0.6\\*AUC\\_1 + 0.4\\*AUC\\_2$.\n\nMy problem is the following, when I compute each AUC (for 2014 and 2015) with a stratified kfold cross validation and I compute the average AUC I get roughly 0.88 and when I submit on the website I end up with 0.67, I guess there is a problem in my code.\n\nHere is my code for choosing the best model for 2014:\n\nRk: to predict on the test set (2014,2015 unknown), I first predict with all 5 features, 2014. Then I add the prediction of 2014 to my feature to predict 2015\n\n```\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LG', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\n\n# stratifiedkfold is defined by default when there is an integer\nscoring = 'roc_auc'\nnum_folds = 10\n\nfor name, model in models:\n    cv_results = cross_validation.cross_val_score(model, X, Y, cv=num_folds, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [python](https://datascience.stackexchange.com/questions/tagged/python)\n\n[Share](https://datascience.stackexchange.com/q/16789)\n\n[Improve this question](https://datascience.stackexchange.com/posts/16789/edit)\n\nFollow\n\n[edited Feb 7, 2017 at 9:17](https://datascience.stackexchange.com/posts/16789/revisions)\n\nbouritosse\n\nasked Feb 6, 2017 at 20:09\n\n[![bouritosse's user avatar](https://www.gravatar.com/avatar/d05cc42fc90ecb44bf614b1f69a90fcc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/19065/bouritosse)\n\n[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse) bouritosse\n\n9366 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- $\\\\begingroup$You mean you have a model scoring 0.88 while the leaderboard(the score on the website) shows 0.67?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 2:27\n\n- $\\\\begingroup$yes that is correct$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 9:15\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Your test data(the 0.88 one) is different from the test data on the website(the 0.67 one). There must be some difference between their scores. Are you concerning about over-fitting?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 10:13\n\n- $\\\\begingroup$kfold cross validation is not supposed to downsize the overfitting ?$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 12:21\n\n- $\\\\begingroup$Can you add your score on training set to your question, that's crucial.$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 12:49\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nAs you have commented, you are concerning about over-fitting.\n\nIn fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find:\n\n1. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) (the famous ResNet paper). Checkout figure.1\n2. This [kernel](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006) of [Two Sigma Financial Modeling Challenge](https://www.kaggle.com/c/two-sigma-financial-modeling) on [Kaggle](https://www.kaggle.com/) says:\n\n> we are getting a public score of 0.0169 which is slightly better than the previous one.\n> Submitting this model to the LB gave me a score of 0.006\n\nIn my practice, I am more willing to concern the scoring gap between TRAIN, VALIDATION(DEVELOP) and TEST dataset. Personally speaking, TRAIN>VALIDATION=TEST is better than TRAIN=VALIDATION>TEST.\n\nEdit: For class imbalance problem, there are some resources:\n\n1. [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This blog shows a common workflow dealing with imbalanced class issue.\n\n2. [Class Imbalance Problem in Data Mining: Review](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This paper compares several algorithms created for solving the class imbalance problem.\n\n\n[Share](https://datascience.stackexchange.com/a/16808)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/16808/edit)\n\nFollow\n\n[edited Feb 8, 2017 at 1:58](https://datascience.stackexchange.com/posts/16808/revisions)\n\nanswered Feb 7, 2017 at 12:48\n\n[![Icyblade's user avatar](https://www.gravatar.com/avatar/0696854959b681c4cefaf494015e8bf4?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/28628/icyblade)\n\n[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade) Icyblade\n\n4,33611 gold badge2424 silver badges3434 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$On the training I get 0.847700964724 AUC and on the validation set LG: 0.909566 (0.032773). With LogisticRegression(class\\_weight='balanced')$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 13:09\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I think the problem is that my class are really imbalance I have 0.19% of failure in class 2014$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 20:06\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f16789%2fgap-leaderboard-score-and-model-scoring-on-a-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [pri...",
      "url": "https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition"
    },
    {
      "title": "",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/abs/2406.01652"
    },
    {
      "title": "Balance model complexity and cross-validated score #",
      "text": "Balance model complexity and cross-validated score &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\nNote\n[Go to the end](#sphx-glr-download-auto-examples-model-selection-plot-grid-search-refit-callable-py)to download the full example code or to run this example in your browser via JupyterLite or Binder.\n# Balance model complexity and cross-validated score[#](#balance-model-complexity-and-cross-validated-score)\nThis example demonstrates how to balance model complexity and cross-validated score by\nfinding a decent accuracy within 1 standard deviation of the best accuracy score while\nminimising the number of[`PCA`](../../modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)components[[1]](#id2). It uses[`GridSearchCV`](../../modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)with a custom refit callable to select\nthe optimal model.\nThe figure shows the trade-off between cross-validated score and the number\nof PCA components. The balanced case is when`n\\_components=10`and`accuracy=0.88`,\nwhich falls into the range within 1 standard deviation of the best accuracy\nscore.\n## References[#](#references)\n[[1](#id1)]\nHastie, T., Tibshirani, R., Friedman, J. (2001). Model Assessment and\nSelection. The Elements of Statistical Learning (pp. 219-260). New York,\nNY, USA: Springer New York Inc.\n```\n# Authors: The scikit-learn developers# SPDX-License-Identifier: BSD-3-Clauseimportmatplotlib.pyplotaspltimportnumpyasnpimportpolarsasplfromsklearn.datasetsimport[load\\_digits](../../modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)fromsklearn.decompositionimport[PCA](../../modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)fromsklearn.linear\\_modelimport[LogisticRegression](../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)fromsklearn.model\\_selectionimport[GridSearchCV](../../modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV),[ShuffleSplit](../../modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)fromsklearn.pipelineimport[Pipeline](../../modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n```\n## Introduction[#](#introduction)\nWhen tuning hyperparameters, we often want to balance model complexity and\nperformance. The \u201cone-standard-error\u201d rule is a common approach: select the simplest\nmodel whose performance is within one standard error of the best model\u2019s performance.\nThis helps to avoid overfitting by preferring simpler models when their performance is\nstatistically comparable to more complex ones.\n## Helper functions[#](#helper-functions)\nWe define two helper functions:\n1. `lower\\_bound`: Calculates the threshold for acceptable performance\n(best score - 1 std)\n2. `best\\_low\\_complexity`: Selects the model with the fewest PCA components that\nexceeds this threshold\n```\ndeflower\\_bound(cv\\_results):&quot;&quot;&quot;Calculate the lower bound within 1 standard deviationof the best `mean\\_test\\_scores`.Parameters----------cv\\_results : dict of numpy(masked) ndarraysSee attribute cv\\_results\\_ of `GridSearchCV`Returns-------floatLower bound within 1 standard deviation of thebest `mean\\_test\\_score`.&quot;&quot;&quot;best\\_score\\_idx=[np.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html#numpy.argmax)(cv\\_results[&quot;&quot;mean\\_test\\_score&quot;&quot;])return(cv\\_results[&quot;&quot;mean\\_test\\_score&quot;&quot;][best\\_score\\_idx]-cv\\_results[&quot;&quot;std\\_test\\_score&quot;&quot;][best\\_score\\_idx])defbest\\_low\\_complexity(cv\\_results):&quot;&quot;&quot;Balance model complexity with cross-validated score.Parameters----------cv\\_results : dict of numpy(masked) ndarraysSee attribute cv\\_results\\_ of `GridSearchCV`.Return------intIndex of a model that has the fewest PCA componentswhile has its test score within 1 standard deviation of the best`mean\\_test\\_score`.&quot;&quot;&quot;threshold=lower\\_bound(cv\\_results)candidate\\_idx=[np.flatnonzero](https://numpy.org/doc/stable/reference/generated/numpy.flatnonzero.html#numpy.flatnonzero)(cv\\_results[&quot;&quot;mean\\_test\\_score&quot;&quot;]&gt;=threshold)best\\_idx=candidate\\_idx[cv\\_results[&quot;&quot;param\\_reduce\\_dim\\_\\_n\\_components&quot;&quot;][candidate\\_idx].argmin()]returnbest\\_idx\n```\n## Set up the pipeline and parameter grid[#](#set-up-the-pipeline-and-parameter-grid)\nWe create a pipeline with two steps:\n1. Dimensionality reduction using PCA\n2. Classification using LogisticRegression\nWe\u2019ll search over different numbers of PCA components to find the optimal complexity.\n```\npipe=[Pipeline](../../modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)([(&quot;&quot;reduce\\_dim&quot;&quot;,[PCA](../../modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)(random\\_state=42)),(&quot;classify&quot;,[LogisticRegression](../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)(random\\_state=42,C=0.01,max\\_iter=1000)),])param\\_grid={&quot;&quot;reduce\\_dim\\_\\_n\\_components&quot;&quot;:[6,8,10,15,20,25,35,45,55]}\n```\n## Perform the search with GridSearchCV[#](#perform-the-search-with-gridsearchcv)\nWe use`GridSearchCV`with our custom`best\\_low\\_complexity`function as the refit\nparameter. This function will select the model with the fewest PCA components that\nstill performs within one standard deviation of the best model.\n```\ngrid=[GridSearchCV](../../modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)(pipe,# Use a non-stratified CV strategy to make sure that the inter-fold# standard deviation of the test scores is informative.cv=[ShuffleSplit](../../modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)(n\\_splits=30,random\\_state=0),n\\_jobs=1,# increase this on your machine to use more physical coresparam\\_grid=param\\_grid,scoring=&quot;accuracy&quot;,refit=best\\_low\\_complexity,return\\_train\\_score=True,)\n```\n## Load the digits dataset and fit the model[#](#load-the-digits-dataset-and-fit-the-model)\n```\nX,y=[load\\_digits](../../modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)(return\\_X\\_y=True)grid.fit(X,y)\n```\n```\nGridSearchCV(cv=ShuffleSplit(n\\_splits=30, random\\_state=0, test\\_size=None, train\\_size=None),\nestimator=Pipeline(steps=[(&#x27;&#x27;reduce\\_dim&#x27;&#x27;, PCA(random\\_state=42)),\n(&#x27;&#x27;classify&#x27;&#x27;,\nLogisticRegression(C=0.01,\nmax\\_iter=1000,\nrandom\\_state=42))]),\nn\\_jobs=1,\nparam\\_grid={&#x27;&#x27;reduce\\_dim\\_\\_n\\_components&#x27;&#x27;: [6, 8, 10, 15, 20, 25, 35,\n45, 55]},\nrefit=&lt;&lt;function best\\_low\\_complexity at 0x7fe86c9c3f60&gt;&gt;,\nreturn\\_train\\_score=True, scoring=&#x27;&#x27;accuracy&#x27;&#x27;)\n```\n**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\nOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**\nGridSearchCV\n[?Documentation for GridSearchCV](https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html)iFitted\nParameters**|[estimatorestimator: estimator object\nThis is assumed to implement the scikit-learn estimator interface.\nEither estimator needs to provide a ``score`` function,\nor ``scoring`` must be passed.](<https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=estimato...",
      "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_refit_callable.html"
    },
    {
      "title": "11.  Common pitfalls and recommended practices #",
      "text": "11. Common pitfalls and recommended practices &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)](index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 11.Common pitfalls and recommended practices[#](#common-pitfalls-and-recommended-practices)\nThe purpose of this chapter is to illustrate some common pitfalls and\nanti-patterns that occur when using scikit-learn. It provides\nexamples of what**not**to do, along with a corresponding correct\nexample.\n## 11.1.Inconsistent preprocessing[#](#inconsistent-preprocessing)\nscikit-learn provides a library of[Dataset transformations](data_transforms.html#data-transforms), which\nmay clean (see[Preprocessing data](modules/preprocessing.html#preprocessing)), reduce\n(see[Unsupervised dimensionality reduction](modules/unsupervised_reduction.html#data-reduction)), expand (see[Kernel Approximation](modules/kernel_approximation.html#kernel-approximation))\nor generate (see[Feature extraction](modules/feature_extraction.html#feature-extraction)) feature representations.\nIf these data transforms are used when training a model, they also\nmust be used on subsequent datasets, whether it\u2019s test data or\ndata in a production system. Otherwise, the feature space will change,\nand the model will not be able to perform effectively.\nFor the following example, let\u2019s create a synthetic dataset with a\nsingle feature:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_regression&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;random\\_state=42&gt;&gt;&gt;X,y=make\\_regression(random\\_state=random\\_state,n\\_features=1,noise=1)&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,test\\_size=0.4,random\\_state=random\\_state)\n```\n**Wrong**\nThe train dataset is scaled, but not the test dataset, so model\nperformance on the test dataset is worse than expected:\n```\n&gt;&gt;&gt;fromsklearn.metricsimportmean\\_squared\\_error&gt;&gt;&gt;fromsklearn.linear\\_modelimportLinearRegression&gt;&gt;&gt;fromsklearn.preprocessingimportStandardScaler&gt;&gt;&gt;scaler=StandardScaler()&gt;&gt;&gt;X\\_train\\_transformed=scaler.fit\\_transform(X\\_train)&gt;&gt;&gt;model=LinearRegression().fit(X\\_train\\_transformed,y\\_train)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))62.80...\n```\n**Right**\nInstead of passing the non-transformed`X\\_test`to`predict`, we should\ntransform the test data, the same way we transformed the training data:\n```\n&gt;&gt;&gt;X\\_test\\_transformed=scaler.transform(X\\_test)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test\\_transformed))0.90...\n```\nAlternatively, we recommend using a[`Pipeline`](modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), which makes it easier to chain transformations\nwith estimators, and reduces the possibility of forgetting a transformation:\n```\n&gt;&gt;&gt;fromsklearn.pipelineimportmake\\_pipeline&gt;&gt;&gt;model=make\\_pipeline(StandardScaler(),LinearRegression())&gt;&gt;&gt;model.fit(X\\_train,y\\_train)Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),(&#39;linearregression&#39;, LinearRegression())])&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))0.90...\n```\nPipelines also help avoiding another common pitfall: leaking the test data\ninto the training data.\n## 11.2.Data leakage[#](#data-leakage)\nData leakage occurs when information that would not be available at prediction\ntime is used when building the model. This results in overly optimistic\nperformance estimates, for example from[cross-validation](modules/cross_validation.html#cross-validation), and thus poorer performance when the model is used\non actually novel data, for example during production.\nA common cause is not keeping the test and train data subsets separate.\nTest data should never be used to make choices about the model.**The general rule is to never call**`fit`**on the test data**. While this\nmay sound obvious, this is easy to miss in some cases, for example when\napplying certain pre-processing steps.\nAlthough both train and test data subsets should receive the same\npreprocessing transformation (as described in the previous section), it is\nimportant that these transformations are only learnt from the training data.\nFor example, if you have a\nnormalization step where you divide by the average value, the average should\nbe the average of the train subset,**not**the average of all the data. If the\ntest subset is included in the average calculation, information from the test\nsubset is influencing the model.\n### 11.2.1.How to avoid data leakage[#](#how-to-avoid-data-leakage)\nBelow are some tips on avoiding data leakage:\n* Always split the data into train and test subsets first, particularly\nbefore any preprocessing steps.\n* Never include test data when using the`fit`and`fit\\_transform`methods. Using all the data, e.g.,`fit(X)`, can result in overly optimistic\nscores.\nConversely, the`transform`method should be used on both train and test\nsubsets as the same preprocessing should be applied to all the data.\nThis can be achieved by using`fit\\_transform`on the train subset and`transform`on the test subset.\n* The scikit-learn[pipeline](modules/compose.html#pipeline)is a great way to prevent data\nleakage as it ensures that the appropriate method is performed on the\ncorrect data subset. The pipeline is ideal for use in cross-validation\nand hyper-parameter tuning functions.\nAn example of data leakage during preprocessing is detailed below.\n### 11.2.2.Data leakage during pre-processing[#](#data-leakage-during-pre-processing)\nNote\nWe here choose to illustrate data leakage with a feature selection step.\nThis risk of leakage is however relevant with almost all transformations\nin scikit-learn, including (but not limited to)[`StandardScaler`](modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler),[`SimpleImputer`](modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), and[`PCA`](modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA).\nA number of[Feature selection](modules/feature_selection.html#feature-selection)functions are available in scikit-learn.\nThey can help remove irrelevant, redundant and noisy features as well as\nimprove your model build time and performance. As with any other type of\npreprocessing, feature selection should**only**use the training data.\nIncluding the test data in feature selection will optimistically bias your\nmodel.\nTo demonstrate we will create this binary classification problem with\n10,000 randomly generated features:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;n\\_samples,n\\_features,n\\_classes=200,10000,2&gt;&gt;&gt;rng=np.random.RandomState(42)&gt;&gt;&gt;X=rng.standard\\_normal((n\\_samples,n\\_features))&gt;&gt;&gt;y=rng.choice(n\\_classes,n\\_samples)\n```\n**Wrong**\nUsing all the data to perform feature selection results in an accuracy score\nmuch higher than chance, even though our targets are completely random.\nThis randomness means that our`X`and`y`are independent and we thus expect\nthe accuracy to be around 0.5. However, since the feature selection step\n\u2018sees\u2019 the test data, the model has an unfair advantage. In the incorrect\nexample below we first use all the data for feature selection and then split\nthe data into training and test subsets for model fitting. The result is a\nmuch higher than expected accuracy score:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;fromsklearn.feature\\_selectionimportSelectKBest&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.metricsimportaccuracy\\_score&gt;&gt;&gt;# Incorrect preprocessing: the entire data is tr...",
      "url": "https://scikit-learn.org/stable/common_pitfalls.html"
    },
    {
      "title": "Overfitting the leaderboard",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=77c6c69f1305124c7a10:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/caseyftw/overfitting-the-leaderboard"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - scaomath/kaggle-jane-street: Machine learning models to predict realtime financial market data provided by Jane Street\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/scaomath/kaggle-jane-street)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/scaomath/kaggle-jane-street)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=scaomath/kaggle-jane-street)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[scaomath](https://github.com/scaomath)/**[kaggle-jane-street](https://github.com/scaomath/kaggle-jane-street)**Public\n* [Notifications](https://github.com/login?return_to=/scaomath/kaggle-jane-street)You must be signed in to change notification settings\n* [Fork16](https://github.com/login?return_to=/scaomath/kaggle-jane-street)\n* [Star50](https://github.com/login?return_to=/scaomath/kaggle-jane-street)\nMachine learning models to predict realtime financial market data provided by Jane Street\n[50stars](https://github.com/scaomath/kaggle-jane-street/stargazers)[16forks](https://github.com/scaomath/kaggle-jane-street/forks)[Branches](https://github.com/scaomath/kaggle-jane-street/branches)[Tags](https://github.com/scaomath/kaggle-jane-street/tags)[Activity](https://github.com/scaomath/kaggle-jane-street/activity)\n[Star](https://github.com/login?return_to=/scaomath/kaggle-jane-street)\n[Notifications](https://github.com/login?return_to=/scaomath/kaggle-jane-street)You must be signed in to change notification settings\n# scaomath/kaggle-jane-street\nmain\n[Branches](https://github.com/scaomath/kaggle-jane-street/branches)[Tags](https://github.com/scaomath/kaggle-jane-street/tags)\n[](https://github.com/scaomath/kaggle-jane-street/branches)[](https://github.com/scaomath/kaggle-jane-street/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[134 Commits](https://github.com/scaomath/kaggle-jane-street/commits/main/)\n[](https://github.com/scaomath/kaggle-jane-street/commits/main/)\n|\n[data](https://github.com/scaomath/kaggle-jane-street/tree/main/data)\n|\n[data](https://github.com/scaomath/kaggle-jane-street/tree/main/data)\n|\n|\n|\n[lgb](https://github.com/scaomath/kaggle-jane-street/tree/main/lgb)\n|\n[lgb](https://github.com/scaomath/kaggle-jane-street/tree/main/lgb)\n|\n|\n|\n[mlp](https://github.com/scaomath/kaggle-jane-street/tree/main/mlp)\n|\n[mlp](https://github.com/scaomath/kaggle-jane-street/tree/main/mlp)\n|\n|\n|\n[models](https://github.com/scaomath/kaggle-jane-street/tree/main/models)\n|\n[models](https://github.com/scaomath/kaggle-jane-street/tree/main/models)\n|\n|\n|\n[.gitattributes](https://github.com/scaomath/kaggle-jane-street/blob/main/.gitattributes)\n|\n[.gitattributes](https://github.com/scaomath/kaggle-jane-street/blob/main/.gitattributes)\n|\n|\n|\n[.gitignore](https://github.com/scaomath/kaggle-jane-street/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/scaomath/kaggle-jane-street/blob/main/.gitignore)\n|\n|\n|\n[README.md](https://github.com/scaomath/kaggle-jane-street/blob/main/README.md)\n|\n[README.md](https://github.com/scaomath/kaggle-jane-street/blob/main/README.md)\n|\n|\n|\n[\\_\\_init\\_\\_.py](https://github.com/scaomath/kaggle-jane-street/blob/main/__init__.py)\n|\n[\\_\\_init\\_\\_.py](https://github.com/scaomath/kaggle-jane-street/blob/main/__init__.py)\n|\n|\n|\n[cv.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv.py)\n|\n[cv.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv.py)\n|\n|\n|\n[cv\\_final.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv_final.py)\n|\n[cv\\_final.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv_final.py)\n|\n|\n|\n[cv\\_splits.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv_splits.py)\n|\n[cv\\_splits.py](https://github.com/scaomath/kaggle-jane-street/blob/main/cv_splits.py)\n|\n|\n|\n[data.py](https://github.com/scaomath/kaggle-jane-street/blob/main/data.py)\n|\n[data.py](https://github.com/scaomath/kaggle-jane-street/blob/main/data.py)\n|\n|\n|\n[eda.ipynb](https://github.com/scaomath/kaggle-jane-street/blob/main/eda.ipynb)\n|\n[eda.ipynb](https://github.com/scaomath/kaggle-jane-street/blob/main/eda.ipynb)\n|\n|\n|\n[eda\\_final.ipynb](https://github.com/scaomath/kaggle-jane-street/blob/main/eda_final.ipynb)\n|\n[eda\\_final.ipynb](https://github.com/scaomath/kaggle-jane-street/blob/main/eda_final.ipynb)\n|\n|\n|\n[iter\\_cv.py](https://github.com/scaomath/kaggle-jane-street/blob/main/iter_cv.py)\n|\n[iter\\_cv.py](https://github.com/scaomath/kaggle-jane-street/blob/main/iter_cv.py)\n|\n|\n|\n[iter\\_cv\\_torch.py](https://github.com/scaomath/kaggle-jane-street/blob/main/iter_cv_torch.py)\n|\n[iter\\_cv\\_torch.py](https://github.com/scaomath/kaggle-jane-street/blob/main/iter_cv_torch.py)\n|\n|\n|\n[janest.code-workspace](https://github.com/scaomath/kaggle-jane-street/blob/main/janest.code-workspace)\n|\n[janest.code-workspace](https://github.com/scaomath/kaggle-jane-street/blob/main/janest.code-workspace)\n|\n|\n|\n[utils.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils.py)\n|\n[utils.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils.py)\n|\n|\n|\n[utils\\_js.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils_js.py)\n|\n[utils\\_js.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils_js.py)\n|\n|\n|\n[utils\\_lgb.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils_lgb.py)\n|\n[utils\\_lgb.py](https://github.com/scaomath/kaggle-jane-street/blob/main/utils_lgb.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Playground for Jane Street Market Prediction Competition on Kaggle\n[](#playground-for-jane-street-market-prediction-competition-on-kaggle)\n# Introduction\n[](#introduction)\nJane Street hosted a code competition of predicting the stock market (Feb 2021 to Aug 2021) using the past high frequency trading data (2 years of data before 2018?) on Kaggle:[https://www.kaggle.com/c/jane-street-market-prediction](https://www.kaggle.com/c/jane-street-market-prediction).\nThe training data provided contain 500 days of high frequency trading data, total 2.4 million rows. The public leaderboard data contain 1 year of high frequency trading data from some time before Aug 2020 and up to that. The private ranges from a random time from July/Aug 2020 up to Aug 2021 (it was March 2021 as of the time of writings). This training dataset contains an anonymized set of features,`feature\\_{0...129}`, representing real stock market data. Each row in the dataset represents a trading opportunity.\nThis is a code competition in that we have to prepare a pipeline of models that can do inference 1 trading opportunity at a time (no peaking into the future) subject to the inference API on Kaggle, and this submission should be able to perform the inference for 1.1 million samples in under 5 hours on cloud.\nFor each...",
      "url": "https://github.com/scaomath/kaggle-jane-street"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    }
  ]
}