{
  "query": "How can I reduce systematic bias in machine learning predictions when the test set has different distribution than training set?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on research on covariate\u2011shift correction, importance\u2011weighting, and robust evaluation, you can mitigate systematic bias when the test distribution differs from the training distribution by following these core steps\u202f([NIPS\u202f2006](https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data), [Gretton\u202f2008](https://www.gatsby.ucl.ac.uk/~gretton/papers/covariateShiftChapter.pdf), [Reddi\u202f2014](https://ojs.aaai.org/index.php/AAAI/article/download/9576/9435)):\n\n1. **Detect distribution shift**  \n   - Use a small set of unlabeled test samples and compare their feature statistics to the training data (e.g., via kernel two\u2011sample tests or density\u2011ratio estimation). A significant mismatch signals covariate shift\u202f([Gretton\u202f2008](https://www.gatsby.ucl.ac.uk/~gretton/papers/covariateShiftChapter.pdf)).\n\n2. **Estimate importance\u2011weights for training examples**  \n   - Apply **Kernel Mean Matching (KMM)**: solve a quadratic program that finds non\u2011negative weights\u202f\\(w_i\\)\u202fso that the weighted training feature mean matches the test feature mean in a reproducing\u2011kernel Hilbert space\u202f([Gretton\u202f2008](https://www.gatsby.ucl.ac.uk/~gretton/papers/covariateShiftChapter.pdf)).  \n   - Alternatively, use the non\u2011parametric weighting method from \u201cCorrecting Sample Selection Bias by Unlabeled Data\u201d, which directly produces resampling weights without explicit density estimation\u202f([Huang\u202f2006](https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data)).\n\n3. **Re\u2011train or fine\u2011tune the model with weighted loss**  \n   - Incorporate the estimated weights into the training objective: minimize \\(\\sum_i w_i \\, \\ell(y_i, f(x_i))\\). This aligns the empirical risk with the target (test) distribution and reduces bias\u202f([Reddi\u202f2014](https://ojs.aaai.org/index.php/AAAI/article/download/9576/9435)).\n\n4. **Apply a doubly\u2011robust correction (optional for high variance)**  \n   - Combine the weighted estimator with a low\u2011variance \u201cprior\u201d model (e.g., a simple baseline). Adjust the final prediction by correcting the weighted estimate relative to this prior, which keeps bias low while controlling variance\u202f([Reddi\u202f2014](https://ojs.aaai.org/index.php/AAAI/article/download/9576/9435)).\n\n5. **Use robust learning algorithms when re\u2011weighting alone is insufficient**  \n   - Implement **Robust Covariate Shift Adjustment (RCSA)**, which jointly learns weights and a predictor that is less sensitive to misspecified model classes\u202f([Wen\u202f2014](https://proceedings.mlr.press/v32/wen14.pdf)).  \n\n6. **Employ calibrated ensembles to hedge against residual shift**  \n   - Train multiple diverse models, calibrate their probability outputs, and combine them (e.g., via weighted averaging). Proper calibration can mitigate accuracy trade\u2011offs under distribution shift\u202f([Kumar\u202f2022](https://arxiv.org/abs/2207.08977)).\n\n7. **Validate under shift with a dedicated evaluation protocol**  \n   - Use tools such as **Mandoline** to assess model performance on held\u2011out test data that reflects the shifted distribution, ensuring that bias reduction actually improves real\u2011world predictions\u202f([Chen\u202f2021](https://arxiv.org/abs/2107.00643)).\n\nFollowing these steps\u2014detecting shift, estimating importance weights (KMM or non\u2011parametric matching), training with weighted loss, optionally applying doubly\u2011robust or RCSA adjustments, leveraging calibrated ensembles, and evaluating with shift\u2011aware metrics\u2014will systematically reduce bias caused by distribution differences between training and test sets.",
      "url": ""
    },
    {
      "title": "Correcting Sample Selection Bias by Unlabeled Data",
      "text": "Correcting Sample Selection Bias by Unlabeled Data\n# Correcting Sample Selection Bias by Unlabeled Data\nJiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch\u00f6lkopf, Alex J. Smola\n[Advances in Neural Information Processing Systems 19 (NIPS 2006)](https://papers.nips.cc/paper_files/paper/2006)\n[Bibtex](https://papers.nips.cc/paper_files/paper/2006/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Bibtex.bib)[Metadata](https://papers.nips.cc/paper_files/paper/2006/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Metadata.json)[Paper](https://papers.nips.cc/paper_files/paper/2006/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Paper.pdf)\n## Abstract\nWe consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to \ufb01rst recover sampling distributions and then make appro- priate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estima- tion. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.\n#### Name Change Policy\n&times;\nRequests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\nUse the \"Report an Issue\" link to request a name change.\nDo not remove: This comment is monitored to verify that the site is working properly",
      "url": "https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2107.00643] Mandoline: Model Evaluation under Distribution Shift\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2107.00643\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2107.00643**(cs)\n[Submitted on 1 Jul 2021 ([v1](https://arxiv.org/abs/2107.00643v1)), last revised 11 Apr 2022 (this version, v2)]\n# Title:Mandoline: Model Evaluation under Distribution Shift\nAuthors:[Mayee Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M),[Karan Goel](https://arxiv.org/search/cs?searchtype=author&amp;query=Goel,+K),[Nimit S. Sohoni](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohoni,+N+S),[Fait Poms](https://arxiv.org/search/cs?searchtype=author&amp;query=Poms,+F),[Kayvon Fatahalian](https://arxiv.org/search/cs?searchtype=author&amp;query=Fatahalian,+K),[Christopher R\u00e9](https://arxiv.org/search/cs?searchtype=author&amp;query=R\u00e9,+C)\nView a PDF of the paper titled Mandoline: Model Evaluation under Distribution Shift, by Mayee Chen and 5 other authors\n[View PDF](https://arxiv.org/pdf/2107.00643)> > Abstract:\n> Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple &#34;slicing functions&#34; - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines. Comments:|33 pages. Published as a conference paper at ICML 2021|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2107.00643](https://arxiv.org/abs/2107.00643)[cs.LG]|\n|(or[arXiv:2107.00643v2](https://arxiv.org/abs/2107.00643v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2107.00643](https://doi.org/10.48550/arXiv.2107.00643)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Nimit Sohoni [[view email](https://arxiv.org/show-email/c336d188/2107.00643)]\n**[[v1]](https://arxiv.org/abs/2107.00643v1)**Thu, 1 Jul 2021 17:57:57 UTC (650 KB)\n**[v2]**Mon, 11 Apr 2022 00:14:55 UTC (489 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Mandoline: Model Evaluation under Distribution Shift, by Mayee Chen and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2107.00643)\n* [TeX Source](https://arxiv.org/src/2107.00643)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2107.00643&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2107.00643&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-07](https://arxiv.org/list/cs.LG/2021-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2107.00643?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.00643)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.00643)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.00643)\n### [1 blog link](https://arxiv.org/tb/2107.00643)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-00643)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-00643)\n[Karan Goel](<https://dblp.uni-trier.de/search/author?author=Karan Goel>)\n[Nimit Sharad Sohoni](<https://dblp.uni-trier.de/search/author?author=Nimit Sharad Sohoni>)\n[Kayvon Fatahalian](<https://dblp.uni-trier.de/search/author?author=Kayvon Fatahalian>)\n[Christopher R\u00e9](<https://dblp.uni-trier.de/search/author?author=Christopher R\u00e9>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2107.00643&amp;description=Mandoline: Model Evaluation under Distribution Shift>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2107.00643&amp;title=Mandoline: Model Evaluation under Distribution Shift>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencema...",
      "url": "https://arxiv.org/abs/2107.00643"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2207.08977] Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2207.08977\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2207.08977**(cs)\n[Submitted on 18 Jul 2022]\n# Title:Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift\nAuthors:[Ananya Kumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A),[Tengyu Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+T),[Percy Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+P),[Aditi Raghunathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Raghunathan,+A)\nView a PDF of the paper titled Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift, by Ananya Kumar and Tengyu Ma and Percy Liang and Aditi Raghunathan\n[View PDF](https://arxiv.org/pdf/2207.08977)> > Abstract:\n> We often see undesirable tradeoffs in robust machine learning where out-of-distribution (OOD) accuracy is at odds with in-distribution (ID) accuracy: a robust classifier obtained via specialized techniques such as removing spurious features often has better OOD but worse ID accuracy compared to a standard classifier trained via ERM. In this paper, we find that ID-calibrated ensembles -- where we simply ensemble the standard and robust models after calibrating on only ID data -- outperforms prior state-of-the-art (based on self-training) on both ID and OOD accuracy. On eleven natural distribution shift datasets, ID-calibrated ensembles obtain the best of both worlds: strong ID accuracy and OOD accuracy. We analyze this method in stylized settings, and identify two important conditions for ensembles to perform well both ID and OOD: (1) we need to calibrate the standard and robust models (on ID data, because OOD data is unavailable), (2) OOD has no anticorrelated spurious features. Comments:|Accepted to UAI 2022|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2207.08977](https://arxiv.org/abs/2207.08977)[cs.LG]|\n|(or[arXiv:2207.08977v1](https://arxiv.org/abs/2207.08977v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2207.08977](https://doi.org/10.48550/arXiv.2207.08977)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ananya Kumar [[view email](https://arxiv.org/show-email/44155113/2207.08977)]\n**[v1]**Mon, 18 Jul 2022 23:14:44 UTC (334 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift, by Ananya Kumar and Tengyu Ma and Percy Liang and Aditi Raghunathan\n* [View PDF](https://arxiv.org/pdf/2207.08977)\n* [TeX Source](https://arxiv.org/src/2207.08977)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2207.08977&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2207.08977&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-07](https://arxiv.org/list/cs.LG/2022-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2207.08977?context=cs)\n[stat](https://arxiv.org/abs/2207.08977?context=stat)\n[stat.ML](https://arxiv.org/abs/2207.08977?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.08977)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.08977)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.08977)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2207.08977&amp;description=Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2207.08977&amp;title=Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2207.08977)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2207.08977"
    },
    {
      "title": "",
      "text": "Doubly Robust Covariate Shift Correction\nSashank J. Reddi\nMachine Learning Department\nCarnegie Mellon University\nsjakkamr@cs.cmu.edu\nBarnabas P \u00b4 oczos \u00b4\nMachine Learning Department\nCarnegie Mellon University\nbapoczos@cs.cmu.edu\nAlex Smola\nMachine Learning Department\nCarnegie Mellon University\nalex@smola.org\nAbstract\nCovariate shift correction allows one to perform supervised\nlearning even when the distribution of the covariates on the\ntraining set does not match that on the test set. This is\nachieved by re-weighting observations. Such a strategy re\u0002moves bias, potentially at the expense of greatly increased\nvariance. We propose a simple strategy for removing bias\nwhile retaining small variance. It uses a biased, low variance\nestimate as a prior and corrects the final estimate relative to\nthe prior. We prove that this yields an efficient estimator and\ndemonstrate good experimental performance.\nIntroduction\nCovariate shift is a common problem when dealing with real\ndata. Quite often the experimental conditions under which a\ntraining set is generated are subtly different from the situa\u0002tion in which the system is deployed. For instance, in can\u0002cer diagnosis the training set may have an overabundance of\ndiseased patients, often of a specific subtype endemic in the\nlocation where the data was gathered. Likewise, due to tem\u0002poral changes in user interest the distribution of covariates in\nadvertising systems is nonstationary. This requires increas\u0002ing the weight of data related to, e.g., \u2018Gangnam style\u2019, when\nprocessing historic data logs.\nA common approach to addressing covariate shift is to\nreweight data such that the reweighted distribution matches\nthe target distribution. Briefly, suppose we observe X :=\n{x1, . . . , xm} drawn iid from q(x), typically with associated\nlabels Y := {y1, . . . , ym} drawn from p(y|x). This consti\u0002tutes the \u2018training set\u2019. However, we need to find a minimizer\nf\n\u2217\np of risk Rp\u2014 defined in Equation (1) \u2014 with regard to\np(y|x)p(x), for which we only have iid draws of the covari\u0002ates X0\n:= {x\n0\n1\n, . . . x0\nm0}. Note that simply minimizing the\nempirical risk on the training data leads to a biased estimate\n(since training set corresponds to samples from q(x)p(y|x)).\nIf p and q are known, this problem can be addressed via im\u0002Copyright \rc 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nportance sampling in the following manner:\nRp[f] = Ex\u223cp(x)Ey|x(`(y, f(x)))\n=\nZ\np(x)\nq(x)\nq(x)Ey|x`(y, f(x))dx\n= Ex\u223cq(x)Ey|x [\u03b2(x)`(y, f(x))] , (1)\nwhere \u03b2(x) := p(x)\nq(x)\nand ` is a loss function. Correspond\u0002ingly, empirical averages with respect to X and X0\ncan\nbe reweighted, see, e.g., (Quinonero-Candela et al. 2008; \u02dc\nCortes et al. 2008) and the references therein for further de\u0002tails. While estimator based on Equation (1) is unbiased, it\ntends to increase the variance of the empirical averages con\u0002siderably by weighting the observations by \u03b2.\nThis issue is particularly exacerbated when the weights\nare large. As a rule of thumb the effective sample size of\na reweighted dataset is meff := k\u03b2(X)k\n2\n1\n/ k\u03b2(X)k\n2\n2 where\n\u03b2(X) is the vector of weights \u03b2(x1), . . . , \u03b2(xm). This quan\u0002tity naturally arises, e.g., for a weighted average of Gaussian\nrandom variables, while deriving Chernoff bounds using the\nweights \u03b2(X) (Gretton et al. 2008), or in the particle filter\u0002ing context (Doucet, de Freitas, and Gordon 2001). To gain\nbetter intuition for meff, consider the case where p = q. In\nthis case, we have high effective sample size (meff = m).\nWhereas in the undesirable case of a single observation hav\u0002ing very high weight, meff \u2248 1. Hence, meff is a good in\u0002dicator of the effect of \u03b2(x) on variance of the weighted\nempirical averages.\nThus, while one might obtain an unbiased estimator via\nEquation (1), it becomes nearly useless when the effective\nsample size is small relative to the original sample size.\nThis situation is frequently observed in practice insofar as\nwe encounter cases where simple covariate shift correction\nnot only fails to improve generalization performance on the\ntest set but, in fact, leads to estimates that perform worse\nthan simply minimizing the empirical risk on the training\ndata (i.e., unweighted estimation). Moreover, in many cases\nthe solutions of the biased and the unbiased risk estimates\nare closer than what the distributions p and q would suggest.\nFigure 1 shows an example of such a scenario.\nThe situation described above is often encountered in\npractice \u2014 covariate shift correction fails to improve mat\u0002ters due to high variance while the unweighted solution per\u0002forms reasonably well. This raises the question of how we\nProceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence 2949\n0\n1\n2\n3\n4\n5\n-6 -4 -2 0 2 4 6\nFigure 1: Assume that the dependence y|x is linear in x, as indi\u0002cated by the green line. In this case, inferring y|x using the blue\ndistribution q, as depicted by the blue crosses (with matching den\u0002sity), would lead to a perfectly accurate estimate, even if the test\nset is drawn according to red distribution p. On the other hand,\nreweighting with p(x)\nq(x) would lead to a very small effective sample\nsize since p and q are very different. While this example is ob\u0002viously somewhat artificial, there exist many situations where the\nminimizer of the biased risk is very good.\ncould benefit from the low variance of the biased estimate\nfound by using q while removing bias via weighting with\n\u03b2. This is precisely what doubly robust estimators address\n\u2014 see, e.g., (Kang and Schafer 2007) for an overview. They\nprovide us with two opportunities to obtain a good estimate.\nIf the unweighted estimate solves the problem, the estimate\nwill be very good and minimizing the unbiased risk will not\nchange the final outcome significantly. Conversely, if the un\u0002weighted estimate is useless, we still have the opportunity to\namend things in the context of estimating f\n\u2217\np by reweighting\nthe dataset. This work focuses on tackling the problem of\ncovariate shift correction from a doubly robust viewpoint by\neffectively utilizing the unweighted estimate.\nMain Contributions: In summary, the paper makes the fol\u0002lowing contributions. (1) We develop a simple, yet powerful,\nframework for doubly robust estimation in the context of co\u0002variate shift correction, which to the best of our knowledge,\nhas not been previously explored. (2) We demonstrate the\ngenerality of the framework by providing several concrete\nexamples. (3) We present a general theory for the framework\nand provide a detailed analysis in the case of kernel meth\u0002ods. (4) Finally, we show good experimental performance\non several UCI datasets.\nRelated Work\nThere has been extensive research in covariate shift correc\u0002tion problem. Most of the work is directed towards estimat\u0002ing the weights \u03b2. Several methods have been proposed to\nestimate these weights by optimization and statistical tech\u0002niques (Gretton et al. 2008; Agarwal, Li, and Smola 2011;\nSugiyama et al. 2008; Wen, nam Yu, and Greiner 2014).\nLikewise, there has been considerable work in developing\ndoubly robust estimators for many statistical and machine\nlearning problems, particularly in the problems involving\nmissing data and reinforcement learning (Kang and Schafer\n2007; Dud\u00b4\u0131k, Langford, and Li 2011; Bang and Robins\n2005). But none of these works address the problem of our\nconcern, namely doubly robust estimation for covariate shift\ncorrection. While a few works, e.g., (Shimodaira 2000), at\u0002tempt to reduce the variance by adjusting the weights and\nthereby, balancing the bias-variance tradeoff, they do not\ntackle the problem from doubly robust estimation point of\nview. In fact, these methods can be used in conjunction with\nour approach.\nThe most relevant to our work are (Kuzborskij and\nOrabona 2013), (Li and Bilmes 2007) and (Daume III 2007).\nAll these works use similar ideas for addressing related\nproblems in domain adaptation. However, none of these\nworks address th...",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9576/9435"
    },
    {
      "title": "",
      "text": "Robust Learning under Uncertain Test Distributions:\nRelating Covariate Shift to Model Misspecification\nJunfeng Wen1JUNFENG.WEN@UALBERTA.CA\nChun-Nam Yu2 CHUN-NAM.YU@ALCATEL-LUCENT.COM\nRussell Greiner1 RGREINER@UALBERTA.CA\n1Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA\n2Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA\nAbstract\nMany learning situations involve learning the\nconditional distribution ppy|xq when the training\ninstances are drawn from the training distribu\u0002tion ptrpxq, even though it will later be used to\npredict for instances drawn from a different test\ndistribution ptepxq. Most current approaches fo\u0002cus on learning how to reweigh the training ex\u0002amples, to make them resemble the test distribu\u0002tion. However, reweighing does not always help,\nbecause (we show that) the test error also de\u0002pends on the correctness of the underlying model\nclass. This paper analyses this situation by view\u0002ing the problem of learning under changing dis\u0002tributions as a game between a learner and an ad\u0002versary. We characterize when such reweighing\nis needed, and also provide an algorithm, robust\ncovariate shift adjustment (RCSA), that provides\nrelevant weights. Our empirical studies, on UCI\ndatasets and a real-world cancer prognostic pre\u0002diction dataset, show that our analysis applies,\nand that our RCSA works effectively.\n1. Introduction\nTraditional machine learning often explicitly or implicitly\nassumes that the data used for training a model come from\nthe same distribution as that of the test data. However, this\nassumption is violated in many real-world applications. For\nexample, biostatisticians often try to collect a large and di\u0002verse training set, perhaps for building prognostic predic\u0002tors for patients with different diseases. When clinicians\ndeploy these predictors, they do not know whether the lo\u0002cal test patient population will be even close to that training\npopulation. Sometimes we can collect a small sample from\nthe target test population, but in most cases we have noth\u0002Proceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy\u0002right 2014 by the author(s).\ning more than weak prior knowledge about how the test\ndistribution may shift, such as anticipated changes in gen\u0002der ratio or age distribution. It is useful to build predictors\nthat are robust against such changes in test distributions.\nIn this work, we investigate the problem of distribu\u0002tion change under covariate shift assumption (Shimodaira,\n2000), in which both training and test distributions share\nthe same conditional distribution ppy|xq, while their\nmarginal distributions, ptrpxq and ptepxq, are different. To\ncorrect the shifted distribution, major efforts have been\ndedicated to importance reweighing (Quionero-Candela\net al., 2009; Sugiyama & Kawanabe, 2012). However,\nreweighing methods will not necessarily improve the per\u0002formance in test set, as prediction accuracy under covariate\nshift is also dependent on model misspecification (White,\n1981). Fig. 1 shows three examples of misspecified mod\u0002els, where we are considering the model class of straight\nlines of the form y\u201cax`b, for xP r\u00b41.5, 2.5s. In Fig. 1(a),\nno straight line is a good fit for the cubic curve across\nthe whole interval, but Model 2 fits the curve reasonably\nwell in the small interval r\u00b40.5, 0.5s. If training data is\nspread all over r\u00b41.5, 2.5s while test data concentrates on\nr\u00b40.5, 0.5s, improvement via reweighing could be signif\u0002icant. The situation in Fig. 1(b) is different: although the\ntrue model is a curve and not a straight line, the best linear\nfit is no more than \u000f away from the value of the true model.\nIn this case, no matter what test distributions we see in the\ninterval r\u00b41.5, 2.5s, the regression loss of the best linear\nmodel will never be more than \u000f from the Bayes optimal\nloss. In Fig. 1(c), the true model is a straight line except at\nx \u201c 0; perhaps this outlier is a cancer patient whose tumour\nspontaneously disappeared on its own. Unless the test dis\u0002tribution concentrates most of its mass at x \u201c 0, the straight\nline fit learned from the training data over the interval will\nstill be a very good predictor. Sometimes we can rule out\nthis type of covariate shift through prior knowledge. If such\noutliers are extremely rare during training time, we would\nnot expect the test population to have many such patients.\nReweighing will not help much in cases 1(b) and 1(c).\nRobust Learning under Uncertain Test Distributions\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n0\n2\n4\n6\n8\n10\n12\n14\n16\nInput\nOutput\nTrue model\nModel 1\nModel 2\n(a) Large misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\nInput\nOutput\n\u2191\n\u2193\n\u03b5\nTrue model\nBest linear fit\n(b) Small misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nInput\nOutput\nTrue model\n(c) Single point misspecification.\nFigure 1. Three different scenarios of model misspecifications.\nIn this paper, we relate covariate shift to model misspecifi\u0002cation and investigate when reweighing can help a learner\ndeal with covariate shift. We introduce a game between a\nlearner and an adversary that performs robust learning. The\nlearner chooses a model \u03b8 from a set \u0398 to minimize the\nloss, while the adversary chooses a reweighing function \u03b1\nfrom a set A to create new test distributions to maximize the\nloss. There are two major contributions in this paper: First,\nwe provide an improved understanding of the relation be\u0002tween covariate shift and model misspecification through\nthis game analysis. If the learner can find a \u03b8 that min\u0002imizes the loss against any possible \u03b1 that the adversary\ncan play, then it is not necessary to perform reweighing\nagainst covariate shift scenarios represented by A. Sec\u0002ond, we provide a systematic method for checking a model\nclass \u0398 against different covariate shift scenarios, such as\nchanging gender ratio and age distributions in the prognos\u0002tic predictor example, to help user decide whether impor\u0002tance reweighing would be beneficial.\nFor practical use, our method can be used to decide if the\nmodel class is sufficient against shifts that are close to a test\nsample; or robust against a known range of potential shifts\nif test sample is unavailable. If the model class is insuffi\u0002cient, we can consider different ways to deal with covariate\nshifts, such as reweighing using unlabelled test samples, or\nexploring a different model class for the problem.\n2. Related Work\nOur work is inspired by Grunwald & Dawid \u00a8 (2004), who\ninterpret maximum entropy as a game between an adver\u0002sary and a learner on minimizing the worst case expected\nlog loss. Teo et al. (2008) and Globerson & Roweis (2006)\nalso consider an adversarial scenario under changing test\nset conditions, but they are concerned with corruption or\ndeletion of features rather than covariate shift.\nMany results on covariate shift correction involve density\nratio estimation. Shimodaira (2000) showed that, given co\u0002variate shift and model misspecification, reweighing each\ninstance with ptepxq{ptrpxq is asymptotically optimal for\nlog-likelihood estimation, where ptrpxq and ptepxq are as\u0002sumed to be known or estimated in advance. Sugiyama\n& Muller \u00a8 (2005) extended this work by proposing an\n(almost) unbiased estimator for L2 generalization error.\nThere are several works focusing on minimizing differ\u0002ent types of divergence between distributions in the liter\u0002ature (Kanamori et al., 2008; Sugiyama et al., 2008; Ya\u0002mada et al., 2011). Kernel mean matching (KMM) (Huang\net al., 2007) reweighs instances to match means in a\nRKHS (Scholkopf & Smola \u00a8 , 2002). Our work and some\nother approaches (Pan et al., 2009) adapt the idea of match\u0002ing means of the datasets to correct shifted distribution,\nbut we extend their approaches from a two-step optimiza\u0002tion to a game framework that jointly learns a model\nand weights with covariate shift correction. Some other\napproaches (Zadrozny, 2004; Bickel et al.,...",
      "url": "https://proceedings.mlr.press/v32/wen14.pdf"
    },
    {
      "title": "",
      "text": "1 Covariate Shift by Kernel Mean Matching\nArthur Gretton1\nAlex Smola1\nJiayuan Huang\nMarcel Schmittfull\nKarsten Borgwardt\nBernhard Sch\u00a8olkopf\nGiven sets of observations of training and test data, we consider the problem of\nre-weighting the training data such that its distribution more closely matches that\nof the test data. We achieve this goal by matching covariate distributions between\ntraining and test sets in a high dimensional feature space (specifically, a reproducing\nkernel Hilbert space). This approach does not require distribution estimation.\nInstead, the sample weights are obtained by a simple quadratic programming\nprocedure. We provide a uniform convergence bound on the distance between\nthe reweighted training feature mean and the test feature mean, a transductive\nbound on the expected loss of an algorithm trained on the reweighted data, and\na connection to single class SVMs. While our method is designed to deal with the\ncase of simple covariate shift (in the sense of Chapter ??), we have also found\nbenefits for sample selection bias on the labels. Our correction procedure yields\nits greatest and most consistent advantages when the learning algorithm returns a\nclassifier/regressor that is \u201csimpler\u201d than the data might suggest.\n1.1 Introduction\nThe default assumption in many learning scenarios is that training and test data\nare drawn independently and identically (iid) from the same distribution. When\nthe distributions on training and test set do not match, we face the problem of\ndataset shift: given a domain of patterns X and labels Y, we obtain training samples\nZtr =\n\b\n(x\ntr\n1\n, ytr\n1\n), . . . ,(x\ntr\nntr\n, ytr\nntr\n)\n\t\n\u2286 X \u00d7 Y from a Borel probability distribution\nPtr(x, y), and test samples Zte =\n\b\n(x\nte\n1\n, yte\n1\n), . . . ,(x\nte\nnte\n, yte\nnte\n)\n\t\n\u2286 X \u00d7 Y drawn from\nanother such distribution Pte(x, y).\n1. Both authors contributed equally.\n2 Covariate Shift by Kernel Mean Matching\nAlthough there exists previous work addressing this problem [Zadrozny, 2004,\nRosset et al., 2004, Heckman, 1979, Lin et al., 2002, Dud\u00b4\u0131k et al., 2005, Shimodaira,\n2000, Sugiyama and M\u00a8uller, 2005], dataset shift has typically been ignored in\nstandard estimation algorithms. Nonetheless, in reality the problem occurs rather\nfrequently. Below, we give some examples of where dataset shift occurs (following\nthe terminology defined by Storkey in Chapter ??).\n1. Suppose we wish to generate a model to diagnose breast cancer. Suppose,\nmoreover, that most women who participate in the breast screening test are\nmiddle-aged and likely to have attended the screening in the preceding 3 years.\nConsequently our sample includes mostly older women and those who have low risk\nof breast cancer because they have been tested before. This problem is referred to\nas sample selection bias. The examples do not reflect the general population with\nrespect to age (which amounts to a bias in Ptr(x)) and they only contain very few\ndiseased cases (i.e. a bias in Ptr(y|x)).\n2. Consider the problem of data analysis using a brain computer interface, where\nthe distribution over incoming signals is known to change as experiments go on\n(subjects tire, the sensor setup changes, etc). In this case it necessary to adapt\nthe estimator to the new distribution of patterns in order to improve performance\n[Sugiyama et al., 2007].\n3. Gene expression profile studies using DNA microarrays are used in tumor\ndiagnosis. A common problem is that the samples are obtained using certain\nprotocols, microarray platforms and analysis techniques, and typically have small\nsample sizes. The test cases are recorded under different conditions, resulting in\na different distribution of gene expression values. In both this and the previous\nexample, a covariate shift has occurred (see Chapter ??).\nIn all cases we would intuitively want to assign more weight those observations in\nthe training set which are most similar to those in the test set, and less weight to\nthose which rarely occur in the test set.\nIn this chapter, we use unlabeled data as the basis for a dataset shift correction\nprocedure for various learning methods. Unlike previous work, we infer the resam\u0002pling weight directly by non-parametric distribution matching between training and\ntesting samples. We do not need to estimate the biasing densities or selection proba\u0002bilities [Zadrozny, 2004, Dud\u00b4\u0131k et al., 2005, Shimodaira, 2000], or to assume advance\nknowledge of the different class probabilities [Lin et al., 2002]. Rather, we account\nfor the difference between Ptr(x, y) and Pte(x, y) by reweighting the training points\nsuch that the means of the training and test points in a reproducing kernel Hilbert\nspace (RKHS) are close. We call this reweighting process kernel mean matching\n(KMM), following our presentation in [Huang et al., 2007]. The present chapter\nexpands on our earlier work in terms of both theoretical and experimental analysis.\nSince our approach does not require density estimation, we are able to state re\u0002sults which apply to arbitrary domains and which do not, in principle, suffer from\nthe curse of dimensionality that befalls high-dimensional density estimation. When\n1.1 Introduction 3\nthe RKHS is universal [Steinwart, 2002], the population solution to this minimisa\u0002tion is exactly the ratio Pte(x, y)/Ptr(x, y); we derive performance guarantees which\ndepend on the maximum ratio between the distributions (but not on the distribu\u0002tions themselves) and which show that our method is consistent. We remark that\nwhen this ratio is large, however, a large sample size would be required to ensure\nthe bound is tight (and to guarantee a good approximation).\nThe required optimisation is a simple quadratic program, and the reweighted\nsample can be incorporated straightforwardly into many regression and classifica\u0002tion algorithms and model selection procedures, such as cross validation. We apply\nour method to a variety of regression and classification benchmarks from UCI and\nelsewhere, as well as to classification of microarrays from prostate and breast cancer\npatients. The experiments demonstrate that sample reweighting by KMM substan\u0002tially improves learning performance in cases where the class of functions output\nby the learning algorithm is \u201csimpler\u201d than the true function (for instance, such a\nclassification algorithm would estimate a decision boundary deliberately smoother\nthan the Bayes optimal boundary that emerges as the sample size increases to in\u0002finity). Indeed, for this case, performance can be improved from close to chance\nlevel to the point of almost matching the performance of a learning algorithm with\nthe \u201ccorrect\u201d complexity. KMM reweighting can also improve performance in cases\nwhere the complexity of the leaned classification/regression function is chosen opti\u0002mally for the data, via parameter selection by cross validation. For most such cases,\nhowever, KMM does not affect performance, or can even make it slightly worse.\nIn general, the estimation problem with two different distributions Ptr(x, y) and\nPte(x, y) is unsolvable, as the two distributions could be arbitrarily far apart. In\nparticular, for arbitrary Ptr(y|x) and Pte(y|x), there is no way we could infer a good\nestimator based on the training sample. For instance, the distributions Ptr(y = 1|x)\nand Pte(y = \u22121|x) could be swapped in binary classification, leading to arbitrarily\nlarge error. The following assumption allows us to address the problem.\nAssumption 1.1 We make the simplifying assumption that Ptr(x, y) and Pte(x, y)\nonly differ via Ptr(x, y) = P(y|x)Ptr(x) and Pte(x, y) = P(y|x)Pte(x). In other\nwords, the conditional probabilities of y|x remain unchanged.\nThis particular case of dataset shift has been termed covariate shift (see examples\nabove, Chapter ?? and [Shimodaira, 2000]). We will see experimentally that even\nin situations where our key assumption is not valid, our method can still be useful\n(see Section 1.6).\nWe begin our presentation in Section 1.2, where we describe the co...",
      "url": "https://www.gatsby.ucl.ac.uk/~gretton/papers/covariateShiftChapter.pdf"
    },
    {
      "title": "Covariate shift and local learning by distribution matching",
      "text": "Covariate shift and local learning by distribution matching | Empirical Inference \u2013MPI-IS\n[Institute Homepage](https://is.mpg.de/)\n[Institute Homepage](https://is.mpg.de/)[Sign In](https://is.mpg.de/ei/en/sign_in)\n[Back](https://is.mpg.de/ei/publications)\n[Empirical Inference](https://is.mpg.de/ei)[Book Chapter](https://is.mpg.de/ei/en/publications?bibtex_type=inbook)[2009](https://is.mpg.de/ei/en/publications?year=2009)# Covariate shift and local learning by distribution matching\n[PDF](http://www.is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/shift-book-for-LeEtAl-webversion_5376[0].pdf)[Web](http://mitpress.mit.edu/books/dataset-shift-machine-learning)\n[![no image](https://is.mpg.de/assets/no_employee-e06e20249dbc33f3e8311182c2fca99231e034c34648cfcc73f637940e3012d2.png)](https://is.mpg.de/ei/person/arthur)\nEmpirical Inference\n[Arthur Gretton](https://is.mpg.de/ei/person/arthur)\n[![Thumb ticker sm l1170153](https://is.mpg.de/uploads/person/image/999145/thumb_ticker_sm_L1170153.jpg)](https://is.mpg.de/ei/person/bs)\nEmpirical Inference\n[Bernhard Sch\u00f6lkopf](https://is.mpg.de/ei/person/bs)\n* Director\n[![no image](https://is.mpg.de/assets/no_employee-e06e20249dbc33f3e8311182c2fca99231e034c34648cfcc73f637940e3012d2.png)](https://is.mpg.de/ei/person/huang)\nEmpirical Inference\n[Jiayuan Huang](https://is.mpg.de/ei/person/huang)\n* Doctoral Researcher\n[![no image](https://is.mpg.de/assets/no_employee-e06e20249dbc33f3e8311182c2fca99231e034c34648cfcc73f637940e3012d2.png)](https://is.mpg.de/ei/person/msl)\nEmpirical Inference\n[Marcel Schmittfull](https://is.mpg.de/ei/person/msl)\n[![Thumb ticker sm borgwardt](https://is.mpg.de/uploads/person/image/99922/thumb_ticker_sm_borgwardt.jpg)](https://is.mpg.de/ei/person/99922)\nEmpirical Inference\n[Karsten Borgwardt](https://is.mpg.de/ei/person/99922)\n* Research Group Leader\nGiven sets of observations of training and test data, we consider the problem of re-weighting the training data such that its distribution more closely matches that of the test data. We achieve this goal by matching covariate distributions between training and test sets in a high dimensional feature space (specifically, a reproducing\rkernel Hilbert space). This approach does not require distribution estimation. Instead, the sample weights are obtained by a simple quadratic programming procedure. We provide a uniform convergence bound on the distance between\rthe reweighted training feature mean and the test feature mean, a transductive bound on the expected loss of an algorithm trained on the reweighted data, and a connection to single class SVMs. While our method is designed to deal with the case of simple covariate shift (in the sense of Chapter ??), we have also found benefits for sample selection bias on the labels. Our correction procedure yields its greatest and most consistent advantages when the learning algorithm returns a classifier/regressor that is simpler&quot; than the data might suggest.\nAuthor(s):|Gretton, A. and Smola, AJ. and Huang, J. and Schmittfull, M. and Borgwardt, KM. and Sch\u00f6lkopf, B.|\n**Links:**|\n* [PDF](http://www.is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/shift-book-for-LeEtAl-webversion_5376[0].pdf)\n* [Web](http://mitpress.mit.edu/books/dataset-shift-machine-learning)|\nBook Title:|Dataset Shift in Machine Learning|\nPages:|131-160|\nYear:|2009|\nDay:|0|\nEditors:|Qui\u00f1onero-Candela, J., Sugiyama, M., Schwaighofer, A. and Lawrence, N. D.|\nPublisher:|MIT Press|\n|\nBibTeX Type:|Book Chapter(inbook)|\n|\n**Address:**|Cambridge, MA, USA|\n|\n**Electronic Archiving:**|grant\\_archive|\n**ISBN:**|978-0-262-17005-5|\n**Language:**|en|\n**Organization:**|Max-Planck-Gesellschaft|\n**School:**|Biologische Kybernetik|\n### BibTeX\n```\n@inbook{5376,\ntitle = {Covariate shift and local learning by distribution matching},\nbooktitle = {Dataset Shift in Machine Learning},\nabstract = {Given sets of observations of training and test data, we consider the problem of re-weighting the training data such that its distribution more closely matches that of the test data. We achieve this goal by matching covariate distributions between training and test sets in a high dimensional feature space (specifically, a reproducing\rkernel Hilbert space). This approach does not require distribution estimation. Instead, the sample weights are obtained by a simple quadratic programming procedure. We provide a uniform convergence bound on the distance between\rthe reweighted training feature mean and the test feature mean, a transductive bound on the expected loss of an algorithm trained on the reweighted data, and a connection to single class SVMs. While our method is designed to deal with the case of simple covariate shift (in the sense of Chapter ??), we have also found benefits for sample selection bias on the labels. Our correction procedure yields its greatest and most consistent advantages when the learning algorithm returns a classifier/regressor that is simpler&quot;&quot; than the data might suggest.},\npages = {131-160},\neditors = {Qui\u00f1onero-Candela, J., Sugiyama, M., Schwaighofer, A. and Lawrence, N. D.},\npublisher = {MIT Press},\norganization = {Max-Planck-Gesellschaft},\nschool = {Biologische Kybernetik},\naddress = {Cambridge, MA, USA},\nyear = {2009},\nauthor = {Gretton, A. and Smola, AJ. and Huang, J. and Schmittfull, M. and Borgwardt, KM. and Sch{\\\\&quot;&quot;o}lkopf, B.}\n}\n```\n</div>\n##### </button>\n[More information**]()\n##### Download QR Code",
      "url": "https://is.mpg.de/ei/publications/5376"
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[2103.02667] Out of Distribution Generalization in Machine Learning\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2103.02667\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2103.02667**(stat)\n[Submitted on 3 Mar 2021]\n# Title:Out of Distribution Generalization in Machine Learning\nAuthors:[Martin Arjovsky](https://arxiv.org/search/stat?searchtype=author&amp;query=Arjovsky,+M)\nView a PDF of the paper titled Out of Distribution Generalization in Machine Learning, by Martin Arjovsky\n[View PDF](https://arxiv.org/pdf/2103.02667)> > Abstract:\n> Machine learning has achieved tremendous success in a variety of domains in recent years. However, a lot of these success stories have been in places where the training and the testing distributions are extremely similar to each other. In everyday situations when models are tested in slightly different data than they were trained on, ML algorithms can fail spectacularly. This research attempts to formally define this problem, what sets of assumptions are reasonable to make in our data and what kind of guarantees we hope to obtain from them. Then, we focus on a certain class of out of distribution problems, their assumptions, and introduce simple algorithms that follow from these assumptions that are able to provide more reliable generalization. A central topic in the thesis is the strong link between discovering the causal structure of the data, finding features that are reliable (when using them to predict) regardless of their context, and out of distribution generalization. Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:2103.02667](https://arxiv.org/abs/2103.02667)[stat.ML]|\n|(or[arXiv:2103.02667v1](https://arxiv.org/abs/2103.02667v1)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2103.02667](https://doi.org/10.48550/arXiv.2103.02667)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Martin Arjovsky [[view email](https://arxiv.org/show-email/75023b39/2103.02667)]\n**[v1]**Wed, 3 Mar 2021 20:35:19 UTC (1,199 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Out of Distribution Generalization in Machine Learning, by Martin Arjovsky\n* [View PDF](https://arxiv.org/pdf/2103.02667)\n* [TeX Source](https://arxiv.org/src/2103.02667)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2103.02667&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2103.02667&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2021-03](https://arxiv.org/list/stat.ML/2021-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2103.02667?context=cs)\n[cs.LG](https://arxiv.org/abs/2103.02667?context=cs.LG)\n[stat](https://arxiv.org/abs/2103.02667?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.02667)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.02667)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.02667)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2103.02667&amp;description=Out of Distribution Generalization in Machine Learning>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2103.02667&amp;title=Out of Distribution Generalization in Machine Learning>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2103.02667)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2103.02667"
    }
  ]
}