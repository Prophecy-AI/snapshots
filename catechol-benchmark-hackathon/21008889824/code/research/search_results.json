{
  "query": "How can I reduce the intercept in the relationship between cross-validation score and test score when there is distribution shift in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent work on cross\u2011validation under dataset shift, you can shrink the intercept (i.e., the systematic bias) between CV\u2011derived scores and true test scores by re\u2011weighting the validation folds and applying bias\u2011corrected CV estimates\u202f([Sugiyama\u202fet\u202fal., 2007](https://jmlr.org/papers/v8/sugiyama07a.html); [Wiley review\u202f2022](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1557)).\n\n**Steps to reduce the intercept**\n\n1. **Detect and quantify the shift**  \n   *Estimate a density\u2011ratio\u202f\\(w(x)=p_{\\text{test}}(x)/p_{\\text{train}}(x)\\)* (e.g., with kernel density estimation or logistic\u2011regression\u2011based propensity scores). This ratio tells you how much each training sample should count when evaluating on the test distribution\u202f([Sugiyama\u202fet\u202fal., 2007](https://jmlr.org/papers/v8/sugiyama07a.html)).\n\n2. **Apply importance\u2011weighted CV (IWCV)**  \n   *During each CV fold, weight the training\u2011set loss by\u202f\\(w(x)\\)* and compute the validation loss with the same weights. IWCV restores the unbiasedness of the CV estimate under covariate shift, directly lowering the systematic offset (intercept) between CV and test performance\u202f([Sugiyama\u202fet\u202fal., 2007](https://jmlr.org/papers/v8/sugiyama07a.html)).\n\n3. **Use bias\u2011corrected CV if weighting is not feasible**  \n   *Prefer exact or approximate leave\u2011one\u2011out CV* (or\u202fk\u2011fold with bias correction when\u202fk\u202f<\u202f10) to minimise the inherent optimism bias that manifests as a positive intercept\u202f([Wiley review\u202f2022](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1557)).\n\n4. **Calibrate the CV\u2011test relationship**  \n   *Fit a simple linear calibration model*\u202f\\( \\hat y_{\\text{test}} = \\alpha \\, \\text{CV\\_score} + \\beta\\) on a small held\u2011out validation set drawn from the target distribution, then **force the intercept\u202f\u03b2\u202f\u2248\u202f0** by either:  \n   - Adding a regularisation term that penalises\u202f|\u03b2|, or  \n   - Re\u2011training the calibration model without an intercept **only if** you are confident that a perfect CV score should correspond to a perfect test score (i.e., the relationship truly passes through the origin). The statistical community advises against dropping the intercept unless this condition is guaranteed\u202f([Stats\u202fSE\u202f2011](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)).\n\n5. **Validate the adjusted estimate**  \n   *Evaluate the weighted\u2011and\u2011calibrated CV scores on an independent test set* from the shifted distribution. If the intercept is still sizable, iterate: refine the density\u2011ratio estimator, increase the number of folds, or adopt more robust shift\u2011adaptation methods such as **Robust Covariate Shift Adjustment (RCSA)**\u202f([Wen\u202fet\u202fal., 2014](https://proceedings.mlr.press/v32/wen14.pdf)).\n\nBy weighting the training data to reflect the test distribution, correcting CV bias, and (only when justified) removing the intercept in the final calibration, the systematic gap between cross\u2011validation and true test performance can be substantially reduced.",
      "url": ""
    },
    {
      "title": "When is it ok to remove the intercept in a linear regression model?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [When is it ok to remove the intercept in a linear regression model?](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked13 years, 3 months ago\n\nModified [1 month ago](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model?lastactivity)\n\nViewed\n230k times\n\n183\n\n$\\\\begingroup$\n\nI am running linear regression models and wondering what the conditions are for removing the intercept term.\n\nIn comparing results from two different regressions where one has the intercept and the other does not, I notice that the $R^2$ of the function without the intercept is much higher. Are there certain conditions or assumptions I should be following to make sure the removal of the intercept term is valid?\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [linear-model](https://stats.stackexchange.com/questions/tagged/linear-model)\n- [r-squared](https://stats.stackexchange.com/questions/tagged/r-squared)\n- [intercept](https://stats.stackexchange.com/questions/tagged/intercept)\n- [faq](https://stats.stackexchange.com/questions/tagged/faq)\n\n[Share](https://stats.stackexchange.com/q/7948)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/7948/edit)\n\nFollow\n\n[edited Sep 22, 2022 at 1:13](https://stats.stackexchange.com/posts/7948/revisions)\n\n[![kjetil b halvorsen's user avatar](https://i.sstatic.net/Nri78.jpg?s=64)](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen)\n\n[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\n80.3k3131 gold badges196196 silver badges639639 bronze badges\n\nasked Mar 7, 2011 at 9:14\n\n[![analyticsPierce's user avatar](https://www.gravatar.com/avatar/3b5598a61f85012ea1de2a5c8cee2e6c?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/1422/analyticspierce)\n\n[analyticsPierce](https://stats.stackexchange.com/users/1422/analyticspierce) analyticsPierce\n\n2,03144 gold badges1414 silver badges66 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- 1\n\n\n\n\n\n$\\\\begingroup$What would the intercept mean in your model? From the information in your question, it seems it would be the expected value of your response when sqft=0 and lotsize=0 and baths=0. Is that ever going to occur in reality?$\\\\endgroup$\n\n\u2013\u00a0[timbp](https://stats.stackexchange.com/users/7644/timbp)\n\nCommentedFeb 27, 2012 at 2:59\n\n- $\\\\begingroup$the intercept has no meaning. so when writing up a formula for the expected value of a house, can I leave this out?$\\\\endgroup$\n\n\u2013\u00a0[Travis](https://stats.stackexchange.com/users/9363/travis)\n\nCommentedFeb 27, 2012 at 3:02\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Instead of the y= a + b1 x1 + b2 x2 + b3x3, can I omit a?$\\\\endgroup$\n\n\u2013\u00a0[Travis](https://stats.stackexchange.com/users/9363/travis)\n\nCommentedFeb 27, 2012 at 3:03\n\n- 3\n\n\n\n\n\n$\\\\begingroup$**NB**: Some of these comments and replies address essentially the same question (framed in the context of a housing price regression) which was merged with this one as a duplicate.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedFeb 27, 2012 at 4:51\n\n- 1\n\n\n\n\n\n$\\\\begingroup$A side point: $R^2$ does not have such good properties when you don't include an intercept in your model, so it's not the best statistic to use for your comparison.$\\\\endgroup$\n\n\u2013\u00a0[Charlie](https://stats.stackexchange.com/users/401/charlie)\n\nCommentedFeb 27, 2012 at 7:43\n\n\n[Add a comment](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\u00a0\\|\n\n## 11 Answers 11\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n125\n\n$\\\\begingroup$\n\nThe **shortest** answer: **never**, unless you are _sure_ that your linear approximation of the data generating process (linear regression model) either by some theoretical or any other reasons _is forced to go through the origin_. If not the other regression parameters will be biased even if intercept is statistically insignificant (strange but it is so, consult [Brooks](http://www.cambridge.org/features/economics/brooks/) _Introductory Econometrics_ for instance). Finally, as I do often explain to my students, by leaving the intercept term you insure that the residual term is zero-mean.\n\nFor your two models case we need more context. It may happen that linear model is not suitable here. For example, you need to log transform first if the model is multiplicative. Having exponentially growing processes it may occasionally happen that $R^2$ for the model without the intercept is \"much\" higher.\n\nScreen the data, test the model with RESET test or any other linear specification test, this may help to see if my guess is true. And, building the models highest $R^2$ is one of the last statistical properties I do really concern about, but it is nice to present to the people who are not so well familiar with econometrics (there are many dirty tricks to make determination close to 1 :)).\n\n[Share](https://stats.stackexchange.com/a/7950)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/7950/edit)\n\nFollow\n\nanswered Mar 7, 2011 at 10:16\n\n[![Dmitrij Celov's user avatar](https://www.gravatar.com/avatar/533a5a7b1a34f5100c8f15022a677137?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/2645/dmitrij-celov)\n\n[Dmitrij Celov](https://stats.stackexchange.com/users/2645/dmitrij-celov) Dmitrij Celov\n\n6,28522 gold badges3030 silver badges4141 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- 11\n\n\n\n\n\n$\\\\begingroup$-1 for \"never\", see example 1 of Joshuas' answer$\\\\endgroup$\n\n\u2013\u00a0[Tomas](https://stats.stackexchange.com/users/5509/tomas)\n\nCommentedDec 2, 2013 at 20:43\n\n- 7\n\n\n\n\n\n$\\\\begingroup$@Curious, \"never\" is written with \"unless\" examples below just show the exceptions when it is legal to remove intercept. When you don't know the data generating process or theory, or are not forced to go through the origin by standardization or any other special model, keep it. Keeping intercept is like using the trash bin to collect all the distortions caused by linear approximation and other simplifications. P.S. practically the response shows that you read just shortest :) Thanks a lot to Joshua (+1) for the extended examples.$\\\\endgroup$\n\n\u2013\u00a0[Dmitrij Celov](https://stats.stackexchange.com/users/2645/dmitrij-celov)\n\nCommentedMay 6, 2014 at 9:56\n\n- 5\n\n\n\n\n\n$\\\\begingroup$You missed the point of Joshua Example 1 and seem to still ignore it completely. In models with categorical covariate the removal of the intercept results in the same model with just different parametrization. This is a legitimate case when intercept can be removed.$\\\\endgroup$\n\n\u2013\u00a0[Tomas](https://stats.stackexchange.com/users/5509/tomas)\n\nCommentedMay 26, 2014 at 15:04\n\n- 3\n\n\n\n\n\n$\\\\begingroup$@Curious, in Joshua example 1, you need to add a new dummy variable for the level of the categorical variable you previously considered as baseline, and this new dummy variable will take the value of the intercept, so you are NOT removing the intercept, just renaming it and reparameterizing the rest of the parameters of the categorical covariate. Therefore the argument of Dmitrij holds.$\\\\endgroup$\n\n\u2013\u00a0[Rufo](https://stats.stackexchange.com/users/39849/rufo)\n\nCommentedApr 15, 2019 at 15:17\n\n\n[Add a comment](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\u00a0\\|\n\n90\n\n$\\\\begingroup$\n\nRemoving the intercept is a different model, but there are plenty of examples where it is legitimate. Answers so far have alrea...",
      "url": "https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"
    },
    {
      "title": "Cross validation for model selection: A review with examples from ...",
      "text": "<div><div><article>\n \n <section>\n <h2>Abstract</h2>\n <p>Specifying, assessing, and selecting among candidate statistical models is fundamental to ecological research. Commonly used approaches to model selection are based on predictive scores and include information criteria such as Akaike's information criterion, and cross validation. Based on data splitting, cross validation is particularly versatile because it can be used even when it is not possible to derive a likelihood (e.g., many forms of machine learning) or count parameters precisely (e.g., mixed-effects models). However, much of the literature on cross validation is technical and spread across statistical journals, making it difficult for ecological analysts to assess and choose among the wide range of options. Here we provide a comprehensive, accessible review that explains important\u2014but often overlooked\u2014technical aspects of cross validation for model selection, such as: bias correction, estimation uncertainty, choice of scores, and selection rules to mitigate overfitting. We synthesize the relevant statistical advances to make recommendations for the choice of cross-validation technique and we present two ecological case studies to illustrate their application. In most instances, we recommend using exact or approximate leave-one-out cross validation to minimize bias, or otherwise <i>k</i>-fold with bias correction if <i>k</i>\u2009&lt;\u200910. To mitigate overfitting when using cross validation, we recommend calibrated selection via our recently introduced modified one-standard-error rule. We advocate for the use of predictive scores in model selection across a range of typical modeling goals, such as exploration, hypothesis testing, and prediction, provided that models are specified in accordance with the stated goal. We also emphasize, as others have done, that inference on parameter estimates is biased if preceded by model selection and instead requires a carefully specified single model or further technical adjustments.</p>\n </section>\n \n \n <section>\n <section>\n \n <h2> INTRODUCTION</h2>\n \n <p>The expression of scientific hypotheses as statistical models is a fundamental component of ecological research. In addition to expert domain knowledge, modern statistical modeling requires substantial technical considerations including the formulation of mathematical descriptions of hypothesized relationships between variables and the structure of stochastic processes (Fox et al.,\u00a0<span><a href=\"#ecm1557-bib-0035\">2015</a></span>). Statistical modeling also involves consideration of computational aspects involved with the fitting of models to data. When existing theory and empirical evidence are insufficient to uniquely inform model choice, alternative models can be formulated and the available data used to assess their relative merits (Claeskens &amp; Hjort,\u00a0<span><a href=\"#ecm1557-bib-0026\">2008</a></span>; Hooten &amp; Hobbs,\u00a0<span><a href=\"#ecm1557-bib-0051\">2015</a></span>). The use of data in this way\u2014to assess and ultimately select among alternative models\u2014is called model selection. As an adjunct to statistical modeling, model selection has become integral to ecological research; indeed, as Tredennick et al. (<span><a href=\"#ecm1557-bib-0084\">2021</a></span>) assert: \u201cconfusion about how to do model selection is confusion about how to do ecology\u201d.</p>\n \n <p>Cross validation is a technique based on data splitting to make predictive assessments of statistical models. Although the specific goal of a statistical analysis\u2014such as hypothesis testing or prediction\u2014can constrain the set of models under consideration, predictive assessment is a broadly applicable and objective basis for both model comparison and selection across a range of modeling goals (Shmueli &amp; Koppius,\u00a0<span><a href=\"#ecm1557-bib-0079\">2011</a></span>). As a technique for predictive assessment, cross validation is extremely flexible due to the breadth of predictive measures (scores) with which it can be used (Gneiting &amp; Raftery,\u00a0<span><a href=\"#ecm1557-bib-0043\">2007</a></span>), the availability of data-splitting strategies that can be employed to account for the structure of the data and/or manage computational costs and estimation bias (Arlot,\u00a0<span><a href=\"#ecm1557-bib-0004\">2008</a></span>), and its broad applicability to both optimisation and Bayesian frameworks. Recent methodological innovations, often involving approximation methods (Vehtari et al.,\u00a0<span><a href=\"#ecm1557-bib-0086\">2017</a></span>), have also improved the computational efficiency of cross-validation algorithms. Further, when the predictive measure is log likelihood, cross-validation estimates the relative expected Kullback\u2013Leibler divergence which is the objective of commonly adopted information-theoretic model selection methods such as Akaike's information criterion (AIC) approximation (Akaike,\u00a0<span><a href=\"#ecm1557-bib-0001\">1973</a></span>).</p>\n \n <p>An important and long-standing concern in model selection is the issue of overfitting\u2014the inclusion of spurious variables in a selected model. For hypothesis testing, overfitting misleads future research and is considered a substantial driver of the current replicability crisis in the sciences (Benjamini,\u00a0<span><a href=\"#ecm1557-bib-0009\">2020</a></span>). For predictive goals, overfitting degrades the generalization of predictive performance to new data. Although it is well known that information-theoretic approaches, and predictive model selection more generally, suffer a tendency to overfit, it is less known that this proclivity is predominantly due to failure to correctly account for score-estimation uncertainty (Piironen &amp; Vehtari,\u00a0<span><a href=\"#ecm1557-bib-0072\">2017a</a></span>). For model selection based on cross validation, an effective and easily applied mitigation strategy is the use of calibrated selection rules which identity the simplest model with comparable predictive performance to the best-scoring model (Yates et al.,\u00a0<span><a href=\"#ecm1557-bib-0091\">2021</a></span>).</p>\n \n <p>In this paper we seek to provide an accessible yet comprehensive review on using and understanding cross validation for model selection, with a focus on ecological problems. Our own efforts to synthesis a coherent picture of the current state of the model-selection literature\u2014which mostly appears as technical results in statistical journals\u2014motivated us to create a useful reference for practitioners who are not necessarily biostatisticians. We include and explain technical aspects that are important but often overlooked, such as bias correction, estimation uncertainty, choice of predictive score, and calibrated selection rules. In addition to the technical and conceptual review we give clear recommendations on cross-validation strategies including adjustments to manage computational demands while accounting for bias and mitigating the risk of overfitting. Prior to an exposition of model scores and cross-validation techniques, the first section of this paper provides an overview of the different goals of statistical modeling, the corresponding purposes of model selection, and the merits of using predictive assessment. In particular we emphasize, as others have done (Breiman &amp; Spector,\u00a0<span><a href=\"#ecm1557-bib-0014\">1992</a></span>; Claeskens &amp; Hjort,\u00a0<span><a href=\"#ecm1557-bib-0026\">2008</a></span>), the incompatibility of model selection with inference on model parameters. Several boxes and tables are included in the paper to provide an easily referenced summary of the topics reviewed or an additional level of detail on specific subjects. We also provide two ecological case studies which are used to illustrate many of the methods recommended in the paper.</p>\n </section>\n <section>\n \n <h2> PREDICTIVE ASSESSMENT, MODELING GOALS, AND THE PURPOSE OF MODEL SELECTION</h2>\n \n <p>Cross validation works by splitting the available data into a pair of training and test sets where the model is fit...",
      "url": "https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1557"
    },
    {
      "title": "Covariate Shift Adaptation by Importance Weighted Cross Validation",
      "text": "[Home Page](https://www.jmlr.org/)\n\n[Papers](https://www.jmlr.org/papers)\n\n[Submissions](https://www.jmlr.org/author-info.html)\n\n[News](https://www.jmlr.org/news.html)\n\n[Editorial Board](https://www.jmlr.org/editorial-board.html)\n\n[Special Issues](https://www.jmlr.org/special_issues/)\n\n[Open Source Software](https://www.jmlr.org/mloss)\n\n[Proceedings (PMLR)](https://proceedings.mlr.press/)\n\n[Data (DMLR)](https://data.mlr.press/)\n\n[Transactions (TMLR)](https://www.jmlr.org/tmlr)\n\n[Search](https://www.jmlr.org/search-jmlr.html)\n\n[Statistics](https://www.jmlr.org/stats.html)\n\n[Login](https://www.jmlr.org/manudb)\n\n[Frequently Asked Questions](https://www.jmlr.org/faq.html)\n\n[Contact Us](https://www.jmlr.org/contact.html)\n\n## Covariate Shift Adaptation by Importance Weighted Cross Validation\n\n**_Masashi Sugiyama, Matthias Krauledat, Klaus-Robert M\u00fcller_**; 8(35):985\u22121005, 2007.\n\n### Abstract\n\nA common assumption in supervised learning is that the input points in\nthe training set follow the _same_ probability distribution as\nthe input points that will be given in the future test phase.\nHowever, this assumption is not satisfied, for example, when the\noutside of the training region is extrapolated. The situation where\nthe training input points and test input points follow\n_different_ distributions while the conditional distribution of\noutput values given input points is unchanged is called the\n_covariate shift_. Under the covariate shift, standard model\nselection techniques such as cross validation do not work as desired\nsince its unbiasedness is no longer maintained. In this paper, we\npropose a new method called _importance weighted cross_\n_validation_ (IWCV), for which we prove its unbiasedness even under the\ncovariate shift. The IWCV procedure is the only one that can be\napplied for unbiased classification under covariate shift, whereas\nalternatives to IWCV exist for regression. The usefulness of our\nproposed method is illustrated by simulations, and furthermore\ndemonstrated in the brain-computer interface, where strong\nnon-stationarity effects can be seen between training and test\nsessions.\n\n\\[abs\\]\\[ [pdf](https://www.jmlr.org/papers/volume8/sugiyama07a/sugiyama07a.pdf)\\]\\[ [bib](https://www.jmlr.org/papers/v8/sugiyama07a.bib)\\]\n\n\n|     |\n| --- |\n| \u00a9 [JMLR](https://www.jmlr.org) 2007.<br>( [edit](https://github.com/JmlrOrg/v8/tree/main/sugiyama07a), [beta](http://jmlr.org/beta/papers/v8/sugiyama07a.html)) |\n\n [Mastodon](https://sigmoid.social/@jmlr)",
      "url": "https://jmlr.org/papers/v8/sugiyama07a.html"
    },
    {
      "title": "",
      "text": "Robust Learning under Uncertain Test Distributions:\nRelating Covariate Shift to Model Misspecification\nJunfeng Wen1JUNFENG.WEN@UALBERTA.CA\nChun-Nam Yu2 CHUN-NAM.YU@ALCATEL-LUCENT.COM\nRussell Greiner1 RGREINER@UALBERTA.CA\n1Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA\n2Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA\nAbstract\nMany learning situations involve learning the\nconditional distribution ppy|xq when the training\ninstances are drawn from the training distribu\u0002tion ptrpxq, even though it will later be used to\npredict for instances drawn from a different test\ndistribution ptepxq. Most current approaches fo\u0002cus on learning how to reweigh the training ex\u0002amples, to make them resemble the test distribu\u0002tion. However, reweighing does not always help,\nbecause (we show that) the test error also de\u0002pends on the correctness of the underlying model\nclass. This paper analyses this situation by view\u0002ing the problem of learning under changing dis\u0002tributions as a game between a learner and an ad\u0002versary. We characterize when such reweighing\nis needed, and also provide an algorithm, robust\ncovariate shift adjustment (RCSA), that provides\nrelevant weights. Our empirical studies, on UCI\ndatasets and a real-world cancer prognostic pre\u0002diction dataset, show that our analysis applies,\nand that our RCSA works effectively.\n1. Introduction\nTraditional machine learning often explicitly or implicitly\nassumes that the data used for training a model come from\nthe same distribution as that of the test data. However, this\nassumption is violated in many real-world applications. For\nexample, biostatisticians often try to collect a large and di\u0002verse training set, perhaps for building prognostic predic\u0002tors for patients with different diseases. When clinicians\ndeploy these predictors, they do not know whether the lo\u0002cal test patient population will be even close to that training\npopulation. Sometimes we can collect a small sample from\nthe target test population, but in most cases we have noth\u0002Proceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy\u0002right 2014 by the author(s).\ning more than weak prior knowledge about how the test\ndistribution may shift, such as anticipated changes in gen\u0002der ratio or age distribution. It is useful to build predictors\nthat are robust against such changes in test distributions.\nIn this work, we investigate the problem of distribu\u0002tion change under covariate shift assumption (Shimodaira,\n2000), in which both training and test distributions share\nthe same conditional distribution ppy|xq, while their\nmarginal distributions, ptrpxq and ptepxq, are different. To\ncorrect the shifted distribution, major efforts have been\ndedicated to importance reweighing (Quionero-Candela\net al., 2009; Sugiyama & Kawanabe, 2012). However,\nreweighing methods will not necessarily improve the per\u0002formance in test set, as prediction accuracy under covariate\nshift is also dependent on model misspecification (White,\n1981). Fig. 1 shows three examples of misspecified mod\u0002els, where we are considering the model class of straight\nlines of the form y\u201cax`b, for xP r\u00b41.5, 2.5s. In Fig. 1(a),\nno straight line is a good fit for the cubic curve across\nthe whole interval, but Model 2 fits the curve reasonably\nwell in the small interval r\u00b40.5, 0.5s. If training data is\nspread all over r\u00b41.5, 2.5s while test data concentrates on\nr\u00b40.5, 0.5s, improvement via reweighing could be signif\u0002icant. The situation in Fig. 1(b) is different: although the\ntrue model is a curve and not a straight line, the best linear\nfit is no more than \u000f away from the value of the true model.\nIn this case, no matter what test distributions we see in the\ninterval r\u00b41.5, 2.5s, the regression loss of the best linear\nmodel will never be more than \u000f from the Bayes optimal\nloss. In Fig. 1(c), the true model is a straight line except at\nx \u201c 0; perhaps this outlier is a cancer patient whose tumour\nspontaneously disappeared on its own. Unless the test dis\u0002tribution concentrates most of its mass at x \u201c 0, the straight\nline fit learned from the training data over the interval will\nstill be a very good predictor. Sometimes we can rule out\nthis type of covariate shift through prior knowledge. If such\noutliers are extremely rare during training time, we would\nnot expect the test population to have many such patients.\nReweighing will not help much in cases 1(b) and 1(c).\nRobust Learning under Uncertain Test Distributions\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n0\n2\n4\n6\n8\n10\n12\n14\n16\nInput\nOutput\nTrue model\nModel 1\nModel 2\n(a) Large misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\nInput\nOutput\n\u2191\n\u2193\n\u03b5\nTrue model\nBest linear fit\n(b) Small misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nInput\nOutput\nTrue model\n(c) Single point misspecification.\nFigure 1. Three different scenarios of model misspecifications.\nIn this paper, we relate covariate shift to model misspecifi\u0002cation and investigate when reweighing can help a learner\ndeal with covariate shift. We introduce a game between a\nlearner and an adversary that performs robust learning. The\nlearner chooses a model \u03b8 from a set \u0398 to minimize the\nloss, while the adversary chooses a reweighing function \u03b1\nfrom a set A to create new test distributions to maximize the\nloss. There are two major contributions in this paper: First,\nwe provide an improved understanding of the relation be\u0002tween covariate shift and model misspecification through\nthis game analysis. If the learner can find a \u03b8 that min\u0002imizes the loss against any possible \u03b1 that the adversary\ncan play, then it is not necessary to perform reweighing\nagainst covariate shift scenarios represented by A. Sec\u0002ond, we provide a systematic method for checking a model\nclass \u0398 against different covariate shift scenarios, such as\nchanging gender ratio and age distributions in the prognos\u0002tic predictor example, to help user decide whether impor\u0002tance reweighing would be beneficial.\nFor practical use, our method can be used to decide if the\nmodel class is sufficient against shifts that are close to a test\nsample; or robust against a known range of potential shifts\nif test sample is unavailable. If the model class is insuffi\u0002cient, we can consider different ways to deal with covariate\nshifts, such as reweighing using unlabelled test samples, or\nexploring a different model class for the problem.\n2. Related Work\nOur work is inspired by Grunwald & Dawid \u00a8 (2004), who\ninterpret maximum entropy as a game between an adver\u0002sary and a learner on minimizing the worst case expected\nlog loss. Teo et al. (2008) and Globerson & Roweis (2006)\nalso consider an adversarial scenario under changing test\nset conditions, but they are concerned with corruption or\ndeletion of features rather than covariate shift.\nMany results on covariate shift correction involve density\nratio estimation. Shimodaira (2000) showed that, given co\u0002variate shift and model misspecification, reweighing each\ninstance with ptepxq{ptrpxq is asymptotically optimal for\nlog-likelihood estimation, where ptrpxq and ptepxq are as\u0002sumed to be known or estimated in advance. Sugiyama\n& Muller \u00a8 (2005) extended this work by proposing an\n(almost) unbiased estimator for L2 generalization error.\nThere are several works focusing on minimizing differ\u0002ent types of divergence between distributions in the liter\u0002ature (Kanamori et al., 2008; Sugiyama et al., 2008; Ya\u0002mada et al., 2011). Kernel mean matching (KMM) (Huang\net al., 2007) reweighs instances to match means in a\nRKHS (Scholkopf & Smola \u00a8 , 2002). Our work and some\nother approaches (Pan et al., 2009) adapt the idea of match\u0002ing means of the datasets to correct shifted distribution,\nbut we extend their approaches from a two-step optimiza\u0002tion to a game framework that jointly learns a model\nand weights with covariate shift correction. Some other\napproaches (Zadrozny, 2004; Bickel et al.,...",
      "url": "https://proceedings.mlr.press/v32/wen14.pdf"
    },
    {
      "title": "Why Do We Need an Intercept in Regression Models? - Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76485a98d03c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fwhy-do-we-need-an-intercept-in-regression-models-76485a98d03c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fwhy-do-we-need-an-intercept-in-regression-models-76485a98d03c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**The Startup**](https://medium.com/swlh?source=post_page---publication_nav-f5af2b715248-76485a98d03c---------------------------------------)\n\n\u00b7\n\nGet smarter at building your thing. Follow to join The Startup\u2019s +8 million monthly readers & +772K followers.\n\n# Why Do We Need an Intercept in Regression Models?\n\n[Kaushik Jagini](https://medium.com/@kaushikjagini95?source=post_page---byline--76485a98d03c---------------------------------------)\n\n5 min read\n\n\u00b7\n\nOct 28, 2020\n\n--\n\n4\n\nListen\n\nShare\n\nLet\u2019s understand the intuition behind the role of an intercept in regression models.\n\nThe caveat for this article is that you are familiar with the simple equation of a straight line **y = mx + c**.\n\nHowever, to understand how it plays a role in regression problems, the familiarity with regression models such as linear regression is required.\n\nWell, there are way too many articles to help us understand what regression models are and how linear regression\u2019s been traditionally popular. However, if you want to understand the same at a granular level, there are very few. Furthermore, there\u2019s no one stop place to understand what\u2019s the role of an intercept in linear regression. Which is exactly my why for writing this.\n\nIn this post, I will start off with explaining how it plays a role geometrically and then move on to develop the intuition behind the role of an intercept. I will conclude by giving a few examples after which **you will be able to tell if intercept is required or not.**\n\nAlright, let\u2019s dive in.\n\nConsider the equation below:\n\nY = 3x. (A line with no intercept)\n\nFor the above equation, if:\n\nx = 0 then y =0;\n\nx=1 then y=3;\n\nx=2 then y =6;\n\nx=3 then y = 9 and so on\u2026\n\nLet\u2019s plot the above co-ordinates.\n\nThe key observation from the figure above is that the line passes through origin.\n\nIf you do not have the intercept, the line will **ALWAYS** pass through the origin.\n\nOkay, so what happens if the line always pass through origin??\n\nIf I ask you to join point A and B in the figure below, you can simply draw a straight line that can pass through them. ( On one condition \u2014 This line should pass through the origin)\n\nAgain, what If I ask you to draw a line that can pass through C and D in the figure below? ( Same condition \u2014 the line should pass through the origin)\n\nPress enter or click to view image in full size\n\nThere\u2019s no possibility. But, if you do not place that extra condition, then you can simply draw as shown below.\n\nPress enter or click to view image in full size\n\nSo, what did just happen here? If we put an extra condition that the line should pass through the origin, there are certain situations where we won\u2019t be able to draw a line to join two points. But, if we remove that condition, we can.\n\nThat condition is nothing but the intercept.\n\n**Can** pass through origin \u2014 A line **without** intercept.\n\n**Cannot** pass through origin \u2014 A line **with** intercept.\n\n> Having an intercept gives our model the freedom to capture ALL the linear patterns while a model with no intercept can capture only those patterns that pass through origin.\n\nOh, that\u2019s great! Now we understood how intercept plays a role in capturing the patterns.\n\nBut how do we interpret it? What can we infer from the linear model equation that the intercept value is 0.745 or 0.2 or -0.34 etc?\n\nDo we always need to have an intercept?\n\nWell, let\u2019s understand the above two with examples.\n\n**Example \u2014 1:**\n\nConsider you are working for any ed-tech startup. Since there is a lot of marketing campaign going on right now, you want to predict the number of course enrollment based on the marketing campaign. So the dependent variable is **course\\_enrollment** and the independent variable is **marketing**.\n\nThe equation here is,\n\ncourse\\_enrollment = (slope \\* marketing) + intercept. \\[of the form y=mx+c \\]\n\nIf the intercept = 0,\n\nY = (slope\\*marketing)\n\nNow, what if the marketing campaign was stopped i.e marketing = 0, will the course enrollment be totally 0? Nope, there will definitely be a minimum number of enrollments. **This, in other words indicate the trend**. What will be my sales if I stop marketing right now? This question can be answered only if we use intercept. Hence, we definitely need to have intercept in this case. Now you know what it means for an intercept with a value **0.745.** For this example, the intercept value of 0.745 indicate the trend when all other variables are 0.\n\n**Example \u2014 2:**\n\nIn this second example consider you are predicting the number of people who are cured from the coronavirus based on their age. Hence, the equation will be -\n\nCured = (Slope\\*age) + intercept\n\nIn the previous example we went ahead with the condition that intercept = 0 and then proved that it cannot be 0. Here we do vice versa.\n\nThe condition is, the intercept is not equal to 0.\n\nIn such cases, what if the input to the equation is such that the age = 0?\n\nSince age = 0 and intercept is not equal to 0, it does not make sense that with age 0, number of people cured is a non-zero.\n\nHence, we do not require intercept in such cases.\n\nThat\u2019s how crucial is the role of an intercept! I hope you got a better idea of it.\n\n> Note \u2014 If and only if you are confident that the output should be zero if all of the input variables are 0, only then can you get rid of using an intercept. Ideally, it\u2019s preferable to always and always use an intercept in the regression models.\n\n_Finally, feel free to add in any points I might have missed. Happy Learning!_\n\nIf you\u2019ve read until here, thank you. I\u2019m sure you and I have something in common.\n\nLet\u2019s connect on Linkedin:\n\nhttps://www.linkedin.com/in/kaushik-jagini-b76ba2139\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----76485a98d03c---------------------------------------)\n\n[Linear Regression](https://medium.com/tag/linear-regression?source=post_page-----76485a98d03c---------------------------------------)\n\n[Intercepts](https://medium.com/tag/intercepts?source=post_page-----76485a98d03c---------------------------------------)\n\n[Data Science](https://medium.com/tag/data-science?source=post_page-----76485a98d03c---------------------------------------)\n\n[Regression](https://medium.com/tag/regression?source=post_page-----76485a98d03c---------------------------------------)\n\n[**Published in The Startup**](https://medium.com/swlh?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\n[856K followers](https://medium.com/swlh/followers?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\n\u00b7 [Last published\u00a01 day ago](https://medium.com/swlh/i-relied-on-one-income-stream-as-a-creator-until-i-couldnt-3df7ff16aee5?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\nGet smarter at building your thing. Follow to join Th...",
      "url": "https://medium.com/swlh/why-do-we-need-an-intercept-in-regression-models-76485a98d03c"
    },
    {
      "title": "KFold Cross Validation does not fix overfitting - Stack Overflow",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [KFold Cross Validation does not fix overfitting](https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked4 years, 3 months ago\n\nModified [4 years ago](https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting?lastactivity)\n\nViewed\n1k times\n\n2\n\nI am separating the features in `X` and `y` then I preprocess my train test data after splitting it with k fold cross validation. After that i fit the train data to my Random Forest Regressor model and calculate the confidence score. Why do i preprocess after splitting? because people tell me that it's more correct to do it that way and i'm keeping that principle since that for the sake of my model performance.\n\nThis is my first time using KFold Cross Validation because my model score overifts and i thought i could fix it with cross validation. I'm still confused of how to use this, i have read the documentation and some articles but i do not really catch how do i really imply it to my model but i tried anyway and my model still overfits. Using train test split or cross validation resulting my model score is still 0.999, I do not know what is my mistake since i'm very new using this method but i think maybe i did it wrong so it does not fix the overfitting. Please tell me what's wrong with my code and how to fix this\n\n```\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nimport scipy.stats as ss\navo_sales = pd.read_csv('avocados.csv')\n\navo_sales.rename(columns = {'4046':'small PLU sold',\n                            '4225':'large PLU sold',\n                            '4770':'xlarge PLU sold'},\n                 inplace= True)\n\navo_sales.columns = avo_sales.columns.str.replace(' ','')\nx = np.array(avo_sales.drop(['TotalBags','Unnamed:0','year','region','Date'],1))\ny = np.array(avo_sales.TotalBags)\n\n# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(x):\n    X_train, X_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n\nimpC = SimpleImputer(strategy='most_frequent')\nX_train[:,8] = impC.fit_transform(X_train[:,8].reshape(-1,1)).ravel()\nX_test[:,8] = impC.transform(X_test[:,8].reshape(-1,1)).ravel()\n\nimp = SimpleImputer(strategy='median')\nX_train[:,1:8] = imp.fit_transform(X_train[:,1:8])\nX_test[:,1:8] = imp.transform(X_test[:,1:8])\n\nle = LabelEncoder()\nX_train[:,8] = le.fit_transform(X_train[:,8])\nX_test[:,8] = le.transform(X_test[:,8])\n\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\nconfidence = rfr.score(X_test, y_test)\nprint(confidence)\n\n```\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [python-3.x](https://stackoverflow.com/questions/tagged/python-3.x)\n- [machine-learning](https://stackoverflow.com/questions/tagged/machine-learning)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation)\n\n[Share](https://stackoverflow.com/q/60684943)\n\n[Improve this question](https://stackoverflow.com/posts/60684943/edit)\n\nFollow\n\n[edited May 20, 2020 at 2:19](https://stackoverflow.com/posts/60684943/revisions)\n\n[![Nicolas Gervais's user avatar](https://i.sstatic.net/nTL9N.jpg?s=64)](https://stackoverflow.com/users/10908375/nicolas-gervais)\n\n[Nicolas Gervais](https://stackoverflow.com/users/10908375/nicolas-gervais)\n\n35.9k1919 gold badges118118 silver badges156156 bronze badges\n\nasked Mar 14, 2020 at 16:32\n\n[![random student's user avatar](https://i.sstatic.net/GvDYI.jpg?s=64)](https://stackoverflow.com/users/10170808/random-student)\n\n[random student](https://stackoverflow.com/users/10170808/random-student) random student\n\n72511 gold badge1717 silver badges3535 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n8\n\nThe reason you're overfitting is because a non-regularized tree-based model will adjust to the data until all training samples are correctly classified. See for example this image:\n\n[![enter image description here](https://i.sstatic.net/iBIA9.png)](https://i.sstatic.net/iBIA9.png)\n\nAs you can see, this does not generalize well. If you don't specify arguments that regularize the trees, the model will fit the test data poorly because it will basically just learn the noise in the training data. There are many ways to regularize trees in `sklearn`, you can find them [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). For instance:\n\n- max\\_features\n- min\\_samples\\_leaf\n- max\\_depth\n\nWith proper regularization, you can get a model that generalizes well to the test data. Look at a regularized model for instance:\n\n[![enter image description here](https://i.sstatic.net/LiGNz.png)](https://i.sstatic.net/LiGNz.png)\n\nTo regularize your model, instantiate the `RandomForestRegressor()` module like this:\n\n```\nrfr = RandomForestRegressor(max_features=0.5, min_samples_leaf=4, max_depth=6)\n\n```\n\nThese argument values are arbitrary, it's up to you to find the ones that fit your data best. You can use domain-specific knowledge to choose these values, or a hyperparameter tuning search like `GridSearchCV` or `RandomizedSearchCV`.\n\nOther than that, imputing the mean and median might bring a lot of noise in your data. I would advise against it unless you had no other choice.\n\n[Share](https://stackoverflow.com/a/60685108)\n\n[Improve this answer](https://stackoverflow.com/posts/60685108/edit)\n\nFollow\n\n[edited Mar 22, 2020 at 12:01](https://stackoverflow.com/posts/60685108/revisions)\n\nanswered Mar 14, 2020 at 16:52\n\n[![Nicolas Gervais's user avatar](https://i.sstatic.net/nTL9N.jpg?s=64)](https://stackoverflow.com/users/10908375/nicolas-gervais)\n\n[Nicolas Gervais](https://stackoverflow.com/users/10908375/nicolas-gervais) Nicolas Gervais\n\n35.9k1919 gold badges118118 silver badges156156 bronze badges\n\n1\n\n- Reducing overfitting is one of the main difficulties in machine learning. There is no easy way to find the right values. Often, the default values are good, sometimes it isn't the case. Something you can do is use randomizedsearchcv or gridsearchcv to explore many combinations of values. Great to know I solved your issue!\n\n\u2013\u00a0[Nicolas Gervais](https://stackoverflow.com/users/10908375/nicolas-gervais)\n\nCommentedMar 15, 2020 at 12:20\n\n\n[Add a comment](https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting)\u00a0\\|\n\n7\n\nWhile @NicolasGervais answer gets to the bottom of why your specific model is overfitting, I think there is a co...",
      "url": "https://stackoverflow.com/questions/60684943/kfold-cross-validation-does-not-fix-overfitting"
    },
    {
      "title": "3.1. Cross-validation: evaluating estimator performance - Scikit-learn",
      "text": "3.1. Cross-validation: evaluating estimator performance &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)](../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 3.1.Cross-validation: evaluating estimator performance[#](#cross-validation-evaluating-estimator-performance)\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called**overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a**test set**`X\\_test,y\\_test`.\nNote that the word \u201cexperiment\u201d is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by[grid search](grid_search.html#grid-search)techniques.\n[![Grid Search Workflow](../_images/grid_search_workflow.png)](../_images/grid_search_workflow.png)\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the[`train\\_test\\_split`](generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)helper function.\nLet\u2019s load the iris data set to fit a linear support vector machine on it:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;fromsklearnimportdatasets&gt;&gt;&gt;fromsklearnimportsvm&gt;&gt;&gt;X,y=datasets.load\\_iris(return\\_X\\_y=True)&gt;&gt;&gt;X.shape,y.shape((150, 4), (150,))\n```\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n```\n&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,test\\_size=0.4,random\\_state=0)&gt;&gt;&gt;X\\_train.shape,y\\_train.shape((90, 4), (90,))&gt;&gt;&gt;X\\_test.shape,y\\_test.shape((60, 4), (60,))&gt;&gt;&gt;clf=svm.SVC(kernel=&#39;linear&#39;,C=1).fit(X\\_train,y\\_train)&gt;&gt;&gt;clf.score(X\\_test,y\\_test)0.96\n```\nWhen evaluating different settings (\u201chyperparameters\u201d) for estimators,\nsuch as the`C`setting that must be manually set for an SVM,\nthere is still a risk of overfitting*on the test set*because the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can \u201cleak\u201d into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called \u201cvalidation set\u201d: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\nA solution to this problem is a procedure called[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called*k*-fold CV,\nthe training set is split into*k*smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the*k*\u201cfolds\u201d:\n* A model is trained using\\\\(k-1\\\\)of the folds as training data;\n* the resulting model is validated on the remaining part of the data\n(i.e., it is used as a test set to compute a performance measure\nsuch as accuracy).\nThe performance measure reported by*k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n[![A depiction of a 5 fold cross validation on a training set, while holding out a test set.](../_images/grid_search_cross_validation.png)](../_images/grid_search_cross_validation.png)\n## 3.1.1.Computing cross-validated metrics[#](#computing-cross-validated-metrics)\nThe simplest way to use cross-validation is to call the[`cross\\_val\\_score`](generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)helper function on the estimator and the dataset.\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset by splitting the data, fitting\na model and computing the score 5 consecutive times (with different splits each\ntime):\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimportcross\\_val\\_score&gt;&gt;&gt;clf=svm.SVC(kernel=&#39;linear&#39;,C=1,random\\_state=42)&gt;&gt;&gt;scores=cross\\_val\\_score(clf,X,y,cv=5)&gt;&gt;&gt;scoresarray([0.96, 1. , 0.96, 0.96, 1. ])\n```\nThe mean score and the standard deviation are hence given by:\n```\n&gt;&gt;&gt;print(&quot;%0.2faccuracy with a standard deviation of%0.2f&quot;%(scores.mean(),scores.std()))0.98 accuracy with a standard deviation of 0.02\n```\nBy default, the score computed at each CV iteration is the`score`method of the estimator. It is possible to change this by using the\nscoring parameter:\n```\n&gt;&gt;&gt;fromsklearnimportmetrics&gt;&gt;&gt;scores=cross\\_val\\_score(...clf,X,y,cv=5,scoring=&#39;&#39;f1\\_macro&#39;&#39;)&gt;&gt;&gt;scoresarray([0.96, 1., 0.96, 0.96, 1.])\n```\nSee[The scoring parameter: defining model evaluation rules](model_evaluation.html#scoring-parameter)for details.\nIn the case of the Iris dataset, the samples are balanced across target\nclasses hence the accuracy and the F1-score are almost equal.\nWhen the`cv`argument is an integer,[`cross\\_val\\_score`](generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)uses the[`KFold`](generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)or[`StratifiedKFold`](generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)strategies by default, the latter\nbeing used if the estimator derives from[`ClassifierMixin`](generated/sklearn.base.ClassifierMixin.html#sklearn.base.ClassifierMixin).\nIt is also possible to use other cross validation strategies by passing a cross\nvalidation iterator instead, for instance:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimportShuffleSplit&gt;&gt;&gt;n\\_samples=X.shape[0]&gt;&gt;&gt;cv=ShuffleSplit(n\\_splits=5,test\\_size=0.3,random\\_state=0)&gt;&gt;&gt;cross\\_val\\_score(clf,X,y,cv=cv)array([0.977, 0.977, 1., 0.955, 1.])\n```\nAnother option is to use an iterable yielding (train, test) splits as arrays of\nindices, for example:\n```\n&gt;&gt;&gt;defcustom\\_cv\\_2folds(X):...n=X.shape[0]...i=1...whilei&lt;=2:...idx=np.arange(n\\*(i-1)/2,n\\*i/2,dtype=int)...yieldidx,idx...i+=1...&gt;&gt;&gt;custom\\_cv=custom\\_cv\\_2folds(X)&gt;&gt;&gt;cross\\_val\\_score(clf,X,y,cv=custom\\_cv)array([1. , 0.973])\n```\nData transformation with held-out data[#](#data-transformation-with-held-out-data)\nJust as it is important to test a predictor on data held-out from\ntraining, preprocessing (such as standardization, feature selection, etc.)\nand similar[data transformations](../data_transforms.html#data-transforms)similarly should\nbe learnt from a training set and applied to held-out da...",
      "url": "https://scikit-learn.org/stable/modules/cross_validation.html"
    },
    {
      "title": "Cross-validation (statistics) - Wikipedia",
      "text": "Cross-validation (statistics) - Wikipedia\n[Jump to content](#bodyContent)\n[![](https://en.wikipedia.org/static/images/icons/wikipedia.png)![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg)![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg)](https://en.wikipedia.org/wiki/Main_Page)\n[Search](https://en.wikipedia.org/wiki/Special:Search)\nSearch\n# Cross-validation (statistics)\n24 languages\n* [\u0627\u0644\u0639\u0631\u0628\u064a\u0629](https://ar.wikipedia.org/wiki/\u062a\u062d\u0642\u0642_\u0645\u062a\u0642\u0627\u0637\u0639)\n* [Catal\u00e0](https://ca.wikipedia.org/wiki/Validaci\u00f3_encreuada)\n* [\u010ce\u0161tina](https://cs.wikipedia.org/wiki/K\u0159\u00ed\u017eov\u00e1_validace)\n* [Deutsch](https://de.wikipedia.org/wiki/Kreuzvalidierungsverfahren)\n* [Eesti](https://et.wikipedia.org/wiki/Ristvalideerimine)\n* [Espa\u00f1ol](https://es.wikipedia.org/wiki/Validaci\u00f3n_cruzada)\n* [Euskara](https://eu.wikipedia.org/wiki/Balidazio_gurutzatu)\n* [\u0641\u0627\u0631\u0633\u06cc](https://fa.wikipedia.org/wiki/\u0631\u0648\u0634_\u0627\u0639\u062a\u0628\u0627\u0631\u0633\u0646\u062c\u06cc_\u0645\u062a\u0642\u0627\u0628\u0644)\n* [Fran\u00e7ais](https://fr.wikipedia.org/wiki/Validation_crois\u00e9e)\n* [\ud55c\uad6d\uc5b4](https://ko.wikipedia.org/wiki/\uad50\ucc28\ud0c0\ub2f9\ub3c4)\n* [Bahasa Indonesia](https://id.wikipedia.org/wiki/Validasi_silang)\n* [Italiano](https://it.wikipedia.org/wiki/Convalida_incrociata)\n* [\u041c\u043e\u043d\u0433\u043e\u043b](https://mn.wikipedia.org/wiki/\u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0435\u043d\u0442\u0447\u0438\u043b\u0430\u043b_(\u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a))\n* [\u65e5\u672c\u8a9e](https://ja.wikipedia.org/wiki/\u4ea4\u5dee\u691c\u8a3c)\n* [Polski](https://pl.wikipedia.org/wiki/Sprawdzian_krzy\u017cowy)\n* [Portugu\u00eas](https://pt.wikipedia.org/wiki/Valida\u00e7\u00e3o_cruzada)\n* [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://ru.wikipedia.org/wiki/\u041f\u0435\u0440\u0435\u043a\u0440\u0451\u0441\u0442\u043d\u0430\u044f_\u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430)\n* [Sunda](https://su.wikipedia.org/wiki/Validasi-silang)\n* [Svenska](https://sv.wikipedia.org/wiki/Korsvalidering)\n* [T\u00fcrk\u00e7e](https://tr.wikipedia.org/wiki/\u00c7apraz_do\u011frulama_(istatistik))\n* [\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430](https://uk.wikipedia.org/wiki/\u041f\u0435\u0440\u0435\u0445\u0440\u0435\u0441\u043d\u0435_\u0437\u0430\u0442\u0432\u0435\u0440\u0434\u0436\u0443\u0432\u0430\u043d\u043d\u044f)\n* [Ti\u1ebfng Vi\u1ec7t](https://vi.wikipedia.org/wiki/Ki\u1ec3m_ch\u1ee9ng_ch\u00e9o)\n* [\u7cb5\u8a9e](https://zh-yue.wikipedia.org/wiki/\u4ea4\u53c9\u9a57\u8b49)\n* [\u4e2d\u6587](https://zh.wikipedia.org/wiki/\u4ea4\u53c9\u9a57\u8b49)\n[Edit links](https://www.wikidata.org/wiki/Special:EntityPage/Q541014#sitelinks-wikipedia)\nFrom Wikipedia, the free encyclopedia\nStatistical model validation technique\n[![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Confusion_matrix.png/250px-Confusion_matrix.png)](https://en.wikipedia.org/wiki/File:Confusion_matrix.png)Comparing the cross-validation accuracy and percent of false negative (overestimation) of five classification models. Size of bubbles represent the standard deviation of cross-validation accuracy (tenfold).[&#91;1&#93;](#cite_note-:1-1)[![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/250px-K-fold_cross_validation_EN.svg.png)](https://en.wikipedia.org/wiki/File:K-fold_cross_validation_EN.svg)Diagram of k-fold cross-validation\n**Cross-validation**,[&#91;2&#93;](#cite_note-2)[&#91;3&#93;](#cite_note-3)[&#91;4&#93;](#cite_note-4)sometimes called**rotation estimation**[&#91;5&#93;](#cite_note-5)[&#91;6&#93;](#cite_note-Kohavi95-6)[&#91;7&#93;](#cite_note-Devijver82-7)or**out-of-sample testing**, is any of various similar[model validation](https://en.wikipedia.org/wiki/Model_validation)techniques for assessing how the results of a[statistical](https://en.wikipedia.org/wiki/Statistics)analysis will[generalize](https://en.wikipedia.org/wiki/Generalization_error)to an independent data set.\nCross-validation includes[resampling](https://en.wikipedia.org/wiki/Resampling_(statistics))and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how[accurately](https://en.wikipedia.org/wiki/Accuracy)a[predictive model](https://en.wikipedia.org/wiki/Predictive_modelling)will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters.\nIn a prediction problem, a model is usually given a dataset of*known data*on which training is run (*training dataset*), and a dataset of*unknown data*(or*first seen*data) against which the model is tested (called the[validation dataset](https://en.wikipedia.org/wiki/Validation_set)or*testing set*).[&#91;8&#93;](#cite_note-8)[&#91;9&#93;](#cite_note-Newbie_question:_Confused_about_train,_validation_and_test_data!-9)The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like[overfitting](https://en.wikipedia.org/wiki/Overfitting)or[selection bias](https://en.wikipedia.org/wiki/Selection_bias)[&#91;10&#93;](#cite_note-10)and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\nOne round of cross-validation involves[partitioning](https://en.wikipedia.org/wiki/Partition_of_a_set)a[sample](https://en.wikipedia.org/wiki/Statistical_sample)of[data](https://en.wikipedia.org/wiki/Data)into[complementary](https://en.wikipedia.org/wiki/Complement_(set_theory))subsets, performing the analysis on one subset (called the*training set*), and validating the analysis on the other subset (called the*validation set*or*testing set*). To reduce[variability](https://en.wikipedia.org/wiki/Variance), in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\nIn summary, cross-validation combines (averages) measures of[fitness](https://en.wikipedia.org/wiki/Goodness_of_fit)in prediction to derive a more accurate estimate of model prediction performance.[&#91;11&#93;](#cite_note-11)\n## Motivation\n[[edit](https://en.wikipedia.org/w/index.php?title=Cross-validation_(statistics)&amp;action=edit&amp;section=1)]\nAssume a[model](https://en.wikipedia.org/wiki/Statistical_model)with one or more unknown[parameters](https://en.wikipedia.org/wiki/Parameters), and a data set to which the model can be fit (the training data set). The fitting process[optimizes](https://en.wikipedia.org/wiki/Optimization_(mathematics))the model parameters to make the model fit the training data as well as possible. If an[independent](https://en.wikipedia.org/wiki/Independence_(probability_theory))sample of validation data is taken from the same[population](https://en.wikipedia.org/wiki/Statistical_population)as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.&#91;*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*&#93;\n### Example: linear regression\n[[edit](https://en.wikipedia.org/w/index.php?title=Cross-validation_(statistics)&amp;action=edit&amp;section=2)]\nIn linear regression, there exist[real](https://en.wikipedia.org/wiki/Real_number)*response values*y1,&#x2026;,yn{\\\\textstyle y\\_{1},\\\\ldots ,y\\_{n}}![{\\textstyle y_{1},\\ldots ,y_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/db507a4d2fe2545e374f0e79740aee0a51053616), and*n**p*-dimensional[vector](https://en.wikipedia.org/wiki/Euclidean_vector)*covariates****x***1, ...,***x****n*. The components of the vector***x****i*are denoted*x**i*1, ...,*x**ip*. If[least squares](https://en.wikipedia.org/wiki/Least_squares)is used to fit a function in the form of a[hyperplane](https://en.wikipedia.org/wiki/Hyperplane)***\u0177***=*a*+***\u03b2***T***x***to the data (***x****i*,*y**i*)&#160;1&#160;\u2264&#160;*i*&#160;\u2264&#160;*n*, then the fit can be assessed using the[mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)(MSE). The MSE for given estimated parameter values*a*and***\u03b2***on the training set (***x****i*,*y**i*)&#160;1&#160;\u2264&#160;*i*&#160;\u2264&#160;*n*is defined as:\nMSE=1n&#x2211;i=1n(y...",
      "url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
    }
  ]
}