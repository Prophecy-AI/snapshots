{
  "query": "How can I reduce the gap between cross-validation and leaderboard score in Kaggle competitions when there is distribution shift?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent research on domain\u2011adaptation validation\u202f([arXiv\u202f2309.03879](https://arxiv.org/abs/2309.03879)) and practical Kaggle over\u2011fitting guides\u202f([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)) together with best\u2011practice validation schemes\u202f([polakowo.io](https://polakowo.io/datadocs/docs/machine-learning/validation-schemes))\u202fthe gap caused by distribution shift can be narrowed with a few focused steps:\n\n1. **Diagnose the shift** \u2013 compare feature distributions between the original training set and the hidden test set using *adversarial validation* (train a classifier to distinguish the two halves and inspect high\u2011importance features). This reveals which variables drive the shift and guides split design\u202f([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).\n\n2. **Build a validation split that mirrors the test distribution** \u2013 instead of random hold\u2011out, split the public training data by the same criterion the organizers likely used (time, groups, or the adversarial\u2011validation score). For example, use the top\u2011scoring adversarial\u2011validation samples as a \u201cpseudo\u2011test\u201d set\u202f([polakowo.io](https://polakowo.io/datadocs/docs/machine-learning/validation-schemes)).\n\n3. **Apply robust cross\u2011validation** \u2013 run k\u2011fold CV multiple times with different random seeds, averaging the scores. Ensure each fold respects the chosen split strategy (e.g., time\u2011aware or group\u2011aware folds) so every fold experiences the same distribution shift\u202f([polakowo.io](https://polakowo.io/datadocs/docs/machine-learning/validation-schemes)).\n\n4. **Use domain\u2011adaptation or test\u2011time adaptation techniques** \u2013 after selecting a base model, fine\u2011tune it on the unlabeled target (pseudo\u2011test) data using unsupervised or source\u2011free adaptation methods. The paper on better DA practices shows that proper validation metrics for these methods improve realistic performance under shift\u202f([arXiv\u202f2309.03879](https://arxiv.org/abs/2309.03879)).\n\n5. **Regularize and early\u2011stop** \u2013 combine L2/Dropout regularization, early stopping on the shift\u2011aware validation set, and modest ensembling (e.g., bagging of top\u2011performing seeds). These reduce over\u2011fitting to the original training distribution and keep the model stable on the shifted test data\u202f([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).\n\n6. **Iterate with pseudo\u2011labeling** \u2013 generate predictions on the pseudo\u2011test set, keep only high\u2011confidence ones, and retrain (or fine\u2011tune) the model. This leverages target\u2011domain information without leaking true labels, further aligning CV and leaderboard performance\u202f([arXiv\u202f2309.03879](https://arxiv.org/abs/2309.03879)).\n\n7. **Monitor the public\u2011vs\u2011private leaderboard gap** \u2013 treat the public leaderboard as an additional validation checkpoint; if the gap widens, revisit steps\u202f1\u20114 to adjust the validation split or adaptation method. Consistent alignment across folds and the public score signals a reduced gap for the final private leaderboard.",
      "url": ""
    },
    {
      "title": "Better Practices for Domain Adaptation",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2309.03879** (cs)\n\n\\[Submitted on 7 Sep 2023\\]\n\n# Title:Better Practices for Domain Adaptation\n\nAuthors: [Linus Ericsson](https://arxiv.org/search/cs?searchtype=author&query=Ericsson,+L), [Da Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+D), [Timothy M. Hospedales](https://arxiv.org/search/cs?searchtype=author&query=Hospedales,+T+M)\n\nView a PDF of the paper titled Better Practices for Domain Adaptation, by Linus Ericsson and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2309.03879)\n\n> Abstract:Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology including Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and Test Time Adaptation (TTA). While the results show that realistically achievable performance is often worse than expected, they also show that using proper validation splits is beneficial, as well as showing that some previously unexplored validation metrics provide the best options to date. Altogether, our improved practices covering data, training, validation and hyperparameter optimisation form a new rigorous pipeline to improve benchmarking, and hence research progress, within this important field going forward.\n\n|     |     |\n| --- | --- |\n| Comments: | AutoML 2023 (Best paper award) |\n| Subjects: | Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2309.03879](https://arxiv.org/abs/2309.03879) \\[cs.LG\\] |\n|  | (or [arXiv:2309.03879v1](https://arxiv.org/abs/2309.03879v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2309.03879](https://doi.org/10.48550/arXiv.2309.03879)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Linus Ericsson \\[ [view email](https://arxiv.org/show-email/8d84eb61/2309.03879)\\]\n\n**\\[v1\\]**\nThu, 7 Sep 2023 17:44:18 UTC (252 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Better Practices for Domain Adaptation, by Linus Ericsson and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2309.03879)\n- [TeX Source](https://arxiv.org/src/2309.03879)\n- [Other Formats](https://arxiv.org/format/2309.03879)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2309.03879&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2309.03879&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-09](https://arxiv.org/list/cs.LG/2023-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2309.03879?context=cs)\n\n[cs.CV](https://arxiv.org/abs/2309.03879?context=cs.CV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2309.03879)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2309.03879)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2309.03879)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2309.03879&description=Better Practices for Domain Adaptation) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2309.03879&title=Better Practices for Domain Adaptation)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2309.03879) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2309.03879"
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "Validation Schemes",
      "text": "Validation Schemes \u00b7datadocs\n[## datadocs\n](https://polakowo.io/datadocs/)\n[Edit](https://github.com/polakowo/datadocs/edit/master/docs/machine-learning/validation-schemes.md)# Validation Schemes\n* Never use data you train on to measure the quality of your model (resubstitution).\n* As the model becomes more computational, the variance (dispersion) of the estimate increases, but bias decreases.\n* Overfitting arises due to the fact that the training set is memorized completely instead of generalizing.\n* This effect is only visible when the same estimator is run on similar data other than training.\n* Hold out part of the available data as a test set.\n* There is still a risk of overfitting because the parameters can be tweaked until the estimator performs optimally.\n* Only the final evaluation should be done on the test set.\n* Set up validation to mimic train/test split.\n* In a competition, you need to identify the train/test split made by organizers.\n* In most cases, data is split by rows, time, groups or combined.\n* Logic of feature engineering depends on the data splitting strategy.\n![](https://polakowo.io/datadocs/assets/grid_search_workflow.png)[Credit](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n#### [](#validation-stage)Validation stage\n* We can observe that:\n* High deviation of the local CV scores.\n* Possible causes:\n* Too little data.\n* Data is too diverse and inconsistent (e.g., December and January for store sales).\n* Extensive validation techniques:\n* Perform k-fold split multiple times with different seeds, and average the scores.\n* Tune a model on one split, evaluate the model on the other.\n* Following problems can be identified before the submission stage:\n* Different scores/optimal parameters between folds.\n* Public leaderboard score will be unreliable because of too little data.\n* Train and test data are from different distributions (using EDA).#### [](#submission-stage)Submission stage\n* We can observe that:\n* LB score is consistently lower/higher than the local score:\n* LB score is not correlated with the local score.\n* Possible causes:\n* We may already have a high variance of CV scores: calculate mean and std of CV scores and estimate if LB is expected.\n* Too little data in public leaderboard: trust your local validation.\n* Train and test are from different distributions: adjust distributions or perform leaderboard probing.\n* Overfitting\n* Incorrect cross-validation strategy\n* Expect LB shuffle because of:\n* Randomness can shuffle scores on the private leaderboard.\n* Little amount of training or/and testing data.\n* Different public/private data or target distributions (e.g., time-based split).\n* [How to Select Your Final Models in a Kaggle Competition](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)## [](#base-models)Base models\n* The following validation schemes are supposed to be used to estimate quality of the model.\n* For getting test predictions don't forget to retrain your model using all training data.#### [](#holdout)Holdout\n* Procedure (`train\\_test\\_split`):\n* Split*train*into two parts:*trainA*and*trainB*(usually 80/20).\n* Fit the model on*trainA*and validate it on*trainB*.\n* Use holdout if scores on each fold are roughly the same.#### [](#k-fold)K-Fold\n* Procedure (`KFold`):\n* Split*train*into \\\\(K\\\\) folds.\n* Iterate though each fold:\n* Refit the model on all folds except the current one.\n* Validate the model on the current fold.\n* Assumes that the samples are i.i.d.\n* The performance measure reported by k-fold CV is the average of the values computed in the loop.\n* Scores deviation in KFold can help to select statistically significant change in scores while tuning a model.\n* The value of \\\\(K\\\\) being large could lead to low bias and high variance (overfitting).\n* The advantage is that entire data is used for training and validation.\n* For classification problems that exhibit a large imbalance (`StratifiedKFold`):\n* Stratified k-fold ensures that relative class frequencies are preserved in each train and validation fold.\n* In this case we would like to know if a model generalizes well to the unseen groups (`GroupKFold`):\n* Group k-fold ensures that the same group is not represented in both testing and training sets.#### [](#loo)LOO\n* LOO is a k-fold scheme where \\\\(K=N\\\\) (`LeaveOneOut`).\n* LOO often results in high variance as an estimator for the test error.\n* 5-10 fold cross validation should be preferred to LOO.\n* Mostly used for sparse datasets.#### [](#time-based-validation)Time-based validation\n* Procedure (`TimeSeriesSplit`):\n* Split*train*into chunks of duration \\\\(T\\\\). Select first \\\\(M\\\\) chunks.\n* Fit a model on these \\\\(M\\\\) chunks and predict for the chunk \\\\(M+1\\\\).\n* Then repeat this procedure for the next chunk and so on (imagine a moving window).\n* Used if the samples have been generated using a time-dependent process.\n* Time series data is characterised by the correlation between observations (autocorrelation).\n* Does not assume that the samples are i.i.d.## [](#meta-models)Meta models\n#### [](#simple-holdout-scheme)Simple holdout scheme\n* Procedure:\n* Split*train*into three parts:*trainA*,*trainB*and*trainC*.\n* Fit \\\\(N\\\\) diverse models on*trainA*.\n* Predict for*trainB*,*trainC*, and*test*(getting meta-features*trainB\\_meta*,*trainC\\_meta*and*test\\_meta*).\n* Fit a meta-model on*trainB\\_meta*and validate it on*trainC\\_meta*.\n* When the meta-model is validated, fit it to*[trainB\\_meta, trainC\\_meta]*and predict for*test\\_meta*.\n* This scheme is usually preferred over the other schemes if dataset is large.\n* Fair validation scheme (validation set of meta-models not used in any way by base models)#### [](#meta-holdout-scheme-with-oof-meta-features)Meta holdout scheme with OOF meta-features\n* Procedure:\n* Split*train*into \\\\(K\\\\) folds.\n* Iterate though each fold:\n* Refit \\\\(N\\\\) diverse models on all folds except the current one.\n* Predict for the current fold.\n* For each object in*train*, we now have \\\\(N\\\\) meta-features (out-of-fold predictions, OOF) (getting*train\\_meta*)\n* Fit the models on*train*and predict for*test*(getting*test\\_meta*)\n* Split*train\\_meta*into two parts:*train\\_metaA*and*train\\_metaB*.\n* Fit a meta-model on*train\\_metaA*and validate it on*train\\_metaB*.\n* When the meta-model is validated, fit it to*train\\_meta*and predict for*test\\_meta*.#### [](#meta-kfold-scheme-with-oof-meta-features)Meta KFold scheme with OOF meta-features\n* Procedure:\n* Obtain OOF predictions for*train\\_meta*and*test\\_meta*.\n* Use KFold scheme on*train\\_meta*to validate the meta-model (with same seed as for OOF).\n* When the meta-model is validated, fit it to*train\\_meta*and predict for*test\\_meta*.#### [](#holdout-scheme-with-oof-meta-features)Holdout scheme with OOF meta-features\n* Procedure:\n* Split*train*into two parts:*trainA*and*trainB*.\n* Fit models to*trainA*and predict for*trainB*(getting*trainB\\_meta*).\n* Obtain OOF predictions for*trainA\\_meta*.\n* Fit a meta-model to*trainA\\_meta*and validate on*trainB\\_meta*.\n* Obtain OOF predictions for*train\\_meta*and*test\\_meta*.\n* Fit the meta-model to*train\\_meta*and predict for*test\\_meta*.\n* Fair validation scheme.#### [](#kfold-scheme-with-oof-meta-features)KFold scheme with OOF meta-features\n* The same as holdout scheme with OOF meta-features but with \\\\(K\\\\) folds instead of*trainA*and*trainB*.\n* This scheme gives the validation score with the least variance compared to the other schemes.\n* But it is also the least efficient one from the computational perspective.\n* Fair validation scheme.#### [](#kfold-scheme-in-time-series)KFold scheme in time series\n* Procedure:\n* Obtain OOF meta-features using time-series split starting with \\\\(M\\\\) chunks.\n* Now we have meta-features for the chunks starting from \\\\(M+1\\\\) (getting*train\\_meta*).\n* Fit the models on*train*and predict for*test*(getting*test\\_meta*).\n* Perform time-series aware cross validation on meta-features.\n* Fit the meta-model t...",
      "url": "https://polakowo.io/datadocs/docs/machine-learning/validation-schemes"
    },
    {
      "title": "How to Use Kaggle",
      "text": "Getting Started on Kaggle | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n# How to Use Kaggle\nkeyboard\\_arrow\\_right[Competitions](https://www.kaggle.com/docs/competitions)\nkeyboard\\_arrow\\_right[Datasets](https://www.kaggle.com/docs/datasets)\nkeyboard\\_arrow\\_right[Public API](https://www.kaggle.com/docs/api)\n[Efficient GPU Usage Tips](https://www.kaggle.com/docs/efficient-gpu-usage)\nkeyboard\\_arrow\\_right[Tensor Processing Units (TPUs)](https://www.kaggle.com/docs/tpu)\nkeyboard\\_arrow\\_right[Models](https://www.kaggle.com/docs/models)\nkeyboard\\_arrow\\_down\nCompetitions Setup\n[Overview](#overview)\nkeyboard\\_arrow\\_right\n[How Kaggle competitions work](#how-kaggle-competitions-work)\nkeyboard\\_arrow\\_right\n[Create your competition\ufe0f](#create-your-competition)\nkeyboard\\_arrow\\_right\n[Prepare the dataset](#prepare-the-dataset)\n[Set up scoring](#set-up-scoring)\n[Creating a New Metric](#creating-a-new-metric)\nkeyboard\\_arrow\\_right\n[Test your competition](#test-your-competition)\nkeyboard\\_arrow\\_right\n[Finalize your settings and descriptions](#finalize-your-settings-and-descriptions)\n[Launch and invite participants](#launch-and-invite-participants)\nkeyboard\\_arrow\\_right\n[FAQs](#faqs)\nkeyboard\\_arrow\\_right[Organizations](https://www.kaggle.com/docs/organizations)\nkeyboard\\_arrow\\_right[Groups](https://www.kaggle.com/docs/groups)\nkeyboard\\_arrow\\_right[Kaggle Packages](https://www.kaggle.com/docs/packages)\nkeyboard\\_arrow\\_right[Notebooks](https://www.kaggle.com/docs/notebooks)\nkeyboard\\_arrow\\_right[MCP Server](https://www.kaggle.com/docs/mcp)\nkeyboard\\_arrow\\_right[Benchmarks](https://www.kaggle.com/docs/benchmarks)\n## Competitions Setup\nCreate a new competition or competition metric\n### Overview\nAnybody can launch a machine learning competition using Kaggle's Community Competitions platform, including educators, researchers, companies, meetup groups, hackathon hosts, or inquisitive individuals! In this guide, you will learn how to set up your own competition, step-by-step.\nBefore diving in, it's helpful to understand how a Kaggle competition works.\n### How Kaggle competitions work\n#### Overview\nEvery competition has two things, a) a clearly defined problem that participants need to solve using a machine learning model and b) a dataset that\u2019s used both for training and evaluating the effectiveness of these models.\nFor example, in the[Store Sales \u2013Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)competition, participants must accurately predict how many of each grocery item will sell using a dataset of past product and sales information from a grocery retailer.\nOnce the competition starts participants can submit their predictions, Kaggle will score them for accuracy, and the team will be placed on a ranked leaderboard. The team at the top of the leaderboard at the deadline wins!\n#### Datasets, Submissions &amp; Leaderboards\nEvery competition\u2019s dataset is split into two smaller datasets.\nOne of these smaller datasets will be given to participants to train their models, typically named`train.csv`.\nThe other dataset will be mostly hidden from participants and used by Kaggle for testing and scoring, named`test.csv`and`solution.csv`(`test.csv`is the same as`solution.csv`except that`test.csv`contains the feature values and`solution.csv`contains the ground truth variable(s) \u2013participants will never, ever see`solution.csv`).\nWhen a participant feels ready to make a submission to the competition, they will use`test.csv`to generate a prediction and upload a CSV file. Kaggle will automatically score the submission for accuracy using the hidden`solution.csv`file.\nMost competitions have a maximum number of submissions that a participant can make each day and a final deadline at which point the leaderboard will be frozen.\nIt\u2019s conceivable that a participant could use the mechanics of a Kaggle competition to overfit a solution - which would be great for winning a competition, but not valuable for a real-world application.\nTo help prevent this, Kaggle has two leaderboards \u2013the public and private leaderboard. The competition host splits the`solution.csv`dataset into two parts, using one part for the public leaderboard and another part for the private leaderboard. Participants generally will now know which samples are public vs private. The private leaderboard is kept a secret until after the competition deadline and is used as the official leaderboard for determining the final ranking.\n### Create your competition\ufe0f\nTo create a new competition, click on the \u201cCreate new competition\u201d button at the top of the Kaggle Community landing page.\nThen, enter a descriptive title, subtitle and URL for your competition. Be as descriptive and to the point as possible. In our example above, the title \u201cStore Sales - Time Series Forecasting\u201d quickly outlines the type of data, the industry of the dataset, and the type of problem to be solved.\nIf you want to create a competition with more privacy, you can limit your competition's visibility and restrict who can join on this page.\nVisibility: Competitions with their visibility set to public are viewable on Kaggle and appear in Kaggle search results. Competitions with visibility set to private are hidden and only accessible via invitation URLs from the host.\nWho Can Join: Competitions access can be set to three levels: anyone, only people with a link and restricted email list. If you select anyone, all Kagglers can join your competition. Selecting only people with a link, will restrict access to those users you provide a special URL. Finally, restricted email list is the most private competition. Only Kagglers with accounts that match the emails or email domains you specify will be able to join. Note: if you select restricted email list, notebooks will be turned off. This provides a way to ensure that any private data that you have in a competition is not accidentally leaked through shared notebooks. You can choose to re-enable notebooks if you choose.\nReview and accept our terms of service, then click \u201cCreate Competition\u201d.\nYour competition listing is now in draft mode. You can take your time to prepare the details before making the competition public.\n#### Offering Prizes\nCommunity competition hosts have the option to offer prizes with a total value of up to $10,000 USD.\nTo set up prizes:\n* Enable Prize Awards: When creating a competition, select \"Competition will award prizes.\" Enter the total amount of prize money to be awarded.\n* Document Prize Rules: You'll need to specify the number of prizes and the amount for each prize on the Competition Rules and Overview pages. Clearly define the criteria for winning in this section. These sections must be completed to launch the competition.Adjusting prize amounts:\n* Prize amounts can be adjusted or turned off entirely only before launch. After you launch a competition, prize settings are locked. We advise you to double-check your prizes before scheduling, as you won't be able to change them after launch.\n* If you are offering a valuable prize that is not cash (eg. gift cards, or valuable objects), please list the monetary value of the prizes in US dollars. The value should not exceed the prize limit of $10,000 USD.Prize fulfillment:\n* Prizes for Community Competitions must be manually awarded and announced.\nLeaderboards for Community Competitions will not display an \"In the money\" designation for winning participants. We advise reaching out to winners directly on Kaggle and announcing winners using the Discussions feature.\n* When you enable prizes for a competition, you are solely responsible for providing and distributing all prizes, fulfilling all promises and commitments, and for complying with all applicable tax rules related to competition winners. Kaggle does not participate in prize distribution or rule enforcement for...",
      "url": "https://www.kaggle.com/docs/competitions-setup"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    },
    {
      "title": "Kaggle Handbook: Fundamentals to Survive a Kaggle Shake-up",
      "text": "<div><div><div><h2>A guide to Kaggle competitions with tips &amp; tricks to get on the good side of the \u201cShake-up\u201d.</h2><div><a href=\"https://medium.com/@ertugruldemir?source=post_page---byline--3dec0c085bc8---------------------------------------\"><div><p></p></div></a></div></div><p>If you\u2019re interested in data science, you probably heard about Kaggle, a platform where hundreds of DS and ML enthusiasts meet, discuss, share and compete.</p><p>In the first post of this series, I\u2019m going to talk briefly about the format of Kaggle competitions and then move on to the fundamental techniques that will help you end up on the better side of the biggest pitfalls of Kaggle competitions: <em>Shake-up</em>s.</p><h2><strong>How do Kaggle competitions work?</strong></h2><p><strong><em>The competition process is basically as follows:</em></strong></p><ul><li>An individual, or more often an organization, identifies a problem and assumes that this issue can be solved by machine learning.</li><li>The data for the problem might already exist or be collected/generated for the competition. Then, a proper metric is established.</li><li>Depending on the sponsors and the complexity of the problem, a prize pool is set.</li><li>The organizers provide Kaggle staff with the necessary data and materials, and finally, the organizers become the Kaggle competition host.</li><li>Participants submit their solutions during the competition phase and receive their final ranking based on the competition metrics.</li></ul><h2>This looks straightforward<strong>. What\u2019s the catch?</strong></h2><p><strong><em>The catch is in the second stage of the competition where your models are tested on data they have never seen before.</em></strong></p><figure><figcaption>Public vs. Private LB, Source: Kaggle</figcaption></figure><p>Kaggle competitions usually consist of two stages. The results of the first stage are displayed on the <strong>Public Leaderboard</strong> \u2014 a live scoreboard \u2014 and the results of the second stage are displayed on the <strong>Private Leaderboard</strong>. The results of the first stage are not decisive for the final ranking, but they are your guide for the private leaderboard. Organizers usually leave only a small part of the test set for the public leaderboard, and the rest is reserved for the private leaderboard, which is the most important for the final rankings. The private leaderboard is hidden from the participants until the competition ends.</p><p>In terms of restrictions and complexity, there are similar contests on Kaggle called <strong>Code Competitions</strong>. In these contests, the test set features are hidden from the participants, which takes the restrictions one step further by limiting the inference times and preventing hand labeling or exploratory data analysis on test data.</p><h2>You were talking about shake-what?!</h2><p><strong><em>That\u2019s the part where your final outcome reveals.</em></strong></p><p>The shake-up is the difference in rank between public and private leaderboards. One of the main goals of Kaggle competitions is to <em>survive </em>the shake-ups. Is it sheer luck to end up on the good side of shake-ups, or are there better ways? How can we assess our chances of survival in shake-ups? What techniques can we use?</p><p>Data scientists are not the kind of people who shut their eyes and hope for the best. So let us figure out how to better <strong><em>estimate</em></strong> the unknown.</p><h2><strong>How do I know what my model will do on the private leaderboard?</strong></h2><p><strong><em>Let\u2019s talk about fundamentals.</em></strong></p><p><strong>Bias-Variance Tradeoff and Overfitting</strong></p><p>Today we can easily train models with a large number of parameters and create highly complex models. Complex models can be very useful, but they can also be very punishing if we are not careful about the <strong>bias-variance tradeoff</strong>. Such models are risky because they are prone to overfitting. Overfitting occurs when a model memorizes the noise of training data instead of learning about the actual pattern behind it. We want our models to <strong>generalize </strong>and capture the actual nature of the data. That\u2019s what we want in Kaggle competitions too, so we can make predictions on a test set that our model hasn\u2019t trained on. The results of these predictions determine your private leaderboard standings. To assess the generalization capabilities of our model, we need to apply <strong>validation techniques</strong> and understand how our model acts in different cases.</p><p>Bias-Variance Tradeoff is beyond the scope of this post, but I highly recommend you take a look if you are not familiar with this concept.</p><h2>What are these validation techniques?</h2><p><strong><em>There are numerous validation techniques but we\u2019re going to talk about more common and Kaggle-specific ones.</em></strong></p><p><strong>Validation Set Approach</strong></p><p>The validation set approach is pretty straightforward. You separate away a fraction of your training data and this new set is called <strong>Hold-Out</strong>:</p><blockquote><p>The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.</p></blockquote><p><em>\u2014 Hands-On Machine Learning with Scikit-Learn and TensorFlow, 2nd Edition.</em></p><p>The latter part of the quote above bears a resemblance to the public leaderboard concept. We can evaluate our final model on the public test set to get an estimate of the generalization error. However, this approach has its own downsides:</p><ul><li>The validation estimate of our model metric can be highly variable, depending on which observations are included in the training set and which are included in the validation set. This is especially true for the <strong>Public Test Set</strong> since we don\u2019t know if the Private Test Set is coming from similar distribution.</li><li>If we take a subset of our training data as the validation set, we decrease the number of observations to train on, which can hamper our model generalization again.</li></ul><p><strong>Cross Validation Approach</strong></p><figure><figcaption>General k-fold cross validation approach with Kaggle competition additions.</figcaption></figure><blockquote><p>This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k \u2212 1 folds.</p></blockquote><p><em>\u2014 Page 203, An Introduction to Statistical Learning, 2021.</em></p><p>This method can solve the problems with the single hold-out set approach that I mentioned above. By creating multiple <strong>folds</strong>, we can test the metric on different cases while observing the variance and eventually using the whole training data! This way, we can have stronger indicators for our model and estimate private leaderboard scores more confidently. Of course, we still assume training and private test set come from similar distributions. However, we train our models on different sets, so together they\u2019re more likely to generalize better than a single holdout set approach.</p><p><strong>Trust Your CV (Cross Validation)</strong></p><p><strong>Trust Your CV </strong>became a pretty popular tagline among Kaggle competitors, reminding them not to build their models based on public leaderboard scores, and I concur. I have participated in quite a few Kaggle competitions over the last few years, and...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8"
    },
    {
      "title": "How (and why) to create a good validation set \u2013 fast.ai",
      "text": "An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\n\nOne of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a [`train_test_split` method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), this method takes a random subset of the data, which is a poor choice for many real-world problems.\n\nThe definitions of _training_, _validation_, and _test_ sets can be fairly nuanced, and the terms are sometimes inconsistently used. In the deep learning community, \u201ctest-time inference\u201d is often used to refer to evaluating on data in production, which is not the technical definition of a test set. As mentioned above, sklearn has a `train_test_split` method, but no `train_validation_test_split`. Kaggle only provides training and test sets, yet to do well, you will need to split their training set into your own validation and training sets. Also, it turns out that Kaggle\u2019s test set is actually sub-divided into two sets. It\u2019s no suprise that many beginners may be confused! I will address these subtleties below.\n\n## First, what is a \u201cvalidation set\u201d?\n\nWhen creating a machine learning model, the ultimate goal is for it to be accurate on new data, not just the data you are using to build it. Consider the below example of 3 different models for a set of data:\n\nunder-fitting and over-fitting\n\nSource: Andrew Ng\u2019s Machine Learning Coursera class\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it\u2019s not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n\nThe underlying idea is that:\n\n- the training set is used to train a given model\n- the validation set is used to choose between models (for instance, does a random forest or a neural net work better for your problem? do you want a random forest with 40 trees or 50 trees?)\n- the test set tells you how you\u2019ve done. If you\u2019ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case.\n\nA key property of the validation and test sets is that they must be representative of the **new data you will see in the future**. This may sound like an impossible order! By definition, you haven\u2019t seen this data yet. But there are still a few things you know about it.\n\n## When is a random subset not good enough?\n\nIt\u2019s instructive to look at a few examples. Although many of these examples come from Kaggle competitions, they are representative of problems you would see in the workplace.\n\n### Time series\n\nIf your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of the available data).\n\nSuppose you want to split the time series data below into training and validation sets:\n\nTime series data\n\nA random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you\u2019ll need in production):\n\na poor choice for your training set\n\nUse the earlier data as your training set (and the later data for the validation set):\n\na better choice for your training set\n\nKaggle currently has a competition to [predict the sales in a chain of Ecuadorian grocery stores](https://www.kaggle.com/c/favorita-grocery-sales-forecasting). Kaggle\u2019s \u201ctraining data\u201d runs from Jan 1 2013 to Aug 15 2017 and the test data spans Aug 16 2017 to Aug 31 2017. A good approach would be to use Aug 1 to Aug 15 2017 as your validation set, and all the earlier data as your training set.\n\n### New people, new boats, new\u2026\n\nYou also need to think about what ways the data you will be making predictions for in production may be _qualitatively different_ from the data you have to train your model with.\n\nIn the Kaggle [distracted driver competition](https://www.kaggle.com/c/state-farm-distracted-driver-detection), the independent data are pictures of drivers at the wheel of a car, and the dependent variable is a category such as texting, eating, or safely looking ahead. If you were the insurance company building a model from this data, note that you would be most interested in how the model performs on drivers you haven\u2019t seen before (since you would likely have training data only for a small group of people). This is true of the Kaggle competition as well: the test data consists of people that weren\u2019t used in the training set.\n\nTwo images of the same person talking on the phone while driving.\n\nIf you put one of the above images in your training set and one in the validation set, your model will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model may be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc).\n\nA similar dynamic was at work in the [Kaggle fisheries competition](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring) to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn\u2019t appear in the training data. This means that you\u2019d want your validation set to include boats that are not in the training set.\n\nSometimes it may not be clear how your test data will differ. For instance, for a problem using satellite imagery, you\u2019d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data.\n\n## The dangers of cross-validation\n\nThe reason that sklearn doesn\u2019t have a `train_validation_test` split is that it is assumed you will often be using **cross-validation**, in which different subsets of the training set serve as the validation set. For example, for a 3-fold cross validation, the data is divided into 3 sets: A, B, and C. A model is first trained on A and B combined as the training set, and evaluated on the validation set C. Next, a model is trained on A and C combined as the training set, and evaluated on validation set B. And so on, with the model performance from the 3 folds being averaged in the end.\n\nHowever, the problem with cross-validation is that it is rarely applicable to real world problems, for all the reasons describedin the above sections. Cross-validation only works in the same cases where you can randomly shuffle your data to choose a validation set.\n\n## Kaggle\u2019s \u201ctraining set\u201d = your training + validation sets\n\nOne great thing about Kaggle competitions is that they force you to think about validation sets more rigorously (in order to do well). For those who are new to Kaggle, it is a platform that hosts machine learning competitions. Kaggle typically breaks the data into two sets you can download:\n\n1. a **training set**, which includes the _independent variables_, as well as the _dependent variable_ (what you are trying to predict). For the example of an Ecuadorian grocery store trying to predict sales, the indep...",
      "url": "https://www.fast.ai/posts/2017-11-13-validation-sets.html"
    },
    {
      "title": "Competing in a data science contest without reading the data",
      "text": "Machine learning competitions have become an extremely popular format for\nsolving prediction and classification problems of all sorts. The most famous\nexample is perhaps the Netflix prize. An even better example is\n[Kaggle](http://www.kaggle.com), an awesome startup that\u2019s\norganized more than a hundred competitions over the past few years.\n\nThe central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of _holdout labels_ not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.\n\nPublic leaderboard of the Heritage Health Prize ( [Source](http://www.heritagehealthprize.com/c/hhp/leaderboard/public))\n\nIn this post, I will describe a method to climb the public leaderboard _without even looking at the data_. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle\u2019s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some\nchallenges that while fundamental have only recently seen increased attention. A follow-up post will describe a [recent paper](http://arxiv.org/abs/1502.04585) with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.\n\nLet me be very clear that my point is _not_ to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I\u2019m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.\n\n## The Kaggle leaderboard mechanism\n\nAt first sight, the Kaggle mechanism looks like the classic _holdout method_. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in . Think of the score as prediction error (smaller is better). For concreteness, let\u2019s fix it to be the _misclassification rate_. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in .\n\nKaggle further splits its \\\\(N\\\\) private labels randomly into \\\\(n\\\\) holdout labels and \\\\(N-n\\\\) test labels. Typically, \\\\(n=0.3N\\\\). The public leaderboard is a sorting of all teams according to their score computed only on the \\\\(n\\\\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels. I will let \\\\(s\\_H(y)\\\\) denote the public score of a submission \\\\(y\\\\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.\n\n## The cautionary tale of wacky boosting\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there\u2019s an unknown set of labels \\\\(y\\\\in\\\\{0,1\\\\}^N\\\\) that I need to predict. Well, I know nothing about \\\\(y\\\\). So here\u2019s what I\u2019m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we\u2019re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I\u2019m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here\u2019s what I do:\n\n**Algorithm** (Wacky Boosting):\n\n1. Choose \\\\(y\\_1,\\\\dots,y\\_k\\\\in\\\\{0,1\\\\}^N\\\\) uniformly at random.\n2. Let \\\\(I = \\\\{ i\\\\in\\[k\\] \\\\colon s\\_H(y\\_i) < 0.5 \\\\}\\\\).\n3. Output \\\\(\\\\hat y=\\\\mathrm{majority} \\\\{ y\\_i \\\\colon i \\\\in I \\\\} \\\\), where the majority is component-wise.\n\nLo and behold, this is what happens:\n\nIn this plot, \\\\(n=4000\\\\) and all numbers are averaged over 5 independent repetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would\u2019ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at DeepCompeting.ly, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso. Two months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from DeepCompeting.ly days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.\n\n### What just happened\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \\\\(y\\_i\\\\) has loss around \\\\(1/2\\\\pm1/\\\\sqrt{n}\\\\). We\u2019re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \\\\(w\\_i\\\\) is roughly \\\\(1/2-c/\\\\sqrt{n}\\\\) for some positive constant \\\\(c>0\\\\). Put differently, each selected \\\\(y\\_i\\\\) is giving us a guess about each label in the unknown holdout set \\\\(H\\\\subseteq \\[N\\]\\\\) that\u2019s correct with probability \\\\(1/2 + \\\\Omega(1/\\\\sqrt{n})\\\\). Since the public score doesn\u2019t depend on labels outside of \\\\(H\\\\), the conditioning does not affect the final test set. The labels outside of \\\\(H\\\\) are still unbiased. Finally, we need to argue that the majority vote \u201cboosts\u201d our slightly biased coin tosses into a stronger bias. More formally, we can show that \\\\(\\\\hat y\\\\) gives us a guess for each label in \\\\(H\\\\) that\u2019s correct with probability\n\\\\\\[\n\\\\frac12 + \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nHence, the public score of \\\\(y\\\\) satisfies\n\\\\\\[\ns\\_H(y) < \\\\frac12 - \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nOutside of \\\\(H\\\\), however, we\u2019re just random guessing with no advantage.\nTo summarize, wacky boosting gives us _a bias of \\\\(\\\\sqrt{k}\\\\) standard deviations on the public score with \\\\(k\\\\) submissions_.\n\nWhat\u2019s important is that the same algorithm still \u201cworks\u201d even if we don\u2019t get exact answers. All we need are answers that are accurate to an additive error of \\\\(1/\\\\sqrt{n}\\\\). This is important since Kaggle rounds its answers to 5 digits of precision. In particular, this attack will work so long as \\\\(n< 10^{10}\\\\).\n\n### Why the holdout method breaks down\n\nThe idea behind the holdout method is that the holdout data serve as a fresh sample providing an unbiased ...",
      "url": "https://blog.mrtz.org/2015/03/09/competition.html"
    }
  ]
}