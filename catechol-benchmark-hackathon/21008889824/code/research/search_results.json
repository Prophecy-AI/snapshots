{
  "query": "How can I reduce the generalization gap between cross-validation and test performance when the test distribution is unknown in chemical property prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the gap between cross\u2011validation (CV) scores and the performance you will see on an unknown test distribution starts with making the CV procedure itself more \u201cout\u2011of\u2011distribution\u2011aware\u201d.  Studies that compare many splitting strategies show that scaffold\u2011based splits (which keep whole molecular scaffolds together) retain a strong linear relationship between in\u2011distribution (ID) and out\u2011of\u2011distribution (OOD) performance (Pearson\u202f\u2248\u202f0.9), whereas similarity\u2011cluster splits break that relationship (Pearson\u202f\u2248\u202f0.4)\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  A practical way to emulate prospective use\u2011cases is to adopt *k\u2011fold n\u2011step forward* CV, which mimics a temporal or chemical\u2011space drift and has been shown to give a more realistic estimate of bioactivity prediction accuracy\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  Adding domain\u2011specific metrics such as \u201cdiscovery yield\u201d and \u201cnovelty error\u201d further clarifies the model\u2019s applicability domain.\n\nBeyond smarter splits, quantifying and calibrating predictive uncertainty narrows the gap.  Large\u2011scale evaluations demonstrate that ensembles built from k\u2011fold CV provide reliable uncertainty estimates, allowing you to flag predictions that are likely OOD\u202f([jcheminf](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9)).  Probabilistic frameworks like **DIONYSUS** improve calibration on low\u2011data chemical sets, making the reported confidence intervals more trustworthy\u202f([RSC](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  When labeled data are scarce, meta\u2011learning approaches that leverage abundant unlabeled molecules can learn to interpolate between ID and OOD regions, substantially boosting robustness to covariate shift\u202f([arXiv](https://arxiv.org/abs/2506.11877)).\n\nIn practice, combine these tactics: (1) use scaffold\u2011 or forward\u2011time CV rather than random splits; (2) report ensemble\u2011derived uncertainties and calibrate them with methods such as DIONYSUS; (3) if possible, augment the training set with unlabeled data via meta\u2011learning or self\u2011supervised pre\u2011training; and (4) adopt standardized CV protocols (e.g., the MatFold framework) to avoid overly optimistic bias\u202f([RSC](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d)).  Together, these steps help ensure that the CV performance you observe is a tighter lower bound on the unknown test\u2011time performance of chemical property predictors.",
      "url": ""
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation",
      "text": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00709-9?)\n# Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 April 2023\n* Volume\u00a015, article\u00a0number49, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nLarge-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n* [Thomas-Martin Dutschmann](#auth-Thomas_Martin-Dutschmann-Aff1)[1](#Aff1),\n* [Lennart Kinzel](#auth-Lennart-Kinzel-Aff1)[1](#Aff1),\n* [Antonius ter Laak](#auth-Antonius-Laak-Aff2)[2](#Aff2)&amp;\n* \u2026* [Knut Baumann](#auth-Knut-Baumann-Aff1)[1](#Aff1)Show authors\n* 7359Accesses\n* 50Citations\n* 3Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9/metrics)\n## Abstract\nIt is insightful to report an estimator that describes how certain a model is in a prediction, additionally to the prediction alone. For regression tasks, most approaches implement a variation of the ensemble method, apart from few exceptions. Instead of a single estimator, a group of estimators yields several predictions for an input. The uncertainty can then be quantified by measuring the disagreement between the predictions, for example by the standard deviation. In theory, ensembles should not only provide uncertainties, they also boost the predictive performance by reducing errors arising from variance. Despite the development of novel methods, they are still considered the \u201cgolden-standard\u201d to quantify the uncertainty of regression models. Subsampling-based methods to obtain ensembles can be applied to all models, regardless whether they are related to deep learning or traditional machine learning. However, little attention has been given to the question whether the ensemble method is applicable to virtually all scenarios occurring in the field of cheminformatics. In a widespread and diversified attempt, ensembles are evaluated for 32 datasets of different sizes and modeling difficulty, ranging from physicochemical properties to biological activities. For increasing ensemble sizes with up to 200 members, the predictive performance as well as the applicability as uncertainty estimator are shown for all combinations of five modeling techniques and four molecular featurizations. Useful recommendations were derived for practitioners regarding the success and minimum size of ensembles, depending on whether predictive performance or uncertainty quantification is of more importance for the task at hand.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-45630-5?as&#x3D;webp)\n### [Ensemble Models](https://link.springer.com/10.1007/978-3-031-45630-5_12?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-48317-7?as&#x3D;webp)\n### [Ensemble Methods for Time Series Forecasting](https://link.springer.com/10.1007/978-3-319-48317-7_13?fromPaywallRec=false)\nChapter\u00a9 2017\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-12240-8?as&#x3D;webp)\n### [Ensemble Models Using Symbolic Regression and Genetic Programming for Uncertainty Estimation in ESG and Alternative Investments](https://link.springer.com/10.1007/978-3-031-12240-8_5?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://jcheminf.biomedcentral.com/subjects/applied-probability)\n* [Applied Statistics](https://jcheminf.biomedcentral.com/subjects/applied-statistics)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://jcheminf.biomedcentral.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Statistical Theory and Methods](https://jcheminf.biomedcentral.com/subjects/statistical-theory-and-methods)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nMachine learning (ML) for drug design purposes holds a long tradition\u00a0[[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR1)], but has recently started to gain further attention due to the success of deep learning (DL)\u00a0[[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR2)]. Yet, the prediction of chemical properties and activities is only one step in a long and resource-intense process of drug design, discovery, and development. When developing ML models, predictions alone are not sufficient and require further analysis\u00a0[[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR3)]. During model construction and testing, errors made by the model can easily be evaluated since the true target values are known. The error distribution allows the estimation of the quality of the model, but cannot be applied when predicting values for new compounds with unknown target values. In this case, it is good practice to provide an estimate of the uncertainty associated with the prediction. Measures quantifying the predictive uncertainty can be used to set a threshold which defines the model\u2019s applicability domain. The latter is defined as follows: \u201cThe applicability domain of a (Q)SAR model is the response and chemical structure space in which the model makes predictions with a given reliability.\u201d\u00a0[[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR4)]. The reliability of a model can either be addressed by quantifying its confidence, or, conversely, its uncertainty. Recent studies make use of the term uncertainty quantification (UQ)\u00a0[[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR5)]. Uncertainty can be of aleatoric nature, relating to the random process that generates the target values, or epistemic nature, implying model-related uncertainty\u00a0[[6](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR6)]. Usually, these two types cannot be fully distinguished\u00a0[[7](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR7)].\nClassification algorithms often provide built-in mechanisms or augmentations to measure their uncertainty\u00a0[[8](https://jcheminf.biomedcentral.com/...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9"
    },
    {
      "title": "Characterizing Uncertainty in Machine Learning for Chemistry",
      "text": "Characterizing Uncertainty in Machine Learning for Chemistry | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Characterizing Uncertainty in Machine Learning for Chemistry\n08 February 2023, Version 1\nThis is not the most recent version. There is a[\nnewer version\n](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)of this content available\nWorking Paper\n## Authors\n* [Esther Heid](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Esther%20Heid)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-8404-6596),\n* [Charles J. McGill](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Charles%20J.%20McGill),\n* [Florence H. Vermeire](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Florence%20H.%20Vermeire),\n* [William H. Green](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=William%20H.%20Green)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nCharacterizing uncertainty in machine learning models has recently gained interest in the context of machine learning reliability, robustness, safety, and active learning. Here, we separate the total uncertainty into contributions from noise in the data (aleatoric) and shortcomings of the model (epistemic), further dividing epistemic uncertainty into model bias and variance contributions. We systematically address the influence of noise, model bias, and model variance in the context of chemical property predictions, where the diverse nature of target properties and the vast chemical chemical space give rise to many different distinct sources of prediction error. We demonstrate that different sources of error can each be significant in different contexts and must be individually addressed during model development. Through controlled experiments on datasets of molecular properties, we show important trends in model performance associated with the level of noise in the dataset, size of the dataset, model architecture, molecule representation, ensemble size, and dataset splitting. In particular, we show that 1) noise in the test set can limit a model's observed performance when the actual performance is much better, 2) using size-extensive model aggregation structures is crucial for extensive property prediction, 3) ensembling is a reliable tool for uncertainty quantification and improvement specifically for the contribution of model variance, and 4) evaluations of cross-validation models understate their performance. We develop general guidelines on how to improve an underperforming model when falling into different uncertainty contexts.\n## Keywords\n[Uncertainty](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Uncertainty)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[Chemical property prediction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Chemical%20property%20prediction)\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nCharacterizing Uncertainty in Machine Learning for Chemistry Scripts\n**Description**\nPython scripts to reproduce the results of the manuscript.\n**Actions**\n[**View**](https://github.com/cjmcgill/characterizing_uncertainty_scripts)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\n[May 17, 2023 Version\n3](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)\n[Feb 16, 2023 Version\n2](https://chemrxiv.org/engage/chemrxiv/article-details/63ecf74d1d2d1840638a8b75)\nFeb 08, 2023 Version 1\n## Metrics\n3,215\n1,200\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2023-00vcg\nD O I: 10.26434/chemrxiv-2023-00vcg [opens in a new tab]](https://doi.org/10.26434/chemrxiv-2023-00vcg)\n## Funding\n**Austrian Science Fund**\nJ-4415\n**Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS)**\n## Author\u2019s competing interest statement\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n## Ethics\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/63e2b82b6d032916bb650914"
    },
    {
      "title": "Real-World Molecular Out-Of-Distribution: Specification and Investigation",
      "text": "We use cookies to distinguish you from other users and to provide you with a better experience on our websites.Close this message to accept cookies or find out how to manage your cookie settings. [Learn more about our Privacy Notice...\\\n\\[opens in a new tab\\]](https://www.cambridge.org/about-us/legal-notices/privacy-notice/)\n\n[Back to\\\nTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\n\nSearch within Theoretical and Computational Chemistry\n\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n\n# Real-World Molecular Out-Of-Distribution: Specification and Investigation\n\n05 June 2023, Version 1\n\nThis is not the most recent version. There is a [newer version](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932) of this content available\n\nWorking Paper\n\n## Authors\n\n- [Prudencio Tossou](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Prudencio%20Tossou),\n- [Cas Wognum](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Cas%20Wognum)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0009-0006-2742-4817),\n- [Michael Craig](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Michael%20Craig),\n- [Hadrien Mary](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Hadrien%20Mary),\n- [Emmanuel Noutahi](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Emmanuel%20Noutahi)\n\n[Show author details](https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1)\n\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nThis study presents a rigorous framework for investigating Molecular Out-Of-Distribution (MOOD) generalization in drug discovery. The concept of MOOD is first clarified through a problem specification that demonstrates how the covariate shifts encountered during real-world deployment can be characterized by the distribution of sample distances to the training set. We find that these shifts can cause performance to drop by up to 60% and uncertainty calibration by up to 40%. This leads us to propose a splitting protocol that aims to close the gap between deployment and testing. Then, using this protocol, a thorough investigation is conducted to assess the impact of model design, model selection and dataset characteristics on MOOD performance and uncertainty calibration. We find that appropriate representations and algorithms with built-in uncertainty estimation are crucial to improve performance and uncertainty calibration. This study sets itself apart by its exhaustiveness and opens an exciting avenue to benchmark meaningful, algorithmic progress in molecular scoring. All related code can be found on Github at https://github.com/cwognum/mood-experiments.\n\n## Keywords\n\n[Molecular Scoring](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20Scoring)\n\n[Out-of-Distribution](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Out-of-Distribution)\n\n[Applicability Domain](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Applicability%20Domain)\n\n[Drug Discovery](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Drug%20Discovery)\n\n## Supplementary materials\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\n\nMOOD: Supplementary Material\n\n**Description**\n\nProvides a variety of additional figures to support the results from the main text.\n\n**Actions**\n\n**Download**\n**(3 MB)**\n\n## Supplementary weblinks\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\n\nMOOD: Code base\n\n**Description**\n\nA Github repository with all the code that was used for the results in the MOOD paper.\n\n**Actions**\n\n[**View**](https://github.com/cwognum/mood-experiments)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\n[Jul 26, 2023 Version\\\n2](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932)\n\nJun 05, 2023 Version 1\n\n## Metrics\n\n3,843\n\n1,822\n\n0\n\nViews\n\nDownloads\n\nCitations\n\n## License\n\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\n\nCC\n\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\n\nBY\n\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\n\nNC\n\nThe content is available under\n[CC BY NC 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by-nc/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2023-q11q4\\\nD O I: 10.26434/chemrxiv-2023-q11q4 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2023-q11q4)\n\n## Funding\n\n**Mitacs**\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1"
    }
  ]
}