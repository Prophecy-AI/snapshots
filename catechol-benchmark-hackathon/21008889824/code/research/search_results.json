{
  "query": "How can transfer learning from pre-trained molecular models improve chemical reaction yield prediction with limited training data?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Transfer learning leverages a model that has already learned general chemical knowledge from a very large, often publicly available corpus of molecules or reactions, and then adapts (fine\u2011tunes) that model on a much smaller, task\u2011specific dataset such as reaction\u2011yield measurements.  Pre\u2011training on millions of SMILES strings or graph\u2011based representations enables the network to capture robust atom\u2011level embeddings, bond\u2011type patterns, and mechanistic motifs that would be impossible to learn from a handful of yield experiments alone.  When the pretrained weights are transferred, the downstream model starts from a chemically informed parameter space, which dramatically reduces over\u2011fitting and improves data efficiency\u202f\u2014\u202fthe same principle that underlies the success of large language models in NLP\u202f([jcheminf](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4), [pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00052g)).\n\nEmpirical studies confirm these benefits for yield prediction.  Kang\u202fet\u202fal. showed that a graph\u2011neural\u2011network (GNN) pretrained on millions of unlabeled reactions outperformed a randomly\u2011initialized GNN when only a few hundred labeled yields were available, achieving higher R\u00b2 and lower mean absolute error than baseline models\u202f([jcheminf](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z)).  Similarly, the ReactionT5 transformer, pretrained on a massive reaction corpus, attained accurate yield forecasts with as few as 1\u202f% of the usual training data, demonstrating that the learned attention patterns transfer directly to yield\u2011related tasks\u202f([jcheminf](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4)).  In a catalytic\u2011reaction setting, Sunoj\u202fet\u202fal. pretrained a recurrent neural network on 1\u202fM molecules and then fine\u2011tuned it on ~5\u202fk transition\u2011metal\u2011catalyzed reactions, obtaining an RMSE of 4.9\u202f% for Buchwald\u2013Hartwig yields\u2014far better than models trained from scratch\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00052g)).  Active transfer\u2011learning approaches (Zimmerman\u202fet\u202fal.) further improve efficiency by selecting the most informative low\u2011data points for fine\u2011tuning, yielding accurate condition and yield predictions with only a few dozen experiments\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)).\n\nThe choice of pre\u2011training data also matters.  Wiest\u202fet\u202fal. demonstrated that pre\u2011training on a modest, mechanistically\u2011related set of pericyclic reactions (e.g., Diels\u2013Alder, Cope, Claisen) gave superior low\u2011data performance compared with pre\u2011training on a 50\u2011fold larger, unrelated USPTO\u2011MIT dataset, highlighting that domain\u2011specific chemical knowledge can be more valuable than sheer size\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlelanding/2025/dd/d4dd00412d)).  Together, these results illustrate that transfer learning from pretrained molecular models provides richer feature representations, mitigates over\u2011fitting, and enables accurate reaction\u2011yield prediction even when only limited experimental data are available.",
      "url": ""
    },
    {
      "title": "ReactionT5: a pre-trained transformer model for accurate chemical reaction prediction with limited data",
      "text": "BMC is moving to Springer Nature Link. [Visit this journal in its new home.](https://link.springer.com/journal/13321/articles)\n\nSearch all BMC articles\n\nSearch\n\nReactionT5: a pre-trained transformer model for accurate chemical reaction prediction with limited data\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-025-01075-4.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-025-01075-4.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-025-01075-4.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-025-01075-4.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 19 August 2025\n\n# ReactionT5: a pre-trained transformer model for accurate chemical reaction prediction with limited data\n\n- [Tatsuya Sagawa](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Tatsuya-Sagawa-Aff1-Aff3) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3) &\n- [Ryosuke Kojima](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Ryosuke-Kojima-Aff2-Aff3) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a017**, Article\u00a0number:\u00a0126 (2025)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 1918 Accesses\n\n- 2 Citations\n\n- 1 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4/metrics)\n\n\n## Abstract\n\nAccurate chemical reaction prediction is critical for reducing both cost and time in drug development. This study introduces ReactionT5, a transformer-based chemical reaction foundation model pre-trained on the Open Reaction Database\u2014a large publicly available reaction dataset. In benchmarks for product prediction, retrosynthesis, and yield prediction, ReactionT5 outperformed existing models. Specifically, ReactionT5 achieved 97.5% accuracy in product prediction, 71.0% in retrosynthesis, and a coefficient of determination of 0.947 in yield prediction. Remarkably, ReactionT5, when fine-tuned with only a limited dataset of reactions, achieved performance on par with models fine-tuned on the complete dataset. Additionally, the visualization of ReactionT5 embeddings illustrates that the model successfully captures and represents the chemical reaction space, indicating effective learning of reaction properties.\n\n### Graphical Abstract\n\n## Scientific contribution\n\nWe propose ReactionT5, a foundation model for chemical reactions pre-trained on a large reaction dataset that outperforms existing models in product prediction, retrosynthesis, and yield prediction tasks, all while maintaining high performance even with limited training data. We also show that the learned embeddings effectively capture the chemical reaction space, and we make the training code and model weights publicly available. We believe that fine-tuning ReactionT5 with users\u2019 in-house data will be beneficial for many chemical applications.\n\n## Introduction\n\nPredicting chemical reactions is pivotal for progress in organic chemistry and drug discovery. Highly accurate predictive models can dramatically reduce the costs associated with exploratory experimentation by forecasting the outcomes of chemical experiments before they are conducted. Consequently, machine learning models that adeptly encapsulate the complexity of organic reactions \\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR1)\\] are anticipated to bolster the efforts of experimental chemists. Deep learning models have surfaced as viable substitutes in recent years, providing data-driven insights derived from extensive reaction datasets and surmounting numerous constraints of traditional methods \\[ [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR2)\\].\n\nIn this context, large-scale pre-trained models utilizing compound libraries have attracted increasing interest in organic chemistry research \\[ [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR3)\\]. These models often conceptualize molecules as symbolic sequences, similar to natural languages. For instance, SMILES-BERT \\[ [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR4)\\] has demonstrated high efficacy in predicting molecular properties by leveraging unsupervised pre-training on molecular structures encoded in the Simplified Molecular-Input Line-Entry System (SMILES) format \\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR5)\\]. While substantial progress has been made with models focused on single molecules, for example, SMILES-BERT \\[ [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR4)\\], ChemBERTa \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR6)\\], MolE \\[ [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR7)\\], MolCLR \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR8)\\], and Molformer \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR9)\\], research on models addressing multiple molecules, such as those used in chemical reactions, remains relatively scarce. A notable exception is T5Chem \\[ [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR10)\\], which, although not explicitly pre-trained on reaction datasets, has shown potential as a reaction prediction model by pre-learning from single molecules and subsequently applying fine-tuning to address multiple tasks, including product, retrosynthesis, and yield prediction. Building upon these advances in molecular representation learning, recent studies have extended pre-training approaches to chemical reactions using reaction SMILES, which are chemical transformations represented in SMILES format. Pre-training on reaction data has been shown to enhance model performance on various downstream tasks, such as yield prediction \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR11)\\] and experimental procedure prediction \\[ [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR12)\\], highlighting the importance of capturing the intricate dependencies within chemical reactions.\n\nChemists anticipate that foundation models for chemical reactions will perform well on target tasks with minimal fine-tuning using a small set of reaction data, as accumulating a large volume of experimental data for training is challenging. This need is particularly pronounced when there is a domain shift between the datasets used for pre-training and those intended for the target tasks, highlighting a critical demand for developing models based on chemical reactions that can be efficiently and effectively fine-tuned with limited data. To our knowledge, there are currently no pre-trained models specifically designed to be fine-tuned on small data sets that have been pre-trained using a large-scale reaction database.\n\nThis study introduces ReactionT5, a chemical reaction foundation model that utilizes the text-to-text transfer transformer (T5) \\[ [13](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR13)\\] architecture. ReactionT5 is distinct in that it incorporates pre-training on a vast reaction database in addition to the conventional pre-training on a molecular library. By leveraging the Open Reaction Database (ORD) \\[ [14](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4#ref-CR14)\\], an expansive, open-access dataset covering...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01075-4"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph neural networks",
      "text": "Search all BMC articles\n\nSearch\n\nImproving chemical reaction yield prediction using pre-trained graph neural networks\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 01 March 2024\n\n# Improving chemical reaction yield prediction using pre-trained graph neural networks\n\n- [Jongmin Han](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Jongmin-Han-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1),\n- [Youngchun Kwon](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youngchun-Kwon-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Youn-Suk Choi](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youn_Suk-Choi-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2) &\n- \u2026\n- [Seokho Kang](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Seokho-Kang-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a025 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 5588 Accesses\n\n- 9 Citations\n\n- 2 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z/metrics)\n\n\n## Abstract\n\nGraph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.\n\n## Introduction\n\nA chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner \\[ [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR3), [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR4), [5](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR5), [6](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR6), [7](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\].\n\nRecently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR6), [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR7)\\] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR9), [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR10)\\].\n\nDespite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.\n\nTo alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR12)\\]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart \\[ [13](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR13), [14](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR14), [15](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR15), [16](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR16), [17](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR18)\\]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented \\[ [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR20)\\]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them \\[ [21](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR21), [22](https://jcheminf.biomedcentral.com/jcheminf.biome...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc01662a)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05681f)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D1SC06932B](https://doi.org/10.1039/D1SC06932B)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022, **13**, 6655-6668\n\n# Predicting reaction conditions from limited data through active transfer learning [\u2020](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b\\#fn1)\n\nEunjae\nShim\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-4085-9659)a,\nJoshua A.\nKammeraad\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0386-7198)ab,\nZiping\nXu\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2591-0356)b,\nAmbuj\nTewari\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6969-7844)bc,\nTim\nCernak\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5407-0643)\\*ad and Paul M.\nZimmerman\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-7444-1314)\\*a\n\naDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [paulzim@umich.edu](mailto:paulzim@umich.edu)\n\nbDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\n\ncDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA\n\ndDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail: [tcernak@med.umich.edu](mailto:tcernak@med.umich.edu)\n\nReceived\n10th December 2021\n, Accepted 10th May 2022\n\nFirst published on 11th May 2022\n\n* * *\n\n## Abstract\n\nTransfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.\n\n* * *\n\n## Introduction\n\nComputers are becoming increasingly capable of performing high-level chemical tasks. [1\u20134](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit1) Machine learning approaches have demonstrated viable retrosynthetic analyses, [5\u20137](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit5) product prediction, [8\u201311](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit8) reaction condition suggestion, [12\u201316](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit12) prediction of stereoselectivity, [17\u201320](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit17) regioselectivity, [19,21\u201324](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) and reaction yield [25,26](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit25) and optimization of reaction conditions. [27\u201330](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit27) These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (e.g. reaction conditions) within a particular domain relate to an outcome (e.g. yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.\n\nExpert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain, [31](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) emulates a tactic commonly employed by human chemists.\n\nTransfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain. [31,32](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Models are first created using the source data, then transferred to the target domain using various algorithms. [19,33\u201335](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit19) For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (e.g., descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible. [31,36](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit31) Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data. [37](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit37) The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.\n\nAnother machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data. [38,39](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b#cit38) Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental result...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06932b"
    },
    {
      "title": "A transfer learning protocol for chemical catalysis using a recurrent neural network adapted from natural language processing",
      "text": "<div><div><p><span></span></p><div><p><span>Received \n 21st December 2021\n </span><span>, Accepted 11th April 2022</span></p><p>First published on 11th April 2022</p><hr/><div><h2>Abstract</h2><p>Minimizing the time and material investments in discovering molecular catalysis would be immensely beneficial. Given the high contemporary importance of homogeneous catalysis in general, and asymmetric catalysis in particular, makes them the most compelling systems for leveraging the power of machine learning (ML). We see an overarching connection between the powerful ML tools such as the transfer learning (TL) used in natural language processing (NLP) and the chemical space, when the latter is described using the SMILES strings conducive for representation learning. We developed a TL protocol, trained on 1 million molecules first, and exploited its ability for accurate predictions of the yield and enantiomeric excess for three diverse reaction classes, encompassing over 5000 transition metal- and organo-catalytic reactions. The TL predicted yields in the Pd-catalyzed Buchwald\u2013Hartwig cross-coupling reaction offered the highest accuracy, with an impressive RMSE of 4.89 implying that 97% of the predicted yields were within 10 units of the actual experimental value. In the case of catalytic asymmetric reactions, such as the enantioselective <span>N</span>,<span>S</span>-acetal formation and asymmetric hydrogenation, RMSEs of 8.65 and 8.38 could be obtained respectively, with the predicted enantioselectivities (%ee) within 10 units of its true value in \u223c90% of the time. The method is highly time-economic as the workflow bypasses collecting the molecular descriptors and hence of direct implication to high throughput discovery of catalytic transformations.</p></div><hr/>\n \n <h2><span>Introduction</span></h2>\n <p><span>Chemical catalysis is a vibrant domain of research pursued alike in industry and academia owing to its importance in energy, automobile, fine chemicals, pharmaceuticals and so on.<a href=\"#cit1\"><sup><span>1,2</span></sup></a> The drive to invent newer catalytic protocols or to impart superior efficiency to the known processes has been perpetual.<a href=\"#cit3\"><sup><span>3,4</span></sup></a> The design of new catalysts, such as for homogeneous molecular catalysis, has remained in the forefront for decades.<a href=\"#cit5\"><sup><span>5,6</span></sup></a> Such efforts are generally guided by chemical intuition and might even demand tiresome loops of trial and error.<a href=\"#cit7\"><sup><span>7</span></sup></a> In recent years, the traditional approaches in catalysis were augmented by tools such as linear regression,<a href=\"#cit8\"><sup><span>8</span></sup></a> machine learning (ML),<a href=\"#cit9\"><sup><span>9</span></sup></a> active learning,<a href=\"#cit10\"><sup><span>10</span></sup></a> and robotics;<a href=\"#cit11\"><sup><span>11,12</span></sup></a> all seem to point to an emerging synergism in reaction discovery.<a href=\"#cit13\"><sup><span>13\u201315</span></sup></a></span></p><p>From a sustainability standpoint, it is high time that we endeavor to develop faster, reliable, and less resource intensive (<span>e.g.</span>, time and material) invention workflows. To make this goal more realistic, ML driven protocols have a highly promising role to play.<a href=\"#cit16\"><sup><span>16,17</span></sup></a> The yield and enantiomeric excess are countable indicators of how good a (asymmetric)catalytic method is, particularly in its developmental stage. One would inevitably encounter a high-dimensional chemical space composed of relevant molecular features of catalysts, substrates, solvent, additives, <span>etc.</span>, for training ML models to predict the yields/ee of catalytic reactions.<a href=\"#cit18\"><sup><span>18,19</span></sup></a> For instance, Rothenberg <span>et al.</span> built a classification and regression model for predicting the turnover number (TON) and turnover frequency (TOF) for 412 Heck reactions.<a href=\"#cit20\"><sup><span>20</span></sup></a> A total of 74 physical organic descriptors were employed for the reaction components such as the substrate, ligand, solvent and so on in addition to the inclusion of reaction conditions (time, temperature, catalyst loading, <span>etc.</span>). The artificial neural networks were found to perform better than the linear regression techniques. The trained model was then utilized to predict the TON and TOF of a virtual library (<span>in silico</span>) of 60000 Heck reactions. In the recent years, there has been a visible increase in efforts in developing new molecular representations capable of improved performance and generalizability. Several approaches, other than those relying on quantum chemically derived molecular descriptors, have emerged. These methods primarily involve the use of various genres of structure-based representations.<a href=\"#cit21\"><sup><span>21\u201323</span></sup></a></p>\n <p>The representation learning methods such as the deep neural networks (DNNs) built on engineered features have found profound applications in chemical space.<a href=\"#cit24\"><sup><span>24,25</span></sup></a> DNNs can also be trained on molecular representations, such as the SMILES (simplified molecular input line entry system) strings, to learn the feature representation involving minimal feature engineering.<a href=\"#cit26\"><sup><span>26</span></sup></a> This approach can grasp the underlying patterns in atomic connectivity and capture the relationship between such features and molecular properties. During the developmental phase in catalysis research, only smaller or fragmented datasets are typically available, thereby necessitating a work-around, should one choose to deploy DNNs. It may be possible that tools from seemingly disparate domains become valuable for a task at hand, provided that shared or latent characteristics exist between them.</p>\n <p>Natural language processing (NLP) is one of the most visible domains of artificial intelligence that provides computers the capability to generate and analyze text/speech.<a href=\"#cit27\"><sup><span>27</span></sup></a> The large/labeled data requirements in NLP could be circumvented by using transfer learning methods,<a href=\"#cit28\"><sup><span>28</span></sup></a> wherein the knowledge acquired from one task (source task) is retained and subsequently utilized for other related tasks (target task). Therefore, NLP could be deployed for those tasks that rely on the language or similar textual data. The SMILES representation of molecules can be considered analogous to a natural language. In fact, interesting applications of NLP-based methods to chemical reactions are now becoming available.<a href=\"#cit29\"><sup><span>29\u201331</span></sup></a> The use of NLP-based models for accurate prediction of various properties of molecules is well-known.<a href=\"#cit32\"><sup><span>32,33</span></sup></a> On the other hand, predicting the reaction outcome that is known to depend on the molecular attributes of catalysts, reactants, solvents and several other factors is challenging and has seldom been reported using language models.</p>\n <p>Currently, most of the ML models for ee or yield predictions are custom-made for specific reactions, limiting their direct transferability to another reaction class. Further, such models are built on atomic/molecular descriptors as obtained through workflows involving time-consuming quantum chemical computations on a large library of molecules. We envisaged that NLP methods in conjunction with the SMILES representation of the molecular space could offer learning tools suitable for chemical catalysis. Such approaches could be transferable and time-economic. Herein, we design ML models that can predict both ee and yield of catalytic reactions. To demonstrate our ML protocol, a repertoire of transition metal-as well as organo-catalytic transformations of high recent interest, such as the (1) Buchwald\u2013Hartwig reaction,<a href=\"#cit34\"><sup><span>34</span...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00052g"
    },
    {
      "title": "Improving reaction prediction through chemically aware transfer learning",
      "text": "<div><div>\n<section>\n<article>\n<div>\n<p><a href=\"#\">\n<span></span> Author affiliations\n</a></p><div>\n<p>\n<span>\n* </span>\n<span>\nCorresponding authors\n</span>\n</p>\n<p>\n<span>\n<sup>a</sup>\n</span>\n<span>\nSchool of Chemistry and Molecular Biosciences, The University of Queensland, Brisbane, QLD 4072, Australia\n</span>\n</p>\n<p>\n<span>\n<sup>b</sup>\n</span>\n<span>\nDepartment of Computer Science and Engineering, University of Notre Dame, USA\n</span>\n</p>\n<p>\n<span>\n<sup>c</sup>\n</span>\n<span>\nDepartment of Chemistry and Biochemistry, University of Notre Dame, USA\n<br/>\n<b>E-mail:</b>\n<a href=\"mailto:owiest@nd.edu\">owiest@nd.edu</a> </span>\n</p>\n</div>\n</div>\n<h3>Abstract</h3>\n<div>\n<p>Practical applications of machine learning (ML) to new chemical domains are often hindered by data scarcity. Here we show how data gaps can be circumvented by means of transfer learning that leverages chemically relevant pre-training data. Case studies are presented in which the outcomes of two classes of pericyclic reactions are predicted: [3,3] rearrangements (Cope and Claisen rearrangements) and [4 + 2] cycloadditions (Diels\u2013Alder reactions). Using the graph-based generative algorithm NERF, we evaluate the data efficiencies achieved with different starting models that we pre-trained on datasets of different sizes and chemical scope. We show that the greatest data efficiency is obtained when the pre-training is performed on smaller datasets of mechanistically related reactions (Diels\u2013Alder, Cope and Claisen, Ene, and Nazarov) rather than &gt;50\u00d7 larger datasets of mechanistically unrelated reactions (USPTO-MIT). These small bespoke datasets were more efficient in both low re-training and low pre-training regimes, and are thus recommended alternatives to large diverse datasets for pre-training ML models.</p>\n<p>\n</p>\n</div>\n</article>\n</section>\n<div>\n<div>\n<h2>Supplementary files</h2>\n<div>\n<ul>\n<li>\n<a href=\"https://www.rsc.org/suppdata/d4/dd/d4dd00412d/d4dd00412d1.pdf\">\n<span>\nSupplementary information\n<span> PDF (598K)</span>\n</span>\n</a>\n</li>\n<li>\n<a href=\"https://www.rsc.org/suppdata/d4/dd/d4dd00412d/d4dd00412d2.zip\">\n<span>\nSupplementary information\n<span> ZIP (101K)</span>\n</span>\n</a>\n</li>\n</ul>\n</div>\n<div>\n<h2>Article information</h2>\n<dl>\n<p>\n</p><dt>DOI</dt>\n<dd><a href=\"https://doi.org/10.1039/D4DD00412D\">https://doi.org/10.1039/D4DD00412D</a></dd>\n<p></p>\n<p>\n</p><dt><strong>Article type</strong></dt>\n<dd>Paper</dd>\n<p></p>\n<p>\n</p><dt>Submitted</dt>\n<dd>31 Dec 2024</dd>\n<p></p>\n<p>\n</p><dt>Accepted</dt>\n<dd>26 Mar 2025</dd>\n<p></p>\n<p>\n</p><dt>First published</dt>\n<dd>28 Mar 2025</dd>\n<p></p>\n<div>\n<dt>\n<strong>This article is Open Access</strong>\n</dt>\n<dd>\n<a href=\"http://creativecommons.org/licenses/by/3.0/\">\n</a>\n</dd>\n</div>\n</dl>\n<div>\n<p>\n</p><h3>\n</h3>\n<p></p>\n<div>\n<p><i><strong>Digital Discovery</strong></i>, 2025, Advance Article\n</p> </div>\n</div>\n<div>\n<div>\n<p></p><h3>Permissions</h3><p></p>\n</div>\n<div>\n<p><a href=\"#\">\n</a></p><h3>\nImproving reaction prediction through chemically aware transfer learning\n</h3>\n<p>\nA. Keto, T. Guo, N. G\u00f6nnheimer, X. Zhang, E. H. Krenske and O. Wiest,\n<i>Digital Discovery</i>, 2025, Advance Article\n, <strong>DOI: </strong>10.1039/D4DD00412D\n</p>\n<p>\nThis article is licensed under a <a href=\"https://creativecommons.org/licenses/by/3.0/\">\nCreative Commons Attribution 3.0 Unported Licence</a>. <b>You can use material from this article\nin other publications without requesting further permissions</b> from the RSC, provided that the\ncorrect acknowledgement is given.\n</p>\n<p>\nRead more about <a href=\"https://www.rsc.org/journals-books-databases/journal-authors-reviewers/licences-copyright-permissions/#acknowledgements\">how to correctly acknowledge RSC content</a>.\n</p>\n<p><a href=\"#\"></a>\n</p></div>\n</div>\n<div>\n<p></p><h3>Social activity</h3><p></p>\n</div>\n</div>\n</div>\n<section>\n<h3>Spotlight</h3>\n<h3>Advertisements</h3>\n</section>\n</div>\n</div></div>",
      "url": "https://pubs.rsc.org/en/content/articlelanding/2025/dd/d4dd00412d"
    },
    {
      "title": "Global reactivity models are impactful in industrial synthesis applications",
      "text": "Neves\u00a0et\u00a0al. Journal of Cheminformatics (2023) 15 : 20 \nhttps://doi.org/10.1186/s13321-023-00685-0\nRESEARCH\n\u00a9 The Author(s) 2023, corrected publication 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 \nInternational License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver \n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a \ncredit line to the data.\nOpen Access\nJournal of Cheminformatics\nGlobal reactivity models are impactful \nin\u00a0industrial synthesis applications\nPaulo Neves1*, Kelly McClure2, Jonas Verhoeven1, Natalia Dyubankova1, Ramil Nugmanov1, Andrey Gedich3, \nSairam Menon4\n, Zhicai Shi2 and J\u00f6rg K. Wegner1\nAbstract\nArtifcial Intelligence is revolutionizing many aspects of the pharmaceutical industry. Deep learning models are now \nroutinely applied to guide drug discovery projects leading to faster and improved fndings, but there are still many \ntasks with enormous unrealized potential. One such task is the reaction yield prediction. Every year more than one \nffth of all synthesis attempts result in product yields which are either zero or too low. This equates to chemical and \nhuman resources being spent on activities which ultimately do not progress the programs, leading to a triple loss \nwhen accounting for the cost of opportunity in time wasted. In this work we pre-train a BERT model on more than 16 \nmillion reactions from 4 diferent data sources, and fne tune it to achieve an uncertainty calibrated global yield pre\u0002diction model. This model is an improvement upon state of the art not just from the increase in pre-train data but also \nby introducing a new embedding layer which solves a few limitations of SMILES and enables integration of additional \ninformation such as equivalents and molecule role into the reaction encoding, the model is called BERT Enriched \nEmbedding (BEE). The model is benchmarked on an open-source dataset against a state-of-the-art synthesis focused \nBERT showing a near 20-point improvement in r2 score. The model is fne-tuned and tested on an internal company \ndata benchmark, and a prospective study shows that the application of the model can reduce the total number of \nnegative reactions (yield under 5%) ran in Janssen by at least 34%. Lastly, we corroborate the previous results through \nexperimental validation, by directly deploying the model in an on-going drug discovery project and showing that \nit can also be used successfully as a reagent recommender due to its fast inference speed and reliable confdence \nestimation, a critical feature for industry application.\nIntroduction\nA key aspect of chemical synthesis is to fnd efcient \nways to prepare \u201csets of compounds\u201d or libraries and \nto explore if they contribute to the design of new lead \ncandidates. Tere is a clear role that data analytics and \nmachine learning (ML) techniques have in the design/\nmake/test cycle of medicinal chemistry, and that is also \nthe case for synthesis specifc challenges in the cycle, \nsuch as synthesizability estimation and condition rec\u0002ommendation amongst others, as discussed in [1]. In the \nlast three years there has been tremendous progress in \nbackward synthesis (given product \u2192 predict suitable \nreactants), forward synthesis (given reactants \u2192 estimate \nproduct), as well as retrosynthesis (multi-step backward) \nand library generation (large-scale virtual screening on \nsingle-step forward or multi-step forward), e.g. by dou\u0002bling the accuracy on backward synthesis estimates. \nAlmost all of those methods focus directly or indirectly \nin the estimation of regioselectivity, but do not tackle the \ncore question \u201ccan this compound be synthesized in an \n*Correspondence:\nPaulo Neves\npneves6@its.jnj.com\n1\n In-Silico Discovery and External Innovation (ISDEI), Janssen Research & \nDevelopment, Janssen Pharmaceutica N.V, Beerse, Belgium\n2\n Discovery Chemistry LJ, Janssen Research & Development, Janssen \nPharmaceutica N.V, Philadelphia, United States of America 3\n Software Country, Tbilisi, Georgia\n4\n Pharma R&D Information Tech, Janssen Research & Development, \nJanssen Pharmaceutica N.V, Beerse, Belgium\nNeves\u00a0et\u00a0al. Journal of Cheminformatics (2023) 15 : 20 Page 2 of 11\nisolatable yield?\u201d or practical drug discovery questions \nlike \u201cwhich conditions can synthesize an entire explora\u0002tory 24 compound set?\u201d. Estimating regioselectivity alone \nis insufcient to predict product yield and thus optimize \nsuccess chances for a single or parallel synthesis experi\u0002ment. Current approaches often rely on empirical \u201crobust \nreactions\u201d expert system rules e.g. DOGS [2], SAVI [3] \nand Enamine REAL. 1\n Such expert-systems have an \naverage success rate of around 80% and raise two main \nconcerns. First, there is no relation between reactant or \nreagent combinations that would indicate a higher or \nlower chance of success. Secondly, there is no growth \nopportunity using industry-scale ELN data to improve \nbusiness processes over time.\nA review from Saebi et\u00a0 al. [4] highlights newer data \nanalytics formulations using deep learning, such as yield \nestimates from Schwaller et\u00a0 al. [5] and Saebi et\u00a0 al. [6]. \nOur work addresses both core challenges showcasing \nthat we can improve \u201creaction success\u201d estimates based \non an industry-scale ELN data integration retrospectively \nas well as prospectively. In contrast to historic \u201crobust \nreaction\u201d estimates with a static success rate, our for\u0002mulation allows to estimate which reactant and reagent \ncombinations will lead to higher success, which coupled \nwith confdence estimates and fast inference speeds ena\u0002bles a reagent recommender formulation that can be \nused to optimize single or multiple reactions simulta\u0002neously in parallel medicinal chemistry exercises. Te \napproach used in our work also enables future growth \nopportunities as the chosen data analytics formulation \nbenefts from additional synthesis and condition data \nthat becomes available over time. Although not used in \nour current approach, future data growth is not limited \nto experimental data alone, quantum chemistry simu\u0002lation data can additionally improve scientifc perfor\u0002mance as shown by Guan et\u00a0 al. [7]. Tis enrichment is \nespecially pronounced in regions of low data availability, \nimproving few-shot learning capacity of reaction success \nestimations.\nState of\u00a0the\u00a0art\nTe percentage of isolated reaction yield is depend\u0002ent on a plethora of factors beyond the representation \nof the reaction as reactants, reagents and products, \nand has a high dependency on the process conditions, \nwhich includes reaction parameters such as molar scale, \ntemperature and concentration, as well as purifcation \nmethod and technology used. Tis high dimensional\u0002ity points to the complexity of accurately modelling \na \u201cglobal\u201d reaction space to predict sensible reaction \nyields, as evidenced by initial studies that used chemi\u0002cal descriptors and traditional ML methods [8]. A work \nwhose resulting performance was not sufcient to fulfl \nthe ultimate goal of enabling chemists to predict optimal \nconditions and prioritize compounds by highest chance \nof synthesis success. Te modelling exercise is even \nmore challenged by datase...",
      "url": "https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-023-00685-0.pdf"
    },
    {
      "title": "Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining \u2020",
      "text": "Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining - Chemical Science (RSC Publishing) DOI:10.1039/D1SC06515G\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06515g)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05660c)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05792h)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D1SC06515G](https://doi.org/10.1039/D1SC06515G)(Edge Article)[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022,**13**, 1446-1458\n# Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining[\u2020](#fn1)\nMingjian Wen[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0013-575X)a,Samuel M. Blaua,Xiaowei Xie[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5618-8768)bc,Shyam DwaraknathdandKristin A. Persson[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2495-5509)\\*ef\naEnergy Technologies Area, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA\nbCollege of Chemistry, University of California, Berkeley, CA 94720, USA\ncMaterials Science Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA\ndLuxembourg Institute of Science and Technology, Luxembourg\neDepartment of Materials Science and Engineering, University of California, Berkeley, CA 94720, USA\nfMolecular Foundry, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA. E-mail:[kapersson@lbl.gov](mailto:kapersson@lbl.gov)\nReceived 22nd November 2021, Accepted 9th January 2022\nFirst published on 11th January 2022\n## Abstract\nMachine learning (ML) methods have great potential to transform chemical discovery by accelerating the exploration of chemical space and drawing scientific insights from data. However, modern chemical reaction ML models, such as those based on graph neural networks (GNNs), must be trained on a large amount of labelled data in order to avoid overfitting the data and thus possessing low accuracy and transferability. In this work, we propose a strategy to leverage unlabelled data to learn accurate ML models for small labelled chemical reaction data. We focus on an old and prominent problem\u2014classifying reactions into distinct families\u2014and build a GNN model for this task. We first pretrain the model on unlabelled reaction data using unsupervised contrastive learning and then fine-tune it on a small number of labelled reactions. The contrastive pretraining learns by making the representations of two augmented versions of a reaction similar to each other but distinct from other reactions. We propose chemically consistent reaction augmentation methods that protect the reaction center and find they are the key for the model to extract relevant information from unlabelled data to aid the reaction classification task. The transfer learned model outperforms a supervised model trained from scratch by a large margin. Further, it consistently performs better than models based on traditional rule-driven reaction fingerprints, which have long been the default choice for small datasets, as well as those based on reaction fingerprints derived from masked language modelling. In addition to reaction classification, the effectiveness of the strategy is tested on regression datasets; the learned GNN-based reaction fingerprints can also be used to navigate the chemical reaction space, which we demonstrate by querying for similar reactions. The strategy can be readily applied to other predictive reaction problems to uncover the power of unlabelled data for learning better models with a limited supply of labels.\n## 1. Introduction\nMachine learning methods, especially deep learning, have significantly expanded a chemist's toolbox, enabling the construction of quantitatively predictive models directly from data without explicitly designing rule-based models using chemical insights and intuitions. They have recently been successfully applied to address challenging chemical reaction problems, ranging from the prediction of reaction and activation energies,[1\u20135](#cit1)reaction products,[6,7](#cit6)and reaction conditions,[8,9](#cit8)as well as designing synthesis routes[10,11](#cit10)to name a few. A key ingredient underlying these successes is that modern machine learning methods excel in extracting the patterns in data from sufficient, labelled training examples.[12](#cit12)It has been shown that the performance of these chemical machine learning models can be systematically improved with the increase of training examples.[1,13](#cit1)Despite various recent efforts to generate large labelled reaction datasets that are suitable for modern machine learning,[3,14\u201317](#cit3)they are typically sparse and still small considering the size of the chemical reaction space.[18](#cit18)Many chemical reaction datasets, especially experimental ones, are rather limited, consisting of only thousands or even hundreds of labelled examples.[19,20](#cit19)For such small datasets, the machine learning models can easily become overfitted, resulting in low accuracy and transferability. Therefore, it would be of interest to seek new approaches to train the models using only a small number of reliable, labelled reactions while still retaining the accuracy.\nWhen the number of labelled reactions is small compared with the complexity of the machine learning model required to perform the task, it helps to seek some other source of information to initialize the feature detectors in the model and then to fine-tune these feature detectors using the limited supply of labels.[21](#cit21)In transfer learning, the source of information is another related supervised learning task that has an abundant number of labelled data. The model transfers beneficial information from the related task to aid its decision-making on the task with limited labels, resulting in improved performance. For example, transfer learning has enabled the molecular transformer to predict reaction outcomes with a small labelled dataset.[22,23](#cit22)Transfer learning, however, still requires a large labelled dataset to train the related task, which often is not readily available. Actually, it is possible to initialize the feature detectors using reactions without any labels at all. Although without explicit labels, unlabelled reactions contain extra information that can be leveraged to learn a better model and they are much easier to obtain. For example, the publicly available USPTO dataset[14](#cit14)contains \u223c3 million reactions, the commercial Reaxys database[24](#cit24)and the CAS database[25](#cit25)have \u223c56 millions and \u223c156 millions records of reactions, respectively. In this work, we present a generic unsupervised learning strategy to distill information from unlabelled chemical reactions. For the purpose of demonstration, we focus on the problem of classifying reactions into distinct families.\nReaction family classification has great value for chemists. It facilitates the communication of complex concepts like how a reaction happens in terms of atomic rearrangement and helps to efficiently navigate the chemical reaction space by systematic indexing of reactions in books and databases.[26\u201328](#cit26)Many iconic rules for reactivity prediction require reactions to be in the same fami...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06515g"
    },
    {
      "title": "Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes",
      "text": "[Jump to main content ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#maincontent) [Jump to site search ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#SearchText)\n\n[Issue 7, 2021](https://pubs.rsc.org/en/journals/journal/qo?issueid=qo008007&type=current)\n\n[![](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=CoverIssue&imageInfo.ImageIdentifier.SerCode=QO&imageInfo.ImageIdentifier.IssueId=QO008007)\\\n\\\nFrom the journal: **Organic Chemistry Frontiers**](https://pubs.rsc.org/en/journals/journal/qo)\n\n## Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes [\u2020](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E\\#fn1)\n\n![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg)\n\n[Yun\\\nZhang](https://pubs.rsc.org/en/results?searchtext=Author%3AYun%20Zhang), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0003-3353-4650)[\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Ling\\\nWang](https://pubs.rsc.org/en/results?searchtext=Author%3ALing%20Wang), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Xinqiao\\\nWang](https://pubs.rsc.org/en/results?searchtext=Author%3AXinqiao%20Wang), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Chengyun\\\nZhang](https://pubs.rsc.org/en/results?searchtext=Author%3AChengyun%20Zhang),_a_[Jiamin\\\nGe](https://pubs.rsc.org/en/results?searchtext=Author%3AJiamin%20Ge),_a_[Jing\\\nTang](https://pubs.rsc.org/en/results?searchtext=Author%3AJing%20Tang),_a_[An\\\nSu](https://pubs.rsc.org/en/results?searchtext=Author%3AAn%20Su)\\*_b_\nand\n[Hongliang\\\nDuan](https://pubs.rsc.org/en/results?searchtext=Author%3AHongliang%20Duan)\\*_a_\n\n[Author affiliations](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n\\\\* Corresponding authors\n\naArtificial Intelligence Aided Drug Discovery Institute, College of Pharmaceutical Sciences, Zhejiang University of Technology, Hangzhou 310014, China\n\n**E-mail:** [hduan@zjut.edu.cn](mailto:hduan@zjut.edu.cn)\n\nbCollege of Chemical Engineering, Zhejiang University of Technology, Hangzhou 310014, China\n\n**E-mail:** [ansu@zjut.edu.cn](mailto:ansu@zjut.edu.cn)\n\n### Abstract\n\nEffective and rapid deep learning method to predict chemical reactions contributes to the research and development of organic chemistry and drug discovery. Despite the outstanding capability of deep learning in retrosynthesis and forward synthesis, predictions based on small chemical datasets generally result in a low accuracy due to an insufficiency of reaction examples. Here, we introduce a new state-of-the-art method, which integrates transfer learning with the transformer model to predict the outcomes of the Baeyer\u2013Villiger reaction which is a representative small dataset reaction. The results demonstrate that introducing a transfer learning strategy markedly improves the top-1 accuracy of the transformer-transfer learning model (81.8%) over that of the transformer-baseline model (58.4%). Moreover, we further introduce data augmentation to the input reaction SMILES, which allows for a better performance and improves the accuracy of the transformer-transfer learning model (86.7%). In summary, both transfer learning and data augmentation methods significantly improve the predictive performance of transformer models, which are powerful methods used in the field of chemistry to eliminate the restriction of limited training data.\n\n![Graphical abstract: Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&imageInfo.ImageIdentifier.ManuscriptID=D0QO01636E&imageInfo.ImageIdentifier.Year=2021)\n\nYou have access to this article\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\nPlease wait while we load your content...\nSomething went wrong. [Try again?](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n[About](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlAbstract)\n\n[Cited by](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlCitation)\n\n[Related](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlRelatedContent)\n\n[Download options Please wait...](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n## Supplementary files\n\n- [Supplementary information\\\nPDF (185K)](https://www.rsc.org/suppdata/d0/qo/d0qo01636e/d0qo01636e1.pdf)\n\n## Article information\n\nDOI[https://doi.org/10.1039/D0QO01636E](https://doi.org/10.1039/D0QO01636E)\n\n**Article type**Research Article\n\nSubmitted26 Dec 2020\n\nAccepted02 Feb 2021\n\nFirst published03 Feb 2021\n\n### Download Citation\n\n_**Org. Chem. Front.**_, 2021, **8**, 1415-1423\n\nBibTexEndNoteMEDLINEProCiteReferenceManagerRefWorksRIS\n\n### Permissions\n\n[Request permissions](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n### Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes\n\nY. Zhang, L. Wang, X. Wang, C. Zhang, J. Ge, J. Tang, A. Su and H. Duan,\n_Org. Chem. Front._, 2021,\u00a0**8**, 1415\n**DOI:** 10.1039/D0QO01636E\n\nTo request permission to reproduce material from this article, please go to the\n[Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD0QO01636E).\n\nIf you are **an author contributing to an RSC publication, you do not need to request permission**\nprovided correct acknowledgement is given.\n\nIf you are **the author of this article, you do not need to request permission to reproduce figures**\n**and diagrams** provided correct acknowledgement is given. If you want to reproduce the whole article\nin a third-party publication (excluding your thesis/dissertation for which permission is not required)\nplease go to the [Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD0QO01636E).\n\nRead more about [how to correctly acknowledge RSC content](https://www.rsc.org/journals-books-databases/journal-authors-reviewers/licences-copyright-permissions/#acknowledgements).\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n### Social activity\n\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/twitter.svg)Tweet](https://twitter.com/intent/tweet/?text=Data+augmentation+and+transfer+learning+strategies+for+reaction+prediction+in+low+chemical+data+regimes+-+now+published+in+Organic+Chemistry+Frontiers&url=https%3a%2f%2fpubs.rsc.org%2fen%2fcontent%2farticlelanding%2f2021%2fqo%2fd0qo01636e)\n\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/wechat.svg)Share](https://pubs.rsc.org/en/Image/GetQrCode?url=https%3A%2F%2Fpubs.rsc.org%2Fen%2Fcontent%2Farticlelanding%2F2021%2Fqo%2Fd0qo01636e)\n\n## Search articles by author\n\nYun Zhang\n\nLing Wang\n\nXinqiao Wang\n\nChengyun Zhang\n\nJiamin Ge\n\nJing Tang\n\nAn Su\n\nHongliang Duan\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\nFetching data from CrossRef.\n\nThis may take some time to load.\n\nLoading related content![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\n\n### Spotlight\n\n### Advertisements",
      "url": "https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E"
    }
  ]
}