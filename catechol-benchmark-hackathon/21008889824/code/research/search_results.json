{
  "query": "What are the best graph neural network architectures for molecular property prediction and how do they achieve state-of-the-art performance?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Graph neural networks (GNNs) have become the dominant paradigm for predicting molecular properties because a molecule can be naturally represented as a graph of atoms (nodes) and bonds (edges).  Recent work shows that augmenting the basic message\u2011passing framework with richer structural priors, 3\u2011D geometry, and interpretability modules yields the current state\u2011of\u2011the\u2011art (SOTA) results on benchmark datasets such as PCQM4Mv2 and OGB\u2011LSC.  \n\nThe most performant architectures reported in the literature include **Kolmogorov\u2013Arnold GNNs (KA\u2011GNNs)**, which embed the Kolmogorov\u2013Arnold representation theorem into the aggregation step, enabling the network to capture complex, high\u2011order interactions while remaining computationally efficient; experiments demonstrate consistent out\u2011performance of conventional GNNs in both accuracy and speed\u202f([nature.com](https://www.nature.com/articles/s42256-025-01087-7)).  **Chain\u2011aware GNNs** further improve representation by explicitly modelling linear chain substructures and learning a dedicated embedding for the central atom of each chain, which helps the model distinguish between similar topologies that differ in chain connectivity\u202f([oup.com](https://academic.oup.com/bioinformatics/article/40/10/btae574/7818417)).  **FragNet** adds four layers of interpretability\u2014atom, bond, fragment, and fragment\u2011connection importance\u2014while retaining prediction accuracy comparable to the best\u2011performing black\u2011box models; the fragment\u2011level attention guides the network toward chemically meaningful substructures\u202f([arxiv.org](https://arxiv.org/html/2410.12156v1)).  \n\nHybrid message\u2011passing/transformer designs have also set new records.  **GPS++** combines a deep (16\u2011layer) message\u2011passing neural network with a Transformer that ingests 3\u2011D atom coordinates and an auxiliary denoising task; this synergy enables the model to learn both local chemical environments and long\u2011range spatial relationships, achieving a mean absolute error of\u202f0.0719\u202fMAE on the PCQM4Mv2 test split and winning the Open Graph Benchmark Large\u2011Scale Challenge\u202f([arxiv.org](https://arxiv.org/abs/2212.02229)).  A closely related architecture, **MoleculeFormer**, integrates a graph convolutional network with a Transformer encoder, leveraging the same geometric cues to boost performance on a variety of property prediction tasks\u202f([nature.com](https://www.nature.com/articles/s42003-025-09064-x)).  \n\nTogether, these advances illustrate that the best GNNs for molecular property prediction are those that (1) enrich the message\u2011passing step with mathematically grounded interaction models (KA\u2011GNN, chain\u2011aware GNN), (2) provide chemically interpretable attention over fragments (FragNet), and (3) fuse graph\u2011level reasoning with powerful Transformer\u2011based global context and 3\u2011D geometry (GPS++, MoleculeFormer).  By capturing both fine\u2011grained local chemistry and long\u2011range spatial effects, these architectures consistently achieve SOTA performance on modern molecular benchmarks.",
      "url": ""
    },
    {
      "title": "Kolmogorov\u2013Arnold graph neural networks for molecular property ...",
      "text": "Kolmogorov\u2013Arnold graph neural networks for molecular property prediction | Nature Machine Intelligence\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Machine Intelligence](https://media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-3a46d3127519f23b44cac085d4e82c58.svg)](https://www.nature.com/natmachintell)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42256-025-01087-7?error=cookies_not_supported&code=64573132-1a09-4a66-8d15-1200342aedf1)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42256)\n* [RSS feed](https://www.nature.com/natmachintell.rss)\nKolmogorov\u2013Arnold graph neural networks for molecular property prediction\n[Download PDF](https://www.nature.com/articles/s42256-025-01087-7.pdf)\n[Download PDF](https://www.nature.com/articles/s42256-025-01087-7.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:11 August 2025# Kolmogorov\u2013Arnold graph neural networks for molecular property prediction\n* [Longlong Li](#auth-Longlong-Li-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3)[na1](#na1),\n* [Yipeng Zhang](#auth-Yipeng-Zhang-Aff3)[ORCID:orcid.org/0009-0009-2895-0220](https://orcid.org/0009-0009-2895-0220)[3](#Aff3)[na1](#na1),\n* [Guanghui Wang](#auth-Guanghui-Wang-Aff1)[1](#Aff1)&amp;\n* \u2026* [Kelin Xia](#auth-Kelin-Xia-Aff3)[ORCID:orcid.org/0000-0003-4183-0943](https://orcid.org/0000-0003-4183-0943)[3](#Aff3)Show authors\n[*Nature Machine Intelligence*](https://www.nature.com/natmachintell)**volume7**,pages1346\u20131354 (2025)[Cite this article](#citeas)\n* 40kAccesses\n* 22Citations\n* 35Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42256-025-01087-7/metrics)\n### Subjects\n* [Applied mathematics](https://www.nature.com/subjects/applied-mathematics)\n* [Computational models](https://www.nature.com/subjects/computational-models)\n## Abstract\nGraph neural networks (GNNs) have shown remarkable success in molecular property prediction as key models in geometric deep learning. Meanwhile, Kolmogorov\u2013Arnold networks (KANs) have emerged as powerful alternatives to multi-layer perceptrons, offering improved expressivity, parameter efficiency and interpretability. To combine the strengths of both frameworks, we propose Kolmogorov\u2013Arnold GNNs (KA-GNNs), which integrate KAN modules into the three fundamental components of GNNs: node embedding, message passing and readout. We further introduce Fourier-series-based univariate functions within KAN to enhance function approximation and provide theoretical analysis to support their expressiveness. Two architectural variants, KA-graph convolutional networks and KA-augmented graph attention networks, are developed and evaluated across seven molecular benchmarks. Experimental results show that KA-GNNs consistently outperform conventional GNNs in terms of both prediction accuracy and computational efficiency. Moreover, our models exhibit improved interpretability by highlighting chemically meaningful substructures. These findings demonstrate that KA-GNNs offer a powerful and generalizable framework for molecular data modelling, drug discovery and beyond.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-59439-1/MediaObjects/41467_2025_59439_Fig1_HTML.png)\n### [Using GNN property predictors as molecule generators](https://www.nature.com/articles/s41467-025-59439-1?fromPaywallRec=false)\nArticleOpen access08 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-023-00751-0/MediaObjects/42256_2023_751_Fig1_HTML.png)\n### [Calibrated geometric deep learning improves kinase\u2013drug binding predictions](https://www.nature.com/articles/s42256-023-00751-0?fromPaywallRec=false)\nArticle06 November 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-023-00654-0/MediaObjects/42256_2023_654_Fig1_HTML.png)\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\nArticleOpen access04 May 2023\n## Main\nEven with the huge successes in efficient experimental tools and state-of-the-art computational models, drug design and discovery is still a time-consuming and extremely costly process[1](https://www.nature.com/articles/s42256-025-01087-7#ref-CR1). Recently, artificial intelligence, in particular artificial intelligence for sciences, has demonstrated its enormous potential and great power in scientific data analysis. The success of AlphFold models in protein structure prediction has fundamentally changed molecular structural and property analysis, and ushered in a new era of drug design and discovery[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s42256-025-01087-7#ref-CR6). In general, all molecule-based artificial intelligence models fall into two categories, that is, molecular feature-based machine learning and end-to-end deep learning[7](https://www.nature.com/articles/s42256-025-01087-7#ref-CR7). The first category relies on molecular descriptors or fingerprints as input features for machine learning models. The key process is molecular featurization (or feature engineering), which extracts or generates molecular features from structural, physical, chemical or biological properties. Among them, structure-based descriptors or fingerprints, especially those derived from topological methods, have proved highly effective[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42256-025-01087-7#ref-CR11). Integrating these topology-based molecular features with machine learning models has led to notable success in various drug design stages, such as protein\u2013ligand binding affinity prediction[12](https://www.nature.com/articles/s42256-025-01087-7#ref-CR12),[13](https://www.nature.com/articles/s42256-025-01087-7#ref-CR13), protein mutation analysis[14](https://www.nature.com/articles/s42256-025-01087-7#ref-CR14),[15](https://www.nature.com/articles/s42256-025-01087-7#ref-CR15)and others[16](https://www.nature.com/articles/s42256-025-01087-7#ref-CR16),[17](https://www.nature.com/articles/s42256-025-01087-7#ref-CR17).\nThe second category includes end-to-end deep learning models that use various molecular representations, such as simplified molecular input line entry system strings, molecule-based images and volumetric data, or molecular graphs, and adopt architectures such as Transformers, two-dimensional (2D) or three-dimensional (3D) convolutional neural networks (CNNs) and geometric deep learning (GDL)[18](#ref-CR18),[19](#ref-CR19),[20](#ref-CR20),[21](#ref-CR21),[22](#ref-CR22),[23](https://www.nature.com/articles/s42256-025-01087-7#ref-CR23). In particular, GDL models, such as graph convolutional networks (GCNs)[24](https://www.nature.com/articles/s42256-025-01087-7#ref-CR24), graph autoencoders[25](https://www.nature.com/articles/s42256-025-01087-7#ref-CR25), graph transformers[26](https://www.nature.com/articles/s42256-025-01087-7#ref-CR26)and so on, have been widely used in molecular data analysis and drug design. However, traditional covalent-bond-based molecular graph representations have various limitations, while incorporating non-covalent ...",
      "url": "https://www.nature.com/articles/s42256-025-01087-7"
    },
    {
      "title": "Chain-aware graph neural networks for molecular property prediction",
      "text": "Just a moment...\n# academic.oup.com\nVerify you are human by completing the action below.\nacademic.oup.com needs to review the security of your connection before proceeding.\nVerification successful\nWaiting for academic.oup.com to respond...\nRay ID:`9bd2c4865f27ad01`\nPerformance &amp; security by[Cloudflare](https://www.cloudflare.com?utm_source=challenge&amp;utm_campaign=m)",
      "url": "https://academic.oup.com/bioinformatics/article/40/10/btae574/7818417"
    },
    {
      "title": "FragNet: A Graph Neural Network for Molecular Property Prediction ...",
      "text": "FragNet: A Graph Neural Network for Molecular Property Prediction with Four Layers of Interpretability\n[1]\\\\fnmGihan\\\\surPanapitiya\n[1]\\\\fnmEmily G\\\\surSaldanha\n[1]\\\\orgdivPacific Northwest National Laboratory,\\\\cityRichland,\\\\stateWA,\\\\postcode99354,\\\\countryUnited States\n# FragNet: A Graph Neural Network for Molecular Property Prediction with Four Layers of Interpretability\n[gihan.panapitiya@pnnl.gov](mailto:gihan.panapitiya@pnnl.gov)\\\\fnmPeiyuan\\\\surGao[peiyuan.gao@pnnl.gov](mailto:peiyuan.gao@pnnl.gov)\\\\fnmC Mark\\\\surMaupin[mark.maupin@pnnl.gov](mailto:mark.maupin@pnnl.gov)[emily.saldanha@pnnl.gov](mailto:emily.saldanha@pnnl.gov)\\*\n###### Abstract\nMolecular property prediction is a crucial step in many modern-day scientific applications including drug discovery and energy storage material design. Despite the availability of numerous machine learning models for this task, we are lacking in models that provide both high accuracies and interpretability of the predictions. We introduce the FragNet architecture, a graph neural network not only capable of achieving prediction accuracies comparable to the current state-of-the-art models, but also able to provide insight on four levels of molecular substructures. This model enables understanding of which atoms, bonds, molecular fragments, and molecular fragment connections are critical in the prediction of a given molecular property. The ability to interpret the importance of connections between fragments is of particular interest for molecules which have substructures that are not connected with regular covalent bonds. The interpretable capabilities of FragNet are key to gaining scientific insights from the model\u2019s learned patterns between molecular structure and molecular properties.\n###### keywords:\nGraph Neural Networks, Deep Learning, Attention, Model Interpretability\n## 1Introduction\nMolecular property prediction is a crucial component of material discovery and design in many modern scientific applications including drug design, energy storage material discovery, catalysis and agrochemicals. Despite the availability of a large number of machine learning models> [\n[> 1\n](https://arxiv.org/html/2410.12156v1#bib.bib1)> ]\nfor molecular property prediction, there is often a trade-off in the ability of models to provide highly accurate predictions versus their ability to provide interpretability of their predictions to enable scientific insights. In this work, we introduce a graph neural network architecture called FragNet which is not only capable of achieving prediction accuracies comparable to or exceeding the accuracies of the current state-of-the-art models but also has the ability to provide insight into four levels of molecular substructures which are the atoms, bonds, fragments and fragment connections. In other words, with this model, one can understand which atoms, bonds, molecular fragments and also which connections between molecular fragments play critical role in predicting a given molecular property. By enhancing the model with the ability to reason about the connections between fragments, we provide improved representations for molecules with substructures that are not connected with regular covalent bonds, such as salts and complexes.\nThere are several prior works which implement deep learning models which leverage attention mechanisms which can provide values for different types of molecular substructures. The AttentiveFP> [\n[> 2\n](https://arxiv.org/html/2410.12156v1#bib.bib2)> ]\nand MoGAT> [\n[> 3\n](https://arxiv.org/html/2410.12156v1#bib.bib3)> ]\nmodels provide atom level importance values. The work by Wu et al.> [\n[> 4\n](https://arxiv.org/html/2410.12156v1#bib.bib4)> ]\nprovides fragment level attention. However, their work is limited in the range of molecular properties on which it evaluates, as it only provides results for the ESOL dataset> [\n[> 5\n](https://arxiv.org/html/2410.12156v1#bib.bib5)> ]\nusing a random train-test split configuration. In contrast, our work provides prediction results for several benchmark datasets in MoleculeNet> [\n[> 6\n](https://arxiv.org/html/2410.12156v1#bib.bib6)> ]\nwhile using more challenging scaffold splitting method> [\n[> 7\n](https://arxiv.org/html/2410.12156v1#bib.bib7)> ]\n. This provides a more comprehensive and robust understanding of how the reasoning of the molecular property prediction models is affected by the target property.\nIn addition to demonstrating the strong property prediction performance of the FragNet model across this challenging set of tasks, we also demonstrate the utility of the multi-layered interpretability mechanisms of the through several case studies on a selection of property prediction tasks. We use model attention weights and contribution values to investigate the reasoning used by the model for both individual molecular predictions as well as aggregated across multiple predictions, to identify the key molecular constituents that drive property variations. To further validate the reasoning extracted from the models, we perform a comparative study of FragNet contribution scores with density functional theory (DFT) computations of electrostatic surface potentials. Finally, we develop and release a interactive browser application to make these types of interpretability studies accessible for other molecular property tasks.\n## 2Results\n### 2.1Model\n![Refer to caption](x1.png)Figure 1:FragNet\u2019s architecture and data representation. (a) Atom and Fragment graphs\u2019 edge features are learned from Bond and Fragment connection graphs respectively. b) Initial fragment features for the fragment graph are the summation of the updated atom features that compose the fragment. (c) Illustration of FragNet\u2019s message passing taking place between two non-covalently bonded substructures. Fragment-Fragment connections are also present between adjacent fragments in each non-covalently bonded structure of the compound.Table 1:FragNet performance measured with RMSE on regression tasks in the MoleculeNet benchmark compared with four state-of-the-art baselines. Best results in bold. Lower is better. Reported here aremean\u00b1plus-or-minus\\\\pm\u00b1standard deviationcorresponding to three random seeds. Except for SimSGT, results for other models were obtained from the work of reference> [\n[> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n.|Dataset|ESOL|LIPO|CEP|\nContextPred> [\n[> 9\n](https://arxiv.org/html/2410.12156v1#bib.bib9)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.196\u00b1plus-or-minus\\\\pm\u00b10.037|0.702\u00b1plus-or-minus\\\\pm\u00b10.020|1.243\u00b1plus-or-minus\\\\pm\u00b10.025|\nAttrMask> [\n[> 9\n](https://arxiv.org/html/2410.12156v1#bib.bib9)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.112\u00b1plus-or-minus\\\\pm\u00b10.048|0.730\u00b1plus-or-minus\\\\pm\u00b10.004|1.256\u00b1plus-or-minus\\\\pm\u00b10.000|\nJOAO> [\n[> 10\n](https://arxiv.org/html/2410.12156v1#bib.bib10)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.120\u00b1plus-or-minus\\\\pm\u00b10.019|0.708\u00b1plus-or-minus\\\\pm\u00b10.007|1.293\u00b1plus-or-minus\\\\pm\u00b10.003|\nGraphMVP> [\n[> 11\n](https://arxiv.org/html/2410.12156v1#bib.bib11)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.064\u00b1plus-or-minus\\\\pm\u00b10.045|0.691\u00b1plus-or-minus\\\\pm\u00b10.013|1.2228\u00b1plus-or-minus\\\\pm\u00b10.001|\nMole-BERT> [\n[> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.015\u00b1plus-or-minus\\\\pm\u00b10.030|0.676\u00b1plus-or-minus\\\\pm\u00b10.017|1.232\u00b1plus-or-minus\\\\pm\u00b10.009|\nSimSGT> [\n[> 12\n](https://arxiv.org/html/2410.12156v1#bib.bib12)> ]\n|0.917\u00b1plus-or-minus\\\\pm\u00b10.028|0.670\u00b1plus-or-minus\\\\pm\u00b10.015|1.036\u00b1plus-or-minus\\\\pm\u00b10.022|\nFragNet|0.881\u00b1plus-or-minus\\\\pm\u00b10.011|0.682\u00b1plus-or-minus\\\\pm\u00b10.031|1.092\u00b1plus-or-minus\\\\pm\u00b10.031|\nTable 2:FragNet performance measured withmean\u00b1plus-or-minus\\\\pm\u00b1standard deviationAUC-ROC corresponding to three random seeds on classification tasks in the MoleculeNet benchmark compared with four state-of-the-art baselines. Best result in bold. Higher is better.|Dataset|Clintox|Sider|Tox21|\nContextPred> [\n[> 8\n](https://arxiv.or...",
      "url": "https://arxiv.org/html/2410.12156v1"
    },
    {
      "title": "GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction",
      "text": "# Quantitative Biology > Quantitative Methods\n\n**arXiv:2212.02229** (q-bio)\n\n\\[Submitted on 18 Nov 2022 ( [v1](https://arxiv.org/abs/2212.02229v1)), last revised 6 Dec 2022 (this version, v2)\\]\n\n# Title:GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction\n\nAuthors: [Dominic Masters](https://arxiv.org/search/q-bio?searchtype=author&query=Masters,+D), [Josef Dean](https://arxiv.org/search/q-bio?searchtype=author&query=Dean,+J), [Kerstin Klaser](https://arxiv.org/search/q-bio?searchtype=author&query=Klaser,+K), [Zhiyi Li](https://arxiv.org/search/q-bio?searchtype=author&query=Li,+Z), [Sam Maddrell-Mander](https://arxiv.org/search/q-bio?searchtype=author&query=Maddrell-Mander,+S), [Adam Sanders](https://arxiv.org/search/q-bio?searchtype=author&query=Sanders,+A), [Hatem Helal](https://arxiv.org/search/q-bio?searchtype=author&query=Helal,+H), [Deniz Beker](https://arxiv.org/search/q-bio?searchtype=author&query=Beker,+D), [Ladislav Ramp\u00e1\u0161ek](https://arxiv.org/search/q-bio?searchtype=author&query=Ramp%C3%A1%C5%A1ek,+L), [Dominique Beaini](https://arxiv.org/search/q-bio?searchtype=author&query=Beaini,+D)\n\nView a PDF of the paper titled GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction, by Dominic Masters and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2212.02229)\n\n> Abstract:This technical report presents GPS++, the first-place solution to the Open Graph Benchmark Large-Scale Challenge (OGB-LSC 2022) for the PCQM4Mv2 molecular property prediction task. Our approach implements several key principles from the prior literature. At its core our GPS++ method is a hybrid MPNN/Transformer model that incorporates 3D atom positions and an auxiliary denoising task. The effectiveness of GPS++ is demonstrated by achieving 0.0719 mean absolute error on the independent test-challenge PCQM4Mv2 split. Thanks to Graphcore IPU acceleration, GPS++ scales to deep architectures (16 layers), training at 3 minutes per epoch, and large ensemble (112 models), completing the final predictions in 1 hour 32 minutes, well under the 4 hour inference budget allocated. Our implementation is publicly available at: [this https URL](https://github.com/graphcore/ogb-lsc-pcqm4mv2).\n\n|     |     |\n| --- | --- |\n| Subjects: | Quantitative Methods (q-bio.QM); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2212.02229](https://arxiv.org/abs/2212.02229) \\[q-bio.QM\\] |\n| (or [arXiv:2212.02229v2](https://arxiv.org/abs/2212.02229v2) \\[q-bio.QM\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2212.02229](https://doi.org/10.48550/arXiv.2212.02229) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dominic Masters \\[ [view email](https://arxiv.org/show-email/c075c224/2212.02229)\\] **[\\[v1\\]](https://arxiv.org/abs/2212.02229v1)**\nFri, 18 Nov 2022 18:11:27 UTC (108 KB)\n**\\[v2\\]**\nTue, 6 Dec 2022 16:53:52 UTC (108 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction, by Dominic Masters and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2212.02229)\n- [TeX Source](https://arxiv.org/src/2212.02229)\n- [Other Formats](https://arxiv.org/format/2212.02229)\n\n[view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\nq-bio.QM\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2212.02229&function=prev&context=q-bio.QM)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2212.02229&function=next&context=q-bio.QM)\n\n[new](https://arxiv.org/list/q-bio.QM/new) \\| [recent](https://arxiv.org/list/q-bio.QM/recent) \\| [2022-12](https://arxiv.org/list/q-bio.QM/2022-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2212.02229?context=cs) [cs.LG](https://arxiv.org/abs/2212.02229?context=cs.LG) [q-bio](https://arxiv.org/abs/2212.02229?context=q-bio)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.02229)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.02229)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.02229)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2212.02229) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2212.02229"
    },
    {
      "title": "MoleculeFormer is a GCN-transformer architecture for molecular property prediction",
      "text": "MoleculeFormer is a GCN-transformer architecture for molecular property prediction | Communications Biology\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Communications Biology](https://media.springernature.com/full/nature-cms/uploads/product/commsbio/header-1658ddf63659f01c16ba5b15ce01e2a5.svg)](https://www.nature.com/commsbio)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42003-025-09064-x?error=cookies_not_supported&code=d9c66d60-7566-4ac8-9852-33512cfcc364)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42003)\n* [RSS feed](https://www.nature.com/commsbio.rss)\nMoleculeFormer is a GCN-transformer architecture for molecular property prediction\n[Download PDF](https://www.nature.com/articles/s42003-025-09064-x.pdf)\n[Download PDF](https://www.nature.com/articles/s42003-025-09064-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:25 November 2025# MoleculeFormer is a GCN-transformer architecture for molecular property prediction\n* [Mingyuan Qin](#auth-Mingyuan-Qin-Aff1)[ORCID:orcid.org/0009-0006-8260-7966](https://orcid.org/0009-0006-8260-7966)[1](#Aff1)[na1](#na1),\n* [Ziyan Sun](#auth-Ziyan-Sun-Aff2)[ORCID:orcid.org/0009-0005-3840-0311](https://orcid.org/0009-0005-3840-0311)[2](#Aff2)[na1](#na1),\n* [Lei Feng](#auth-Lei-Feng-Aff3)[3](#Aff3)[na1](#na1),\n* [Chongyin Han](#auth-Chongyin-Han-Aff2)[ORCID:orcid.org/0000-0002-0032-3710](https://orcid.org/0000-0002-0032-3710)[2](#Aff2),\n* [Jingjing Xia](#auth-Jingjing-Xia-Aff1-Aff2-Aff3)[ORCID:orcid.org/0000-0002-2892-4032](https://orcid.org/0000-0002-2892-4032)[1](#Aff1),[2](#Aff2),[3](#Aff3)[na2](#na2)&amp;\n* \u2026* [Lianyi Han](#auth-Lianyi-Han-Aff1)[ORCID:orcid.org/0000-0002-6364-7843](https://orcid.org/0000-0002-6364-7843)[1](#Aff1)[na2](#na2)Show authors\n[*Communications Biology*](https://www.nature.com/commsbio)**volume8**, Article\u00a0number:1668(2025)[Cite this article](#citeas)\n* 2505Accesses\n* 1Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42003-025-09064-x/metrics)\n### Subjects\n* [Computational biology and bioinformatics](https://www.nature.com/subjects/computational-biology-and-bioinformatics)\n* [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n## Abstract\nArtificial intelligence is increasingly important in drug discovery, particularly in molecular property prediction. Graph Neural Networks can model molecular structures as graphs, using structural data to predict molecular properties and biological activities effectively. However, molecular feature optimization and model integration remain challenges. To address these challenges, we propose MoleculeFormer, a multi-scale feature integration model based on Graph Convolutional Network-Transformer architecture. It uses independent Graph Convolutional Network and Transformer modules to extract features from atom and bond graphs while incorporating rotational equivariance constraints and prior molecular fingerprints. The model captures both local and global features and introduces 3D structural information with invariance to rotation and translation. Experiments on 28 datasets show robust performance across various drug discovery tasks, including efficacy/toxicity prediction, phenotype screening, and ADME evaluation. The integration of attention mechanisms enhances interpretability, and the model demonstrates strong noise resistance, establishing MoleculeFormer as an effective, generalizable solution for molecular prediction tasks.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-59439-1/MediaObjects/41467_2025_59439_Fig1_HTML.png)\n### [Using GNN property predictors as molecule generators](https://www.nature.com/articles/s41467-025-59439-1?fromPaywallRec=false)\nArticleOpen access08 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-92218-y/MediaObjects/41598_2025_92218_Fig1_HTML.png)\n### [Integrating convolutional layers and biformer network with forward-forward and backpropagation training](https://www.nature.com/articles/s41598-025-92218-y?fromPaywallRec=false)\nArticleOpen access28 February 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-024-01155-w/MediaObjects/42004_2024_1155_Fig1_HTML.png)\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://www.nature.com/articles/s42004-024-01155-w?fromPaywallRec=false)\nArticleOpen access05 April 2024\n## Introduction\nIn recent years, AI-based approaches have greatly improved the accuracy and efficiency of predicting small molecule properties. These methods utilize machine learning and deep learning techniques to dissect the relationship between molecular structures and their properties, including physicochemical properties, and other relevant characteristics of small molecules, thereby predicting biological activities.\nWith advancements in computational power and the widespread availability of big data, AI-based approaches can comprehensively and rapidly explore molecular information space, uncovering patterns and regularities hidden within large datasets. These methods include but are not limited to QSAR and QSPR[1](https://www.nature.com/articles/s42003-025-09064-x#ref-CR1),[2](https://www.nature.com/articles/s42003-025-09064-x#ref-CR2), which effectively handle complex relationships between molecular structures and multiple properties. Such capabilities have established AI as a transformative tool for drug discovery, material design, environmental science, and other fields.\nEmbedding small molecule features remains a critical step, emphasizing the importance of extracting relevant molecular characteristics as a key research direction[3](https://www.nature.com/articles/s42003-025-09064-x#ref-CR3). This process includes mainly molecular representation and atomic representation. Both approaches take SMILES as input and ultimately output the feature representation of the entire molecule.\nMolecular representation expresses molecular features through prior knowledge, including molecular descriptors and fingerprints. Common molecular descriptors include physicochemical descriptors, topological descriptors, and quantum chemical descriptors[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s42003-025-09064-x#ref-CR6). Some descriptors that have explicit relationships with specific properties may perform poorly in tasks where prior knowledge is not well-established[7](https://www.nature.com/articles/s42003-025-09064-x#ref-CR7). Molecular descriptors provide detailed physicochemical information through numerical computation, while molecular fingerprints are a more structured encoding method[8](https://www.nature.com/articles/s42003-025-09064-x#ref-CR8). They mainly generate a binary or hashed code by identifying structural fragments, functional groups, or substructures within molecules. The encoding logic and feature content of molecular fingerprints are different. Typically, multiple fingerprints are used to represent a molecule and thus describe its structural patterns. These fingerprint inputs are modeled using both traditional machine learning methods, such as Naive Bayes (NB)[9](https://w...",
      "url": "https://www.nature.com/articles/s42003-025-09064-x"
    },
    {
      "title": "Efficient Learning of Molecular Properties Using Graph Neural ...",
      "text": "Efficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge | ACS Omega\nOpens in a new windowOpens an external websiteOpens an external website in a new window\nClose this dialog\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising.To learn more, view the following link:[Privacy Policy](http://www.acs.org/content/acs/en/privacy.html)\nManage Preferences\nClose Cookie Preferences\nRecently Viewed[**close modal](javascript:void(0))\n[Skip to article](#article__left-side)[Skip to sidebar](#article_content-right)\n* [ACS](http://www.acs.org)\n* [ACS Publications](https://pubs.acs.org/)\n* [C&amp;EN](https://cen.acs.org)\n* [CAS](https://www.cas.org)\n[Access through institution](https://pubs.acs.org/action/ssostart?redirectUri=/doi/10.1021/acsomega.5c07178)\n[Log In](https://pubs.acs.org/action/ssoRequestForLoginPage)\n[![ACS Publications. Most Trusted. Most Cited. Most Read](https://pubs.acs.org/specs/products/achs/images/pubslogo-big.gif)](https://pubs.acs.org/)\n[![ACS Omega](https://pubs.acs.org/cms/10.1021/acsomega/asset/16adb841-3216-db84-0321-adb84103216a/title.png)](https://pubs.acs.org/journal/acsodf)\nEfficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)\n* **Share\nShare on\n* [**Facebook](https://pubs.acs.org/#facebook)\n* [**X](https://pubs.acs.org/#x)\n* [**Wechat](https://pubs.acs.org/#wechat)\n* [**LinkedIn](https://pubs.acs.org/#linkedin)\n* [**Reddit](https://pubs.acs.org/#reddit)\n* [**Email](https://pubs.acs.org/#email)\n* [**Bluesky](https://pubs.acs.org/#bluesky)\n* **Jump to\n* [Abstract](#Abstract)\n* [Introduction](#_i2)\n* [Previous Work](#_i3)\n* [The Graph Neural Network Model](#_i4)\n* [Results](#_i9)\n* [Discussion](#_i21)\n* [Conclusion](#_i22)\n* [Data Availability](#dataAvailabilityNotesSection)\n* [Author Information](#authorInformationSection)\n* [Acknowledgments](#_i24)\n* [Additional Notes](#_i25)\n* [References](#_i26)\n* [Cited By](#citeThis)\n* **ExpandCollapse\n[**More](#)\n**Back to top\n**Close quick search form\n**clear**search\nACS OmegaAll Publications/Website\n[Advanced Search](https://pubs.acs.org/search/advanced)\n**Close\nPublications[**](#)\n## CONTENT TYPES\n* All Types\n## SUBJECTS\nPublications: All Types[**](#)\nClose## NextPrevious\n![Figure 1]()![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n[**Download Hi-Res Image](#)[**Download to MS-PowerPoint](#)[****Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)*ACS Omega*2025, 10, 45, 54421-54429\n[ADVERTISEMENT](http://acsmediakit.org)\n[![Journal Logo](specs/products/achs/releasedAssets/images/loading/loader.gif)](https://pubs.acs.org/journal/acsodf)\n[******Get e-Alerts](#)\n* [![Open Access](https://pubs.acs.org/specs/products/achs/releasedAssets/images/access-control/open-access.svg)](https://acsopenscience.org/researchers/open-access/)\nThis publication is Open Access under the license indicated.[Learn More](https://pubs.acs.org/page/access-types)\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)\n* **Share\nShare on\n* [**Facebook](https://pubs.acs.org/#facebook)\n* [**X](https://pubs.acs.org/#x)\n* [**WeChat](https://pubs.acs.org/#wechat)\n* [**LinkedIn](https://pubs.acs.org/#linkedin)\n* [**Reddit](https://pubs.acs.org/#reddit)\n* [**Email](https://pubs.acs.org/#email)\n* [**Bluesky](https://pubs.acs.org/#bluesky)\n* **Jump to\n* [Abstract](#Abstract)\n* [Introduction](#_i2)\n* [Previous Work](#_i3)\n* [The Graph Neural Network Model](#_i4)\n* [Results](#_i9)\n* [Discussion](#_i21)\n* [Conclusion](#_i22)\n* [Data Availability](#dataAvailabilityNotesSection)\n* [Author Information](#authorInformationSection)\n* [Acknowledgments](#_i24)\n* [Additional Notes](#_i25)\n* [References](#_i26)\n* [Cited By](#citeThis)\n* **ExpandCollapse\nArticleNovember 6, 2025\n# Efficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge\n**Click to copy article linkArticle link copied!\n* Tetiana Lutchyn\nTetiana Lutchyn\nDepartment of Physics and Technology, The Arctic University of Norway, Troms\u00f8 9019, Norway\nMore by[Tetiana Lutchyn](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Tetiana%20Lutchyn)\n* Marie Mardal\nMarie Mardal\nDepartment of Chemistry, The Arctic University of Norway, Troms\u00f8 9019, Norway\nDepartment of Forensic Medicine, University of Copenhagen, 1172 K\u00f8benhavn, Denmark\nMore by[Marie Mardal](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Marie%20Mardal)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-3203-7305](https://orcid.org/0000-0002-3203-7305)\n* Benjamin Ricaud**\\***\nBenjamin Ricaud\nDepartment of Physics and Technology, The Arctic University of Norway, Troms\u00f8 9019, Norway\n**\\***Email:[firstname.lastname@uit.no](mailto:firstname.lastname@uit.no)\nMore by[Benjamin Ricaud](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Benjamin%20Ricaud)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-7611-4448](https://orcid.org/0000-0002-7611-4448)\n[**Open PDF](https://pubs.acs.org/doi/pdf/10.1021/acsomega.5c07178?ref=article_openPDF)\n## ACS Omega\nCite this:*ACS Omega*2025, 10, 45, 54421\u201354429\n**Click to copy citationCitation copied!\n[https://pubs.acs.org/doi/10.1021/acsomega.5c07178](https://pubs.acs.org/doi/10.1021/acsomega.5c07178)\n[https://doi.org/10.1021/acsomega.5c07178](https://doi.org/10.1021/acsomega.5c07178)\nPublishedNovember 6, 2025\n[****](#)\n### Publication History\n* **\nReceived\n22 July 2025\n* **\nAccepted\n27 October 2025\n* **\nRevised\n21 October 2025\n* **\nPublished\nonline6 November 2025\n* **\nPublished\nin issue18 November 2025\nresearch-article\nCopyright \u00a92025 The Authors. Published by American Chemical Society. This publication is licensed under\n[CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n### License Summary\\*\nYou are free toshare(copy and redistribute) this article in any medium or format and toadapt(remix, transform, and build upon) the material for any purpose, even commercially within the parameters below:\n* ![cc licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/cc.svg)\nCreative Com...",
      "url": "https://pubs.acs.org/doi/10.1021/acsomega.5c07178"
    },
    {
      "title": "Applying graph neural network models to molecular property ...",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- View\u00a0**PDF**\n- Download full issue\n\nSearch ScienceDirect\n\n## [Artificial Intelligence Chemistry](https://www.sciencedirect.com/journal/artificial-intelligence-chemistry)\n\n[Volume 2, Issue 1](https://www.sciencedirect.com/journal/artificial-intelligence-chemistry/vol/2/issue/1), June 2024, 100050\n\n# Applying graph neural network models to molecular property prediction using high-quality experimental data [\u2606](https://www.sciencedirect.com/www.sciencedirect.com\\#aep-article-footnote-id1)\n\nAuthor links open overlay panelChenQu, Barry I.Schneider, Anthony J.Kearsley, WalidKeyrouz, Thomas C.Allison\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.aichem.2024.100050](https://doi.org/10.1016/j.aichem.2024.100050) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S2949747724000083&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nOpen access\n\n## Abstract\n\nGraph neural networks have been successfully applied to machine learning models related to molecules and crystals, due to the similarity between a molecule/crystal and a graph. In this paper, we present three models that are trained with high-quality experimental data to predict three molecular properties (Kov\u00e1ts retention index, normal boiling point, and mass spectrum), using the same GNN architecture. We show that graph representations of molecules, combined with deep learning methodologies and high-quality data sets, lead to accurate machine learning models to predict molecular properties.\n\n- Previous article in issue\n- Next article in issue\n\n## Keywords\n\nKov\u00e1ts retention index\n\nBoiling point\n\nMass spectrum\n\nGraph neural network\n\nDeep learning\n\nRecommended articles\n\n[\u2606](https://www.sciencedirect.com/www.sciencedirect.com#baep-article-footnote-id1)\n\nOfficial contribution of the National Institute of Standards and Technology; not subject to copyright in the United States.\n\nPublished by Elsevier B.V.",
      "url": "https://www.sciencedirect.com/science/article/pii/S2949747724000083"
    },
    {
      "title": "Graph neural networks for materials science and chemistry - PMC",
      "text": "Graph neural networks for materials science and chemistry - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1038/s43246-022-00315-6)\n* [](pdf/43246_2022_Article_315.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Springer Nature - PMC COVID-19 Collection logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-phenaturepg.png)\nCommun Mater\n. 2022 Nov 26;3(1):93. doi:[10.1038/s43246-022-00315-6](https://doi.org/10.1038/s43246-022-00315-6)\n# Graph neural networks for materials science and chemistry\n[Patrick Reiser](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Reiser P\"[Author]>)\n### Patrick Reiser\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n2Institute of Nanotechnology, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany\nFind articles by[Patrick Reiser](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Reiser P\"[Author]>)\n1,2,[Marlen Neubert](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Neubert M\"[Author]>)\n### Marlen Neubert\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\nFind articles by[Marlen Neubert](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Neubert M\"[Author]>)\n1,[Andr\u00e9 Eberhard](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Eberhard A\"[Author]>)\n### Andr\u00e9 Eberhard\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\nFind articles by[Andr\u00e9 Eberhard](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Eberhard A\"[Author]>)\n1,[Luca Torresi](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Torresi L\"[Author]>)\n### Luca Torresi\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\nFind articles by[Luca Torresi](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Torresi L\"[Author]>)\n1,[Chen Zhou](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Zhou C\"[Author]>)\n### Chen Zhou\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\nFind articles by[Chen Zhou](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Zhou C\"[Author]>)\n1,[Chen Shao](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Shao C\"[Author]>)\n### Chen Shao\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n6Present Address: Institute for Applied Informatics and Formal Description Systems, Karlsruhe Institute of Technology, Kaiserstr. 89, 76133 Karlsruhe, Germany\nFind articles by[Chen Shao](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Shao C\"[Author]>)\n1,6,[Houssam Metni](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Metni H\"[Author]>)\n### Houssam Metni\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n3ECPM, Universit\u00e9 de Strasbourg, 25 Rue Becquerel, 67087 Strasbourg, France\nFind articles by[Houssam Metni](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Metni H\"[Author]>)\n1,3,[Clint van Hoesel](<https://pubmed.ncbi.nlm.nih.gov/?term=\"van Hoesel C\"[Author]>)\n### Clint van Hoesel\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n4Department of Applied Physics, Eindhoven University of Technology, Groene Loper 19, 5612 AP Eindhoven, The Netherlands\nFind articles by[Clint van Hoesel](<https://pubmed.ncbi.nlm.nih.gov/?term=\"van Hoesel C\"[Author]>)\n1,4,[Henrik Schopmans](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Schopmans H\"[Author]>)\n### Henrik Schopmans\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n2Institute of Nanotechnology, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany\nFind articles by[Henrik Schopmans](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Schopmans H\"[Author]>)\n1,2,[Timo Sommer](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Sommer T\"[Author]>)\n### Timo Sommer\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n5Institute for Theory of Condensed Matter, Karlsruhe Institute of Technology, Wolfgang-Gaede-Str. 1, 76131 Karlsruhe, Germany\n7Present Address: School of Chemistry, Trinity College Dublin, College Green, Dublin 2, Ireland\nFind articles by[Timo Sommer](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Sommer T\"[Author]>)\n1,5,7,[Pascal Friederich](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Friederich P\"[Author]>)\n### Pascal Friederich\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n2Institute of Nanotechnology, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany\nFind articles by[Pascal Friederich](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Friederich P\"[Author]>)\n1,2,\u2709\n* Author information\n* Article notes\n* Copyright and License information\n1Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Am Fasanengarten 5, 76131 Karlsruhe, Germany\n2Institute of Nanotechnology, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany\n3ECPM, Universit\u00e9 de Strasbourg, 25 Rue Becquerel, 67087 Strasbourg, France\n4Department of Applied Physics, Eindhoven University of Technology, Groene Loper 19, 5612 AP Eindhoven, The Netherlands\n5Institute for Theory of Condensed Matter, Karlsruhe Institute of Technology, Wolfgang-Gaede-Str. 1, 76131 Karlsruhe, Germany\n6Present Address: Institute for Applied Informatics and Formal Description Systems, Karlsruhe Institute of Technology, Kaiserstr. 89, 76133 Karlsruhe, Germany\n7Present Address: School of Chemistry, Trinity College Dublin, College Green, Dublin 2, Ireland\n\u2709Corresponding author.\nReceived 2022 May 10; Accepted 2022 Nov 7; Issue date 2022.\n\u00a9The Author(s) 2022\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a cre...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9702700"
    }
  ]
}