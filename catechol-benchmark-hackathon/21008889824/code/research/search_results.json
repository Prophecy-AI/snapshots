{
  "query": "How can aggressive regularization reduce overfitting and improve test set generalization in neural network ensembles for small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Aggressive regularization helps neural\u2011network ensembles avoid memorising tiny training sets, forces the individual members to learn smaller, more stable weights, and encourages diverse predictions, which together lower variance and improve test\u2011set performance on small data\u202f([IBM](https://www.ibm.com/think/topics/regularization),\u202f[Regularized Neural Ensemblers](https://arxiv.org/html/2410.04520v2)).\n\n**Steps to apply aggressive regularization in ensembles for small datasets**\n\n1. **Add strong L1/L2 penalties** \u2013 set a larger weight\u2011decay coefficient (e.g.,\u202f\u03bb\u202f\u2248\u202f0.01\u20130.1) so that small weights are pushed toward zero (L1) or uniformly shrunk (L2), reducing model capacity and overfitting\u202f([LunarTech](https://www.lunartech.ai/blog/mastering-l1-and-l2-regularization-the-definitive-guide-to-preventing-overfitting-in-neural-networks-2)).  \n\n2. **Impose weight\u2011norm constraints** \u2013 enforce a maximum\u2011norm (or L2\u2011norm) bound on each layer\u2019s parameters; this lets you train with higher learning rates while keeping weights bounded, further limiting variance\u202f([Machine Learning Mastery](https://www.machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning)).  \n\n3. **Use aggressive dropout** \u2013 apply dropout rates of 0.3\u20130.5 (or higher) to hidden units of each ensemble member. For very small datasets, monitor validation loss because overly high rates can hurt, but moderate aggression decorrelates learned features and reduces memorisation\u202f([CodeCompass](https://codecompass00.substack.com/p/dropout-a-simple-solution-to-a-complex-problem)).  \n\n4. **Regularize the ensemble itself** \u2013 during training, randomly drop whole base\u2011model predictions (as in \u201cRegularized Neural Ensemblers\u201d) so the meta\u2011learner cannot rely on any single member, which forces diversity and lowers overfitting\u202f([Regularized Neural Ensemblers](https://arxiv.org/html/2410.04520v2)).  \n\n5. **Jointly tune regularization hyper\u2011parameters** \u2013 treat weight decay, dropout probability, and early\u2011stopping patience as a shared hyper\u2011parameter set and optimise them on a partially overlapping hold\u2011out split. Joint optimisation often matches or improves performance compared with tuning each model separately\u202f([On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/html/2511.04160v1)).  \n\n6. **Validate on a dedicated small\u2011validation set** \u2013 because the training set is limited, keep a separate validation split (e.g., 10\u201320\u202f% of data) to monitor the trade\u2011off between training error increase and test\u2011set accuracy gain; stop training when validation loss stops improving (early stopping).  \n\n7. **Evaluate ensemble diversity** \u2013 compute pairwise prediction covariance or disagreement; if diversity is low, increase regularization strength (e.g., higher dropout or stronger weight constraints) until the ensemble shows reduced correlation, which correlates with better generalisation\u202f([Ensemble\u2011based Decorrelation Method](https://ijcai.org/proceedings/2018/0301.pdf)).  \n\nFollowing these steps forces each network in the ensemble to be simpler and more distinct, which mitigates overfitting on limited data and yields higher, more reliable test\u2011set performance.",
      "url": ""
    },
    {
      "title": "Mastering L1 and L2 Regularization: The Definitive Guide to ...",
      "text": "Mastering L1 and L2 Regularization: The Definitive Guide to Preventing Overfitting in Neural Networks - LUNARTECH\n[\nWebflow Homepage\n![](https://cdn.prod.website-files.com/670b041cc58f983b09ee069a/670ee8378e3c8b52aa62ab4a_Horizontal%20White_1%404x.png)](https://www.lunartech.ai/)\nView dashboard\n* Academy\n* Enterprises\n* Technologies\n* Solutions\n* Company\n![](https://cdn.prod.website-files.com/670b041cc58f983b09ee069a/6727e748ca6a58bff4c754cd_Social%20follow.svg)\n[\nContact\n](https://www.lunartech.ai/contact)[\nTry Free\n](https://app.lunartech.ai/users/onboarding/plans)\n![](https://cdn.prod.website-files.com/670b041cc58f983b09ee069a/6727e748ca6a58bff4c754ee_menu-dark(24x24)%402x.svg)\n[\nBack\n](https://www.lunartech.ai/blog)\nMastering L1 and L2 Regularization: The Definitive Guide to Preventing Overfitting in Neural Networks\nJanuary 18, 2025\n[Artificial Intelligence](https://www.lunartech.ai/directory/artificial-intelligence-horizons)\n[Open Source Resources](https://www.lunartech.ai/directory/open-source-resources)\n[Videos](https://www.lunartech.ai/directory/videos)\n![](https://cdn.prod.website-files.com/670b041cc58f983b09ee070c/677746a6be69cef1476a2ef7_Leonardo_Phoenix_10_A_futuristic_hightech_abstract_composition_2%20(8).jpg)\n[![Embedded YouTube video](https://img.youtube.com/vi/TDwpx-9M2IE/0.jpg)](https://www.youtube.com/watch?v=TDwpx-9M2IE)\nIn the realm of**machine learning**and**deep learning**, building models that generalize effectively to unseen data is paramount. One of the most persistent challenges in this journey is**overfitting**\u2014a scenario where a model excels on training data but falters on new, unseen datasets. To combat this,**L1 and L2 regularization**have emerged as essential techniques that enhance model robustness and generalization. This comprehensive guide delves deep into the mechanics, differences, and practical applications of L1 and L2 regularization, equipping practitioners with the knowledge to optimize their neural networks for superior performance.\n## Chapter 1: The Essence of Regularization in Neural Networks\n**Regularization**serves as a fundamental strategy in neural network training, designed to prevent overfitting by introducing constraints that limit the complexity of the model. Without regularization, neural networks, especially those with a vast number of parameters, have a propensity to memorize training data, including its noise and outliers. This memorization undermines the model\u2019s ability to generalize, leading to poor performance on validation and test datasets.\nRegularization techniques like**L1 and L2**work by adding penalty terms to the loss function, which discourage the model from assigning excessively large weights to any single feature. This encourages the network to develop more generalized and balanced representations, enhancing its ability to perform well on diverse datasets. By controlling the magnitude of weights, regularization ensures that the model remains both robust and efficient, avoiding the pitfalls of over-complexity.\nMoreover, regularization contributes to the interpretability of models. In high-dimensional datasets, where the number of features can be overwhelming, regularization helps in identifying the most influential features, thereby simplifying the model and making it more comprehensible. This is particularly valuable in domains like healthcare and finance, where understanding the underlying factors driving predictions is as crucial as the predictions themselves.\nIn essence, regularization acts as a guardian of model simplicity and generalization, ensuring that neural networks remain effective and reliable across a spectrum of applications. By mitigating overfitting, regularization enhances the model\u2019s predictive power, making it a cornerstone technique in the arsenal of machine learning practitioners.\n## Chapter 2: Unveiling L1 Regularization\n**L1 regularization**, also known as**Lasso Regression**, introduces a penalty equal to the absolute value of the magnitude of coefficients. Mathematically, this is represented as adding the sum of the absolute weights multiplied by a regularization parameter \u03bb\\\\lambda\u03bb to the loss function. This penalty encourages sparsity in the model, effectively driving some weights to zero.\nThe primary advantage of L1 regularization lies in its ability to perform**feature selection**. By pushing less important feature weights to zero, L1 simplifies the model by eliminating irrelevant or redundant features. This not only enhances the model&#x27;s interpretability but also reduces computational complexity, making it more efficient for real-time applications. For instance, in text classification tasks with thousands of word features, L1 regularization can identify and retain only the most significant words, streamlining the model without sacrificing accuracy.\nFurthermore, L1 regularization is particularly effective in high-dimensional datasets where the number of features exceeds the number of observations. In such scenarios, traditional models without regularization can become unstable and prone to overfitting. L1 regularization mitigates this by enforcing sparsity, ensuring that the model remains robust and generalizes well to new data.\nHowever, L1 regularization is not without its challenges. While it excels in feature selection, it can sometimes be unstable in the presence of highly correlated features, arbitrarily selecting one feature from a group of correlated ones while ignoring others. This limitation necessitates careful consideration of the data structure and may require complementary techniques to ensure comprehensive feature representation.\nIn summary,**L1 regularization**is a powerful tool for simplifying neural networks through feature selection, enhancing both model interpretability and efficiency. Its ability to induce sparsity makes it indispensable in high-dimensional and complex modeling tasks, providing a balanced approach to preventing overfitting while maintaining essential predictive capabilities.\n## Chapter 3: Demystifying L2 Regularization\n**L2 regularization**, commonly referred to as**Ridge Regression**, introduces a penalty equal to the square of the magnitude of coefficients. This is achieved by adding the sum of the squared weights multiplied by a regularization parameter \u03bb\\\\lambda\u03bb to the loss function. Unlike L1, L2 regularization does not promote sparsity but instead**shrinkages weights towards zero**in a more uniform manner.\nThe core strength of L2 regularization lies in its ability to**distribute the penalty**across all weights, ensuring that no single feature dominates the model. This proportional shrinkage prevents the model from becoming overly reliant on any particular feature, fostering a more balanced and generalized representation of the data. In applications like image recognition, where each pixel contributes to the overall prediction, L2 regularization ensures that the model leverages the collective information without overemphasizing specific areas.\nAdditionally, L2 regularization enhances the**numerical stability**of the model. By controlling the magnitude of weights, it mitigates issues like**gradient explosion**, where large weights can lead to unstable and divergent training processes. This stability is crucial in deep neural networks, where the accumulation of weights across multiple layers can significantly impact the training dynamics and overall performance.\nMoreover, L2 regularization is particularly effective in scenarios where all features are expected to contribute to the outcome. Unlike L1, which zeroes out less important features, L2 maintains a balance by slightly reducing the influence of all features, thereby preserving the integrity and richness of the model&#x27;s representation.\nHowever, the uniform shrinkage of L2 regularization means it does not inherently perform feature selection. This can be a drawback in high-dimensional settings where identifying key features is essential. In such cas...",
      "url": "https://www.lunartech.ai/blog/mastering-l1-and-l2-regularization-the-definitive-guide-to-preventing-overfitting-in-neural-networks-2"
    },
    {
      "title": "What Is Regularization? | IBM",
      "text": "What Is Regularization? | IBM\n# What is regularization?\n## Authors\nJacob Murel Ph.D.\nSenior Technical Content Creator\n[Eda Kavlakoglu](https://www.ibm.com/think/author/eda-kavlakoglu.html)\nBusiness Development + Partnerships\nIBM Research\n## What is regularization?\nRegularization is a set of methods for reducing overfitting in machine learning models. Typically, regularization trades a marginal decrease in training accuracy for an increase in generalizability.\nRegularization encompasses a range of techniques to correct for[overfitting](https://www.ibm.com/topics/overfitting)in[machine learning](https://www.ibm.com/think/topics/machine-learning)models. As such, regularization is a method for increasing a model\u2019s generalizability\u2014that is, its ability to produce accurate predictions on new[datasets](https://www.ibm.com/think/topics/dataset).1Regularization provides this increased generalizability at the sake of increased training error. In other words, regularization methods typically lead to less accurate predictions on[training data](https://www.ibm.com/think/topics/training-data)but more accurate predictions on test data.\nRegularization differs from optimization. Essentially, the former increases model generalizability while the latter increases model training accuracy. Both are important concepts in machine learning and data science.\nThere are many forms of regularization. Anything in the way of a complete guide requires a much longer book-length treatment. Nevertheless, this article provides an overview of the theory necessary to understand regularization\u2019s purpose in machine learning as well as a survey of several popular regularization techniques.\n### Bias-variance tradeoff\nThis concession of increased training error for decreased testing error is known as bias-variance tradeoff. Bias-variance tradeoff is a well-known problem in machine learning. It\u2019s necessary to first define \u201cbias\u201d and \u201cvariance.\u201d To put it briefly:\n* **Bias**measures the average difference between predicted values and true values. As bias increases, a model predicts less accurately on a training dataset. High bias refers to high error in training.\n* **Variance**measures the difference between predictions across various realizations of a given model. As variance increases, a model predicts less accurately on unseen data. High variance refers to high error during testing and validation.\nBias and variance thus inversely represent model accuracy on training and test sets respectively.2Obviously, developers aim to reduce both model bias and variance. Simultaneous reduction in both is not always possible, resulting in the need for regularization. Regularization decreases model variance at the cost of increased bias.\n### Regression model fits\nBy increasing bias and decreasing variance, regularization resolves model overfitting. Overfitting occurs when error on training data decreases while error on testing data ceases decreasing or begins increasing.3In other words, overfitting describes models with low bias and high variance. However, if regularization introduces too much bias, then a model will underfit.\nDespite its name,[underfitting](https://www.ibm.com/think/topics/underfitting)does not denote overfitting\u2019s opposite. Rather underfitting describes models characterized by high bias and high variance. An underfitted model produces unsatisfactorily erroneous predictions during training and testing. This often results from insufficient training data or[parameters](https://www.ibm.com/think/topics/model-parameters).\nRegularization, however, can potentially lead to model underfitting as well. If too much bias is introduced through regularization, model variance can cease to decrease and even increase. Regularization may have this effect particularly on simple models, that is, models with few parameters. In determining the type and degree of regularization to implement, then, one must consider a model\u2019s complexity, dataset, and so forth.4\nIndustry newsletter\n### The latest AI trends, brought to you by experts\nGet curated insights on the most important\u2014and intriguing\u2014AI news. Subscribe to our weekly Think newsletter. See the[IBM Privacy Statement](https://www.ibm.com/privacy).\n### Thank you! You are subscribed.\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe[here](https://www.ibm.com/account/reg/signup?formid&#61;news-urx-51525). Refer to our[IBM Privacy Statement](https://www.ibm.com/us-en/privacy)for more information.\n## Types of regularization with linear models\n[Linear regression](https://www.ibm.com/think/topics/linear-regression)and[logistic regression](https://www.ibm.com/think/topics/logistic-regression)are both predictive models underpinning machine learning. Linear regression (or ordinary least squares) aims to measure and predict the impact of one or more predictors on a given output by finding the best fitting line through provided data points (that is, training data). Logistic regression aims to determine the class probabilities of by way of a binary output given a range of predictors. In other words, linear regression makes continuous quantitative predictions while logistic regression produces discrete categorical predictions.5\nOf course, as the number of predictors increase in either regression model, the input-output relationship is not always straightforward and requires manipulation of the regression formula. Enter regularization. There are three main forms of regularization for regression models. Note that this list is only a brief survey. Application of these regularization techniques in either linear or logistic regression varies minutely.\n* **Lasso regression**(or L1 regularization) is a regularization technique that penalizes high-value, correlated coefficients. It introduces a regularization term (also called, penalty term) into the model\u2019s sum of squared errors (SSE) loss function. This penalty term is the absolute value of the sum of coefficients. Controlled in turn by the hyperparameter lambda (\u03bb), it reduces select feature weights to zero. Lasso regression thereby removes[multicollinear features](https://www.ibm.com/think/topics/multicollinearity)from the model altogether.\n* **Ridge regression**(or[L2 regularization](https://www.ibm.com/think/topics/ridge-regression)) is regularization technique that similarly penalizes high-value coefficients by introducing a penalty term in the SSE loss function. It differs from lasso regression however. First, the penalty term in ridge regression is the squared sum of coefficients rather than the absolute value of coefficients. Second, ridge regression does not enact feature selection. While lasso regression\u2019s penalty term can remove features from the model by shrinking coefficient values to zero, ridge regression only shrinks feature weights toward zero but never to zero.\n* **Elastic net regularization**essentially combines both ridge and lasso regression but inserting both the L1 and L2 penalty terms into the SSE loss function. L2 and L1 derive their penalty term value, respectively, by squaring or taking the absolute value of the sum of the feature weights. Elastic net inserts both of these penalty values into the cost function (SSE) equation. In this way, elastic net addresses multicollinearity while also enabling feature selection.6\nIn statistics, these methods are also dubbed \u201ccoefficient shrinkage,\u201d as they shrink predictor coefficient values in the predictive model. In all three techniques, the strength of the penalty term is controlled by lambda, which can be calculated using various[cross-validation](https://www.ibm.com/docs/en/spss-modeler/18.0.0?topic&#61;settings-cross-validation)techniques.\nMixture of Experts | 9 January, episode 89\n### Decoding AI: Weekly News Roundup\nJoin our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in A...",
      "url": "https://www.ibm.com/think/topics/regularization"
    },
    {
      "title": "Dropout: A Simple Solution to a Complex Problem",
      "text": "[![The Code Compass](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F482a15d5-2e94-4343-b2e9-9cc058622f4e_717x717.png)](https://codecompass00.substack.com/)\n\n# [The Code Compass](https://codecompass00.substack.com/)\n\nSubscribeSign in\n\n# Dropout: A Simple Solution to a Complex Problem\n\n### Learn about dropout, its variants and how to apply them in your next project\n\n[![CodeCompass's avatar](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc68eb12c-3638-4aa1-ac3b-0317e5f2f2df_677x670.png)](https://substack.com/@codecompass00)\n\n[CodeCompass](https://substack.com/@codecompass00)\n\nApr 10, 2024\n\n\u2219 Paid\n\n6\n\n[2](https://codecompass00.substack.com/p/dropout-a-simple-solution-to-a-complex-problem/comments)\n1\n\n[Share](javascript:void(0))\n\n_Get a list of personally curated and freely accessible ML, NLP, and computer vision resources for FREE on newsletter sign-up._\n\nSubscribe\n\n_To read more on this topic see the references section at the bottom. Consider [sharing this post](https://open.substack.com/pub/codecompass00/p/dropout-a-simple-solution-to-a-complex-problem?r=rcorn&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true) with someone who wants to know more about machine learning._\n\n[![](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa29b9327-fbf4-416d-b9e3-88732b21cdaf_898x332.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa29b9327-fbf4-416d-b9e3-88732b21cdaf_898x332.png)\n\n* * *\n\n## **0\\. Introduction**\n\nOverfitting \\[10\\] is a major obstacle in training deep neural networks. It occurs when the network \u201cmemorizes\u201d the training data too well, leading to poor performance on unseen examples. Dropout, a simple and elegant technique introduced in 2012, offers a powerful solution to this problem.\n\n[![](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810397e9-d46f-4309-a2aa-0dafa4d071e0_631x426.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810397e9-d46f-4309-a2aa-0dafa4d071e0_631x426.png) Overfitting is a common problem where the model does really well on the training data but does not generalize on unseen data (\u201cvalidation\u201d split).\n\nWe delve into the technical details of (the original) Dropout and its variants, explore their successful applications, and provide practical advice for incorporating it into your own neural network architectures.\n\n## **1\\. Dropout: Tackling Overfitting Head-On**\n\n### **First Introduced**\n\nDropout was introduced by Hinton et al. in their paper \u201cImproving neural networks by preventing co-adaptation of feature detectors\u201d \\[2, 5\\] in 2012. It is one of the de-facto regularizers \\[9\\] in the neural network world.\n\n### **Usefulness**\n\nDropout prevents overfitting by randomly deactivating **neurons** during training, encouraging the network to learn robust features from different subsets of neurons during training. This is done on the activations and not the weights. It is commonly used after fully connected layers.\n\n[![](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1c8d255-89c1-46fb-906d-af9335f21433_782x468.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1c8d255-89c1-46fb-906d-af9335f21433_782x468.png) A model with no dropout vs. when dropout is applied.\n\n### **Technical Details**\n\nDropout introduces randomness during training by probabilistically dropping out neurons, effectively creating an ensemble of thinned networks.\n\nDuring each training iteration, a fraction of the input units (neurons) is set to zero with a specified probability, typically between 0.2 and 0.5. This stochastic process encourages the network to learn robust representations by preventing reliance on specific neurons and promoting feature generalization.\n\n[![](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79c8f9c0-5be3-4f19-9115-d85ef30d0881_766x375.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79c8f9c0-5be3-4f19-9115-d85ef30d0881_766x375.png) The matrix of weights, W without dropout and with dropout. W maps the input neurons to output neurons i.e. y = Wx. Dropped neurons lead to entire columns of the matrix being masked out.\n\nDuring training, to compensate for the deactivated neurons, the remaining activations are scaled up by a factor of 1 / (1 - p), ensuring the expected average activation remains unchanged.\n\nLet\u2019s assume that we set p=0.1 during training. This means when we do many batches of forward pass, each neuron will be masked 10% of the time i.e. on average only 90% of the signal is passed on to the next layer. During training, the output is scaled by 1/(1-0.1) = 1/0.9, so we can offset the fact that only 90% or 0.9 of the signal is passed on to the next layer.\n\nDuring deployment, dropout behaves as an identity function i.e. the output is the input.\n\n### **Difference from Other Dropout Variants**\n\nSets individual activations to zero and not the weight itself. It is applicable to any layer type.\n\nEffective for regularization \\[9\\] and preventing overfitting in various neural network architectures. This technique has been instrumental in the success of numerous deep learning architectures, including the architecture that won the 2012 ImageNet competition, a breakthrough [1](https://codecompass00.substack.com/p/dropout-a-simple-solution-to-a-complex-problem#footnote-1-143361476) in computer vision (Krizhevsky et al., 2012 \\[4\\]).\n\n[![](fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa329b318-4499-4adf-8ae2-d60248fec78a_1325x459.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa329b318-4499-4adf-8ae2-d60248fec78a_1325x459.png)_\u201cWe use dropout in the first two fully-connected layers ... Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.\u201d_ \\[4\\]. AlexNet architecture from \\[4\\].\n\n### **Practical Advice**\n\nStart without any dropout and set up the training pipeline. Once set, when the training loss is lower than the validation loss it signals overfitting. That is a good sign to use dropout.\n\nBegin with a dropout rate between 0.2 and 0.5. Monitor model performance closely during training to detect any signs of underfitting or overfitting, and adjust dropout rates accordingly.\n\nBe cautious when applying dropout to small datasets or networks with fewer parameters, as aggressive dropout rates may lead to worse generalization.\n\n* * *\n\n_Continue reading more:_\n\n[![Uber's Billion Dollar Problem: Predicting ETAs Reliably And Quickly](https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff475ec40-1eda-48a1-9f5a-57544842557e_1358x975.png)](https://codecompass00.substack.com/p/uber-billion-dollar-problem-predicting-eta)\n\n[**Uber's Billion Dollar Problem: Predicting ETAs Reliably And Quickly**](https://codecompass00.substack.com/p/uber-billion-dollar-problem-predicting-eta)\n\n[CodeCompass](https://substack.com/profile/45941603-codecompass)\n\n\u00b7\n\nApril 25, 2024\n\n[Read full story](https://codecompass00.substack.com/p/uber-billion-dollar-problem-predicting-eta)\n\n[![How Netflix Uses Machine Learning To Decide What Content To Create Next For Its 260M Users](https://substackcdn...",
      "url": "https://codecompass00.substack.com/p/dropout-a-simple-solution-to-a-complex-problem"
    },
    {
      "title": "A Gentle Introduction to Weight Constraints in Deep Learning",
      "text": "### [Navigation](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 6, 2019in[Deep Learning Performance](https://machinelearningmastery.com/category/better-deep-learning/)[10](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/#comments)\n\nShare _Tweet_Share\n\nWeight regularization methods like weight decay introduce a penalty to the loss function when training a neural network to encourage the network to use small weights.\n\nSmaller weights in a neural network can result in a model that is more stable and less likely to [overfit the training dataset](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/), in turn having better performance when making a prediction on new data.\n\nUnlike weight regularization, a weight constraint is a trigger that checks the size or magnitude of the weights and scales them so that they are all below a pre-defined threshold. The constraint forces weights to be small and can be used instead of weight decay and in conjunction with more aggressive network configurations, such as [very large learning rates](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/).\n\nIn this post, you will discover the use of weight constraint regularization as an alternative to weight penalties to reduce overfitting in deep neural networks.\n\nAfter reading this post, you will know:\n\n- Weight penalties encourage but do not require neural networks to have small weights.\n- Weight constraints, such as the L2 norm and maximum norm, can be used to force neural networks to have small weights during training.\n- Weight constraints can improve generalization when used in conjunction with other regularization methods like dropout.\n\n**Kick-start your project** with my new book [Better Deep Learning](https://machinelearningmastery.com/better-deep-learning/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n![A Gentle Introduction to Weight Constraints to Reduce Generalization Error in Deep Learning](https://machinelearningmastery.com/wp-content/uploads/2018/11/A-Gentle-Introduction-to-Weight-Constraints-to-Reduce-Generalization-Error-in-Deep-Learning.jpg)\n\nA Gentle Introduction to Weight Constraints to Reduce Generalization Error in Deep Learning\n\nPhoto by [Dawn Ellner](https://www.flickr.com/photos/naturesdawn/2835320842/), some rights reserved.\n\n## Overview\n\n- Alternative to Penalties for Large Weights\n- Force Small Weights\n- How to Use a Weight Constraint\n- Example Uses of Weight Constraints\n- Tips for Using Weight Constraints\n\n## Alternative to Penalties for Large Weights\n\nLarge weights in a neural network are a sign of overfitting.\n\nA network with large weights has very likely learned the statistical noise in the training data. This results in a model that is unstable, and very sensitive to changes to the input variables. In turn, the overfit network has poor performance when making predictions on new unseen data.\n\nA popular and effective technique to address the problem is to update the loss function that is optimized during training to take the size of the weights into account.\n\nThis is called a penalty, as the larger the weights of the network become, the more the network is penalized, resulting in larger loss and, in turn, larger updates. The effect is that the penalty encourages weights to be small, or no larger than is required during the training process, in turn reducing overfitting.\n\nA problem in using a penalty is that although it does encourage the network toward smaller weights, it does not force smaller weights.\n\nA neural network trained with weight regularization penalty may still allow large weights, in some cases very large weights.\n\n## Force Small Weights\n\nAn alternate solution to using a penalty for the size of network weights is to use a weight constraint.\n\nA weight constraint is an update to the network that checks the size of the weights, and if the size exceeds a predefined limit, the weights are rescaled so that their size is below the limit or between a range.\n\nYou can think of a weight constraint as an if-then rule checking the size of the weights while the network is being trained and only coming into effect and making weights small when required. Note, for efficiency, it does not have to be implemented as an if-then rule and often is not.\n\nUnlike adding a penalty to the loss function, a weight constraint ensures the weights of the network are small, instead of mearly encouraging them to be small.\n\nIt can be useful on those problems or with networks that resist other regularization methods, such as weight penalties.\n\nWeight constraints prove especially useful when you have configured your network to use alternative regularization methods to weight regularization and yet still desire the network to have small weights in order to reduce overfitting. One often-cited example is the use of a weight constraint regularization with [dropout regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n\n> Although dropout alone gives significant improvements, using dropout along with \\[weight constraint\\] regularization, \\[\u2026\\] provides a significant boost over just using dropout.\n\n\u2014 [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.\n\n### Want Better Results with Deep Learning?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nDownload Your FREE Mini-Course\n\n## How to Use a Weight Constraint\n\nA constraint is enforced on each node within a layer.\n\nAll nodes within the layer use the same constraint, and often multiple hidden layers within the same network will use the same constraint.\n\nRecall that when we talk about the [vector norm](https://machinelearningmastery.com/vector-norms-machine-learning/) in general, that this is the magnitude of the vector of weights in a node, and by default is calculated as the L2 norm, e.g. the square root of the sum of the squared values in the vector.\n\nSome examples of constraints that could be used include:\n\n- Force the vector norm to be 1.0 (e.g. the unit norm).\n- Limit the maximum size of the vector norm (e.g. the maximum norm).\n- Limit the minimum and maximum size of the vector norm (e.g. the min\\_max norm).\n\nThe maximum norm, also called max-norm or maxnorm, is a popular constraint because it is less aggressive than other norms such as the unit norm, simply setting an upper bound.\n\n> Max-norm regularization has been previously used \\[\u2026\\] It typically improves the performance of stochastic gradient descent training of deep neural nets \u2026\n\n\u2014 [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.\n\nWhen using a limit or a range, a hyperparameter must be specified. Given that weights are small, the hyperparameter too is often a small integer value, such as a value between 1 and 4.\n\n> \u2026 we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4.\n\n\u2014 [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html), 2014.\n\nIf the norm exceeds the specified range or limit, the weights are rescaled or normalized such that their magnitude is below the specified parameter or within the specified range.\n\n> If a weight-update violates this constraint, we renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty prevents weights from growing very large no matter how large the p...",
      "url": "https://www.machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning"
    },
    {
      "title": "On Joint Regularization and Calibration in Deep Ensembles",
      "text": "On Joint Regularization and Calibration in Deep Ensembles\n# On Joint Regularization and Calibration in Deep Ensembles\nLaurits Fredsgaardlaula@dtu.dk\nDepartment of Applied Mathematics and Computer Science\nTechnical University of DenmarkMikkel N. Schmidtmnsc@dtu.dk\nDepartment of Applied Mathematics and Computer Science\nTechnical University of Denmark\n###### Abstract\nDeep ensembles are a powerful tool in machine learning, improving both model performance and uncertainty calibration. While ensembles are typically formed by training and tuning models individually, evidence suggests that jointly tuning the ensemble can lead to better performance. This paper investigates the impact of jointly tuning weight decay, temperature scaling, and early stopping on both predictive performance and uncertainty quantification. Additionally, we propose a partially overlapping holdout strategy as a practical compromise between enabling joint evaluation and maximizing the use of data for training. Our results demonstrate that jointly tuning the ensemble generally matches or improves performance, with significant variation in effect size across different tasks and metrics. We highlight the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering an attractive practical solution. We believe our findings provide valuable insights and guidance for practitioners looking to optimize deep ensemble models. Code is available at:[https://github.com/lauritsf/ensemble-optimality-gap](https://github.com/lauritsf/ensemble-optimality-gap)\n## 1Introduction\nDeep ensembles are a simple and practical method that combines multiple independently trained models to enhance predictive accuracy, improve robustness, and provide uncertainty estimates> (\n> lakshminarayananSimpleScalablePredictive2017\n> )\n. Their effectiveness relies on having diverse members that have uncorrelated errors, which reduces variance and minimizes the impact of individual model mistakes> (\n> hansenNeuralNetworkEnsembles1990\n> ; > kroghStatisticalMechanicsEnsemble1997\n> )\n.\nWhile individual models in an ensemble may differ in architecture, training set, and other factors, a common practice is to train ensembles using the same model architecture, where the only differences are the initializations and the order in which the training examples are presented. This also offers a simple and effective method for selecting regularization hyperparameters such as weight decay and dropout: These settings can be optimized for a single model, typically through grid search, and then used to train the ensemble members independently. Similarly, if post-hoc calibration or early stopping is used, it is often applied to each ensemble member independently.\nThis approach simplifies the tuning process, but while an ensemble of well-regularized and well-calibrated models will generally perform well, it may not be the optimal strategy.\nThis potential mismatch, which we term the*ensemble optimality gap*, arises because what is optimal for a single model is not necessarily optimal for the final ensemble. The theoretical basis for this is that the expected loss of an ensemble is fundamentally different from the average loss of its members, often involving a beneficial diversity term> (\n> woodUnifiedTheoryDiversity2023\n> ; > kroghStatisticalMechanicsEnsemble1997\n> )\n.\nConsequently, using a single model\u2019s validation performance as a proxy for the final ensemble\u2019s test performance creates a generalization mismatch. This flawed validation process prevents the discovery of hyperparameters that might make beneficial trade-offs, such as sacrificing marginal individual model performance for a larger gain in diversity. While other research focuses on explicitly inducing diversity, our work investigates a simpler premise: closing the optimality gap by ensuring the validation objective correctly mirrors the final deployment objective\u2014the ensemble itself.\nPrevious work has shown that allowing individual models within an ensemble to overfit to a certain extent can lead to improvements in both prediction accuracy> (\n> sollichLearningEnsemblesHow1995\n> )\nand calibration> (\n> wuShouldEnsembleMembers2021\n> )\n. In practice, however, this is often disregarded because tuning the complete ensemble by holdout or cross-validation can be a considerable computational expense or does not seamlessly fit into existing workflows. Hyperparameter tuning (such as grid search) requires training an entire ensemble for each parameter combination, scaling the computational cost with the ensemble size. However, methods like early stopping can be validated during parallel training with minimal added cost, whereas post hoc techniques like temperature scaling can be evaluated on the ensemble without additional expense.\nIn this paper, we systematically explore the ensemble optimality gap across three key aspects of deep ensemble training and calibration: weight decay tuning, temperature scaling, and early stopping. Our objective is to assess the magnitude of this effect in common settings and demonstrate how it can be mitigated by optimizing for ensemble performance. In particular, we examine how the optimality gap influences model accuracy, uncertainty calibration, and predictive likelihood, as well as investigate its impact on ensemble diversity.\nTo enhance ensemble diversity, a well-known approach is to train the ensemble using a k-fold cross-validation strategy, where each ensemble member is validated on separate holdout data. While this approach can improve the estimation of generalization error for a single model, it prevents direct validation of the full ensemble performance, as a common validation data must be held out for all ensemble members. This leads to a choice between increasing ensemble diversity and having the ability to tune the ensemble as a whole. Both strategies have been demonstrated to lead to improved performance. We explore a strategy that balances these factors by using partially overlapping holdout sets across ensemble members.\nFinally, when training standard deep ensembles is impractical, techniques like batch ensembles or multiple-input multiple-output (MIMO) ensembles offer viable alternatives. In these approaches, the ensemble is formed by*sub-models*with partially shared parameters within a single, larger model that is trained in one run. We demonstrate how our overlapping holdout strategy can be applied in the batch ensemble setting and compare its performance across different initialization strategies.\nIn summary, our work addresses the following aspects:\n* \u2022We demonstrate several settings in which the*ensemble optimality gap*is significant, and show to which extent it can be mitigated by jointly tuning the ensemble.\n* \u2022We propose a novel*overlapping holdout*validation strategy that sits between using a common shared holdout set and using independent holdouts as in k-fold cross-validation.\n* \u2022We present a case study based on*batch ensembling*that demonstrates how an ensemble can be jointly trained and tuned in a single run with the overlapping holdout validation strategy.\nWe validate our study empirically using four diverse benchmark tasks spanning image, graph, tabular, and text classification. Our results demonstrate clear benefits from validating the ensemble jointly, especially for early stopping and temperature scaling, compared to validating individual models, while joint weight decay tuning shows more nuanced effects predominantly related to calibration. We assess the utility of the overlapping holdout strategy in different settings and also provide key insights regarding initialization choices for efficient batch ensembles. Collectively, these findings offer concrete guidance for practitioners on navigating the trade-offs between individual and joint optimization when training and calibrating deep ensembles.\n## 2Background\n### 2.1Ensemble Methods: Foundations and Diversity\nEnsemb...",
      "url": "https://arxiv.org/html/2511.04160v1"
    },
    {
      "title": "Regularized Neural Ensemblers",
      "text": "Regularized Neural Ensemblers\n\\*Equal contribution.# Regularized Neural Ensemblers\nSebastian Pineda ArangoUniversity of FreiburgMaciej JanowskiUniversity of FreiburgLennart PuruckerUniversity of FreiburgArber ZelaUniversity of Freiburg\nFrank HutterPriorLabsELLIS Institute T\u00fcbingenUniversity of FreiburgJosif GrabockaUniversity of Technology N\u00fcrnberg\n###### Abstract\nEnsemble methods are known for enhancing the accuracy and robustness of machine learning models by combining multiple base learners. However, standard approaches like greedy or random ensembling often fall short, as they assume a constant weight across samples for the ensemble members. This can limit expressiveness and hinder performance when aggregating the ensemble predictions. In this study, we explore employing regularized neural networks as ensemble methods, emphasizing the significance of dynamic ensembling to leverage diverse model predictions adaptively. Motivated by the risk of learning low-diversity ensembles, we propose regularizing the ensembling model by randomly dropping base model predictions during the training. We demonstrate this approach provides lower bounds for the diversity within the ensemble, reducing overfitting and improving generalization capabilities. Our experiments showcase that the regularized neural ensemblers yield competitive results compared to strong baselines across several modalities such as computer vision, natural language processing, and tabular data.\n## 1Introduction\n![Refer to caption](x1.png)Figure 1:Wrong Models Per Samples Across Meta-Datasets.Every dark cell represents data instances where a model\u2019s prediction is wrong. Different models fail on different instances, therefore, only instance-specific dynamic ensembles are optimal.\nEnsembling machine learning models is a well-established practice among practitioners and researchers, primarily due to its enhanced generalization performance over single-model predictions> (Ganaie et\u00a0al.,, [> 2022\n](https://arxiv.org/html/2410.04520v2#bib.bib20)> ; Erickson et\u00a0al.,, [> 2020\n](https://arxiv.org/html/2410.04520v2#bib.bib16)> ; Feurer et\u00a0al.,, [> 2015\n](https://arxiv.org/html/2410.04520v2#bib.bib17)> ; Wang et\u00a0al.,, [> 2020\n](https://arxiv.org/html/2410.04520v2#bib.bib50)> )\n. Ensembles are favored for their superior accuracy and ability to provide calibrated uncertainty estimates and increased robustness against covariate shifts> (Lakshminarayanan et\u00a0al.,, [> 2017\n](https://arxiv.org/html/2410.04520v2#bib.bib30)> )\n. Combined with their relative simplicity, these properties make ensembling the method of choice for many applications, such as medical imaging and autonomous driving, where reliability is paramount. A popular set-up consists of ensembling from a pool, after training them separately, a.k.a. post-hoc ensembling> (\n[> Purucker and Beel, 2023b, ](https://arxiv.org/html/2410.04520v2#bib.bib40)> )\n. This allows users to use models trained during hyperparameter optimization.\nDespite these advantages, selecting post-hoc models that are accurate and diverse remains a challenging combinatorial problem, especially as the pool of candidate models grows. Commonly used heuristics, particularly in the context of tabular data, such as greedy selection> (Caruana et\u00a0al.,, [> 2004\n](https://arxiv.org/html/2410.04520v2#bib.bib7)> )\nand various weighting schemes, attempt to optimize ensemble performance based on metrics evaluated on a held-out validation set or through cross-validation. However, these methods face significant limitations. Specifically, choosing models to include in the ensemble and determining optimal ensembling strategies (e.g., stacking weights) are critical decisions that, if not carefully managed, can lead to overfitting on the validation data. Although neural networks are good candidates for generating ensembling weights, few studies rely on them as a post-hoc ensembling approach. We believe this happens primarily due to a lack of ensemble-related inductive biases that provide regularization.\nIn this work, we introduce a novel approach to post-hoc ensembling using neural networks. Our proposed*Neural Ensembler*dynamically generates the weights for each base model in the ensemble on a per-instance basis, a.k.a dynamical ensemble selection> (Ko et\u00a0al.,, [> 2008\n](https://arxiv.org/html/2410.04520v2#bib.bib27)> ; Cavalin et\u00a0al.,, [> 2013\n](https://arxiv.org/html/2410.04520v2#bib.bib8)> )\n. To mitigate the risk of overfitting the validation set, we introduce a regularization technique inspired by the inductive biases inherent to the ensembling task. Specifically, we propose randomly dropping base models during training, inspired by previous work on DropOut in Deep Learning> (Srivastava et\u00a0al.,, [> 2014\n](https://arxiv.org/html/2410.04520v2#bib.bib48)> )\n. Furthermore, our method is modality-agnostic, as it only relies on the base model predictions. For instance, for classification, we use the class predictions of the base models as input.\nIn summary, our contributions are as follows:\n1. 1.\nWe propose a simple yet effective post-hoc ensembling method based on a regularized neural network that dynamically ensembles base models and is modality-agnostic.\n2. 2.\nTo prevent the formation of low-diversity ensembles, the regularization technique randomly drops base model predictions during training. We demonstrate theoretically that this lower bounds the diversity of the generated ensemble, and validate its effect empirically.\n3. 3.\nThrough extensive experiments, we show that Neural Ensemblers consistently select competitive ensembles across a wide range of data modalities, including tabular data (for both classification and regression), computer vision, and natural language processing.\nTo promote reproducibility, we have made our code publicly available in the following anonymous repository111[https://github.com/machinelearningnuremberg/RegularizedNeuralEnsemblers](https://github.com/machinelearningnuremberg/RegularizedNeuralEnsemblers).\nWe hope that our codebase,\nalong with the diverse set of benchmarks used in our experiments, will serve as a valuable resource for the development and evaluation of future post-hoc ensembling methods.\n## 2Background and Motivation\nPost-hoc ensembling uses set of fitted base models{z1,\u2026,zM}subscript\ud835\udc671\u2026subscript\ud835\udc67\ud835\udc40\\\\{z\\_{1},...,z\\_{M}\\\\}{ italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT }such that every model outputs predictionszm\u2062(x):\u211d\ud835\udd3b\u2192\u211d:subscript\ud835\udc67\ud835\udc5a\ud835\udc65\u2192superscript\u211d\ud835\udd3b\u211dz\\_{m}(x):\\\\mathbb{R}^{D}\\\\rightarrow\\\\mathbb{R}italic\\_z start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT ( italic\\_x ) : roman\\_\u211d start\\_POSTSUPERSCRIPT roman\\_\ud835\udd3b end\\_POSTSUPERSCRIPT \u2192roman\\_\u211d. These outputs are combined by a stacking ensemblerf\u2062(z\u2062(x);\u03b8):=f\u2062(z1\u2062(x),\u2026,zM\u2062(x);\u03b8):\u211d\ud835\udd44\u2192\u211d:assign\ud835\udc53\ud835\udc67\ud835\udc65\ud835\udf03\ud835\udc53subscript\ud835\udc671\ud835\udc65\u2026subscript\ud835\udc67\ud835\udc40\ud835\udc65\ud835\udf03\u2192superscript\u211d\ud835\udd44\u211df(z(x);\\\\theta):=f(z\\_{1}(x),...,z\\_{M}(x);\\\\theta):\\\\mathbb{R}^{M}\\\\rightarrow%\n\\\\mathbb{R}italic\\_f ( italic\\_z ( italic\\_x ) ; italic\\_\u03b8 ) := italic\\_f ( italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ( italic\\_x ) , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT ( italic\\_x ) ; italic\\_\u03b8 ) : roman\\_\u211d start\\_POSTSUPERSCRIPT roman\\_\ud835\udd44 end\\_POSTSUPERSCRIPT \u2192roman\\_\u211d, wherez\u2062(x)=[z1\u2062(x),\u2026,zM\u2062(x)]\ud835\udc67\ud835\udc65subscript\ud835\udc671\ud835\udc65\u2026subscript\ud835\udc67\ud835\udc40\ud835\udc65z(x)=[z\\_{1}(x),...,z\\_{M}(x)]italic\\_z ( italic\\_x ) = [ italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ( italic\\_x ) , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT ( italic\\_x ) ]is the concatenation of the base models predictions. While the base models are fitted using a training set\ud835\udc9fTrainsubscript\ud835\udc9fTrain\\\\mathcal{D}\\_{\\\\mathrm{Train}}caligraphic\\_D start\\_POSTSUBSCRIPT roman\\_Train end\\_POSTSUBSCRIPT, the ensembler\u2019s parameters\u03b8\ud835\udf03\\\\thetaitalic\\_\u03b8are typically obtained by minimizing a loss function on a validation set\ud835\udc9fValsubscript\ud835\udc9fVal\\\\mathcal{D}\\_{\\\\mathrm{Val}}caligraphic\\_D start\\_POSTSUBSCRIPT roman\\_Val end\\_POSTSUBSCRIPTsuch that:\n|\u03b8\u2208arg\u2062min\u03b8...",
      "url": "https://arxiv.org/html/2410.04520v2"
    },
    {
      "title": "Regularizing Deep Neural Networks with an Ensemble-based Decorrelation Method",
      "text": "Shuqin Gu, Yuexian Hou, Lipeng Zhang, Yazhou Zhang Regularizing Deep Neural Networks with an Ensemble-based Decorrelation Method https://ijcai.org/proceedings/2018/0301.pdf\nRegularizing Deep Neural Networks with an Ensemble-based Decorrelation Method\nShuqin Gu, Yuexian Hou, Lipeng Zhang, Yazhou Zhang\n2018-07-04\nRegularizing Deep Neural Networks with an Ensemble-based Decorrelation\nMethod\nShuqin Gu1, Yuexian Hou1\u2217, Lipeng Zhang2and Yazhou Zhang1\n1School of Computer Science and Technology, Tianjin University, Tianjin, China\n2School of Computer Software, Tianjin University, Tianjin, China\n{shuqingu,yxhou,lpzhang,yzhou zhang}@tju.edu.cn\nAbstract\nAlthough Deep Neural Networks (DNNs) have\nachieved excellent performance in many tasks, im\u0002proving the generalization capacity of DNNs still\nremains a challenge. In this work, we propose a\nnovel regularizer named Ensemble-based Decorre\u0002lation Method (EDM), which is motivated by the\nidea of the ensemble learning to improve general\u0002ization capacity of DNNs. EDM can be applied to\nhidden layers in fully connected neural networks or\nconvolutional neural networks. We treat each hid\u0002den layer as an ensemble of several base learner\u0002s through dividing all the hidden units into sever\u0002al non-overlapping groups, and each group will be\nviewed as a base learner. EDM encourages DNNs\nto learn more diverse representations by minimiz\u0002ing the covariance between all base learners dur\u0002ing the training step. Experimental results on M\u0002NIST and CIFAR datasets demonstrate that EDM\ncan effectively reduce the overfitting and improve\nthe generalization capacity of DNNs.\n1 Introduction\nDeep Neural Networks (DNNs) have achieved great suc\u0002cess in many tasks such as image classification [Krizhevsky\net al., 2012], machine translation [Wu et al., 2016], lan\u0002guage modeling [Jozefowicz et al., 2016] and speech recog\u0002nition [Graves et al., 2013], which benefits from its power\u0002ful learning ability. However, DNNs always bring another\nproblem\u2014overfitting. That is why the large scale dataset\u0002s are necessary for training DNNs. Therefore, the study on\nhow to avoid overfitting while retaining the strong ability of\nthe DNNs has become important and meaningful. So far,\nsome regularization methods have been proposed to solve this\nproblem, such as Weight Decay [Krogh and Hertz, 1992],\nDropout [Srivastava et al., 2014], DropConnect [Wan et al.,\n2013], etc. These methods improve the generalization capaci\u0002ty of DNNs in different ways. For instance, DropConnect sets\na randomly selected subset of weights within the network to\nzero. Although these methods have been applied to prevent\noverfitting, they are all in an implicit way.\n\u2217Corresponding author: Yuexian Hou.\nRecently, some studies try to explore the reason of the\noverfitting and find more effective way to avoid overfitting\nin DNNs [Srivastava et al., 2014; Cogswell et al., 2015;\nXiong et al., 2016]. Srivastava et al. [2014] took a further\nstep to explore the nature of the overfitting. They found that\nfor each hidden unit, Dropout could prevent co-adaptation\nby making the presence of other hidden units unreliable. In\u0002spired by the explanation of Dropout, Cogswell et al. [2015]\nproposed an effective method named DeCov. They limited\nthe correlation between each units in the same layer by reduc\u0002ing their cross-covariances to improve the generalization per\u0002formance of the network. However, DeCov tends to influence\nthe learning ability of DNNs because of the breakdown of the\ncorrelation between all hidden units. This opinion is proved\nby our experiments which will be shown in the following\nsections. Meaningfully, some researchers used quantum-like\nmotivations to explain the representation of the neural net\u0002works [Levine et al., 2017]. Actually, the parallel structure of\nneural networks can be naturally regarded as a classical sim\u0002ulation of general quantum superposition or entanglement s\u0002tates[Xie et al., 2015], which implies that the neural networks\nis not only a realization of a single statistical-hypothesis but\nalso a realization of multi-statistical-hypothesis. Hence, it is\nintriguing to analyze neural networks via some point of view\nof multi-statistical-hypothesis, e.g., ensemble learning.\nIn this paper, we propose a new regularization method\nnamed Ensemble-based Decorrelation Method (EDM). D\u0002ifferent from other existing studies, we analyze each hid\u0002den layer in DNNs from the perspective of ensemble learn\u0002ing [Zhou, 2012]. Thus, in EDM, the hidden units in the same\nlayer can be divided into several non-overlapping groups,\neach one will be viewed as a base learner.\nThe goal of EDM is the same as ensemble learning, both of\nwhich aim to improve the learning ability of each base learner\nand increase diversity between different base learners to get\ndiverse features simultaneously. Aiming to improve learning\nability, we try to obtain stronger base learners through as\u0002signing several units in one group instead of splitting them\none by one. For increasing diversity, we limit the correla\u0002tion between different groups, so as to decrease the general\u0002ization error and enhance the generalization capacity of the\nDNNs according to the bias-variance-covariance decomposi\u0002tion [Ueda and Nakano, 1996].\nBesides fully connected neural network, EDM can also be\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)\n2177\n\u2026\n\u2026\u2026\n\u2026\nInput layer\nHidden layer\nM base learners\nOutput layer\nl\n1 H M\nl H\nFigure 1: The architecture of Ensemble-based Fully Connected Neu\u0002ral Network. The hidden layer is averagely divided into M nonover\u0002lapping groups, and each group is regarded as a base learner with k\nunits.\napplied to the pooling layers in Convolutional Neural Net\u0002work (CNN). In the pooling layer, the only difference is that\na pooled feature map is considered as a base learner. In this\npaper, we adopt covariance between base learners as the mea\u0002surement of correlation, and put it to the loss function of the\nnetwork as a regularization term.\nThe experimental results show that comparing with other\nexisting regularization methods, EDM can achieve better per\u0002formance and effectively improve the generalization capacity\nof DNNs. Further, the correlation between hidden units are\ndemonstrated as a crucial factor for reducing the overfitting\nin DNNs.\n2 Ensemble-based Decorrelation Method\nThis section introduces the Ensemble-based Decorrelation\nMethod (EDM), a regularizer that aims to decrease correla\u0002tion between hidden units in hidden layer so as to reduce the\noverfitting in DNNs.\n2.1 Theoretical Motivation\nIt is well known that the bias-variance decomposition is an\nimportant theory tool for explaining the generalization per\u0002formance of the learning algorithms [Geman et al., 1992]. It\ndivides the generalization error of a learner into three parts,\ni.e., bias, variance and noise. Since the noise is difficult to\nestimate, it is usually subsumed into the bias term. Let yD\ndenote the target on the dataset D and fD denote the predict\u0002ed output of the learner on the dataset D, the error function\ncan be defined as:\nErrD(fD) = E[(fD \u2212 yD)\n2\n]\n= (E[fD] \u2212 yD)\n2 + E[(fD \u2212 E[fD])2\n]\n= bias(fD)\n2 + variance(fD) (1)\nwhere the bias and variance of the learner fD are defined as:\nbias(fD) = E[fD] \u2212 yD (2)\nvariance(fD) = E(fD \u2212 E[fD])2(3)\nBased on this, Ueda et al. [1996] proposed a bias-variance\u0002covariance decomposition for an ensemble of M base learn\u0002ers {f\n1\nD, f 2D, ..., fMD }, which is defined as:\nErrens\nD (FD) = bias(FD)\n2 +\n1\nM\nvariance(FD)\n+(1 \u2212\n1\nM\n)covariance(FD)\n(4)\nwhere\nbias =\n1\nM\nX\nM\nm=1\n(E[f\nm\nD ] \u2212 yD) (5)\nvariance =\n1\nM\nX\nM\nm=1\nE(f\nm\nD \u2212 E[f\nm\nD ])2\n(6)\ncovariance =\n1\nM(M \u2212 1)\nX\nM\nm=1\nX\nM\nn=1,n6=m\nE(f\nm\nD \u2212 E[f\nm\nD ])E(f\nn\nD \u2212 E[f\nn\nD])\n(7)\nwhere FD =\n1\nM\nPM\nm=1 f\nm\nD . The formula (4) shows that the\ngeneralization error depends heavily on the covariance term,\nwhich models the correlation between different base learners.\nThe smaller the covariance, the better the ensemble [Zhou,\n2012].\nMotivated by this the...",
      "url": "https://ijcai.org/proceedings/2018/0301.pdf"
    },
    {
      "title": "Understanding Overfitting in Machine Learning: Causes, Impacts, and ...",
      "text": "Understanding Overfitting in Machine Learning: Causes, Impacts, and Solutions | Lenovo US\nTEMPORARILY UNAVAILABLE\nDISCONTINUED\nTemporary Unavailable\nCooming Soon!\n. Additional units will be charged at the non-eCoupon price.Purchase additional now\nWe're sorry, the maximum quantity you are able to buy at this amazing eCoupon price is\nSign in or Create an Account to Save Your Cart!\nSign in or Create an Account to Join Rewards\nView Cart\nRemove\nYour cart is empty!\nDon\u2019t miss out on the latest products and savings \u2014find your next favorite laptop, PC, or accessory today.\nitem(s) in cart\nSome items in your cart are no longer available. Please visit[cart](https://www.lenovo.com/us/en/cart)for more details.\nhas been deleted\nPlease review your cart as items have changed.\nof\nContains Add-ons\nSubtotal\nProceed to Checkout\nYes\nNo\nPopular Searches\nWhat are you looking for today ?\nTrending\nRecent Searches\nItems\nAll\nCancel\nTop Suggestions\nView All &gt;\nStarting at\n[](https://www.lenovo.com/us/en/d/deals/doorbusters/?IPromoID=LEN944203)\n******4-Day Sale!**Save big on PCs &amp;&amp; Tech.[**Shop Now &gt;**](https://www.lenovo.com/us/en/d/deals/doorbusters/?IPromoID=LEN944203)\n****Buy online, pick up select products at Best Buy.[**Shop Pick up &gt;**](https://www.lenovo.com/us/en/d/bopis/?IPromoID=LEN775727)\n******My Lenovo Rewards!**Earn 3%-9% in rewards and get free expedited delivery on select products.[**Join for Free &gt;**](https://account.lenovo.com/us/en/account/rewards/?PromoID=LEN775755)\n****Lease-to-own today with Katapult. Get started with an initial lease payment as low as $1!\u202f\\*[**Learn More &gt;**](https://www.lenovo.com/us/en/landingpage/lenovo-financing-options/?IPromoID=LEN771093)\n**Shopping for a business?**New Lenovo Pro members get $100 off first order of $1,000+, exclusive savings &amp; 1:1 tech support.[**Learn More &gt;**](https://www.lenovo.com/us/en/business/benefits/?PromoID=LEN818484)\n[Home](https://www.lenovo.com/us/en/)&gt;[Knowledgebase](https://www.lenovo.com/us/en/knowledgebase/)&gt;Understanding Overfitting In Machine Learning Causes Impacts And Solutions\n# Understanding Overfitting in Machine Learning: Causes, Impacts, and Solutions\nOverfitting is a common challenge in machine learning that occurs when a model learns the training data too well, including its noise and irrelevant details. While this may lead to excellent performance on the training dataset, it often results in poor generalization to unseen data. Overfitting undermines the predictive power of a model, making it less effective in real-world applications.\nUnderstanding overfitting is crucial for developing robust machine learning models. By identifying its causes, recognizing its impacts, and implementing strategies to mitigate it, data scientists and machine learning practitioners can ensure their models perform reliably across diverse datasets.\n## Causes of Overfitting\n### Excessive Model Complexity\n**High model complexity**: Overfitting often arises when a model is excessively complex relative to the size and diversity of the training data. Complex models, such as deep neural networks with numerous layers and parameters, can capture intricate patterns in the data, including noise and outliers. While this may improve training accuracy, it hampers the model's ability to generalize.\n### Insufficient Training Data\n**Limited dataset size**: When the training dataset is too small, the model may struggle to identify generalizable patterns. Instead, it memorizes specific details of the training data, leading to overfitting. Insufficient data diversity exacerbates this issue, as the model lacks exposure to a wide range of scenarios.\n### Lack of Regularization\n**Absence of constraints**: Regularization techniques, such as L1 and L2 regularization, are designed to prevent overfitting by penalizing overly complex models. Without regularization, models are free to fit the training data too closely, increasing the risk of overfitting.\n### Overtraining\n**Excessive training epochs**: Training a model for too many epochs can lead to overfitting. As the model continues to learn, it starts capturing noise and irrelevant details in the training data, which reduces its ability to generalize.\n### Data Imbalance\n**Uneven class distribution**: Overfitting can occur when the training data is imbalanced, meaning certain classes are overrepresented while others are underrepresented. The model may perform well on the dominant class but fail to generalize to underrepresented classes.\n## Impacts of Overfitting\n### Poor Generalization\n**Reduced predictive accuracy**: The primary impact of overfitting is poor generalization. Models that overfit perform well on training data but fail to make accurate predictions on unseen data, limiting their practical utility.\n### Increased Error Rates\n**Higher test error**: Overfitting leads to increased error rates on test data. This discrepancy between training and test performance is a clear indicator of overfitting.\n### Inefficient Resource Utilization\n**Wasted computational resources**: Overfitting results in models that are overly complex and resource-intensive, yet ineffective in real-world applications. This inefficiency can lead to wasted time and computational power.\n### Misleading Insights\n**Incorrect conclusions**: Overfitted models may produce misleading insights, as their predictions are based on noise and irrelevant details rather than meaningful patterns. This can have serious consequences in critical applications like healthcare or finance.\n## Strategies to Prevent Overfitting\n### Regularization Techniques\n**L1 and L2 regularization**: Regularization adds penalties to the loss function for overly complex models. L1 regularization encourages sparsity by penalizing large coefficients, while L2 regularization reduces the magnitude of coefficients to prevent overfitting.\n### Cross-Validation\n**K-fold cross-validation**: Cross-validation involves splitting the dataset into multiple subsets and training the model on different combinations of these subsets. This technique helps assess the model's performance on unseen data and reduces the risk of overfitting.\n### Early Stopping\n**Monitoring validation loss**: Early stopping involves halting training when the validation loss stops improving. This prevents the model from overtraining and capturing noise in the data.\n### Data Augmentation\n**Enhancing dataset diversity**: Data augmentation techniques, such as rotation, flipping, and scaling, increase the diversity of the training data. This helps the model learn generalizable patterns and reduces the risk of overfitting.\n### Pruning\n**Simplifying model architecture**: Pruning involves removing unnecessary parameters or layers from the model. This reduces complexity and prevents the model from fitting noise in the training data.\n### Increasing Dataset Size\n**Collecting more data**: Expanding the training dataset size provides the model with more examples to learn from, improving its ability to generalize. Diverse data sources further enhance this effect.\n### Balancing Data\n**Addressing class imbalance**: Techniques like oversampling, undersampling, or synthetic data generation can balance the class distribution in the training dataset, reducing the risk of overfitting.\n## Key Workloads Affected by Overfitting\n### Healthcare Applications\n**Medical diagnosis**: Overfitting in healthcare models can lead to inaccurate diagnoses, as the model may rely on irrelevant patterns in the training data. Ensuring robust generalization is critical for reliable predictions in medical applications.\n### Financial Forecasting\n**Market predictions**: Financial models that overfit may produce misleading forecasts, leading to poor investment decisions. Regularization and cross-validation are essential to mitigate overfitting in this domain.\n### Autonomous Systems\n**Self-driving cars**: Overfitting in autonomous systems can result in unsafe behavior, as the model may fail to generali...",
      "url": "https://www.lenovo.com/us/en/knowledgebase/understanding-overfitting-in-machine-learning-causes-impacts-and-solutions"
    }
  ]
}