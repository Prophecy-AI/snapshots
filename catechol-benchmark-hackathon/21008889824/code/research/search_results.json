{
  "query": "How can I improve out-of-distribution generalization when the test set contains completely new chemical solvents not seen during training?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving out\u2011of\u2011distribution (OOD) generalization for completely new solvents hinges on three complementary ideas that have emerged in recent molecular\u2011ML research.\u202fFirst, **meta\u2011learning with unlabeled data** can teach a model how to interpolate between the training (in\u2011distribution) chemistry and the unseen solvent space.  By treating the OOD shift as a task\u2011level adaptation problem, a meta\u2011learner learns update rules that remain stable even when covariate shift is large, yielding markedly better predictions on novel compounds\u202f([arxiv.org](https://arxiv.org/abs/2506.11877)).\u202fSecond, **large\u2011scale, chemically diverse pre\u2011training** provides a strong foundation that reduces reliance on the narrow training set.  Models such as the \u201cMolecules\u2011to\u2011Materials\u201d foundation model are trained on billions of atomic environments and have been shown to transfer more robustly to new property tasks, including solvents that were never observed during fine\u2011tuning\u202f([arxiv.org](https://arxiv.org/abs/2310.16802)).\u202fThird, **topology\u2011aware robust optimization** and **OOD\u2011aware data augmentation** further tighten the generalization gap: incorporating graph\u2011topology regularizers during training encourages the network to respect molecular connectivity patterns, while score\u2011based generative models can synthesize plausible out\u2011of\u2011distribution solvents for augmentation, effectively expanding the support of the training distribution\u202f([arxiv.org](https://arxiv.org/abs/2307.13943); [arxiv.org](https://arxiv.org/abs/2206.07632)).\n\nBenchmark studies (e.g., the BOOM suite) reveal that OOD performance is highly sensitive to **representation choice, architecture, and hyper\u2011parameter tuning**.  Graph neural networks with edge\u2011wise attention, invariant descriptors (e.g., SOAP or physicochemical fingerprints), and calibrated probabilistic heads (such as the DIONYSUS framework) consistently outperform na\u00efve baselines on solvent\u2011property tasks\u202f([arxiv.org](https://arxiv.org/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf); [rsc.org](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)).\u202fSystematic ablations suggest that **pre\u2011training on a broad chemical corpus, followed by modest fine\u2011tuning with topology\u2011aware regularization and OOD data generated by score\u2011based models**, yields the most reliable gains.\n\nIn practice, a workflow that combines these elements can be adopted: (1) pre\u2011train a graph\u2011based model on a large, diverse molecular database; (2) generate a synthetic set of candidate solvents using a score\u2011based OOD generator; (3) apply meta\u2011learning or few\u2011shot adaptation on the combined real\u2011plus\u2011synthetic data, enforcing topology\u2011aware loss terms; and (4) calibrate the final predictions with a probabilistic post\u2011processor (e.g., DIONYSUS) before deployment.  This pipeline addresses covariate shift, enriches the training support, and aligns the model\u2019s inductive biases with chemical reality, thereby improving OOD generalization for entirely new solvents.",
      "url": ""
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Quantitative Biology > Biomolecules",
      "text": "[2206.07632] Exploring Chemical Space with Score-based Out-of-distribution Generation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[q-bio](https://arxiv.org/list/q-bio/recent)&gt;arXiv:2206.07632\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Quantitative Biology \\> Biomolecules\n**arXiv:2206.07632**(q-bio)\n[Submitted on 6 Jun 2022 ([v1](https://arxiv.org/abs/2206.07632v1)), last revised 3 Jun 2023 (this version, v3)]\n# Title:Exploring Chemical Space with Score-based Out-of-distribution Generation\nAuthors:[Seul Lee](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Lee,+S),[Jaehyeong Jo](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Jo,+J),[Sung Ju Hwang](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Hwang,+S+J)\nView a PDF of the paper titled Exploring Chemical Space with Score-based Out-of-distribution Generation, by Seul Lee and 2 other authors\n[View PDF](https://arxiv.org/pdf/2206.07632)> > Abstract:\n> A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules rather than generating unseen yet trivial ones. We experimentally validate that MOOD is able to explore the chemical space beyond the training distribution, generating molecules that outscore ones found with existing methods, and even the top 0.01% of the original training pool. Our code is available at [> this https URL\n](https://github.com/SeulLee05/MOOD)> . Comments:|ICML 2023|\nSubjects:|Biomolecules (q-bio.BM); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)|\nCite as:|[arXiv:2206.07632](https://arxiv.org/abs/2206.07632)[q-bio.BM]|\n|(or[arXiv:2206.07632v3](https://arxiv.org/abs/2206.07632v3)[q-bio.BM]for this version)|\n|[https://doi.org/10.48550/arXiv.2206.07632](https://doi.org/10.48550/arXiv.2206.07632)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Seul Lee [[view email](https://arxiv.org/show-email/db3d2662/2206.07632)]\n**[[v1]](https://arxiv.org/abs/2206.07632v1)**Mon, 6 Jun 2022 06:17:11 UTC (4,160 KB)\n**[[v2]](https://arxiv.org/abs/2206.07632v2)**Tue, 9 May 2023 10:31:37 UTC (3,728 KB)\n**[v3]**Sat, 3 Jun 2023 08:43:39 UTC (3,728 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Exploring Chemical Space with Score-based Out-of-distribution Generation, by Seul Lee and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2206.07632)\n* [TeX Source](https://arxiv.org/src/2206.07632)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nq-bio.BM\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2206.07632&amp;function=prev&amp;context=q-bio.BM) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2206.07632&amp;function=next&amp;context=q-bio.BM)\n[new](https://arxiv.org/list/q-bio.BM/new)|[recent](https://arxiv.org/list/q-bio.BM/recent)|[2022-06](https://arxiv.org/list/q-bio.BM/2022-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2206.07632?context=cs)\n[cs.LG](https://arxiv.org/abs/2206.07632?context=cs.LG)\n[physics](https://arxiv.org/abs/2206.07632?context=physics)\n[physics.chem-ph](https://arxiv.org/abs/2206.07632?context=physics.chem-ph)\n[q-bio](https://arxiv.org/abs/2206.07632?context=q-bio)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2206.07632)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2206.07632)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2206.07632)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2206.07632&amp;description=Exploring Chemical Space with Score-based Out-of-distribution Generation>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2206.07632&amp;title=Exploring Chemical Space with Score-based Out-of-distribution Generation>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is c...",
      "url": "https://arxiv.org/abs/2206.07632"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2310.16802] From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2310.16802\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2310.16802**(cs)\n[Submitted on 25 Oct 2023 ([v1](https://arxiv.org/abs/2310.16802v1)), last revised 6 May 2024 (this version, v2)]\n# Title:From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction\nAuthors:[Nima Shoghi](https://arxiv.org/search/cs?searchtype=author&amp;query=Shoghi,+N),[Adeesh Kolluru](https://arxiv.org/search/cs?searchtype=author&amp;query=Kolluru,+A),[John R. Kitchin](https://arxiv.org/search/cs?searchtype=author&amp;query=Kitchin,+J+R),[Zachary W. Ulissi](https://arxiv.org/search/cs?searchtype=author&amp;query=Ulissi,+Z+W),[C. Lawrence Zitnick](https://arxiv.org/search/cs?searchtype=author&amp;query=Zitnick,+C+L),[Brandon M. Wood](https://arxiv.org/search/cs?searchtype=author&amp;query=Wood,+B+M)\nView a PDF of the paper titled From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction, by Nima Shoghi and 5 other authors\n[View PDF](https://arxiv.org/pdf/2310.16802)[HTML (experimental)](https://arxiv.org/html/2310.16802v2)> > Abstract:\n> Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simultaneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of $\\sim$120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch, and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks. Please visit [> this https URL\n](https://nima.sh/jmp)> for further information. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2310.16802](https://arxiv.org/abs/2310.16802)[cs.LG]|\n|(or[arXiv:2310.16802v2](https://arxiv.org/abs/2310.16802v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2310.16802](https://doi.org/10.48550/arXiv.2310.16802)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Nima Shoghi [[view email](https://arxiv.org/show-email/1ab9933f/2310.16802)]\n**[[v1]](https://arxiv.org/abs/2310.16802v1)**Wed, 25 Oct 2023 17:32:23 UTC (3,150 KB)\n**[v2]**Mon, 6 May 2024 16:57:10 UTC (3,599 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction, by Nima Shoghi and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2310.16802)\n* [HTML (experimental)](https://arxiv.org/html/2310.16802v2)\n* [TeX Source](https://arxiv.org/src/2310.16802)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2310.16802&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2310.16802&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-10](https://arxiv.org/list/cs.LG/2023-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2310.16802?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.16802)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.16802)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.16802)\n### [2 blog links](https://arxiv.org/tb/2310.16802)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2310.16802&amp;description=From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2310.16802&amp;title=From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nB...",
      "url": "https://arxiv.org/abs/2310.16802"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2307.13943] Topology-aware Robust Optimization for Out-of-distribution Generalization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2307.13943\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2307.13943**(cs)\n[Submitted on 26 Jul 2023]\n# Title:Topology-aware Robust Optimization for Out-of-distribution Generalization\nAuthors:[Fengchun Qiao](https://arxiv.org/search/cs?searchtype=author&amp;query=Qiao,+F),[Xi Peng](https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+X)\nView a PDF of the paper titled Topology-aware Robust Optimization for Out-of-distribution Generalization, by Fengchun Qiao and 1 other authors\n[View PDF](https://arxiv.org/pdf/2307.13943)> > Abstract:\n> Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach. Comments:|In ICLR 2023 (17 pages including appendix). The source code and pre-trained models are publicly available at:[this https URL](https://github.com/joffery/TRO)|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2307.13943](https://arxiv.org/abs/2307.13943)[cs.LG]|\n|(or[arXiv:2307.13943v1](https://arxiv.org/abs/2307.13943v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2307.13943](https://doi.org/10.48550/arXiv.2307.13943)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|International Conference on Learning Representations 2023|\n## Submission history\nFrom: Fengchun Qiao [[view email](https://arxiv.org/show-email/6c231506/2307.13943)]\n**[v1]**Wed, 26 Jul 2023 03:48:37 UTC (18,348 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Topology-aware Robust Optimization for Out-of-distribution Generalization, by Fengchun Qiao and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2307.13943)\n* [TeX Source](https://arxiv.org/src/2307.13943)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2307.13943&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2307.13943&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-07](https://arxiv.org/list/cs.LG/2023-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2307.13943?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2307.13943)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2307.13943)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2307.13943)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2307.13943&amp;description=Topology-aware Robust Optimization for Out-of-distribution Generalization>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2307.13943&amp;title=Topology-aware Robust Optimization for Out-of-distribution Generalization>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2307.13943)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2307.13943"
    },
    {
      "title": "",
      "text": "[Jump to main content ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b#maincontent) [Jump to site search ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b#SearchText)\n\n[Issue 3, 2023](https://pubs.rsc.org/en/journals/journal/dd?issueid=dd002003&type=current)\n\n[![](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=CoverIssue&imageInfo.ImageIdentifier.SerCode=DD&imageInfo.ImageIdentifier.IssueId=DD002003)\\\n\\\nFrom the journal: **Digital Discovery**](https://pubs.rsc.org/en/journals/journal/dd)\n\n## Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS [\u2020](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b\\#fn1)\n\n![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg)\n\n[Gary\\\nTom](https://pubs.rsc.org/en/results?searchtext=Author%3AGary%20Tom), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-8470-6515)_abc_[Riley J.\\\nHickman](https://pubs.rsc.org/en/results?searchtext=Author%3ARiley%20J.%20Hickman), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-5762-1006)_abc_[Aniket\\\nZinzuwadia](https://pubs.rsc.org/en/results?searchtext=Author%3AAniket%20Zinzuwadia),_d_[Afshan\\\nMohajeri](https://pubs.rsc.org/en/results?searchtext=Author%3AAfshan%20Mohajeri), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-3858-3024)_e_[Benjamin\\\nSanchez-Lengeling](https://pubs.rsc.org/en/results?searchtext=Author%3ABenjamin%20Sanchez-Lengeling) [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-1116-1745)_f_\nand\n[Al\u00e1n\\\nAspuru-Guzik](https://pubs.rsc.org/en/results?searchtext=Author%3AAl%C3%A1n%20Aspuru-Guzik)\\*_abcghi_\n\n[Author affiliations](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)\n\n\\\\* Corresponding authors\n\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada\n\n**E-mail:** [alan@aspuru.com](mailto:alan@aspuru.com)\n\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\n\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\n\ndHarvard Medical School, Harvard University, Boston, MA, USA\n\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\n\nfGoogle Research, Brain Team, USA\n\ngDepartment of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, ON, Canada\n\nhDepartment of Materials Science & Engineering, University of Toronto, Toronto, ON, Canada\n\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\n\n### Abstract\n\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (<2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution data _via_ ablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n\n![Graphical abstract: Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&imageInfo.ImageIdentifier.ManuscriptID=D2DD00146B&imageInfo.ImageIdentifier.Year=2023)\n\nThis article is Open Access\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/Ajax-GA-Loader.gif)\nPlease wait while we load your content...\nSomething went wrong. [Try again?](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)\n\n[About](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b#pnlAbstract)\n\n[Cited by](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b#pnlCitation)\n\n[Related](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b#pnlRelatedContent)\n\n[Download options Please wait...](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)\n\n## Supplementary files\n\n- [Supplementary information\\\nPDF (3099K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b1.pdf)\n- [Supplementary information\\\nPDF (961K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b2.pdf)\n- [Supplementary information\\\nPDF (1649K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b3.pdf)\n- [Supplementary information\\\nPDF (2727K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b4.pdf)\n- [Supplementary information\\\nPDF (3095K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b5.pdf)\n- [Supplementary information\\\nPDF (3749K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b6.pdf)\n- [Supplementary information\\\nPDF (1118K)](https://www.rsc.org/suppdata/d2/dd/d2dd00146b/d2dd00146b7.pdf)\n\n## Transparent peer review\n\nTo support increased transparency, we offer authors the option to publish the peer review history alongside their article.\n\n[View this article\u2019s peer review history](https://www.webofscience.com/api/gateway/wos/peer-review/10.1039/D2DD00146B)\n\n## Article information\n\nDOI[https://doi.org/10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)\n\n**Article type**Paper\n\nSubmitted21 Dec 2022\n\nAccepted21 Apr 2023\n\nFirst published02 May 2023\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/open-access-icon-orange.png)\n\n**This article is Open Access**[![Creative Commons BY-NC license](https://www.rsc-cdn.org/pubs-core/2022.0.171/content/NewImages/CCBY-NC.svg)](http://creativecommons.org/licenses/by-nc/3.0/)\n\n### Download Citation\n\n_**Digital Discovery**_, 2023, **2**, 759-774\n\nBibTexEndNoteMEDLINEProCiteReferenceManagerRefWorksRIS\n\n### Permissions\n\n[Request permissions](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b)\n\n### Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS\n\nG. Tom, R. J. Hickman, A. Zinzuwadia, A. Mohajeri, B. Sanchez-Lengeling and A. Aspuru-Guzik,\n_Digital Discovery_, 2023,\u00a0**2**, 759\n**DOI:** 10.1039/D2DD00146B\n\nThis article is licensed under a [Creative Commons Attribution-NonCommercial 3.0 Unported Licence](https://creativecommons.org/licenses/by-nc/3.0/). **You can use material from**\n**this article in other publications, without requesting further permission** from the RSC,\nprovided that the correct acknowledgement is given and it is not used for commercial purposes.\n\nTo request permission **to reproduce material from this article in a commercial publication**,\nplease go to the [Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD2DD00146B).\n\nIf you are **an author contributing to an RSC publication, you do not need to request permission**\nprovided correct acknowledgement is given.\n\nIf you are **the author of this article, you do not need to request permission to reproduce figures**\n**and diagrams** provided correct acknowledgement is give...",
      "url": "https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00146b"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2505.01912] BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2505.01912\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2505.01912**(cs)\n[Submitted on 3 May 2025 ([v1](https://arxiv.org/abs/2505.01912v1)), last revised 19 Dec 2025 (this version, v2)]\n# Title:BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nAuthors:[Evan R. Antoniuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Antoniuk,+E+R),[Shehtab Zaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaman,+S),[Tal Ben-Nun](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Nun,+T),[Peggy Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[James Diffenderfer](https://arxiv.org/search/cs?searchtype=author&amp;query=Diffenderfer,+J),[Busra Sahin](https://arxiv.org/search/cs?searchtype=author&amp;query=Sahin,+B),[Obadiah Smolenski](https://arxiv.org/search/cs?searchtype=author&amp;query=Smolenski,+O),[Tim Hsu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsu,+T),[Anna M. Hiszpanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Hiszpanski,+A+M),[Kenneth Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+K),[Bhavya Kailkhura](https://arxiv.org/search/cs?searchtype=author&amp;query=Kailkhura,+B),[Brian Van Essen](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Essen,+B)\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n[View PDF](https://arxiv.org/pdf/2505.01912)[HTML (experimental)](https://arxiv.org/html/2505.01912v2)> > Abstract:\n> Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\\mathbf{BOOM}$, $\\mathbf{b}$enchmarks for $\\mathbf{o}$ut-$\\mathbf{o}$f-distribution $\\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at [> this https URL\n](https://github.com/FLASK-LLNL/BOOM)> Subjects:|Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2505.01912](https://arxiv.org/abs/2505.01912)[cs.LG]|\n|(or[arXiv:2505.01912v2](https://arxiv.org/abs/2505.01912v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2505.01912](https://doi.org/10.48550/arXiv.2505.01912)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Evan Antoniuk [[view email](https://arxiv.org/show-email/6abb5e36/2505.01912)]\n**[[v1]](https://arxiv.org/abs/2505.01912v1)**Sat, 3 May 2025 19:51:23 UTC (35,134 KB)\n**[v2]**Fri, 19 Dec 2025 23:00:10 UTC (16,070 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2505.01912)\n* [HTML (experimental)](https://arxiv.org/html/2505.01912v2)\n* [TeX Source](https://arxiv.org/src/2505.01912)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2505.01912&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2505.01912&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-05](https://arxiv.org/list/cs.LG/2025-05)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/2505.01912?context=cond-mat)\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2505.01912?context=cond-mat.mtrl-sci)\n[cs](https://arxiv.org/abs/2505.01912?context=cs)\n[cs.AI](https://arxiv.org/abs/2505.01912?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.01912)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.01912)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.01912)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2505.01912&amp;description=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2505.01912&amp;title=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*...",
      "url": "https://arxiv.org/abs/2505.01912"
    },
    {
      "title": "Real-World Molecular Out-Of-Distribution: Specification and Investigation",
      "text": "We use cookies to distinguish you from other users and to provide you with a better experience on our websites.Close this message to accept cookies or find out how to manage your cookie settings. [Learn more about our Privacy Notice...\\\n\\[opens in a new tab\\]](https://www.cambridge.org/about-us/legal-notices/privacy-notice/)\n\n[Back to\\\nTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\n\nSearch within Theoretical and Computational Chemistry\n\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n\n# Real-World Molecular Out-Of-Distribution: Specification and Investigation\n\n05 June 2023, Version 1\n\nThis is not the most recent version. There is a [newer version](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932) of this content available\n\nWorking Paper\n\n## Authors\n\n- [Prudencio Tossou](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Prudencio%20Tossou),\n- [Cas Wognum](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Cas%20Wognum)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0009-0006-2742-4817),\n- [Michael Craig](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Michael%20Craig),\n- [Hadrien Mary](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Hadrien%20Mary),\n- [Emmanuel Noutahi](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Emmanuel%20Noutahi)\n\n[Show author details](https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1)\n\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nThis study presents a rigorous framework for investigating Molecular Out-Of-Distribution (MOOD) generalization in drug discovery. The concept of MOOD is first clarified through a problem specification that demonstrates how the covariate shifts encountered during real-world deployment can be characterized by the distribution of sample distances to the training set. We find that these shifts can cause performance to drop by up to 60% and uncertainty calibration by up to 40%. This leads us to propose a splitting protocol that aims to close the gap between deployment and testing. Then, using this protocol, a thorough investigation is conducted to assess the impact of model design, model selection and dataset characteristics on MOOD performance and uncertainty calibration. We find that appropriate representations and algorithms with built-in uncertainty estimation are crucial to improve performance and uncertainty calibration. This study sets itself apart by its exhaustiveness and opens an exciting avenue to benchmark meaningful, algorithmic progress in molecular scoring. All related code can be found on Github at https://github.com/cwognum/mood-experiments.\n\n## Keywords\n\n[Molecular Scoring](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20Scoring)\n\n[Out-of-Distribution](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Out-of-Distribution)\n\n[Applicability Domain](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Applicability%20Domain)\n\n[Drug Discovery](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Drug%20Discovery)\n\n## Supplementary materials\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\n\nMOOD: Supplementary Material\n\n**Description**\n\nProvides a variety of additional figures to support the results from the main text.\n\n**Actions**\n\n**Download**\n**(3 MB)**\n\n## Supplementary weblinks\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\n\nMOOD: Code base\n\n**Description**\n\nA Github repository with all the code that was used for the results in the MOOD paper.\n\n**Actions**\n\n[**View**](https://github.com/cwognum/mood-experiments)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\n[Jul 26, 2023 Version\\\n2](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932)\n\nJun 05, 2023 Version 1\n\n## Metrics\n\n3,843\n\n1,822\n\n0\n\nViews\n\nDownloads\n\nCitations\n\n## License\n\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\n\nCC\n\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\n\nBY\n\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\n\nNC\n\nThe content is available under\n[CC BY NC 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by-nc/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2023-q11q4\\\nD O I: 10.26434/chemrxiv-2023-q11q4 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2023-q11q4)\n\n## Funding\n\n**Mitacs**\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1"
    },
    {
      "title": "",
      "text": "We use cookies to distinguish you from other users and to provide you with a better experience on our websites.Close this message to accept cookies or find out how to manage your cookie settings. [Learn more about our Privacy Notice...\\\n\\[opens in a new tab\\]](https://www.cambridge.org/about-us/legal-notices/privacy-notice/)\n\n[Back to\\\nTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\n\nSearch within Theoretical and Computational Chemistry\n\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n\n# Molecular deep learning at the edge of chemical space\n\n17 March 2025, Version 1\n\nWorking Paper\n\n## Authors\n\n- [Derek van Tilborg](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Derek%20van%20Tilborg)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-4473-0657),\n- [Luke Rossen](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Luke%20Rossen)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0009-0008-4942-0360),\n- [Francesca Grisoni](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Francesca%20Grisoni)\n\n[Show author details](https://chemrxiv.org/engage/chemrxiv/article-details/67d45bb181d2151a02600e0a)\n\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nMolecular machine learning models often fail to generalize beyond the chemical space of their training data, limiting their ability to reliably perform predictions on structurally novel bioactive molecules. To advance the ability of machine learning to go beyond the \u2018edge\u2019 of their training chemical space, we introduce a joint modeling approach that combines molecular property prediction with molecular reconstruction, enabling us to estimate model generalizability through a new reconstruction-based \u2018unfamiliarity\u2019 metric. Via a systematic analysis spanning more than 30 bioactivity datasets, we demonstrate that unfamiliarity not only effectively identifies out-of-distribution molecules but also serves as a reliable predictor of classifier performance. Even when faced with the presence of strong distribution shifts, unfamiliarity yields robust and meaningful molecular insights that go unnoticed by traditional methods. Our findings highlight that joint modelling can be a powerful strategy for extending the reach of machine learning models into uncharted regions of chemical space, advancing the discovery of diverse and novel molecules.\n\n## Keywords\n\n[out-of-distribution](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=out-of-distribution)\n\n[joint modeling](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=joint%20modeling)\n\n[molecular machine learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=molecular%20machine%20learning)\n\n[drug discovery](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=drug%20discovery)\n\n## Supplementary weblinks\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\n\nGitHub\n\n**Description**\n\nOur codebase to replicate the study\n\n**Actions**\n\n[**View**](https://github.com/molML/jointmolecularmodel)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\nMar 17, 2025 Version 1\n\n## Metrics\n\n1,871\n\n1,021\n\n0\n\nViews\n\nDownloads\n\nCitations\n\n## License\n\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\n\nCC\n\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\n\nBY\n\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\n\nNC\n\nThe content is available under\n[CC BY NC 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by-nc/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2025-qj4k3\\\nD O I: 10.26434/chemrxiv-2025-qj4k3 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2025-qj4k3)\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67d45bb181d2151a02600e0a"
    }
  ]
}