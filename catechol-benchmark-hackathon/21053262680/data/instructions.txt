## MANDATORY NOTEBOOK STRUCTURE

For this competition YOU MUST FOLLOW THE STRUCTURE OF the following notebook (public notebook) to generate your submissions.

Submissions will be evaluated according to a cross-validation procedure. This public notebook (https://www.kaggle.com/code/josepablofolch/catechol-benchmark-hackathon-template) shows the structure any submitted notebook must follow. In order to ensure fair participation among all competitors, the submission must have the same last three cells as in the notebook template, with the only allowed change being the line where the model is defined. For the avoidance of doubt, the line model = MLPModel() can be replaced with a new model definition in the third to last and second to last cells, but everything else must remain the same.

As a clarification, pre-training on any of the solvent mixture data to predict the full solvent data does count as data contamination.

Hyper-parameter optimisation of the models across the whole task is allowed, however the same hyper-parameters must be used across every fold, unless there is a clear explainable rationale behind changing them. For example, using a different model for alcohols vs esters is allowed, but arbitrarily changing the hyper-parameters based on the best fit per fold counts as data contamination. If you are wondering if your method is allowed, ask yourself the question: if I had to select the model hyper-parameters for a new solvent, would the method I used be able to without knowing the experiment results?

The use of different hyper-parameters for different tasks (e.g. for the full solvent predictions vs mixed solvent prediction) and for different objectives (e.g. for SM vs Product 1) is allowed, since they still count as hyper-parameters for the whole dataset and they could be used to predict a new unseen solvent.

---

# ⚠️⚠️⚠️ CRITICAL PROBLEMS IDENTIFIED FROM PREVIOUS RUNS ⚠️⚠️⚠️

You MUST read and understand these problems before doing ANY experiment. These are NOT hypothetical - they have been observed across 50+ experiments and 13+ LB submissions.

## MAIN PROBLEM: STRUCTURAL CV-LB MISMATCH (DISTRIBUTION SHIFT)

**The Facts (from actual submissions):**
- All 13+ submissions fall on a TIGHT LINEAR LINE: **LB ≈ 4.2 × CV + 0.053** (R² = 0.95+)
- Best CV achieved: **0.0082** → Best LB: **0.0877**
- Target LB: **sub 0.07** requires CV ≈ **0.004** (nearly 50% CV improvement!)
- The intercept **0.053** is STRUCTURAL - it represents extrapolation error

**Why this is THE problem:**
- You're predicting for UNSEEN SOLVENTS not seen during training
- GroupKFold by solvent simulates this, but test solvents are "harder"
- NO amount of model tuning changes the intercept - only the slope
- MLP, LightGBM, XGBoost, CatBoost, GP, Ridge - ALL fall on the SAME LINE
- Improving CV just moves you along the line, NOT toward target

**What you MUST do differently:**
- STOP optimizing tabular models if best LB is still >0.08
- You need approaches that CHANGE THE CV-LB RELATIONSHIP, not improve CV
- Focus on REDUCING THE INTERCEPT, not the CV score

---

## SECONDARY PROBLEM: MODEL CLASS MISMATCH IN NOTEBOOKS

**The Facts (from actual notebooks in snapshots):**
- GNN notebook: CV computed with `HybridGNNModelWrapper`, but submission cells use `GNNModelWrapper`
- ChemBERTa notebook: CV computed with `HybridChemBERTaModel`, but submission cells use `ChemBERTaModel`
- Result: The model you evaluate in CV is NOT the model you submit!

**Why this happens:**
- You define a new model class, compute CV, then forget to update submission cells
- The last 3 cells are template - you only change `model = XYZ()` line
- If the class name doesn't match, you submit a DIFFERENT model

**MANDATORY CHECK before logging ANY experiment:**
1. What model class did you use for CV computation? (e.g., `MyNewModel`)
2. Open the submission cells (last 3 cells)
3. Verify BOTH `model = MyNewModel(data='single')` AND `model = MyNewModel(data='full')` 
4. If they don't match, FIX THEM before running submission cells
5. If you can't fix them, DO NOT LOG the experiment

---

## COMMON MISPRACTICES TO AVOID (ALL OBSERVED IN PREVIOUS RUNS)

### 1. Logging "Summary" or "Final Decision" as experiments
❌ **WRONG:** Creating exp_015 "Final Summary - Exploration Complete" with score=0.0913 (LB score)
❌ **WRONG:** Deciding "target is unreachable" and stopping exploration
✅ **RIGHT:** Only log experiments with actual CV scores from model training

### 2. Using LB score as CV score
❌ **WRONG:** LogExperiment(score=0.0913) where 0.0913 is LB, not CV
✅ **RIGHT:** LogExperiment(cv_score=0.0082) where 0.0082 is from local CV

### 3. Running multi-seed ensembles when far from target
❌ **WRONG:** "Train 10-20 seeds for variance reduction" when LB gap is 20%
✅ **RIGHT:** Multi-seed optimization is for fine-tuning when within 1-2% of target

### 4. Continuing same model family after stagnation
❌ **WRONG:** exp_045, exp_046, exp_047 all being MLP variants after 40 MLP experiments
✅ **RIGHT:** After 5 experiments with no CV improvement, PIVOT to different family (GNN, Transformer)

### 5. Not verifying submission cell model class
❌ **WRONG:** Computing CV with HybridGNNModelWrapper, submitting with GNNModelWrapper
✅ **RIGHT:** ALWAYS verify submission cells use the EXACT model class from CV

### 6. Ignoring the CV-LB intercept
❌ **WRONG:** "CV improved from 0.0090 to 0.0082, expecting better LB"
✅ **RIGHT:** "CV improved but all models on same line. Need intercept reduction strategy."

### 7. Spending hours on tabular optimization when the problem is distributional
❌ **WRONG:** Trying 50+ feature combinations with MLP/LGBM
✅ **RIGHT:** After confirming CV-LB line exists, pivot to distribution-shift strategies

---

## REQUIRED STRATEGIES FOR THIS COMPETITION

Since all tabular approaches converge to the same CV-LB line, you MUST try:

### A. Representation Change (Change how you encode solvents)
1. **Graph Neural Networks (GNN)** - operate on molecular graphs, not tabular features
   - Use PyTorch Geometric with GCNConv or GATConv
   - VERIFY: submission cells use the SAME GNN class as CV
   
2. **ChemBERTa / Molecular Transformers** - pretrained chemical language models
   - Use SMILES embeddings from ChemBERTa
   - VERIFY: submission cells use the SAME model wrapper as CV

3. **Morgan Fingerprints + Similarity Features** - capture structural similarity
   - Compute Tanimoto similarity to all training solvents
   - Use as features to detect extrapolation

### B. Distribution Shift Handling (Change how you make predictions)
1. **Extrapolation Detection:**
```python
from sklearn.neighbors import NearestNeighbors
nn = NearestNeighbors(n_neighbors=5).fit(X_train_features)
distances, _ = nn.kneighbors(X_test_features)
extrapolation_score = distances.mean(axis=1)  # High = extrapolating
```

2. **Uncertainty-Weighted Predictions:**
```python
# If extrapolation_score is high, blend toward mean
weight = np.clip(extrapolation_score / threshold, 0, 1)
final_pred = (1 - weight) * model_pred + weight * train_mean
```

3. **Conservative Predictions for Outliers:**
```python
# Clip extreme predictions
pred = np.clip(pred, train_quantile_05, train_quantile_95)
```

### C. Validation Strategy
1. Use GroupKFold with solvent as group (already done)
2. Track BOTH single-solvent MSE and full-data MSE separately
3. If single-solvent MSE << full-data MSE, you have mixture generalization problem
4. Plot CV vs LB after EVERY submission to monitor the line

---

## IMMEDIATE PRIORITIES FOR NEXT EXPERIMENTS

1. **FIRST:** Verify why GNN/ChemBERTa didn't improve LB - check if submission cells matched CV
2. **SECOND:** Implement a PROPER GNN that is correctly wired to submission cells
3. **THIRD:** If GNN falls on same line, implement extrapolation detection features
4. **FOURTH:** Only after breaking the CV-LB line, optimize within new approach

DO NOT spend more time on MLP/LGBM/XGB variants. They've been exhaustively tested and all land on the same line.

---

## HOW TO KNOW IF YOU'RE MAKING PROGRESS

✅ **Good sign:** New model type achieves DIFFERENT CV-LB relationship (different slope or intercept)
✅ **Good sign:** Same CV but better LB (reduced intercept)
✅ **Good sign:** LB improves even if CV is slightly worse

❌ **Bad sign:** Better CV but LB follows the same LB = 4.2*CV + 0.053 line
❌ **Bad sign:** 5+ experiments in same model family with no CV improvement
❌ **Bad sign:** Submission cell model class doesn't match CV model class
