## What I Understood

The junior researcher implemented **exp_122: Per-Solvent-Class Models** following a hypothesis that different solvent chemical classes (alcohols, esters, ethers, ketones, etc.) might have different CV-LB relationships. The approach was to train separate CatBoost+XGBoost models for each solvent class, hoping this would reduce the intercept in the CV-LB line.

**Result**: CV=0.016787 (single=0.016180, full=0.017395), which is **107% WORSE** than the best CV of 0.008092.

The researcher correctly identified why this failed: when leaving one solvent out, we often leave out the only representative of that class, so class-specific models can't be trained properly. This is a fundamental limitation of the leave-one-out validation scheme combined with class-specific modeling.

---

## Technical Execution Assessment

**Validation**: SOUND ✓
- Leave-One-Out validation correctly implemented (24 folds single, 13 folds full)
- Same validation scheme as baseline experiments for fair comparison
- CV score verified in metrics.json: 0.016787

**Leakage Risk**: None detected ✓
- Features computed from training data only
- Scaler fitted on training data, applied to test
- Models trained independently per fold

**Score Integrity**: VERIFIED ✓
- CV=0.016787 verified in metrics.json
- Single solvent MSE: 0.016180
- Full data MSE: 0.017395
- Results are consistent with the model architecture

**Code Quality**: GOOD ✓
- **Model class consistency: VERIFIED** - `PerClassModel` used in both CV (cells 8, 9) and submission cells (cells 11, 12)
- Submission cells correctly structured following template
- Solvent classification logic is reasonable
- Submission cells were NOT executed (no execution timestamps in last 3 cells)

**Verdict: TRUSTWORTHY** - Results can be trusted. This is a properly implemented experiment.

---

## Strategic Assessment

### The Per-Class Approach: A Reasonable Hypothesis, Fundamental Flaw

**Key Finding**: CV=0.016787 is 107% worse than best CV (0.008092).

**Why this failed (researcher correctly identified):**
1. Leave-one-out validation leaves out the ONLY representative of some classes
2. Class-specific models have less training data and overfit
3. The distribution shift is NOT class-specific - it affects all solvents similarly

**The researcher's conclusion is correct**: "The CV-LB intercept problem is fundamental and cannot be solved by training class-specific models."

### Experiment Trajectory Analysis

After 123 experiments, the team has exhaustively explored:
- ✅ Tabular models (MLP, LGBM, XGBoost, CatBoost, GP, Ridge, RF)
- ✅ GNN attempts (multiple implementations)
- ✅ ChemBERTa embeddings
- ✅ Ensemble methods (mean, weighted, median)
- ✅ Calibration strategies (shrink toward mean)
- ✅ Physics constraints (mass balance, softmax normalization)
- ✅ Yield ratio prediction
- ✅ Per-class models (this experiment)
- ✅ Pseudo-labeling, domain adversarial, uncertainty weighting

**The exploration has been EXHAUSTIVE.**

### Key Insight from Public Kernels

I reviewed the top public kernels:

1. **ens-model kernel (matthewmaree)**: Uses CatBoost + XGBoost ensemble with:
   - Correlation-based feature filtering (threshold=0.90)
   - Feature priority system (spange > acs > drfps > frag > smiles)
   - Clipping and renormalization of predictions
   - Different weights for single vs full data (7:6 vs 1:2)

2. **mixall kernel (lishellliang)**: Uses **GroupKFold (5 splits)** instead of Leave-One-Out!
   - This is a DIFFERENT validation scheme
   - Claims "good CV-LB" relationship
   - Uses MLP + XGBoost + RF + LightGBM ensemble with Optuna tuning

**CRITICAL OBSERVATION**: The mixall kernel uses GroupKFold instead of Leave-One-Out. This might explain why some kernels have better CV-LB alignment - they're using a different validation scheme that better matches the competition's evaluation.

### Remaining Submissions: 3

With only 3 submissions left and best LB at ~0.088 (estimated from CV-LB relationship), the situation is challenging. Target is 0.0347.

---

## What's Working

1. **Excellent scientific rigor**: The researcher correctly implemented per-class models and drew the right conclusion about why it failed
2. **Proper validation**: CV methodology is sound and consistent
3. **Model class consistency**: Submission cells correctly use the same model class as CV
4. **Good documentation**: Notes clearly explain the hypothesis, results, and conclusions
5. **Learning from experiments**: The researcher correctly identified that the problem is NOT class-specific

---

## Key Concerns

### CRITICAL: The CV-LB Intercept Problem Remains Unsolved

**Observation**: After 123 experiments, all approaches fall on the same CV-LB line.

**Why it matters**: 
- The target (0.0347) appears mathematically unreachable with current approaches
- All model families (MLP, LGBM, XGBoost, CatBoost, GP, GNN) fall on the SAME line
- The intercept represents structural distribution shift between train/test solvents

### HIGH: Per-Class Models Made Things WORSE

**Observation**: CV=0.016787 is 107% worse than best CV.

**Why it matters**: This confirms that the problem is NOT about solvent class differences. The distribution shift affects all solvents similarly, regardless of their chemical class.

### MEDIUM: Only 3 Submissions Remaining

**Observation**: 3 submissions left, best LB is ~0.088, target is 0.0347.

**Why it matters**: Each submission is precious. Need high-leverage experiments.

### MEDIUM: Potential Validation Scheme Mismatch

**Observation**: The mixall kernel uses GroupKFold (5 splits) instead of Leave-One-Out.

**Why it matters**: If the competition's evaluation uses a different validation scheme than what we're using locally, our CV scores may not accurately predict LB scores. This could explain part of the CV-LB gap.

---

## Top Priority for Next Experiment

### DO NOT SUBMIT exp_122

The CV=0.016787 is 107% worse than the best CV (0.008092). Based on the CV-LB relationship, this would give LB ≈ 0.12+, much worse than best LB (~0.088).

### RECOMMENDED: Investigate Validation Scheme Alignment

**Key Insight**: The mixall kernel uses GroupKFold (5 splits) instead of Leave-One-Out. This might be why some kernels have better CV-LB alignment.

**Immediate Action**: 
1. Check if the competition's evaluation uses GroupKFold or Leave-One-Out
2. If GroupKFold, recompute CV with GroupKFold to get better LB estimates
3. This could reveal that our best models have better LB than we thought

### Alternative: Submit Best Known Model

If time is limited, consider submitting the best CV model (exp_050: CatBoost+XGBoost, CV=0.008092) if it hasn't been successfully submitted yet.

**Check submission history**: Was exp_050 successfully submitted? If not, this should be the priority.

### Long-term Strategy (if more time available)

1. **Replicate ens-model kernel exactly**: This kernel has sophisticated feature engineering (correlation filtering, priority system) that we may not have fully replicated
2. **Try GroupKFold validation**: This might give better CV-LB alignment
3. **Focus on reducing prediction variance**: The ens-model kernel uses clipping and renormalization which might help

---

## Summary

| Dimension | Assessment |
|-----------|------------|
| Technical Execution | ✅ TRUSTWORTHY - Per-class models correctly implemented |
| Strategic Direction | ❌ Approach made CV 107% worse, confirms problem is NOT class-specific |
| Key Finding | CV=0.016787 (107% worse than best) |
| Top Priority | **DO NOT SUBMIT. Investigate validation scheme alignment (GroupKFold vs LOO). Submit best known model if not already submitted.** |

## Confidence Levels

- **Very High (99%)**: The per-class model is correctly implemented
- **Very High (95%)**: The approach failed because LOO leaves out the only class representative
- **High (85%)**: The CV-LB gap is partly due to validation scheme mismatch
- **Medium (60%)**: GroupKFold might give better CV-LB alignment
- **Low (20%)**: Any approach can reach the target with 3 submissions remaining

## THE TARGET IS STILL REACHABLE (BUT REQUIRES A PARADIGM SHIFT)

The benchmark paper achieved MSE 0.0039. The current best LB is ~0.088. The target (0.0347) is between these values.

**Key insight**: After 123 experiments, all tabular approaches converge to the same CV-LB line. The target requires CHANGING THIS RELATIONSHIP, not improving CV.

**IMMEDIATE ACTIONS:**
1. **DO NOT SUBMIT exp_122** - CV is 107% worse than best
2. **Check if best model (exp_050) was successfully submitted** - if not, fix and resubmit
3. **Investigate validation scheme alignment** - GroupKFold vs LOO
4. **Consider replicating ens-model kernel exactly** - it has sophisticated feature engineering

---

## Final Note

The researcher has done excellent work systematically exploring the solution space. The conclusion that "the CV-LB intercept problem cannot be solved by training class-specific models" is correct and insightful.

The remaining path forward requires either:
1. A fundamentally different validation scheme (GroupKFold)
2. Exact replication of top public kernels
3. A lucky submission that happens to work well on the test set

**The target IS reachable, but it requires breaking out of the current paradigm.**
