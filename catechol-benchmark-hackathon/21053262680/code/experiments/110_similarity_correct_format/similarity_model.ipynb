{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf35a9a8",
   "metadata": {},
   "source": [
    "# Experiment 110: SimilarityAwareModel with CORRECT Submission Format\n",
    "\n",
    "## Goal\n",
    "Combine the SimilarityAwareModel from exp_108 (CV=0.0092) with the correct submission format from exp_109.\n",
    "\n",
    "## Key Fix\n",
    "- exp_108 had wrong submission format (Product 2, Product 3, SM columns)\n",
    "- exp_109 had correct format but used EnsembleModel instead of SimilarityAwareModel\n",
    "- This experiment uses SimilarityAwareModel with correct format\n",
    "\n",
    "## Hypothesis\n",
    "If chemical similarity-based conservative predictions can change the CV-LB relationship,\n",
    "we might see LB better than expected from the line LB = 4.29 × CV + 0.0528."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add data path\n",
    "sys.path.append('/home/data/')\n",
    "\n",
    "from utils import (\n",
    "    INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, \n",
    "    INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, \n",
    "    INPUT_LABELS_FULL_FEATURES,\n",
    "    generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n",
    ")\n",
    "\n",
    "# Override load functions to use local paths\n",
    "DATA_PATH = '/home/data/'\n",
    "TARGET_LABELS = ['Product 2', 'Product 3', 'SM']\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    assert name in [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\", \"smiles\"]\n",
    "    features = pd.read_csv(f'{DATA_PATH}{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e28729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDKit for Morgan fingerprints\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "# Load SMILES lookup\n",
    "smiles_lookup = load_features(\"smiles\")\n",
    "print(f\"SMILES lookup shape: {smiles_lookup.shape}\")\n",
    "\n",
    "# Compute Morgan fingerprints for all solvents\n",
    "def get_morgan_fp(smiles, radius=2, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "\n",
    "smiles_col = [c for c in smiles_lookup.columns if 'SMILES' in c.upper()][0]\n",
    "solvent_fps = {}\n",
    "for solvent_name in smiles_lookup.index:\n",
    "    smiles = smiles_lookup.loc[solvent_name, smiles_col]\n",
    "    fp = get_morgan_fp(smiles)\n",
    "    if fp is not None:\n",
    "        solvent_fps[solvent_name] = fp\n",
    "\n",
    "print(f\"Computed fingerprints for {len(solvent_fps)} solvents\")\n",
    "\n",
    "# Tanimoto similarity function\n",
    "def compute_tanimoto_similarity(fp1, fp2):\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0424d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes and feature engineering\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import reduce\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "_SOLVENT_TABLE_CACHE = None\n",
    "\n",
    "def feature_priority(name: str) -> int:\n",
    "    if name.startswith(\"spange_\"): return 5\n",
    "    if name.startswith(\"acs_\"): return 4\n",
    "    if name.startswith(\"drfps_\"): return 3\n",
    "    if name.startswith(\"frag_\"): return 2\n",
    "    if name.startswith(\"smiles_\"): return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.8):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    high_corr_pairs = []\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                high_corr_pairs.append((col_i, col_j, cval))\n",
    "    for col_i, col_j, cval in high_corr_pairs:\n",
    "        if col_i in to_drop or col_j in to_drop:\n",
    "            continue\n",
    "        p_i = feature_priority(col_i)\n",
    "        p_j = feature_priority(col_j)\n",
    "        if p_i > p_j:\n",
    "            drop = col_j\n",
    "        elif p_j > p_i:\n",
    "            drop = col_i\n",
    "        else:\n",
    "            idx_i = df.columns.get_loc(col_i)\n",
    "            idx_j = df.columns.get_loc(col_j)\n",
    "            drop = col_i if idx_i > idx_j else col_j\n",
    "        to_drop.add(drop)\n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "def add_numeric_features(X_numeric):\n",
    "    X_num = X_numeric.copy()\n",
    "    cols = set(X_num.columns)\n",
    "    if {\"Temperature\", \"Residence Time\"} <= cols:\n",
    "        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n",
    "        T = X_num[\"Temperature\"]\n",
    "        rt = X_num[\"Residence Time\"]\n",
    "        X_num[\"T_x_RT\"] = T * rt\n",
    "        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "        X_num[\"T_inv\"] = 1 / T\n",
    "        X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    return X_num\n",
    "\n",
    "print(\"Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_solvent_feature_table(threshold=0.90):\n",
    "    global _SOLVENT_TABLE_CACHE\n",
    "    if _SOLVENT_TABLE_CACHE is not None:\n",
    "        return _SOLVENT_TABLE_CACHE\n",
    "    print(\">>> Building solvent feature table...\")\n",
    "    sources = [\"spange_descriptors\", \"acs_pca_descriptors\", \"drfps_catechol\", \"fragprints\"]\n",
    "    dfs = []\n",
    "    for src in sources:\n",
    "        df_src = load_features(src).copy()\n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"})\n",
    "            count = values.sum(axis=0).T\n",
    "            drop_cols = count[count == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols)\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        else:\n",
    "            if src == \"spange_descriptors\":\n",
    "                prefix = \"spange\"\n",
    "            elif src == \"acs_pca_descriptors\":\n",
    "                prefix = \"acs\"\n",
    "            else:\n",
    "                prefix = src\n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        dfs.append(df_src)\n",
    "    combined = reduce(lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"), dfs)\n",
    "    combined = combined.set_index(\"SOLVENT NAME\")\n",
    "    print(f\"Combined feature table shape (before corr filter): {combined.shape}\")\n",
    "    combined, _ = filter_correlated_features(combined, threshold=threshold)\n",
    "    print(f\"Final solvent feature table shape: {combined.shape}\")\n",
    "    _SOLVENT_TABLE_CACHE = combined\n",
    "    return combined\n",
    "\n",
    "print(\"build_solvent_feature_table defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self):\n",
    "        self.featurizer = build_solvent_feature_table()\n",
    "        dummy_num = pd.DataFrame([[0] * len(INPUT_LABELS_NUMERIC)], columns=INPUT_LABELS_NUMERIC)\n",
    "        numeric_dim = add_numeric_features(dummy_num).shape[1]\n",
    "        self.feats_dim = numeric_dim + self.featurizer.shape[1]\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n",
    "        X_solvent = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_out = np.concatenate([X_numeric.values, X_solvent.values], axis=1)\n",
    "        return torch.tensor(X_out, dtype=torch.double)\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self):\n",
    "        self.featurizer = build_solvent_feature_table()\n",
    "        dummy_num = pd.DataFrame([[0] * len(INPUT_LABELS_NUMERIC)], columns=INPUT_LABELS_NUMERIC)\n",
    "        numeric_dim = add_numeric_features(dummy_num).shape[1]\n",
    "        self.feats_dim = numeric_dim + 2 * self.featurizer.shape[1] + 1\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n",
    "        X_solvent_A = self.featurizer.loc[X[\"SOLVENT A NAME\"]].values\n",
    "        X_solvent_B = self.featurizer.loc[X[\"SOLVENT B NAME\"]].values\n",
    "        X_solvent_B_pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "        X_out = np.concatenate([X_numeric.values, X_solvent_A, X_solvent_B, X_solvent_B_pct], axis=1)\n",
    "        return torch.tensor(X_out, dtype=torch.double)\n",
    "\n",
    "print(\"Featurizers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "class CatBoostModel(BaseModel):\n",
    "    def __init__(self, data=\"single\", verbose=False, random_state=42):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.cat_params = dict(\n",
    "                random_seed=random_state, loss_function=\"MultiRMSE\",\n",
    "                depth=3, learning_rate=0.07, n_estimators=1050,\n",
    "                l2_leaf_reg=3.5, bootstrap_type=\"Bayesian\",\n",
    "                bagging_temperature=0.225, grow_policy=\"SymmetricTree\",\n",
    "                rsm=0.75, verbose=verbose,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.cat_params = dict(\n",
    "                random_seed=random_state, loss_function=\"MultiRMSE\",\n",
    "                depth=3, learning_rate=0.06, n_estimators=1100,\n",
    "                l2_leaf_reg=2.5, bootstrap_type=\"Bayesian\",\n",
    "                bagging_temperature=0.2, grow_policy=\"SymmetricTree\",\n",
    "                rsm=0.7, verbose=verbose,\n",
    "            )\n",
    "        self.model = None\n",
    "\n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.model = CatBoostRegressor(**self.cat_params)\n",
    "        self.model.fit(X_np, Y_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        out = self.model.predict(X_np)\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        if out.ndim == 1:\n",
    "            out = out.reshape(-1, 1)\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"CatBoostModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1863952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class XGBModel(BaseModel):\n",
    "    def __init__(self, data=\"single\", random_state=42, verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        if data == \"single\":\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state, objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\", subsample=0.5, reg_lambda=0.6,\n",
    "                reg_alpha=0.0, n_estimators=1000, min_child_weight=1,\n",
    "                max_depth=4, max_delta_step=1, learning_rate=0.02,\n",
    "                grow_policy=\"depthwise\", gamma=0.0, colsample_bytree=0.3,\n",
    "                colsample_bylevel=0.6,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state, objective=\"reg:squarederror\",\n",
    "                tree_method=\"approx\", subsample=0.8, reg_lambda=0.5,\n",
    "                reg_alpha=0.0, n_estimators=1200, min_child_weight=1,\n",
    "                max_depth=5, max_delta_step=1, learning_rate=0.015,\n",
    "                grow_policy=\"depthwise\", gamma=0.0, colsample_bytree=0.4,\n",
    "                colsample_bylevel=0.5,\n",
    "            )\n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "\n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"XGBModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d31c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model\n",
    "class EnsembleModel(BaseModel):\n",
    "    def __init__(self, data=\"single\", verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        if data == \"single\":\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        self.cat_model = CatBoostModel(data=data, verbose=verbose)\n",
    "        self.xgb_model = XGBModel(data=data, verbose=verbose)\n",
    "\n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        self.cat_model.train_model(train_X, train_Y, device, verbose)\n",
    "        self.xgb_model.train_model(train_X, train_Y, device, verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        return out\n",
    "\n",
    "print(\"EnsembleModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimilarityAwareModel - THE KEY INNOVATION\n",
    "class SimilarityAwareModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Uses chemical similarity (Tanimoto on Morgan fingerprints) to detect\n",
    "    when we're extrapolating to solvents that are very different from\n",
    "    training solvents. When similarity is low, blend toward training mean.\n",
    "    \"\"\"\n",
    "    def __init__(self, data=\"single\", similarity_threshold=0.3, blend_weight=0.2, verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.blend_weight = blend_weight\n",
    "        self.verbose = verbose\n",
    "        self.base_model = EnsembleModel(data=data, verbose=verbose)\n",
    "        self.train_mean = None\n",
    "        self.train_solvents = None\n",
    "\n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        # Store training mean for blending\n",
    "        self.train_mean = train_Y.mean().values\n",
    "        \n",
    "        # Store training solvents for similarity computation\n",
    "        if self.data_mode == \"single\":\n",
    "            self.train_solvents = train_X[\"SOLVENT NAME\"].unique().tolist()\n",
    "        else:\n",
    "            self.train_solvents = list(set(train_X[\"SOLVENT A NAME\"].unique().tolist() + \n",
    "                                           train_X[\"SOLVENT B NAME\"].unique().tolist()))\n",
    "        \n",
    "        # Train base model\n",
    "        self.base_model.train_model(train_X, train_Y, device, verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get base model predictions\n",
    "        base_preds = self.base_model.predict(X).numpy()\n",
    "        \n",
    "        # Compute similarity for each test sample\n",
    "        if self.data_mode == \"single\":\n",
    "            test_solvents = X[\"SOLVENT NAME\"].values\n",
    "        else:\n",
    "            # For mixed solvents, use the primary solvent (A)\n",
    "            test_solvents = X[\"SOLVENT A NAME\"].values\n",
    "        \n",
    "        # Compute max similarity to any training solvent\n",
    "        similarities = []\n",
    "        for test_solvent in test_solvents:\n",
    "            if test_solvent not in solvent_fps:\n",
    "                similarities.append(0.0)  # Unknown solvent = no similarity\n",
    "                continue\n",
    "            test_fp = solvent_fps[test_solvent]\n",
    "            max_sim = 0.0\n",
    "            for train_solvent in self.train_solvents:\n",
    "                if train_solvent in solvent_fps:\n",
    "                    sim = compute_tanimoto_similarity(test_fp, solvent_fps[train_solvent])\n",
    "                    max_sim = max(max_sim, sim)\n",
    "            similarities.append(max_sim)\n",
    "        \n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        # Blend toward training mean for low-similarity samples\n",
    "        final_preds = base_preds.copy()\n",
    "        for i, sim in enumerate(similarities):\n",
    "            if sim < self.similarity_threshold:\n",
    "                # Blend toward training mean\n",
    "                final_preds[i] = (1 - self.blend_weight) * base_preds[i] + self.blend_weight * self.train_mean\n",
    "        \n",
    "        # Clip and normalize\n",
    "        final_preds = np.clip(final_preds, a_min=0.0, a_max=None)\n",
    "        if final_preds.shape[1] > 1:\n",
    "            totals = final_preds.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            final_preds = final_preds / divisor\n",
    "        \n",
    "        return torch.tensor(final_preds, dtype=torch.double)\n",
    "\n",
    "print(\"SimilarityAwareModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick CV evaluation\n",
    "import tqdm\n",
    "\n",
    "def evaluate_cv(model_class, **kwargs):\n",
    "    \"\"\"Evaluate using leave-one-out CV\"\"\"\n",
    "    # Single solvent\n",
    "    X, Y = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X, Y)\n",
    "    all_preds_single = []\n",
    "    all_true_single = []\n",
    "    \n",
    "    for fold_idx, split in tqdm.tqdm(enumerate(split_generator), desc=\"single\"):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        model = model_class(data='single', **kwargs)\n",
    "        model.train_model(train_X, train_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        all_preds_single.append(predictions.numpy())\n",
    "        all_true_single.append(test_Y.values)\n",
    "    \n",
    "    preds_single = np.vstack(all_preds_single)\n",
    "    true_single = np.vstack(all_true_single)\n",
    "    mse_single = np.mean((preds_single - true_single) ** 2)\n",
    "    print(f\"Single Solvent MSE: {mse_single:.6f}\")\n",
    "    \n",
    "    # Full data\n",
    "    X, Y = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "    all_preds_full = []\n",
    "    all_true_full = []\n",
    "    \n",
    "    for fold_idx, split in tqdm.tqdm(enumerate(split_generator), desc=\"full\"):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        model = model_class(data='full', **kwargs)\n",
    "        model.train_model(train_X, train_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        all_preds_full.append(predictions.numpy())\n",
    "        all_true_full.append(test_Y.values)\n",
    "    \n",
    "    preds_full = np.vstack(all_preds_full)\n",
    "    true_full = np.vstack(all_true_full)\n",
    "    mse_full = np.mean((preds_full - true_full) ** 2)\n",
    "    print(f\"Full Data MSE: {mse_full:.6f}\")\n",
    "    \n",
    "    # Combined\n",
    "    n_single = preds_single.shape[0] * preds_single.shape[1]\n",
    "    n_full = preds_full.shape[0] * preds_full.shape[1]\n",
    "    combined_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "    print(f\"\\nCombined MSE (CV score): {combined_mse:.6f}\")\n",
    "    \n",
    "    return combined_mse, mse_single, mse_full\n",
    "\n",
    "# Evaluate SimilarityAwareModel with best parameters from exp_108\n",
    "print(\"Evaluating SimilarityAwareModel (st=0.3, bw=0.2)...\")\n",
    "cv_score, mse_single, mse_full = evaluate_cv(SimilarityAwareModel, similarity_threshold=0.3, blend_weight=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ca00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "import json\n",
    "\n",
    "metrics = {\n",
    "    'cv_score': float(cv_score),\n",
    "    'mse_single': float(mse_single),\n",
    "    'mse_full': float(mse_full),\n",
    "    'similarity_threshold': 0.3,\n",
    "    'blend_weight': 0.2,\n",
    "    'notes': 'SimilarityAwareModel with CORRECT submission format. Uses Tanimoto similarity on Morgan fingerprints.'\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/110_similarity_correct_format/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Metrics saved\")\n",
    "print(f\"\\nCV Score: {cv_score:.6f}\")\n",
    "print(f\"Expected LB from line: {4.29 * cv_score + 0.0528:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601d284",
   "metadata": {},
   "source": [
    "## Submission Cells (CORRECT FORMAT)\n",
    "\n",
    "These cells use the EXACT format from the template:\n",
    "- task, fold, row, target_1, target_2, target_3 columns\n",
    "- task=0 for single_solvent, task=1 for full data\n",
    "\n",
    "**CRITICAL: Using SimilarityAwareModel, NOT EnsembleModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = SimilarityAwareModel(data='single', similarity_threshold=0.3, blend_weight=0.2)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = SimilarityAwareModel(data='full', similarity_threshold=0.3, blend_weight=0.2)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index(drop=True)\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/code/experiments/110_similarity_correct_format/submission.csv\", index=True)\n",
    "\n",
    "# Also copy to main submission folder\n",
    "import shutil\n",
    "shutil.copy(\"/home/code/experiments/110_similarity_correct_format/submission.csv\", \"/home/submission/submission.csv\")\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nLast 5 rows:\")\n",
    "print(submission.tail())\n",
    "\n",
    "# Read back and verify format\n",
    "sub_check = pd.read_csv(\"/home/submission/submission.csv\")\n",
    "print(f\"\\nRead back columns: {sub_check.columns.tolist()}\")\n",
    "expected_cols = ['id', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']\n",
    "assert list(sub_check.columns) == expected_cols, f\"Wrong columns: {list(sub_check.columns)}\"\n",
    "print(f\"\\n✅ FORMAT VERIFIED: {expected_cols}\")\n",
    "print(f\"\\n✅ MODEL CLASS: SimilarityAwareModel (matches CV computation)\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
