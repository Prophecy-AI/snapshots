{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6140ca5",
   "metadata": {},
   "source": [
    "# Experiment 092: Conservative Predictions for Extrapolation\n",
    "\n",
    "**Goal**: Attack the CV-LB intercept problem by detecting extrapolation and blending predictions toward training mean.\n",
    "\n",
    "**Rationale**: The CV-LB relationship has intercept 0.0525 > target 0.0347. This intercept represents structural distribution shift. If we detect when we're extrapolating (predicting for solvents far from training distribution) and blend toward the training mean, we can reduce the intercept.\n",
    "\n",
    "**Approach**:\n",
    "1. Use our best model (GP+MLP+LGBM ensemble) as base\n",
    "2. Compute extrapolation score based on distance to training solvents\n",
    "3. Blend predictions toward training mean for high-uncertainty cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acefc04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:35:59.993557Z",
     "iopub.status.busy": "2026-01-16T12:35:59.993022Z",
     "iopub.status.idle": "2026-01-16T12:36:01.635925Z",
     "shell.execute_reply": "2026-01-16T12:36:01.635472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import lightgbm as lgb\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define constants\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\",\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT NAME\",\n",
    "]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "# Local data loading functions\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'/home/data/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).all(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Imports complete\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46854377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.637039Z",
     "iopub.status.busy": "2026-01-16T12:36:01.636889Z",
     "iopub.status.idle": "2026-01-16T12:36:01.639838Z",
     "shell.execute_reply": "2026-01-16T12:36:01.639490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10b8b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.640698Z",
     "iopub.status.busy": "2026-01-16T12:36:01.640603Z",
     "iopub.status.idle": "2026-01-16T12:36:01.646320Z",
     "shell.execute_reply": "2026-01-16T12:36:01.645980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizers with Arrhenius features (from best model)\n",
    "class PrecomputedFeaturizerWithArrhenius(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 5  # +5 for Time, Temp, 1/T, log(t), t/T\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Arrhenius-inspired features\n",
    "        inv_temp = 1.0 / (temp + 273.15)  # 1/T in Kelvin\n",
    "        log_time = np.log(res_time + 1)  # log(t+1)\n",
    "        time_over_temp = res_time / (temp + 273.15)  # t/T\n",
    "        \n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        feats = self.features.loc[solvent_names].values\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, inv_temp, log_time, time_over_temp, feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "    \n",
    "    def get_solvent_features(self, X):\n",
    "        \"\"\"Get only solvent features for extrapolation detection.\"\"\"\n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        return self.features.loc[solvent_names].values\n",
    "\n",
    "class PrecomputedFeaturizerMixedWithArrhenius(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 6  # +6 for Time, Temp, %B, 1/T, log(t), t/T\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Arrhenius-inspired features\n",
    "        inv_temp = 1.0 / (temp + 273.15)\n",
    "        log_time = np.log(res_time + 1)\n",
    "        time_over_temp = res_time / (temp + 273.15)\n",
    "        \n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        \n",
    "        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, sb_pct, inv_temp, log_time, time_over_temp, mixture_feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "    \n",
    "    def get_solvent_features(self, X):\n",
    "        \"\"\"Get mixture solvent features for extrapolation detection.\"\"\"\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        return (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "\n",
    "print(\"Featurizers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2947b05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.647143Z",
     "iopub.status.busy": "2026-01-16T12:36:01.647051Z",
     "iopub.status.idle": "2026-01-16T12:36:01.650405Z",
     "shell.execute_reply": "2026-01-16T12:36:01.650070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP defined\n"
     ]
    }
   ],
   "source": [
    "# MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[64, 32], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"MLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0dd72cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:49.965489Z",
     "iopub.status.busy": "2026-01-16T12:36:49.964904Z",
     "iopub.status.idle": "2026-01-16T12:36:49.975791Z",
     "shell.execute_reply": "2026-01-16T12:36:49.975428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConservativeExtrapolationModel defined\n"
     ]
    }
   ],
   "source": [
    "# Conservative Extrapolation Model\n",
    "class ConservativeExtrapolationModel(BaseModel):\n",
    "    \"\"\"Model that detects extrapolation and blends toward training mean.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', blend_threshold=0.5, blend_strength=0.3):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.blend_threshold = blend_threshold  # Percentile threshold for extrapolation\n",
    "        self.blend_strength = blend_strength  # How much to blend toward mean (0-1)\n",
    "        \n",
    "        # Featurizer\n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerWithArrhenius('spange_descriptors')\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixedWithArrhenius('spange_descriptors')\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = SimpleMLP(\n",
    "            input_dim=self.smiles_featurizer.feats_dim,\n",
    "            output_dim=3,\n",
    "            hidden_dims=[64, 32],\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # LightGBM\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "        \n",
    "        # Scaler and extrapolation detector\n",
    "        self.scaler = StandardScaler()\n",
    "        self.solvent_scaler = StandardScaler()\n",
    "        self.nn_detector = None\n",
    "        self.train_mean = None\n",
    "        self.train_distances = None\n",
    "        \n",
    "        # Ensemble weights (MLP and LGBM only - simpler)\n",
    "        self.weights = [0.5, 0.5]  # MLP, LGBM\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32,\n",
    "                    optimizer=torch.optim.Adam, criterion=nn.MSELoss, device=None, verbose=False):\n",
    "        # Store training mean for blending\n",
    "        self.train_mean = train_Y.values.mean(axis=0)\n",
    "        \n",
    "        # Featurize\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Get solvent features for extrapolation detection\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(train_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.fit_transform(solvent_feats)\n",
    "        \n",
    "        # Fit nearest neighbor detector on training solvent features\n",
    "        self.nn_detector = NearestNeighbors(n_neighbors=min(5, len(solvent_feats_scaled)))\n",
    "        self.nn_detector.fit(solvent_feats_scaled)\n",
    "        \n",
    "        # Compute training distances for threshold calibration\n",
    "        train_distances, _ = self.nn_detector.kneighbors(solvent_feats_scaled)\n",
    "        self.train_distances = train_distances.mean(axis=1)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm.fit(X_scaled_df, train_Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor_scaled, train_Y_tensor),\n",
    "            batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        criterion_inst = criterion()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_inst.zero_grad()\n",
    "                loss = criterion_inst(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer_inst.step()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Get solvent features for extrapolation detection\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(test_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.transform(solvent_feats)\n",
    "        \n",
    "        # Compute extrapolation scores\n",
    "        test_distances, _ = self.nn_detector.kneighbors(solvent_feats_scaled)\n",
    "        extrapolation_scores = test_distances.mean(axis=1)\n",
    "        \n",
    "        # Compute threshold based on training distances\n",
    "        threshold = np.percentile(self.train_distances, self.blend_threshold * 100)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_preds = (\n",
    "            self.weights[0] * mlp_preds +\n",
    "            self.weights[1] * lgb_preds\n",
    "        )\n",
    "        \n",
    "        # Apply conservative blending for extrapolation\n",
    "        # Higher extrapolation score -> blend more toward training mean\n",
    "        blend_weights = np.clip((extrapolation_scores - threshold) / (threshold + 1e-8), 0, 1)\n",
    "        blend_weights = blend_weights.reshape(-1, 1) * self.blend_strength\n",
    "        \n",
    "        # Blend toward training mean\n",
    "        final_preds = (1 - blend_weights) * ensemble_preds + blend_weights * self.train_mean\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print(\"ConservativeExtrapolationModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b453bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:49.976647Z",
     "iopub.status.busy": "2026-01-16T12:36:49.976545Z",
     "iopub.status.idle": "2026-01-16T12:36:50.375278Z",
     "shell.execute_reply": "2026-01-16T12:36:50.374925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: (656, 3), (656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([5, 3])\n",
      "Sample predictions:\n",
      "tensor([[-0.0382, -0.0159,  0.9191],\n",
      "        [-0.0209,  0.0081,  0.8811],\n",
      "        [-0.0084,  0.0255,  0.8335]], dtype=torch.float64)\n",
      "Training mean: [0.14993233 0.12337957 0.52219232]\n"
     ]
    }
   ],
   "source": [
    "# Test the model quickly\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape}, {Y_single.shape}\")\n",
    "\n",
    "# Quick test\n",
    "model = ConservativeExtrapolationModel(data='single', blend_threshold=0.5, blend_strength=0.3)\n",
    "model.train_model(X_single, Y_single, num_epochs=10)\n",
    "preds = model.predict(X_single[:5])\n",
    "print(f\"Test predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions:\\n{preds[:3]}\")\n",
    "print(f\"Training mean: {model.train_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f363dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:59.529643Z",
     "iopub.status.busy": "2026-01-16T12:36:59.529167Z",
     "iopub.status.idle": "2026-01-16T12:38:54.966915Z",
     "shell.execute_reply": "2026-01-16T12:38:54.966504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with blend_threshold=0.5, blend_strength=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 0: MSE = 0.045027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 1: MSE = 0.021312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 2: MSE = 0.008168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 3: MSE = 0.018493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 4: MSE = 0.024982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 5: MSE = 0.004683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 6: MSE = 0.021023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 7: MSE = 0.015934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 8: MSE = 0.023208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 9: MSE = 0.012468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 10: MSE = 0.004757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 11: MSE = 0.017689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 12: MSE = 0.006315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 13: MSE = 0.001582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 14: MSE = 0.003102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 15: MSE = 0.027427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 16: MSE = 0.017386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 17: MSE = 0.011708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 18: MSE = 0.008918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 19: MSE = 0.003271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 20: MSE = 0.005417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 21: MSE = 0.010132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 22: MSE = 0.019211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Fold 23: MSE = 0.009560\n",
      "\n",
      "Single Solvent CV MSE: 0.014241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 0: MSE = 0.025531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 1: MSE = 0.014065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 2: MSE = 0.006935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 3: MSE = 0.021678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 4: MSE = 0.012799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 5: MSE = 0.020548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 6: MSE = 0.003504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 7: MSE = 0.009203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 8: MSE = 0.010505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 9: MSE = 0.016260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 10: MSE = 0.002450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 11: MSE = 0.020802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fold 12: MSE = 0.017703\n",
      "\n",
      "Full Data CV MSE: 0.013999\n",
      "\n",
      "=== Combined CV MSE: 0.014120 ===\n"
     ]
    }
   ],
   "source": [
    "# Run CV with different blend parameters\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(blend_threshold=0.5, blend_strength=0.3, verbose=True):\n",
    "    \"\"\"Compute CV score with conservative extrapolation.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ConservativeExtrapolationModel(\n",
    "            data='single', \n",
    "            blend_threshold=blend_threshold, \n",
    "            blend_strength=blend_strength\n",
    "        )\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ConservativeExtrapolationModel(\n",
    "            data='full', \n",
    "            blend_threshold=blend_threshold, \n",
    "            blend_strength=blend_strength\n",
    "        )\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    # Combined CV\n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "# First test with default parameters\n",
    "print(\"Testing with blend_threshold=0.5, blend_strength=0.3\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score(blend_threshold=0.5, blend_strength=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd901406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:42:19.266722Z",
     "iopub.status.busy": "2026-01-16T12:42:19.266346Z",
     "iopub.status.idle": "2026-01-16T12:42:19.268753Z",
     "shell.execute_reply": "2026-01-16T12:42:19.268432Z"
    }
   },
   "outputs": [],
   "source": [
    "# The conservative blending approach hurts CV because CV tests on held-out solvents\n",
    "# that are still similar to training solvents. The blending would only help on truly\n",
    "# unseen test solvents.\\n\\n# Let me try a different approach: add solvent similarity features\\n# This helps the model understand when it's extrapolating\\n\\nprint(\\\"CV with no blending: 0.010097\\\")\\nprint(\\\"CV with blending (0.3): 0.014120\\\")\\nprint(\\\"Baseline: 0.008298\\\")\\nprint(\\\"\\\\nConclusion: Conservative blending hurts CV performance.\\\")\\nprint(\\\"The approach might help LB but we can't validate it with CV.\\\")\\nprint(\\\"\\\\nLet me try adding solvent similarity features instead.\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38198e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:42:48.596351Z",
     "iopub.status.busy": "2026-01-16T12:42:48.595848Z",
     "iopub.status.idle": "2026-01-16T12:42:48.605056Z",
     "shell.execute_reply": "2026-01-16T12:42:48.604692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilarityAwareModel defined\n"
     ]
    }
   ],
   "source": [
    "# New approach: Add solvent similarity features\n",
    "# This helps the model understand how similar the test solvent is to training solvents\n",
    "\n",
    "class SimilarityAwareModel(BaseModel):\n",
    "    \"\"\"Model with solvent similarity features.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "        # Featurizer\n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerWithArrhenius('spange_descriptors')\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixedWithArrhenius('spange_descriptors')\n",
    "        \n",
    "        # MLP (will be created after we know input dim)\n",
    "        self.mlp = None\n",
    "        \n",
    "        # LightGBM\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "        \n",
    "        # Scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        self.solvent_scaler = StandardScaler()\n",
    "        \n",
    "        # Training solvent features for similarity computation\n",
    "        self.train_solvent_feats = None\n",
    "        \n",
    "        # Ensemble weights\n",
    "        self.weights = [0.5, 0.5]  # MLP, LGBM\n",
    "        \n",
    "    def compute_similarity_features(self, solvent_feats):\n",
    "        \"\"\"Compute similarity features to training solvents.\"\"\"\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        \n",
    "        distances = euclidean_distances(solvent_feats, self.train_solvent_feats)\n",
    "        \n",
    "        # Similarity features\n",
    "        min_dist = distances.min(axis=1, keepdims=True)\n",
    "        mean_dist = distances.mean(axis=1, keepdims=True)\n",
    "        max_dist = distances.max(axis=1, keepdims=True)\n",
    "        \n",
    "        # Number of \"similar\" training solvents (within threshold)\n",
    "        threshold = np.median(distances)\n",
    "        n_similar = (distances < threshold).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return np.hstack([min_dist, mean_dist, max_dist, n_similar])\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32,\n",
    "                    optimizer=torch.optim.Adam, criterion=nn.MSELoss, device=None, verbose=False):\n",
    "        # Featurize\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        # Get solvent features\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(train_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.fit_transform(solvent_feats)\n",
    "        self.train_solvent_feats = solvent_feats_scaled\n",
    "        \n",
    "        # Compute similarity features\n",
    "        sim_feats = self.compute_similarity_features(solvent_feats_scaled)\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = np.hstack([X_np, sim_feats])\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_combined)\n",
    "        \n",
    "        # Create MLP with correct input dim\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        self.mlp = SimpleMLP(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=3,\n",
    "            hidden_dims=[64, 32],\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm.fit(X_scaled_df, train_Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor_scaled, train_Y_tensor),\n",
    "            batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        criterion_inst = criterion()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_inst.zero_grad()\n",
    "                loss = criterion_inst(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer_inst.step()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        \n",
    "        # Get solvent features\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(test_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.transform(solvent_feats)\n",
    "        \n",
    "        # Compute similarity features\n",
    "        sim_feats = self.compute_similarity_features(solvent_feats_scaled)\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = np.hstack([X_np, sim_feats])\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.transform(X_combined)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_preds = (\n",
    "            self.weights[0] * mlp_preds +\n",
    "            self.weights[1] * lgb_preds\n",
    "        )\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print(\"SimilarityAwareModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae723df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SimilarityAwareModel CV\n",
    "def compute_similarity_cv(verbose=True):\n",
    "    \"\"\"Compute CV score with similarity features.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = SimilarityAwareModel(data='single')\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = SimilarityAwareModel(data='full')\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "print(\"Testing SimilarityAwareModel...\")\n",
    "single_cv_sim, full_cv_sim, combined_cv_sim = compute_similarity_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different blend parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing different blend parameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test different combinations\n",
    "for threshold in [0.3, 0.5, 0.7]:\n",
    "    for strength in [0.1, 0.3, 0.5]:\n",
    "        print(f\"\\nTesting threshold={threshold}, strength={strength}\")\n",
    "        _, _, cv = compute_cv_score(blend_threshold=threshold, blend_strength=strength, verbose=False)\n",
    "        results.append({'threshold': threshold, 'strength': strength, 'cv': cv})\n",
    "        print(f\"CV = {cv:.6f}\")\n",
    "\n",
    "# Find best\n",
    "results_df = pd.DataFrame(results)\n",
    "best_idx = results_df['cv'].idxmin()\n",
    "best_params = results_df.loc[best_idx]\n",
    "print(f\"\\n=== Best Parameters ===\")\n",
    "print(f\"Threshold: {best_params['threshold']}\")\n",
    "print(f\"Strength: {best_params['strength']}\")\n",
    "print(f\"CV: {best_params['cv']:.6f}\")\n",
    "print(f\"\\nBaseline CV: 0.008298\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "best_threshold = best_params['threshold']\n",
    "best_strength = best_params['strength']\n",
    "best_cv = best_params['cv']\n",
    "\n",
    "results_dict = {\n",
    "    'cv_score': float(best_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'combined_cv': float(combined_cv),\n",
    "    'best_threshold': float(best_threshold),\n",
    "    'best_strength': float(best_strength),\n",
    "    'model': 'ConservativeExtrapolationModel (GP+MLP+LGBM with extrapolation blending)',\n",
    "    'baseline_cv': 0.008298,\n",
    "    'all_results': results\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/092_conservative_extrapolation/metrics.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Best CV: {best_cv:.6f}\")\n",
    "print(f\"Baseline CV: 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - best_cv) / 0.008298 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bc8f7",
   "metadata": {},
   "source": [
    "## Generate Submission (if CV is better than baseline)\n",
    "\n",
    "The following cells follow the official template structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28662c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameters for submission\n",
    "BEST_THRESHOLD = best_threshold\n",
    "BEST_STRENGTH = best_strength\n",
    "\n",
    "print(f\"Using threshold={BEST_THRESHOLD}, strength={BEST_STRENGTH}\")\n",
    "print(f\"Best CV: {best_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09537022",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ConservativeExtrapolationModel(data='single', blend_threshold=BEST_THRESHOLD, blend_strength=BEST_STRENGTH)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ConservativeExtrapolationModel(data='full', blend_threshold=BEST_THRESHOLD, blend_strength=BEST_STRENGTH)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27353cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
