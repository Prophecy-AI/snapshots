{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6140ca5",
   "metadata": {},
   "source": [
    "# Experiment 092: Conservative Predictions for Extrapolation\n",
    "\n",
    "**Goal**: Attack the CV-LB intercept problem by detecting extrapolation and blending predictions toward training mean.\n",
    "\n",
    "**Rationale**: The CV-LB relationship has intercept 0.0525 > target 0.0347. This intercept represents structural distribution shift. If we detect when we're extrapolating (predicting for solvents far from training distribution) and blend toward the training mean, we can reduce the intercept.\n",
    "\n",
    "**Approach**:\n",
    "1. Use our best model (GP+MLP+LGBM ensemble) as base\n",
    "2. Compute extrapolation score based on distance to training solvents\n",
    "3. Blend predictions toward training mean for high-uncertainty cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acefc04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:35:59.993557Z",
     "iopub.status.busy": "2026-01-16T12:35:59.993022Z",
     "iopub.status.idle": "2026-01-16T12:36:01.635925Z",
     "shell.execute_reply": "2026-01-16T12:36:01.635472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import lightgbm as lgb\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define constants\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\",\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT NAME\",\n",
    "]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "# Local data loading functions\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'/home/data/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).all(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Imports complete\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46854377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.637039Z",
     "iopub.status.busy": "2026-01-16T12:36:01.636889Z",
     "iopub.status.idle": "2026-01-16T12:36:01.639838Z",
     "shell.execute_reply": "2026-01-16T12:36:01.639490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10b8b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.640698Z",
     "iopub.status.busy": "2026-01-16T12:36:01.640603Z",
     "iopub.status.idle": "2026-01-16T12:36:01.646320Z",
     "shell.execute_reply": "2026-01-16T12:36:01.645980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizers with Arrhenius features (from best model)\n",
    "class PrecomputedFeaturizerWithArrhenius(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 5  # +5 for Time, Temp, 1/T, log(t), t/T\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Arrhenius-inspired features\n",
    "        inv_temp = 1.0 / (temp + 273.15)  # 1/T in Kelvin\n",
    "        log_time = np.log(res_time + 1)  # log(t+1)\n",
    "        time_over_temp = res_time / (temp + 273.15)  # t/T\n",
    "        \n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        feats = self.features.loc[solvent_names].values\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, inv_temp, log_time, time_over_temp, feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "    \n",
    "    def get_solvent_features(self, X):\n",
    "        \"\"\"Get only solvent features for extrapolation detection.\"\"\"\n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        return self.features.loc[solvent_names].values\n",
    "\n",
    "class PrecomputedFeaturizerMixedWithArrhenius(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 6  # +6 for Time, Temp, %B, 1/T, log(t), t/T\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Arrhenius-inspired features\n",
    "        inv_temp = 1.0 / (temp + 273.15)\n",
    "        log_time = np.log(res_time + 1)\n",
    "        time_over_temp = res_time / (temp + 273.15)\n",
    "        \n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        \n",
    "        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, sb_pct, inv_temp, log_time, time_over_temp, mixture_feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "    \n",
    "    def get_solvent_features(self, X):\n",
    "        \"\"\"Get mixture solvent features for extrapolation detection.\"\"\"\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        return (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "\n",
    "print(\"Featurizers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2947b05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.647143Z",
     "iopub.status.busy": "2026-01-16T12:36:01.647051Z",
     "iopub.status.idle": "2026-01-16T12:36:01.650405Z",
     "shell.execute_reply": "2026-01-16T12:36:01.650070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP defined\n"
     ]
    }
   ],
   "source": [
    "# MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[64, 32], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"MLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0dd72cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:01.651318Z",
     "iopub.status.busy": "2026-01-16T12:36:01.651217Z",
     "iopub.status.idle": "2026-01-16T12:36:01.661137Z",
     "shell.execute_reply": "2026-01-16T12:36:01.660802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConservativeExtrapolationModel defined\n"
     ]
    }
   ],
   "source": [
    "# Conservative Extrapolation Model\n",
    "class ConservativeExtrapolationModel(BaseModel):\n",
    "    \"\"\"Model that detects extrapolation and blends toward training mean.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', blend_threshold=0.5, blend_strength=0.3):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.blend_threshold = blend_threshold  # Percentile threshold for extrapolation\n",
    "        self.blend_strength = blend_strength  # How much to blend toward mean (0-1)\n",
    "        \n",
    "        # Featurizer\n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerWithArrhenius('spange_descriptors')\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixedWithArrhenius('spange_descriptors')\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = SimpleMLP(\n",
    "            input_dim=self.smiles_featurizer.feats_dim,\n",
    "            output_dim=3,\n",
    "            hidden_dims=[64, 32],\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # LightGBM\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "        \n",
    "        # GP for uncertainty\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42)\n",
    "        \n",
    "        # Scaler and extrapolation detector\n",
    "        self.scaler = StandardScaler()\n",
    "        self.solvent_scaler = StandardScaler()\n",
    "        self.nn_detector = None\n",
    "        self.train_mean = None\n",
    "        self.train_distances = None\n",
    "        \n",
    "        # Ensemble weights (from best model)\n",
    "        self.weights = [0.3, 0.4, 0.3]  # GP, MLP, LGBM\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32,\n",
    "                    optimizer=torch.optim.Adam, criterion=nn.MSELoss, device=None, verbose=False):\n",
    "        # Store training mean for blending\n",
    "        self.train_mean = train_Y.values.mean(axis=0)\n",
    "        \n",
    "        # Featurize\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Get solvent features for extrapolation detection\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(train_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.fit_transform(solvent_feats)\n",
    "        \n",
    "        # Fit nearest neighbor detector on training solvent features\n",
    "        self.nn_detector = NearestNeighbors(n_neighbors=min(5, len(solvent_feats_scaled)))\n",
    "        self.nn_detector.fit(solvent_feats_scaled)\n",
    "        \n",
    "        # Compute training distances for threshold calibration\n",
    "        train_distances, _ = self.nn_detector.kneighbors(solvent_feats_scaled)\n",
    "        self.train_distances = train_distances.mean(axis=1)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm.fit(X_scaled_df, train_Y_np)\n",
    "        \n",
    "        # Train GP (on subset for speed)\n",
    "        n_gp = min(200, len(X_scaled))\n",
    "        indices = np.random.choice(len(X_scaled), n_gp, replace=False)\n",
    "        self.gp.fit(X_scaled[indices], train_Y_np[indices, 0])  # GP for first target only\n",
    "        \n",
    "        # Train MLP\n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor_scaled, train_Y_tensor),\n",
    "            batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        criterion_inst = criterion()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_inst.zero_grad()\n",
    "                loss = criterion_inst(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer_inst.step()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Get solvent features for extrapolation detection\n",
    "        solvent_feats = self.smiles_featurizer.get_solvent_features(test_X)\n",
    "        solvent_feats_scaled = self.solvent_scaler.transform(solvent_feats)\n",
    "        \n",
    "        # Compute extrapolation scores\n",
    "        test_distances, _ = self.nn_detector.kneighbors(solvent_feats_scaled)\n",
    "        extrapolation_scores = test_distances.mean(axis=1)\n",
    "        \n",
    "        # Compute threshold based on training distances\n",
    "        threshold = np.percentile(self.train_distances, self.blend_threshold * 100)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # GP predictions (with uncertainty)\n",
    "        gp_mean, gp_std = self.gp.predict(X_scaled, return_std=True)\n",
    "        # Expand GP prediction to all targets (use same prediction for simplicity)\n",
    "        gp_preds = np.column_stack([gp_mean, gp_mean, gp_mean])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_preds = (\n",
    "            self.weights[0] * gp_preds +\n",
    "            self.weights[1] * mlp_preds +\n",
    "            self.weights[2] * lgb_preds\n",
    "        )\n",
    "        \n",
    "        # Apply conservative blending for extrapolation\n",
    "        # Higher extrapolation score -> blend more toward training mean\n",
    "        blend_weights = np.clip((extrapolation_scores - threshold) / threshold, 0, 1)\n",
    "        blend_weights = blend_weights.reshape(-1, 1) * self.blend_strength\n",
    "        \n",
    "        # Blend toward training mean\n",
    "        final_preds = (1 - blend_weights) * ensemble_preds + blend_weights * self.train_mean\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print(\"ConservativeExtrapolationModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b453bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:36:09.455350Z",
     "iopub.status.busy": "2026-01-16T12:36:09.454916Z",
     "iopub.status.idle": "2026-01-16T12:36:11.333440Z",
     "shell.execute_reply": "2026-01-16T12:36:11.333059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: (656, 3), (656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: torch.Size([5, 3])\n",
      "Sample predictions:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n",
      "Training mean: [0.14993233 0.12337957 0.52219232]\n"
     ]
    }
   ],
   "source": [
    "# Test the model quickly\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape}, {Y_single.shape}\")\n",
    "\n",
    "# Quick test\n",
    "model = ConservativeExtrapolationModel(data='single', blend_threshold=0.5, blend_strength=0.3)\n",
    "model.train_model(X_single, Y_single, num_epochs=10)\n",
    "preds = model.predict(X_single[:5])\n",
    "print(f\"Test predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions:\\n{preds[:3]}\")\n",
    "print(f\"Training mean: {model.train_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f363dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with different blend parameters\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(blend_threshold=0.5, blend_strength=0.3, verbose=True):\n",
    "    \"\"\"Compute CV score with conservative extrapolation.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ConservativeExtrapolationModel(\n",
    "            data='single', \n",
    "            blend_threshold=blend_threshold, \n",
    "            blend_strength=blend_strength\n",
    "        )\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ConservativeExtrapolationModel(\n",
    "            data='full', \n",
    "            blend_threshold=blend_threshold, \n",
    "            blend_strength=blend_strength\n",
    "        )\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    # Combined CV\n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "# First test with default parameters\n",
    "print(\"Testing with blend_threshold=0.5, blend_strength=0.3\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score(blend_threshold=0.5, blend_strength=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different blend parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing different blend parameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test different combinations\n",
    "for threshold in [0.3, 0.5, 0.7]:\n",
    "    for strength in [0.1, 0.3, 0.5]:\n",
    "        print(f\"\\nTesting threshold={threshold}, strength={strength}\")\n",
    "        _, _, cv = compute_cv_score(blend_threshold=threshold, blend_strength=strength, verbose=False)\n",
    "        results.append({'threshold': threshold, 'strength': strength, 'cv': cv})\n",
    "        print(f\"CV = {cv:.6f}\")\n",
    "\n",
    "# Find best\n",
    "results_df = pd.DataFrame(results)\n",
    "best_idx = results_df['cv'].idxmin()\n",
    "best_params = results_df.loc[best_idx]\n",
    "print(f\"\\n=== Best Parameters ===\")\n",
    "print(f\"Threshold: {best_params['threshold']}\")\n",
    "print(f\"Strength: {best_params['strength']}\")\n",
    "print(f\"CV: {best_params['cv']:.6f}\")\n",
    "print(f\"\\nBaseline CV: 0.008298\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "best_threshold = best_params['threshold']\n",
    "best_strength = best_params['strength']\n",
    "best_cv = best_params['cv']\n",
    "\n",
    "results_dict = {\n",
    "    'cv_score': float(best_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'combined_cv': float(combined_cv),\n",
    "    'best_threshold': float(best_threshold),\n",
    "    'best_strength': float(best_strength),\n",
    "    'model': 'ConservativeExtrapolationModel (GP+MLP+LGBM with extrapolation blending)',\n",
    "    'baseline_cv': 0.008298,\n",
    "    'all_results': results\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/092_conservative_extrapolation/metrics.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Best CV: {best_cv:.6f}\")\n",
    "print(f\"Baseline CV: 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - best_cv) / 0.008298 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bc8f7",
   "metadata": {},
   "source": [
    "## Generate Submission (if CV is better than baseline)\n",
    "\n",
    "The following cells follow the official template structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28662c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameters for submission\n",
    "BEST_THRESHOLD = best_threshold\n",
    "BEST_STRENGTH = best_strength\n",
    "\n",
    "print(f\"Using threshold={BEST_THRESHOLD}, strength={BEST_STRENGTH}\")\n",
    "print(f\"Best CV: {best_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09537022",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ConservativeExtrapolationModel(data='single', blend_threshold=BEST_THRESHOLD, blend_strength=BEST_STRENGTH)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ConservativeExtrapolationModel(data='full', blend_threshold=BEST_THRESHOLD, blend_strength=BEST_STRENGTH)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27353cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
