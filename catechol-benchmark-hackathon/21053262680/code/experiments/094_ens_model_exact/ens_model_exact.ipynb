{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d50412",
   "metadata": {},
   "source": [
    "# Experiment 094: Ens-Model Kernel Exact Implementation\n",
    "\n",
    "**Goal**: Implement the ens-model kernel exactly as shown in the public kernel.\n",
    "\n",
    "**Key Features**:\n",
    "1. ALL feature sources (spange, acs_pca, drfps, fragprints, smiles)\n",
    "2. Correlation-based filtering with priority (spange > acs > drfps > frag > smiles)\n",
    "3. CatBoost + XGBoost ensemble with weights: single (7:6), full (1:2)\n",
    "4. Clipping to non-negative and renormalization if sum > 1\n",
    "5. Engineered numeric features (T_x_RT, RT_log, T_inv, RT_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e0a95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:24.892854Z",
     "iopub.status.busy": "2026-01-16T13:09:24.892330Z",
     "iopub.status.idle": "2026-01-16T13:09:25.852369Z",
     "shell.execute_reply": "2026-01-16T13:09:25.851980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efc82b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.853597Z",
     "iopub.status.busy": "2026-01-16T13:09:25.853446Z",
     "iopub.status.idle": "2026-01-16T13:09:25.857893Z",
     "shell.execute_reply": "2026-01-16T13:09:25.857554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c792b573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.858807Z",
     "iopub.status.busy": "2026-01-16T13:09:25.858708Z",
     "iopub.status.idle": "2026-01-16T13:09:25.861699Z",
     "shell.execute_reply": "2026-01-16T13:09:25.861367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada9dd72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.862607Z",
     "iopub.status.busy": "2026-01-16T13:09:25.862511Z",
     "iopub.status.idle": "2026-01-16T13:09:25.867966Z",
     "shell.execute_reply": "2026-01-16T13:09:25.867612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature priority and correlation filtering (from ens-model kernel)\n",
    "_SOLVENT_TABLE_CACHE = None\n",
    "\n",
    "def feature_priority(name):\n",
    "    \"\"\"Assign priority score to feature name based on prefix.\"\"\"\n",
    "    if name.startswith(\"spange_\"):\n",
    "        return 5\n",
    "    if name.startswith(\"acs_\"):\n",
    "        return 4\n",
    "    if name.startswith(\"drfps_\"):\n",
    "        return 3\n",
    "    if name.startswith(\"frag_\"):\n",
    "        return 2\n",
    "    if name.startswith(\"smiles_\"):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.8):\n",
    "    \"\"\"Drop columns that are highly correlated with any other column.\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    # Drop constant columns first\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    if constant_cols:\n",
    "        numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    # Find all pairs with corr > threshold\n",
    "    high_corr_pairs = []\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            cval = upper.iloc[i, j]\n",
    "            if cval > threshold:\n",
    "                high_corr_pairs.append((col_i, col_j, cval))\n",
    "    \n",
    "    # For each pair, decide which column to drop\n",
    "    for col_i, col_j, cval in high_corr_pairs:\n",
    "        if col_i in to_drop or col_j in to_drop:\n",
    "            continue\n",
    "        \n",
    "        p_i = feature_priority(col_i)\n",
    "        p_j = feature_priority(col_j)\n",
    "        \n",
    "        if p_i > p_j:\n",
    "            drop = col_j\n",
    "        elif p_j > p_i:\n",
    "            drop = col_i\n",
    "        else:\n",
    "            idx_i = df.columns.get_loc(col_i)\n",
    "            idx_j = df.columns.get_loc(col_j)\n",
    "            drop = col_i if idx_i > idx_j else col_j\n",
    "        \n",
    "        to_drop.add(drop)\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print(\"Feature filtering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083ec98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.868904Z",
     "iopub.status.busy": "2026-01-16T13:09:25.868799Z",
     "iopub.status.idle": "2026-01-16T13:09:25.872009Z",
     "shell.execute_reply": "2026-01-16T13:09:25.871663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature engineering defined\n"
     ]
    }
   ],
   "source": [
    "# Numeric feature engineering\n",
    "def add_numeric_features(X_numeric):\n",
    "    \"\"\"Add engineered numeric features.\"\"\"\n",
    "    X_num = X_numeric.copy()\n",
    "    cols = set(X_num.columns)\n",
    "    \n",
    "    if {\"Temperature\", \"Residence Time\"} <= cols:\n",
    "        # Convert Temperature to Kelvin\n",
    "        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n",
    "        \n",
    "        T = X_num[\"Temperature\"]\n",
    "        rt = X_num[\"Residence Time\"]\n",
    "        \n",
    "        # Interaction term\n",
    "        X_num[\"T_x_RT\"] = T * rt\n",
    "        \n",
    "        # Log transformation\n",
    "        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n",
    "        \n",
    "        # Inverse temperature\n",
    "        X_num[\"T_inv\"] = 1 / T\n",
    "        \n",
    "        # Scaled residence time\n",
    "        X_num[\"RT_scaled\"] = rt / rt.mean()\n",
    "    \n",
    "    return X_num\n",
    "\n",
    "print(\"Numeric feature engineering defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9accbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.873429Z",
     "iopub.status.busy": "2026-01-16T13:09:25.873309Z",
     "iopub.status.idle": "2026-01-16T13:09:25.878598Z",
     "shell.execute_reply": "2026-01-16T13:09:25.878247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solvent feature table builder defined\n"
     ]
    }
   ],
   "source": [
    "# Build solvent feature table\n",
    "def build_solvent_feature_table(threshold=0.90):\n",
    "    \"\"\"Build combined solvent feature table from multiple sources.\"\"\"\n",
    "    global _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    if _SOLVENT_TABLE_CACHE is not None:\n",
    "        return _SOLVENT_TABLE_CACHE\n",
    "    \n",
    "    print(\">>> Building solvent feature table...\")\n",
    "    \n",
    "    sources = [\n",
    "        \"spange_descriptors\",\n",
    "        \"acs_pca_descriptors\",\n",
    "        \"drfps_catechol\",\n",
    "        \"fragprints\",\n",
    "        \"smiles\",\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for src in sources:\n",
    "        df_src = load_features(src).copy()\n",
    "        \n",
    "        if \"SOLVENT NAME\" not in df_src.columns:\n",
    "            df_src = df_src.reset_index().rename(columns={\"index\": \"SOLVENT NAME\"})\n",
    "        \n",
    "        # Bit-table filtering for binary fingerprints\n",
    "        if src in [\"drfps_catechol\", \"fragprints\"]:\n",
    "            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n",
    "            \n",
    "            # Drop all-zero and all-one columns\n",
    "            df_src = df_src.loc[:, (df_src != 0).any(axis=0)]\n",
    "            df_src = df_src.loc[:, (df_src != 1).any(axis=0)]\n",
    "            \n",
    "            values = df_src.drop(columns={\"SOLVENT NAME\"}, errors=\"ignore\")\n",
    "            count = values.sum(axis=0).T\n",
    "            drop_cols = count[count == 1].index\n",
    "            df_src = df_src.drop(columns=drop_cols, errors=\"ignore\")\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        else:\n",
    "            if src == \"spange_descriptors\":\n",
    "                prefix = \"spange\"\n",
    "            elif src == \"acs_pca_descriptors\":\n",
    "                prefix = \"acs\"\n",
    "            else:\n",
    "                prefix = \"smiles\"\n",
    "            \n",
    "            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n",
    "            df_src = df_src.rename(columns={c: f\"{prefix}_{c}\" for c in cols_to_rename})\n",
    "        \n",
    "        dfs.append(df_src)\n",
    "    \n",
    "    # Merge all sources\n",
    "    merged = reduce(lambda left, right: pd.merge(left, right, on=\"SOLVENT NAME\", how=\"outer\"), dfs)\n",
    "    merged = merged.set_index(\"SOLVENT NAME\")\n",
    "    \n",
    "    print(f\"Merged shape before filtering: {merged.shape}\")\n",
    "    \n",
    "    # Apply correlation filtering\n",
    "    merged_filtered, dropped = filter_correlated_features(merged, threshold=threshold)\n",
    "    \n",
    "    print(f\"Merged shape after filtering: {merged_filtered.shape}\")\n",
    "    print(f\"Dropped {len(dropped)} columns\")\n",
    "    \n",
    "    _SOLVENT_TABLE_CACHE = merged_filtered\n",
    "    return _SOLVENT_TABLE_CACHE\n",
    "\n",
    "print(\"Solvent feature table builder defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbe3554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.879586Z",
     "iopub.status.busy": "2026-01-16T13:09:25.879486Z",
     "iopub.status.idle": "2026-01-16T13:09:25.884044Z",
     "shell.execute_reply": "2026-01-16T13:09:25.883724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined\n"
     ]
    }
   ],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    \"\"\"Featurizer for single-solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        solvent_names = X[\"SOLVENT NAME\"]\n",
    "        X_solvent = self.solvent_table.loc[solvent_names].reset_index(drop=True)\n",
    "        \n",
    "        X_combined = pd.concat([X_numeric.reset_index(drop=True), X_solvent], axis=1)\n",
    "        \n",
    "        return torch.tensor(X_combined.values, dtype=torch.double)\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    \"\"\"Featurizer for mixed-solvent data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solvent_table = build_solvent_feature_table(threshold=0.90)\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[[\"Residence Time\", \"Temperature\"]].copy()\n",
    "        X_numeric = add_numeric_features(X_numeric)\n",
    "        \n",
    "        pct_b = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "        \n",
    "        solvent_a_names = X[\"SOLVENT A NAME\"]\n",
    "        solvent_b_names = X[\"SOLVENT B NAME\"]\n",
    "        \n",
    "        X_a = self.solvent_table.loc[solvent_a_names].values\n",
    "        X_b = self.solvent_table.loc[solvent_b_names].values\n",
    "        \n",
    "        # Weighted average of solvent features\n",
    "        X_solvent = (1 - pct_b) * X_a + pct_b * X_b\n",
    "        X_solvent_df = pd.DataFrame(X_solvent, columns=self.solvent_table.columns)\n",
    "        \n",
    "        X_combined = pd.concat([X_numeric.reset_index(drop=True), X_solvent_df], axis=1)\n",
    "        \n",
    "        return torch.tensor(X_combined.values, dtype=torch.double)\n",
    "\n",
    "print(\"Featurizers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88590e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:25.884855Z",
     "iopub.status.busy": "2026-01-16T13:09:25.884756Z",
     "iopub.status.idle": "2026-01-16T13:09:26.251745Z",
     "shell.execute_reply": "2026-01-16T13:09:26.251386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostModel defined\n"
     ]
    }
   ],
   "source": [
    "# CatBoost Model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "class CatBoostModel(BaseModel):\n",
    "    \"\"\"CatBoost model with per-target regressors.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', random_state=42, verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.cat_params = dict(\n",
    "                random_state=random_state,\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3.0,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.cat_params = dict(\n",
    "                random_state=random_state,\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3.0,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = CatBoostRegressor(**self.cat_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.models is None:\n",
    "            raise RuntimeError(\"CatBoostModel not trained\")\n",
    "        \n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # Clip to non-negative\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        \n",
    "        # Renormalize if sum > 1\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"CatBoostModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148a18df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:26.252855Z",
     "iopub.status.busy": "2026-01-16T13:09:26.252702Z",
     "iopub.status.idle": "2026-01-16T13:09:26.258052Z",
     "shell.execute_reply": "2026-01-16T13:09:26.257733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBModel defined\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "import xgboost as xgb\n",
    "\n",
    "class XGBModel(BaseModel):\n",
    "    \"\"\"XGBoost model with per-target regressors.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', random_state=42, verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state,\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=1.0,\n",
    "                reg_alpha=0.1,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            self.xgb_params = dict(\n",
    "                random_state=random_state,\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=1.0,\n",
    "                reg_alpha=0.1,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "        self.n_targets = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        Y_np = train_Y.values\n",
    "        self.n_targets = Y_np.shape[1]\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(self.n_targets):\n",
    "            m = xgb.XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(m)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.models is None:\n",
    "            raise RuntimeError(\"XGBModel not trained\")\n",
    "        \n",
    "        X_tensor = self.smiles_featurizer.featurize(X)\n",
    "        X_np = X_tensor.detach().cpu().numpy()\n",
    "        \n",
    "        preds_list = [m.predict(X_np) for m in self.models]\n",
    "        out = np.column_stack(preds_list)\n",
    "        \n",
    "        # Clip to non-negative\n",
    "        out = np.clip(out, a_min=0.0, a_max=None)\n",
    "        \n",
    "        # Renormalize if sum > 1\n",
    "        if out.shape[1] > 1:\n",
    "            totals = out.sum(axis=1, keepdims=True)\n",
    "            divisor = np.maximum(totals, 1.0)\n",
    "            out = out / divisor\n",
    "        \n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print(\"XGBModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11394d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T13:09:26.259044Z",
     "iopub.status.busy": "2026-01-16T13:09:26.258827Z",
     "iopub.status.idle": "2026-01-16T13:09:26.262820Z",
     "shell.execute_reply": "2026-01-16T13:09:26.262226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsembleModel defined\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Model\n",
    "class EnsembleModel(BaseModel):\n",
    "    \"\"\"Weighted ensemble of CatBoost and XGBoost.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', verbose=False):\n",
    "        self.data_mode = data\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Optimized weights per dataset (from ens-model kernel)\n",
    "        if data == 'single':\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        # Normalize weights\n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        # Initialize base models\n",
    "        self.cat_model = CatBoostModel(data=data)\n",
    "        self.xgb_model = XGBModel(data=data)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        self.cat_model.train_model(train_X, train_Y)\n",
    "        self.xgb_model.train_model(train_X, train_Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        \n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"EnsembleModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59279e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape}, {Y_single.shape}\")\n",
    "\n",
    "model = EnsembleModel(data='single')\n",
    "model.train_model(X_single, Y_single)\n",
    "preds = model.predict(X_single[:5])\n",
    "print(f\"Test predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions:\\n{preds[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(verbose=True):\n",
    "    \"\"\"Compute CV score with ens-model approach.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='single')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='full')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "print(\"Running CV...\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'cv_score': float(combined_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'model': 'EnsembleModel (CatBoost + XGBoost with correlation filtering)',\n",
    "    'baseline_cv': 0.008298,\n",
    "    'improvement': f\"{(0.008298 - combined_cv) / 0.008298 * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/094_ens_model_exact/metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Combined CV: {combined_cv:.6f}\")\n",
    "print(f\"Baseline CV: 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - combined_cv) / 0.008298 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48311fd4",
   "metadata": {},
   "source": [
    "## Generate Submission\n",
    "\n",
    "The following cells follow the official template structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94743831",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
