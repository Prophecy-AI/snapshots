{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7ce035",
   "metadata": {},
   "source": [
    "# Experiment 097: Frozen ChemBERTa Embeddings + Ensemble\n",
    "\n",
    "**Goal**: Use pre-trained ChemBERTa embeddings as features instead of training from scratch.\n",
    "\n",
    "**Rationale**:\n",
    "- GNNs trained from scratch failed (exp_096: CV 452% worse)\n",
    "- ChemBERTa was pre-trained on 10M+ SMILES strings\n",
    "- Frozen embeddings capture molecular structure without overfitting\n",
    "- Combines pre-trained knowledge with our best ensemble approach\n",
    "\n",
    "**Approach**:\n",
    "1. Extract ChemBERTa embeddings for all 24 solvents\n",
    "2. Use embeddings + Arrhenius features for GP+MLP+LGBM ensemble\n",
    "3. Compare CV to baseline (0.0081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80228be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import lightgbm as lgb\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313212ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73923245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SMILES lookup\n",
    "SMILES_DF = pd.read_csv(f'{DATA_PATH}/smiles_lookup.csv', index_col=0)\n",
    "SPANGE_DF = load_features(\"spange_descriptors\")\n",
    "\n",
    "print(f\"SMILES lookup: {SMILES_DF.shape}\")\n",
    "print(f\"Spange: {SPANGE_DF.shape}\")\n",
    "print(f\"\\nSample SMILES:\")\n",
    "print(SMILES_DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ChemBERTa model and extract embeddings\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "print(\"Loading ChemBERTa model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for all solvents\n",
    "def extract_chemberta_embedding(smiles, tokenizer, model):\n",
    "    \"\"\"Extract ChemBERTa embedding for a single SMILES string.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        # Mean pool over token dimension (excluding padding)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        embedding = (sum_embeddings / sum_mask).squeeze().numpy()\n",
    "        return embedding\n",
    "\n",
    "# Extract embeddings for all solvents\n",
    "print(\"Extracting ChemBERTa embeddings for all solvents...\")\n",
    "embeddings_dict = {}\n",
    "for solvent_name in SMILES_DF.index:\n",
    "    smiles = SMILES_DF.loc[solvent_name, 'solvent smiles']\n",
    "    embedding = extract_chemberta_embedding(smiles, tokenizer, model)\n",
    "    embeddings_dict[solvent_name] = embedding\n",
    "    print(f\"  {solvent_name}: {embedding.shape}\")\n",
    "\n",
    "# Create DataFrame\n",
    "CHEMBERTA_DF = pd.DataFrame.from_dict(embeddings_dict, orient='index')\n",
    "print(f\"\\nChemBERTa embeddings shape: {CHEMBERTA_DF.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b053573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[128, 64], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"MLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemBERTa Ensemble Model\n",
    "class ChemBERTaEnsembleModel(BaseModel):\n",
    "    \"\"\"Ensemble model using ChemBERTa embeddings + Arrhenius features.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', hidden_dims=[128, 64], dropout=0.1, num_epochs=100, lr=0.001):\n",
    "        self.data = data\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout = dropout\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Models\n",
    "        self.mlp = None\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "        \n",
    "        # GP\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42)\n",
    "        \n",
    "        # Scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Ensemble weights (from best model)\n",
    "        self.weights = [0.3, 0.4, 0.3]  # GP, MLP, LGBM\n",
    "        \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract features from input data.\"\"\"\n",
    "        # Numeric features with Arrhenius\n",
    "        time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        temp_k = temp + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        \n",
    "        if self.data == 'single':\n",
    "            # Single solvent - use ChemBERTa embeddings\n",
    "            chemberta = CHEMBERTA_DF.loc[X['SOLVENT NAME']].values\n",
    "            spange = SPANGE_DF.loc[X['SOLVENT NAME']].values\n",
    "        else:\n",
    "            # Mixed solvents - weighted average\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            chemberta_a = CHEMBERTA_DF.loc[X['SOLVENT A NAME']].values\n",
    "            chemberta_b = CHEMBERTA_DF.loc[X['SOLVENT B NAME']].values\n",
    "            chemberta = (1 - pct) * chemberta_a + pct * chemberta_b\n",
    "            \n",
    "            spange_a = SPANGE_DF.loc[X['SOLVENT A NAME']].values\n",
    "            spange_b = SPANGE_DF.loc[X['SOLVENT B NAME']].values\n",
    "            spange = (1 - pct) * spange_a + pct * spange_b\n",
    "        \n",
    "        return np.hstack([time, temp, inv_temp, log_time, interaction, chemberta, spange])\n",
    "    \n",
    "    def train_model(self, train_X, train_Y, device=None, verbose=False):\n",
    "        X_np = self._get_features(train_X)\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm.fit(X_scaled_df, Y_np)\n",
    "        \n",
    "        # Train GP (on subset for speed)\n",
    "        n_gp = min(200, len(X_scaled))\n",
    "        indices = np.random.choice(len(X_scaled), n_gp, replace=False)\n",
    "        self.gp.fit(X_scaled[indices], Y_np[indices, 0])  # GP for first target\n",
    "        \n",
    "        # Build and train MLP\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        self.mlp = SimpleMLP(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=3,\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "        Y_tensor = torch.tensor(Y_np, dtype=torch.float32).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=self.lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "        \n",
    "        self.mlp.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_X, batch_Y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp(batch_X)\n",
    "                loss = criterion(pred, batch_Y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_np = self._get_features(X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_mean = self.gp.predict(X_scaled)\n",
    "        gp_preds = np.column_stack([gp_mean, gp_mean, gp_mean])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_preds = (\n",
    "            self.weights[0] * gp_preds +\n",
    "            self.weights[1] * mlp_preds +\n",
    "            self.weights[2] * lgb_preds\n",
    "        )\n",
    "        \n",
    "        # Clip to non-negative\n",
    "        final_preds = np.clip(final_preds, 0, None)\n",
    "        \n",
    "        return torch.tensor(final_preds, dtype=torch.double)\n",
    "\n",
    "print(\"ChemBERTaEnsembleModel defined\")\n",
    "print(f\"Feature dimension: {5 + CHEMBERTA_DF.shape[1] + SPANGE_DF.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64796dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape}, {Y_single.shape}\")\n",
    "\n",
    "model = ChemBERTaEnsembleModel(data='single', num_epochs=20)\n",
    "model.train_model(X_single, Y_single)\n",
    "preds = model.predict(X_single[:5])\n",
    "print(f\"Test predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions:\\n{preds[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(verbose=True):\n",
    "    \"\"\"Compute CV score with ChemBERTa embeddings.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ChemBERTaEnsembleModel(data='single', num_epochs=100)\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ChemBERTaEnsembleModel(data='full', num_epochs=100)\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "print(\"Running CV with ChemBERTa embeddings...\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'cv_score': float(combined_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'model': 'ChemBERTaEnsembleModel (GP+MLP+LGBM with ChemBERTa embeddings)',\n",
    "    'baseline_cv': 0.0081,\n",
    "    'improvement': f\"{(0.0081 - combined_cv) / 0.0081 * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/097_chemberta_embeddings/metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Combined CV: {combined_cv:.6f}\")\n",
    "print(f\"Baseline CV: 0.0081\")\n",
    "print(f\"Improvement: {(0.0081 - combined_cv) / 0.0081 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efdb32",
   "metadata": {},
   "source": [
    "## Generate Submission (if CV is better than baseline)\n",
    "\n",
    "The following cells follow the official template structure.\n",
    "\n",
    "**CRITICAL**: The model class in submission cells MUST match the CV computation class (`ChemBERTaEnsembleModel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should generate submission\n",
    "if combined_cv < 0.0081:\n",
    "    print(f\"CV {combined_cv:.6f} is BETTER than baseline 0.0081!\")\n",
    "    print(\"Generating submission...\")\n",
    "    GENERATE_SUBMISSION = True\n",
    "else:\n",
    "    print(f\"CV {combined_cv:.6f} is WORSE than baseline 0.0081\")\n",
    "    print(\"Not generating submission.\")\n",
    "    GENERATE_SUBMISSION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65872d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ChemBERTaEnsembleModel(data='single', num_epochs=100)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f761d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ChemBERTaEnsembleModel(data='full', num_epochs=100)  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b64329",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
