{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 115863,
     "databundleVersionId": 13836289,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:29.261092Z",
     "iopub.execute_input": "2025-11-28T09:35:29.261445Z",
     "iopub.status.idle": "2025-11-28T09:35:29.269162Z",
     "shell.execute_reply.started": "2025-11-28T09:35:29.261419Z",
     "shell.execute_reply": "2025-11-28T09:35:29.26808Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES, load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:30.139478Z",
     "iopub.execute_input": "2025-11-28T09:35:30.139961Z",
     "iopub.status.idle": "2025-11-28T09:35:30.146028Z",
     "shell.execute_reply.started": "2025-11-28T09:35:30.139934Z",
     "shell.execute_reply": "2025-11-28T09:35:30.14438Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        raise NotImplementedError\n\n    def featurize(X, Y):\n        raise NotImplementedError\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n\n    def train_model(self, X_train, y_train):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:30.942662Z",
     "iopub.execute_input": "2025-11-28T09:35:30.943396Z",
     "iopub.status.idle": "2025-11-28T09:35:30.949868Z",
     "shell.execute_reply.started": "2025-11-28T09:35:30.943366Z",
     "shell.execute_reply": "2025-11-28T09:35:30.948729Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "_SOLVENT_TABLE_CACHE = None\n\nfrom functools import reduce\n\nimport torch\n\ntorch.set_default_dtype(torch.double)\n\n\ndef feature_priority(name : str) -> int:\n    \"\"\"\n    Assign a priority score to a feature name based on its prefix.\n    Higher number = more important to keep during correlation filtering.\n    \"\"\"\n    if name.startswith(\"spange_\"):\n        return 5\n    if name.startswith(\"acs_\"):\n        return 4\n    if name.startswith(\"drfps_\"):\n        return 3\n    if name.startswith(\"frag_\"):\n        return 2\n    if name.startswith(\"smiles_\"):\n        return 1\n    return 0\n\n\ndef filter_correlated_features(\n    df : pd.DataFrame,\n    threshold : float = 0.8,\n):\n    \"\"\"\n    Drop columns that are highly correlated with any other column.\n\n    Logic:\n      - Only numeric columns are considered.\n      - Find all pairs with |corr| > threshold.\n      - For each pair, drop ONE feature:\n          * Prefer to KEEP higher-priority prefixes (spange > acs > drfps > frag > smiles).\n          * If equal priority, drop the one that appears later in the original column order.\n      - Constant (zero-variance) columns are removed first.\n    \"\"\"\n    numeric_df = df.select_dtypes(include = [np.number])\n\n    print(f\"[filter_correlated_features] numeric shape: {numeric_df.shape}\")\n\n    if numeric_df.shape[1] == 0:\n        print(\"No numeric columns found, skipping correlation filter.\")\n        return df, []\n\n    # Drop constant columns first (std = 0) to avoid NaNs in correlation\n    std = numeric_df.std(axis = 0)\n    constant_cols = std[std == 0].index.tolist()\n    if constant_cols:\n        print(\n            f\"[filter_correlated_features] dropping {len(constant_cols)} \"\n            f\"constant columns before corr\"\n        )\n        numeric_df = numeric_df.drop(columns = constant_cols)\n\n    # Correlation matrix\n    corr = numeric_df.corr().abs()\n\n    # Upper triangle only\n    upper = corr.where(np.triu(np.ones(corr.shape), k = 1).astype(bool)).fillna(0.0)\n\n    cols = upper.columns.tolist()\n    to_drop = set()\n\n    # Build list of all pairs (i, j) with corr > threshold\n    high_corr_pairs = []\n    for i, col_i in enumerate(cols):\n        for j in range(i + 1, len(cols)):\n            col_j = cols[j]\n            cval = upper.iloc[i, j]\n            if cval > threshold:\n                high_corr_pairs.append((col_i, col_j, cval))\n\n    print(\n        f\"[filter_correlated_features] found {len(high_corr_pairs)} \"\n        f\"pairs with |corr| > {threshold}\"\n    )\n\n    # For each pair, decide which column to drop\n    for col_i, col_j, cval in high_corr_pairs:\n        # If either already marked to drop, skip\n        if col_i in to_drop or col_j in to_drop:\n            continue\n\n        p_i = feature_priority(col_i)\n        p_j = feature_priority(col_j)\n\n        if p_i > p_j:\n            drop = col_j\n        elif p_j > p_i:\n            drop = col_i\n        else:\n            # Same priority; drop the one that appears later in original df\n            idx_i = df.columns.get_loc(col_i)\n            idx_j = df.columns.get_loc(col_j)\n            drop = col_i if idx_i > idx_j else col_j\n\n        to_drop.add(drop)\n\n    # Merge with constant cols\n    all_to_drop = list(set(constant_cols).union(to_drop))\n\n    print(\n        f\"[filter_correlated_features] threshold = {threshold}, \"\n        f\"dropping {len(to_drop)} correlated + {len(constant_cols)} constant \"\n        f\"= {len(all_to_drop)} total columns\"\n    )\n\n    df_filtered = df.drop(columns = all_to_drop, errors = \"ignore\")\n\n    return df_filtered, all_to_drop\n\n\n# ---------------- NUMERIC FEATURE ENGINEERING ---------------- #\n\ndef add_numeric_features(X_numeric : pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add engineered numeric features (e.g. temperature transformations,\n    interaction terms, and scaled residence time).\n    \"\"\"\n    X_num = X_numeric.copy()\n    cols = set(X_num.columns)\n\n    if {\"Temperature\", \"Residence Time\"} <= cols:\n        # Convert Temperature to Kelvin\n        X_num[\"Temperature\"] = X_num[\"Temperature\"] + 273.15\n\n        T = X_num[\"Temperature\"]\n        rt = X_num[\"Residence Time\"]\n\n        # Interaction term\n        X_num[\"T_x_RT\"] = T * rt\n\n        # Log transformation (avoid log(0))\n        X_num[\"RT_log\"] = np.log(rt + 1e-6)\n\n        # Inverse temperature\n        X_num[\"T_inv\"] = 1 / T\n\n        # Scaled residence time\n        X_num[\"RT_scaled\"] = rt / rt.mean()\n\n    return X_num\n\n\n# ---------------- SOLVENT TABLE COMBINER ---------------- #\n\ndef build_solvent_feature_table(threshold : float = 0.90):\n    \"\"\"\n    Build and cache a combined solvent feature table from multiple sources,\n    then apply correlation-based feature filtering.\n    \"\"\"\n    global _SOLVENT_TABLE_CACHE\n\n    # If already built the table once, reuse it.\n    if _SOLVENT_TABLE_CACHE is not None:\n        return _SOLVENT_TABLE_CACHE\n\n    print(\">>> Building solvent feature table (first and only time)...\")\n\n    sources = [\n        \"spange_descriptors\",\n        \"acs_pca_descriptors\",\n        \"drfps_catechol\",\n        \"fragprints\",\n        \"smiles\",\n    ]\n\n    dfs = []\n\n    for src in sources:\n        df_src = load_features(src).copy()\n\n        if \"SOLVENT NAME\" not in df_src.columns:\n            df_src = df_src.reset_index().rename(columns = {\"index\" : \"SOLVENT NAME\"})\n\n        # --- Bit-table filtering for binary fingerprints ---\n        if src in [\"drfps_catechol\", \"fragprints\"]:\n            prefix = \"drfps\" if src == \"drfps_catechol\" else \"frag\"\n\n            # Drop all-zero and all-one columns\n            df_src = df_src.loc[:, (df_src != 0).any(axis = 0)]\n            df_src = df_src.loc[:, (df_src != 1).any(axis = 0)]\n\n            values = df_src.drop(columns = {\"SOLVENT NAME\"})\n            count = values.sum(axis = 0).T\n            drop_cols = count[count == 1].index\n            df_src = df_src.drop(columns = drop_cols)\n\n            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n            df_src = df_src.rename(columns = {c : f\"{prefix}_{c}\" for c in cols_to_rename})\n\n        else:\n            if src == \"spange_descriptors\":\n                prefix = \"spange\"\n            elif src == \"acs_pca_descriptors\":\n                prefix = \"acs\"\n            elif src == \"smiles\":\n                prefix = \"smiles\"\n            else:\n                prefix = src\n\n            cols_to_rename = [c for c in df_src.columns if c != \"SOLVENT NAME\"]\n            df_src = df_src.rename(columns = {c : f\"{prefix}_{c}\" for c in cols_to_rename})\n\n        # Drop any SMILES-like columns that slipped through\n        smiles_like = [c for c in df_src.columns if \"SMILES\" in c.upper()]\n        df_src = df_src.drop(columns = smiles_like, errors = \"ignore\")\n\n        df_src = df_src.set_index(\"SOLVENT NAME\")\n        dfs.append(df_src)\n\n    # Join all feature tables on solvent name\n    featurizer = reduce(lambda l, r : l.join(r, how = \"inner\"), dfs)\n    print(f\"Combined feature table shape (before corr filter): {featurizer.shape}\")\n\n    featurizer_filtered, dropped_cols = filter_correlated_features(\n        featurizer,\n        threshold = threshold,\n    )\n\n    print(f\"Dropped {len(dropped_cols)} columns at threshold = {threshold}\")\n    print(f\"Final solvent feature table shape: {featurizer_filtered.shape}\")\n\n    # Cache the final table so it won't rebuild again\n    _SOLVENT_TABLE_CACHE = featurizer_filtered\n\n    return featurizer_filtered\n\n\n# ---------------- SINGLE-SOLVENT FEATURIZER ---------------- #\n\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    \"\"\"\n    Featurizer for single-solvent experiments:\n      - engineered numeric features\n      - joined solvent descriptor table\n    \"\"\"\n    def __init__(self):\n        # Build full solvent table (cached)\n        self.featurizer = build_solvent_feature_table()\n\n        # Compute numeric dim using a dummy row\n        dummy_num = pd.DataFrame(\n            [[0] * len(INPUT_LABELS_NUMERIC)],\n            columns = INPUT_LABELS_NUMERIC,\n        )\n        numeric_dim = add_numeric_features(dummy_num).shape[1]\n\n        self.feats_dim = numeric_dim + self.featurizer.shape[1]\n\n    def featurize(self, X):\n        X_numeric = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n        X_solvent = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n\n        X_out = np.concatenate([X_numeric.values, X_solvent.values], axis = 1)\n\n        return torch.tensor(\n            X_out,\n            dtype = torch.double,\n        )\n\n\n# ---------------- MIXED-SOLVENT FEATURIZER ---------------- #\n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    \"\"\"\n    Featurizer for mixed-solvent experiments:\n      - engineered numeric features\n      - solvent features for A and B\n      - linear mixture according to SolventB%\n    \"\"\"\n    def __init__(self):\n        self.featurizer = build_solvent_feature_table()\n\n        dummy_num = pd.DataFrame(\n            [[0] * len(INPUT_LABELS_NUMERIC)],\n            columns = INPUT_LABELS_NUMERIC,\n        )\n        numeric_dim = add_numeric_features(dummy_num).shape[1]\n\n        self.feats_dim = numeric_dim + self.featurizer.shape[1]\n\n    def featurize(self, X):\n        X_numeric = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())\n\n        A = self.featurizer.loc[X[\"SOLVENT A NAME\"]].values\n        B = self.featurizer.loc[X[\"SOLVENT B NAME\"]].values\n\n        frac_B = X[\"SolventB%\"].values.reshape(-1, 1)\n        frac_A = 1 - frac_B\n\n        mixed = A * frac_A + B * frac_B\n\n        X_out = np.concatenate([X_numeric.values, mixed], axis = 1)\n\n        return torch.tensor(\n            X_out,\n            dtype = torch.double,\n        )",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:31.766795Z",
     "iopub.execute_input": "2025-11-28T09:35:31.767142Z",
     "iopub.status.idle": "2025-11-28T09:35:36.567752Z",
     "shell.execute_reply.started": "2025-11-28T09:35:31.767118Z",
     "shell.execute_reply": "2025-11-28T09:35:36.566762Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from catboost import CatBoostRegressor\n\n\nclass CatBoostModel(BaseModel):\n    \"\"\"\n    CatBoost-based model for reaction yields.\n\n    Uses different hyperparameters for:\n      - data=\"single\": single-solvent dataset\n      - data!=\"single\": full / mixed-solvent dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        data: str = \"single\",\n        verbose: bool = False,\n        random_state: int = 42,\n    ):\n        self.data_mode = data\n        self.verbose = verbose\n        self.random_state = random_state\n\n        # Select featurizer and tuned CatBoost parameters based on data mode\n        if data == \"single\":\n            self.smiles_featurizer = PrecomputedFeaturizer()\n\n            self.cat_params = dict(\n                random_seed = random_state,\n                loss_function = \"MultiRMSE\",\n                depth = 3,\n                learning_rate = 0.07,\n                n_estimators = 1050,\n                l2_leaf_reg = 3.5,\n                bootstrap_type = \"Bayesian\",\n                bagging_temperature = 0.225,\n                grow_policy = \"SymmetricTree\",\n                rsm = 0.75,\n                verbose = verbose,\n            )\n\n        else:\n            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n\n            self.cat_params = dict(\n                random_seed = random_state,\n                loss_function = \"MultiRMSE\",\n                depth = 3,\n                learning_rate = 0.06,\n                n_estimators = 1100,\n                l2_leaf_reg = 2.5,\n                bootstrap_type = \"Bayesian\",\n                bagging_temperature = 0.25,\n                grow_policy = \"SymmetricTree\",\n                rsm = 0.75,\n                verbose = verbose,\n            )\n\n        self.model = None\n        self.n_targets = None\n\n    def train_model(\n        self,\n        train_X,\n        train_Y,\n        device = None,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Featurize inputs and fit CatBoostRegressor on multi-target labels.\n        \"\"\"\n        # Featurize SMILES + numeric inputs, then convert to NumPy for CatBoost\n        X_tensor = self.smiles_featurizer.featurize(train_X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        # train_Y expected as shape (n_samples, n_targets)\n        Y_np = train_Y.values\n        self.n_targets = Y_np.shape[1]\n\n        self.model = CatBoostRegressor(**self.cat_params)\n        self.model.fit(X_np, Y_np)\n\n        if verbose or self.verbose:\n            print(\n                f\"[CatBoostModel] Training complete in '{self.data_mode}' mode \"\n                f\"with {self.n_targets} target(s).\"\n            )\n\n    def predict(self, X):\n        \"\"\"\n        Predict yields, clip negatives to 0, and for multi-target outputs\n        ensure non-negative rows with sum <= 1 by down-scaling if needed.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError(\"Model is not trained. Call train_model(...) first.\")\n\n        # Featurize and convert to NumPy for prediction\n        X_tensor = self.smiles_featurizer.featurize(X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        out = self.model.predict(X_np)\n        out = np.asarray(out)\n\n        # Ensure 2D shape: (n_samples, n_targets)\n        if out.ndim == 1:\n            out = out.reshape(-1, 1)\n\n        # Clip to non-negative yields\n        out_before_clip = out.copy()\n        out = np.clip(out, a_min = 0.0, a_max = None)\n\n        # For multi-target: if row-sum > 1, scale down so sum becomes 1\n        if out.shape[1] > 1:\n            totals = out.sum(axis = 1, keepdims = True)\n            divisor = np.maximum(totals, 1.0)\n            out = out / divisor\n\n    \n        return torch.tensor(out, dtype=torch.double)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:36.569124Z",
     "iopub.execute_input": "2025-11-28T09:35:36.569555Z",
     "iopub.status.idle": "2025-11-28T09:35:37.473383Z",
     "shell.execute_reply.started": "2025-11-28T09:35:36.569525Z",
     "shell.execute_reply": "2025-11-28T09:35:37.472216Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from xgboost import XGBRegressor\n\nclass XGBModel(BaseModel):\n    \"\"\"\n    XGBoost-based model for reaction yields.\n\n    Uses different hyperparameters for:\n      - data = \"single\": single-solvent dataset\n      - data != \"single\": full / mixed-solvent dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        data : str = \"single\",\n        random_state : int = 42,\n        verbose : bool = False,\n    ):\n        self.data_mode = data\n        self.verbose = verbose\n        self.random_state = random_state\n\n        if data == \"single\":\n            self.smiles_featurizer = PrecomputedFeaturizer()\n\n            self.xgb_params = dict(\n                random_state = random_state,\n                objective = \"reg:squarederror\",\n                tree_method = \"hist\",\n                subsample = 0.5,\n                reg_lambda = 0.6,\n                reg_alpha = 0.0,\n                n_estimators = 1000,\n                min_child_weight = 1,\n                max_depth = 4,\n                max_delta_step = 1,\n                learning_rate = 0.02,\n                grow_policy = \"depthwise\",\n                gamma = 0.0,\n                colsample_bytree = 0.3,\n                colsample_bylevel = 0.6,\n            )\n\n        else:\n            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n\n            self.xgb_params = dict(\n                random_state = random_state,\n                objective = \"reg:squarederror\",\n                tree_method = \"approx\",\n                subsample = 0.5,\n                reg_lambda = 0.6,\n                reg_alpha = 0.0,\n                n_estimators = 1000,\n                min_child_weight = 1,\n                max_depth = 4,\n                max_delta_step = 1,\n                learning_rate = 0.02,\n                grow_policy = \"lossguide\",\n                gamma = 0.0,\n                colsample_bytree = 0.3,\n                colsample_bylevel = 0.6,\n            )\n\n        self.models = None\n        self.n_targets = None\n\n    def train_model(\n        self,\n        train_X,\n        train_Y,\n        device = None,\n        verbose : bool = False,\n    ):\n        \"\"\"\n        Featurize inputs and fit one XGBRegressor per target.\n        \"\"\"\n        X_tensor = self.smiles_featurizer.featurize(train_X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        Y_np = train_Y.values\n        self.n_targets = Y_np.shape[1]\n\n        self.models = []\n        for t in range(self.n_targets):\n            model_t = XGBRegressor(**self.xgb_params)\n            model_t.fit(X_np, Y_np[:, t])\n            self.models.append(model_t)\n\n        if verbose or self.verbose:\n            print(\n                f\"[XGBModel] Training complete in '{self.data_mode}' mode \"\n                f\"with {self.n_targets} target(s) and {len(self.models)} model(s).\"\n            )\n\n    def predict(self, X):\n        \"\"\"\n        Predict yields, clip negatives to 0, and for multi-target outputs\n        ensure non-negative rows with sum <= 1 by down-scaling if needed.\n        \"\"\"\n        if self.models is None or self.n_targets is None:\n            raise RuntimeError(\"Model is not trained. Call train_model(...) first.\")\n\n        X_tensor = self.smiles_featurizer.featurize(X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        preds_list = [m.predict(X_np) for m in self.models]\n        out = np.column_stack(preds_list)\n\n        out = np.clip(out, a_min = 0.0, a_max = None)\n\n        if out.shape[1] > 1:\n            totals = out.sum(axis = 1, keepdims = True)\n            divisor = np.maximum(totals, 1.0)\n            out = out / divisor\n\n        return torch.tensor(out, dtype = torch.double)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:37.474408Z",
     "iopub.execute_input": "2025-11-28T09:35:37.474958Z",
     "iopub.status.idle": "2025-11-28T09:35:37.490667Z",
     "shell.execute_reply.started": "2025-11-28T09:35:37.474924Z",
     "shell.execute_reply": "2025-11-28T09:35:37.489606Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestRegressor\n\n\nclass RFModel(BaseModel):\n    \"\"\"\n    Random Forest model for reaction yields.\n    Trains one regressor per target and applies the same postprocessing rules\n    used in the CatBoost and XGB models.\n\n    This model actually performed much worse than the CatBoost and XGB models, and harmed performance when added in the ensemble, \n    so it is not actually used.\n    \"\"\"\n\n    def __init__(\n        self,\n        data : str = \"single\",\n        random_state : int = 42,\n        verbose : bool = False,\n    ):\n        self.data_mode = data\n        self.verbose = verbose\n        self.random_state = random_state\n\n        if data == \"single\":\n            self.smiles_featurizer = PrecomputedFeaturizer()\n\n            self.rf_params = dict(\n                random_state = random_state,\n                n_estimators = 450,\n                min_samples_split = 2,\n                min_samples_leaf = 1,\n                max_features = \"sqrt\",\n                max_depth = 10,\n                bootstrap = True,\n            )\n\n        else:\n            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n\n            self.rf_params = dict(\n                random_state = random_state,\n                n_estimators = 300,\n                min_samples_split = 2,\n                min_samples_leaf = 1,\n                max_features = \"sqrt\",\n                max_depth = None,\n                bootstrap = True,\n            )\n\n        self.models = None\n        self.n_targets = None\n\n    def train_model(\n        self,\n        train_X,\n        train_Y,\n        device = None,\n        verbose : bool = False,\n    ):\n        \"\"\"\n        Featurize inputs and train one RandomForestRegressor per target.\n        \"\"\"\n        X_tensor = self.smiles_featurizer.featurize(train_X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        Y_np = train_Y.values\n        self.n_targets = Y_np.shape[1]\n\n        self.models = []\n        for t in range(self.n_targets):\n            m = RandomForestRegressor(**self.rf_params)\n            m.fit(X_np, Y_np[:, t])\n            self.models.append(m)\n\n        if verbose or self.verbose:\n            print(\n                f\"[RFModel] Training complete in '{self.data_mode}' mode \"\n                f\"with {self.n_targets} target(s) and {len(self.models)} model(s).\"\n            )\n\n    def predict(self, X):\n        \"\"\"\n        Predict yields, enforce non-negativity, and renormalise if needed.\n        \"\"\"\n        if self.models is None:\n            raise RuntimeError(\"RFModel not trained\")\n\n        X_tensor = self.smiles_featurizer.featurize(X)\n        X_np = X_tensor.detach().cpu().numpy()\n\n        preds_list = [m.predict(X_np) for m in self.models]\n        out = np.column_stack(preds_list)\n\n        out = np.clip(out, a_min = 0.0, a_max = None)\n\n        if out.shape[1] > 1:\n            totals = out.sum(axis = 1, keepdims = True)\n            divisor = np.maximum(totals, 1.0)\n            out = out / divisor\n\n        return torch.tensor(out, dtype = torch.double)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:37.49246Z",
     "iopub.execute_input": "2025-11-28T09:35:37.492918Z",
     "iopub.status.idle": "2025-11-28T09:35:37.946624Z",
     "shell.execute_reply.started": "2025-11-28T09:35:37.492887Z",
     "shell.execute_reply": "2025-11-28T09:35:37.945776Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class EnsembleModel(BaseModel):\n    \"\"\"\n    Weighted ensemble of CatBoostModel and XGBModel.\n    Each base model predicts independently; outputs are combined\n    via a weighted average. Base models already handle clipping\n    and renormalisation internally.\n    \"\"\"\n\n    def __init__(\n        self,\n        data : str = \"single\",\n        verbose : bool = False,\n    ):\n        self.data_mode = data\n        self.verbose = verbose\n\n        # Optimised fixed weights per dataset\n        if data == \"single\":\n            cat_weight = 7.0\n            xgb_weight = 6.0\n        else:\n            # multi-solvent / full dataset\n            cat_weight = 1.0\n            xgb_weight = 2.0\n\n        # Normalise ensemble weights\n        w_sum = cat_weight + xgb_weight\n        self.cat_weight = cat_weight / w_sum\n        self.xgb_weight = xgb_weight / w_sum\n\n        # Initialise base models (fixed hyperparameters)\n        self.cat_model = CatBoostModel(data = data)\n        self.xgb_model = XGBModel(data = data)\n\n    def train_model(\n        self,\n        train_X,\n        train_Y,\n        device = None,\n        verbose : bool = False,\n    ):\n        \"\"\"\n        Train each base model on the same dataset.\n        \"\"\"\n        self.cat_model.train_model(train_X, train_Y)\n        self.xgb_model.train_model(train_X, train_Y)\n\n        if verbose or self.verbose:\n            print(\n                f\"[EnsembleModel] Trained CatBoost and XGB models \"\n                f\"in '{self.data_mode}' mode.\"\n            )\n\n    def predict(self, X):\n        \"\"\"\n        Predict with each model and return a weighted average.\n        Base models already perform clipping and multi-target normalisation.\n        \"\"\"\n        cat_pred = self.cat_model.predict(X)\n        xgb_pred = self.xgb_model.predict(X)\n\n        out = (\n            self.cat_weight * cat_pred\n            + self.xgb_weight * xgb_pred\n        )\n\n        return out",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:37.947558Z",
     "iopub.execute_input": "2025-11-28T09:35:37.947844Z",
     "iopub.status.idle": "2025-11-28T09:35:37.957543Z",
     "shell.execute_reply.started": "2025-11-28T09:35:37.947822Z",
     "shell.execute_reply": "2025-11-28T09:35:37.956258Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nimport tqdm\n\nX, Y = load_data(\"single_solvent\")\n\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = EnsembleModel() # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_single_solvent = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:35:37.958549Z",
     "iopub.execute_input": "2025-11-28T09:35:37.958868Z",
     "iopub.status.idle": "2025-11-28T09:36:39.725551Z",
     "shell.execute_reply.started": "2025-11-28T09:35:37.958842Z",
     "shell.execute_reply": "2025-11-28T09:36:39.724396Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nX, Y = load_data(\"full\")\n\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = EnsembleModel(data = 'full') # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_full_data = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:36:39.727256Z",
     "iopub.execute_input": "2025-11-28T09:36:39.727534Z",
     "iopub.status.idle": "2025-11-28T09:37:56.790425Z",
     "shell.execute_reply.started": "2025-11-28T09:36:39.727513Z",
     "shell.execute_reply": "2025-11-28T09:37:56.789372Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-28T09:38:05.777424Z",
     "iopub.execute_input": "2025-11-28T09:38:05.777861Z",
     "iopub.status.idle": "2025-11-28T09:38:05.814743Z",
     "shell.execute_reply.started": "2025-11-28T09:38:05.777828Z",
     "shell.execute_reply": "2025-11-28T09:38:05.8136Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}