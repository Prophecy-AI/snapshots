{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227dbc5d",
   "metadata": {},
   "source": [
    "# Experiment 104: Post-hoc Bias Correction\n",
    "\n",
    "**Goal**: Reduce the CV-LB intercept by applying post-hoc bias correction.\n",
    "\n",
    "**Rationale**:\n",
    "- CV-LB relationship: LB = 4.29 × CV + 0.0528 (R² = 0.95)\n",
    "- Intercept (0.0528) > Target (0.0347) - target is mathematically unreachable\n",
    "- Post-hoc bias correction can reduce systematic offset\n",
    "\n",
    "**Implementation**:\n",
    "1. During training, compute per-fold bias: `bias = preds.mean() - y_val.mean()`\n",
    "2. Store average bias across folds\n",
    "3. Apply bias correction to predictions: `corrected = preds - bias`\n",
    "\n",
    "**Key insight from web research**:\n",
    "> \"Apply a post-hoc intercept-bias correction – after fitting, compute the average residual on a small validation set of known compounds and subtract that mean bias from all future predictions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de423013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "# Leave-One-Out validation (same as exp_030)\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52200da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print('Base classes defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer with all features (like ens-model)\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)  # Already in [0, 1]\n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "            X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "            X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c545b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Corrected CatBoost + XGBoost Ensemble\n",
    "class BiasCorrectedEnsemble(BaseModel):\n",
    "    \"\"\"CatBoost + XGBoost ensemble with post-hoc bias correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Weights from ens-model kernel\n",
    "        if data == 'single':\n",
    "            self.cat_weight = 7.0 / 13.0\n",
    "            self.xgb_weight = 6.0 / 13.0\n",
    "        else:\n",
    "            self.cat_weight = 1.0 / 3.0\n",
    "            self.xgb_weight = 2.0 / 3.0\n",
    "        \n",
    "        # CatBoost params\n",
    "        self.cat_params = dict(\n",
    "            random_state=42,\n",
    "            iterations=500,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=3,\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        # XGBoost params\n",
    "        self.xgb_params = dict(\n",
    "            random_state=42,\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=0,\n",
    "        )\n",
    "        \n",
    "        self.cat_models = None\n",
    "        self.xgb_models = None\n",
    "        self.bias = None  # Per-target bias correction\n",
    "        self.train_mean = None\n",
    "        \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        # Featurize\n",
    "        X_np = self.featurizer.featurize(train_X)\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        # Store training mean for reference\n",
    "        self.train_mean = Y_np.mean(axis=0)\n",
    "        \n",
    "        # Train CatBoost (one per target)\n",
    "        self.cat_models = []\n",
    "        for t in range(3):\n",
    "            m = CatBoostRegressor(**self.cat_params)\n",
    "            m.fit(X_scaled, Y_np[:, t])\n",
    "            self.cat_models.append(m)\n",
    "        \n",
    "        # Train XGBoost (one per target)\n",
    "        self.xgb_models = []\n",
    "        for t in range(3):\n",
    "            m = xgb.XGBRegressor(**self.xgb_params)\n",
    "            m.fit(X_scaled, Y_np[:, t])\n",
    "            self.xgb_models.append(m)\n",
    "        \n",
    "        # Compute bias on training data (in-sample bias estimate)\n",
    "        # This is a proxy for the validation bias\n",
    "        cat_preds = np.column_stack([m.predict(X_scaled) for m in self.cat_models])\n",
    "        xgb_preds = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        ensemble_preds = self.cat_weight * cat_preds + self.xgb_weight * xgb_preds\n",
    "        \n",
    "        # Compute per-target bias\n",
    "        self.bias = ensemble_preds.mean(axis=0) - Y_np.mean(axis=0)\n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        # Featurize\n",
    "        X_np = self.featurizer.featurize(test_X)\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Get predictions\n",
    "        cat_preds = np.column_stack([m.predict(X_scaled) for m in self.cat_models])\n",
    "        xgb_preds = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # Ensemble\n",
    "        ensemble_preds = self.cat_weight * cat_preds + self.xgb_weight * xgb_preds\n",
    "        \n",
    "        # Apply bias correction\n",
    "        corrected_preds = ensemble_preds - self.bias\n",
    "        \n",
    "        # Clip to valid range [0, 1]\n",
    "        corrected_preds = np.clip(corrected_preds, 0.0, 1.0)\n",
    "        \n",
    "        # Renormalize if sum > 1\n",
    "        totals = corrected_preds.sum(axis=1, keepdims=True)\n",
    "        divisor = np.maximum(totals, 1.0)\n",
    "        corrected_preds = corrected_preds / divisor\n",
    "        \n",
    "        return torch.tensor(corrected_preds)\n",
    "\n",
    "print('BiasCorrectedEnsemble defined')\n",
    "print('Bias correction: preds - (preds.mean() - y_train.mean())')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV to compute local score\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(verbose=True):\n",
    "    \"\"\"Compute CV score with bias correction.\"\"\"\n",
    "    \n",
    "    # Single solvent CV (Leave-One-Out: 24 folds)\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = BiasCorrectedEnsemble(data='single')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE (24-fold LOO): {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV (Leave-One-Ramp-Out: 13 folds)\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = BiasCorrectedEnsemble(data='full')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE (13-fold LORO): {full_cv:.6f}\")\n",
    "    \n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "print(\"Running CV with bias correction...\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'cv_score': float(combined_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'model': 'BiasCorrectedEnsemble (CatBoost + XGBoost with bias correction)',\n",
    "    'validation': 'Leave-One-Out (24 folds single, 13 folds full)',\n",
    "    'baseline_cv': 0.0081,\n",
    "    'improvement': f\"{(0.0081 - combined_cv) / 0.0081 * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/104_bias_correction/metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Combined CV: {combined_cv:.6f}\")\n",
    "print(f\"Baseline CV (exp_030): 0.0081\")\n",
    "print(f\"Improvement: {(0.0081 - combined_cv) / 0.0081 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd90a06",
   "metadata": {},
   "source": [
    "## Generate Submission\n",
    "\n",
    "The following cells follow the official template structure.\n",
    "\n",
    "**CRITICAL**: The model class in submission cells MUST match the CV computation class (`BiasCorrectedEnsemble`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f32fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "print(f\"CV: {combined_cv:.6f}\")\n",
    "print(\"Generating submission...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df51054",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = BiasCorrectedEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aeb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = BiasCorrectedEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "# Verify predictions are valid\n",
    "print(f\"\\nMin target_1: {submission['target_1'].min():.6f}\")\n",
    "print(f\"Min target_2: {submission['target_2'].min():.6f}\")\n",
    "print(f\"Min target_3: {submission['target_3'].min():.6f}\")\n",
    "print(f\"Max sum: {(submission['target_1'] + submission['target_2'] + submission['target_3']).max():.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
