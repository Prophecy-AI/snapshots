{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01ebe00",
   "metadata": {},
   "source": [
    "# Experiment 103: Mixall Kernel with SolventB% Bug Fix\n",
    "\n",
    "**Goal**: Fix the critical SolventB% scaling bug from exp_102.\n",
    "\n",
    "**Bug in exp_102**:\n",
    "```python\n",
    "sb_pct = X['SolventB%'].values.reshape(-1, 1) / 100.0  # BUG!\n",
    "```\n",
    "\n",
    "**The Problem**:\n",
    "- SolventB% is ALREADY in [0, 1] range (verified: min=0.0, max=1.0)\n",
    "- Dividing by 100 makes a 50% mixture (0.5) become 0.005\n",
    "- This completely breaks mixture predictions\n",
    "\n",
    "**Fix**:\n",
    "```python\n",
    "sb_pct = X['SolventB%'].values.reshape(-1, 1)  # Already in [0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86facde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d290c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupKFold validation (5 splits)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate Group K-Fold splits across the solvents (5-fold).\"\"\"\n",
    "    groups = X[\"SOLVENT NAME\"]\n",
    "    n_groups = len(groups.unique())\n",
    "    n_splits = min(5, n_groups)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield (\n",
    "            (X.iloc[train_idx], Y.iloc[train_idx]),\n",
    "            (X.iloc[test_idx], Y.iloc[test_idx]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate Group K-Fold splits across the solvent ramps (5-fold).\"\"\"\n",
    "    groups = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    \n",
    "    n_groups = len(groups.unique())\n",
    "    n_splits = min(5, n_groups)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield (\n",
    "            (X.iloc[train_idx], Y.iloc[train_idx]),\n",
    "            (X.iloc[test_idx], Y.iloc[test_idx]),\n",
    "        )\n",
    "\n",
    "print('GroupKFold validation functions defined (5 splits)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(X, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print('Base classes defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d781ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers with BUG FIX\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 2  # +2 for Time, Temp\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        \n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        feats = self.features.loc[solvent_names].values\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 3  # +3 for Time, Temp, %B\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        \n",
    "        # BUG FIX: SolventB% is ALREADY in [0, 1] range - DO NOT divide by 100!\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)  # FIXED: No / 100.0\n",
    "        \n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        \n",
    "        # Correct mixture interpolation\n",
    "        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, sb_pct, mixture_feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "print('Featurizers defined with BUG FIX')\n",
    "print('SolventB% is now used directly (already in [0, 1] range)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f88794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced MLP\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print('EnhancedMLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleModel with clipping and renormalization\n",
    "class EnsembleModel(BaseModel):\n",
    "    def __init__(self, data='single', hidden_dims=[128, 64, 32], dropout=0.2, use_tta=False, \n",
    "                 weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "        self.data = data\n",
    "        self.use_tta = use_tta\n",
    "        self.weights = weights\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer()\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed()\n",
    "            \n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp = EnhancedMLP(self.smiles_featurizer.feats_dim, hidden_dims=hidden_dims, dropout=dropout)\n",
    "        \n",
    "        # GBDT Models\n",
    "        self.xgb_params = dict(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "        self.rf_params = dict(n_estimators=100, max_depth=10, random_state=42)\n",
    "        self.lgb_params = dict(n_estimators=100, num_leaves=31, learning_rate=0.1, random_state=42, verbose=-1)\n",
    "        \n",
    "        self.xgb = MultiOutputRegressor(xgb.XGBRegressor(**self.xgb_params))\n",
    "        self.rf = MultiOutputRegressor(RandomForestRegressor(**self.rf_params))\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(**self.lgb_params))\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, batch_size=32, lr=0.001, \n",
    "                    optimizer=torch.optim.Adam, criterion=nn.MSELoss, device=None, verbose=False):\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Ensure strict DataFrame format with string column names for LightGBM compatibility\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train GBDT models\n",
    "        self.xgb.fit(X_scaled_df, train_Y_np)\n",
    "        self.rf.fit(X_scaled_df, train_Y_np)\n",
    "        self.lgbm.fit(X_scaled_df, train_Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(TensorDataset(X_tensor_scaled, train_Y_tensor), \n",
    "                                  batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        criterion_inst = criterion()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_inst.zero_grad()\n",
    "                loss = criterion_inst(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer_inst.step()\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # Ensure strict DataFrame format with string column names for LightGBM compatibility\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP Preds\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "            \n",
    "        # GBDT Preds\n",
    "        xgb_preds = self.xgb.predict(X_scaled_df)\n",
    "        rf_preds = self.rf.predict(X_scaled_df)\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # Weighted Ensemble\n",
    "        final_preds = (self.weights[0] * mlp_preds + \n",
    "                       self.weights[1] * xgb_preds + \n",
    "                       self.weights[2] * rf_preds + \n",
    "                       self.weights[3] * lgb_preds)\n",
    "        \n",
    "        # Clip predictions to valid range [0, 1]\n",
    "        final_preds = np.clip(final_preds, 0.0, 1.0)\n",
    "        \n",
    "        # Renormalize if sum > 1 (mass balance constraint)\n",
    "        totals = final_preds.sum(axis=1, keepdims=True)\n",
    "        divisor = np.maximum(totals, 1.0)\n",
    "        final_preds = final_preds / divisor\n",
    "                       \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print('EnsembleModel defined with clipping and renormalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV to compute local score\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score(verbose=True):\n",
    "    \"\"\"Compute CV score with GroupKFold (5 splits).\"\"\"\n",
    "    \n",
    "    # Single solvent CV (5 folds)\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='single')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nSingle Solvent CV MSE (5-fold): {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV (5 folds)\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='full')\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        if verbose:\n",
    "            print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    if verbose:\n",
    "        print(f\"\\nFull Data CV MSE (5-fold): {full_cv:.6f}\")\n",
    "    \n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "print(\"Running CV with FIXED SolventB% handling...\")\n",
    "single_cv, full_cv, combined_cv = compute_cv_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'cv_score': float(combined_cv),\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'model': 'EnsembleModel (MLP + XGB + RF + LGB) with FIXED SolventB%',\n",
    "    'validation': 'GroupKFold (5 splits)',\n",
    "    'fix': 'Removed / 100.0 from SolventB% - it is already in [0, 1] range'\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/103_mixall_fixed/metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Combined CV (with fix): {combined_cv:.6f}\")\n",
    "print(f\"exp_102 CV (with bug): 0.013542\")\n",
    "print(f\"Improvement: {(0.013542 - combined_cv) / 0.013542 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8f5d4",
   "metadata": {},
   "source": [
    "## Generate Submission\n",
    "\n",
    "The following cells follow the official template structure.\n",
    "\n",
    "**CRITICAL**: The model class in submission cells MUST match the CV computation class (`EnsembleModel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "print(f\"CV (with fix): {combined_cv:.6f}\")\n",
    "print(\"Generating submission...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90356f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98305c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "# Verify predictions are valid\n",
    "print(f\"\\nMin target_1: {submission['target_1'].min():.6f}\")\n",
    "print(f\"Min target_2: {submission['target_2'].min():.6f}\")\n",
    "print(f\"Min target_3: {submission['target_3'].min():.6f}\")\n",
    "print(f\"Max sum: {(submission['target_1'] + submission['target_2'] + submission['target_3']).max():.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
