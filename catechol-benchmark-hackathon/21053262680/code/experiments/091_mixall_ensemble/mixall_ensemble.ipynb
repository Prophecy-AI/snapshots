{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e4ac4e",
   "metadata": {},
   "source": [
    "# Experiment 091: MixAll Ensemble (MLP + XGBoost + RF + LightGBM)\n",
    "\n",
    "Implementing the mixall kernel approach with proper Leave-One-Out validation.\n",
    "\n",
    "Key features:\n",
    "- Ensemble of 4 models: MLP, XGBoost, RandomForest, LightGBM\n",
    "- Spange descriptors (13 features)\n",
    "- Weighted ensemble with learned weights\n",
    "- Official Leave-One-Out validation (not GroupKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/data')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import (\n",
    "    INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC,\n",
    "    INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES,\n",
    "    load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n",
    ")\n",
    "\n",
    "print(\"Imports complete\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed79479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34607f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizers\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 2  # +2 for Time, Temp\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        \n",
    "        solvent_names = X['SOLVENT NAME']\n",
    "        feats = self.features.loc[solvent_names].values\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        self.features = load_features(features)\n",
    "        self.feats_dim = self.features.shape[1] + 3  # +3 for Time, Temp, %B\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        res_time = X['Residence Time'].values.reshape(-1, 1)\n",
    "        temp = X['Temperature'].values.reshape(-1, 1)\n",
    "        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "        \n",
    "        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n",
    "        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n",
    "        \n",
    "        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n",
    "        \n",
    "        final_feats = np.hstack([res_time, temp, sb_pct, mixture_feats])\n",
    "        return torch.tensor(final_feats, dtype=torch.float32)\n",
    "\n",
    "print(\"Featurizers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b864bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced MLP\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=3, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"MLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model (MLP + XGBoost + RF + LightGBM)\n",
    "class EnsembleModel(BaseModel):\n",
    "    def __init__(self, data='single', hidden_dims=[128, 64, 32], dropout=0.2, \n",
    "                 weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout = dropout\n",
    "        self.weights = weights  # [mlp, xgb, rf, lgb]\n",
    "        \n",
    "        # Featurizer\n",
    "        if data == 'single':\n",
    "            self.smiles_featurizer = PrecomputedFeaturizer('spange_descriptors')\n",
    "        else:\n",
    "            self.smiles_featurizer = PrecomputedFeaturizerMixed('spange_descriptors')\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = EnhancedMLP(\n",
    "            input_dim=self.smiles_featurizer.feats_dim,\n",
    "            output_dim=3,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # XGBoost\n",
    "        self.xgb_params = {\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        self.xgb = MultiOutputRegressor(xgb.XGBRegressor(**self.xgb_params))\n",
    "        \n",
    "        # Random Forest\n",
    "        self.rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        self.rf = MultiOutputRegressor(RandomForestRegressor(**self.rf_params))\n",
    "        \n",
    "        # LightGBM\n",
    "        self.lgb_params = {\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42,\n",
    "            'verbosity': -1\n",
    "        }\n",
    "        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(**self.lgb_params))\n",
    "        \n",
    "        # Scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def train_model(self, train_X, train_Y, num_epochs=100, lr=1e-3, batch_size=32,\n",
    "                    optimizer=torch.optim.Adam, criterion=nn.MSELoss, device=None, verbose=False):\n",
    "        # Featurize\n",
    "        X_tensor = self.smiles_featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        train_Y_np = train_Y.values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        # Create DataFrame for GBDT models\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Train GBDT models\n",
    "        self.xgb.fit(X_scaled_df, train_Y_np)\n",
    "        self.rf.fit(X_scaled_df, train_Y_np)\n",
    "        self.lgbm.fit(X_scaled_df, train_Y_np)\n",
    "        \n",
    "        # Train MLP\n",
    "        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mlp.to(device)\n",
    "        \n",
    "        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor_scaled, train_Y_tensor),\n",
    "            batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        criterion_inst = criterion()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.mlp.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_inst.zero_grad()\n",
    "                loss = criterion_inst(self.mlp(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer_inst.step()\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        X_tensor = self.smiles_featurizer.featurize(test_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        X_scaled = self.scaler.transform(X_np)\n",
    "        \n",
    "        # DataFrame for GBDT\n",
    "        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        device = next(self.mlp.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n",
    "        \n",
    "        # GBDT predictions\n",
    "        xgb_preds = self.xgb.predict(X_scaled_df)\n",
    "        rf_preds = self.rf.predict(X_scaled_df)\n",
    "        lgb_preds = self.lgbm.predict(X_scaled_df)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_preds = (\n",
    "            self.weights[0] * mlp_preds +\n",
    "            self.weights[1] * xgb_preds +\n",
    "            self.weights[2] * rf_preds +\n",
    "            self.weights[3] * lgb_preds\n",
    "        )\n",
    "        \n",
    "        return torch.tensor(final_preds)\n",
    "\n",
    "print(\"EnsembleModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d630b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model quickly\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: {X_single.shape}, {Y_single.shape}\")\n",
    "\n",
    "# Quick test\n",
    "model = EnsembleModel(data='single')\n",
    "model.train_model(X_single, Y_single, num_epochs=10)\n",
    "preds = model.predict(X_single[:5])\n",
    "print(f\"Test predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions:\\n{preds[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run proper CV with Leave-One-Out\n",
    "import tqdm\n",
    "\n",
    "def compute_cv_score():\n",
    "    \"\"\"Compute CV score using official Leave-One-Out validation.\"\"\"\n",
    "    \n",
    "    # Single solvent CV\n",
    "    X_single, Y_single = load_data(\"single_solvent\")\n",
    "    split_generator = generate_leave_one_out_splits(X_single, Y_single)\n",
    "    \n",
    "    single_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='single')\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        single_mse_list.append(mse)\n",
    "        print(f\"Single Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    single_cv = np.mean(single_mse_list)\n",
    "    print(f\"\\nSingle Solvent CV MSE: {single_cv:.6f}\")\n",
    "    \n",
    "    # Full data CV\n",
    "    X_full, Y_full = load_data(\"full\")\n",
    "    split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "    \n",
    "    full_mse_list = []\n",
    "    for fold_idx, split in enumerate(split_generator):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = EnsembleModel(data='full')\n",
    "        model.train_model(train_X, train_Y, num_epochs=100)\n",
    "        \n",
    "        predictions = model.predict(test_X)\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        \n",
    "        mse = np.mean((predictions_np - test_Y.values) ** 2)\n",
    "        full_mse_list.append(mse)\n",
    "        print(f\"Full Fold {fold_idx}: MSE = {mse:.6f}\")\n",
    "    \n",
    "    full_cv = np.mean(full_mse_list)\n",
    "    print(f\"\\nFull Data CV MSE: {full_cv:.6f}\")\n",
    "    \n",
    "    # Combined CV (average of single and full)\n",
    "    combined_cv = (single_cv + full_cv) / 2\n",
    "    print(f\"\\n=== Combined CV MSE: {combined_cv:.6f} ===\")\n",
    "    \n",
    "    return single_cv, full_cv, combined_cv\n",
    "\n",
    "single_cv, full_cv, combined_cv = compute_cv_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd164b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CV results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'single_cv': float(single_cv),\n",
    "    'full_cv': float(full_cv),\n",
    "    'combined_cv': float(combined_cv),\n",
    "    'model': 'EnsembleModel (MLP + XGBoost + RF + LightGBM)',\n",
    "    'features': 'spange_descriptors',\n",
    "    'weights': [0.25, 0.25, 0.25, 0.25]\n",
    "}\n",
    "\n",
    "with open('/home/code/experiments/091_mixall_ensemble/metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved\")\n",
    "print(f\"Combined CV: {combined_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bd326",
   "metadata": {},
   "source": [
    "## Generate Submission\n",
    "\n",
    "The following cells follow the official template structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = EnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad54606",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "# Also save to standard location\n",
    "import shutil\n",
    "shutil.copy(\"submission.csv\", \"/home/submission/submission.csv\")\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
