{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdfccfe",
   "metadata": {},
   "source": [
    "# Pseudo-labeling for Distribution Adaptation\n",
    "\n",
    "**Strategy**: Use confident test predictions to augment training data, adapting the model to the test distribution.\n",
    "\n",
    "**Hypothesis**: The CV-LB gap is due to distribution shift (test solvents are different from training). Pseudo-labeling can help the model adapt to the test distribution.\n",
    "\n",
    "**Implementation**:\n",
    "1. Train initial model on training data\n",
    "2. Make predictions on test data (held-out fold)\n",
    "3. Select high-confidence predictions (low variance across ensemble)\n",
    "4. Add pseudo-labeled samples to training\n",
    "5. Retrain model on augmented data\n",
    "6. Repeat for a few iterations\n",
    "\n",
    "**Model**: CatBoost+XGBoost ensemble (best performing tabular model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fb20e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:00.801135Z",
     "iopub.status.busy": "2026-01-16T10:33:00.800639Z",
     "iopub.status.idle": "2026-01-16T10:33:02.320771Z",
     "shell.execute_reply": "2026-01-16T10:33:02.320358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7291232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:02.322266Z",
     "iopub.status.busy": "2026-01-16T10:33:02.321820Z",
     "iopub.status.idle": "2026-01-16T10:33:02.326331Z",
     "shell.execute_reply": "2026-01-16T10:33:02.325989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acab9245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:02.327270Z",
     "iopub.status.busy": "2026-01-16T10:33:02.327161Z",
     "iopub.status.idle": "2026-01-16T10:33:02.358420Z",
     "shell.execute_reply": "2026-01-16T10:33:02.358089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942c692e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:02.359350Z",
     "iopub.status.busy": "2026-01-16T10:33:02.359229Z",
     "iopub.status.idle": "2026-01-16T10:33:02.364715Z",
     "shell.execute_reply": "2026-01-16T10:33:02.364318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dimension: 145\n"
     ]
    }
   ],
   "source": [
    "# Full Featurizer (for MLP and LGBM) - 145 features\n",
    "class FullFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Full feature dimension: {FullFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c66846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:02.365635Z",
     "iopub.status.busy": "2026-01-16T10:33:02.365536Z",
     "iopub.status.idle": "2026-01-16T10:33:02.438576Z",
     "shell.execute_reply": "2026-01-16T10:33:02.438222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PseudoLabelingCatXGBEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# CatBoost + XGBoost Ensemble with Pseudo-labeling\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "import tqdm\n",
    "\n",
    "class PseudoLabelingCatXGBEnsemble:\n",
    "    def __init__(self, data='single', n_iterations=3, confidence_threshold=0.8):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.scalers = [StandardScaler() for _ in range(3)]\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        self.cat_weight = 0.5\n",
    "        self.xgb_weight = 0.5\n",
    "        self.n_iterations = n_iterations\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "    def _train_base_models(self, X_feats, Y_vals):\n",
    "        \"\"\"Train base CatBoost and XGBoost models\"\"\"\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            y_scaled = self.scalers[i].fit_transform(Y_vals[:, i:i+1]).ravel()\n",
    "            \n",
    "            # CatBoost\n",
    "            cat_model = cb.CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                random_seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "            cat_model.fit(X_feats, y_scaled)\n",
    "            self.catboost_models.append(cat_model)\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=3,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_feats, y_scaled)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "    \n",
    "    def _predict_with_uncertainty(self, X_feats):\n",
    "        \"\"\"Predict with uncertainty estimation (variance between CatBoost and XGBoost)\"\"\"\n",
    "        preds = []\n",
    "        uncertainties = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            cat_pred = self.catboost_models[i].predict(X_feats)\n",
    "            xgb_pred = self.xgb_models[i].predict(X_feats)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            pred_scaled = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "            pred = self.scalers[i].inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "            preds.append(pred)\n",
    "            \n",
    "            # Uncertainty = absolute difference between models\n",
    "            uncertainty = np.abs(cat_pred - xgb_pred)\n",
    "            uncertainties.append(uncertainty)\n",
    "        \n",
    "        return np.stack(preds, axis=1), np.stack(uncertainties, axis=1)\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        \"\"\"Train with pseudo-labeling iterations\"\"\"\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        Y_vals = Y.values\n",
    "        \n",
    "        # Initial training\n",
    "        self._train_base_models(X_feats, Y_vals)\n",
    "        \n",
    "        # Note: In the actual CV loop, we don't have access to test data\n",
    "        # So pseudo-labeling is done within the training data using a held-out set\n",
    "        # This is a simplified version that just trains normally\n",
    "        # The real benefit would come from using unlabeled test data\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        preds, _ = self._predict_with_uncertainty(X_feats)\n",
    "        return torch.tensor(preds)\n",
    "\n",
    "print('PseudoLabelingCatXGBEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a04e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:33:02.439930Z",
     "iopub.status.busy": "2026-01-16T10:33:02.439756Z",
     "iopub.status.idle": "2026-01-16T10:33:02.447654Z",
     "shell.execute_reply": "2026-01-16T10:33:02.447285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfTrainingCatXGBEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# Actually, pseudo-labeling in CV is tricky because we don't have access to test data\n",
    "# Let's try a different approach: Self-training within the training data\n",
    "# We'll use a held-out portion of training data as \"pseudo-test\" data\n",
    "\n",
    "class SelfTrainingCatXGBEnsemble:\n",
    "    def __init__(self, data='single', n_iterations=2, holdout_ratio=0.2):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.scalers = [StandardScaler() for _ in range(3)]\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        self.cat_weight = 0.5\n",
    "        self.xgb_weight = 0.5\n",
    "        self.n_iterations = n_iterations\n",
    "        self.holdout_ratio = holdout_ratio\n",
    "        \n",
    "    def _train_base_models(self, X_feats, Y_vals):\n",
    "        \"\"\"Train base CatBoost and XGBoost models\"\"\"\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        self.scalers = [StandardScaler() for _ in range(3)]\n",
    "        \n",
    "        for i in range(3):\n",
    "            y_scaled = self.scalers[i].fit_transform(Y_vals[:, i:i+1]).ravel()\n",
    "            \n",
    "            # CatBoost\n",
    "            cat_model = cb.CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                random_seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "            cat_model.fit(X_feats, y_scaled)\n",
    "            self.catboost_models.append(cat_model)\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=3,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_feats, y_scaled)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "    \n",
    "    def _predict_raw(self, X_feats):\n",
    "        \"\"\"Predict without uncertainty\"\"\"\n",
    "        preds = []\n",
    "        for i in range(3):\n",
    "            cat_pred = self.catboost_models[i].predict(X_feats)\n",
    "            xgb_pred = self.xgb_models[i].predict(X_feats)\n",
    "            pred_scaled = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "            pred = self.scalers[i].inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "            preds.append(pred)\n",
    "        return np.stack(preds, axis=1)\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        \"\"\"Train with self-training iterations\"\"\"\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        Y_vals = Y.values\n",
    "        \n",
    "        # Split into train and holdout\n",
    "        n = len(X_feats)\n",
    "        n_holdout = int(n * self.holdout_ratio)\n",
    "        indices = np.random.permutation(n)\n",
    "        train_idx = indices[n_holdout:]\n",
    "        holdout_idx = indices[:n_holdout]\n",
    "        \n",
    "        X_train = X_feats[train_idx]\n",
    "        Y_train = Y_vals[train_idx]\n",
    "        X_holdout = X_feats[holdout_idx]\n",
    "        Y_holdout = Y_vals[holdout_idx]\n",
    "        \n",
    "        # Initial training on subset\n",
    "        self._train_base_models(X_train, Y_train)\n",
    "        \n",
    "        # Self-training iterations\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Predict on holdout\n",
    "            holdout_preds = self._predict_raw(X_holdout)\n",
    "            \n",
    "            # Combine original training data with pseudo-labeled holdout\n",
    "            X_combined = np.vstack([X_train, X_holdout])\n",
    "            Y_combined = np.vstack([Y_train, holdout_preds])\n",
    "            \n",
    "            # Retrain on combined data\n",
    "            self._train_base_models(X_combined, Y_combined)\n",
    "        \n",
    "        # Final training on all data\n",
    "        self._train_base_models(X_feats, Y_vals)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        preds = self._predict_raw(X_feats)\n",
    "        return torch.tensor(preds)\n",
    "\n",
    "print('SelfTrainingCatXGBEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on single solvent data\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: {len(X)} samples, {len(X[\"SOLVENT NAME\"].unique())} solvents')\n",
    "\n",
    "all_mse = []\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(generate_leave_one_out_splits(X, Y))):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = SelfTrainingCatXGBEnsemble(data='single', n_iterations=2, holdout_ratio=0.2)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    preds = model.predict(test_X).numpy()\n",
    "    targets = test_Y.values\n",
    "    \n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    all_mse.append(mse)\n",
    "\n",
    "single_mse = np.mean(all_mse)\n",
    "print(f'\\nSingle Solvent MSE: {single_mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39064ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on full data\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "print(f'Full data: {len(X_full)} samples')\n",
    "\n",
    "all_mse_full = []\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(generate_leave_one_ramp_out_splits(X_full, Y_full))):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = SelfTrainingCatXGBEnsemble(data='full', n_iterations=2, holdout_ratio=0.2)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    preds = model.predict(test_X).numpy()\n",
    "    targets = test_Y.values\n",
    "    \n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    all_mse_full.append(mse)\n",
    "\n",
    "full_mse = np.mean(all_mse_full)\n",
    "print(f'\\nFull Data MSE: {full_mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aabb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "n_single = 656\n",
    "n_full = 1227  # Corrected from previous wrong value\n",
    "overall_mse = (single_mse * n_single + full_mse * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== Self-Training Results ===')\n",
    "print(f'Single Solvent MSE: {single_mse:.6f}')\n",
    "print(f'Full Data MSE: {full_mse:.6f}')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nBest baseline (CatBoost+XGBoost): 0.008092')\n",
    "print(f'Difference: {(overall_mse - 0.008092) / 0.008092 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also try a simpler approach: Conservative predictions\n",
    "# Blend predictions toward the training mean when uncertainty is high\n",
    "\n",
    "class ConservativeCatXGBEnsemble:\n",
    "    def __init__(self, data='single', blend_factor=0.1):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.scalers = [StandardScaler() for _ in range(3)]\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        self.cat_weight = 0.5\n",
    "        self.xgb_weight = 0.5\n",
    "        self.blend_factor = blend_factor\n",
    "        self.train_means = None\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        Y_vals = Y.values\n",
    "        \n",
    "        # Store training means for conservative blending\n",
    "        self.train_means = Y_vals.mean(axis=0)\n",
    "        \n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            y_scaled = self.scalers[i].fit_transform(Y_vals[:, i:i+1]).ravel()\n",
    "            \n",
    "            # CatBoost\n",
    "            cat_model = cb.CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                random_seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "            cat_model.fit(X_feats, y_scaled)\n",
    "            self.catboost_models.append(cat_model)\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=3,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_feats, y_scaled)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            cat_pred = self.catboost_models[i].predict(X_feats)\n",
    "            xgb_pred = self.xgb_models[i].predict(X_feats)\n",
    "            \n",
    "            # Ensemble\n",
    "            pred_scaled = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "            pred = self.scalers[i].inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "            \n",
    "            # Conservative blending toward training mean\n",
    "            pred = (1 - self.blend_factor) * pred + self.blend_factor * self.train_means[i]\n",
    "            preds.append(pred)\n",
    "        \n",
    "        return torch.tensor(np.stack(preds, axis=1))\n",
    "\n",
    "print('ConservativeCatXGBEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee77d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conservative predictions with different blend factors\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "for blend_factor in [0.0, 0.05, 0.1, 0.15, 0.2]:\n",
    "    all_mse = []\n",
    "    for fold_idx, split in enumerate(generate_leave_one_out_splits(X, Y)):\n",
    "        (train_X, train_Y), (test_X, test_Y) = split\n",
    "        \n",
    "        model = ConservativeCatXGBEnsemble(data='single', blend_factor=blend_factor)\n",
    "        model.train_model(train_X, train_Y)\n",
    "        \n",
    "        preds = model.predict(test_X).numpy()\n",
    "        targets = test_Y.values\n",
    "        \n",
    "        mse = np.mean((preds - targets) ** 2)\n",
    "        all_mse.append(mse)\n",
    "    \n",
    "    print(f'Blend factor {blend_factor:.2f}: Single Solvent MSE = {np.mean(all_mse):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline (blend_factor=0) should match our best model\n",
    "# Let's verify and then run the full CV with the best blend factor\n",
    "\n",
    "# First, let's run the standard CatBoost+XGBoost ensemble to verify baseline\n",
    "class CatXGBEnsemble:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data = data\n",
    "        self.mixed = (data == 'full')\n",
    "        self.featurizer = FullFeaturizer(mixed=self.mixed)\n",
    "        self.scalers = [StandardScaler() for _ in range(3)]\n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        self.cat_weight = 0.5\n",
    "        self.xgb_weight = 0.5\n",
    "        \n",
    "    def train_model(self, X, Y):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        Y_vals = Y.values\n",
    "        \n",
    "        self.catboost_models = []\n",
    "        self.xgb_models = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            y_scaled = self.scalers[i].fit_transform(Y_vals[:, i:i+1]).ravel()\n",
    "            \n",
    "            # CatBoost\n",
    "            cat_model = cb.CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                random_seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "            cat_model.fit(X_feats, y_scaled)\n",
    "            self.catboost_models.append(cat_model)\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                reg_lambda=3,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_feats, y_scaled)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_feats = self.featurizer.featurize(X)\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            cat_pred = self.catboost_models[i].predict(X_feats)\n",
    "            xgb_pred = self.xgb_models[i].predict(X_feats)\n",
    "            \n",
    "            # Ensemble\n",
    "            pred_scaled = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "            pred = self.scalers[i].inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "            preds.append(pred)\n",
    "        \n",
    "        return torch.tensor(np.stack(preds, axis=1))\n",
    "\n",
    "print('CatXGBEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5508672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full CV with baseline CatXGBEnsemble\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f'Single solvent data: {len(X)} samples')\n",
    "\n",
    "all_mse = []\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(generate_leave_one_out_splits(X, Y))):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = CatXGBEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    preds = model.predict(test_X).numpy()\n",
    "    targets = test_Y.values\n",
    "    \n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    all_mse.append(mse)\n",
    "\n",
    "single_mse_baseline = np.mean(all_mse)\n",
    "print(f'\\nBaseline Single Solvent MSE: {single_mse_baseline:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full CV on full data\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "print(f'Full data: {len(X_full)} samples')\n",
    "\n",
    "all_mse_full = []\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(generate_leave_one_ramp_out_splits(X_full, Y_full))):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = CatXGBEnsemble(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    preds = model.predict(test_X).numpy()\n",
    "    targets = test_Y.values\n",
    "    \n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    all_mse_full.append(mse)\n",
    "\n",
    "full_mse_baseline = np.mean(all_mse_full)\n",
    "print(f'\\nBaseline Full Data MSE: {full_mse_baseline:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall baseline CV score\n",
    "n_single = 656\n",
    "n_full = 1227\n",
    "overall_mse_baseline = (single_mse_baseline * n_single + full_mse_baseline * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== Baseline CatXGBEnsemble Results ===')\n",
    "print(f'Single Solvent MSE: {single_mse_baseline:.6f}')\n",
    "print(f'Full Data MSE: {full_mse_baseline:.6f}')\n",
    "print(f'Overall MSE: {overall_mse_baseline:.6f}')\n",
    "print(f'\\nExpected (from exp_049): 0.008092')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = CatXGBEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2478c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = CatXGBEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e44171",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
