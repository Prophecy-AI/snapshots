{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7105e10d",
   "metadata": {},
   "source": [
    "# Loop 114 Analysis: CV-LB Relationship and Strategy Assessment\n",
    "\n",
    "## Goal\n",
    "Analyze the CV-LB relationship across all 24 submissions and determine the best path forward with only 3 submissions remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928314ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:22:59.278983Z",
     "iopub.status.busy": "2026-01-16T20:22:59.278421Z",
     "iopub.status.idle": "2026-01-16T20:23:00.011312Z",
     "shell.execute_reply": "2026-01-16T20:23:00.010922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total submissions with LB: 14\n",
      "        exp      cv      lb\n",
      "0   exp_000  0.0111  0.0982\n",
      "1   exp_001  0.0123  0.1065\n",
      "2   exp_003  0.0105  0.0972\n",
      "3   exp_005  0.0104  0.0969\n",
      "4   exp_006  0.0097  0.0946\n",
      "5   exp_007  0.0093  0.0932\n",
      "6   exp_009  0.0092  0.0936\n",
      "7   exp_012  0.0090  0.0913\n",
      "8   exp_024  0.0087  0.0893\n",
      "9   exp_026  0.0085  0.0887\n",
      "10  exp_030  0.0083  0.0877\n",
      "11  exp_035  0.0098  0.0970\n",
      "12  exp_073  0.0084  0.1451\n",
      "13  exp_111  0.0129  0.1063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# All submissions with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},  # Best LB\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "    {'exp': 'exp_073', 'cv': 0.0084, 'lb': 0.1451},  # OUTLIER - model mismatch?\n",
    "    {'exp': 'exp_111', 'cv': 0.0129, 'lb': 0.1063},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(f\"Total submissions with LB: {len(df)}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001aef9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:23:00.012633Z",
     "iopub.status.busy": "2026-01-16T20:23:00.012491Z",
     "iopub.status.idle": "2026-01-16T20:23:00.018831Z",
     "shell.execute_reply": "2026-01-16T20:23:00.018463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid submissions (excluding exp_073): 13\n",
      "\n",
      "=== CV-LB Relationship ===\n",
      "Linear fit: LB = 4.0895 × CV + 0.0546\n",
      "R² = 0.9607\n",
      "Standard error: 0.2494\n",
      "\n",
      "=== Target Analysis ===\n",
      "Target LB: 0.0347\n",
      "Intercept: 0.0546\n",
      "Intercept > Target? True\n",
      "CRITICAL: Intercept (0.0546) > Target (0.0347)!\n",
      "Target is MATHEMATICALLY UNREACHABLE with approaches on this line!\n",
      "Required CV would be: -0.004872 (NEGATIVE = IMPOSSIBLE)\n",
      "\n",
      "=== Best Achieved ===\n",
      "Best CV: 0.0083\n",
      "Best LB: 0.0877\n",
      "Gap to target: 0.0530 (152.7%)\n",
      "\n",
      "Expected LB for best CV (0.0083): 0.0886\n",
      "Actual best LB: 0.0877\n",
      "Difference: -0.0009\n"
     ]
    }
   ],
   "source": [
    "# Exclude outlier exp_073 (likely model class mismatch)\n",
    "df_valid = df[df['exp'] != 'exp_073'].copy()\n",
    "print(f\"Valid submissions (excluding exp_073): {len(df_valid)}\")\n",
    "\n",
    "# Fit linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_valid['cv'], df_valid['lb'])\n",
    "\n",
    "print(f\"\\n=== CV-LB Relationship ===\")\n",
    "print(f\"Linear fit: LB = {slope:.4f} × CV + {intercept:.4f}\")\n",
    "print(f\"R² = {r_value**2:.4f}\")\n",
    "print(f\"Standard error: {std_err:.4f}\")\n",
    "\n",
    "# Target analysis\n",
    "target = 0.0347\n",
    "print(f\"\\n=== Target Analysis ===\")\n",
    "print(f\"Target LB: {target}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Intercept > Target? {intercept > target}\")\n",
    "\n",
    "if intercept < target:\n",
    "    required_cv = (target - intercept) / slope\n",
    "    print(f\"Required CV to hit target: {required_cv:.6f}\")\n",
    "else:\n",
    "    print(f\"CRITICAL: Intercept ({intercept:.4f}) > Target ({target})!\")\n",
    "    print(f\"Target is MATHEMATICALLY UNREACHABLE with approaches on this line!\")\n",
    "    print(f\"Required CV would be: {(target - intercept) / slope:.6f} (NEGATIVE = IMPOSSIBLE)\")\n",
    "\n",
    "# Best achieved\n",
    "best_cv = df_valid['cv'].min()\n",
    "best_lb = df_valid['lb'].min()\n",
    "print(f\"\\n=== Best Achieved ===\")\n",
    "print(f\"Best CV: {best_cv:.4f}\")\n",
    "print(f\"Best LB: {best_lb:.4f}\")\n",
    "print(f\"Gap to target: {best_lb - target:.4f} ({(best_lb - target) / target * 100:.1f}%)\")\n",
    "\n",
    "# Expected LB for best CV\n",
    "expected_lb = slope * best_cv + intercept\n",
    "print(f\"\\nExpected LB for best CV ({best_cv:.4f}): {expected_lb:.4f}\")\n",
    "print(f\"Actual best LB: {best_lb:.4f}\")\n",
    "print(f\"Difference: {best_lb - expected_lb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc574eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:23:00.019888Z",
     "iopub.status.busy": "2026-01-16T20:23:00.019795Z",
     "iopub.status.idle": "2026-01-16T20:23:00.024070Z",
     "shell.execute_reply": "2026-01-16T20:23:00.023733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== exp_112 (Pseudo-Labeling) Analysis ===\n",
      "CV: 0.009566\n",
      "Expected LB from line: 0.0937\n",
      "\n",
      "If LB ≈ 0.0937: Pseudo-labeling is ON THE LINE (no improvement)\n",
      "If LB < 0.0887: Pseudo-labeling CHANGED the relationship (promising!)\n",
      "If LB > 0.0987: Something is WRONG\n",
      "\n",
      "=== What We Need ===\n",
      "Target LB: 0.0347\n",
      "Current best LB: 0.0877\n",
      "Improvement needed: 0.0530 (60.4%)\n",
      "\n",
      "This is a MASSIVE improvement - unlikely with incremental changes.\n",
      "We need to BREAK THE CV-LB LINE, not just improve CV.\n"
     ]
    }
   ],
   "source": [
    "# Analyze exp_112 (pseudo-labeling)\n",
    "exp_112_cv = 0.009566\n",
    "exp_112_expected_lb = slope * exp_112_cv + intercept\n",
    "print(f\"=== exp_112 (Pseudo-Labeling) Analysis ===\")\n",
    "print(f\"CV: {exp_112_cv:.6f}\")\n",
    "print(f\"Expected LB from line: {exp_112_expected_lb:.4f}\")\n",
    "print(f\"\\nIf LB ≈ {exp_112_expected_lb:.4f}: Pseudo-labeling is ON THE LINE (no improvement)\")\n",
    "print(f\"If LB < {exp_112_expected_lb - 0.005:.4f}: Pseudo-labeling CHANGED the relationship (promising!)\")\n",
    "print(f\"If LB > {exp_112_expected_lb + 0.005:.4f}: Something is WRONG\")\n",
    "\n",
    "# What would we need?\n",
    "print(f\"\\n=== What We Need ===\")\n",
    "print(f\"Target LB: {target}\")\n",
    "print(f\"Current best LB: {best_lb:.4f}\")\n",
    "print(f\"Improvement needed: {best_lb - target:.4f} ({(best_lb - target) / best_lb * 100:.1f}%)\")\n",
    "print(f\"\\nThis is a MASSIVE improvement - unlikely with incremental changes.\")\n",
    "print(f\"We need to BREAK THE CV-LB LINE, not just improve CV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00541daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:23:00.025110Z",
     "iopub.status.busy": "2026-01-16T20:23:00.025011Z",
     "iopub.status.idle": "2026-01-16T20:23:00.028255Z",
     "shell.execute_reply": "2026-01-16T20:23:00.027904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Approaches Tried (from session_state) ===\n",
      "1. MLP with Arrhenius kinetics\n",
      "2. LightGBM\n",
      "3. DRFP + PCA\n",
      "4. Combined Spange + DRFP\n",
      "5. Deep Residual MLP (FAILED)\n",
      "6. Large Ensemble (15 models)\n",
      "7. Simpler Model [64, 32]\n",
      "8. CatBoost + XGBoost ensemble\n",
      "9. GNN (CV=0.026 - 3x worse)\n",
      "10. ChemBERTa (CV=0.028 - 3.5x worse)\n",
      "11. Chemical Similarity blending (exp_111)\n",
      "12. Pseudo-labeling (exp_112)\n",
      "\n",
      "=== Key Insight ===\n",
      "ALL tabular approaches (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line.\n",
      "GNN and ChemBERTa have WORSE CV (3x worse) - not promising.\n",
      "Chemical similarity blending (exp_111) was ON THE LINE.\n",
      "Pseudo-labeling (exp_112) is likely ON THE LINE too.\n",
      "\n",
      "The problem is STRUCTURAL - test solvents are fundamentally different from training.\n"
     ]
    }
   ],
   "source": [
    "# Analyze what approaches have been tried\n",
    "print(\"=== Approaches Tried (from session_state) ===\")\n",
    "approaches = [\n",
    "    \"MLP with Arrhenius kinetics\",\n",
    "    \"LightGBM\",\n",
    "    \"DRFP + PCA\",\n",
    "    \"Combined Spange + DRFP\",\n",
    "    \"Deep Residual MLP (FAILED)\",\n",
    "    \"Large Ensemble (15 models)\",\n",
    "    \"Simpler Model [64, 32]\",\n",
    "    \"CatBoost + XGBoost ensemble\",\n",
    "    \"GNN (CV=0.026 - 3x worse)\",\n",
    "    \"ChemBERTa (CV=0.028 - 3.5x worse)\",\n",
    "    \"Chemical Similarity blending (exp_111)\",\n",
    "    \"Pseudo-labeling (exp_112)\",\n",
    "]\n",
    "\n",
    "for i, approach in enumerate(approaches, 1):\n",
    "    print(f\"{i}. {approach}\")\n",
    "\n",
    "print(f\"\\n=== Key Insight ===\")\n",
    "print(\"ALL tabular approaches (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line.\")\n",
    "print(\"GNN and ChemBERTa have WORSE CV (3x worse) - not promising.\")\n",
    "print(\"Chemical similarity blending (exp_111) was ON THE LINE.\")\n",
    "print(\"Pseudo-labeling (exp_112) is likely ON THE LINE too.\")\n",
    "print(\"\\nThe problem is STRUCTURAL - test solvents are fundamentally different from training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f9bc9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:23:00.029066Z",
     "iopub.status.busy": "2026-01-16T20:23:00.028977Z",
     "iopub.status.idle": "2026-01-16T20:23:00.032380Z",
     "shell.execute_reply": "2026-01-16T20:23:00.032029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Remaining Options ===\n",
      "\n",
      "1. SUBMIT exp_112 to confirm pseudo-labeling doesn't help\n",
      "   - Expected LB: ~0.094 (on the line)\n",
      "   - If on line: confirms label smoothing doesn't help\n",
      "   - Uses 1 of 3 remaining submissions\n",
      "\n",
      "2. Try DIRECT CALIBRATION\n",
      "   - Apply a calibration factor to predictions\n",
      "   - calibration_factor = target / expected_lb = 0.0347 / 0.0877 = 0.396\n",
      "   - This is a heuristic but might help\n",
      "\n",
      "3. Try UNCERTAINTY-WEIGHTED PREDICTIONS\n",
      "   - Train multiple models with different seeds\n",
      "   - Weight predictions by inverse variance\n",
      "   - More confident = higher weight\n",
      "\n",
      "4. Try CONSERVATIVE PREDICTIONS for dissimilar solvents\n",
      "   - Compute Tanimoto similarity to training solvents\n",
      "   - If low similarity, blend toward training mean\n",
      "\n",
      "5. REPLICATE ens-model kernel exactly\n",
      "   - The public kernel achieves good LB\n",
      "   - Uses CatBoost + XGBoost with specific weights\n",
      "   - Single: 7:6, Full: 1:2\n",
      "\n",
      "=== CRITICAL DECISION ===\n",
      "With only 3 submissions remaining, we need to be strategic.\n",
      "The target (0.0347) is 60% below our best LB (0.0877).\n",
      "This is a HUGE gap that requires a FUNDAMENTAL change, not optimization.\n"
     ]
    }
   ],
   "source": [
    "# What's left to try?\n",
    "print(\"=== Remaining Options ===\")\n",
    "print(\"\\n1. SUBMIT exp_112 to confirm pseudo-labeling doesn't help\")\n",
    "print(\"   - Expected LB: ~0.094 (on the line)\")\n",
    "print(\"   - If on line: confirms label smoothing doesn't help\")\n",
    "print(\"   - Uses 1 of 3 remaining submissions\")\n",
    "\n",
    "print(\"\\n2. Try DIRECT CALIBRATION\")\n",
    "print(\"   - Apply a calibration factor to predictions\")\n",
    "print(\"   - calibration_factor = target / expected_lb = 0.0347 / 0.0877 = 0.396\")\n",
    "print(\"   - This is a heuristic but might help\")\n",
    "\n",
    "print(\"\\n3. Try UNCERTAINTY-WEIGHTED PREDICTIONS\")\n",
    "print(\"   - Train multiple models with different seeds\")\n",
    "print(\"   - Weight predictions by inverse variance\")\n",
    "print(\"   - More confident = higher weight\")\n",
    "\n",
    "print(\"\\n4. Try CONSERVATIVE PREDICTIONS for dissimilar solvents\")\n",
    "print(\"   - Compute Tanimoto similarity to training solvents\")\n",
    "print(\"   - If low similarity, blend toward training mean\")\n",
    "\n",
    "print(\"\\n5. REPLICATE ens-model kernel exactly\")\n",
    "print(\"   - The public kernel achieves good LB\")\n",
    "print(\"   - Uses CatBoost + XGBoost with specific weights\")\n",
    "print(\"   - Single: 7:6, Full: 1:2\")\n",
    "\n",
    "print(\"\\n=== CRITICAL DECISION ===\")\n",
    "print(\"With only 3 submissions remaining, we need to be strategic.\")\n",
    "print(\"The target (0.0347) is 60% below our best LB (0.0877).\")\n",
    "print(\"This is a HUGE gap that requires a FUNDAMENTAL change, not optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e212fa8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:23:00.033330Z",
     "iopub.status.busy": "2026-01-16T20:23:00.033223Z",
     "iopub.status.idle": "2026-01-16T20:23:00.036557Z",
     "shell.execute_reply": "2026-01-16T20:23:00.036200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RECOMMENDATION ===\n",
      "\n",
      "1. DO NOT submit exp_112 (pseudo-labeling)\n",
      "   - It's likely on the line (CV=0.0096 is WORSE than best CV=0.0081)\n",
      "   - Would waste a submission\n",
      "\n",
      "2. INSTEAD, try a FUNDAMENTALLY DIFFERENT approach:\n",
      "   a) Domain-adversarial training\n",
      "   b) Conformal prediction for uncertainty\n",
      "   c) Physics-informed constraints (Arrhenius, mass balance)\n",
      "   d) Scaffold-based splitting for better CV-LB alignment\n",
      "\n",
      "3. The benchmark paper achieved MSE 0.0039\n",
      "   - They used GNN with attention mechanisms\n",
      "   - Our GNN attempts had CV=0.026 (3x worse)\n",
      "   - But they might have had model class mismatches!\n",
      "\n",
      "4. CHECK: Did GNN/ChemBERTa experiments have correct submission cells?\n",
      "   - If submission cells used different model class, LB would be wrong\n",
      "   - This could explain why GNN/ChemBERTa didn't help\n",
      "\n",
      "=== IMMEDIATE ACTION ===\n",
      "1. Verify GNN/ChemBERTa submission cell model classes\n",
      "2. If they were wrong, FIX and re-run\n",
      "3. If they were correct, try domain-adversarial training\n",
      "4. Save submissions for approaches that CHANGE the CV-LB relationship\n"
     ]
    }
   ],
   "source": [
    "# Final recommendation\n",
    "print(\"=== FINAL RECOMMENDATION ===\")\n",
    "print(\"\\n1. DO NOT submit exp_112 (pseudo-labeling)\")\n",
    "print(\"   - It's likely on the line (CV=0.0096 is WORSE than best CV=0.0081)\")\n",
    "print(\"   - Would waste a submission\")\n",
    "\n",
    "print(\"\\n2. INSTEAD, try a FUNDAMENTALLY DIFFERENT approach:\")\n",
    "print(\"   a) Domain-adversarial training\")\n",
    "print(\"   b) Conformal prediction for uncertainty\")\n",
    "print(\"   c) Physics-informed constraints (Arrhenius, mass balance)\")\n",
    "print(\"   d) Scaffold-based splitting for better CV-LB alignment\")\n",
    "\n",
    "print(\"\\n3. The benchmark paper achieved MSE 0.0039\")\n",
    "print(\"   - They used GNN with attention mechanisms\")\n",
    "print(\"   - Our GNN attempts had CV=0.026 (3x worse)\")\n",
    "print(\"   - But they might have had model class mismatches!\")\n",
    "\n",
    "print(\"\\n4. CHECK: Did GNN/ChemBERTa experiments have correct submission cells?\")\n",
    "print(\"   - If submission cells used different model class, LB would be wrong\")\n",
    "print(\"   - This could explain why GNN/ChemBERTa didn't help\")\n",
    "\n",
    "print(\"\\n=== IMMEDIATE ACTION ===\")\n",
    "print(\"1. Verify GNN/ChemBERTa submission cell model classes\")\n",
    "print(\"2. If they were wrong, FIX and re-run\")\n",
    "print(\"3. If they were correct, try domain-adversarial training\")\n",
    "print(\"4. Save submissions for approaches that CHANGE the CV-LB relationship\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
