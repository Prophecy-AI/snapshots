{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7105e10d",
   "metadata": {},
   "source": [
    "# Loop 114 Analysis: CV-LB Relationship and Strategy Assessment\n",
    "\n",
    "## Goal\n",
    "Analyze the CV-LB relationship across all 24 submissions and determine the best path forward with only 3 submissions remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# All submissions with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},  # Best LB\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "    {'exp': 'exp_073', 'cv': 0.0084, 'lb': 0.1451},  # OUTLIER - model mismatch?\n",
    "    {'exp': 'exp_111', 'cv': 0.0129, 'lb': 0.1063},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(f\"Total submissions with LB: {len(df)}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001aef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude outlier exp_073 (likely model class mismatch)\n",
    "df_valid = df[df['exp'] != 'exp_073'].copy()\n",
    "print(f\"Valid submissions (excluding exp_073): {len(df_valid)}\")\n",
    "\n",
    "# Fit linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_valid['cv'], df_valid['lb'])\n",
    "\n",
    "print(f\"\\n=== CV-LB Relationship ===\")\n",
    "print(f\"Linear fit: LB = {slope:.4f} × CV + {intercept:.4f}\")\n",
    "print(f\"R² = {r_value**2:.4f}\")\n",
    "print(f\"Standard error: {std_err:.4f}\")\n",
    "\n",
    "# Target analysis\n",
    "target = 0.0347\n",
    "print(f\"\\n=== Target Analysis ===\")\n",
    "print(f\"Target LB: {target}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Intercept > Target? {intercept > target}\")\n",
    "\n",
    "if intercept < target:\n",
    "    required_cv = (target - intercept) / slope\n",
    "    print(f\"Required CV to hit target: {required_cv:.6f}\")\n",
    "else:\n",
    "    print(f\"CRITICAL: Intercept ({intercept:.4f}) > Target ({target})!\")\n",
    "    print(f\"Target is MATHEMATICALLY UNREACHABLE with approaches on this line!\")\n",
    "    print(f\"Required CV would be: {(target - intercept) / slope:.6f} (NEGATIVE = IMPOSSIBLE)\")\n",
    "\n",
    "# Best achieved\n",
    "best_cv = df_valid['cv'].min()\n",
    "best_lb = df_valid['lb'].min()\n",
    "print(f\"\\n=== Best Achieved ===\")\n",
    "print(f\"Best CV: {best_cv:.4f}\")\n",
    "print(f\"Best LB: {best_lb:.4f}\")\n",
    "print(f\"Gap to target: {best_lb - target:.4f} ({(best_lb - target) / target * 100:.1f}%)\")\n",
    "\n",
    "# Expected LB for best CV\n",
    "expected_lb = slope * best_cv + intercept\n",
    "print(f\"\\nExpected LB for best CV ({best_cv:.4f}): {expected_lb:.4f}\")\n",
    "print(f\"Actual best LB: {best_lb:.4f}\")\n",
    "print(f\"Difference: {best_lb - expected_lb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc574eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze exp_112 (pseudo-labeling)\n",
    "exp_112_cv = 0.009566\n",
    "exp_112_expected_lb = slope * exp_112_cv + intercept\n",
    "print(f\"=== exp_112 (Pseudo-Labeling) Analysis ===\")\n",
    "print(f\"CV: {exp_112_cv:.6f}\")\n",
    "print(f\"Expected LB from line: {exp_112_expected_lb:.4f}\")\n",
    "print(f\"\\nIf LB ≈ {exp_112_expected_lb:.4f}: Pseudo-labeling is ON THE LINE (no improvement)\")\n",
    "print(f\"If LB < {exp_112_expected_lb - 0.005:.4f}: Pseudo-labeling CHANGED the relationship (promising!)\")\n",
    "print(f\"If LB > {exp_112_expected_lb + 0.005:.4f}: Something is WRONG\")\n",
    "\n",
    "# What would we need?\n",
    "print(f\"\\n=== What We Need ===\")\n",
    "print(f\"Target LB: {target}\")\n",
    "print(f\"Current best LB: {best_lb:.4f}\")\n",
    "print(f\"Improvement needed: {best_lb - target:.4f} ({(best_lb - target) / best_lb * 100:.1f}%)\")\n",
    "print(f\"\\nThis is a MASSIVE improvement - unlikely with incremental changes.\")\n",
    "print(f\"We need to BREAK THE CV-LB LINE, not just improve CV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00541daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "print(\"=== Approaches Tried (from session_state) ===\")\n",
    "approaches = [\n",
    "    \"MLP with Arrhenius kinetics\",\n",
    "    \"LightGBM\",\n",
    "    \"DRFP + PCA\",\n",
    "    \"Combined Spange + DRFP\",\n",
    "    \"Deep Residual MLP (FAILED)\",\n",
    "    \"Large Ensemble (15 models)\",\n",
    "    \"Simpler Model [64, 32]\",\n",
    "    \"CatBoost + XGBoost ensemble\",\n",
    "    \"GNN (CV=0.026 - 3x worse)\",\n",
    "    \"ChemBERTa (CV=0.028 - 3.5x worse)\",\n",
    "    \"Chemical Similarity blending (exp_111)\",\n",
    "    \"Pseudo-labeling (exp_112)\",\n",
    "]\n",
    "\n",
    "for i, approach in enumerate(approaches, 1):\n",
    "    print(f\"{i}. {approach}\")\n",
    "\n",
    "print(f\"\\n=== Key Insight ===\")\n",
    "print(\"ALL tabular approaches (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line.\")\n",
    "print(\"GNN and ChemBERTa have WORSE CV (3x worse) - not promising.\")\n",
    "print(\"Chemical similarity blending (exp_111) was ON THE LINE.\")\n",
    "print(\"Pseudo-labeling (exp_112) is likely ON THE LINE too.\")\n",
    "print(\"\\nThe problem is STRUCTURAL - test solvents are fundamentally different from training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's left to try?\n",
    "print(\"=== Remaining Options ===\")\n",
    "print(\"\\n1. SUBMIT exp_112 to confirm pseudo-labeling doesn't help\")\n",
    "print(\"   - Expected LB: ~0.094 (on the line)\")\n",
    "print(\"   - If on line: confirms label smoothing doesn't help\")\n",
    "print(\"   - Uses 1 of 3 remaining submissions\")\n",
    "\n",
    "print(\"\\n2. Try DIRECT CALIBRATION\")\n",
    "print(\"   - Apply a calibration factor to predictions\")\n",
    "print(\"   - calibration_factor = target / expected_lb = 0.0347 / 0.0877 = 0.396\")\n",
    "print(\"   - This is a heuristic but might help\")\n",
    "\n",
    "print(\"\\n3. Try UNCERTAINTY-WEIGHTED PREDICTIONS\")\n",
    "print(\"   - Train multiple models with different seeds\")\n",
    "print(\"   - Weight predictions by inverse variance\")\n",
    "print(\"   - More confident = higher weight\")\n",
    "\n",
    "print(\"\\n4. Try CONSERVATIVE PREDICTIONS for dissimilar solvents\")\n",
    "print(\"   - Compute Tanimoto similarity to training solvents\")\n",
    "print(\"   - If low similarity, blend toward training mean\")\n",
    "\n",
    "print(\"\\n5. REPLICATE ens-model kernel exactly\")\n",
    "print(\"   - The public kernel achieves good LB\")\n",
    "print(\"   - Uses CatBoost + XGBoost with specific weights\")\n",
    "print(\"   - Single: 7:6, Full: 1:2\")\n",
    "\n",
    "print(\"\\n=== CRITICAL DECISION ===\")\n",
    "print(\"With only 3 submissions remaining, we need to be strategic.\")\n",
    "print(\"The target (0.0347) is 60% below our best LB (0.0877).\")\n",
    "print(\"This is a HUGE gap that requires a FUNDAMENTAL change, not optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e212fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print(\"=== FINAL RECOMMENDATION ===\")\n",
    "print(\"\\n1. DO NOT submit exp_112 (pseudo-labeling)\")\n",
    "print(\"   - It's likely on the line (CV=0.0096 is WORSE than best CV=0.0081)\")\n",
    "print(\"   - Would waste a submission\")\n",
    "\n",
    "print(\"\\n2. INSTEAD, try a FUNDAMENTALLY DIFFERENT approach:\")\n",
    "print(\"   a) Domain-adversarial training\")\n",
    "print(\"   b) Conformal prediction for uncertainty\")\n",
    "print(\"   c) Physics-informed constraints (Arrhenius, mass balance)\")\n",
    "print(\"   d) Scaffold-based splitting for better CV-LB alignment\")\n",
    "\n",
    "print(\"\\n3. The benchmark paper achieved MSE 0.0039\")\n",
    "print(\"   - They used GNN with attention mechanisms\")\n",
    "print(\"   - Our GNN attempts had CV=0.026 (3x worse)\")\n",
    "print(\"   - But they might have had model class mismatches!\")\n",
    "\n",
    "print(\"\\n4. CHECK: Did GNN/ChemBERTa experiments have correct submission cells?\")\n",
    "print(\"   - If submission cells used different model class, LB would be wrong\")\n",
    "print(\"   - This could explain why GNN/ChemBERTa didn't help\")\n",
    "\n",
    "print(\"\\n=== IMMEDIATE ACTION ===\")\n",
    "print(\"1. Verify GNN/ChemBERTa submission cell model classes\")\n",
    "print(\"2. If they were wrong, FIX and re-run\")\n",
    "print(\"3. If they were correct, try domain-adversarial training\")\n",
    "print(\"4. Save submissions for approaches that CHANGE the CV-LB relationship\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
