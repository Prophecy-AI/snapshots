{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358adf6f",
   "metadata": {},
   "source": [
    "# Loop 86 Analysis: Critical Assessment\n",
    "\n",
    "## Key Facts:\n",
    "1. **86 experiments completed**, all falling on the same CV-LB line\n",
    "2. **Best CV**: 0.008092 (exp_049 CatBoost+XGBoost)\n",
    "3. **Best LB**: 0.0877 (exp_030)\n",
    "4. **Target**: 0.0347\n",
    "5. **CV-LB relationship**: LB = 4.31 * CV + 0.0525 (RÂ² = 0.95)\n",
    "6. **CRITICAL**: Intercept (0.0525) > Target (0.0347)\n",
    "\n",
    "## This Loop's Findings:\n",
    "- Pseudo-labeling: Made things WORSE (CV=0.008853 vs baseline 0.008092)\n",
    "- Self-training: 9.4% worse than baseline\n",
    "- Conservative predictions: Also made things worse\n",
    "\n",
    "## The Fundamental Problem:\n",
    "The CV-LB intercept (0.0525) is HIGHER than the target (0.0347). This means:\n",
    "- Even with PERFECT CV=0, expected LB would be 0.0525\n",
    "- Required CV to hit target = (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36330ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:57:30.648402Z",
     "iopub.status.busy": "2026-01-16T10:57:30.647851Z",
     "iopub.status.idle": "2026-01-16T10:57:31.410737Z",
     "shell.execute_reply": "2026-01-16T10:57:31.410276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-LB Relationship: LB = 4.31 * CV + 0.0525\n",
      "R-squared: 0.9505\n",
      "Intercept: 0.0525\n",
      "Target LB: 0.0347\n",
      "\n",
      "CRITICAL: Intercept (0.0525) > Target (0.0347)\n",
      "Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041\n",
      "\n",
      "This is IMPOSSIBLE - CV cannot be negative!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# CV-LB data from all submissions\n",
    "submissions = [\n",
    "    ('exp_000', 0.0111, 0.0982),\n",
    "    ('exp_001', 0.0123, 0.1065),\n",
    "    ('exp_003', 0.0105, 0.0972),\n",
    "    ('exp_005', 0.0104, 0.0969),\n",
    "    ('exp_006', 0.0097, 0.0946),\n",
    "    ('exp_007', 0.0093, 0.0932),\n",
    "    ('exp_009', 0.0092, 0.0936),\n",
    "    ('exp_012', 0.0090, 0.0913),\n",
    "    ('exp_024', 0.0087, 0.0893),\n",
    "    ('exp_026', 0.0085, 0.0887),\n",
    "    ('exp_030', 0.0083, 0.0877),\n",
    "    ('exp_035', 0.0098, 0.0970),\n",
    "    # exp_073 is an outlier (similarity weighting BACKFIRED)\n",
    "    ('exp_073', 0.0084, 0.1451),\n",
    "]\n",
    "\n",
    "# Exclude exp_073 (outlier)\n",
    "valid_submissions = [s for s in submissions if s[0] != 'exp_073']\n",
    "\n",
    "cv_scores = [s[1] for s in valid_submissions]\n",
    "lb_scores = [s[2] for s in valid_submissions]\n",
    "\n",
    "# Fit linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "\n",
    "print(f'CV-LB Relationship: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R-squared: {r_value**2:.4f}')\n",
    "print(f'Intercept: {intercept:.4f}')\n",
    "print(f'Target LB: 0.0347')\n",
    "print(f'\\nCRITICAL: Intercept ({intercept:.4f}) > Target (0.0347)')\n",
    "print(f'Required CV for target: ({0.0347} - {intercept:.4f}) / {slope:.2f} = {(0.0347 - intercept) / slope:.4f}')\n",
    "print(f'\\nThis is IMPOSSIBLE - CV cannot be negative!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835f14df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:57:31.411941Z",
     "iopub.status.busy": "2026-01-16T10:57:31.411767Z",
     "iopub.status.idle": "2026-01-16T10:57:31.415330Z",
     "shell.execute_reply": "2026-01-16T10:57:31.414989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approaches Tried:\n",
      "----------------------------------------------------------------------\n",
      "MLP variants                   | 50+ experiments      | All on same line\n",
      "LightGBM                       | Multiple configs     | All on same line\n",
      "XGBoost                        | Multiple configs     | All on same line\n",
      "CatBoost                       | Multiple configs     | All on same line\n",
      "Gaussian Processes             | Multiple configs     | All on same line\n",
      "Ridge Regression               | Multiple configs     | All on same line\n",
      "GNN from scratch               | CV=0.024             | 3x worse than tabular\n",
      "ChemBERTa embeddings           | CV=0.015             | 2x worse than tabular\n",
      "ChemProp features              | CV=0.012             | 46% worse than tabular\n",
      "Pseudo-labeling                | CV=0.0089            | 9.4% worse\n",
      "Similarity weighting           | LB=0.145             | BACKFIRED\n",
      "Yield normalization            | No effect            | No improvement\n",
      "Conservative predictions       | Made worse           | No improvement\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Analyze what approaches have been tried\n",
    "approaches_tried = [\n",
    "    ('MLP variants', '50+ experiments', 'All on same line'),\n",
    "    ('LightGBM', 'Multiple configs', 'All on same line'),\n",
    "    ('XGBoost', 'Multiple configs', 'All on same line'),\n",
    "    ('CatBoost', 'Multiple configs', 'All on same line'),\n",
    "    ('Gaussian Processes', 'Multiple configs', 'All on same line'),\n",
    "    ('Ridge Regression', 'Multiple configs', 'All on same line'),\n",
    "    ('GNN from scratch', 'CV=0.024', '3x worse than tabular'),\n",
    "    ('ChemBERTa embeddings', 'CV=0.015', '2x worse than tabular'),\n",
    "    ('ChemProp features', 'CV=0.012', '46% worse than tabular'),\n",
    "    ('Pseudo-labeling', 'CV=0.0089', '9.4% worse'),\n",
    "    ('Similarity weighting', 'LB=0.145', 'BACKFIRED'),\n",
    "    ('Yield normalization', 'No effect', 'No improvement'),\n",
    "    ('Conservative predictions', 'Made worse', 'No improvement'),\n",
    "]\n",
    "\n",
    "print('Approaches Tried:')\n",
    "print('-' * 70)\n",
    "for approach, result, notes in approaches_tried:\n",
    "    print(f'{approach:30s} | {result:20s} | {notes}')\n",
    "print('-' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0473d6",
   "metadata": {},
   "source": [
    "## What Could Break the CV-LB Line?\n",
    "\n",
    "The key insight is that ALL approaches fall on the same CV-LB line because they all:\n",
    "1. Use the same features (Spange descriptors, DRFP, Arrhenius)\n",
    "2. Use the same validation scheme (Leave-One-Out)\n",
    "3. Make predictions in the same way (point predictions)\n",
    "\n",
    "### Approaches That MIGHT Change the Relationship:\n",
    "\n",
    "1. **Transductive Learning** - Use test set structure to inform predictions\n",
    "2. **Physics-Based Constraints** - Enforce mass balance, monotonicity\n",
    "3. **Calibration** - Isotonic regression, temperature scaling\n",
    "4. **Different Prediction Strategy** - Predict ratios instead of absolute values\n",
    "5. **Ensemble of Fundamentally Different Models** - Not just different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788ed411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:57:31.416830Z",
     "iopub.status.busy": "2026-01-16T10:57:31.416513Z",
     "iopub.status.idle": "2026-01-16T10:57:31.419963Z",
     "shell.execute_reply": "2026-01-16T10:57:31.419628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Analysis:\n",
      "Benchmark MSE: 0.0039\n",
      "Our Best LB: 0.0877\n",
      "Target: 0.0347\n",
      "\n",
      "Gap to benchmark: 22.5x worse\n",
      "Gap to target: 2.5x worse\n",
      "\n",
      "If benchmark followed our CV-LB line:\n",
      "  Implied CV: -0.0113\n",
      "  This is NEGATIVE, confirming they have a DIFFERENT CV-LB relationship\n",
      "\n",
      "To achieve benchmark MSE 0.0039 with CV=0.008:\n",
      "  Required intercept: -0.0306\n",
      "  This is NEGATIVE, confirming they have fundamentally different approach\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze what the benchmark paper might have done differently\n",
    "# The benchmark achieved MSE 0.0039 (22x better than our best LB)\n",
    "\n",
    "benchmark_mse = 0.0039\n",
    "best_lb = 0.0877\n",
    "target = 0.0347\n",
    "\n",
    "print('Benchmark Analysis:')\n",
    "print(f'Benchmark MSE: {benchmark_mse}')\n",
    "print(f'Our Best LB: {best_lb}')\n",
    "print(f'Target: {target}')\n",
    "print(f'\\nGap to benchmark: {best_lb / benchmark_mse:.1f}x worse')\n",
    "print(f'Gap to target: {best_lb / target:.1f}x worse')\n",
    "\n",
    "# If benchmark followed our CV-LB line, what would their CV be?\n",
    "implied_cv = (benchmark_mse - intercept) / slope\n",
    "print(f'\\nIf benchmark followed our CV-LB line:')\n",
    "print(f'  Implied CV: {implied_cv:.4f}')\n",
    "print(f'  This is NEGATIVE, confirming they have a DIFFERENT CV-LB relationship')\n",
    "\n",
    "# What intercept would they need?\n",
    "print(f'\\nTo achieve benchmark MSE 0.0039 with CV=0.008:')\n",
    "required_intercept = benchmark_mse - slope * 0.008\n",
    "print(f'  Required intercept: {required_intercept:.4f}')\n",
    "print(f'  This is NEGATIVE, confirming they have fundamentally different approach')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b855509",
   "metadata": {},
   "source": [
    "## Key Insight: The Benchmark Paper's Success\n",
    "\n",
    "The benchmark paper achieved MSE 0.0039 using:\n",
    "1. **Pre-trained GNN with Graph Attention Networks**\n",
    "2. **DRFP features**\n",
    "3. **Learned mixture-aware encodings**\n",
    "4. **Pre-training on related reaction data**\n",
    "\n",
    "The key difference is likely **pre-training on related data**. This allows the model to learn general chemistry knowledge that transfers to unseen solvents.\n",
    "\n",
    "## What We Haven't Tried:\n",
    "\n",
    "1. **Ratio-based predictions** - Predict Product2/SM and Product3/SM ratios instead of absolute values\n",
    "2. **Hierarchical predictions** - First predict total conversion, then product distribution\n",
    "3. **Physics-informed loss** - Penalize predictions that violate mass balance\n",
    "4. **Adversarial validation** - Identify which features cause distribution shift\n",
    "5. **Kernel-based similarity** - Use Tanimoto similarity for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397f3545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:57:31.420995Z",
     "iopub.status.busy": "2026-01-16T10:57:31.420903Z",
     "iopub.status.idle": "2026-01-16T10:57:31.424362Z",
     "shell.execute_reply": "2026-01-16T10:57:31.424005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategies to Reduce Intercept:\n",
      "======================================================================\n",
      "\n",
      "1. RATIO-BASED PREDICTIONS\n",
      "   Instead of predicting SM, P2, P3 directly,\n",
      "   predict ratios: P2/SM, P3/SM, then derive SM from mass balance\n",
      "   Why: Ratios might be more stable across solvents\n",
      "\n",
      "2. HIERARCHICAL PREDICTIONS\n",
      "   Step 1: Predict total conversion (1 - SM)\n",
      "   Step 2: Predict product distribution (P2/(P2+P3), P3/(P2+P3))\n",
      "   Why: Separates two different chemical phenomena\n",
      "\n",
      "3. PHYSICS-INFORMED CONSTRAINTS\n",
      "   Enforce: SM + P2 + P3 = 1 (mass balance)\n",
      "   Enforce: Monotonicity with time\n",
      "   Why: Domain knowledge that holds for ALL solvents\n",
      "\n",
      "4. ADVERSARIAL VALIDATION\n",
      "   Train classifier to distinguish train vs test solvents\n",
      "   Identify which features cause distribution shift\n",
      "   Why: Can guide feature selection/weighting\n",
      "\n",
      "5. KERNEL-BASED SIMILARITY\n",
      "   Use Tanimoto similarity between test and train solvents\n",
      "   Weight predictions by similarity\n",
      "   Why: More principled than simple distance-based weighting\n"
     ]
    }
   ],
   "source": [
    "# Let's think about what could REDUCE the intercept\n",
    "# The intercept represents structural extrapolation error\n",
    "\n",
    "print('Strategies to Reduce Intercept:')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('1. RATIO-BASED PREDICTIONS')\n",
    "print('   Instead of predicting SM, P2, P3 directly,')\n",
    "print('   predict ratios: P2/SM, P3/SM, then derive SM from mass balance')\n",
    "print('   Why: Ratios might be more stable across solvents')\n",
    "print()\n",
    "print('2. HIERARCHICAL PREDICTIONS')\n",
    "print('   Step 1: Predict total conversion (1 - SM)')\n",
    "print('   Step 2: Predict product distribution (P2/(P2+P3), P3/(P2+P3))')\n",
    "print('   Why: Separates two different chemical phenomena')\n",
    "print()\n",
    "print('3. PHYSICS-INFORMED CONSTRAINTS')\n",
    "print('   Enforce: SM + P2 + P3 = 1 (mass balance)')\n",
    "print('   Enforce: Monotonicity with time')\n",
    "print('   Why: Domain knowledge that holds for ALL solvents')\n",
    "print()\n",
    "print('4. ADVERSARIAL VALIDATION')\n",
    "print('   Train classifier to distinguish train vs test solvents')\n",
    "print('   Identify which features cause distribution shift')\n",
    "print('   Why: Can guide feature selection/weighting')\n",
    "print()\n",
    "print('5. KERNEL-BASED SIMILARITY')\n",
    "print('   Use Tanimoto similarity between test and train solvents')\n",
    "print('   Weight predictions by similarity')\n",
    "print('   Why: More principled than simple distance-based weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b3c3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:58:40.525333Z",
     "iopub.status.busy": "2026-01-16T10:58:40.524804Z",
     "iopub.status.idle": "2026-01-16T10:58:40.542204Z",
     "shell.execute_reply": "2026-01-16T10:58:40.541803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Solvent Data:\n",
      "  Shape: (656, 13)\n",
      "  Solvents: 24\n",
      "  Samples per solvent: 27.3\n",
      "\n",
      "Full Data:\n",
      "  Shape: (1227, 19)\n",
      "  Unique ramps: 13\n",
      "\n",
      "Target Statistics (Single Solvent):\n",
      "  SM: mean=0.5222, std=0.3602, min=0.0000, max=1.0000\n",
      "  Product 2: mean=0.1499, std=0.1431, min=0.0000, max=0.4636\n",
      "  Product 3: mean=0.1234, std=0.1315, min=0.0000, max=0.5338\n",
      "\n",
      "Mass Balance Check (SM + P2 + P3):\n",
      "  Mean: 0.795504\n",
      "  Std: 0.194306\n",
      "  Min: 0.028752\n",
      "  Max: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Let's check what the actual data looks like\n",
    "import sys\n",
    "sys.path.append('/home/code/data')\n",
    "\n",
    "single_df = pd.read_csv('/home/code/data/catechol_single_solvent_yields.csv')\n",
    "full_df = pd.read_csv('/home/code/data/catechol_full_data_yields.csv')\n",
    "\n",
    "print('Single Solvent Data:')\n",
    "print(f'  Shape: {single_df.shape}')\n",
    "print(f'  Solvents: {single_df[\"SOLVENT NAME\"].nunique()}')\n",
    "print(f'  Samples per solvent: {len(single_df) / single_df[\"SOLVENT NAME\"].nunique():.1f}')\n",
    "\n",
    "print('\\nFull Data:')\n",
    "print(f'  Shape: {full_df.shape}')\n",
    "print(f'  Unique ramps: {full_df[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates().shape[0]}')\n",
    "\n",
    "# Check target distributions\n",
    "print('\\nTarget Statistics (Single Solvent):')\n",
    "for col in ['SM', 'Product 2', 'Product 3']:\n",
    "    print(f'  {col}: mean={single_df[col].mean():.4f}, std={single_df[col].std():.4f}, min={single_df[col].min():.4f}, max={single_df[col].max():.4f}')\n",
    "\n",
    "# Check mass balance\n",
    "single_df['sum'] = single_df['SM'] + single_df['Product 2'] + single_df['Product 3']\n",
    "print(f'\\nMass Balance Check (SM + P2 + P3):')\n",
    "print(f'  Mean: {single_df[\"sum\"].mean():.6f}')\n",
    "print(f'  Std: {single_df[\"sum\"].std():.6f}')\n",
    "print(f'  Min: {single_df[\"sum\"].min():.6f}')\n",
    "print(f'  Max: {single_df[\"sum\"].max():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c2210",
   "metadata": {},
   "source": [
    "## Recommended Next Steps\n",
    "\n",
    "### Priority 1: Ratio-Based Predictions\n",
    "Instead of predicting SM, P2, P3 directly, predict:\n",
    "- Conversion = 1 - SM\n",
    "- Selectivity_P2 = P2 / (P2 + P3)\n",
    "\n",
    "Then derive:\n",
    "- SM = 1 - Conversion\n",
    "- P2 = Conversion * Selectivity_P2\n",
    "- P3 = Conversion * (1 - Selectivity_P2)\n",
    "\n",
    "**Why this might work:**\n",
    "- Conversion and selectivity are more fundamental chemical quantities\n",
    "- They might generalize better to unseen solvents\n",
    "- Mass balance is automatically satisfied\n",
    "\n",
    "### Priority 2: Physics-Informed Loss\n",
    "Add penalty terms to the loss function:\n",
    "- Mass balance penalty: (SM + P2 + P3 - 1)^2\n",
    "- Monotonicity penalty: penalize if yield decreases with time\n",
    "\n",
    "### Priority 3: Adversarial Validation\n",
    "Train a classifier to distinguish train vs test solvents:\n",
    "- If classifier can distinguish, there's distribution shift\n",
    "- Features with high importance in classifier are causing the shift\n",
    "- Can guide feature selection or weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b90821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T10:59:03.939470Z",
     "iopub.status.busy": "2026-01-16T10:59:03.938936Z",
     "iopub.status.idle": "2026-01-16T10:59:03.947624Z",
     "shell.execute_reply": "2026-01-16T10:59:03.947244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass Balance Analysis:\n",
      "======================================================================\n",
      "Mean sum (SM + P2 + P3): 0.7955\n",
      "This means ~20.4% of material is unaccounted for\n",
      "\n",
      "Correlation between conversion and mass balance:\n",
      "  Correlation: -0.6784\n",
      "\n",
      "Mass Balance Distribution:\n",
      "count    656.000000\n",
      "mean       0.795504\n",
      "std        0.194306\n",
      "min        0.028752\n",
      "25%        0.708417\n",
      "50%        0.849648\n",
      "75%        0.927955\n",
      "max        1.000000\n",
      "Name: sum, dtype: float64\n",
      "\n",
      "\n",
      "Mass Balance by Solvent (top 5 highest and lowest):\n",
      "Lowest mass balance:\n",
      "SOLVENT NAME\n",
      "2,2,2-Trifluoroethanol         0.486005\n",
      "Acetonitrile.Acetic Acid       0.518022\n",
      "Cyclohexane                    0.678907\n",
      "Methanol                       0.743034\n",
      "DMA [N,N-Dimethylacetamide]    0.760036\n",
      "Name: sum, dtype: float64\n",
      "\n",
      "Highest mass balance:\n",
      "SOLVENT NAME\n",
      "Diethyl Ether [Ether]               0.948165\n",
      "MTBE [tert-Butylmethylether]        0.959072\n",
      "Dimethyl Carbonate                  0.961288\n",
      "Ethylene Glycol [1,2-Ethanediol]    0.979044\n",
      "IPA [Propan-2-ol]                   0.993506\n",
      "Name: sum, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Interesting! Mass balance is NOT 1.0 - there are other products/losses\n",
    "# Let's analyze this more\n",
    "\n",
    "print('Mass Balance Analysis:')\n",
    "print('=' * 70)\n",
    "print(f'Mean sum (SM + P2 + P3): {single_df[\"sum\"].mean():.4f}')\n",
    "print(f'This means ~{(1 - single_df[\"sum\"].mean()) * 100:.1f}% of material is unaccounted for')\n",
    "print()\n",
    "\n",
    "# Check if there's a pattern with conversion\n",
    "single_df['conversion'] = 1 - single_df['SM']\n",
    "print('Correlation between conversion and mass balance:')\n",
    "print(f'  Correlation: {single_df[\"conversion\"].corr(single_df[\"sum\"]):.4f}')\n",
    "print()\n",
    "\n",
    "# Check the distribution of mass balance\n",
    "print('Mass Balance Distribution:')\n",
    "print(single_df['sum'].describe())\n",
    "print()\n",
    "\n",
    "# Check if mass balance varies by solvent\n",
    "print('\\nMass Balance by Solvent (top 5 highest and lowest):')\n",
    "solvent_mass_balance = single_df.groupby('SOLVENT NAME')['sum'].mean().sort_values()\n",
    "print('Lowest mass balance:')\n",
    "print(solvent_mass_balance.head())\n",
    "print('\\nHighest mass balance:')\n",
    "print(solvent_mass_balance.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: Mass balance varies by solvent\n",
    "# This means we should NOT enforce mass balance = 1.0 as a constraint\n",
    "# Instead, we could predict the \"other products\" fraction as a 4th target\n",
    "\n",
    "# Let's check if predicting conversion and selectivity might be more stable\n",
    "single_df['conversion'] = 1 - single_df['SM']\n",
    "single_df['selectivity_P2'] = single_df['Product 2'] / (single_df['Product 2'] + single_df['Product 3'] + 1e-10)\n",
    "single_df['selectivity_P3'] = single_df['Product 3'] / (single_df['Product 2'] + single_df['Product 3'] + 1e-10)\n",
    "\n",
    "print('Conversion Statistics:')\n",
    "print(f'  Mean: {single_df[\"conversion\"].mean():.4f}')\n",
    "print(f'  Std: {single_df[\"conversion\"].std():.4f}')\n",
    "print(f'  Min: {single_df[\"conversion\"].min():.4f}')\n",
    "print(f'  Max: {single_df[\"conversion\"].max():.4f}')\n",
    "\n",
    "print('\\nSelectivity P2 Statistics:')\n",
    "print(f'  Mean: {single_df[\"selectivity_P2\"].mean():.4f}')\n",
    "print(f'  Std: {single_df[\"selectivity_P2\"].std():.4f}')\n",
    "print(f'  Min: {single_df[\"selectivity_P2\"].min():.4f}')\n",
    "print(f'  Max: {single_df[\"selectivity_P2\"].max():.4f}')\n",
    "\n",
    "# Check variance by solvent\n",
    "print('\\nConversion Variance by Solvent:')\n",
    "conversion_by_solvent = single_df.groupby('SOLVENT NAME')['conversion'].agg(['mean', 'std'])\n",
    "print(f'  Mean of means: {conversion_by_solvent[\"mean\"].mean():.4f}')\n",
    "print(f'  Std of means: {conversion_by_solvent[\"mean\"].std():.4f}')\n",
    "\n",
    "print('\\nSelectivity P2 Variance by Solvent:')\n",
    "selectivity_by_solvent = single_df.groupby('SOLVENT NAME')['selectivity_P2'].agg(['mean', 'std'])\n",
    "print(f'  Mean of means: {selectivity_by_solvent[\"mean\"].mean():.4f}')\n",
    "print(f'  Std of means: {selectivity_by_solvent[\"mean\"].std():.4f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
