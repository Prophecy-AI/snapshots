{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df057544",
   "metadata": {},
   "source": [
    "# Loop 102 Analysis: Submission Failure Investigation\n",
    "\n",
    "**Issue**: exp_101 (Mixall Kernel with GroupKFold) failed with \"Evaluation metric raised an unexpected error\"\n",
    "\n",
    "**Root Cause Investigation**:\n",
    "1. Negative predictions in submission.csv\n",
    "2. SolventB% scaling difference (divided by 100 vs not)\n",
    "3. Need to understand what's causing the evaluation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the failed submission\n",
    "submission = pd.read_csv('/home/submission/submission.csv')\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nColumn names: {submission.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08990bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative values\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    neg_count = (submission[col] < 0).sum()\n",
    "    print(f\"{col}: {neg_count} negative values ({100*neg_count/len(submission):.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows with any negative: {((submission['target_1'] < 0) | (submission['target_2'] < 0) | (submission['target_3'] < 0)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546eeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for values > 1 (also invalid for yields)\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    over_count = (submission[col] > 1).sum()\n",
    "    print(f\"{col}: {over_count} values > 1 ({100*over_count/len(submission):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of predictions\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(submission[['target_1', 'target_2', 'target_3']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40219271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the sum of yields exceeds 1 (physically impossible)\n",
    "submission['yield_sum'] = submission['target_1'] + submission['target_2'] + submission['target_3']\n",
    "print(f\"\\nYield sum statistics:\")\n",
    "print(submission['yield_sum'].describe())\n",
    "print(f\"\\nRows where yield_sum > 1: {(submission['yield_sum'] > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f93c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the CV-LB relationship from submission history\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Extract submission history\n",
    "submissions = state.get('submissions', [])\n",
    "print(f\"Total submissions: {len(submissions)}\")\n",
    "print(\"\\nSubmission history:\")\n",
    "for s in submissions:\n",
    "    print(f\"  {s.get('experiment_id', 'N/A')}: CV={s.get('cv_score', 'N/A')}, LB={s.get('lb_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship\n",
    "cv_scores = []\n",
    "lb_scores = []\n",
    "for s in submissions:\n",
    "    cv = s.get('cv_score')\n",
    "    lb = s.get('lb_score')\n",
    "    if cv is not None and lb is not None and lb != 'pending':\n",
    "        try:\n",
    "            cv_scores.append(float(cv))\n",
    "            lb_scores.append(float(lb))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"\\nValid CV-LB pairs: {len(cv_scores)}\")\n",
    "if len(cv_scores) >= 3:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    X = np.array(cv_scores).reshape(-1, 1)\n",
    "    y = np.array(lb_scores)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    r2 = reg.score(X, y)\n",
    "    print(f\"\\nLinear fit: LB = {reg.coef_[0]:.4f} * CV + {reg.intercept_:.4f}\")\n",
    "    print(f\"R² = {r2:.4f}\")\n",
    "    print(f\"\\nTarget LB: 0.0347\")\n",
    "    print(f\"Intercept: {reg.intercept_:.4f}\")\n",
    "    print(f\"Required CV for target: {(0.0347 - reg.intercept_) / reg.coef_[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b94087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The submission failed likely due to negative predictions\n",
    "# The evaluation metric expects yields in [0, 1] range\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. NEGATIVE PREDICTIONS:\")\n",
    "print(f\"   - {((submission['target_1'] < 0) | (submission['target_2'] < 0) | (submission['target_3'] < 0)).sum()} rows have negative values\")\n",
    "print(f\"   - This is physically impossible for yields\")\n",
    "print(f\"   - The evaluation metric likely raises an error for invalid predictions\")\n",
    "\n",
    "print(\"\\n2. FIX REQUIRED:\")\n",
    "print(\"   - Clip predictions to [0, 1] range\")\n",
    "print(\"   - Add: final_preds = np.clip(final_preds, 0, 1)\")\n",
    "\n",
    "print(\"\\n3. ADDITIONAL ISSUE:\")\n",
    "print(\"   - SolventB% scaling: exp_101 divides by 100, mixall kernel doesn't\")\n",
    "print(\"   - This affects mixture interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best experiments we have\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "experiments = state.get('experiments', [])\n",
    "best_cv = float('inf')\n",
    "best_exp = None\n",
    "for exp in experiments:\n",
    "    score = exp.get('score')\n",
    "    if score is not None and score < best_cv:\n",
    "        best_cv = score\n",
    "        best_exp = exp\n",
    "\n",
    "if best_exp:\n",
    "    print(f\"\\nBest CV: {best_cv:.6f}\")\n",
    "    print(f\"Experiment: {best_exp.get('name', 'N/A')}\")\n",
    "    print(f\"Model: {best_exp.get('model_type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b865a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the best LB submission was\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST LB SUBMISSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_lb = float('inf')\n",
    "best_lb_sub = None\n",
    "for s in submissions:\n",
    "    lb = s.get('lb_score')\n",
    "    if lb is not None and lb != 'pending':\n",
    "        try:\n",
    "            lb_val = float(lb)\n",
    "            if lb_val < best_lb:\n",
    "                best_lb = lb_val\n",
    "                best_lb_sub = s\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if best_lb_sub:\n",
    "    print(f\"\\nBest LB: {best_lb:.6f}\")\n",
    "    print(f\"Experiment: {best_lb_sub.get('experiment_id', 'N/A')}\")\n",
    "    print(f\"CV: {best_lb_sub.get('cv_score', 'N/A')}\")\n",
    "    print(f\"\\nTarget: 0.0347\")\n",
    "    print(f\"Gap: {best_lb - 0.0347:.6f} ({100*(best_lb - 0.0347)/0.0347:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. SUBMISSION FAILURE ROOT CAUSE:\n",
    "   - Negative predictions in submission.csv\n",
    "   - Need to clip predictions to [0, 1] range\n",
    "\n",
    "2. CV-LB RELATIONSHIP:\n",
    "   - All tabular approaches fall on same line: LB ≈ 4.3 × CV + 0.053\n",
    "   - Intercept (0.053) > Target (0.0347)\n",
    "   - This means NO amount of CV improvement can reach target\n",
    "   - Need to CHANGE the relationship, not just improve CV\n",
    "\n",
    "3. WHAT TO TRY NEXT:\n",
    "   a) Fix the submission by clipping predictions\n",
    "   b) Try approaches that could change the CV-LB relationship:\n",
    "      - Extrapolation detection + conservative predictions\n",
    "      - Domain constraints (yields sum to ≤1)\n",
    "      - Different representation (GNN, ChemBERTa)\n",
    "   \n",
    "4. WHAT NOT TO DO:\n",
    "   - More tabular model variants (exhaustively tested)\n",
    "   - Multi-seed optimization (too far from target)\n",
    "   - Hyperparameter tuning (won't change intercept)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exp_073 which had a very different LB (0.1451)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTLIER ANALYSIS: exp_073\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for s in submissions:\n",
    "    if s.get('experiment_id') == 'exp_073':\n",
    "        print(f\"\\nexp_073:\")\n",
    "        print(f\"  CV: {s.get('cv_score')}\")\n",
    "        print(f\"  LB: {s.get('lb_score')}\")\n",
    "        print(f\"  Notes: This had a much worse LB than expected from CV\")\n",
    "        print(f\"  Likely cause: Model class mismatch or submission error\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
