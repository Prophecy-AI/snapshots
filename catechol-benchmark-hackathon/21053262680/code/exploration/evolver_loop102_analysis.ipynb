{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df057544",
   "metadata": {},
   "source": [
    "# Loop 102 Analysis: Submission Failure Investigation\n",
    "\n",
    "**Issue**: exp_101 (Mixall Kernel with GroupKFold) failed with \"Evaluation metric raised an unexpected error\"\n",
    "\n",
    "**Root Cause Investigation**:\n",
    "1. Negative predictions in submission.csv\n",
    "2. SolventB% scaling difference (divided by 100 vs not)\n",
    "3. Need to understand what's causing the evaluation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fbf5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:33:29.156582Z",
     "iopub.status.busy": "2026-01-16T16:33:29.156084Z",
     "iopub.status.idle": "2026-01-16T16:33:29.163740Z",
     "shell.execute_reply": "2026-01-16T16:33:29.163389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (1883, 8)\n",
      "\n",
      "Column names: ['id', 'index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']\n",
      "\n",
      "First few rows:\n",
      "   id  index  task  fold  row  target_1  target_2  target_3\n",
      "0   0      0     0     0    0  0.013813  0.008498  0.866816\n",
      "1   1      1     0     0    1  0.014356  0.008836  0.863481\n",
      "2   2      2     0     0    2  0.321047  0.221328  0.051202\n",
      "3   3      3     0     0    3  0.325469  0.223192  0.042331\n",
      "4   4      4     0     0    4  0.322876  0.223271  0.045700\n",
      "5   5      5     0     0    5  0.021825  0.026440  0.865749\n",
      "6   6      6     0     0    6  0.021825  0.026440  0.865749\n",
      "7   7      7     0     0    7  0.021825  0.026440  0.865749\n",
      "8   8      8     0     0    8  0.021825  0.026440  0.865749\n",
      "9   9      9     0     0    9 -0.005979 -0.004412  0.950159\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the failed submission\n",
    "submission = pd.read_csv('/home/submission/submission.csv')\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nColumn names: {submission.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08990bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:33:29.164662Z",
     "iopub.status.busy": "2026-01-16T16:33:29.164568Z",
     "iopub.status.idle": "2026-01-16T16:33:29.179560Z",
     "shell.execute_reply": "2026-01-16T16:33:29.179199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 56 negative values (2.97%)\n",
      "target_2: 63 negative values (3.35%)\n",
      "target_3: 19 negative values (1.01%)\n",
      "\n",
      "Total rows with any negative: 92\n"
     ]
    }
   ],
   "source": [
    "# Check for negative values\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    neg_count = (submission[col] < 0).sum()\n",
    "    print(f\"{col}: {neg_count} negative values ({100*neg_count/len(submission):.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows with any negative: {((submission['target_1'] < 0) | (submission['target_2'] < 0) | (submission['target_3'] < 0)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546eeb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:33:29.180503Z",
     "iopub.status.busy": "2026-01-16T16:33:29.180415Z",
     "iopub.status.idle": "2026-01-16T16:33:29.183199Z",
     "shell.execute_reply": "2026-01-16T16:33:29.182855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 0 values > 1 (0.00%)\n",
      "target_2: 0 values > 1 (0.00%)\n",
      "target_3: 3 values > 1 (0.16%)\n"
     ]
    }
   ],
   "source": [
    "# Check for values > 1 (also invalid for yields)\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    over_count = (submission[col] > 1).sum()\n",
    "    print(f\"{col}: {over_count} values > 1 ({100*over_count/len(submission):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3649ddaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:33:29.184030Z",
     "iopub.status.busy": "2026-01-16T16:33:29.183941Z",
     "iopub.status.idle": "2026-01-16T16:33:29.190772Z",
     "shell.execute_reply": "2026-01-16T16:33:29.190464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction statistics:\n",
      "          target_1     target_2     target_3\n",
      "count  1883.000000  1883.000000  1883.000000\n",
      "mean      0.146767     0.129071     0.514198\n",
      "std       0.122193     0.107345     0.341795\n",
      "min      -0.019779    -0.024991    -0.017362\n",
      "25%       0.035375     0.033334     0.136847\n",
      "50%       0.102743     0.095480     0.644871\n",
      "75%       0.268811     0.225374     0.845212\n",
      "max       0.407124     0.371803     1.004625\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of predictions\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(submission[['target_1', 'target_2', 'target_3']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40219271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:33:29.191662Z",
     "iopub.status.busy": "2026-01-16T16:33:29.191575Z",
     "iopub.status.idle": "2026-01-16T16:33:29.195988Z",
     "shell.execute_reply": "2026-01-16T16:33:29.195627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yield sum statistics:\n",
      "count    1883.000000\n",
      "mean        0.790037\n",
      "std         0.165299\n",
      "min         0.158422\n",
      "25%         0.712481\n",
      "50%         0.830684\n",
      "75%         0.909277\n",
      "max         1.052307\n",
      "Name: yield_sum, dtype: float64\n",
      "\n",
      "Rows where yield_sum > 1: 31\n"
     ]
    }
   ],
   "source": [
    "# Check if the sum of yields exceeds 1 (physically impossible)\n",
    "submission['yield_sum'] = submission['target_1'] + submission['target_2'] + submission['target_3']\n",
    "print(f\"\\nYield sum statistics:\")\n",
    "print(submission['yield_sum'].describe())\n",
    "print(f\"\\nRows where yield_sum > 1: {(submission['yield_sum'] > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f93c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the CV-LB relationship from submission history\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Extract submission history\n",
    "submissions = state.get('submissions', [])\n",
    "print(f\"Total submissions: {len(submissions)}\")\n",
    "print(\"\\nSubmission history:\")\n",
    "for s in submissions:\n",
    "    print(f\"  {s.get('experiment_id', 'N/A')}: CV={s.get('cv_score', 'N/A')}, LB={s.get('lb_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship\n",
    "cv_scores = []\n",
    "lb_scores = []\n",
    "for s in submissions:\n",
    "    cv = s.get('cv_score')\n",
    "    lb = s.get('lb_score')\n",
    "    if cv is not None and lb is not None and lb != 'pending':\n",
    "        try:\n",
    "            cv_scores.append(float(cv))\n",
    "            lb_scores.append(float(lb))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"\\nValid CV-LB pairs: {len(cv_scores)}\")\n",
    "if len(cv_scores) >= 3:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    X = np.array(cv_scores).reshape(-1, 1)\n",
    "    y = np.array(lb_scores)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    r2 = reg.score(X, y)\n",
    "    print(f\"\\nLinear fit: LB = {reg.coef_[0]:.4f} * CV + {reg.intercept_:.4f}\")\n",
    "    print(f\"R² = {r2:.4f}\")\n",
    "    print(f\"\\nTarget LB: 0.0347\")\n",
    "    print(f\"Intercept: {reg.intercept_:.4f}\")\n",
    "    print(f\"Required CV for target: {(0.0347 - reg.intercept_) / reg.coef_[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b94087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The submission failed likely due to negative predictions\n",
    "# The evaluation metric expects yields in [0, 1] range\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. NEGATIVE PREDICTIONS:\")\n",
    "print(f\"   - {((submission['target_1'] < 0) | (submission['target_2'] < 0) | (submission['target_3'] < 0)).sum()} rows have negative values\")\n",
    "print(f\"   - This is physically impossible for yields\")\n",
    "print(f\"   - The evaluation metric likely raises an error for invalid predictions\")\n",
    "\n",
    "print(\"\\n2. FIX REQUIRED:\")\n",
    "print(\"   - Clip predictions to [0, 1] range\")\n",
    "print(\"   - Add: final_preds = np.clip(final_preds, 0, 1)\")\n",
    "\n",
    "print(\"\\n3. ADDITIONAL ISSUE:\")\n",
    "print(\"   - SolventB% scaling: exp_101 divides by 100, mixall kernel doesn't\")\n",
    "print(\"   - This affects mixture interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best experiments we have\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "experiments = state.get('experiments', [])\n",
    "best_cv = float('inf')\n",
    "best_exp = None\n",
    "for exp in experiments:\n",
    "    score = exp.get('score')\n",
    "    if score is not None and score < best_cv:\n",
    "        best_cv = score\n",
    "        best_exp = exp\n",
    "\n",
    "if best_exp:\n",
    "    print(f\"\\nBest CV: {best_cv:.6f}\")\n",
    "    print(f\"Experiment: {best_exp.get('name', 'N/A')}\")\n",
    "    print(f\"Model: {best_exp.get('model_type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b865a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the best LB submission was\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST LB SUBMISSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_lb = float('inf')\n",
    "best_lb_sub = None\n",
    "for s in submissions:\n",
    "    lb = s.get('lb_score')\n",
    "    if lb is not None and lb != 'pending':\n",
    "        try:\n",
    "            lb_val = float(lb)\n",
    "            if lb_val < best_lb:\n",
    "                best_lb = lb_val\n",
    "                best_lb_sub = s\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if best_lb_sub:\n",
    "    print(f\"\\nBest LB: {best_lb:.6f}\")\n",
    "    print(f\"Experiment: {best_lb_sub.get('experiment_id', 'N/A')}\")\n",
    "    print(f\"CV: {best_lb_sub.get('cv_score', 'N/A')}\")\n",
    "    print(f\"\\nTarget: 0.0347\")\n",
    "    print(f\"Gap: {best_lb - 0.0347:.6f} ({100*(best_lb - 0.0347)/0.0347:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. SUBMISSION FAILURE ROOT CAUSE:\n",
    "   - Negative predictions in submission.csv\n",
    "   - Need to clip predictions to [0, 1] range\n",
    "\n",
    "2. CV-LB RELATIONSHIP:\n",
    "   - All tabular approaches fall on same line: LB ≈ 4.3 × CV + 0.053\n",
    "   - Intercept (0.053) > Target (0.0347)\n",
    "   - This means NO amount of CV improvement can reach target\n",
    "   - Need to CHANGE the relationship, not just improve CV\n",
    "\n",
    "3. WHAT TO TRY NEXT:\n",
    "   a) Fix the submission by clipping predictions\n",
    "   b) Try approaches that could change the CV-LB relationship:\n",
    "      - Extrapolation detection + conservative predictions\n",
    "      - Domain constraints (yields sum to ≤1)\n",
    "      - Different representation (GNN, ChemBERTa)\n",
    "   \n",
    "4. WHAT NOT TO DO:\n",
    "   - More tabular model variants (exhaustively tested)\n",
    "   - Multi-seed optimization (too far from target)\n",
    "   - Hyperparameter tuning (won't change intercept)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exp_073 which had a very different LB (0.1451)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTLIER ANALYSIS: exp_073\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for s in submissions:\n",
    "    if s.get('experiment_id') == 'exp_073':\n",
    "        print(f\"\\nexp_073:\")\n",
    "        print(f\"  CV: {s.get('cv_score')}\")\n",
    "        print(f\"  LB: {s.get('lb_score')}\")\n",
    "        print(f\"  Notes: This had a much worse LB than expected from CV\")\n",
    "        print(f\"  Likely cause: Model class mismatch or submission error\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
