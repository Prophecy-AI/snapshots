{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd7d9e5",
   "metadata": {},
   "source": [
    "# Loop 105 Analysis: Breaking the CV-LB Line\n",
    "\n",
    "## Key Problem\n",
    "- CV-LB relationship: LB = 4.29 × CV + 0.0528 (R² = 0.95)\n",
    "- Intercept (0.0528) > Target (0.0347)\n",
    "- All 105 experiments fall on the same line\n",
    "- Best LB: 0.0877 (exp_030, CV=0.0083)\n",
    "- Target: 0.0347 (152.8% gap)\n",
    "\n",
    "## Analysis Goals\n",
    "1. Understand why all models fall on the same CV-LB line\n",
    "2. Identify approaches that could CHANGE the relationship\n",
    "3. Analyze what the top public kernels are doing differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Submission history with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "    {'exp': 'exp_073', 'cv': 0.0084, 'lb': 0.1451},  # Outlier - similarity weighting\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(f\"Total submissions with LB: {len(df)}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit CV-LB relationship (excluding outlier exp_073)\n",
    "df_clean = df[df['exp'] != 'exp_073'].copy()\n",
    "\n",
    "X = df_clean['cv'].values.reshape(-1, 1)\n",
    "y = df_clean['lb'].values\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "r2 = reg.score(X, y)\n",
    "\n",
    "print(f\"\\n=== CV-LB Relationship ===\")\n",
    "print(f\"LB = {slope:.3f} × CV + {intercept:.4f}\")\n",
    "print(f\"R² = {r2:.4f}\")\n",
    "print(f\"\\nIntercept: {intercept:.4f}\")\n",
    "print(f\"Target LB: 0.0347\")\n",
    "print(f\"Gap: Intercept ({intercept:.4f}) > Target (0.0347)\")\n",
    "print(f\"\\nRequired CV to hit target: (0.0347 - {intercept:.4f}) / {slope:.3f} = {(0.0347 - intercept) / slope:.6f}\")\n",
    "print(\"\\n⚠️ IMPOSSIBLE: Required CV is NEGATIVE!\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_clean['cv'], df_clean['lb'], c='blue', s=100, label='Submissions')\n",
    "plt.scatter([0.0084], [0.1451], c='red', s=100, marker='x', label='exp_073 (outlier)')\n",
    "\n",
    "# Fit line\n",
    "cv_range = np.linspace(0, 0.015, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'b--', label=f'LB = {slope:.2f}×CV + {intercept:.4f}')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.0347, color='green', linestyle=':', linewidth=2, label='Target LB = 0.0347')\n",
    "\n",
    "# Intercept line\n",
    "plt.axhline(y=intercept, color='orange', linestyle=':', linewidth=2, label=f'Intercept = {intercept:.4f}')\n",
    "\n",
    "plt.xlabel('CV Score (MSE)')\n",
    "plt.ylabel('LB Score (MSE)')\n",
    "plt.title('CV vs LB Relationship - All Models on Same Line')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_analysis_loop105.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /home/code/exploration/cv_lb_analysis_loop105.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "approaches = {\n",
    "    'MLP variants': ['exp_000', 'exp_003', 'exp_005', 'exp_006', 'exp_007'],\n",
    "    'LightGBM': ['exp_001'],\n",
    "    'Ridge/GP': ['exp_009', 'exp_012'],\n",
    "    'CatBoost+XGBoost': ['exp_024', 'exp_026', 'exp_030'],\n",
    "    'GNN attempts': ['exp_049', 'exp_050', 'exp_079'],\n",
    "    'ChemBERTa attempts': ['exp_052', 'exp_053'],\n",
    "    'Similarity weighting': ['exp_073'],\n",
    "    'Bias correction': ['exp_104'],\n",
    "}\n",
    "\n",
    "print(\"=== Approaches Tried ===\")\n",
    "for approach, exps in approaches.items():\n",
    "    print(f\"\\n{approach}:\")\n",
    "    for exp in exps:\n",
    "        match = df[df['exp'] == exp]\n",
    "        if len(match) > 0:\n",
    "            print(f\"  {exp}: CV={match['cv'].values[0]:.4f}, LB={match['lb'].values[0]:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {exp}: LB pending or failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The intercept problem\n",
    "print(\"\\n=== THE INTERCEPT PROBLEM ===\")\n",
    "print(f\"\"\"\\nThe CV-LB relationship shows:\n",
    "- Slope: {slope:.3f} (each 0.001 CV improvement → {slope*0.001:.4f} LB improvement)\n",
    "- Intercept: {intercept:.4f} (structural error even at CV=0)\n",
    "\n",
    "The intercept represents EXTRAPOLATION ERROR:\n",
    "- When predicting for unseen solvents, the model makes systematic errors\n",
    "- This error is INDEPENDENT of how well the model fits training data\n",
    "- All tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) have the SAME intercept\n",
    "\n",
    "To reach target LB=0.0347:\n",
    "- Option 1: Reduce intercept (change the CV-LB relationship)\n",
    "- Option 2: Achieve negative CV (impossible)\n",
    "\n",
    "We MUST focus on Option 1: Reducing the intercept.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== STRATEGIES TO REDUCE INTERCEPT ===\")\n",
    "print(\"\"\"\n",
    "1. REPRESENTATION CHANGE:\n",
    "   - GNN: Operate on molecular graphs, not tabular features\n",
    "   - ChemBERTa: Use pretrained chemical language model embeddings\n",
    "   - Morgan fingerprints with similarity features\n",
    "   \n",
    "2. DISTRIBUTION SHIFT HANDLING:\n",
    "   - Extrapolation detection: Identify when predicting for \"hard\" solvents\n",
    "   - Conservative blending: Blend toward training mean when extrapolating\n",
    "   - Uncertainty quantification: Use ensemble variance to weight predictions\n",
    "   \n",
    "3. DOMAIN CONSTRAINTS:\n",
    "   - Physics-based constraints (Arrhenius, mass balance)\n",
    "   - Yield normalization (sum ≤ 1)\n",
    "   \n",
    "4. VALIDATION STRATEGY:\n",
    "   - The \"mixall\" kernel uses GroupKFold instead of Leave-One-Out\n",
    "   - This might give a different CV-LB relationship\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the mixall kernel approach\n",
    "print(\"\\n=== MIXALL KERNEL ANALYSIS ===\")\n",
    "print(\"\"\"\n",
    "The 'mixall' kernel (lishellliang) uses a DIFFERENT validation strategy:\n",
    "- GroupKFold with 5 splits instead of Leave-One-Out (24 folds)\n",
    "- This means each fold has ~5 solvents in test set instead of 1\n",
    "- The CV score might be more representative of LB\n",
    "\n",
    "Key differences:\n",
    "1. Leave-One-Out: 24 folds, each with 1 solvent in test\n",
    "2. GroupKFold(5): 5 folds, each with ~5 solvents in test\n",
    "\n",
    "The GroupKFold approach might:\n",
    "- Give a more stable CV estimate\n",
    "- Better represent the LB distribution\n",
    "- Potentially have a different CV-LB relationship\n",
    "\n",
    "However, the competition template REQUIRES Leave-One-Out for submission.\n",
    "So we can't change the validation for submission, but we could use\n",
    "GroupKFold for model selection.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== ENS-MODEL KERNEL ANALYSIS ===\")\n",
    "print(\"\"\"\n",
    "The 'ens-model' kernel (matthewmaree) uses:\n",
    "- CatBoost + XGBoost ensemble\n",
    "- Comprehensive feature engineering:\n",
    "  - Spange descriptors (physicochemical)\n",
    "  - ACS PCA descriptors\n",
    "  - DRFP fingerprints (filtered)\n",
    "  - Fragprints\n",
    "  - SMILES features\n",
    "- Correlation-based feature filtering (threshold=0.90)\n",
    "- Numeric feature engineering:\n",
    "  - Temperature in Kelvin\n",
    "  - T × RT interaction\n",
    "  - log(RT)\n",
    "  - 1/T (inverse temperature)\n",
    "  - RT_scaled\n",
    "\n",
    "This is similar to what we've tried, but with more comprehensive features.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we haven't tried adequately\n",
    "print(\"\\n=== UNEXPLORED OR INADEQUATELY EXPLORED ===\")\n",
    "print(\"\"\"\n",
    "1. PROPER GNN IMPLEMENTATION:\n",
    "   - Many GNN experiments failed on submission (model class mismatch)\n",
    "   - Need to verify submission cells use the SAME model class as CV\n",
    "   - GNN could give a different CV-LB relationship\n",
    "\n",
    "2. EXTRAPOLATION-AWARE PREDICTIONS:\n",
    "   - Detect when we're predicting for \"hard\" solvents\n",
    "   - Blend toward training mean when extrapolating\n",
    "   - This could reduce the intercept\n",
    "\n",
    "3. PSEUDO-LABELING:\n",
    "   - Use confident predictions on test set to augment training\n",
    "   - Could help with distribution shift\n",
    "\n",
    "4. DOMAIN ADAPTATION:\n",
    "   - Train on source domain (training solvents)\n",
    "   - Adapt to target domain (test solvents)\n",
    "\n",
    "5. CONSERVATIVE PREDICTIONS:\n",
    "   - Clip extreme predictions\n",
    "   - Use ensemble variance to identify uncertain predictions\n",
    "   - Make uncertain predictions more conservative\n",
    "\n",
    "6. DIFFERENT VALIDATION FOR MODEL SELECTION:\n",
    "   - Use GroupKFold for hyperparameter tuning\n",
    "   - Use Leave-One-Out only for final submission\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDED NEXT STEPS ===\")\n",
    "print(\"\"\"\n",
    "1. IMMEDIATE: Implement extrapolation-aware conservative predictions\n",
    "   - Compute distance to nearest training solvent\n",
    "   - Blend toward training mean when extrapolating\n",
    "   - This directly targets the intercept problem\n",
    "\n",
    "2. SHORT-TERM: Debug GNN/ChemBERTa submission failures\n",
    "   - Verify model class consistency between CV and submission\n",
    "   - These approaches might give a different CV-LB relationship\n",
    "\n",
    "3. MEDIUM-TERM: Try pseudo-labeling\n",
    "   - Use confident predictions to augment training\n",
    "   - Could help with distribution shift\n",
    "\n",
    "4. LONG-TERM: Implement proper domain adaptation\n",
    "   - This is more complex but could be very effective\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
