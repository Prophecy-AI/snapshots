{
  "query": "How do you improve generalization for molecular property prediction when test molecules are structurally different from training molecules using graph neural networks?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving the ability of graph neural networks (GNNs) to predict molecular properties when the test set contains structures that are far from those seen during training is an out\u2011of\u2011distribution (OOD) problem.  Standard GNNs assume that training and test graphs share the same distribution, so performance drops when the chemical scaffolds, functional\u2011group patterns or size distributions shift.  Recent work therefore focuses on breaking the reliance on spurious correlations in the learned representations and on endowing the model with mechanisms to adapt to new chemical space.  One line of research proposes **OOD\u2011GNN**, which decorrelates relevant and irrelevant graph features using random Fourier features and learns per\u2011graph weights that suppress spurious patterns, leading to markedly better OOD performance on synthetic and real\u2011world molecular benchmarks\u301010.1080.0.0\u3011(https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2022_Out-of-Distribution%20Generalized%20Graph%20Neural%20Network.pdf).  An architecture\u2011level study also shows that self\u2011attention mechanisms and other building blocks strongly influence OOD generalization, suggesting that careful design of the GNN encoder can mitigate distribution shift effects\u3010arxiv.org\u3011(https://arxiv.org/abs/2402.08228).\n\nA complementary set of strategies tackles the problem from the data and training\u2011procedure side.  **Self\u2011supervised pre\u2011training** of GNNs at both node and graph levels provides rich, transferable representations that avoid negative transfer when fine\u2011tuned on a new property task\u3010arxiv.org\u3011(https://arxiv.org/pdf/1905.12265).  **Graph neural processes** treat each molecule as a stochastic function and learn to produce predictive distributions that can quickly adapt to unseen structures, improving docking\u2011score predictions under distribution shift\u3010jcheminf.biomedcentral.com\u3011(https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2).  **Auxiliary learning with task\u2011specific adaptation** jointly trains a pretrained GNN on multiple related tasks and uses bi\u2011level optimization to weight task gradients, which has been shown to raise out\u2011of\u2011distribution accuracy by up to\u202f8\u202f%\u3010jcheminf.biomedcentral.com\u3011(https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7) and in the OpenReview version of the same work\u3010openreview.net\u3011(https://openreview.net/pdf/41e1a95e6fc281b863d44eb407ff5a07d8f83bef.pdf).  **Transfer learning in a multi\u2011fidelity setting** leverages inexpensive low\u2011fidelity measurements together with a smaller high\u2011fidelity set, allowing the GNN to learn robust features that generalize across fidelity gaps\u3010nature.com\u3011(https://www.nature.com/articles/s41467-024-45566-8).  Finally, selecting the most compatible auxiliary datasets through **MolGroup**, which predicts dataset affinity by combining graph\u2011structure similarity and task similarity, helps avoid negative transfer and further boosts performance on the target OOD task\u3010arxiv.org\u3011(https://arxiv.org/abs/2307.04052).  Together, these approaches\u2014robust architectural design, self\u2011supervised pre\u2011training, uncertainty\u2011aware neural processes, adaptive multi\u2011task learning, multi\u2011fidelity transfer, and principled auxiliary\u2011data selection\u2014constitute a practical toolkit for improving GNN generalization when test molecules differ structurally from the training set.",
      "url": ""
    },
    {
      "title": "Graph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization",
      "text": "Search all BMC articles\n\nSearch\n\nGraph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00904-2.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00904-2.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00904-2.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00904-2.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 23 October 2024\n\n# Graph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization\n\n- [Miguel Garc\u00eda-Orteg\u00f3n](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Miguel-Garc_a_Orteg_n-Aff1-Aff2-Aff3) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3),\n- [Srijit Seal](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Srijit-Seal-Aff4) [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff4),\n- [Carl Rasmussen](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Carl-Rasmussen-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Andreas Bender](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Andreas-Bender-Aff3) [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3) &\n- \u2026\n- [Sergio Bacallado](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Sergio-Bacallado-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a0115 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 1169 Accesses\n\n- 1 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2/metrics)\n\n\nThis article has been [updated](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#change-history)\n\n### Abstract\n\nNeural processes (NPs) are models for meta-learning which output uncertainty estimates. So far, most studies of NPs have focused on low-dimensional datasets of highly-correlated tasks. While these homogeneous datasets are useful for benchmarking, they may not be representative of realistic transfer\u00a0learning. In particular, applications in scientific research may prove especially challenging due to the potential novelty of meta-testing tasks. Molecular property prediction is one such research area that is characterized by sparse datasets of many functions on a shared molecular space. In this paper, we study the application of graph NPs to molecular property prediction with DOCKSTRING, a diverse dataset of docking scores. Graph NPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in chemoinformatics, as well as alternative techniques for transfer learning and meta-learning. In order to increase meta-generalization to divergent test functions, we propose fine-tuning strategies that adapt the parameters of NPs. We find that adaptation can substantially increase NPs' regression performance while maintaining good calibration of uncertainty estimates. Finally, we present a Bayesian optimization experiment which showcases the potential advantages of NPs over Gaussian processes in iterative screening. Overall, our results suggest that NPs on molecular graphs hold great potential for molecular property prediction in the low-data setting.\n\n### Scientific contribution\n\nNeural processes are a family of meta-learning algorithms which deal with data scarcity by transferring information across tasks and making probabilistic predictions. We evaluate their performance on regression and optimization molecular tasks using docking scores, finding them to outperform classical single-task and transfer-learning models. We examine the issue of generalization to divergent test tasks, which is a general concern of meta-learning algorithms in science, and propose strategies to alleviate it.\n\n## Introduction\n\nA major difficulty in the application of machine learning (ML) to molecular property prediction in drug discovery is the scarcity of labeled data. Experimental assays are expensive and time-consuming, and data collection is biased towards certain bioactivities (e.g. protein targets deemed medically relevant or commercially profitable) or molecules (e.g. those that are easier to acquire or synthesize). As a result, chemoinformatic datasets are highly sparse and non-overlapping. In a typical pharmaceutical company\u2019s chemical library, it is estimated that less than 1% of all the compound-assay pairs have been measured\u00a0\\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR1)\\]. Even more strikingly, public databases are as little as 0.05% complete\u00a0\\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR1), [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR2)\\].\n\nMeta-learning, or \u201clearning to learn\u201d, is a machine-learning paradigm that attempts to achieve fast adaptation to novel tasks given a small number of labeled datapoints\u00a0\\[ [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR3)\\]. The meta-learning setting is similar to transfer learning in that it attempts to take advantage of existing information to improve predictions on downstream tasks. However, instead of transferring knowledge from a single pre-training task with many labels, meta-learning attempts to transfer knowledge from multiple meta-training tasks with few labels each. Later, during meta-testing, the model is evaluated on unseen tasks, using a few labeled datapoints from each task as examples. These example points encode information about the meta-test task and are called the contexts. In turn, the query points of interest that we want to predict for each meta-task are called the targets. The ability to learn from meta-training tasks without overfitting and generalizing to novel meta-testing tasks is called meta-generalization.\n\nThe meta-learning setting may be appropriate in molecular property prediction\u00a0\\[ [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR4)\\], since measurements from many different molecular tasks have been collected historically and could be used for meta-training. Examples of molecular tasks are physicochemical properties, protein binding affinities, phenotypic assays or ADMET endpoints\u00a0\\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR5), [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR6)\\]. Typically, each task comprises too few datapoints to train a large neural model, but collectively a large set of bioactivities may be useful to learn biases of molecular functions, as well as molecular representations. However, given the sheer diversity of molecular tasks that are available, extra care should be taken to ensure that the biases learnt during meta-training are adequate for meta-testing. For example, tasks related to physicochemical properties, which are intrinsic to molecules, may be very different from cell assays, which depend on the complex interplay between molecules and a biological system\u00a0\\[ [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR7)\\].\n\nIn addition to data efficiency and learning from sparse datasets, another feature that is desirable is the ability to produce uncertainty estimates\u00a0\\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#re...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2"
    },
    {
      "title": "Enhancing molecular property prediction with auxiliary learning and task-specific adaptation",
      "text": "Search all BMC articles\n\nSearch\n\nEnhancing molecular property prediction with auxiliary learning and task-specific adaptation\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00880-7.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00880-7.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00880-7.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00880-7.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 24 July 2024\n\n# Enhancing molecular property prediction with auxiliary learning and task-specific adaptation\n\n- [Vishal Dey](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Vishal-Dey-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1) &\n- [Xia Ning](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Xia-Ning-Aff1-Aff2-Aff3) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a085 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 1535 Accesses\n\n- 2 Citations\n\n- 1 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7/metrics)\n\n\n## Abstract\n\nPretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients (\\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\)), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.\n\n**Scientific contribution**\n\nWe introduce a novel framework for adapting pretrained GNNs to molecular tasks using auxiliary learning to address the critical issue of negative transfer. Leveraging novel gradient surgery techniques such as \\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\), the proposed adaptation framework represents a significant departure from the dominant pretraining fine-tuning approach for molecular GNNs. Our contributions are significant for drug discovery research, especially for tasks with limited data, filling a notable gap in the efficient adaptation of pretrained models for molecular GNNs.\n\n## Introduction\n\nAccurate prediction of molecular properties is pivotal in drug discovery \\[ [39](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR39)\\], as it accelerates the identification of potential molecules with desired properties. Developing computational models for property prediction relies on learning effective representations of molecules \\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR5)\\]. In this regard, Graph Neural Networks (GNNs) have shown impressive results in learning effective representations for molecular property prediction tasks \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR12), [37](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR37)\\]. Inspired by the paradigm of pretraining followed by fine-tuning, widely recognized for its impact in natural language understanding \\[ [27](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR27), [38](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR38)\\], molecular GNNs are often pretrained \\[ [17](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR17)\\] on a large corpus of molecules. Such a corpus might encompass irrelevant data for the target property prediction task. This can lead the GNNs to learn features that do not benefit the target task. Consequently, pretrained GNNs are fine-tuned with the target task to encode task-specific features. However, vanilla fine-tuning can potentially lead to poor generalization, particularly when dealing with diverse downstream tasks, limited data, and the need to generalize across varying scaffolds \\[ [40](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR40)\\].\n\nTo improve generalization, auxiliary learning has recently garnered attention \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR8), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR20), [21](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR21)\\]. Auxiliary learning leverages informative signals from self-supervised tasks on unlabeled data, to improve the performance of the target tasks. However, its application in the context of molecular graphs, specifically for molecular property prediction, remains largely unexplored. Following this line of work, in this paper, we explore how to adapt pretrained molecular GNNs by combining widely-used self-supervised tasks with the target task using respective task-specific data (with self-supervised and target task labels). However, a critical challenge in such an adaptation is caused by negative transfer \\[ [29](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR29)\\], where auxiliary tasks might impede rather than aid the target task \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR9), [30](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR30)\\].\n\nTo address this challenge, we develop novel gradient surgery-based adaptation strategies, referred to as Rotation of Conflicting Gradients (\\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\)) and Bi-level Optimization with Gradient Rotation (\\\\(\\\\mathop {\\\\texttt{BLO}\\\\text {+}\\\\texttt{RCGrad}}\\\\limits\\\\)). Such strategies mitigate negative transfer from auxiliary tasks by learning to align conflicting gradients. Overall, our adaptation strategies improved the target task performance by as much as 7.7% over vanilla fine-tuning. Moreover, our findings indicate that the developed adaptation strategies are particularly effective in tasks with limited labeled data, which is a common challenge in molecular property prediction tasks. Our comprehensive investigation of multiple adaptation strategies for pretrained molecular GNNs represents a notable contribution in addressing the limited benefit of pretrained GNNs \\[ [34](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR34)\\], and in improving generalizability across a diverse set of downstream tasks with limited data.\n\n## Related work\n\n### Pretraining an...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7"
    },
    {
      "title": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting",
      "text": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=8370ae3e-f2c3-44c8-9e8b-b5e96c227834)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nTransfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:26 February 2024# Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n* [David Buterez](#auth-David-Buterez-Aff1)[ORCID:orcid.org/0000-0001-6558-0833](https://orcid.org/0000-0001-6558-0833)[1](#Aff1),\n* [Jon Paul Janet](#auth-Jon_Paul-Janet-Aff2)[ORCID:orcid.org/0000-0001-7825-4797](https://orcid.org/0000-0001-7825-4797)[2](#Aff2),\n* [Steven J. Kiddle](#auth-Steven_J_-Kiddle-Aff3)[3](#Aff3),\n* [Dino Oglic](#auth-Dino-Oglic-Aff4)[4](#Aff4)&amp;\n* \u2026* [Pietro Li\u00f3](#auth-Pietro-Li_-Aff1)[ORCID:orcid.org/0000-0002-0540-5053](https://orcid.org/0000-0002-0540-5053)[1](#Aff1)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:1517(2024)[Cite this article](#citeas)\n* 32kAccesses\n* 77Citations\n* 5Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-45566-8/metrics)\n### Subjects\n* [Computational biology and bioinformatics](https://www.nature.com/subjects/computational-biology-and-bioinformatics)\n* [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n* [Quantum mechanics](https://www.nature.com/subjects/quantum-mechanics)\n## Abstract\nWe investigate the potential of graph neural networks for transfer learning and improving molecular property prediction on sparse and expensive to acquire high-fidelity data by leveraging low-fidelity measurements as an inexpensive proxy for a targeted property of interest. This problem arises in discovery processes that rely on screening funnels for trading off the overall costs against throughput and accuracy. Typically, individual stages in these processes are loosely connected and each one generates data at different scale and fidelity. We consider this setup holistically and demonstrate empirically that existing transfer learning techniques for graph neural networks are generally unable to harness the information from multi-fidelity cascades. Here, we propose several effective transfer learning strategies and study them in transductive and inductive settings. Our analysis involves a collection of more than 28 million unique experimental protein-ligand interactions across 37 targets from drug discovery by high-throughput screening and 12 quantum properties from the dataset QMugs. The results indicate that transfer learning can improve the performance on sparse tasks by up to eight times while using an order of magnitude less high-fidelity training data. Moreover, the proposed methods consistently outperform existing transfer learning strategies for graph-structured data on drug discovery and quantum mechanics datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-022-00501-8/MediaObjects/42256_2022_501_Fig1_HTML.png)\n### [An adaptive graph learning method for automated molecular interactions and properties predictions](https://www.nature.com/articles/s42256-022-00501-8?fromPaywallRec=false)\nArticle23 June 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-023-45269-y/MediaObjects/41598_2023_45269_Fig1_HTML.png)\n### [Binding affinity predictions with hybrid quantum-classical convolutional neural networks](https://www.nature.com/articles/s41598-023-45269-y?fromPaywallRec=false)\nArticleOpen access20 October 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-05905-z/MediaObjects/41586_2023_5905_Fig1_HTML.png)\n### [Computational approaches streamlining drug discovery](https://www.nature.com/articles/s41586-023-05905-z?fromPaywallRec=false)\nArticle26 April 2023\n## Introduction\nWe investigate the potential of graph neural networks (GNNs) for transfer learning and improved molecular property prediction in the context of funnels or screening cascades characteristic of drug discovery and/or molecular design. GNNs have emerged as a powerful and widely-used class of algorithms for molecular property prediction thanks to their natural ability to learn from molecular structures represented as atoms and bonds[1](#ref-CR1),[2](#ref-CR2),[3](https://www.nature.com/articles/s41467-024-45566-8#ref-CR3), as well as in the life sciences in general[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s41467-024-45566-8#ref-CR6). However, their potential for transfer learning is yet to be established. The screening cascade refers to a multi-stage approach where one starts with cheap and relatively noisy methods (high-throughput screening, molecular mechanics calculations, etc.) that allow for screening a large number of molecules. This is followed by increasingly accurate and more expensive evaluations that come with much lower throughput, up to the experimental characterisation of compounds. Individual stages or tiers in the screening funnel are, thus, used to make a reduction of the search space and focus the evaluation of more expensive properties on the promising regions. In this way, the funnel maintains a careful trade-off between the scale, cost, and accuracy. The progression from one tier to another is typically done manually by selecting subsets of molecules from the library screened at the previous stage or via a surrogate model that focuses the screening budget of the next step on the part of the chemical space around the potential hits. Such surrogate models are typically built using the data originating from a single tier and, thus, without leveraging measurements of different fidelity.\nFor efficient use of experimental resources, it is beneficial to have good predictive models operating on sparse datasets and guiding the high-fidelity evaluations relative to properties of interest. The latter is the most expensive part of the funnel and to efficiently support it, we consider it in a transfer learning setting designed to leverage low-fidelity observations to improve the effectiveness of predictive models on sparse and high-fidelity experimental data. In drug discovery applications of this setup, low-fidelity measurements can be seen as ground truth values that have been corrupted by noise, experimental or reading artefacts, or are simply performed using less precise but cheaper experiments. For quantum mechanics...",
      "url": "https://www.nature.com/articles/s41467-024-45566-8"
    },
    {
      "title": "Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2402.08228** (cs)\n\n\\[Submitted on 13 Feb 2024 ( [v1](https://arxiv.org/abs/2402.08228v1)), last revised 14 Feb 2024 (this version, v2)\\]\n\n# Title:Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective\n\nAuthors: [Kai Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo,+K), [Hongzhi Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen,+H), [Wei Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin,+W), [Yaming Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo,+Y), [Jiliang Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+J), [Yi Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang,+Y)\n\nView a PDF of the paper titled Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective, by Kai Guo and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.08228) [HTML (experimental)](https://arxiv.org/html/2402.08228v2)\n\n> Abstract:Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \\\\textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \\\\textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGAT, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2402.08228](https://arxiv.org/abs/2402.08228) \\[cs.LG\\] |\n| (or [arXiv:2402.08228v2](https://arxiv.org/abs/2402.08228v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.08228](https://doi.org/10.48550/arXiv.2402.08228) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Kai Guo \\[ [view email](https://arxiv.org/show-email/65284c82/2402.08228)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.08228v1)**\nTue, 13 Feb 2024 05:38:45 UTC (1,662 KB)\n**\\[v2\\]**\nWed, 14 Feb 2024 16:26:09 UTC (1,662 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective, by Kai Guo and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.08228)\n- [HTML (experimental)](https://arxiv.org/html/2402.08228v2)\n- [TeX Source](https://arxiv.org/src/2402.08228)\n- [Other Formats](https://arxiv.org/format/2402.08228)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.08228&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.08228&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-02](https://arxiv.org/list/cs.LG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.08228?context=cs) [cs.AI](https://arxiv.org/abs/2402.08228?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.08228)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.08228)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.08228)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.08228) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2402.08228"
    },
    {
      "title": "",
      "text": "Published as a conference paper at ICLR 2020\nSTRATEGIES FOR PRE-TRAINING GRAPH NEURAL\nNETWORKS\nWeihua Hu1\u2217, Bowen Liu2\u2217, Joseph Gomes4, Marinka Zitnik5,\nPercy Liang1, Vijay Pande3, Jure Leskovec1\n1Department of Computer Science, 2Chemistry, 3Bioengineering, Stanford University,\n4Department of Chemical and Biochemical Engineering, The University of Iowa,\n5Department of Biomedical Informatics, Harvard University\n{weihuahu,liubowen,pliang,jure}@cs.stanford.edu,\njoe-gomes@uiowa.edu, marinka@hms.harvard.edu, pande@stanford.edu\nABSTRACT\nMany applications of machine learning require a model to make accurate pre\u0002dictions on test examples that are distributionally different from training ones,\nwhile task-specific labels are scarce during training. An effective approach to this\nchallenge is to pre-train a model on related tasks where data is abundant, and then\nfine-tune it on a downstream task of interest. While pre-training has been effective\nin many language and vision domains, it remains an open question how to effec\u0002tively use pre-training on graph datasets. In this paper, we develop a new strategy\nand self-supervised methods for pre-training Graph Neural Networks (GNNs). The\nkey to the success of our strategy is to pre-train an expressive GNN at the level of\nindividual nodes as well as entire graphs so that the GNN can learn useful local\nand global representations simultaneously. We systematically study pre-training\non multiple graph classification datasets. We find that na\u00efve strategies, which\npre-train GNNs at the level of either entire graphs or individual nodes, give limited\nimprovement and can even lead to negative transfer on many downstream tasks.\nIn contrast, our strategy avoids negative transfer and improves generalization sig\u0002nificantly across downstream tasks, leading up to 9.4% absolute improvements in\nROC-AUC over non-pre-trained models and achieving state-of-the-art performance\nfor molecular property prediction and protein function prediction.\n1 INTRODUCTION\nTransfer learning refers to the setting where a model, initially trained on some tasks, is re-purposed\non different but related tasks. Deep transfer learning has been immensely successful in computer\nvision (Donahue et al., 2014; Girshick et al., 2014; Zeiler & Fergus, 2014) and natural language\nprocessing (Devlin et al., 2019; Peters et al., 2018; Mikolov et al., 2013). Despite being an effective\napproach to transfer learning, few studies have generalized pre-training to graph data.\nPre-training has the potential to provide an attractive solution to the following two fundamental\nchallenges with learning on graph datasets (Pan & Yang, 2009; Hendrycks et al., 2019): First,\ntask-specific labeled data can be extremely scarce. This problem is exacerbated in important graph\ndatasets from scientific domains, such as chemistry and biology, where data labeling (e.g., biological\nexperiments in a wet laboratory) is resource- and time-intensive (Zitnik et al., 2018). Second, graph\ndata from real-world applications often contain out-of-distribution samples, meaning that graphs in\nthe training set are structurally very different from graphs in the test set. Out-of-distribution prediction\nis common in real-world graph datasets, for example, when one wants to predict chemical properties\nof a brand-new, just synthesized molecule, which is different from all molecules synthesized so far,\nand thereby different from all molecules in the training set.\nHowever, pre-training on graph datasets remains a hard challenge. Several key studies (Xu et al.,\n2017; Ching et al., 2018; Wang et al., 2019) have shown that successful transfer learning is not only a\n\u2217Equal contribution. Project website, data and code: http://snap.stanford.edu/gnn-pretrain\n1\narXiv:1905.12265v3 [cs.LG] 18 Feb 2020\nPublished as a conference paper at ICLR 2020\n(a.i) (a.ii) (a.iii)\nNode-level Graph-level\nAttribute\nprediction\nAttribute\nMasking\nSupervised\nAttribute\nPrediction\nStructural\nSimilarity\nPrediction\nStructure\nprediction\nContext\nPrediction\n(b) Categorization of our pre-training methods\nGraph space Node space\nGraph embeddings Node embeddings Linear classifier\nFigure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically\ndifferent nodes) can be well separated, however, node embeddings are not composable, and thus\nresulting graph embeddings (denoted by their classes, + and \u2212) that are created by pooling node-level\nembeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well\nseparated, however the embeddings of individual nodes do not necessarily capture their domain\u0002specific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are\nwell separated, while at the same time, the embedding space is also composable. This allows for\naccurate and robust representations of entire graphs and enables robust transfer of pre-trained models\nto a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially,\nour methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training\n(Supervised Attribute Prediction) enable both node-level and graph-level pre-training.\nmatter of increasing the number of labeled pre-training datasets that are from the same domain as\nthe downstream task. Instead, it requires substantial domain expertise to carefully select examples\nand target labels that are correlated with the downstream task of interest. Otherwise, the transfer of\nknowledge from related pre-training tasks to a new downstream task can harm generalization, which\nis known as negative transfer (Rosenstein et al., 2005) and significantly limits the applicability and\nreliability of pre-trained models.\nPresent work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural\nNetworks (GNNs) (Kipf & Welling, 2017; Hamilton et al., 2017a; Ying et al., 2018b; Xu et al., 2019;\n2018) for graph-level property prediction. Our work presents two key contributions. (1) We conduct\nthe first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build\ntwo large new pre-training datasets, which we share with the community: a chemistry dataset with\n2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific\ndatasets are crucial to investigate pre-training and that existing downstream benchmark datasets\nare too small to evaluate models in a statistically reliable way. (2) We develop an effective pre\u0002training strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution\ngeneralization on hard transfer-learning problems.\nIn our systematic study, we show that pre-training GNNs does not always help. Na\u00efve pre-training\nstrategies can lead to negative transfer on many downstream tasks. Strikingly, a seemingly strong\npre-training strategy (i.e., graph-level multi-task supervised pre-training using a state-of-the-art graph\nneural network architecture for graph-level prediction tasks) only gives marginal performance gains.\nFurthermore, this strategy even leads to negative transfer on many downstream tasks (2 out of 8\nmolecular datasets and 13 out of 40 protein prediction tasks).\nWe develop an effective strategy for pre-training GNNs. The key idea is to use easily accessible\nnode-level information and encourage GNNs to capture domain-specific knowledge about nodes and\nedges, in addition to graph-level knowledge. This helps the GNN to learn useful representations\nat both global and local levels (Figure 1 (a.iii)), and is crucial to be able to generate graph-level\nrepresentations (which are obtained by pooling node representations) that are robust and transferable\nto diverse downstream tasks (Figure 1). Our strategy is in contrast to na\u00efve strategies that either\nleverage only at graph-level properties (Figure 1 (a.ii)) or node-level properties (Figure 1 (a.i)).\nEmpirically, our pre-training strategy used together...",
      "url": "https://arxiv.org/pdf/1905.12265"
    },
    {
      "title": "Learning to Group Auxiliary Datasets for Molecule",
      "text": "# Quantitative Biology > Biomolecules\n\n**arXiv:2307.04052** (q-bio)\n\n\\[Submitted on 8 Jul 2023 ( [v1](https://arxiv.org/abs/2307.04052v1)), last revised 8 Nov 2023 (this version, v2)\\]\n\n# Title:Learning to Group Auxiliary Datasets for Molecule\n\nAuthors: [Tinglin Huang](https://arxiv.org/search/q-bio?searchtype=author&query=Huang,+T), [Ziniu Hu](https://arxiv.org/search/q-bio?searchtype=author&query=Hu,+Z), [Rex Ying](https://arxiv.org/search/q-bio?searchtype=author&query=Ying,+R)\n\nView a PDF of the paper titled Learning to Group Auxiliary Datasets for Molecule, by Tinglin Huang and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2307.04052)\n\n> Abstract:The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mechanism optimized through a bi-level optimization framework. Empowered by the meta gradient, the routing mechanism is optimized toward maximizing the target dataset's performance and quantifies the affinity as the gating score. As a result, MolGroup is capable of predicting the optimal combination of auxiliary datasets for each target dataset. Our extensive experiments demonstrate the efficiency and effectiveness of MolGroup, showing an average improvement of 4.41%/3.47% for GIN/Graphormer trained with the group of molecule datasets selected by MolGroup on 11 target molecule datasets.\n\n|     |     |\n| --- | --- |\n| Comments: | Accepted at NeurIPS 2023, Camera Ready Version |\n| Subjects: | Biomolecules (q-bio.BM); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2307.04052](https://arxiv.org/abs/2307.04052) \\[q-bio.BM\\] |\n|  | (or [arXiv:2307.04052v2](https://arxiv.org/abs/2307.04052v2) \\[q-bio.BM\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2307.04052](https://doi.org/10.48550/arXiv.2307.04052)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tinglin Huang \\[ [view email](https://arxiv.org/show-email/37c649b2/2307.04052)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2307.04052v1)**\nSat, 8 Jul 2023 22:02:22 UTC (10,446 KB)\n\n**\\[v2\\]**\nWed, 8 Nov 2023 23:03:35 UTC (10,454 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Learning to Group Auxiliary Datasets for Molecule, by Tinglin Huang and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2307.04052)\n- [TeX Source](https://arxiv.org/src/2307.04052)\n- [Other Formats](https://arxiv.org/format/2307.04052)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nq-bio.BM\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2307.04052&function=prev&context=q-bio.BM)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2307.04052&function=next&context=q-bio.BM)\n\n[new](https://arxiv.org/list/q-bio.BM/new) \\| [recent](https://arxiv.org/list/q-bio.BM/recent) \\| [2023-07](https://arxiv.org/list/q-bio.BM/2023-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2307.04052?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2307.04052?context=cs.LG)\n\n[q-bio](https://arxiv.org/abs/2307.04052?context=q-bio)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2307.04052)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2307.04052)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2307.04052)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2307.04052&description=Learning to Group Auxiliary Datasets for Molecule) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2307.04052&title=Learning to Group Auxiliary Datasets for Molecule)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2307.04052) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2307.04052"
    },
    {
      "title": "Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation",
      "text": "Enhancing Molecular Property Prediction with Auxiliary\nLearning and Task-Specific Adaptation\nVishal Dey\nThe Ohio State University, Columbus\ndey.78@osu.edu\nXia Ning\nThe Ohio State University, Columbus\nning.104@osu.edu\nAbstract\nPretrained Graph Neural Networks (GNNs) have been widely adopted for various\nmolecular property prediction tasks. Despite their ability to capture rich chemical\nknowledge, traditional finetuning of such pretrained GNNs on the target task\ncan lead to poor generalization. To address this, we explore the adaptation of\npretrained GNNs to the target task by jointly training them with multiple auxiliary\ntasks. This could enable the GNNs to learn both general and task-specific features,\nwhich may benefit the target task. However, an effective adaptation strategy\nneeds to determine the relevance of auxiliary tasks with the target task, which\nposes a major challenge. In this regard, we investigate multiple strategies to\nadaptively combine task gradients or learn task weights via bi-level optimization.\nOur experiments with state-of-the-art pretrained GNNs demonstrate the efficacy\nof our proposed methods, with improvements of up to 8.45% over finetuning.\nOverall, this suggests that incorporating auxiliary tasks along with target task\nfinetuning can be an effective way to improve the generalizability of pretrained\nGNNs for molecular property prediction tasks, and thus inspires future research.\n1 Introduction\nAccurate prediction of molecular properties is pivotal in drug discovery[1], as it accelerates the\nidentification of potential molecules with desired characteristics. Developing computational models\nfor property prediction relies on learning effective representations of molecules[2]. In this regard,\nGraph Neural Networks (GNNs) have shown impressive results in learning effective representations\nfor molecular property prediction tasks[3\u20137]. Inspired by the paradigm of pretraining followed by\nfinetuning in large language models (LLMs)[8, 9], molecular GNNs are often pretrained[10] on a\nlarge corpus of molecules (for literature review, refer to Appendix A.1), which might encompass\nirrelevant data for the target property prediction task (e.g., toxicity). This can lead the GNNs to learn\nfeatures that do not benefit the target task. Consequently, pretrained GNNs are finetuned with the\ntarget task to encode task-specific features. However, vanilla finetuning can potentially lead to poor\ngeneralization, particularly when dealing with diverse downstream tasks, limited data, and the need\nto generalize across varying scaffold distributions[11].\nTo improve generalization, auxiliary learning has recently garnered attention[12, 13], notably in\nthe domains of natural language processing (NLP)[14] and computer vision[15, 16]. Auxiliary\nlearning leverages informative signals from self-supervised tasks on unlabeled data, to improve the\nperformance on the target tasks (for a brief literature review, refer to Appendix A.2). Following\nthis line of work, we explore how to adapt pretrained molecular GNNs by combining widely-used\nself-supervised tasks with the target task using respective task-specific data (with self-supervised and\ntarget task labels). Our contribution lies in a preliminary investigation of multiple adaptation strategies\nfor pretrained molecular GNNs in molecular property prediction using well-established concepts\nof auxiliary learning. The significance of our contribution lies in addressing the limited benefit of\npretrained GNNs[17], and in improving generalizability across a diverse set of downstream tasks with\nlimited data. Overall, our proposed adaptation strategies improved the target task performance by as\nmuch as 8.45% over vanilla finetuning. Moreover, our findings indicate that the proposed adaptation\nDey et al., Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\n(Extended Abstract). Presented at the Second Learning on Graphs Conference (LoG 2023), Virtual Event,\nNovember 27\u201330, 2023.\nEnhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\nGNN\nPretraining\nTask 1\nPretraining\nTask \nPretraining\nTask \nRandomly Initialized GNN:\nPretrained GNN:\n(a) Pretraining Stage\nGNN\nTarget Auxiliary Auxiliary \nAdaptation Strategy\nTask-specific\nparamaeters\nPretrained GNN\n(b) Adaptation Stage\nFigure 1: Off-the-shelf available pretrained GNNs are transferred for target task-specific adaptation.\nstrategies are particularly effective in tasks with limited labeled data, which is a common challenge in\nmolecular property prediction tasks.\n2 Methods\nMotivated by the success of continued pretraining and adaptation in pretrained Large Language\nModels (LLMs) [18\u201320], we investigate adaptation of off-the-shelf pretrained molecular GNNs (e.g.,\nSupervised+ContextPred[10] denoted as Sup-C) to target molecular property prediction tasks. Via\nsuch an adaptation, we aim to leverage existing self-supervised (SSL) tasks designed for molecular\nGNNs and transfer learned knowledge from such tasks to the target task. We employ the existing SSL\ntasks typically used in molecular pretraining such as masked atom prediction (AM), edge prediction\n(EP), context prediction (CP), graph infomax (IG), and motif prediction (MP) (detailed in Appendix\nB.1). We refer to these tasks as auxiliary tasks. Intuitively, these auxiliary tasks can potentially\ncapture diverse chemical semantics and rich structural patterns at varying granularities. By utilizing\nSSL objectives on target task-specific data, auxiliary tasks augment the pretrained GNNs with richer\nrepresentations. Such representations, in turn, can improve the generalizability of the target property\nprediction task. Henceforth, the term \u201cGNN\u201d refers to off-the-shelf pretrained molecular GNN.\nFigure 1 presents an overview of the adaptation setup. Formally, we adapt a GNN with parameters \u0398\nto optimize the performance on the target task Tt. We achieve this by jointly training Tt with auxiliary\ntasks {Ta}\nk\ni=1:\nmin\n\u0398,\u03a8,\u03a6i\u2208{1..k}\nLt +\nX\nk\ni=1\nwiLa,i,\nwhere Lt and La,i denote the target task loss and i-th auxiliary task loss, respectively, and \u03a8 and\n\u03a6i\u2208{1,...,k} denotes task-specific learnable parameters for the target and k-auxiliary tasks, respec\u0002tively, and w indicates the influence of the auxiliary tasks on the target task. Through the above\noptimization, all the parameters are simultaneously updated in an end-to-end manner. Note that the\nabove optimization does not optimize w\u2013 we will introduce an approach that can additionally learn\nw. In fact, the key to effective adaptation lies in accurately determining w, such that the combined\ntask gradients can backpropagate relevant training signals to the shared GNN as follows:\n\u0398\n(t+1) := \u0398(t) \u2212 \u03b1\n\u0012\ngt +\nXk\ni=1\nwiga,i\u0013,\nwhere gt = \u2207\u0398Lt, and ga,i = \u2207\u0398La,i denote the gradients updating \u0398 from the target and i-th\nauxiliary task, respectively, and \u03b1 denotes the learning rate. We experiment with multiple strategies\nto adaptively combine task gradients, or learn adaptive w, as opposed to using fixed weights or\nconducting expensive grid-search to explore all possible w.\n2.1 Gradient Cosine Similarity (GCS)\nOne such strategy to meaningfully combine task gradients is based on gradient cosine similarity\n(GCS). Intuitively, GCS measures the alignment between task gradients during training, providing\ninsights into the relatedness of auxiliary tasks with the target task. High GCS indicates that the\nauxiliary tasks provide complementary information and thus, can benefit the target task. Conversely,\nlow GCS indicates potential orthogonality or even conflict between tasks. Thus, GCS can naturally\nbe used to quantify the relatedness of auxiliary tasks with the target task over the course of training.\nWe compute GCS and update \u0398 as:\n\u0398\n(t+1) := \u0398(t) \u2212 \u03b1\n\u0012\ngt +\nXk\ni=1\nmax (0, cos (gt, ga,i)) ga,i)\n\u0013\n,\n2\nEnhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation\nwhere, in essence, we drop the tasks with ...",
      "url": "https://openreview.net/pdf/41e1a95e6fc281b863d44eb407ff5a07d8f83bef.pdf"
    },
    {
      "title": "",
      "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nOOD-GNN: Out-of-Distribution Generalized\nGraph Neural Network\nHaoyang Li, Xin Wang, Member, IEEE, Ziwei Zhang, Member, IEEE, Wenwu Zhu, Fellow, IEEE\nAbstract\u2014Graph neural networks (GNNs) have achieved impressive performance when testing and training graph data come from\nidentical distribution. However, existing GNNs lack out-of-distribution generalization abilities so that their performance substantially\ndegrades when there exist distribution shifts between testing and training graph data. To solve this problem, in this work, we propose an\nout-of-distribution generalized graph neural network (OOD-GNN) for achieving satisfactory performance on unseen testing graphs that\nhave different distributions with training graphs. Our proposed OOD-GNN employs a novel nonlinear graph representation decorrelation\nmethod utilizing random Fourier features, which encourages the model to eliminate the statistical dependence between relevant and\nirrelevant graph representations through iteratively optimizing the sample graph weights and graph encoder. We further present a global\nweight estimator to learn weights for training graphs such that variables in graph representations are forced to be independent. The\nlearned weights help the graph encoder to get rid of spurious correlations and, in turn, concentrate more on the true connection between\nlearned discriminative graph representations and their ground-truth labels. We conduct extensive experiments to validate the\nout-of-distribution generalization abilities on two synthetic and 12 real-world datasets with distribution shifts. The results demonstrate that\nour proposed OOD-GNN significantly outperforms state-of-the-art baselines.\nIndex Terms\u2014Graph Representation Learning, Graph Neural Networks, Out-of-Distribution Generalization.\nF\n1 INTRODUCTION\nGRAPH structured data is ubiquitous in the real world,\ne.g., biology networks [1], social networks [2], molec\u0002ular graphs [3], knowledge graphs [4], etc. Recently, deep\nlearning models on graphs, especially graph neural networks\n(GNNs) [5\u20137], have increasingly emerged as prominent ap\u0002proaches for representation learning of graphs [8]. Significant\nmethodological advances have been made in the field of\nGNNs, which have achieved promising performance in a\nwide variety of applications [9\u201312].\nDespite their enormous success, the existing GNN ap\u0002proaches for graph representation learning generally assume\nthat the testing and training graph data are independently\nsampled from the identical distribution, i.e., the I.I.D. assump\u0002tion. In many real-world scenarios, however, it is difficult\nto guarantee this assumption to be valid. In particular, the\ntesting distribution may suffer unobserved or uncontrolled\nshifts compared with the training distribution. For example,\nin the field of drug discovery, the prediction of biochemical\nproperties of molecules is commonly trained on limited\navailable experimental data, but the model needs to be\ntested on an extraordinarily diverse and combinatorially\nlarge universe of candidate molecules [13, 14]. The model per\u0002formance of existing methods can be substantially degraded\nunder distribution shifts due to the lack of out-of-distribution\n(OOD) generalization ability in realistic data splits [3, 15].\nTherefore, it is of paramount importance to learn GNNs\ncapable of out-of-distribution generalization and achieve\nrelatively stable performances under distribution shifts,\n\u2022 H. Li, X. Wang, Z. Zhang and W. Zhu are with the Department\nof Computer Science and Technology in Tsinghua University, Bei\u0002jing, China. E-mail: lihy18@mails.tsinghua.edu.cn, {xin_wang, zwzhang,\nwwzhu}@tsinghua.edu.cn Corresponding authors: X. Wang and W. Zhu\nManuscript received April 19, 2005; revised August 26, 2015.\nespecially for some high-stake applications, e.g., medical\ndiagnosis [16], criminal justice [17], financial analysis [18],\nand molecular prediction [3], etc.\nSome pioneering works [19\u201321] focus on the size gen\u0002eralization problem by testing on larger graphs than the\ntraining graphs. Besides size generalization, the capability of\nout-of-distribution generalization for GNNs is not explored\nuntil recently [22]. In out-of-distribution scenarios, when\nthere exist complex heterogeneous distribution shifts, the\nperformance of current GNN models can degrade substan\u0002tially, which is mainly induced by the spurious correlations.\nThe spurious correlations intrinsically come from the subtle\ncorrelations between irrelevant representations and relevant\nrepresentations [23, 24]. For example, in the field of drug\ndiscovery (see Figure 1c), the GNN models trained on\nmolecules with one group of scaffolds (two-dimensional\nstructural frameworks of molecules) may learn the spurious\ncorrelations between the scaffolds and labels (i.e., whether\nsome drug can inhibit HIV replication) [3, 15]. When tested\non molecules with different scaffolds (out-of-distribution\ntesting molecules), the existing GNN models may make\nincorrect predictions based on the spurious correlations.\nIn this paper, we propose to learn decorrelated graph\nrepresentations through sample reweighting [25, 26] to\neliminate the dependence between irrelevant and relevant\nrepresentations, which is one of the major causes of degrad\u0002ing model performance under distribution shifts. However,\nlearning decorrelated graph representations to improve out\u0002of-distribution generalization for GNNs is fundamentally\ndifferent from traditional methods and thus remains largely\nunexplored and challenging. Specifically, it poses the follow\u0002ing challenges.\n\u2022 GNNs fuse heterogeneous information from node fea\u0002tures and graph structures such that the complex and\nThis article has been accepted for publication in IEEE Transactions on Knowledge and Data Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TKDE.2022.3193725\n\u00a9 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Tsinghua University. Downloaded on July 26,2022 at 14:02:04 UTC from IEEE Xplore. Restrictions apply.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\n(a) TRIANGLES (b) MNIST-75SP: Super-pixel Graphs\n(c) OGB Molecule Dataset [27]. For validating OOD generalization, this dataset is split based on the scaffolds\n(i.e., two-dimensional structural frameworks) of molecules. The testing set consists of structurally distinct\nmolecules with scaffolds that are not in the training set. Please refer to Section 4.1.2 for more details.\nFig. 1: Examples of out-of-distribution testing graphs under complex distribution shifts. Figure 1a denotes the models are\ntrained on small graphs but tested on larger graphs. Figure 1b denotes the models trained with clean node features but\ntested with noisy features. Figure 1c represents a more realistic and challenging case, i.e., distribution shifts exist on both\ngraph structures and node features.\nunobserved non-linear dependencies among represen\u0002tations are much more difficult to be measured and\neliminated than the linear cases for decorrelation of\nnon-graph data.\n\u2022 Although sample reweighting is effective on small\ndatasets, for real-world large-scale graphs, it is inefficient\nor even infeasible to consistently learn a global weight\nfor each graph in the dataset due to the high computa\u0002tional complexity and excessive storage consumption.\nTo tackle these challenges, we propose a novel out-of\u0002distribution generalized graph neural network (OOD-GNN)\ncapable of handling graph distribution shifts in complex and\nheterogeneous situations. In particular, we first propose to\neliminate the statistical dependence between relevant and\nirrelevant graph representations of the graph encoder by a\nnovel nonlinear graph representation decorrelation me...",
      "url": "https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2022_Out-of-Distribution%20Generalized%20Graph%20Neural%20Network.pdf"
    }
  ]
}