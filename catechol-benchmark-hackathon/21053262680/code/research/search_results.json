{
  "query": "What is the best approach to achieve low MSE on the catechol benchmark hackathon for predicting chemical reaction yields with unseen solvents?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the\u202fCatechol\u202fBenchmark papers and the accompanying dataset released by Imperial\u202fCollege\u202fand SOLVE\u202fChemistry, the most effective way to minimise mean\u2011squared error (MSE) on the hackathon \u2013 especially when the test set contains solvents that were not seen during training \u2013 is to combine a graph\u2011neural\u2011network (GNN) reaction encoder with continuous solvent descriptors, and to leverage transfer\u2011learning and active\u2011learning loops for few\u2011shot generalisation\u202f([arXiv\u202f2512.19530](https://arxiv.org/abs/2512.19530); [arXiv\u202f2506.07619](https://arxiv.org/pdf/2506.07619); Imperial news\u202f2025)\u202f.\n\n**Step\u2011by\u2011step workflow (\u22486 steps)**  \n\n1. **Load the Catechol transient\u2011flow dataset** \u2013 it contains >\u202f1200 reaction\u2011condition triples (reactants, catalyst, solvent, temperature, time) with measured yields and continuous solvent parameters\u202f([Imperial\u202f2025](https://www.imperial.ac.uk/news/articles/2025/imperial-and-solve-chemistry-bring-chemical-reaction-data-to-machine-learning)).  \n\n2. **Encode reactions as graphs** \u2013 represent each molecule (reactants, products, solvent) as a molecular graph and feed them to a message\u2011passing GNN (e.g., DimeNet, GCN). Include the solvent node with its continuous physicochemical features (dielectric constant, polarity, viscosity, etc.) as described in the GNN benchmark\u202f([arXiv\u202f2512.19530](https://arxiv.org/abs/2512.19530)).  \n\n3. **Pre\u2011train the GNN on a large public yield dataset** (e.g., USPTO or the 41\u202fk amide\u2011coupling set) to learn generic reaction\u2011level embeddings, then fine\u2011tune on the Catechol data. This transfer\u2011learning step has been shown to improve regression performance on the benchmark\u202f([arXiv\u202f2506.07619](https://arxiv.org/pdf/2506.07619)).  \n\n4. **Apply few\u2011shot learning for unseen solvents** \u2013 during fine\u2011tuning, treat each new solvent as a \u201cnew class\u201d and use a prototypical\u2011network head or meta\u2011learning (MAML) so the model can adapt with only a handful of examples.  \n\n5. **Iteratively perform active learning** \u2013 after an initial model is trained, query the most uncertain solvent\u2011condition points (e.g., highest predictive variance) from the remaining pool, run the corresponding experiments (or use the provided simulated data), add them to the training set, and retrain. This strategy reduces MSE on out\u2011of\u2011distribution solvents\u202f([arXiv\u202f2506.07619](https://arxiv.org/pdf/2506.07619)).  \n\n6. **Validate and tune hyper\u2011parameters** \u2013 evaluate on a held\u2011out set of unseen solvents using MSE; adjust learning rate, GNN depth, and the weighting of solvent features until the lowest MSE is achieved.  \n\nFollowing this pipeline\u2014graph\u2011based reaction encoding with continuous solvent features, transfer\u2011learning from a large yield corpus, few\u2011shot/meta\u2011learning for new solvents, and an active\u2011learning loop for targeted data acquisition\u2014has been identified as the best practice for attaining the lowest possible MSE on the Catechol benchmark hackathon.",
      "url": ""
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\nMachine learning has promised to change the landscape of laboratory chem\u0002istry, with impressive results in molecular property prediction and reaction retro\u0002synthesis. However, chemical datasets are often inaccessible to the machine\nlearning community as they tend to require cleaning, thorough understanding of the\nchemistry, or are simply not available. In this paper, we introduce a novel dataset\nfor yield prediction, providing the first-ever transient flow dataset for machine\nlearning benchmarking, covering over 1200 process conditions. While previous\ndatasets focus on discrete parameters, our experimental set-up allow us to sample\na large number of continuous process conditions, generating new challenges for\nmachine learning models. We focus on solvent selection, a task that is particularly\ndifficult to model theoretically and therefore ripe for machine learning applica\u0002tions. We showcase benchmarking for regression algorithms, transfer-learning\napproaches, feature engineering, and active learning, with important applications\ntowards solvent replacement and sustainable manufacturing.\n1 Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u0002powering the world of the natural sciences: from famous examples such as AlphaFold for protein\npredictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\ndiscovery [5], among many more. However, we seldom see the machine learning community bench\u0002mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\ndata, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\nexpensive the data can be to produce, resulting in many datasets being locked behind closed doors by\nlarge companies.\nAIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u0002ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\naddressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\nChemistry (https://www.solvechemistry.com), we present a first important step into addressing\nthe dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\nmachine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\nbeing the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\na drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nPreprint.\narXiv:2506.07619v2 [cs.LG] 27 Nov 2025\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\nsolvents and for improved solvent replacement tools. However, most of the solvent replacement tools\nfocus purely on learning unsupervised representations of solvents, with the hope that experimentalists\ncan find solvents with similar properties to replace those with environmental concerns. A much\nstronger approach would consider the interaction of a variety of different solvents with a reaction of\ninterest to directly predict reaction yields, in such a way that the best possible solvent can be selected\naccording to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical\nreaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n[9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\ndue to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\nprediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\nlack the continuous reaction conditions, such as temperature and residence time, that are required to\nscale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that\nallows for quick and efficient screening of continuous reaction conditions. We specifically provide\nyield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\nmeasurements across the residence time, temperature, and solvent space. We answer the call for\nmore flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\nchallenges to current machine learning methods for chemistry, and identify potential solutions.\n1.1 Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning\nbenchmarking tends to be poor. This can be a result of improper formatting or documentation,\nincomplete information about reaction conditions or the experimental set-up, or the lack of machine\nreadability, leading to limited usage by the ML community. However, some effort has been made\nto address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n[18], a repository containing over 2M different reactions, many of which come from US patent data\n(USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\nlearning readiness and data inconsistencies across reactions.\nORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\ndataset for forward and retro-synthetic prediction using transformers; however, it also shows that\nyield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\nconclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\nconcentrations, and duration have a significant effect on yield. The assumption that every reaction in\nthe dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\nmodels for yield, and highlighting the importance of creating datasets with full (including potentially\nsub-optimal) reaction conditions.\nMore relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u0002coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\nand Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\npurposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\nand Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n[16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\nfocus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\nand residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\nour case temperature and residence time), as well as providing a pseudo-continuous representation of\nsolvents themselves through the use of solvent mixtures.\nPerhaps the closest example to our dataset is presented in Nguyen et al. [24], who used high\u0002throughput experimentation to screen 12708 catal...",
      "url": "https://arxiv.org/pdf/2506.07619"
    },
    {
      "title": "Imperial and SOLVE Chemistry bring chemical reaction data to machine learning",
      "text": "Imperial and SOLVE Chemistry bring chemical reaction data to machine learning | Imperial News | Imperial College London[Skip to main content](#content)[View accessibility support page](https://www.imperial.ac.uk/about-the-site/accessibility/)\n[](https://www.imperial.ac.uk/)\nSearchSearchMenu\nSearch\n## Website navigation\nClose[](https://www.imperial.ac.uk/)\nKey linksThis section\n[Skip to section navigation](#section-nav)\n* [News](https://www.imperial.ac.uk/news/)\n* [Science](https://www.imperial.ac.uk/news/articles/?topic=Science)# Imperial and SOLVE Chemistry bring chemical reaction data to machine learning\nby[David Silverman](#authorbox)\n22 December 2025\n![Three people in lab coats](https://www.imperial.ac.uk/news/media/news-images/3000x2000-main-article-images/solve-neurips.jpg)Team members from SOLVE Chemistry\nResearchers from Imperial and its spinout company SOLVE Chemistry have presented a chemical dataset at the prestigious AI conference NeurIPS that could help accelerate the use of machine learning to solve solvent challenges in industrial chemistry.\nIndustrial chemists often use prior data to help predict reaction outcomes such as how a certain solvent or temperature setting will perform in a manufacturing process. But existing datasets are patchy &ndash; for example, they typically only include certain solvents and certain temperatures. They are therefore not powerful enough to reliably predict the best way to produce a chemical.\nThe new dataset contains comprehensive data on one industrially relevant reaction, catechol rearrangement, that could be used to effectively train machine learning algorithms to predict which solvents and conditions will give the best yields. It could also make it possible to train models that find the highest\u2011yielding options within a shortlist of more sustainable solvents or at the lowest feasible temperature.\nThe project to acquire and test the new dataset was initiated by[Professor Kim Jelfs](https://profiles.imperial.ac.uk/k.jelfs)in Imperial&rsquo;s[Department of Chemistry](https://www.imperial.ac.uk/chemistry/)and[Professor Ruth Misener](https://profiles.imperial.ac.uk/r.misener)in the[Department of Computing](https://www.imperial.ac.uk/computing/), working as part of[AIChemy](https://aichemy.ac.uk/), an EPSRC AI hub for Chemistry.[SOLVE Chemistry](https://www.solvechemistry.com/)was funded to produce the first publicly available dataset to include dense sampling of a continuous solvent and temperature space using technology developed by the company.\n> The new dataset could be used to effectively train machine learning algorithms to predict which solvents and conditions give the best yields. It could help use more sustainable solvents and lower temperatures.\n> > Imperial researchers led by PhD student[Toby Boyne](https://profiles.imperial.ac.uk/t.boyne23)then demonstrated some ways this data can be used to develop and test predictive algorithms. Mr Boyne said: &ldquo;Predicting the impacts of different solvents is already a significant challenge. By including a range of solvent classes, as well as mixtures of solvents, we hope this dataset inspires the machine learning community to develop models that better capture solvent effects, and are robust across a range of experimental conditions.&rdquo;\nTo gather the data, the researchers used automated flow chemistry techniques developed by SOLVE Chemistry, which was founded by Imperial graduates Dr Linden Schrecker and Dr Jose Pablo Folch from their EPSRC- and BASF-funded PhDs. This enabled them to gather the reaction data continuously as reactions evolved over time.\nIn the case of temperature and residence time, this allowed them to gather dense enough data points to represent the variables as continuous rather than discrete. This could make it easier for machine learning models to detect nonlinear relationships between, for example, temperature and yield.\nIn the case of solvent selection, which is traditionally a categorical variable, they obtained continuous data using solvent mixtures, which allows solvent conditions to be explored in a more varied design space. For example, instead of just testing pure water and pure ethanol, they ran reactions along continuous ramps of water-ethanol mixtures and other solvent blends.\n&ldquo;What makes a solvent good for one reaction may not be what makes it good for another reaction,&rdquo; explained SOLVE Chemistry co-founder and Chief Scientific Officer, Dr Jose Pablo Folch. &ldquo;We&rsquo;re creating a workflow that combines cutting-edge machine learning and unique data collection to quickly uncover solvent effects on reactions of commercial interest.&rdquo;\nThe team put together 1,220 data points, which for chemical data is substantial and has potential to enable an entire class of models that were not previously testable at scale in chemistry. Following their publication in NeurIPS, Gabriel Gibberd in the Department of Chemical Engineering, a graduate from Imperial's[Digital Chemistry](https://www.imperial.ac.uk/study/courses/postgraduate-taught/digital-chemistry/)MSc programme, has used the dataset to achieve an even greater machine learning performance, also accepted into NeurIPS.\n## Hackathon\nThe NeurIPS paper is accompanied by a public hackathon, designed to give researchers early access to the dataset and challenge them to build models that predict unseen reaction outcomes as accurately as possible. &ldquo;Whoever wins will have a solution that will catalyse the next frontier of research in this area,&rdquo; said Dr Linden Schrecker, SOLVE Chemistry&rsquo;s co-founder and CEO.\n[Join the Catechol Benchmark Hackathon](https://www.kaggle.com/competitions/catechol-benchmark-hackathon/overview)\n## Future developments\nThe team aims to make the dataset a springboard for new research in both chemistry and machine learning. By providing a unique, well\u2011curated dataset that combines time\u2011series, temperature-series and continuous solvent spaces, they give machine learning developers a realistic but tractable test bed for ideas in few\u2011shot learning, active learning and representation learning.\nAs researchers compete in the hackathon and build on the open data and code, the expectation is that new modelling strategies will emerge that can then be transferred to other reactions, other materials systems and industrial workflows.\n## Further reading\n[The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning (PDF)](https://arxiv.org/pdf/2506.07619)\n[Catechol Benchmark dataset](https://www.kaggle.com/datasets/aichemy/catechol-benchmark)\n[SoDaDE: Solvent Data-Driven Embeddings with Small Transformer Models (PDF)](https://arxiv.org/pdf/2509.22302?)\n[AIChemy](https://aichemy.ac.uk/)\n## Share this article\n* [](https://www.facebook.com/sharer/sharer.php?u=https://www.imperial.ac.uk/news/articles/2025/imperial-and-solve-chemistry-bring-chemical-reaction-data-to-machine-learning/)\n* [](https://www.linkedin.com/shareArticle?mini=true&url=https://www.imperial.ac.uk/news/articles/2025/imperial-and-solve-chemistry-bring-chemical-reaction-data-to-machine-learning/)\n* Copy link\n## Related tags\n* [Artificial Intelligence](https://www.imperial.ac.uk/news/articles/?tags=Artificial%20Intelligence)\n* [Engineering Chemical Eng](https://www.imperial.ac.uk/news/articles/?tags=Engineering%20Chemical%20Eng)\n* [Engineering Computing](https://www.imperial.ac.uk/news/articles/?tags=Engineering%20Computing)\n* [Enterprise](https://www.imperial.ac.uk/news/articles/?tags=Enterprise)\n* [Entrepreneurship](https://www.imperial.ac.uk/news/articles/?tags=Entrepreneurship)\n## Study subjects\n* [Chemical engineering](https://www.imperial.ac.uk/study/subjects/chemical-engineering/)\n* [Chemistry](https://www.imperial.ac.uk/study/subjects/chemistry/)\n* [Computer science](https://www.imperial.ac.uk/study/subjects/computer-science/)\nArticle text (excluding photos or graphics) &copy; Imperial College London.\nPhotos and graphics subject to third ...",
      "url": "https://www.imperial.ac.uk/news/articles/2025/imperial-and-solve-chemistry-bring-chemical-reaction-data-to-machine-learning"
    },
    {
      "title": "Predicting Chemical Reaction Yields",
      "text": "Predicting Chemical Reaction Yields | RXN yield prediction\n* * [rxn\\_yields](#)\n* [Overview](https://rxn4chemistry.github.io/rxn_yields//)\n* [Data](https://rxn4chemistry.github.io/rxn_yields/data)\n* [Training Tutorial](https://rxn4chemistry.github.io/rxn_yields/model_training)\n* [Evaluation Buchwald Hartwig](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_buchwald_hartwig_yields_prediction)\n* [Evaluation Suzuki Miyaura](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_suzuki_miyaura_yields_prediction)\n* [USPTO Exploration](https://rxn4chemistry.github.io/rxn_yields/uspto_data_exploration)\n# Predicting Chemical Reaction Yields\nPredicting the yield of a chemical reaction from a reaction SMILES using Transformers\nArtificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, successfully became part of the organic chemists\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, reaction yields models have been less investigated, despite the enormous potential of accurately predicting them. Reaction yields models, describing the percentage of the reactants that is converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the dataset applicability in reaction yields predictions.\nThis repository complements our studies on[predicting chemical reaction yields](https://iopscience.iop.org/article/10.1088/2632-2153/abc81d)(published in Machine Learning: Science and Technology) and[data augmentation and uncertainty estimation for yield predictions](https://doi.org/10.26434/chemrxiv.13286741)(presented at the Machine Learning for Molecules Workshop at NeurIPS 2020).\n## Install[](#Install)\nAs the library is based on the chemoinformatics toolkit[RDKit](http://www.rdkit.org)it is best installed using the[Anaconda](https://docs.conda.io/en/latest/miniconda.html)package manager. Once you have conda, you can simply run:\n```\n`conda create -n yields python=3.6 -y\nconda activate yields\nconda install -c rdkit rdkit=2020.03.3.0 -y\nconda install -c tmap tmap -y`\n```\n```\n`git clone https://github.com/rxn4chemistry/rxn\\_yields.git\ncd rxn\\_yields\npip install -e .`\n```\n**NOTE:**\nIf you are fine-tuning your own models. Make sure that the pretrained model (from which you start training) is loaded from a folder with the same structure as for our[rxnfp models](https://github.com/rxn4chemistry/rxnfp/tree/master/rxnfp/models/transformers/bert_pretrained).\n## Approach - predicting yields from reaction SMILES[](#Approach---predicting-yields-from-reaction-SMILES)\nTransformer models have recently revolutionised Natural Language Processing and were also successfully applied to task in chemistry, using a text-based representation of molecules and chemical reactions called Simplified molecular-input line-entry system (SMILES).\nSequence-2-Sequence transformers as in[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)were used for:\n* Chemical Reaction Prediction\n* [Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction](https://pubs.acs.org/doi/full/10.1021/acscentsci.9b00576)\n* [Carbohydrate Transformer: Predicting Regio- and Stereoselective Reactions Using Transfer Learning](http://dx.doi.org/10.26434/chemrxiv.11935635)\n* Multi-step retrosynthesis\n* [Predicting retrosynthetic pathways using a combined linguistic model and hyper-graph exploration strategy](http://dx.doi.org/10.1039/c9sc05704h)\n* [Unassisted Noise-Reduction of Chemical Reactions Data Sets](https://chemrxiv.org/articles/Unassisted_Noise-Reduction_of_Chemical_Reactions_Data_Sets/12395120/1)\nEncoder Transformers like[BERT](https://openreview.net/forum?id=SkZmKmWOWH)and[ALBERT](https://openreview.net/forum?id=H1eA7AEtvS)for:\n* Reaction fingerprints and classification\n* [Mapping the Space of Chemical Reactions using Attention-Based Neural Networks](https://chemrxiv.org/articles/Data-Driven_Chemical_Reaction_Classification_with_Attention-Based_Neural_Networks/9897365)\n* Atom rearrangements during chemical reactions\n* [Unsupervised Attention-Guided Atom-Mapping](https://chemrxiv.org/articles/Unsupervised_Attention-Guided_Atom-Mapping/12298559)\nThose studies show that Transformer models are able to learn organic chemistry and chemical reactions from SMILES.\nHere we asked the question, how well a**BERT**model would perform when applied to a**yield prediction**task:\n![](https://rxn4chemistry.github.io/rxn_yields/images/pipeline.jpg)\n**Figure:**Pipeline and task description.\nTo do so, we started with the reaction fingerprint models from the[rxnfp](https://rxn4chemistry.github.io/rxnfp/)library and added a fine-tuning regression head through[SimpleTransformers.ai](https://simpletransformers.ai). As we don't need to change the hyperparameters of the base model, we only tune the learning rate for the training and the dropout probability.\nWe explored two high-throughput experiment (HTE) data sets and then also the yields data found in the USPTO data base.\n## Buchwald-Hartwig HTE data set[](#Buchwald-Hartwig-HTE-data-set)\n### Canonical reaction representation[](#Canonical-reaction-representation)\nOne of the best studied reaction yield is the one that was published by Ahneman et al. in[Predicting reaction performance in C\u2013N cross-coupling using machine learning](https://science.sciencemag.org/content/360/6385/186.full), where the authors have used DFT-computed descriptors as inputs to different machine learning descriptors. There best model was a random forest model. More recently,[one-hot encodings](https://science.sciencemag.org/content/362/6416/eaat8603)and[multi-fingerprint features (MFF)](https://www.sciencedirect.com/science/article/pii/S2451929420300851)as input representations were investigated. Here, we show competitive results starting simply from a text-based reaction SMILES input to our models.\n![](https://rxn4chemistry.github.io/rxn_yields/images/buchwald_hartwig.jpg)\n**Figure:**a) Summary of the results on the Buchwald\u2013Hartwig data set. b) Example regression plot for the first random-split.\n### Augmentated reaction representations[](#Augmentated-reaction-representations)\nWe were able to further improve the results on this data set using data augmentation on reaction SMILES (molecule order permuations and SMILES randomisations). This extension will be presented at the NeurIPS 2020[Machine Learning for Molecules Workshop](https://nips.cc/Conferences/2020/ScheduleMultitrack?event=16136).\n![](https://rxn4chemistry.github.io/rxn_yields/images/rxn_randomizations.png)\n**Figure:**The two different data augmentation techniques investigated in the NeurIPS workshop paper.\n#### Results[](#Results)\n![](https://rxn4chemistry.github.io/rxn_yields/images/results_augm.png)\n**Figure:**a) Results on the 70/30 random splits, averaged over 10 splits. b) Comparison of DFT descriptors + RF, canonical SMILES and data augmented randomized SMILES on reduced training sets. c) Out-of-sample test sets\nOn random splits 70/30 in a), the data augmented Yield-BERT models perform better than ...",
      "url": "https://rxn4chemistry.github.io/rxn_yields"
    },
    {
      "title": "Training Regression Models on chemical reactions",
      "text": "Training Regression Models on chemical reactions | RXN yield prediction\n* * [rxn\\_yields](#)\n* [Overview](https://rxn4chemistry.github.io/rxn_yields//)\n* [Data](https://rxn4chemistry.github.io/rxn_yields/data)\n* [Training Tutorial](https://rxn4chemistry.github.io/rxn_yields/model_training)\n* [Evaluation Buchwald Hartwig](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_buchwald_hartwig_yields_prediction)\n* [Evaluation Suzuki Miyaura](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_suzuki_miyaura_yields_prediction)\n* [USPTO Exploration](https://rxn4chemistry.github.io/rxn_yields/uspto_data_exploration)\n# Training Regression Models on chemical reactions\nHere we show how simple it is to train reaction BERTs on any regression task.\n## Available tools[](#Available-tools)\nBERT and related transformer models have revolutionised Natural Language Processing. The implementation of such models is conveniently made available through the[Huggingface Transformers](https://github.com/huggingface/transformers)library. We based already based our previous work on reaction fingerprints / classification and atom-mapping on this library. To train the yield regression models in this work, we used the[SimpleTransformers.ai](https://simpletransformers.ai), which contains all you need to add fine-tuning heads on top of transformers, run trainings and evaluations.\n## SmilesTokenizer[](#SmilesTokenizer)\nOne key difference compared to human languages, when compared to chemistry are the tokens and tokenizers. In this work, we use the tokenizer introduced our previous[rxnfp](https://rxn4chemistry.github.io/rxnfp/)work with the same regex as in the[Molecular Transformer](https://github.com/pschwllr/MolecularTransformer).\n```\nfromrxnfp.tokenizationimportget\\_default\\_tokenizer,SmilesTokenizerfromrdkit.ChemimportrdChemReactionssmiles\\_tokenizer=get\\_default\\_tokenizer()reaction\\_smiles=&#39;CC(C)[C@@H](C)CCBr.[Na]C#N&gt;&gt;CC([C@@H](C)CCC#N)C&#39;rxn=rdChemReactions.ReactionFromSmarts(reaction\\_smiles,useSmiles=True)print(smiles\\_tokenizer.tokenize(reaction\\_smiles))rxn\n```\n```\nwandb:WARNINGW&amp;&amp;B installed but not logged in. Run `wandb login` or set the WANDB\\_API\\_KEY env variable.\nSetting &#39;&#39;max\\_len\\_single\\_sentence&#39;&#39; is now deprecated. This value is automatically set up.\nSetting &#39;&#39;max\\_len\\_sentences\\_pair&#39;&#39; is now deprecated. This value is automatically set up.\n```\n```\n[&#39;C&#39;, &#39;C&#39;, &#39;(&#39;, &#39;C&#39;, &#39;)&#39;, &#39;[C@@H]&#39;, &#39;(&#39;, &#39;C&#39;, &#39;)&#39;, &#39;C&#39;, &#39;C&#39;, &#39;Br&#39;, &#39;.&#39;, &#39;[Na]&#39;, &#39;C&#39;, &#39;#&#39;, &#39;N&#39;, &#39;&gt;&gt;&#39;, &#39;C&#39;, &#39;C&#39;, &#39;(&#39;, &#39;[C@@H]&#39;, &#39;(&#39;, &#39;C&#39;, &#39;)&#39;, &#39;C&#39;, &#39;C&#39;, &#39;C&#39;, &#39;#&#39;, &#39;N&#39;, &#39;)&#39;, &#39;C&#39;]\n```\nAs the tokenizer is normally hard-coded in the SimpleTransformers library we need to change it, we therefore create a`SmilesClassificationModel`class, as seen in the[`core`](https://rxn4chemistry.github.io/rxn_yields/core.html)module.\n```\nMODEL\\_CLASSES={&quot;bert&quot;:(BertConfig,BertForSequenceClassification,SmilesTokenizer),}\n```\nOnce this is done, the SimpleTransformers library can be used as usual.\n## Pretrained reaction BERT models[](#Pretrained-reaction-BERT-models)\nThere are currently two reaction BERT models in the`rxnfp`library -`pretrained`(trained with on a reaction MLM task) and`ft`(additionally trained on a reaction classification task). For this example, we will use the`pretrained`model as starting point for the training of our Yield-BERT. On the Buchwald-Hartwig reactions both base models performed similarly.\n```\nimportpkg\\_resourcesimporttorchfromrxnfp.modelsimportSmilesClassificationModelmodel\\_path=pkg\\_resources.resource\\_filename(&quot;rxnfp&quot;,f&quot;&quot;models/transformers/bert\\_pretrained&quot;&quot;# change pretrained to ft to start from the other base model)yield\\_bert=SmilesClassificationModel(&#39;bert&#39;,model\\_path,use\\_cuda=torch.cuda.is\\_available())\n```\n```\nSetting &#39;&#39;max\\_len\\_single\\_sentence&#39;&#39; is now deprecated. This value is automatically set up.\nSetting &#39;&#39;max\\_len\\_sentences\\_pair&#39;&#39; is now deprecated. This value is automatically set up.\n```\n## Prepare the data[](#Prepare-the-data)\nLoad the reaction SMILES and yield values into a DataFrame with columns`['text', 'labels']`.\nThe same procedure could be applied to any reaction (or molecule) regression task.\n```\nimportpandasaspdfromrxn\\_yields.dataimportgenerate\\_buchwald\\_hartwig\\_rxnsdf=pd.read\\_excel(&#39;&#39;../data/Buchwald-Hartwig/Dreher\\_and\\_Doyle\\_input\\_data.xlsx&#39;&#39;,sheet\\_name=&#39;&#39;FullCV\\_01&#39;&#39;)df[&#39;rxn&#39;]=generate\\_buchwald\\_hartwig\\_rxns(df)train\\_df=df.iloc[:2767][[&#39;rxn&#39;,&#39;Output&#39;]]test\\_df=df.iloc[2767:][[&#39;rxn&#39;,&#39;Output&#39;]]#train\\_df.columns=[&#39;text&#39;,&#39;labels&#39;]test\\_df.columns=[&#39;text&#39;,&#39;labels&#39;]mean=train\\_df.labels.mean()std=train\\_df.labels.std()train\\_df[&#39;labels&#39;]=(train\\_df[&#39;labels&#39;]-mean)/stdtest\\_df[&#39;labels&#39;]=(test\\_df[&#39;labels&#39;]-mean)/stdtrain\\_df.head()\n```\n||text|labels|\n0|CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COc1...|1.387974|\n1|Brc1ccccn1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C...|-0.796876|\n2|CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2...|-0.827835|\n3|CCOC(=O)c1cnoc1.CN1CCCN2CCCN=C12.COc1ccc(OC)c(...|-0.464841|\n4|CN1CCCN2CCCN=C12.COc1ccc(Cl)cc1.COc1ccc(OC)c(P...|-1.186082|\n## Hyperparameter tuning[](#Hyperparameter-tuning)\nMost of the hyperparameter are already fixed by the base model. Here we decided only to tune the`dropout probability`and the`learning rate`. SimpleTransformers has[wandb](https://www.wandb.com)nicely integrated. An example how to setup a hyperparameter sweep can be found in the training scripts. The wandb parameters are read using[dotenv](https://pypi.org/project/python-dotenv/).\n## Training[](#Training)\nAs you can also be seen from the training scripts, once the data is in the right shape a training run can be started within a few lines of code.\nFor this example we will go with the following parameters,\n> {dropout=0.7987, learning_rate=0.00009659},\n> and launch a training. We have to reinitiate the BERT model with the correct parameters.\n```\nmodel\\_args={&#39;&#39;num\\_train\\_epochs&#39;&#39;:15,&#39;&#39;overwrite\\_output\\_dir&#39;&#39;:True,&#39;&#39;learning\\_rate&#39;&#39;:0.00009659,&#39;&#39;gradient\\_accumulation\\_steps&#39;&#39;:1,&#39;regression&#39;:True,&quot;&quot;num\\_labels&quot;&quot;:1,&quot;fp16&quot;:False,&quot;&quot;evaluate\\_during\\_training&quot;&quot;:False,&#39;&#39;manual\\_seed&#39;&#39;:42,&quot;&quot;max\\_seq\\_length&quot;&quot;:300,&quot;&quot;train\\_batch\\_size&quot;&quot;:16,&quot;&quot;warmup\\_ratio&quot;&quot;:0.00,&quot;config&quot;:{&#39;&#39;hidden\\_dropout\\_prob&#39;&#39;:0.7987}}model\\_path=pkg\\_resources.resource\\_filename(&quot;rxnfp&quot;,f&quot;&quot;models/transformers/bert\\_pretrained&quot;&quot;# change pretrained to ft to start from the other base model)yield\\_bert=SmilesClassificationModel(&quot;bert&quot;,model\\_path,num\\_labels=1,args=model\\_args,use\\_cuda=torch.cuda.is\\_available())yield\\_bert.train\\_model(train\\_df,output\\_dir=f&quot;&quot;outputs\\_buchwald\\_hartwig\\_test\\_project&quot;&quot;,eval\\_df=test\\_df)\n```\n## Predictions[](#Predictions)\nTo load a trained model and make yield predictions. We change the`model\\_path`to the folder that contains the trained model and use the`predict`method.\n```\nmodel\\_path=&#39;&#39;../trained\\_models/buchwald\\_hartwig/FullCV\\_01\\_split\\_2768/checkpoint-2595-epoch-15&#39;&#39;trained\\_yield\\_bert=SmilesClassificationModel(&#39;bert&#39;,model\\_path,num\\_labels=1,args={&quot;regression&quot;:True},use\\_cuda=torch.cuda.is\\_available())yield\\_predicted=trained\\_yield\\_bert.predict(test\\_df.head(10).text.values)[0]yield\\_predicted=yield\\_p...",
      "url": "https://rxn4chemistry.github.io/rxn_yields/model_training"
    },
    {
      "title": "The challenge of balancing model sensitivity and robustness in predicting yields: a benchmarking study of amide coupling reactions",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2023/sc/d3sc03902a)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/sc/d3sc03539e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/sc/d3sc03641c)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D3SC03902A](https://doi.org/10.1039/D3SC03902A)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2023, **14**, 10835-10846\n\n# The challenge of balancing model sensitivity and robustness in predicting yields: a benchmarking study of amide coupling reactions [\u2020](https://pubs.rsc.org/pubs.rsc.org\\#fn1)\n\nZhen\nLiu\na,\nYurii S.\nMoroz\nbcd and Olexandr\nIsayev\n\\*aaDepartment of Chemistry, Mellon College of Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA. E-mail: [olexandr@olexandrisayev.com](mailto:olexandr@olexandrisayev.com)bEnamine Ltd, Ky\u00efv, 02660, UkrainecChemspace LLC, Ky\u00efv, 02094, UkrainedTaras Shevchenko National University of Ky\u00efv, Ky\u00efv, 01601, Ukraine\n\nReceived\n27th July 2023\n, Accepted 12th September 2023\n\nFirst published on 13th September 2023\n\n## Abstract\n\nAccurate prediction of reaction yield is the holy grail for computer-assisted synthesis prediction, but current models have failed to generalize to large literature datasets. To understand the causes and inspire future design, we systematically benchmarked the yield prediction task. We carefully curated and augmented a literature dataset of 41239 amide coupling reactions, each with information on reactants, products, intermediates, yields, and reaction contexts, and provided 3D structures for the molecules. We calculated molecular features related to 2D and 3D structure information, as well as physical and electronic properties. These descriptors were paired with 4 categories of machine learning methods (linear, kernel, ensemble, and neural network), yielding valuable benchmarks about feature and model performance. Despite the excellent performance on a high-throughput experiment (HTE) dataset (R2 around 0.9), no method gave satisfactory results on the literature data. The best performance was an R2 of 0.395 \u00b1 0.020 using the stack technique. Error analysis revealed that reactivity cliff and yield uncertainty are among the main reasons for incorrect predictions. Removing reactivity cliffs and uncertain reactions boosted the R2 to 0.457 \u00b1 0.006. These results highlight that yield prediction models must be sensitive to the reactivity change due to the subtle structure variance, as well as be robust to the uncertainty associated with yield measurements.\n\n## Introduction\n\nComputer-assisted synthesis prediction (CASP) is a field of computational chemistry that aims to develop algorithms and software tools to assist chemists in predicting the outcomes of chemical reactions. CASP uses machine learning (ML) and artificial intelligence (AI) techniques to predict the feasibility, yield, and optimal conditions for a chemical reaction. Recent exploratory studies in the field of reaction predictions, show applications in retrosynthesis, [1,2](https://pubs.rsc.org/pubs.rsc.org#cit1) product prediction, [3\u20135](https://pubs.rsc.org/pubs.rsc.org#cit3) selectivity, [6](https://pubs.rsc.org/pubs.rsc.org#cit6) and other relevant tasks. [7,8](https://pubs.rsc.org/pubs.rsc.org#cit7) Accurately predicting reaction yields is one of the key objectives in CASP as many reaction-related tasks can be framed as yield optimization problems. Yield serves as the ultimate metric for selecting reagents in a single reaction or planning a synthesis pathway. However, despite its importance, predicting the theoretical yield remains challenging because the yield depends on many observable and unobservable factors throughout the reaction process, including the interaction between molecules, environment conditions, and human operations.\n\nWhile impressive yield prediction performance (R2 is around 0.9) has been achieved in many high-throughput experiment (HTE) datasets, the yield prediction R2 score on large literature datasets is usually unsatisfactory. [9\u201316](https://pubs.rsc.org/pubs.rsc.org#cit9) For example, the Doyle group reported an example of predicting reaction yields with a random forest model on the Buchwald\u2013Hartwig HTE dataset. [9](https://pubs.rsc.org/pubs.rsc.org#cit9) The dataset contains 4608 C\u2013N cross-coupling reactions and the R2 score and mean absolute error (MAE) were 0.92 and 7.8%, respectively. Since then, the dataset has become a standard benchmark dataset for many yield prediction models. Schwaller et al. reported a Yield-BERT model for reaction yield predictions. [10](https://pubs.rsc.org/pubs.rsc.org#cit10) Although the R2 score for the yield prediction task was as high as 0.94 on the Buchwald\u2013Hartwig dataset, [9](https://pubs.rsc.org/pubs.rsc.org#cit9) the performance dropped sharply (i.e., R2 around 0.2) on the literature dataset. [17,18](https://pubs.rsc.org/pubs.rsc.org#cit17) The staggering performance difference of yield prediction on the HTE dataset and the literature dataset is widespread. Recently, Grzybowski [11](https://pubs.rsc.org/pubs.rsc.org#cit11) and Glorius [15](https://pubs.rsc.org/pubs.rsc.org#cit15) studied this phenomenon, suggesting that the unsatisfactory ML performance may be due to the popular trend in the literature dataset induced by human bias in experiment design and result reporting. However, augmenting the dataset with zero or low-yield reactions did not significantly improve the performance, indicating that additional factors might degrade the model performance.\n\nTo understand the causes for failures in a large literature dataset, we systematically investigated the yield prediction task. We tested 4 categories of ML models (i.e., linear methods, kernel methods, ensemble methods, and neural networks) on an HTE yield dataset and a large literature yield dataset. We utilized a set of 4608 Buchwald\u2013Hartwig reactions from Doyle [9](https://pubs.rsc.org/pubs.rsc.org#cit9) et al. to represent the HTE dataset, given its extensive prior modeling. We curated 41239 amide coupling reactions from Reaxys [19](https://pubs.rsc.org/pubs.rsc.org#cit19) to represent the literature dataset. These reactions were chosen due to their significance in medicinal chemistry and the substantial volume of available data. While the Buchwald\u2013Hartwig reactions and the amide coupling reactions are very different, they possess characteristics inherent to the HTE and large literature datasets, respectively. The phenomena observed in the context of Buchwald\u2013Hartwig reactions and amide coupling reactions can be extrapolated to typical HTE datasets and large literature datasets, respectively. Besides the SMILES of reactants and products, the reaction context (i.e., time, temperature, reagents, condition, and solvent) was also extracted where possible from Reaxys to construct the amide coupling dataset. Please note that the reaction yields were extracted as they appeared in the Reaxys database, regardless of the reaction scale. Also, we augmented the literature dataset with reaction intermediates, optimized 3D structures of the molecules, and 2D/3D descriptors derived from the SMILES and conformers. All amide coupling reactions were catalyzed by carbodiimides to minimize irrelevant variables in this investigation. The carbodiimides include 1-ethyl-3-(3-dimethylaminopropyl)carbodiimide (EDC), N,N\u2032-dicyclohexylcarbodiimide (DCC), and N,N\u2032-diisopropylcarbodiimide (DIC). The combination of different reaction descriptors and model categories enabled a systematic yield prediction benchmark, providing insights into the key factors that influence the reaction yield prediction challenge.\n\nOur results demonstrated that most models gave unsatisfactory predictions (R2 < 0.5) in a large and diverse literature dataset even if they achieved excellent predictions (R2 \\> 0.9) on a caref...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/sc/d3sc03902a"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    }
  ]
}