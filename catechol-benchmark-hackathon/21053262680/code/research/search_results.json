{
  "query": "What techniques do Kaggle winners use when the CV-LB intercept is higher than the target score, making the target mathematically unreachable with standard approaches?",
  "results": [
    {
      "title": "ANSWER",
      "text": "When the cross\u2011validation (CV) curve extrapolates above the public leaderboard (LB) target \u2013 i.e., the CV\u2011LB intercept is higher than the score you need \u2013 top Kaggle competitors stop treating the problem as a pure modelling exercise and add \u201cmeta\u2011strategies\u201d that align the validation signal with the hidden test set or even bypass it altogether.\n\n**Robust validation and \u201cladder\u201d tricks** \u2013 Winners first tighten the link between CV and LB by using more realistic split schemes (time\u2011series, group\u2011K\u2011fold, stratified folds that respect leakage patterns) and by limiting the precision of submitted probabilities, a technique described in the Ladder algorithm that reduces over\u2011fitting to the adaptive leaderboard\u202f([mlr.press](https://proceedings.mlr.press/v37/blum15.pdf)).\u202fThey also monitor the CV\u2011LB intercept and, if it is unfavourable, they deliberately inject small amounts of noise or adjust the scoring metric to keep the public score from drifting too far from the true test distribution.\n\n**Oracle\u2011style label inference and hill\u2011climbing** \u2013 When the gap appears mathematically insurmountable, some winners exploit the fact that the competition platform returns a precise loss (or accuracy) for each submission. By crafting batches of predictions whose probabilities are chosen to make the reported log\u2011loss uniquely identify the true labels, they can gradually reconstruct the hidden test labels and then submit near\u2011perfect predictions. This \u201clog\u2011loss oracle\u201d attack was demonstrated on a Kaggle competition, achieving a loss of\u202f0.00000\u202fwithout any real model training\u202f([arxiv.org](https://arxiv.org/abs/1707.01825)).\u202fOnce a set of true labels is recovered, a simple hill\u2011climbing or weighted ensemble (often built with the \u201chill\u2011climbing\u201d algorithm from the Grandmasters Playbook) can push the public score past the original target\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\n**Ensembling and micro\u2011outlier tricks** \u2013 Even without label leakage, winners combine many diverse models (different backbones, feature sets, and training seeds) and fine\u2011tune ensemble weights using a small public\u2011LB subset, a process sometimes called \u201chill\u2011climbing\u201d in the Grandmasters Playbook.\u202fAdditionally, they prune micro\u2011outliers that disproportionately hurt the loss, a low\u2011effort technique that can shave enough points to close the CV\u2011LB gap\u202f([towardsdatascience.com](https://towardsdatascience.com/micro-outlier-removal-climb-high-in-kaggle-with-minimum-effort-using-this-technique-6b5ba9d5014f)).  \n\nTogether, these strategies let winners reach or surpass a target score that would be impossible under standard CV\u2011only modelling, by either making the validation more faithful, exploiting the leaderboard oracle, or aggressively refining ensembles and data cleaning.",
      "url": ""
    },
    {
      "title": "Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:1707.01825** (cs)\n\n\\[Submitted on 6 Jul 2017\\]\n\n# Title:Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle\n\nAuthors: [Jacob Whitehill](https://arxiv.org/search/cs?searchtype=author&query=Whitehill,+J)\n\nView a PDF of the paper titled Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle, by Jacob Whitehill\n\n[View PDF](https://arxiv.org/pdf/1707.01825)\n\n> Abstract:In the context of data-mining competitions (e.g., Kaggle, KDDCup, ILSVRC Challenge), we show how access to an oracle that reports a contestant's log-loss score on the test set can be exploited to deduce the ground-truth of some of the test examples. By applying this technique iteratively to batches of $m$ examples (for small $m$), all of the test labels can eventually be inferred. In this paper, (1) We demonstrate this attack on the first stage of a recent Kaggle competition (Intel & MobileODT Cancer Screening) and use it to achieve a log-loss of $0.00000$ (and thus attain a rank of #4 out of 848 contestants), without ever training a classifier to solve the actual task. (2) We prove an upper bound on the batch size $m$ as a function of the floating-point resolution of the probability estimates that the contestant submits for the labels. (3) We derive, and demonstrate in simulation, a more flexible attack that can be used even when the oracle reports the accuracy on an unknown (but fixed) subset of the test set's labels. These results underline the importance of evaluating contestants based only on test data that the oracle does not examine.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1707.01825](https://arxiv.org/abs/1707.01825) \\[cs.LG\\] |\n|  | (or [arXiv:1707.01825v1](https://arxiv.org/abs/1707.01825v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.1707.01825](https://doi.org/10.48550/arXiv.1707.01825)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jacob Whitehill \\[ [view email](https://arxiv.org/show-email/89a85416/1707.01825)\\]\n\n**\\[v1\\]**\nThu, 6 Jul 2017 14:54:54 UTC (1,059 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle, by Jacob Whitehill\n\n- [View PDF](https://arxiv.org/pdf/1707.01825)\n- [TeX Source](https://arxiv.org/src/1707.01825)\n- [Other Formats](https://arxiv.org/format/1707.01825)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1707.01825&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1707.01825&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2017-07](https://arxiv.org/list/cs.LG/2017-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1707.01825?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.01825)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.01825)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.01825)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#Whitehill17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/Whitehill17)\n\n[Jacob Whitehill](https://dblp.uni-trier.de/search/author?author=Jacob%20Whitehill)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1707.01825&description=Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1707.01825&title=Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1707.01825) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/1707.01825"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Micro-Outlier removal: climb high in Kaggle with minimum effort using this technique",
      "text": "Micro-Outlier removal: climb high in Kaggle with minimum effort using this technique | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n# Micro-Outlier removal: climb high in Kaggle with minimum effort using this technique\nHow I made a big jump with minimum effort in the Kaggle Titanic challenge leaderboard using micro-outlier removal\n[Pranay Dave](https://towardsdatascience.com/author/pranay-dave9/)\nApr 11, 2022\n5 min read\nShare\n![Micro-outlier (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/1dBCvpDRdc08tKiIuq28_zg.png)Micro-outlier (image by author)\nI experienced something amazing. I climbed 8000 ranks on the[Kaggle Titanic competition](https://www.kaggle.com/competitions/titanic)leaderboard in just a few attempts and in 10 minutes.\nAnd here is more amazing news. I did it with minimum effort. I did not do feature engineering. I did not fill in the missing values. I only used a few of the columns. I did not use any complicated machine learning algorithm. Just a plain simple decision tree.\nIn this story, I will tell you**about the magical technique**which made this happen.\nAt the time of my last submission, I was ranked 4057 out of 14404 teams (in the top 30%)\n> The objective of this story is not to develop the best model, but how to climb the leaderboard with minimal effort.\n***Before applying the magical technique &#8211; The rank is 12616***\n![Kaggle rank before (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/1eqU55xsURzClXK2pDbYOzQ.png)Kaggle rank before (image by author)\n***After******applying the magical technique &#8211; The rank is 4057! Woah!***\n![Kaggle rank after (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/1bCn2uu77nn6X5Qc6WB4pdA.png)Kaggle rank after (image by author)\nLet me coin a term for the technique which I have used\n**> Micro-outlier removal\n**\nVoila, the term sounds good. This term does not exist yet. If you are reading this story, then you might be seeing this term for the first time.\n## The motivation behind the micro-outlier removal technique\nEven though we have many techniques to improve machine learning models, sometimes you have the feeling that something is missing. You may say that we have everything &#8211; hyper-parameter optimization, grid-search, and even auto-ml. So what on the earth could be missing?\nWell, for me, the missing thing is an intuition-based visual approach. Augmenting all machine learning optimization techniques with an intuition-based visual approach can really give you the edge to go beyond usual.\n![Left photo Photo by Markus Spiske on [Unsplash](https://unsplash.com/s/photos/detective?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText), Right photo Photo by Ali Hajian on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/04/1EyQgHK7Z5kvsILlFqutfew.png)Left photo Photo by[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText)on[[Unsplash](https://unsplash.com/s/photos/detective?utm\\_source=unsplash&amp;&amp;utm\\_medium=referral&amp;&amp;utm\\_content=creditCopyText)](https://unsplash.com/s/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText), Right photo Photo by[Ali Hajian](https://unsplash.com/@alisvisuals?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText)on Unsplash\nSo let us see what a micro-outlier looks like.\n## Locating Micro-outliers\nFirst, here is some background information about the model training on the Titanic data which I used. To keep things simple,\n* I have taken only the following fields as they are: PClass, Sex, SibSp, Parch, Fare, Embarked.\n* The field age is not taken, as it contains a lot of missing value.\n* There is no feature engineering\n* The machine learning algorithm used is a basic 5-level decision tree with a 70\u201330 train-test split\nShown here is the decision boundary based on the train dataset and decision tree algorithm. The legend in the diagram below indicates the meaning of the colors in the figure below.\n![Decision surface and train data (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/1CoE2iJIS9nhLMIlYTAW64g.png)Decision surface and train data (image by author)\nWe can make the following observations:\nThe decision surface which predicts survival (green area) is mostly located in the middle. The decision surface which predicts non-survival (red area) is mostly located on the sides.\nGenerally, the passengers which did not survive (blue points) are grouped together. Similarly, the passengers which survived (green points) are grouped together.\nThe micro-outliers can be visually located as follows:\n* survivors within a cluster of non-survivors\n* non-survivors within a cluster of survivors\nThe figure below shows**micro-outliers with a white arrow**.\n![Identifying Micro-outliers (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/14z0DdbtF2Ae2UOtQU8Qirg.png)Identifying Micro-outliers (image by author)\nNow let us analyze the micro-outliers.\n## Analyzing the micro-outliers\nIn order to better understand the micro-outliers, let us analyze the micro-outlier situated at the top left. The visual way of analysis is shown in the animated image below. It shows a radar plot for the columns for each point as we hover over the point.\n![Analyzing the micro-outlier (image by author)](https://towardsdatascience.com/wp-content/uploads/2022/04/1J82MhFThc89pRJZSDrvaXQ.gif)Analyzing the micro-outlier (image by author)\nYou will observe that all points are related to passengers who are male, have a high PCLass (which means 3rd class), and those who embarked from port S. All these passengers did not survive, except the micro-outlier point.\nThe micro-outlier[here](https://www.encyclopedia-titanica.org/titanic-survivor/eugene-patrick-daly.html)is the passenger Eugene Patrick Daly. You can read about he survived in the link here\nHe was a 3rd class passenger located on the lower decks and he had jumped in the cold water. He had no chance of surviving. However, he claimed that the thickness of his overcoat attributed to his survival, a garment he held on to for many years and which he named his &quot;lucky coat.&quot;\n![Micro-outlier example Source - https://www.encyclopedia-titanica.org/titanic-survivor/eugene-patrick-daly.html](https://towardsdatascience.com/wp-content/uploads/2022/04/1jOPmASw210I6YwUwPPWCtA.png)Micro-outlier example Source &#8211;[https://www.encyclopedia-titanica.org/titanic-survivor/eugene-patrick-daly.html](https://www.encyclopedia-titanica.org/titanic-survivor/eugene-patrick-daly.html)\n**Though we are happy for him, he is not good for machine learning!**People who luckily survived due to some vague reasons such as the thickness of the overcoat are outliers which messes up the machine learning model. Neither do we have features on who jumped and the thickness of the overcoat for each passenger. So the best thing to do is to remove it from the training data.\nThe micro-outlier visual technique automatically identifies such &#8216;lucky&#8217; persons automatically in the titanic data! You will not be able to do this with any classic outlier detection algorithms.\nI removed 6 micro-outliers, trained the model, and made my submission. There was a big climb in the leadership board compared to submission without the micro-outlier technique.\n## Conclusion\nMicro-outliers removal is a goo...",
      "url": "https://towardsdatascience.com/micro-outlier-removal-climb-high-in-kaggle-with-minimum-effort-using-this-technique-6b5ba9d5014f"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "Allstate Claims Severity Competition, 2nd Place Winner\u2019s Interview: Alexey Noskov",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff4e4ce18fcfc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fallstate-claims-severity-competition-2nd-place-winners-interview-alexey-noskov-f4e4ce18fcfc&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fallstate-claims-severity-competition-2nd-place-winners-interview-alexey-noskov-f4e4ce18fcfc&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**Kaggle Blog**](https://medium.com/kaggle-blog?source=post_page---publication_nav-4b0982ce16a3-f4e4ce18fcfc---------------------------------------)\n\n\u00b7\n\nOfficial Kaggle Blog!\n\n# Allstate Claims Severity Competition, 2nd Place Winner\u2019s Interview: Alexey Noskov\n\n[Kaggle Team](https://medium.com/@kaggleteam?source=post_page---byline--f4e4ce18fcfc---------------------------------------)\n\n7 min read\n\n\u00b7\n\nFeb 27, 2017\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nThe [Allstate Claims Severity recruiting competition](https://web.archive.org/web/20180822210051/https://www.kaggle.com/c/allstate-claims-severity) ran on Kaggle from October to December 2016. As Kaggle\u2019s most popular recruiting competitions to-date, it attracted over 3,000 entrants who competed to predict the loss value associated with Allstate insurance claims.\n\nIn this interview, [Alexey Noskov](https://web.archive.org/web/20180822210051/https://www.kaggle.com/alexeynoskov) walks us through how he came in second place by creating features based on distance from cluster centroids and applying newfound intuitions for (hyper)-parameter tuning. Along the way, he provides details on his favorite tips and tricks including lots of feature engineering and implementing a custom objective function for XGBoost.\n\nPress enter or click to view image in full size\n\n# Background\n\nI have MSc in computer science and work as a software engineer at [Evil Martians](https://web.archive.org/web/20180822210051/https://evilmartians.com/).\n\nI became interested in data science about 4 years ago \u2014 first I watched [Andrew Ng\u2019s famous course](https://web.archive.org/web/20180822210051/https://www.coursera.org/learn/machine-learning), then some others, but I lacked experience with real problems and struggled to get some. But things changed when around beginning of 2015 I got to know Kaggle, which seem to be the missing piece, as it allowed me to get experience in complex problems and learn from the others, improving my data science and machine learning skills.\n\nSo, for two years already I\u2019ve participated in Kaggle competitions as much as I can, and it\u2019s one of the most fun and productive pursuits I\u2019ve had.\n\nI noticed this competition during the end of [Bosch Production Line Performance](https://web.archive.org/web/20180822210051/https://www.kaggle.com/c/bosch-production-line-performance), and I became interested in it because of the moderate data size and mangled data, so I can focus on general methods of building and improving models. So I entered it as soon as I got some time.\n\n# Data preprocessing and feature engineering\n\nFirst, I needed to fix skew in target variable. Initially I applied log-transform, and it worked good enough, but some time after I switched to other transformations like `log(loss + 200)` or `loss ^ 0.25`, which worked somewhat better.\n\nFor features \u2014 first of all, I needed to encode categorical variables. For this I used basic one-hot encoding for some models, but also so-called lexical encoding, when value of encoded category is produced from its name (A becomes 0, B \u2014 1, Z \u2014 26, AA \u2014 27, and so on).\n\nI tried to find some meaningful features, but had no success at it. Also there were [some kernels](https://web.archive.org/web/20180822210051/https://www.kaggle.com/c/allstate-claims-severity) which provided insights into the nature of some variables, and tried to de-mangle them but I couldn\u2019t get any improvement from it. So, I switched to using general automated methods.\n\nThe first of such methods was, of course, SVD, which I\u2019ve applied to numerical variables and one-hot encoded categorical features. It helped to improve some high-variance models, like FM and NN.\n\nSecond, and more complex, was clustering the data and creating a new set of features based on the distance to cluster centers (i.e., applying [RBF](https://web.archive.org/web/20180822210051/http://ccy.dd.ncu.edu.tw/~chen/course/Neural/ch5/RBF.htm) to them) \u2014 it helped to create a bunch of unsupervised non-linear features, which helped to improve most of my models.\n\nAnd third, the last trick I used was forming categorical interaction features, applying lexical encoding to them. These combinations may be easily extracted from XGBoost models by just trying the most important categorical features, or better, analysing the model dump with the excellent [Xgbfi](https://web.archive.org/web/20180822210051/https://github.com/Far0n/xgbfi) tool.\n\n# First-level models\n\nBased on these features, I built a lot of different models which I evaluated using the usual k-fold cross-validation.\n\nFirst of all, there was linear regression, which gave me about 1237.43406 CV / 1223.28163 LB score, which is not very much of course, but provides some baseline. But after adding cluster features to it, it became 1202.70592 CV / 1189.64998 LB, which is much better for such a simple model.\n\nThen, I tried scikit-learn [RandomForestRegressor](https://web.archive.org/web/20180822210051/http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [ExtraTreesRegressor](https://web.archive.org/web/20180822210051/http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) models, of which random forest was best, giving 1199.82233 CV / 1176.44433 LB after some tuning, and improved to 1186.23675 CV / 1166.85340 LB after adding categorical feature combinations. One problem with this model was that however scikit-learn supports MAE loss, it\u2019s very slow and impossible to use, so I needed to use basic MSE, which has some bias in this competition.\n\nThe best model of scikit-learn which helped me was [GradientBoostingRegressor](https://web.archive.org/web/20180822210051/http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), which was able to directly optimize MAE loss and gave me 1151.11060 CV / 1126.30971 LB\n\nI also tried [LibFM](https://web.archive.org/web/20180822210051/https://github.com/srendle/libfm/) model, which gave me 1196.11333 CV / 1155.68632 LB in a basic version and 1177.69251 CV / 1150.37290 LB after adding cluster features to it.\n\nBut the main workhorses of this competitions were, of course, [XGBoost](https://web.archive.org/web/20180822210051/http://xgboost.readthedocs.io/en/latest/) and neural net models:\n\nIn the beginning, my XGBoost models provided something about 1133.00048 CV / 1112.86570 LB, but then I\u2019ve applied some tricks which improved it to 1122.64977 CV / 1105.43686 LB:\n\n- Averaging multiple runs of XGBoost with different seeds \u2014 it helps to reduce model variance;\n- Adding categorical combination features;\n- Modifying objective function to be closer to MAE;\n- Tuning model parameters \u2014 I didn\u2019t have much experience with it before, so [this ...",
      "url": "https://medium.com/kaggle-blog/allstate-claims-severity-competition-2nd-place-winners-interview-alexey-noskov-f4e4ce18fcfc"
    },
    {
      "title": "\u3010Kaggle Method\u3011ISIC2024 9th solution explained",
      "text": "\u3010Kaggle Method\u3011ISIC2024 9th solution explained\n[](https://zenn.dev/)\n[![Yuto](https://storage.googleapis.com/zenn-user-upload/avatar/ede567a447.jpeg)Yuto](https://zenn.dev/yuto_mo)\n\ud83d\udc0a# \u3010Kaggle Method\u3011ISIC2024 9th solution explained\n2024/09/27\u306b\u516c\u958b\n[](<https://twitter.com/intent/tweet?url=https://zenn.dev/yuto_mo/articles/faef74147012df&amp;text=\u3010Kaggle Method\u3011ISIC2024 9th solution explained\uff5cYuto&amp;hashtags=zenn>)[](http://www.facebook.com/sharer.php?u=https://zenn.dev/yuto_mo/articles/faef74147012df)[](<https://b.hatena.ne.jp/add?mode=confirm&amp;url=https://zenn.dev/yuto_mo/articles/faef74147012df&amp;title=\u3010Kaggle Method\u3011ISIC2024 9th solution explained\uff5cYuto>)\n[\n![](https://storage.googleapis.com/zenn-user-upload/topics/fb39b9c866.png)\n\u6a5f\u68b0\u5b66\u7fd2](https://zenn.dev/topics/machinelearning)[\n![](https://storage.googleapis.com/zenn-user-upload/topics/3126e1f253.jpeg)\nKaggle\n](https://zenn.dev/topics/kaggle)[\n![](https://static.zenn.studio/images/drawing/tech-icon.svg)\ntech\n](https://zenn.dev/tech-or-idea)\nThis time, I'll explain the 9th solution of Kaggle's ISIC2024 competition.\n# [](#0.-highlights)0. Highlights\n\u30fbChapter 2: Easy and strong strategy, validation by multiple seeds CV and LB.\n\u30fbChapter 4.1:\nFor diversity, he chose a different backbone.\n\u30fbTwo efficientnet\\_b0 with image size 256. LB: 15.5 and 15.8 with TTA 16.1\n\u30fbswinv2\\_tiny\\_window8\\_256 with image size 256. LB: 15.9\n\u30fbconvnextv2\\_tiny.fcmae\\_ft\\_in22k\\_in1k with image size 224. LB: 15.8\n\u30fbChapter 4.1: Augmentation similar to the previous competition's winner was used. (many of the winners of this competition, also used augmentation similar to the previous competition.)\n\u30fbChapter 4.2: Model ensemble weights from hill-climbing algorithm from[here](https://www.kaggle.com/code/cdeotte/public-lb-1st-place-solution).\n# [](#1.-overview)1. Overview\nHe builds 2 (+1 from public) different tabular pipelines with various features, data, and model combinations paired with predictions from various image model backbones.\nThen he blended the results of these pipelines and made his final predictions. Weights for the different pipelines found via the hill-climbing algorithm.\n# [](#2.-cv-strategy)2. CV Strategy\n\u30fb5-fold StratifiedGroupKfold for each of all experiments\nFor each feature he added, he tried them first with 5 different seed combinations. If they improved the score, he submitted it to the LB. And if they improved the score there as well, he kept that feature.\n# [](#3.-tabular-models)3. Tabular Models\n\u30fb[Baseline](https://www.kaggle.com/code/snnclsr/lgbm-baseline-with-new-features)\n1. Pipeline 1\nThis is his original script where he tried to extract as many contextual features as possible from the patient data.\nHe extracted the Z-score, range of the features, skewness, kurtosis, feature/max (feature), and mean of efficientnet predictions as an image feature.\nHe subsampled the negatives with a ratio of 0.02.\nOne LGBM and one Catboost here.\nThis pipeline gave 18.3 on its own in the public LB with a CV of 17.7.\n2. Pipeline 2\nWith his code, I was swinging between 10th to 20th place in the LB until @greysky 's notebook came out. Quite luckily, we were using the same CV setup. I wanted to try how the blend would perform, so I added a couple of my features to that notebook such as\n```\n`.with\\_columns(n\\_images\\_per\\_location=pl.col(\"isic\\_id\").count().over([\"patient\\_id\",\"tbp\\_lv\\_location\\_simple\"]))`\n```\nand his swin-transformer model's predictions as an image feature. On its own, this pipeline gave 17.5 CV and 18.3 LB. The blend with pipeline 1 gave his first significant boost of 18.6.\n1. Pipeline 3\nFor more diversity, He created rank, and different groupby features on top of the previous features and used a different subset of the data (0.05 negative sampling ratio). This pipeline gave a 17.5 CV and 18.0 LB on its own.### [](#4.1-image-models)4.1 Image models\nFor diversity, he chose a different backbone.\n\u30fbTwo efficientnet\\_b0 with image size 256. LB: 15.5 and 15.8 with TTA 16.1\n\u30fbswinv2\\_tiny\\_window8\\_256 with image size 256. LB: 15.9\n\u30fbconvnextv2\\_tiny.fcmae\\_ft\\_in22k\\_in1k with image size 224. LB: 15.8\nIt seems the little models were performing well.\nAfter many experiments, the consistent configurations are here:\n\u30fbUse 5% of negative data, and upsampled the positives by 10.\n\u30fb3epoch. more epochs make the result worse.\n\u30fblr: 1e-4 for efficient-net and swin-tranformer\n\u30fblr: below scheduler for convnext\n```\n`scheduler = torch.optim.lr\\_scheduler.StepLR(optimizer, step\\_size=1, gamma=0.1)`\n```\n\u30fboptimizer: Adam\n\u30fbloss function: BCE.\n\u30fbaugmentation: similar to the previous competition's winner\n```\n`train\\_transforms=A.Compose([A.VerticalFlip(p=0.5),A.HorizontalFlip(p=0.5),A.RandomBrightnessContrast(brightness\\_limit=0.2,contrast\\_limit=0.2,p=0.75),A.OneOf([A.MotionBlur(blur\\_limit=5),A.MedianBlur(blur\\_limit=5),A.GaussianBlur(blur\\_limit=5),A.GaussNoise(var\\_limit=(5.0,30.0)),],p=0.7),A.OneOf([A.OpticalDistortion(distort\\_limit=1.0),A.GridDistortion(num\\_steps=5,distort\\_limit=1.),A.ElasticTransform(alpha=3),],p=0.7),A.CLAHE(clip\\_limit=4.0,p=0.7),A.HueSaturationValue(hue\\_shift\\_limit=10,sat\\_shift\\_limit=20,val\\_shift\\_limit=10,p=0.5),A.ShiftScaleRotate(shift\\_limit=0.1,scale\\_limit=0.1,rotate\\_limit=15,border\\_mode=0,p=0.85),A.CoarseDropout(p=0.7),A.Resize(CFG.img\\_size,CFG.img\\_size),A.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),ToTensorV2()])valid\\_transforms=A.Compose([A.Resize(CFG.img\\_size,CFG.img\\_size),A.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),ToTensorV2()])`\n```\nSomething funny here at least for the people with a vision background, his image results were stuck at some point at 14.8, and whatever he tried did not work. But moved`A.Resize()`to the end just before the normalization improved his scores to the 15.4-15.5 range.\n### [](#4.2-final-blend)4.2 Final Blend\nTo find the weights of the final models I used the hill-climbing algorithm. By the implementation[here](https://www.kaggle.com/code/cdeotte/public-lb-1st-place-solution)by[@cdeotte](https://www.kaggle.com/cdeotte).\n```\n`model weight0pipe3\\_pred\\_lgb0.4076601pipe3\\_pred\\_xgb0.2725372pipe1\\_pred\\_lgb0.1521623pipe1\\_pred\\_cat0.1424414pipe\\_2\\_pred0.1241995pipe3\\_pred\\_cat-0.099000`\n```\nThe CV of this setup was 18.2 and the LB was 18.7 which was his best CV and LB at the same time.\n# [](#5.-things-did-not-work)5. Things did not work\n\u30fbMany many features\n\u30fbFor image models, he tried to use hard negatives where the models were making the highest errors.\n\u30fbFocal loss performed always worse than BCE. The setup was difficult.\n\u30fbMixup was a nice addition for the diversity but performed usually poorly when added to the ensemble.\n\u30fbStacking additional ExtraTreeClassifier/LogisticRegression model on top of the solution\n\u30fbScaling predictions with \\*\\* 0.5 or rank ensemble.\n\u30fbFixing the tbv\\_lv\\_y. There were 5 patients in the dataset and each of them was from the same hospital with negative tbv\\_lv\\_y value. He added the min per patient to fix it. The CV was slightly better but there was no improvement on the LB.\n\u30fbDullrazor algorithm for hair removal\n\u30fbHair augmentations\n\u30fbSample weights based on lesion id\n\u30fbTraining image models from scratch with dataset mean&amp;std instead of imagenet init\n# [](#6.-summary)6. Summary\nThere were so many things to learn. Looking back on the competition's solutions is a little bit hard, but it is worth spending time on, please try it if you are interested in.\n# [](#7.-references)7. References\n[1][9th Place Solution](https://www.kaggle.com/competitions/isic-2024-challenge/discussion/532577)\n[](<https://twitter.com/intent/tweet?url=https://zenn.dev/yuto_mo/articles/faef74147012df&amp;text=\u3010Kaggle Method\u3011ISIC2024 9th solution explained\uff5cYuto&amp;hashtags=zenn>)[](http://www.facebook.com/sharer.php?u=https://zenn.dev/yuto_mo/articles/faef74147012df)[](<https://b.hatena.ne.jp/add?mode=confirm&amp;url=https://zenn.dev/yuto_mo/articles/faef74147012df&amp;title=\u3010Kaggle Method\u3011ISIC2024 9th solution explained\uff5cYuto>)\n[![Yu...",
      "url": "https://zenn.dev/yuto_mo/articles/faef74147012df"
    },
    {
      "title": "How to Lead a Data Science Contest without Reading the Data - KDnuggets",
      "text": "# How to Lead a Data Science Contest without Reading the Data\n\n[<= Previous post](https://www.kdnuggets.com/2015/05/data-science-workforce-optimization-reducing-employee-attrition.html)\n\n[Next post =>](https://www.kdnuggets.com/2015/05/most-viewed-data-mining-videos-youtube.html)\n\n[Accuracy](https://www.kdnuggets.com/tag/accuracy), [Benchmark](https://www.kdnuggets.com/tag/benchmark), [Competition](https://www.kdnuggets.com/tag/competition), [Kaggle](https://www.kdnuggets.com/tag/kaggle), [Model Performance](https://www.kdnuggets.com/tag/model-performance)\n\nWe examine a \u201cwacky\u201d boosting method that lets you climb the public leaderboard without even looking at the data . But there is a catch, so read on before trying to win Kaggle competitions with this approach.\n\n* * *\n\n![c](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2012%2012'%3E%3C/svg%3E)[comments](https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html#comments)\n\n**By Moritz Hardt**\n\nMachine learning competitions have become an extremely popular format for\u00a0solving prediction and classification problems of all sorts. The most famous\u00a0example is perhaps the Netflix prize. An even better example is\u00a0[Kaggle](https://www.kaggle.com), an awesome startup that\u2019s\u00a0organized more than a hundred competitions over the past few years.\n\nThe central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of _holdout labels_ not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.\n\n![Heritage Prize public leaderboard](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20595%200'%3E%3C/svg%3E)\n\nPublic leaderboard of the Heritage Health Prize ( [Source](http://www.heritagehealthprize.com/c/hhp/leaderboard/public))\n\nIn this post, I will describe a method to climb the public leaderboard _without even looking at the data_. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle\u2019s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some challenges that while fundamental have only recently seen increased attention. A follow-up post will describe a [recent paper](http://arxiv.org/abs/1502.04585) with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.\n\nLet me be very clear that my point is _not_ to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I\u2019m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.\n\n**The Kaggle leaderboard mechanism**\n\nAt first sight, the Kaggle mechanism looks like the classic _holdout method_. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in \\[0,1\\]. Think of the score as prediction error (smaller is better). For concreteness, let\u2019s fix it to be the _misclassification rate_. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in \\[0,1\\].\n\nKaggle further splits its \\\\(N\\\\) private labels randomly into \\\\(n\\\\) holdout labels and \\\\(N-n\\\\) test labels. Typically, \\\\(n=0.3N\\\\). The public leaderboard is a sorting of all teams according to their score computed only on the \\\\(n\\\\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels. I will let \\\\(s\\_H(y)\\\\) denote the public score of a submission \\\\(y\\\\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.\n\n**The cautionary tale of wacky boosting**\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there\u2019s an unknown set of labels \\\\(y\\\\in\\\\{0,1\\\\}^N\\\\) that I need to predict. Well, I know nothing about \\\\(y\\\\). So here\u2019s what I\u2019m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we\u2019re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I\u2019m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here\u2019s what I do:\n\n**Algorithm** (Wacky Boosting):\n\n1. Choose \\\\(y\\_1,\\\\dots,y\\_k\\\\in\\\\{0,1\\\\}^N\\\\) uniformly at random.\n2. Let \\\\(I = \\\\{ i\\\\in\\[k\\] \\\\colon s\\_H(y\\_i) < 0.5 \\\\}\\\\).\n3. Output \\\\(\\\\hat y=\\\\mathrm{majority} \\\\{ y\\_i \\\\colon i \\\\in I \\\\} \\\\), where the majority is component-wise.\n\nLo and behold, this is what happens:\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20595%200'%3E%3C/svg%3E)\n\nIn this plot, \\\\(n=4000\\\\) and all numbers are averaged over 5 independent repetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would\u2019ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at _DeepCompeting.ly_, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso.\n\nTwo months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from _DeepCompeting.ly_ days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.\n\n**What just happened**\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \\\\(y\\_i\\\\) has loss around \\\\(1/2\\\\pm1/\\\\sqrt{n}\\\\). We\u2019re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \\\\(w\\_i\\\\) is roughly \\\\(1/2-c/\\\\sqrt{n}\\\\) for some positive constant \\\\(c>0\\\\). Put differently, each selected \\\\(y\\_i\\\\) is giving us a guess about each label in the unknown holdout set \\\\(H\\\\subseteq \\[N\\]\\\\) that\u2019s corre...",
      "url": "https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) [2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2) 3 [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Model Training**\n\nWe can improve a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forset is the number of trees in the forest and the maximum number of features used in developing each tree.\u00a0**We need to understand how models work and what impact does each parameter have to the model\u2019s performance, be it accuracy, robustness or speed.**\n\nNormally we would find the best set of parameters by a process called\u00a0**[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**. Actually what it does is simply iterating through all the possible combinations and find the best one.\n\nBy the way, random forest usually reach optimum when\u00a0`max_features`\u00a0is set to the square root of the total number of features.\n\nHere I\u2019d like to stress some points about tuning XGB. These parameters are generally considered to have real impacts on its performance:\n\n- `eta`: Step size used in updating weights. Lower\u00a0`eta`\u00a0means slower training but better convergence.\n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration. This is to combat overfitting.\n- `colsample_bytree`: The ratio of features used in each iteration. This is like\u00a0`max_features`\u00a0in\u00a0`RandomForestClassifier`.\n- `max_depth`: The maximum depth of each tree. Unlike random forest,\u00a0**gradient boosting would eventually overfit if we do not limit its depth**.\n- `early_stopping_rounds`: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n2. Set\u00a0`eta`\u00a0to a relatively high value (e.g. 0.05 ~ 0.1),\u00a0`num_round`\u00a0to 300 ~ 500.\n3. Use grid search to find the best combination of other parameters.\n4. Gradually lower\u00a0`eta`\u00a0until we reach the optimum.\n5. **Use the validation set as\u00a0`watch_list`\u00a0to re-train the model with the best parameters. Observe how score changes on validation set in each iteration. Find the optimal value for\u00a0`early_stopping_rounds`.**\n\nFinally, note that models with randomness all have a parameter like\u00a0`seed`\u00a0or\u00a0`random_state`\u00a0to control the random seed.\u00a0**You must record this**\u00a0with all other parameters when you get a good model. Otherwise you wouldn\u2019t be able to reproduce it.\n\n**Cross Validation**\n\n**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**\u00a0is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse.\u00a0**It is widely believed that we should trust our CV scores under such situation.** Ideally we would want\u00a0**CV scores obtained by different approaches to improve in sync with each other and with the LB score**, but this is not always possible.\n\nUsually\u00a0**5-fold CV**\u00a0is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nHow to do CV properly is not a trivial problem. It requires constant experiment and case-by-case discussion. Many Kagglers share their CV approaches (like\u00a0[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)) after competitions when they feel that reliable CV is not easy.\n\n**Ensemble Generation**\n\n[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\u00a0refers to the technique of combining different models. It\u00a0**reduces both bias and variance of the final model**\u00a0(you can find a proof\u00a0[here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)), thus\u00a0**increasing the score and reducing the risk of overfitting**. Recently it became virtually impossible to win prize without using ensemble in Kaggle competitions.\n\nCommon approaches of ensemble learning are:\n\n- **Bagging**: Use different random subsets of training data to train each base model. Then all the base models vote to generate the final predictions. This is how random forest works.\n- **Boosting**: Train base models iteratively, modify the weights of training samples according to the last iteration. This is how gradient boosted trees work. (Actually it\u2019s not the whole story. Apart from boosting, GBTs try to learn the residuals of earlier iterations.) It performs better than bagging but is more prone to overfitting.\n- **Blending**: Use non-overlapping data to train different base models and take a weighted average of them to obtain the final predictions. This is easy to implement but uses less data.\n- **Stacking**: To be discussed next.\n\nIn theory, for the ensemble to perform well, two factors matter:\n\n- **Base models should be as unrelated as possibly**. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.\n- **Performance of base models shouldn\u2019t differ to much.**\n\nActually we have a\u00a0**trade-off**\u00a0here. In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.\n\n**Stacking**\n\nCompared with blending, stacking makes better use of training data. Here\u2019s a diagram of how it works:\n\n[![Stacking](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2099%200'%3E%3C/svg%3E)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)\n\n_(Taken from\u00a0[Faron](https://www.kaggle.com/mmueller). Many thanks!)_\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example. First we split the training data into 5 folds. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold.\u00a0**You have to keep the predictions on the testing data as well.**\u00a0This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data. After 5 iterations we will obtain a matrix of shape\u00a0`#(samples in training data) X #(base models)`. This matrix is then fed to the stacker (it\u2019s just another model) in the second level. After the stacker is fitted, use the predictions on testing data by base models ( **each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape**) as the input for the stacker and obtain our final predictions.\n\nMaybe it\u2019s better to just show the codes:\n\nPrize winners usually have larger and much more complicated ensembles. For beginner, implementing a correct 5-fold stacking is good enough.\n\n**\\*Pipeline**\n\nWe can see that the workflow for a Kaggle competition is quite complex, especially for model selection and ensemble. Ideally, we need a highly automated pipeline capable of:\n\n- **Modularized feature transformations**. We only need to write a few lines of codes (or better, rules / DSLs) and the new feature is added to the training set.\n- **Automated grid search**. We only need to set up models and parameter grid, the search w...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3"
    }
  ]
}