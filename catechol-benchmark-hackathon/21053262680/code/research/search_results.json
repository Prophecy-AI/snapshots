{
  "query": "How do you use frozen ChemBERTa embeddings as features for molecular property prediction with small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Using the ChemBERTa model to generate **frozen embeddings** (i.e., fixed\u2011vector representations) and then feeding those vectors to a lightweight predictor is illustrated in DeepChem\u2019s transfer\u2011learning tutorial and the Molfeat transformer\u2011finetuning guide\u202f([DeepChem](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers); [Molfeat](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n**Steps**\n\n1. **Set up the environment** \u2013 install the required packages:  \n   ```bash\n   mamba install -c conda-forge transformers \"tokenizers <0.13.2\" molfeat pytorch scikit-learn\n   ```  \n   (Molfeat\u2019s guide lists the same dependencies)\u202f([Molfeat](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n2. **Load the pretrained ChemBERTa model** \u2013 use HuggingFace\u2019s `AutoModel` and the matching tokenizer (ByteLevel or Molfeat\u2019s `SmilesConverter`):  \n   ```python\n   from transformers import AutoModel, AutoTokenizer\n   tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n   model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n   model.eval()          # freeze weights\n   ```  \n   (The tutorial notes that ChemBERTa is a RoBERTa\u2011style SMILES transformer)\u202f([DeepChem](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers)).\n\n3. **Convert SMILES strings to token IDs** \u2013 for each molecule in your small dataset:  \n   ```python\n   inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True)\n   ```  \n   (Molfeat\u2019s `SmilesConverter` performs the same step)\u202f([Molfeat](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n4. **Extract frozen embeddings** \u2013 run the model without gradient tracking and pool the hidden states (e.g., mean\u2011pool the last layer) to obtain a fixed\u2011size vector:  \n   ```python\n   with torch.no_grad():\n       outputs = model(**inputs)\n   # mean\u2011pool over token dimension\n   embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n   ```  \n   These vectors are the \u201clearned representations\u201d that DeepChem recommends using as features for downstream tasks\u202f([DeepChem](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers)).\n\n5. **Save the embeddings** (optional) \u2013 e.g., `np.save(\"embeddings.npy\", embeddings)` so they can be reused without re\u2011computing.\n\n6. **Train a simple predictor on the small dataset** \u2013 feed the frozen embeddings to any conventional ML model (random forest, SVM, linear regression, or a shallow neural net):  \n   ```python\n   from sklearn.ensemble import RandomForestRegressor\n   model = RandomForestRegressor()\n   model.fit(embeddings, y_labels)   # y_labels = property values\n   ```  \n   The DeepChem tutorial demonstrates exactly this workflow: using ChemBERTa embeddings as features for graph\u2011convolution or attention models, but a plain scikit\u2011learn model works equally well for limited data\u202f([DeepChem](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers)).\n\n7. **Evaluate** \u2013 predict on a held\u2011out test set using the same embedding extraction pipeline and compute metrics (RMSE, ROC\u2011AUC, etc.) to confirm that the frozen ChemBERTa features improve performance on the small dataset.  \n\nThese steps let you leverage the rich, pretrained ChemBERTa knowledge while keeping the downstream model lightweight\u2014ideal for scenarios where only a few hundred (or fewer) labeled molecules are available.",
      "url": ""
    },
    {
      "title": "Tutorial: ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction using a Smiles Tokenization Strategy\n       \n       \u00b6",
      "text": "DeepChem\n[\n![DeepChem Logo](https://deepchem.io/deepchem.github.io//_next/static/media/deepchem-logo.95f3f074.png)\nDeepChem\n](https://deepchem.io/)\n**\n**## Tutorials\n# Tutorial: ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction using a Smiles Tokenization Strategy[\u00b6](#Tutorial:-ChemBERTa:-Large-Scale-Self-Supervised-Pretraining-for-Molecular-Property-Prediction-using-a-Smiles-Tokenization-Strategy)\n![alt text](https://huggingface.co/front/assets/huggingface_mask.svg)\nBy Seyone Chithrananda ([Twitter](https://twitter.com/SeyoneC))\nDeep learning for chemistry and materials science remains a novel field with lots of potiential. However, the popularity of transfer learning based methods in areas such as natural language processing (NLP) and computer vision have not yet been effectively developed in computational chemistry + machine learning. Using HuggingFace's suite of models and the ByteLevel tokenizer, we are able to train a large-transformer model, RoBERTa, on a large corpus of 10,000,000 SMILES strings from a commonly known benchmark chemistry dataset, PubChem.\nTraining RoBERTa over 10 epochs, the model achieves a pretty good loss of 0.198, and may likely continue to converge if trained for a larger number of epochs. The model can predict masked/corrupted tokens within a SMILES sequence/molecule, allowing for variants of a molecule within discoverable chemical space to be predicted.\nBy applying the representations of functional groups and atoms learned by the model, we can try to tackle problems of toxicity, solubility, drug-likeness, and synthesis accessibility on smaller datasets using the learned representations as features for graph convolution and attention models on the graph structure of molecules, as well as fine-tuning of BERT. Finally, we propose the use of attention visualization as a helpful tool for chemistry practitioners and students to quickly identify important substructures in various chemical properties.\nAdditionally, visualization of the attention mechanism have been seen through previous research as incredibly valuable towards chemical reaction classification. The applications of open-sourcing large-scale transformer models such as RoBERTa with HuggingFace may allow for the acceleration of these individual research directions.\nA link to a repository which includes the training, uploading and evaluation notebook (with sample predictions on compounds such as Remdesivir) can be found[here](https://github.com/seyonechithrananda/bert-loves-chemistry). All of the notebooks can be copied into a new Colab runtime for easy execution. This repository will be updated with new features, such as attention visualization, easier benchmarking infrastructure, and more. The work behind this tutorial has been published on[Arxiv](https://arxiv.org/abs/2010.09885), and was accepted for a**poster presentation at NeurIPS 2020's ML for Molecules Workshop**.\nFor the sake of this tutorial, we'll be fine-tuning a pre-trained ChemBERTa on a small-scale molecule dataset, Clintox, to show the potiential and effectiveness of HuggingFace's NLP-based transfer learning applied to computational chemistry. Output for some cells are purposely cleared for readability, so do not worry if some output messages for your cells differ!\nIn short, there are three major components we'll be going over in this notebook.\n1. Masked token inference predictions on SMILES strings\n2. Attention visualizaiton of the PubChem-10M model\n3. Fine-tuninhg BPE-ChemBERTa and Smiles-Tokenizer ChemBERTa model's on the CLintox toxicity dataset.\n**Don't worry if you aren't familiar with some of these terms. We will explain them later in the tutorial!**\nIf you're looking to dive deeper, check out the poster[here](https://seyonechithrananda.com/ChemBERTa-DeepChem-e1244b82c8fb40bca8c3c882acb9baa5).\n## Colab[\u00b6](#Colab)\nThis tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\n## Setup[\u00b6](#Setup)\nTo run DeepChem within Colab, you'll need to run the following cell of installation commands. This will take about 5 minutes to run to completion and install your environment.\nIn\u00a0[1]:\n```\n!curl -Lo conda\\_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab\\_install.pyimportconda\\_installerconda\\_installer.install()!/root/miniconda/bin/conda info -e\n```\n```\n% Total % Received % Xferd Average Speed Time Time Time Current\nDload Upload Total Spent Left Speed\n100 3501 100 3501 0 0 16995 0 --:--:-- --:--:-- --:--:-- 16995\n```\n```\nadd /root/miniconda/lib/python3.7/site-packages to PYTHONPATH\npython version: 3.7.10\nremove current miniconda\nfetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86\\_64.sh\ndone\ninstalling miniconda to /root/miniconda\ndone\ninstalling rdkit, openmm, pdbfixer\nadded omnia to channels\nadded conda-forge to channels\ndone\nconda packages installation finished!\n```\n```\n# conda environments:\n#\nbase \\* /root/miniconda\n```\nIn\u00a0[2]:\n```\n!pip install --pre deepchemimportdeepchemdeepchem.\\_\\_version\\_\\_\n```\n```\nRequirement already satisfied: deepchem in /usr/local/lib/python3.7/dist-packages (2.5.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.4.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem) (0.22.2.post1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.19.5)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.0.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;deepchem) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;deepchem) (2018.9)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;deepchem) (1.15.0)\n```\n```\nwandb:WARNINGW&amp;&amp;B installed but not logged in. Run `wandb login` or set the WANDB\\_API\\_KEY env variable.wandb:WARNINGW&amp;&amp;B installed but not logged in. Run `wandb login` or set the WANDB\\_API\\_KEY env variable.\n```\nOut[2]:\n```\n'2.5.0'\n```\nIn\u00a0[3]:\n```\nfromrdkitimportChem\n```\nWe want to install NVIDIA's Apex tool, for the training pipeline used by`simple-transformers`and Weights and Biases. This package enables us to use 16-bit training, mixed precision, and distributed training without any changes to our code. Generally GPUs are good at doing 32-bit(single precision) math, not at 16-bit(half) nor 64-bit(double precision). Therefore traditionally deep learning model trainings are done in 32-bit. By switching to 16-bit, we\u2019ll be using half the memory and theoretically less computation at the expense of the available number range and precision. However, pure 16-bit training creates a lot of problems for us (imprecise weight updates, gradient underflow and overflow).**Mixed precision training, with Apex, alleviates these problems**.\nWe will be installing`simple-transformers`, a library which builds ontop of HuggingFace's`transformers`package specifically for fine-tuning ChemBERTa.\nIn\u00a0[\u00a0]:\n```\n!git clone https://github.com/NVIDIA/apex!cd/content/apex!pip install -v --no-cache-dir /content/apex!pip install transformers!pip install simpletransformers!pip install wandb!cd..\n```\nIn\u00a0[5]:\n```\nimportsys!test-d bertviz\\_repo&amp;&amp;echo\"FYI: bertviz\\_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz\\_repo\"# !rm -r b...",
      "url": "https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers"
    },
    {
      "title": "Finetuning a pretrained transformer - molfeat",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html#huggingface-transformer-finetuning)\n\n# Finetuning a pretrained transformer\n\nIn\u00a0\\[1\\]:\n\nCopied!\n\n```\n%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n```\n\n%load\\_ext autoreload\n%autoreload 2\nimport torch\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n## HuggingFace Transformer Finetuning [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#huggingface-transformer-finetuning)\n\nCommunity contribution\n\nCurious how one would run this tutorial on [Graphcore IPUs](https://www.graphcore.ai/products/ipu)? See this tutorial contributed by [@s-maddrellmander](https://github.com/s-maddrellmander):\n[![Run on Gradient](https://camo.githubusercontent.com/c9931a1689c37ab786edd3e1e5f59b9a6f7d097628c4689ce2432563ef884524/68747470733a2f2f6173736574732e706170657273706163652e696f2f696d672f6772616469656e742d62616467652e737667)](https://ipu.dev/yoyy6N)\n\nWe have previously shown how [Molfeat integrates with PyTorch in general](https://molfeat-docs.datamol.io/stable/tutorials/integration.html) and even with [Pytorch Geometric](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html). Now we will demonstrate how to use molfeat to finetune a pretrained transformer. This tutorial will walk you through an example of finetuning the ChemBERTa pretrained model for molecular property prediction. These same principles can be applied to any pretrained transformers available in molfeat.\n\nTo run this tutorial, you will need to install `transformers` and `tokenizers`.\n\n`mamba install -c conda-forge transformers \"tokenizers <0.13.2\"`\n\nAdvanced users\n\nThis tutorial is for advanced users that are comfortable with the APIs of molfeat and Hugging Face transformers.\n\nIn\u00a0\\[2\\]:\n\nCopied!\n\n```\nfrom molfeat.utils.converters import SmilesConverter\nfrom molfeat.trans.pretrained import PretrainedHFTransformer\n\n```\n\nfrom molfeat.utils.converters import SmilesConverter\nfrom molfeat.trans.pretrained import PretrainedHFTransformer\n\n### Featurizer [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#featurizer)\n\nPretrained Transformer Featurizer in molfeat have an underlying object `featurizer` that can handle both tokenization and embedding.\n\nWe will leverage this structure in molfeat to initialize our transformer model, but also to tokenize our molecules\n\nWe first start by defining our featurizer. Here we will use the ChemBERTa pretrained model.\n\nIn\u00a0\\[3\\]:\n\nCopied!\n\n```\nfeaturizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)\n\n```\n\nfeaturizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)\n\n- Note the use of preload to preload the model in the `__init__`\n- Note how we define a pooling mechanism here. Molfeat provides [several poolers that you can explore in the API](https://molfeat-docs.datamol.io/stable/tutorials/api/molfeat.utils.html#pooling). Because a pooling layer can already be specified and will be accessible through the `_pooling_obj` attribute we will not bother defining one later. Instead we will just retrieve the one from the featurizer.\n\n### Dataset [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#dataset)\n\nFor the dataset, we will use the `BBBP` dataset, which contains binary labels of blood-brain barrier penetration.\n\nIn\u00a0\\[4\\]:\n\nCopied!\n\n```\ndf = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")\n\n```\n\ndf = pd.read\\_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")\n\nIn\u00a0\\[5\\]:\n\nCopied!\n\n```\ndf.head()\n\n```\n\ndf.head()\n\nOut\\[5\\]:\n\n|  | num | name | p\\_np | smiles |\n| --- | --- | --- | --- | --- |\n| 0 | 1 | Propanolol | 1 | \\[Cl\\].CC(C)NCC(O)COc1cccc2ccccc12 |\n| 1 | 2 | Terbutylchlorambucil | 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl |\n| 2 | 3 | 40730 | 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... |\n| 3 | 4 | 24 | 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C |\n| 4 | 5 | cloxacillin | 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N\\[C@H\\]3\\[C@H\\]4SC(C)(C)... |\n\nNow we just need to define our PyTorch Dataset. As discussed above, we will leverage the internal structure of our transformer\n\nIn\u00a0\\[6\\]:\n\nCopied!\n\n```\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\nclass DTset(Dataset):\n    def __init__(self, smiles, y, mf_featurizer):\n        super().__init__()\n        self.smiles = smiles\n        self.mf_featurizer = mf_featurizer\n        self.y = torch.tensor(y).float()\n        # here we use the molfeat mf_featurizer to convert the smiles to\n        # corresponding tokens based on the internal tokenizer\n        # we just want the data from the batch encoding object\n        self.transformed_mols = self.mf_featurizer._convert(smiles)\n\n    @property\n    def embedding_dim(self):\n        return len(self.mf_featurizer)\n\n    @property\n    def max_length(self):\n        return self.transformed_mols.shape[-1]\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def collate_fn(self, **kwargs):\n        # the default collate fn self.mf_featurizer.get_collate_fn(**kwargs)\n        # returns None, which should just concatenate the inputs\n        # You could also use `transformers.default_data_collator` instead\n        return self.mf_featurizer.get_collate_fn(**kwargs)\n\n    def __getitem__(self, index):\n        datapoint = dict((name, val[index]) for name, val in self.transformed_mols.items())\n        datapoint[\"y\"] = self.y[index]\n        return datapoint\n\n```\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import default\\_data\\_collator\nclass DTset(Dataset):\ndef \\_\\_init\\_\\_(self, smiles, y, mf\\_featurizer):\nsuper().\\_\\_init\\_\\_()\nself.smiles = smiles\nself.mf\\_featurizer = mf\\_featurizer\nself.y = torch.tensor(y).float()\n\\# here we use the molfeat mf\\_featurizer to convert the smiles to\n\\# corresponding tokens based on the internal tokenizer\n\\# we just want the data from the batch encoding object\nself.transformed\\_mols = self.mf\\_featurizer.\\_convert(smiles)\n@property\ndef embedding\\_dim(self):\nreturn len(self.mf\\_featurizer)\n@property\ndef max\\_length(self):\nreturn self.transformed\\_mols.shape\\[-1\\]\ndef \\_\\_len\\_\\_(self):\nreturn self.y.shape\\[0\\]\ndef collate\\_fn(self, \\*\\*kwargs):\n\\# the default collate fn self.mf\\_featurizer.get\\_collate\\_fn(\\*\\*kwargs)\n\\# returns None, which should just concatenate the inputs\n\\# You could also use \\`transformers.default\\_data\\_collator\\` instead\nreturn self.mf\\_featurizer.get\\_collate\\_fn(\\*\\*kwargs)\ndef \\_\\_getitem\\_\\_(self, index):\ndatapoint = dict((name, val\\[index\\]) for name, val in self.transformed\\_mols.items())\ndatapoint\\[\"y\"\\] = self.y\\[index\\]\nreturn datapoint\n\nIn\u00a0\\[7\\]:\n\nCopied!\n\n```\ndataset = DTset(df.smiles.values, df.p_np.values, featurizer)\ngenerator = torch.Generator().manual_seed(42)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dt, test_dt = torch.utils.data.random_split(dataset, [train_size, test_size], generator=generator)\n\n```\n\ndataset = DTset(df.smiles.values, df.p\\_np.values, featurizer)\ngenerator = torch.Generator().manual\\_seed(42)\ntrain\\_size = int(0.8 \\* len(dataset))\ntest\\_size = len(dataset) - train\\_size\ntrain\\_dt, test\\_dt = torch.utils.data.random\\_split(dataset, \\[train\\_size, test\\_size\\], generator=generator)\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\nIn\u00a0\\[8\\]:\n\nCopied!\n\n```\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dt, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn())\ntest_loader = DataLoader(test_dt, batch_size=BATCH_SIZE, shuffle=False, collate_fn=dataset.collate_fn())\n\n```\n\nBATCH\\_SIZE = 64\ntrain\\_loader = DataLoader(train\\_dt, batch\\_size=BATCH\\_SIZE, shuffle=True, collate\\_fn=dat...",
      "url": "https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html"
    },
    {
      "title": "Why bother? - molfeat",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io#one-featurizer-to-rule-them-all)\n\n# Why bother?\n\nIn\u00a0\\[1\\]:\n\nCopied!\n\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n%load\\_ext autoreload\n%autoreload 2\n\n## One featurizer to rule them all? [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#One-featurizer-to-rule-them-all?)\n\nContrary to many other machine learning domains, _molecular_ featurization (i.e. the process of transforming a molecule into a vector) lacks a good default. It remains unclear how we can effectively capture the richness of molecular data in a unified representation and what works best heavily depends on the nature and constraints of the task you are trying to model. It is therefore good practice to try different featurization schemes: From structural fingerprints, to physico-chemical descriptors and pre-trained embeddings.\n\n## Don't take our word for it [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Don't-take-our-word-for-it)\n\nTo demonstrate the impact a featurizer can have, we setup two simple benchmarks.\n\n1. To demonstrate the impact on modeling, we will use two datasets from [MoleculeNet](https://moleculenet.org/datasets-1).\n2. To demonstrate the impact on search, we will use the [RDKit Benchmarking Platform](https://github.com/rdkit/benchmarking_platform).\n\nWe will compare the performance of three different featurizers:\n\n- **ECFP6** \\[1\\]: Binary, circular fingerprints where each bit indicates the presence of particular substructures of a radius up to 3 bonds away from an atom.\n- **Mordred** \\[2\\]: Continuous descriptors with more than 1800 2D and 3D descriptors.\n- **ChemBERTa** \\[3\\]: Learned representations from a pre-trained SMILES transformer model.\n\n### Modeling [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Modeling)\n\nWe will compare the performance on two datasets using scikit-learn [AutoML](https://github.com/automl/auto-sklearn) \\[4, 5\\] models.\n\nIn\u00a0\\[3\\]:\n\nCopied!\n\n```\nimport os\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport autosklearn.classification\nimport autosklearn.regression\nfrom sklearn.metrics import mean_absolute_error, roc_auc_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom rdkit.Chem import SaltRemover\n\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\n```\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport autosklearn.classification\nimport autosklearn.regression\nfrom sklearn.metrics import mean\\_absolute\\_error, roc\\_auc\\_score\nfrom sklearn.model\\_selection import GroupShuffleSplit\nfrom rdkit.Chem import SaltRemover\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf\\_transformers import PretrainedHFTransformer\n\nIn\u00a0\\[4\\]:\n\nCopied!\n\n```\ndef load_dataset(uri: str, readout_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\n    df = pd.read_csv(uri)\n    smiles = df[\"smiles\"].values\n    y = df[readout_col].values\n    return smiles, y\n\ndef preprocess_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\n    with dm.without_rdkit_log():\n        mol = dm.to_mol(smi, ordered=True, sanitize=False)\n        mol = dm.sanitize_mol(mol)\n        if mol is None:\n            return\n\n        mol = dm.standardize_mol(mol, disconnect_metals=True)\n        remover = SaltRemover.SaltRemover()\n        mol = remover.StripMol(mol, dontRemoveEverything=True)\n\n    return dm.to_smiles(mol)\n\ndef scaffold_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\n    scaffolds = [dm.to_smiles(dm.to_scaffold_murcko(dm.to_mol(smi))) for smi in smiles]\n    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    return next(splitter.split(smiles, groups=scaffolds))\n```\n\ndef load\\_dataset(uri: str, readout\\_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\ndf = pd.read\\_csv(uri)\nsmiles = df\\[\"smiles\"\\].values\ny = df\\[readout\\_col\\].values\nreturn smiles, y\ndef preprocess\\_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\nwith dm.without\\_rdkit\\_log():\nmol = dm.to\\_mol(smi, ordered=True, sanitize=False)\nmol = dm.sanitize\\_mol(mol)\nif mol is None:\nreturn\nmol = dm.standardize\\_mol(mol, disconnect\\_metals=True)\nremover = SaltRemover.SaltRemover()\nmol = remover.StripMol(mol, dontRemoveEverything=True)\nreturn dm.to\\_smiles(mol)\ndef scaffold\\_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\nscaffolds = \\[dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi))) for smi in smiles\\]\nsplitter = GroupShuffleSplit(n\\_splits=1, test\\_size=0.2, random\\_state=42)\nreturn next(splitter.split(smiles, groups=scaffolds))\n\nIn\u00a0\\[5\\]:\n\nCopied!\n\n```\n# Setup the featurizers\ntrans_ecfp = FPVecTransformer(kind=\"ecfp:6\", n_jobs=-1)\ntrans_mordred = FPVecTransformer(kind=\"mordred\", replace_nan=True, n_jobs=-1)\ntrans_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles')\n```\n\n\\# Setup the featurizers\ntrans\\_ecfp = FPVecTransformer(kind=\"ecfp:6\", n\\_jobs=-1)\ntrans\\_mordred = FPVecTransformer(kind=\"mordred\", replace\\_nan=True, n\\_jobs=-1)\ntrans\\_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles')\n\n```\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\n  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n\n```\n\n#### Lipophilicity [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Lipophilicity)\n\nLipophilicity is a regression task with 4200 molecules\n\nIn\u00a0\\[6\\]:\n\nCopied!\n\n```\n# Prepare the Lipophilicity dataset\nsmiles, y_true = load_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\nsmiles = np.array([preprocess_smiles(smi) for smi in smiles])\nsmiles = np.array([smi for smi in smiles if smi != \"\"])\n\nX = {\n    \"ECFP\": trans_ecfp(smiles),\n    \"Mordred\": trans_mordred(smiles),\n    \"ChemBERTa\": trans_chemberta(smiles),\n}\n```\n\n\\# Prepare the Lipophilicity dataset\nsmiles, y\\_true = load\\_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\nsmiles = np.array(\\[preprocess\\_smiles(smi) for smi in smiles\\])\nsmiles = np.array(\\[smi for smi in smiles if smi != \"\"\\])\nX = {\n\"ECFP\": trans\\_ecfp(smiles),\n\"Mordred\": trans\\_mordred(smiles),\n\"ChemBERTa\": trans\\_chemberta(smiles),\n}\n\n```\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **pass...",
      "url": "https://molfeat-docs.datamol.io/0.8.0/benchmark.html"
    },
    {
      "title": "Why should you care?",
      "text": "Why should you care? - molfeat\n[Skip to content](#use-cases)\n# Why should you care?\nIn\u00a0[1]:\nCopied!\n```\n%load\\_extautoreload%autoreload2\n```\n%load\\_ext autoreload\n%autoreload 2\nUnlike many other machine learning domains, molecular featurization (i.e. the process of transforming a molecule into a vector) lacks a consistently good default.\nIt is still an open question how to best capture the complexity of molecular data with a unified representation. Which molecular representation works best depends largely on which task you are modeling. To achieve optimal performance, it is wise to experiment with a variety of featurization schemes, from structural fingerprints to physicochemical descriptors and pre-trained embeddings.\n## Use cases[\u00b6](#use-cases)\nMolecular representations / featurizers are an integral part of any molecular modelling workflow and are commonly used for:\n* Search - to find molecules with similar electronic properties, similar structures, or similar biological activity for a target.\n* Clustering - to group molecules based on their features and derive hypotheses around the relationship between structure and activity\n* Modeling - to build QSAR model for molecular property/activity prediction## Importance of the choice of molecular representation[\u00b6](#importance-of-the-choice-of-molecular-representation)\nTo demonstrate the impact a featurizer can have, we establish two simple benchmarks:\n1. To demonstrate the impact on modeling, we will use two datasets from[MoleculeNet](https://moleculenet.org/datasets-1)[1].\n2. To demonstrate the impact on search, we will use the[RDKit Benchmarking Platform](https://github.com/rdkit/benchmarking_platform)[2, 3].\nWe will compare the performance of three different featurizers:\n* **ECFP6**[4]: Binary, circular fingerprints where each bit indicates the presence of particular substructures of a radius up to 3 bonds away from an atom.\n* **Mordred**[5]: Continuous descriptors with more than 1800 2D and 3D descriptors.\n* **ChemBERTa**[6]: Learned representations from a pre-trained SMILES transformer model.\nTl;dr - Importance of molecular representation\nNo featurizer consistently stood out for either task or even within a task category:\n* **Modeling:**The Mordred featurizer outperforms the next best featurizer by \\~20% on the*Lipophilicity*prediction task. On*ClinTox*, however, things are reversed and ChemBERTa outperforms the other featurizers by \\~about 18%.\n* **Search**: ECFP outperforms ChemBERTa and Mordred and is the best option across the board, although it's target dependent.\nThese quick examples show the context-dependent nature and thus the importance of experimenting with trying different featurizers.**In short, the perfect molecular featurizer doesn\u2019t exist (yet!)**. All have their pros and cons depending on the data and the downstream task.\n### Modeling[\u00b6](#modeling)\nWe will compare the performance on two datasets using scikit-learn[AutoML](https://github.com/automl/auto-sklearn)[7, 8] models.\nIn\u00a0[\u00a0]:\nCopied!\n```\n!pipinstallautosklearn\n```\n! pip install autosklearn\nIn\u00a0[2]:\nCopied!\n```\nimportosimporttqdmimportfsspecimportpickleimportwarningsimportnumpyasnpimportpandasaspdimportdatamolasdmimportmatplotlib.pyplotaspltimportautosklearn.classificationimportautosklearn.regressionfromcollectionsimportdefaultdictfromrdkit.ChemimportSaltRemoverfromsklearn.metricsimportmean\\_absolute\\_error,roc\\_auc\\_scorefromsklearn.model\\_selectionimportGroupShuffleSplitfromsklearn.neighborsimportKNeighborsClassifierfrommolfeat.trans.fpimportFPVecTransformerfrommolfeat.trans.pretrained.hf\\_transformersimportPretrainedHFTransformer\n```\nimport os\nimport tqdm\nimport fsspec\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport matplotlib.pyplot as plt\nimport autosklearn.classification\nimport autosklearn.regression\nfrom collections import defaultdict\nfrom rdkit.Chem import SaltRemover\nfrom sklearn.metrics import mean\\_absolute\\_error, roc\\_auc\\_score\nfrom sklearn.model\\_selection import GroupShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf\\_transformers import PretrainedHFTransformer\nIn\u00a0[3]:\nCopied!\n```\n# Making the output less verbosewarnings.simplefilter(\"ignore\")os.environ[\"PYTHONWARNINGS\"]=\"ignore\"os.environ[\"TOKENIZERS\\_PARALLELISM\"]=\"false\"dm.disable\\_rdkit\\_log()\n```\n# Making the output less verbose\nwarnings.simplefilter(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\nos.environ[\"TOKENIZERS\\_PARALLELISM\"] = \"false\"\ndm.disable\\_rdkit\\_log()\nIn\u00a0[4]:\nCopied!\n```\ndefload\\_dataset(uri:str,readout\\_col:str):\"\"\"Loads the MoleculeNet dataset\"\"\"df=pd.read\\_csv(uri)smiles=df[\"smiles\"].valuesy=df[readout\\_col].valuesreturnsmiles,ydefpreprocess\\_smiles(smi):\"\"\"Preprocesses the SMILES string\"\"\"mol=dm.to\\_mol(smi,ordered=True,sanitize=False)try:mol=dm.sanitize\\_mol(mol)except:# noqa: E722mol=NoneifmolisNone:returnmol=dm.standardize\\_mol(mol,disconnect\\_metals=True)remover=SaltRemover.SaltRemover()mol=remover.StripMol(mol,dontRemoveEverything=True)returndm.to\\_smiles(mol)defscaffold\\_split(smiles):\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"scaffolds=[dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi)))forsmiinsmiles]splitter=GroupShuffleSplit(n\\_splits=1,test\\_size=0.2,random\\_state=42)returnnext(splitter.split(smiles,groups=scaffolds))\n```\ndef load\\_dataset(uri: str, readout\\_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\ndf = pd.read\\_csv(uri)\nsmiles = df[\"smiles\"].values\ny = df[readout\\_col].values\nreturn smiles, y\ndef preprocess\\_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\nmol = dm.to\\_mol(smi, ordered=True, sanitize=False)\ntry:\nmol = dm.sanitize\\_mol(mol)\nexcept: # noqa: E722\nmol = None\nif mol is None:\nreturn\nmol = dm.standardize\\_mol(mol, disconnect\\_metals=True)\nremover = SaltRemover.SaltRemover()\nmol = remover.StripMol(mol, dontRemoveEverything=True)\nreturn dm.to\\_smiles(mol)\ndef scaffold\\_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\nscaffolds = [dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi))) for smi in smiles]\nsplitter = GroupShuffleSplit(n\\_splits=1, test\\_size=0.2, random\\_state=42)\nreturn next(splitter.split(smiles, groups=scaffolds))\nIn\u00a0[5]:\nCopied!\n```\n# Setup the featurizerstrans\\_ecfp=FPVecTransformer(kind=\"ecfp:6\",n\\_jobs=-1)trans\\_mordred=FPVecTransformer(kind=\"mordred\",replace\\_nan=True,n\\_jobs=-1)trans\\_chemberta=PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\",notation=\"smiles\")\n```\n# Setup the featurizers\ntrans\\_ecfp = FPVecTransformer(kind=\"ecfp:6\", n\\_jobs=-1)\ntrans\\_mordred = FPVecTransformer(kind=\"mordred\", replace\\_nan=True, n\\_jobs=-1)\ntrans\\_chemberta = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", notation=\"smiles\")\n#### Lipophilicity[\u00b6](#lipophilicity)\nLipophilicity is a regression task with 4200 molecules\nIn\u00a0[6]:\nCopied!\n```\n# Prepare the Lipophilicity datasetsmiles,y\\_true=load\\_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\",\"exp\")smiles=np.array([preprocess\\_smiles(smi)forsmiinsmiles])smiles=np.array([smiforsmiinsmilesifdm.to\\_mol(smi)isnotNone])feats\\_ecfp,ind\\_ecfp=trans\\_ecfp(smiles,ignore\\_errors=True)feats\\_mordred,ind\\_mordred=trans\\_mordred(smiles,ignore\\_errors=True)feats\\_chemberta,ind\\_chemberta=trans\\_chemberta(smiles,ignore\\_errors=True)X={\"ECFP\":feats\\_ecfp[ind\\_ecfp],\"Mordred\":feats\\_mordred[ind\\_mordred],\"ChemBERTa\":feats\\_chemberta[ind\\_chemberta],}\n```\n# Prepare the Lipophilicity dataset\nsmiles, y\\_true = load\\_dataset(\n\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\"\n)\nsmiles = np.array([preprocess\\_smiles(smi) for smi in smiles])\nsmiles = np.array([smi for smi in smiles if dm.to\\_mol(smi) is not None])\nfeats\\_ecfp, ind\\_ecfp = trans\\_ecfp(smiles, ignore\\_errors=True)\nfeats\\_mordred, ind\\_mordred = trans\\_mordred(smiles, ignore\\_errors=True)\nfeats\\_chem...",
      "url": "https://molfeat-docs.datamol.io/stable/benchmark.html"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "deepchem/examples/tutorials/Transfer\\_Learning\\_With\\_ChemBERTa\\_Transformers.ipynb at master \u00b7deepchem/deepchem \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=deepchem/deepchem)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[deepchem](https://github.com/deepchem)/**[deepchem](https://github.com/deepchem/deepchem)**Public\n* [Notifications](https://github.com/login?return_to=/deepchem/deepchem)You must be signed in to change notification settings\n* [Fork2k](https://github.com/login?return_to=/deepchem/deepchem)\n* [Star6.5k](https://github.com/login?return_to=/deepchem/deepchem)\n</turbo-frame></main>\nYou can\u2019t perform that action at this time.\n</div>",
      "url": "https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - frogstar-world-b/Transfer-Learning-ChemBERTa: Apply fine-tuning and transfer learning for regression to ChemBERTa, a BERT-like model applied to chemical SMILES data.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=frogstar-world-b/Transfer-Learning-ChemBERTa)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[frogstar-world-b](https://github.com/frogstar-world-b)/**[Transfer-Learning-ChemBERTa](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa)**Public\n* [Notifications](https://github.com/login?return_to=/frogstar-world-b/Transfer-Learning-ChemBERTa)You must be signed in to change notification settings\n* [Fork1](https://github.com/login?return_to=/frogstar-world-b/Transfer-Learning-ChemBERTa)\n* [Star18](https://github.com/login?return_to=/frogstar-world-b/Transfer-Learning-ChemBERTa)\nApply fine-tuning and transfer learning for regression to ChemBERTa, a BERT-like model applied to chemical SMILES data.\n[18stars](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/stargazers)[1fork](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/forks)[Branches](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/branches)[Tags](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/tags)[Activity](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/activity)\n[Star](https://github.com/login?return_to=/frogstar-world-b/Transfer-Learning-ChemBERTa)\n[Notifications](https://github.com/login?return_to=/frogstar-world-b/Transfer-Learning-ChemBERTa)You must be signed in to change notification settings\n# frogstar-world-b/Transfer-Learning-ChemBERTa\nmain\n[Branches](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/branches)[Tags](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/tags)\n[](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/branches)[](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[6 Commits](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/commits/main/)\n[](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/commits/main/)\n|\n[data](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/tree/main/data)\n|\n[data](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/tree/main/data)\n|\n|\n|\n[README.md](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/README.md)\n|\n[README.md](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/README.md)\n|\n|\n|\n[fine-tune.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/fine-tune.ipynb)\n|\n[fine-tune.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/fine-tune.ipynb)\n|\n|\n|\n[preprocessing.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/preprocessing.ipynb)\n|\n[preprocessing.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/preprocessing.ipynb)\n|\n|\n|\n[transfer-learning-plus-descriptors.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/transfer-learning-plus-descriptors.ipynb)\n|\n[transfer-learning-plus-descriptors.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/transfer-learning-plus-descriptors.ipynb)\n|\n|\n|\n[transfer-learning.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/transfer-learning.ipynb)\n|\n[transfer-learning.ipynb](https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa/blob/main/transfer-learning.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Transfer Learning vs. Fine-Tuning ChemBERTa for Regression\n[](#transfer-learning-vs-fine-tuning-chemberta-for-regression)\n## Summary\n[](#summary)\nThis repository demonstrates the use of fine-tuning vs transfer learning for a regression task with[ChemBERTa](https://arxiv.org/abs/2010.09885), a specialized BERT-like model applied to chemical SMILES data. SMILES (Simplified Molecular Input Line Entry System) is a notation for representing chemical structures as text. We explore when transfer learning might be more appropriate than fine-tuning ChemBERTa given our dataset, which is significantly smaller than the model's pre-training data (a few hundred vs 77 millions examples).\nThe regression task is to predict the pIC50 values for inhibiting the catalytic activity of Dihydrofolate Reductase ([DHFR](https://en.wikipedia.org/wiki/Dihydrofolate_reductase)) in homo sapiens. DHFR is a crucial enzyme in the folate metabolic pathway, and inhibiting its catalytic activity can disrupt the production of tetrahydrofolate, which is necessary for DNA synthesis. This disruption can slow down or prevent cancer cell replication, making DHFR an important target for cancer treatment.\npIC50 is a measure of a substance's potency, representing the negative logarithm (base 10) of its Inhibitory Concentration at 50% (IC50).\n## Dataset\n[](#dataset)\nDownloaded from[https://github.com/KISysBio/qsar-models/tree/master](https://github.com/KISysBio/qsar-models/tree/master), the dataset consists of SMILES representations, molecular descriptors, and corresponding pIC50 values.\n## Requirements\n[](#requirements)\nBefore running the notebooks, ensure you have the following dependencies installed:\n* Python 3.6+\n* PyTorch\n* Transformers library (Hugging Face)\n* XGBoost\n* NumPy\n* pandas\n* scikit-learn\n* scipy\n* matplotlib\n* tqdm\n* RDKit\nYou can install these packages using Conda and pip:\nUsing Conda (recommended for RDKit):\n```\nconda create -n myenv python=3.7#Create a new Conda environment (optional)conda activate myenv#Activate the Conda environment (if created)conda install -c conda-forge rdkit\npip install torch transformers xgboost numpy pandas scikit-learn scipy matplotlib tqdm\n```\nThis will ensure proper installation of RDKit through Conda, which is a common practice in cheminformatics. Make sure to create and activate a Conda environment as needed.\n## Notebooks\n[](#notebooks)\n### 1. Data Preprocessing and Light EDA\n[](#1-data-preprocessing-and-light-eda)\n* Notebook:`preprocessing.ipynb`\n* This notebook covers data preprocessing and exploratory data analysis (EDA).\n### 2. Fine-Tuning ChemBERTa\n[](#2-fine-tuning-chemberta)\n* Notebook:`fine-tune.ipynb`\n* In this notebook, ChemBERTa is fine-tuned using the Transformers library. The fine-tuned model is trained on the SMILES representations of molecules to predict pIC50 values.\n### 3. Transfer Learn...",
      "url": "https://github.com/frogstar-world-b/Transfer-Learning-ChemBERTa"
    },
    {
      "title": "Finetuning a pretrained transformer",
      "text": "[![datamol.io](https://tribe-s3-production.imgix.net/r1yG29WQNB6t4Imyy71ay?fit=max&w=200&auto=compress,format)\\\ndatamol.io](https://portal.valencelabs.com/datamol)\n\n## Finetuning a pretrained transformer\n\n[![Cas](https://tribe-s3-production.imgix.net/cvedEnmcF8GP5Vs2T56Hw?fit=max&w=500&auto=compress,format)](https://portal.valencelabs.com/member/mdX6VzUZIa)\n\n[Cas](https://portal.valencelabs.com/member/mdX6VzUZIa)\n\n[2 years ago](https://portal.valencelabs.com/datamol/post/finetuning-a-pretrained-transformer-mS8Wq94AQ4U4mif)\u00a0\u00b7\u00a0Leading the Polaris project at Valence Labs\n\n> **This tutorial was last updated in August 2023. For the most up to date tutorials, please see our** [documentation](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html) [**site.**](https://molfeat-docs.datamol.io/stable/benchmark.html)\n\nCopy\n\n```\n\n%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n```\n\n## HuggingFace Transformer Finetuning\n\n> **Community contribution**\n>\n> Curious how one would run this tutorial on [Graphcore IPUs](https://www.graphcore.ai/products/ipu)? See this [tutorial](https://ipu.dev/yoyy6N) contributed by [@s-maddrellmander](https://github.com/s-maddrellmander)\n\nWe have previously shown how [Molfeat integrates with PyTorch in general](https://molfeat-docs.datamol.io/stable/tutorials/integration.html) and even with [Pytorch Geometric](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html). Now we will demonstrate how to use molfeat to finetune a pretrained transformer. This tutorial will walk you through an example of finetuning the ChemBERTa pretrained model for molecular property prediction. These same principles can be applied to any pretrained transformers available in molfeat.\n\nTo run this tutorial, you will need to install `transformers` and `tokenizers`.\n\n`mamba install -c conda-forge transformers \"tokenizers <0.13.2\"`\n\nCopy\n\n```\nfrom molfeat.utils.converters import SmilesConverter\nfrom molfeat.trans.pretrained import PretrainedHFTransformer\n\n```\n\n### Featurizer\n\nPretrained Transformer Featurizer in molfeat have an underlying object `featurizer` that can handle both tokenization and embedding.\n\nWe will leverage this structure in molfeat to initialize our transformer model, but also to tokenize our molecules\n\nWe first start by defining our featurizer. Here we will use the ChemBERTa pretrained model.\n\nCopy\n\n```\nfeaturizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)\n\n```\n\n- Note the use of preload to preload the model in the `__init__`\n\n- Note how we define a pooling mechanism here. Molfeat provides [several poolers that you can explore in the API](https://molfeat-docs.datamol.io/stable/tutorials/api/molfeat.utils.html#pooling). Because a pooling layer can already be specified and will be accessible through the `_pooling_obj` attribute we will not bother defining one later. Instead we will just retrieve the one from the featurizer.\n\n\n### Dataset\n\nFor the dataset, we will use the `BBBP` dataset, which contains binary labels of blood-brain barrier penetration.\n\nCopy\n\n```\ndf = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")\n\n```\n\nCopy\n\n```\ndf.head()\n\n```\n\n**numnamep\\_npsmiles0** 1Propanolol1\\[Cl\\].CC(C)NCC(O)COc1cccc2ccccc12 **1** 2Terbutylchlorambucil1C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl **2** 3407301c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... **3** 4241C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C **4** 5cloxacillin1Cc1onc(c2ccccc2Cl)c1C(=O)N\\[C@H\\]3\\[C@H\\]4SC(C)(C)...\n\nNow we just need to define our PyTorch Dataset. As discussed above, we will leverage the internal structure of our transformer\n\nCopy\n\n```\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\nclass DTset(Dataset):\n    def __init__(self, smiles, y, mf_featurizer):\n        super().__init__()\n        self.smiles = smiles\n        self.mf_featurizer = mf_featurizer\n        self.y = torch.tensor(y).float()\n        # here we use the molfeat mf_featurizer to convert the smiles to\n        # corresponding tokens based on the internal tokenizer\n        # we just want the data from the batch encoding object\n        self.transformed_mols = self.mf_featurizer._convert(smiles)\n\n    @property\n    def embedding_dim(self):\n        return len(self.mf_featurizer)\n\n    @property\n    def max_length(self):\n        return self.transformed_mols.shape[-1]\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def collate_fn(self, **kwargs):\n        # the default collate fn self.mf_featurizer.get_collate_fn(**kwargs)\n        # returns None, which should just concatenate the inputs\n        # You could also use `transformers.default_data_collator` instead\n        return self.mf_featurizer.get_collate_fn(**kwargs)\n\n    def __getitem__(self, index):\n        datapoint = dict((name, val[index]) for name, val in self.transformed_mols.items())\n        datapoint[\"y\"] = self.y[index]\n        return datapoint\n\n```\n\nCopy\n\n```\ndataset = DTset(df.smiles.values, df.p_np.values, featurizer)\ngenerator = torch.Generator().manual_seed(42)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dt, test_dt = torch.utils.data.random_split(dataset, [train_size, test_size], generator=generator)\n\n```\n\nCopy\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\nCopy\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\nCopy\n\n```\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dt, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn())\ntest_loader = DataLoader(test_dt, batch_size=BATCH_SIZE, shuffle=False, collate_fn=dataset.collate_fn())\n\n```\n\n### Network + Training\n\nWe are ready to go, now we just need to define our Model for finetuning pretrained ChemBerta on the BBBP task.\n\nCopy\n\n```\nclass AwesomeNet(torch.nn.Module):\n    def __init__(self, mf_featurizer, hidden_size=128, dropout=0.1, output_size=1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        # we get the underlying model from the molfeat featurizer\n        # here we fetch the \"base\" huggingface transformer model\n        # and not the wrapper around for MLM\n        # this is principally to get smaller model and training efficiency\n        base_pretrained_model = getattr(mf_featurizer.featurizer.model, mf_featurizer.featurizer.model.base_model_prefix)\n        self.embedding_layer = copy.deepcopy(base_pretrained_model)\n        self.embedding_dim = mf_featurizer.featurizer.model.config.hidden_size\n        # given that we are not concatenating layers, the following is equivalent\n        # self.embedding_dim = len(mf_featurizer)\n        # we get the the pooling layer from the molfeat featurizer\n        self.pooling_layer = mf_featurizer._pooling_obj\n        self.hidden_layer = torch.nn.Sequential(\n            torch.nn.Dropout(p=dropout),\n            torch.nn.Linear(len(mf_featurizer), self.hidden_size),\n            torch.nn.ReLU()\n        )\n        self.output_layer = torch.nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, *, y=None, **kwargs):\n        # get embeddings\n        x = self.embedding_layer(**kwargs)\n        # we take the last hidden state\n        # you could also set `output_hidden_states` to true above\n        # and take x[\"hidden_states\"][-1] instead\n        emb = x[\"last_hidden_state\"]\n        # run poolings\n        h = self.pooling_layer(\n            emb,\n            kwargs[\"input_ids\"],\n            mask=kwargs.get('attention_mask'),\n        )\n        # run through our custom and optional hidden layer\n        h = self.hidden_layer(h)\n        # run through output layers to get logits\n        return self.output_layer(h)\n\n```\n\nCopy\n\n```\nDEVICE = \"cpu\"\nNUM_EPOCHS = 10\nLEARNING_RATE = 1e-3\nPNA_AGGREGATORS = ['mean', 'min', 'max', 'std']\nPNA_SCALERS = ['identity', 'amplification', 'attenuation']\n\nmodel = Aw...",
      "url": "https://portal.valencelabs.com/datamol/post/finetuning-a-pretrained-transformer-mS8Wq94AQ4U4mif"
    },
    {
      "title": "| notebook.community",
      "text": "[notebook.community](https://notebook.community/)\n\n[Edit and run](https://deepnote.com/launch?url=https://github.com/deepchem/deepchem/blob/master/examples/tutorials/22_Transfer_Learning_With_HuggingFace_tox21.ipynb)\n\n# Part 22, ChemBERTa: Pre-training a BERT-like model for masked language modelling of SMILES and molecular property prediction.\n\nBy Seyone Chithrananda ( [Twitter](https://twitter.com/SeyoneC))\n\nDeep learning for chemistry and materials science remains a novel field with lots of potiential. However, the popularity of transfer learning based methods in areas such as NLP and computer vision have not yet been effectively developed in computational chemistry + machine learning. Using HuggingFace's suite of models and the ByteLevel tokenizer, we are able to train a large-transformer model, RoBERTa, on a large corpus of 100k SMILES strings from a commonly known benchmark chemistry dataset, ZINC.\n\nTraining RoBERTa over 5 epochs, the model achieves a pretty good loss of 0.398, and may likely continue to decrease if trained for a larger number of epochs. The model can predict tokens within a SMILES sequence/molecule, allowing for variants of a molecule within discoverable chemical space to be predicted.\n\nBy applying the representations of functional groups and atoms learned by the model, we can try to tackle problems of toxicity, solubility, drug-likeness, and synthesis accessibility on smaller datasets using the learned representations as features for graph convolution and attention models on the graph structure of molecules, as well as fine-tuning of BERT. Finally, we propose the use of attention visualization as a helpful tool for chemistry practitioners and students to quickly identify important substructures in various chemical properties.\n\nAdditionally, visualization of the attention mechanism have been seen through previous research as incredibly valuable towards chemical reaction classification. The applications of open-sourcing large-scale transformer models such as RoBERTa with HuggingFace may allow for the acceleration of these individual research directions.\n\nA link to a repository which includes the training, uploading and evaluation notebook (with sample predictions on compounds such as Remdesivir) can be found [here](https://github.com/seyonechithrananda/bert-loves-chemistry). All of the notebooks can be copied into a new Colab runtime for easy execution.\n\nFor the sake of this tutorial, we'll be fine-tuning RoBERTa on a small-scale molecule dataset, to show the potiential and effectiveness of HuggingFace's NLP-based transfer learning applied to computational chemistry. Output for some cells are purposely cleared for readability, so do not worry if some output messages for your cells differ!\n\nInstalling DeepChem from source, alongside RDKit for molecule visualizations\n\n```\n\nIn\u00a0[1]:\n\n!pip install transformers\n```\n\n```\n\nCollecting transformers\n  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 675kB 4.6MB/s\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\nCollecting sentencepiece\n  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 23.9MB/s\nRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\nCollecting tokenizers==0.7.0\n  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8MB 40.2MB/s\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\nRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\nCollecting sacremoses\n  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 57.9MB/s\nRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... done\n  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5b83ab4c2e1f1420040b2a1c7b2a43e2f0eb4c3ae1c251ab5ff24cc5baf3bff9\n  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\nSuccessfully built sacremoses\nInstalling collected packages: sentencepiece, tokenizers, sacremoses, transformers\nSuccessfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n\n```\n\n```\n\nIn\u00a0[2]:\n\nimport sys\n!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n# !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\nif not 'bertviz_repo' in sys.path:\n  sys.path += ['bertviz_repo']\n!pip install regex\n```\n\n```\n\nCloning into 'bertviz_repo'...\nremote: Enumerating objects: 1074, done.\nremote: Total 1074 (delta 0), reused 0 (delta 0), pack-reused 1074\nReceiving objects: 100% (1074/1074), 99.41 MiB | 27.70 MiB/s, done.\nResolving deltas: 100% (687/687), done.\nRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n\n```\n\nWe want to install NVIDIA's Apex tool, for the training pipeline used by `simple-transformers` and Weights and Biases.\n\n```\n\nIn\u00a0[\u00a0]:\n\n!git clone https://github.com/NVIDIA/apex\n!cd /content/apex\n!pip install -v --no-cache-dir /content/apex\n!cd ..\n```\n\nNow, to ensure our model demonstrates an understanding of chemical syntax and molecular structure, we'll be testing it on predicting a masked token/character within the SMILES molecule for Remdesivir.\n\n```\n\nIn\u00a0[4]:\n\n# Test if NVIDIA apex training tool works\nfrom apex import amp\n```\n\n```\n\nIn\u00a0[5]:\n\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\nfrom bertviz import head_view\n\nmodel = AutoModelWithLMHead.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\ntokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n\nfill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n```\n\n```\n\n/usr/local/l...",
      "url": "https://notebook.community/deepchem/deepchem/examples/tutorials/22_Transfer_Learning_With_HuggingFace_tox21"
    }
  ]
}