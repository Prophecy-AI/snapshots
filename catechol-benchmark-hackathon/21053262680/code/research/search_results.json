{
  "query": "How do you implement a Graph Neural Network for molecular property prediction using PyTorch Geometric with SMILES input?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Implementing a molecular\u2011property GNN with PyTorch\u202fGeometric from SMILES strings** ([Result\u202f5](https://www.blopig.com/blog/2022/02/how-to-turn-a-smiles-string-into-a-molecular-graph-for-pytorch-geometric), [Result\u202f3](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/smiles.html), [Result\u202f2](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/molecule_net.html), [Result\u202f7](https://medium.com/data-science/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4), [Result\u202f8](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html))  \n\n1. **Install the required packages**  \n   ```bash\n   conda install -c conda-forge pytorch-geometric rdkit\n   ```  \n   (PyG provides the GNN layers; RDKit parses SMILES.)  \n\n2. **Import libraries**  \n   ```python\n   import torch\n   from torch_geometric.data import Data, DataLoader\n   from torch_geometric.nn import GCNConv, global_mean_pool\n   from torch_geometric.utils import from_smiles   # converts SMILES \u2192 Data\n   from rdkit import Chem\n   ```  \n   (Package imports as shown in the tutorial.)\u202f([Result\u202f5](https://www.blopig.com/blog/2022/02/how-to-turn-a-smiles-string-into-a-molecular-graph-for-pytorch-geometric))  \n\n3. **Convert SMILES strings to graph objects**  \n   ```python\n   smiles_list = [\"CCO\", \"c1ccccc1\", ...]          # your molecules\n   data_list = [from_smiles(s) for s in smiles_list]   # creates torch_geometric.data.Data\n   ```  \n   `from_smiles` internally uses RDKit to build node/edge feature tensors (atom maps, bond maps) as defined in the utility module.\u202f([Result\u202f3](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/smiles.html), [Result\u202f2](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/molecule_net.html))  \n\n4. **Create a DataLoader**  \n   ```python\n   loader = DataLoader(data_list, batch_size=32, shuffle=True)\n   ```  \n\n5. **Define a simple GCN for property prediction**  \n   ```python\n   class MolGCN(torch.nn.Module):\n       def __init__(self, hidden_dim=64, out_dim=1):\n           super().__init__()\n           self.conv1 = GCNConv(in_channels=data_list[0].x.shape[1], out_channels=hidden_dim)\n           self.conv2 = GCNConv(hidden_dim, hidden_dim)\n           self.lin   = torch.nn.Linear(hidden_dim, out_dim)\n\n       def forward(self, x, edge_index, batch):\n           x = torch.relu(self.conv1(x, edge_index))\n           x = torch.relu(self.conv2(x, edge_index))\n           x = global_mean_pool(x, batch)          # graph\u2011level readout\n           return self.lin(x).squeeze()\n   ```  \n   This follows the GCN architecture described in the \u201cBuilding a Graph Convolutional Network\u201d tutorial.\u202f([Result\u202f7](https://medium.com/data-science/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4))  \n\n6. **Train the model**  \n   ```python\n   model = MolGCN()\n   optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n   loss_fn = torch.nn.MSELoss()   # use appropriate loss (e.g., BCE for classification)\n\n   for epoch in range(100):\n       model.train()\n       total_loss = 0\n       for batch in loader:\n           optimizer.zero_grad()\n           pred = model(batch.x, batch.edge_index, batch.batch)\n           loss = loss_fn(pred, batch.y)   # assume batch.y holds the target property\n           loss.backward()\n           optimizer.step()\n           total_loss += loss.item()\n       print(f\"Epoch {epoch}: loss {total_loss/len(loader):.4f}\")\n   ```  \n   The training loop mirrors the example shown in the MolFeat\u2011PyG integration tutorial.\u202f([Result\u202f8](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html))  \n\n7. **Evaluate** (optional)  \n   ```python\n   model.eval()\n   with torch.no_grad():\n       preds, trues = [], []\n       for batch in val_loader:\n           pred = model(batch.x, batch.edge_index, batch.batch)\n           preds.append(pred.cpu())\n           trues.append(batch.y.cpu())\n   # compute RMSE, R\u00b2, etc.\n   ```  \n\nThese steps take raw SMILES strings, turn them into PyG graph objects, build a GCN, and train it to predict molecular properties. Adjust the architecture, loss, and hyper\u2011parameters to suit your specific dataset (e.g., using `torch_geometric.datasets.MoleculeNet` for standard benchmarks).",
      "url": ""
    },
    {
      "title": "Source code for torch_geometric.datasets.molecule_net",
      "text": "<div><div><pre><span></span><span>import</span><span> </span><span>os</span>\n<span>import</span><span> </span><span>os.path</span><span> </span><span>as</span><span> </span><span>osp</span>\n<span>import</span><span> </span><span>re</span>\n<span>import</span><span> </span><span>warnings</span>\n<span>from</span><span> </span><span>typing</span><span> </span><span>import</span> <span>Callable</span><span>,</span> <span>Dict</span><span>,</span> <span>Optional</span><span>,</span> <span>Tuple</span><span>,</span> <span>Union</span>\n<span>import</span><span> </span><span>torch</span>\n<span>from</span><span> </span><span>torch_geometric.data</span><span> </span><span>import</span> <span>InMemoryDataset</span><span>,</span> <span>download_url</span><span>,</span> <span>extract_gz</span>\n<span>from</span><span> </span><span>torch_geometric.utils</span><span> </span><span>import</span> <span>from_smiles</span> <span>as</span> <span>_from_smiles</span>\n<p><a href=\"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.MoleculeNet.html#torch_geometric.datasets.MoleculeNet\">[docs]</a><span>class</span><span> </span><span>MoleculeNet</span><span>(</span><span>InMemoryDataset</span><span>):</span>\n<span> </span><span>r</span><span>\"\"\"The `MoleculeNet &lt;http://moleculenet.org/datasets-1&gt;`_ benchmark</span>\n<span> collection from the `\"MoleculeNet: A Benchmark for Molecular Machine</span>\n<span> Learning\" &lt;https://arxiv.org/abs/1703.00564&gt;`_ paper, containing datasets</span>\n<span> from physical chemistry, biophysics and physiology.</span>\n<span> All datasets come with the additional node and edge features introduced by</span>\n<span> the :ogb:`null`</span>\n<span> `Open Graph Benchmark &lt;https://ogb.stanford.edu/docs/graphprop/&gt;`_.</span>\n<span> Args:</span>\n<span> root (str): Root directory where the dataset should be saved.</span>\n<span> name (str): The name of the dataset (:obj:`\"ESOL\"`, :obj:`\"FreeSolv\"`,</span>\n<span> :obj:`\"Lipo\"`, :obj:`\"PCBA\"`, :obj:`\"MUV\"`, :obj:`\"HIV\"`,</span>\n<span> :obj:`\"BACE\"`, :obj:`\"BBBP\"`, :obj:`\"Tox21\"`, :obj:`\"ToxCast\"`,</span>\n<span> :obj:`\"SIDER\"`, :obj:`\"ClinTox\"`).</span>\n<span> transform (callable, optional): A function/transform that takes in an</span>\n<span> :obj:`torch_geometric.data.Data` object and returns a transformed</span>\n<span> version. The data object will be transformed before every access.</span>\n<span> (default: :obj:`None`)</span>\n<span> pre_transform (callable, optional): A function/transform that takes in</span>\n<span> an :obj:`torch_geometric.data.Data` object and returns a</span>\n<span> transformed version. The data object will be transformed before</span>\n<span> being saved to disk. (default: :obj:`None`)</span>\n<span> pre_filter (callable, optional): A function that takes in an</span>\n<span> :obj:`torch_geometric.data.Data` object and returns a boolean</span>\n<span> value, indicating whether the data object should be included in the</span>\n<span> final dataset. (default: :obj:`None`)</span>\n<span> force_reload (bool, optional): Whether to re-process the dataset.</span>\n<span> (default: :obj:`False`)</span>\n<span> from_smiles (callable, optional): A custom function that takes a SMILES</span>\n<span> string and outputs a :obj:`~torch_geometric.data.Data` object.</span>\n<span> If not set, defaults to :meth:`~torch_geometric.utils.from_smiles`.</span>\n<span> (default: :obj:`None`)</span>\n<span> **STATS:**</span>\n<span> .. list-table::</span>\n<span> :widths: 20 10 10 10 10 10</span>\n<span> :header-rows: 1</span>\n<span> * - Name</span>\n<span> - #graphs</span>\n<span> - #nodes</span>\n<span> - #edges</span>\n<span> - #features</span>\n<span> - #classes</span>\n<span> * - ESOL</span>\n<span> - 1,128</span>\n<span> - ~13.3</span>\n<span> - ~27.4</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - FreeSolv</span>\n<span> - 642</span>\n<span> - ~8.7</span>\n<span> - ~16.8</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - Lipophilicity</span>\n<span> - 4,200</span>\n<span> - ~27.0</span>\n<span> - ~59.0</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - PCBA</span>\n<span> - 437,929</span>\n<span> - ~26.0</span>\n<span> - ~56.2</span>\n<span> - 9</span>\n<span> - 128</span>\n<span> * - MUV</span>\n<span> - 93,087</span>\n<span> - ~24.2</span>\n<span> - ~52.6</span>\n<span> - 9</span>\n<span> - 17</span>\n<span> * - HIV</span>\n<span> - 41,127</span>\n<span> - ~25.5</span>\n<span> - ~54.9</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - BACE</span>\n<span> - 1513</span>\n<span> - ~34.1</span>\n<span> - ~73.7</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - BBBP</span>\n<span> - 2,050</span>\n<span> - ~23.9</span>\n<span> - ~51.6</span>\n<span> - 9</span>\n<span> - 1</span>\n<span> * - Tox21</span>\n<span> - 7,831</span>\n<span> - ~18.6</span>\n<span> - ~38.6</span>\n<span> - 9</span>\n<span> - 12</span>\n<span> * - ToxCast</span>\n<span> - 8,597</span>\n<span> - ~18.7</span>\n<span> - ~38.4</span>\n<span> - 9</span>\n<span> - 617</span>\n<span> * - SIDER</span>\n<span> - 1,427</span>\n<span> - ~33.6</span>\n<span> - ~70.7</span>\n<span> - 9</span>\n<span> - 27</span>\n<span> * - ClinTox</span>\n<span> - 1,484</span>\n<span> - ~26.1</span>\n<span> - ~55.5</span>\n<span> - 9</span>\n<span> - 2</span>\n<span> \"\"\"</span>\n <span>url</span> <span>=</span> <span>'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/</span><span>{}</span><span>'</span>\n <span># Format: name: (display_name, url_name, csv_name, smiles_idx, y_idx)</span>\n <span>names</span><span>:</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>Tuple</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>,</span> <span>str</span><span>,</span> <span>int</span><span>,</span> <span>Union</span><span>[</span><span>int</span><span>,</span> <span>slice</span><span>]]]</span> <span>=</span> <span>{</span>\n <span>'esol'</span><span>:</span> <span>(</span><span>'ESOL'</span><span>,</span> <span>'delaney-processed.csv'</span><span>,</span> <span>'delaney-processed'</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>2</span><span>),</span>\n <span>'freesolv'</span><span>:</span> <span>(</span><span>'FreeSolv'</span><span>,</span> <span>'SAMPL.csv'</span><span>,</span> <span>'SAMPL'</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>),</span>\n <span>'lipo'</span><span>:</span> <span>(</span><span>'Lipophilicity'</span><span>,</span> <span>'Lipophilicity.csv'</span><span>,</span> <span>'Lipophilicity'</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>),</span>\n <span>'pcba'</span><span>:</span> <span>(</span><span>'PCBA'</span><span>,</span> <span>'pcba.csv.gz'</span><span>,</span> <span>'pcba'</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>slice</span><span>(</span><span>0</span><span>,</span> <span>128</span><span>)),</span>\n <span>'muv'</span><span>:</span> <span>(</span><span>'MUV'</span><span>,</span> <span>'muv.csv.gz'</span><span>,</span> <span>'muv'</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>slice</span><span>(</span><span>0</span><span>,</span> <span>17</span><span>)),</span>\n <span>'hiv'</span><span>:</span> <span>(</span><span>'HIV'</span><span>,</span> <span>'HIV.csv'</span><span>,</span> <span>'HIV'</span><span>,</span> <span>0</span><span>,</span> <span>-</span><span>1</span><span>),</span>\n <span>'bace'</span><span>:</span> <span>(</span><span>'BACE'</span><span>,</span> <span>'bace.csv'</span><span>,</span> <span>'bace'</span><span>,</span> <span>0</span><span>,</span> <span>2</span><span>),</span>\n <span>'bbbp'</span><span>:</span> <span>(</span><span>'BBBP'</span><span>,</span> <span>'BBBP.csv'</span><span>,</span> <span>'BBBP'</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>2</span><span>),</span>\n <span>'tox21'</span><span>:</span> <span>(</span><span>'Tox21'</span><span>,</span> <span>'tox21.csv.gz'</span><span>,</span> <span>'tox21'</span><span>...",
      "url": "https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/molecule_net.html"
    },
    {
      "title": "torch_geometric.utils.smiles \u2014 pytorch_geometric documentation",
      "text": "- [Module code](https://pytorch-geometric.readthedocs.io/en/latest/_modules/index.html)\n- torch\\_geometric.utils.smiles\n\n* * *\n\n# Source code for torch\\_geometric.utils.smiles\n\n```\nfromtypingimport Any, Dict, List\n\nimporttorch\n\nimporttorch_geometric\n\nx_map: Dict[str, List[Any]] = {\n    'atomic_num':\n    list(range(0, 119)),\n    'chirality': [\n        'CHI_UNSPECIFIED',\n        'CHI_TETRAHEDRAL_CW',\n        'CHI_TETRAHEDRAL_CCW',\n        'CHI_OTHER',\n        'CHI_TETRAHEDRAL',\n        'CHI_ALLENE',\n        'CHI_SQUAREPLANAR',\n        'CHI_TRIGONALBIPYRAMIDAL',\n        'CHI_OCTAHEDRAL',\n    ],\n    'degree':\n    list(range(0, 11)),\n    'formal_charge':\n    list(range(-5, 7)),\n    'num_hs':\n    list(range(0, 9)),\n    'num_radical_electrons':\n    list(range(0, 5)),\n    'hybridization': [\n        'UNSPECIFIED',\n        'S',\n        'SP',\n        'SP2',\n        'SP3',\n        'SP3D',\n        'SP3D2',\n        'OTHER',\n    ],\n    'is_aromatic': [False, True],\n    'is_in_ring': [False, True],\n}\n\ne_map: Dict[str, List[Any]] = {\n    'bond_type': [\n        'UNSPECIFIED',\n        'SINGLE',\n        'DOUBLE',\n        'TRIPLE',\n        'QUADRUPLE',\n        'QUINTUPLE',\n        'HEXTUPLE',\n        'ONEANDAHALF',\n        'TWOANDAHALF',\n        'THREEANDAHALF',\n        'FOURANDAHALF',\n        'FIVEANDAHALF',\n        'AROMATIC',\n        'IONIC',\n        'HYDROGEN',\n        'THREECENTER',\n        'DATIVEONE',\n        'DATIVE',\n        'DATIVEL',\n        'DATIVER',\n        'OTHER',\n        'ZERO',\n    ],\n    'stereo': [\n        'STEREONONE',\n        'STEREOANY',\n        'STEREOZ',\n        'STEREOE',\n        'STEREOCIS',\n        'STEREOTRANS',\n    ],\n    'is_conjugated': [False, True],\n}\n\n[docs]deffrom_rdmol(mol: Any) -> 'torch_geometric.data.Data':\nr\"\"\"Converts a :class:`rdkit.Chem.Mol` instance to a\n    :class:`torch_geometric.data.Data` instance.\n\n    Args:\n        mol (rdkit.Chem.Mol): The :class:`rdkit` molecule.\n    \"\"\"\n    fromrdkitimport Chem\n\n    fromtorch_geometric.dataimport Data\n\n    assert isinstance(mol, Chem.Mol)\n\n    xs: List[List[int]] = []\n    for atom in mol.GetAtoms():\n        row: List[int] = []\n        row.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n        row.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n        row.append(x_map['degree'].index(atom.GetTotalDegree()))\n        row.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n        row.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n        row.append(x_map['num_radical_electrons'].index(\n            atom.GetNumRadicalElectrons()))\n        row.append(x_map['hybridization'].index(str(atom.GetHybridization())))\n        row.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n        row.append(x_map['is_in_ring'].index(atom.IsInRing()))\n        xs.append(row)\n\n    x = torch.tensor(xs, dtype=torch.long).view(-1, 9)\n\n    edge_indices, edge_attrs = [], []\n    for bond in mol.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n\n        e = []\n        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n\n        edge_indices += [[i, j], [j, i]]\n        edge_attrs += [e, e]\n\n    edge_index = torch.tensor(edge_indices)\n    edge_index = edge_index.t().to(torch.long).view(2, -1)\n    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n\n    if edge_index.numel() > 0:  # Sort indices.\n        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n\n    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n\n[docs]deffrom_smiles(\n    smiles: str,\n    with_hydrogen: bool = False,\n    kekulize: bool = False,\n) -> 'torch_geometric.data.Data':\nr\"\"\"Converts a SMILES string to a :class:`torch_geometric.data.Data`\n    instance.\n\n    Args:\n        smiles (str): The SMILES string.\n        with_hydrogen (bool, optional): If set to :obj:`True`, will store\n            hydrogens in the molecule graph. (default: :obj:`False`)\n        kekulize (bool, optional): If set to :obj:`True`, converts aromatic\n            bonds to single/double bonds. (default: :obj:`False`)\n    \"\"\"\n    fromrdkitimport Chem, RDLogger\n\n    RDLogger.DisableLog('rdApp.*')  # type: ignore\n\n    mol = Chem.MolFromSmiles(smiles)\n\n    if mol is None:\n        mol = Chem.MolFromSmiles('')\n    if with_hydrogen:\n        mol = Chem.AddHs(mol)\n    if kekulize:\n        Chem.Kekulize(mol)\n\n    data = from_rdmol(mol)\n    data.smiles = smiles\n    return data\n\n[docs]defto_rdmol(\n    data: 'torch_geometric.data.Data',\n    kekulize: bool = False,\n) -> Any:\n\"\"\"Converts a :class:`torch_geometric.data.Data` instance to a\n    :class:`rdkit.Chem.Mol` instance.\n\n    Args:\n        data (torch_geometric.data.Data): The molecular graph data.\n        kekulize (bool, optional): If set to :obj:`True`, converts aromatic\n            bonds to single/double bonds. (default: :obj:`False`)\n    \"\"\"\n    fromrdkitimport Chem\n\n    mol = Chem.RWMol()\n\n    assert data.x is not None\n    assert data.num_nodes is not None\n    assert data.edge_index is not None\n    assert data.edge_attr is not None\n    for i in range(data.num_nodes):\n        atom = Chem.Atom(int(data.x[i, 0]))\n        atom.SetChiralTag(Chem.rdchem.ChiralType.values[int(data.x[i, 1])])\n        atom.SetFormalCharge(x_map['formal_charge'][int(data.x[i, 3])])\n        atom.SetNumExplicitHs(x_map['num_hs'][int(data.x[i, 4])])\n        atom.SetNumRadicalElectrons(x_map['num_radical_electrons'][int(\n            data.x[i, 5])])\n        atom.SetHybridization(Chem.rdchem.HybridizationType.values[int(\n            data.x[i, 6])])\n        atom.SetIsAromatic(bool(data.x[i, 7]))\n        mol.AddAtom(atom)\n\n    edges = [tuple(i) for i in data.edge_index.t().tolist()]\n    visited = set()\n\n    for i in range(len(edges)):\n        src, dst = edges[i]\n        if tuple(sorted(edges[i])) in visited:\n            continue\n\n        bond_type = Chem.BondType.values[int(data.edge_attr[i, 0])]\n        mol.AddBond(src, dst, bond_type)\n\n        # Set stereochemistry:\n        stereo = Chem.rdchem.BondStereo.values[int(data.edge_attr[i, 1])]\n        if stereo != Chem.rdchem.BondStereo.STEREONONE:\n            db = mol.GetBondBetweenAtoms(src, dst)\n            db.SetStereoAtoms(dst, src)\n            db.SetStereo(stereo)\n\n        # Set conjugation:\n        is_conjugated = bool(data.edge_attr[i, 2])\n        mol.GetBondBetweenAtoms(src, dst).SetIsConjugated(is_conjugated)\n\n        visited.add(tuple(sorted(edges[i])))\n\n    mol = mol.GetMol()\n\n    if kekulize:\n        Chem.Kekulize(mol)\n\n    Chem.SanitizeMol(mol)\n    Chem.AssignStereochemistry(mol)\n\n    return mol\n\n[docs]defto_smiles(\n    data: 'torch_geometric.data.Data',\n    kekulize: bool = False,\n) -> str:\n\"\"\"Converts a :class:`torch_geometric.data.Data` instance to a SMILES\n    string.\n\n    Args:\n        data (torch_geometric.data.Data): The molecular graph.\n        kekulize (bool, optional): If set to :obj:`True`, converts aromatic\n            bonds to single/double bonds. (default: :obj:`False`)\n    \"\"\"\n    fromrdkitimport Chem\n    mol = to_rdmol(data, kekulize=kekulize)\n    return Chem.MolToSmiles(mol, isomericSmiles=True)\n\n```",
      "url": "https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/smiles.html"
    },
    {
      "title": "How to turn a SMILES string into a molecular graph for Pytorch Geometric",
      "text": "Despite some of their technical [issues](https://www.blopig.com/blog/2021/10/issues-with-graph-neural-networks-the-cracks-are-where-the-light-shines-through/), graph neural networks (GNNs) are quickly being adopted as one of the state-of-the-art methods for molecular property prediction. The differentiable extraction of molecular features from low-level molecular graphs has become a viable (although not always superior) alternative to classical molecular representation techniques such as Morgan fingerprints and molecular descriptor vectors.\n\nBut molecular data usually comes in the sequential form of labeled SMILES strings. It is not obvious for beginners how to optimally transform a SMILES string into a structured molecular graph object that can be used as an input for a GNN. In this post, we show how to convert a SMILES string into a molecular graph object which can subsequently be used for graph-based machine learning. We do so within the framework of **Pytorch Geometric** which currently is one of the best and most commonly used Python-based GNN-libraries.\n\nWe divide our task into three high-level steps:\n\n1. We define a function that maps an RDKit atom object to a suitable atom feature vector.\n2. We define a function that maps an RDKit bond object to a suitable bond feature vector.\n3. We define a function that takes as its input a list of SMILES strings and associated labels and then uses the functions from 1.) and 2.) to create a list of labeled Pytorch Geometric graph objects as its output.\n\n**Step 0: Import Packages**\n\nAs always, we first import the necessary Python packages for our endeavour:\n\n```\n# import packages\n\n# general tools\nimport numpy as np\n\n# RDkit\nfrom rdkit import Chem\nfrom rdkit.Chem.rdmolops import GetAdjacencyMatrix\n\n# Pytorch and Pytorch Geometric\nimport torch\nfrom torch_geometric.data import Data\nfrom torch.utils.data import DataLoader\n```\n\n**Step 1: Atom Featurisation**\n\nWe start by defining an auxiliary function which transforms a value x into a one-hot encoding based on a list of permitted values for x:\n\n```\ndef one_hot_encoding(x, permitted_list):\n    \"\"\"\n    Maps input elements x which are not in the permitted list to the last element\n    of the permitted list.\n    \"\"\"\n\n    if x not in permitted_list:\n        x = permitted_list[-1]\n\n    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n\n    return binary_encoding\n```\n\nNow we use this auxiliary function to define the actual atom featurisation function:\n\n```\ndef get_atom_features(atom,\n                      use_chirality = True,\n                      hydrogens_implicit = True):\n    \"\"\"\n    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n    \"\"\"\n\n    # define list of permitted atoms\n\n    permitted_list_of_atoms =  ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca','Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n\n    if hydrogens_implicit == False:\n        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n\n    # compute atom features\n\n    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n\n    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n\n    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n\n    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n\n    is_in_a_ring_enc = [int(atom.IsInRing())]\n\n    is_aromatic_enc = [int(atom.GetIsAromatic())]\n\n    atomic_mass_scaled = [float((atom.GetMass() - 10.812)/116.092)]\n\n    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)]\n\n    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)]\n\n    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n\n    if use_chirality == True:\n        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n        atom_feature_vector += chirality_type_enc\n\n    if hydrogens_implicit == True:\n        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n        atom_feature_vector += n_hydrogens_enc\n\n    return np.array(atom_feature_vector)\n```\n\nTo encapsulate as much information as possible within the molecular graph, we include a plethora of atomic features: atom type, number of heavy atom neighbours, formal charge, hybridisation type, whether the atom is in a ring, whether the atom is aromatic, atomic mass, Van der Waals radius, and covalent radius. The last three properties are numerical in nature and are thus automatically scaled to a reasonable range using empirically estimated quantities. The user can explicitly specify whether to use chirality as a stereochemical feature and whether to treat hydrogen atoms implicitly or explicitly.\n\n**Step 2: Bond Featurisation**\n\nNow that we have constructed a function to conveniently turn RDKit atom objects into feature vectors, we define an analogous function for RDKit bond objects:\n\n```\ndef get_bond_features(bond,\n                      use_stereochemistry = True):\n    \"\"\"\n    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n    \"\"\"\n\n    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n\n    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n\n    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n\n    bond_is_in_ring_enc = [int(bond.IsInRing())]\n\n    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n\n    if use_stereochemistry == True:\n        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n        bond_feature_vector += stereo_type_enc\n\n    return np.array(bond_feature_vector)\n```\n\nThe bond features we consider in the above function are: bond type, whether the bond is conjugated, and whether the bond is in a ring. As an additional option, the user can specify whether to include E-Z stereochemical features around double bonds.\n\n**Step 3: Generating labeled Pytorch Geometric Graph Objects**\n\nEquipped with suitable functions to turn RDKit atom objects and RDKit bond objects into informative feature vectors, we swiftly move on to define a function which turns a list of SMILES strings and an associated list of labels (such as pKi values) into a list of Pytorch Geometric graph objects:\n\n```\ndef create_pytorch_geometric_graph_data_list_from_smiles_and_labels(x_smiles, y):\n    \"\"\"\n    Inputs:\n\n    x_smiles = [smiles_1, smiles_2, ....] ... a list of SMILES strings\n    y = [y_1, y_2, ...] ... a list of numerial labels for the SMILES strings (such as associated pKi values)\n\n    Outputs:\n\n    data_list = [G_1, G_2, ...] ... a list of torch_geometric.data.Data objects which represent labeled molecular graphs that can readily be used for machine learning\n\n    \"\"\"\n\n    data_list = []\n\n    for (smiles, y_val) in zip(x_smiles, y):\n\n        # convert SMILES to RDKit mol object\n        mol = Chem.MolFromSmiles(smiles)\n\n        # get feature dimensions\n        n_nodes = mol.GetNumAtoms()\n        n_edges = 2*mol.GetNumBonds()\n        unrelated_smiles = \"O=O\"\n        unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n        n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n        n_edge_features = len(get_bond_features(unrelated_mol.GetBondBetweenAtoms(0,1)))\n\n        # construct node feature matrix X of shape ...",
      "url": "https://www.blopig.com/blog/2022/02/how-to-turn-a-smiles-string-into-a-molecular-graph-for-pytorch-geometric"
    },
    {
      "title": "Building A Graph Convolutional Network for Molecular Property Prediction",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F978b0ae10ec4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-978b0ae10ec4---------------------------------------)\n\n\u00b7\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n## Artificial Intelligence\n\n# Building A Graph Convolutional Network for Molecular Property Prediction\n\n## Tutorial to make molecular graphs and develop a simple PyTorch-based GCN\n\n[Gaurav Deshmukh](https://medium.com/@ChemAndCode?source=post_page---byline--978b0ae10ec4---------------------------------------)\n\n17 min read\n\n\u00b7\n\nDec 23, 2023\n\n--\n\n6\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nPhoto by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nArtificial intelligence has taken the world by storm. Every week, new models, tools, and applications emerge that promise to push the boundaries of human endeavor. The availability of open-source tools that enable users to train and employ complex machine learning models in a modest number of lines of code have truly democratized AI; at the same time, while many of these off-the-shelf models may provide excellent predictive capabilities, their usage as black box models may deprive inquisitive students of AI of a deeper understanding of how they work and why they were developed in the first place. This understanding is particularly important in the natural sciences, where knowing that a model is accurate is not enough \u2014 it is also essential to know its connection to other physical theories, its limitations, and its generalizability to other systems. In this article, we will explore the basics of one particular ML model \u2014 a graph convolutional network \u2014 through the lens of chemistry. This is not meant to be a mathematically rigorous exploration; instead, we will try to compare features of the network with traditional models in the natural sciences and think about why it works as well as it does.\n\n## 1\\. The need for graphs and graph neural networks\n\nA model in chemistry or physics is usually a continuous function, say _y=f(x\u2081, x\u2082, x\u2083, \u2026, x\u2099)_, in which _x\u2081, x\u2082, x\u2083, \u2026, x\u2099_ are the inputs and _y_ is the output. An example of such a model is the equation that determines the electrostatic interaction (or force) between two point charges _q\u2081_ and _q\u2082_ separated by a distance _r_ present in a medium with relative permittivity _\u03b5\u1d63_, commonly termed as Coulomb\u2019s law.\n\nPress enter or click to view image in full size\n\nFigure 1: The Coulomb equation as a model for electrostatic interactions between point charges (Image by author)\n\nIf we did not know this relationship but, hypothetically, had multiple datapoints each including the interaction between point charges (the output) and the corresponding inputs, we could fit an artificial neural network to predict the interaction for any given point charges for any given separation in a medium with a specified permittivity. In the case of this problem, admittedly ignoring some important caveats, creating a data-driven model for a physical problem is relatively straightforward.\n\nNow consider the problem of prediction of a particular property, say solubility in water, from the structure of a molecule. First, there is no obvious set of inputs to describe a molecule. You could use various features, such as bond lengths, bond angles, number of different types of elements, number of rings, and so forth. However, there is no guarantee that any such arbitrary set is bound to work well for all molecules.\n\nSecond, unlike the example of the point charges, the inputs may not necessarily reside in a continuous space. For example, we can think of methanol, ethanol, and propanol as a set of molecules with increasing chain lengths; there is no notion, however, of anything between them \u2014 chain length is a discrete parameter and there is no way to interpolate between methanol and ethanol to get other molecules. Having a continuous space of inputs is essential to calculate derivatives of the model, which can then be used for optimization of the chosen property.\n\nTo overcome these problems, various methods for encoding molecules have been proposed. One such method is textual representation using schemes such as SMILES and SELFIES. There is a large body of literature on this representation, and I direct the interested reader to this [helpful review](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf). The second method involves representing molecules as graphs. While each method has its advantages and shortcomings, graph representations feel more intuitive for chemistry.\n\nA graph is a mathematical structure consisting of nodes connected by edges that represent relationships between nodes. Molecules fit naturally into this structure \u2014 atoms become nodes, and bonds become edges. Each node in the graph is represented by a vector that encodes properties of the corresponding atom. Usually, a one-hot encoding scheme suffices (more on this in the next section). These vectors can be stacked to create a _node matrix._ Relationships between nodes \u2014 denoted by edges \u2014 can be delineated through a square _adjacency matrix,_ wherein every element _a\u1d62\u2c7c_ is either 1 or 0 depending on whether the two nodes _i_ and _j_ are connected by an edge or not respectively. The diagonal elements are set to 1, indicating a self-connection, which makes the matrix amenable to convolutions (as you will see in the next section). More complex graph representations can be developed, in which edge properties are also one-hot encoded in a separate matrix, but we shall leave that for another article. These node and adjacency matrices will serve as inputs to our model.\n\nPress enter or click to view image in full size\n\nFigure 2: Representation of an acetamide molecule as a graph with one-hot encodings of atomic numbers of nodes (Image by author)\n\nTypically, artificial neural network models accept a 1-dimensional vector of inputs. For multidimensional inputs, such as images, a class of models called convolutional neural networks was developed. In our case too we have 2-dimensional matrices as inputs, and therefore, need a modified network that can accept these as inputs. Graph neural networks were developed to operate on such node and adjacency matrices to convert them into appropriate 1-dimensional vectors that can then be passed through hidden layers of a vanilla artificial neural network to generate outputs. There are many types of graph neural networks, such as graph convolutional networks, message passing networks, graph attention networks, and so forth, which primarily differ ...",
      "url": "https://medium.com/data-science/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4"
    },
    {
      "title": "Training a GNN with PyG",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html#pyg-integration)\n\n# Training a GNN with PyG\n\nIn\u00a0\\[1\\]:\n\nCopied!\n\n```\n%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\n\n```\n\n%load\\_ext autoreload\n%autoreload 2\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\n\n## PyG integration [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html\\#pyg-integration)\n\nCommunity contribution\n\nCurious how one would run this tutorial on [Graphcore IPUs](https://www.graphcore.ai/products/ipu)? See this tutorial contributed by [@s-maddrellmander](https://github.com/s-maddrellmander):\n[![Run on Gradient](https://camo.githubusercontent.com/c9931a1689c37ab786edd3e1e5f59b9a6f7d097628c4689ce2432563ef884524/68747470733a2f2f6173736574732e706170657273706163652e696f2f696d672f6772616469656e742d62616467652e737667)](https://ipu.dev/lDvDHL)\n\nAs seen in the [molfeat integration tutorial](https://molfeat-docs.datamol.io/stable/tutorials/integration.html), molfeat integrates easily with the PyTorch ecosystem. In this tutorial, we will demonstrate how you can integrate molfeat with [PyG](https://pytorch-geometric.readthedocs.io/en/latest/) for training SOTA GNNs.\n\nTo run this tutorial, you will need to install `pytorch-geometric`.\n\n`mamba install -c conda-forge pytorch_geometric`\n\nIn\u00a0\\[2\\]:\n\nCopied!\n\n```\nfrom molfeat.trans.graph.adj import PYGGraphTransformer\nfrom molfeat.calc.atom import AtomCalculator\nfrom molfeat.calc.bond import EdgeMatCalculator\n\n```\n\nfrom molfeat.trans.graph.adj import PYGGraphTransformer\nfrom molfeat.calc.atom import AtomCalculator\nfrom molfeat.calc.bond import EdgeMatCalculator\n\n### Featurizer [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html\\#featurizer)\n\nWe first start by defining our featurizer. We will use the `PYGGraphTransformer` from molfeat with atom and bond featurizers\n\nIn\u00a0\\[3\\]:\n\nCopied!\n\n```\nfeaturizer = PYGGraphTransformer(\n    atom_featurizer=AtomCalculator(),\n    bond_featurizer=EdgeMatCalculator()\n)\n\n```\n\nfeaturizer = PYGGraphTransformer(\natom\\_featurizer=AtomCalculator(),\nbond\\_featurizer=EdgeMatCalculator()\n)\n\n### Dataset [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html\\#dataset)\n\nFor the dataset, we will use the `Lipophilicity` dataset (LogD) from MoleculeNet, which contains experimental results of octanol/water distribution coefficient at pH=7.4\n\nIn\u00a0\\[4\\]:\n\nCopied!\n\n```\ndf = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\")\n\n```\n\ndf = pd.read\\_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\")\n\nIn\u00a0\\[5\\]:\n\nCopied!\n\n```\ndf.head()\n\n```\n\ndf.head()\n\nOut\\[5\\]:\n\n|  | CMPD\\_CHEMBLID | exp | smiles |\n| --- | --- | --- | --- |\n| 0 | CHEMBL596271 | 3.54 | Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14 |\n| 1 | CHEMBL1951080 | -1.18 | COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)... |\n| 2 | CHEMBL1771 | 3.69 | COC(=O)\\[C@@H\\](N1CCc2sccc2C1)c3ccccc3Cl |\n| 3 | CHEMBL234951 | 3.37 | OC\\[C@H\\](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C... |\n| 4 | CHEMBL565079 | 3.10 | Cc1cccc(C\\[C@H\\](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N... |\n\nSince training a network with PyTorch requires defining a dataset and dataloader, we can define our custom dataset that will take **(1)** the SMILES, **(2)** the LogD measurement, and **(3)** our molfeat transformer as input to generate the data point we need for model training.\n\nIn\u00a0\\[6\\]:\n\nCopied!\n\n```\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch_geometric.utils import degree\n\nclass DTset(Dataset):\n    def __init__(self, smiles, y, featurizer):\n        super().__init__()\n        self.smiles = smiles\n        self.featurizer = featurizer\n        self.featurizer.auto_self_loop()\n        self.y = torch.tensor(y).unsqueeze(-1).float()\n        self.transformed_mols = self.featurizer(smiles)\n        self._degrees = None\n\n    @property\n    def num_atom_features(self):\n        return self.featurizer.atom_dim\n\n    @property\n    def num_output(self):\n        return self.y.shape[-1]\n\n    def __len__(self):\n        return len(self.transformed_mols)\n\n    @property\n    def num_bond_features(self):\n        return self.featurizer.bond_dim\n\n\n    @property\n    def degree(self):\n        if self._degrees is  None:\n            max_degree = -1\n            for data in self.transformed_mols:\n                d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n                max_degree = max(max_degree, int(d.max()))\n            # Compute the in-degree histogram tensor\n            deg = torch.zeros(max_degree + 1, dtype=torch.long)\n            for data in self.transformed_mols:\n                d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n                deg += torch.bincount(d, minlength=deg.numel())\n            self._degrees = deg\n        return self._degrees\n\n    def collate_fn(self, **kwargs):\n        # luckily the molfeat featurizer provides a collate functoin for PyG\n        return self.featurizer.get_collate_fn(**kwargs)\n\n    def __getitem__(self, index):\n        return self.transformed_mols[index], self.y[index]\n\n```\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch\\_geometric.utils import degree\nclass DTset(Dataset):\ndef \\_\\_init\\_\\_(self, smiles, y, featurizer):\nsuper().\\_\\_init\\_\\_()\nself.smiles = smiles\nself.featurizer = featurizer\nself.featurizer.auto\\_self\\_loop()\nself.y = torch.tensor(y).unsqueeze(-1).float()\nself.transformed\\_mols = self.featurizer(smiles)\nself.\\_degrees = None\n@property\ndef num\\_atom\\_features(self):\nreturn self.featurizer.atom\\_dim\n@property\ndef num\\_output(self):\nreturn self.y.shape\\[-1\\]\ndef \\_\\_len\\_\\_(self):\nreturn len(self.transformed\\_mols)\n@property\ndef num\\_bond\\_features(self):\nreturn self.featurizer.bond\\_dim\n@property\ndef degree(self):\nif self.\\_degrees is None:\nmax\\_degree = -1\nfor data in self.transformed\\_mols:\nd = degree(data.edge\\_index\\[1\\], num\\_nodes=data.num\\_nodes, dtype=torch.long)\nmax\\_degree = max(max\\_degree, int(d.max()))\n\\# Compute the in-degree histogram tensor\ndeg = torch.zeros(max\\_degree + 1, dtype=torch.long)\nfor data in self.transformed\\_mols:\nd = degree(data.edge\\_index\\[1\\], num\\_nodes=data.num\\_nodes, dtype=torch.long)\ndeg += torch.bincount(d, minlength=deg.numel())\nself.\\_degrees = deg\nreturn self.\\_degrees\ndef collate\\_fn(self, \\*\\*kwargs):\n\\# luckily the molfeat featurizer provides a collate functoin for PyG\nreturn self.featurizer.get\\_collate\\_fn(\\*\\*kwargs)\ndef \\_\\_getitem\\_\\_(self, index):\nreturn self.transformed\\_mols\\[index\\], self.y\\[index\\]\n\nIn\u00a0\\[7\\]:\n\nCopied!\n\n```\ndataset = DTset(df.smiles.values, df.exp.values, featurizer)\ngenerator = torch.Generator().manual_seed(42)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dt, test_dt = torch.utils.data.random_split(dataset, [train_size, test_size], generator=generator)\n\n```\n\ndataset = DTset(df.smiles.values, df.exp.values, featurizer)\ngenerator = torch.Generator().manual\\_seed(42)\ntrain\\_size = int(0.8 \\* len(dataset))\ntest\\_size = len(dataset) - train\\_size\ntrain\\_dt, test\\_dt = torch.utils.data.random\\_split(dataset, \\[train\\_size, test\\_size\\], generator=generator)\n\nIn\u00a0\\[8\\]:\n\nCopied!\n\n```\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dt, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn(return_pair=False))\ntest_loader = DataLoader(test_dt, batch_size=BATCH_SIZE, shuffle=False, collate_fn=dataset.collate_fn(return_pair=False))\n\n```\n\nBATCH\\_SIZE = 64\ntrain\\_loader = DataLoader(train\\_dt, batch\\_size=BATCH\\_SIZE, shuffle=True, collate\\_fn=dataset.collate\\_fn(return\\_pair=False))\ntest\\_loader = DataLoader(test\\_dt, batch\\_size=BATCH\\_SIZE, shuffle=False, collate\\_fn=dataset.collate\\_fn(return\\_pair=False))\n\n### Network + Training [\u00b6](https...",
      "url": "https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html"
    },
    {
      "title": "torch_geometric.llm.models.MoleculeGPT \u2014 pytorch_geometric documentation",
      "text": "<div> \n <div>\n <nav>\n \n </nav>\n <section><nav>\n <i></i>\n <a href=\"https://pytorch-geometric.readthedocs.io/en/latest/index.html\">pytorch_geometric</a>\n </nav>\n <section>\n<dl>\n<dt>\n<em><span>class</span><span> </span></em><span><span>MoleculeGPT</span></span><span>(</span><em><span><span>llm</span></span><span><span>:</span></span><span> </span><span><a href=\"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.llm.models.LLM.html#torch_geometric.llm.models.LLM\"><span>LLM</span></a></span></em>, <em><span><span>graph_encoder</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module\"><span>Module</span></a></span></em>, <em><span><span>smiles_encoder</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module\"><span>Module</span></a></span></em>, <em><span><span>mlp_out_channels</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/functions.html#int\"><span>int</span></a></span><span> </span><span><span>=</span></span><span> </span><span><span>32</span></span></em>, <em><span><span>max_tokens</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.Optional\"><span>Optional</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/functions.html#int\"><span>int</span></a><span><span>]</span></span></span><span> </span><span><span>=</span></span><span> </span><span><span>20</span></span></em><span>)</span><a href=\"https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/llm/models/molecule_gpt.html#MoleculeGPT\"><span><span>[source]</span></span></a><a href=\"#torch_geometric.llm.models.MoleculeGPT\">\uf0c1</a></dt>\n<dd><p>Bases: <a href=\"https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module\"><code><span>Module</span></code></a></p>\n<p>The MoleculeGPT model from the <a href=\"https://ai4d3.github.io/papers/34.pdf\">\u201cMoleculeGPT: Instruction\nFollowing Large Language Models for Molecular Property Prediction\u201d</a> paper.</p>\n<dl>\n<dt>Parameters<span>:</span></dt>\n<dd><ul>\n<li><p><strong>llm</strong> (<a href=\"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.llm.models.LLM.html#torch_geometric.llm.models.LLM\"><em>LLM</em></a>) \u2013 The LLM to use.</p></li>\n<li><p><strong>graph_encoder</strong> (<a href=\"https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module\"><em>torch.nn.Module</em></a>) \u2013 Encode 2D molecule graph.</p></li>\n<li><p><strong>smiles_encoder</strong> (<a href=\"https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module\"><em>torch.nn.Module</em></a>) \u2013 Encode 1D SMILES.</p></li>\n<li><p><strong>mlp_out_channels</strong> (<a href=\"https://docs.python.org/3/library/functions.html#int\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 The size of each embedding\nafter qformer encoding. (default: <code><span>32</span></code>)</p></li>\n<li><p><strong>max_tokens</strong> (<a href=\"https://docs.python.org/3/library/functions.html#int\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 Max output tokens of 1D/2D encoder.\n(default: <code><span>20</span></code>)</p></li>\n</ul>\n</dd>\n</dl>\n<div>\n<p>Warning</p>\n<p>This module has been tested with the following HuggingFace models</p>\n<ul>\n<li><p><code><span>llm_to_use=\"lmsys/vicuna-7b-v1.5\"</span></code></p></li>\n</ul>\n<p>and may not work with other models. See other models at <a href=\"https://huggingface.co/models\">HuggingFace\nModels</a> and let us know if you\nencounter any issues.</p>\n</div>\n<dl>\n<dt>\n<span><span>forward</span></span><span>(</span><em><span><span>x</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor\"><span>Tensor</span></a></span></em>, <em><span><span>edge_index</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor\"><span>Tensor</span></a></span></em>, <em><span><span>batch</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor\"><span>Tensor</span></a></span></em>, <em><span><span>edge_attr</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.Optional\"><span>Optional</span></a><span><span>[</span></span><a href=\"https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor\"><span>Tensor</span></a><span><span>]</span></span></span></em>, <em><span><span>smiles</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.List\"><span>List</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/stdtypes.html#str\"><span>str</span></a><span><span>]</span></span></span></em>, <em><span><span>instructions</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.List\"><span>List</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/stdtypes.html#str\"><span>str</span></a><span><span>]</span></span></span></em>, <em><span><span>label</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.List\"><span>List</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/stdtypes.html#str\"><span>str</span></a><span><span>]</span></span></span></em>, <em><span><span>additional_text_context</span></span><span><span>:</span></span><span> </span><span><a href=\"https://docs.python.org/3/library/typing.html#typing.Optional\"><span>Optional</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/typing.html#typing.List\"><span>List</span></a><span><span>[</span></span><a href=\"https://docs.python.org/3/library/stdtypes.html#str\"><span>str</span></a><span><span>]</span></span><span><span>]</span></span></span><span> </span><span><span>=</span></span><span> </span><span><span>None</span></span></em><span>)</span><a href=\"https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/llm/models/molecule_gpt.html#MoleculeGPT.forward\"><span><span>[source]</span></span></a><a href=\"#torch_geometric.llm.models.MoleculeGPT.forward\">\uf0c1</a></dt>\n<dd><p>Define the computation performed at every call.</p>\n<p>Should be overridden by all subclasses.</p>\n<div>\n<p>Note</p>\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code><span>Module</span></code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n</div>\n</dd></dl>\n</dd></dl>\n</section>\n </section>\n </div>\n \n</div>",
      "url": "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.llm.models.MoleculeGPT.html"
    },
    {
      "title": "torch_geometric.datasets.MoleculeNet \u2014 pytorch_geometric documentation",
      "text": "torch_geometric.datasets.MoleculeNet \u2014 pytorch_geometric documentation https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.MoleculeNet.html\ntorch_geometric.datasets.MoleculeNet \u2014 pytorch_geometric documentation\nNone\n2025-01-01T00:00:00Z\n# torch_geometric.datasets.MoleculeNet [](pytorch-geometric.readthedocs.io/en/latest/...)\n_class_ MoleculeNet( _root:[str](https://docs.python.org/3/library/stdtypes.html#str)_, _name:[str](https://docs.python.org/3/library/stdtypes.html#str)_, _transform:[Optional](https://docs.python.org/3/library/typing.html#typing.Optional)[ [Callable](https://docs.python.org/3/library/typing.html#typing.Callable)]=None_, _pre_transform:[Optional](https://docs.python.org/3/library/typing.html#typing.Optional)[ [Callable](https://docs.python.org/3/library/typing.html#typing.Callable)]=None_, _pre_filter:[Optional](https://docs.python.org/3/library/typing.html#typing.Optional)[ [Callable](https://docs.python.org/3/library/typing.html#typing.Callable)]=None_, _force_reload:[bool](https://docs.python.org/3/library/functions.html#bool)=False_, _from_smiles:[Optional](https://docs.python.org/3/library/typing.html#typing.Optional)[ [Callable](https://docs.python.org/3/library/typing.html#typing.Callable)]=None_) [[source]](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/molecule_net.html#MoleculeNet) [](pytorch-geometric.readthedocs.io/en/latest/...)\nBases: [`InMemoryDataset`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.InMemoryDataset.html#torch_geometric.data.InMemoryDataset)\nThe [MoleculeNet](http://moleculenet.org/datasets-1) benchmark\ncollection from the [\u201cMoleculeNet: A Benchmark for Molecular Machine\nLearning\u201d](https://arxiv.org/abs/1703.00564) paper, containing datasets\nfrom physical chemistry, biophysics and physiology.\nAll datasets come with the additional node and edge features introduced by\nthe\n[Open Graph Benchmark](https://ogb.stanford.edu/docs/graphprop/).\nParameters:\n- **root** ( [_str_](https://docs.python.org/3/library/stdtypes.html#str)) \u2013 Root directory where the dataset should be saved.\n- **name** ( [_str_](https://docs.python.org/3/library/stdtypes.html#str)) \u2013 The name of the dataset ( `\"ESOL\"`, `\"FreeSolv\"`,\n`\"Lipo\"`, `\"PCBA\"`, `\"MUV\"`, `\"HIV\"`,\n`\"BACE\"`, `\"BBBP\"`, `\"Tox21\"`, `\"ToxCast\"`,\n`\"SIDER\"`, `\"ClinTox\"`).\n- **transform** ( _callable_ _,_ _optional_) \u2013 A function/transform that takes in an\n[`torch_geometric.data.Data`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) object and returns a transformed\nversion. The data object will be transformed before every access.\n(default: [`None`](https://docs.python.org/3/library/constants.html#None))\n- **pre_transform** ( _callable_ _,_ _optional_) \u2013 A function/transform that takes in\nan [`torch_geometric.data.Data`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) object and returns a\ntransformed version. The data object will be transformed before\nbeing saved to disk. (default: [`None`](https://docs.python.org/3/library/constants.html#None))\n- **pre_filter** ( _callable_ _,_ _optional_) \u2013 A function that takes in an\n[`torch_geometric.data.Data`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) object and returns a boolean\nvalue, indicating whether the data object should be included in the\nfinal dataset. (default: [`None`](https://docs.python.org/3/library/constants.html#None))\n- **force_reload** ( [_bool_](https://docs.python.org/3/library/functions.html#bool) _,_ _optional_) \u2013 Whether to re-process the dataset.\n(default: [`False`](https://docs.python.org/3/library/constants.html#False))\n- **from_smiles** ( _callable_ _,_ _optional_) \u2013 A custom function that takes a SMILES\nstring and outputs a [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) object.\nIf not set, defaults to [`from_smiles()`](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.from_smiles).\n(default: [`None`](https://docs.python.org/3/library/constants.html#None))\n**STATS:**\n| Name | #graphs | #nodes | #edges | #features | #classes |\n| ESOL | 1,128 | ~13.3 | ~27.4 | 9 | 1 |\n| FreeSolv | 642 | ~8.7 | ~16.8 | 9 | 1 |\n| Lipophilicity | 4,200 | ~27.0 | ~59.0 | 9 | 1 |\n| PCBA | 437,929 | ~26.0 | ~56.2 | 9 | 128 |\n| MUV | 93,087 | ~24.2 | ~52.6 | 9 | 17 |\n| HIV | 41,127 | ~25.5 | ~54.9 | 9 | 1 |\n| BACE | 1513 | ~34.1 | ~73.7 | 9 | 1 |\n| BBBP | 2,050 | ~23.9 | ~51.6 | 9 | 1 |\n| Tox21 | 7,831 | ~18.6 | ~38.6 | 9 | 12 |\n| ToxCast | 8,597 | ~18.7 | ~38.4 | 9 | 617 |\n| SIDER | 1,427 | ~33.6 | ~70.7 | 9 | 27 |\n| ClinTox | 1,484 | ~26.1 | ~55.5 | 9 | 2 |",
      "url": "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.MoleculeNet.html"
    },
    {
      "title": "T035 \u00b7 GNN-based molecular property prediction",
      "text": "[Skip to content](https://projects.volkamerlab.org/projects.volkamerlab.org#talktorials/T035_graph_neural_networks)\n\n# T035 \u00b7 GNN-based molecular property prediction [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#talktorials-t035-graph-neural-networks--page-root)\n\n**Note**: This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n\nAuthors:\n\n- Paula Linh Kramer, 2022, [Volkamer Lab](https://volkamerlab.org/), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University\n\n\n## Aim of this talktorial [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Aim-of-this-talktorial)\n\nIn this tutorial, we will first explain the basic concepts of graph neural networks (GNNs) and present two different GNN architectures. We apply our neural networks to the `QM9` dataset, which is a dataset containing small molecules. With this dataset, we want to predict molecular properties. We demonstrate how to train and evaluate GNNs step by step using PyTorch Geometric.\n\n### Contents in _Theory_ [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Contents-in-Theory)\n\n- GNN Tasks\n\n- Message Passing\n\n- Graph Convolutional Network (GCN)\n\n- Graph Isomorphism Network (GIN)\n\n- Training a GNN\n\n- Applications of GNNs\n\n\n### Contents in _Practical_ [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Contents-in-Practical)\n\n- Dataset\n\n- Defining a GCN and GIN\n\n- Training a GNN\n\n- Evaluating the model\n\n\n### References [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#References)\n\n- Articles:\n\n  - Atz, Kenneth, Francesca Grisoni, and Gisbert Schneider. _Geometric Deep Learning on Molecular Representations_, [Nature Machine Intelligence 3.12 (2021): 1023-1032](https://arxiv.org/pdf/2107.12375.pdf)\n\n  - Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. _How Powerful are Graph Neural Networks?_, [International Conference on Learning Representations (ICLR 2019)](https://arxiv.org/abs/1810.00826v3)\n\n  - Welling, Max, and Thomas N. Kipf. _Semi-supervised classification with graph convolutional networks_, [International Conference on Learning Representations (ICLR 2017)](https://arxiv.org/pdf/1609.02907.pdf)\n\n  - Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. _Neural Message Passing for Quantum Chemistry_, [International conference on machine learning. PMLR, 2017](https://arxiv.org/pdf/1704.01212.pdf)\n- Blog posts:\n\n  - Maxime Labonne, _Graph Convolutional Networks: Introduction to GNNs_, [Maxime Labonne](https://mlabonne.github.io/blog/intrognn/)\n\n  - Maxime Labonne, _GIN: How to Design the Most Powerful Graph Neural Network_, [Maxime Labonne](https://mlabonne.github.io/blog/gin/)\n\n  - Vortana Say, _How To Save and Load Model In PyTorch With A Complete Example_, [towardsdatascience](https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee)\n\n  - Michael Bronstein, _Expressive power of graph neural networks and the Weisfeiler-Lehman test_, [towardsdatascience](https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)\n\n  - Benjamin Sanchez-Lengeling, Emily Reif, _A Gentle Introduction to Graph Neural Networks_, [Distill](https://distill.pub/2021/gnn-intro/)\n- Tutorials:\n\n  - _Pytorch Geometric Documentation_, [Colab Notebooks and Video Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)\n\n  - _Pytorch Geometric Documentation_, [Introduction by Example](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#learning-methods-on-graphs)\n\n## Theory [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Theory)\n\n### Graph Neural Networks [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Graph-Neural-Networks)\n\nThere are several ways to represent molecules which are explained and discussed in **Talktorial T033**. If we work with molecules, one intuitive approach to apply deep learning to certain tasks is to make use of the graph structure of molecules. Graph neural networks can directly work on given graphs. Molecules can easily be represented as a graph, as seen in Figure 1. Given a graph \\\\(G=(V, E)\\\\), \\\\(V\\\\) describes the vertices or nodes. In molecular graphs, a node\n\\\\(v\\_i \\\\in \\\\mathbb{R}^{d\\_v}\\\\) represents an atom. Nodes can have \\\\(d\\_v\\\\) different features, such as atomic number and chirality. Edges usually correspond to covalent bonds between the atoms. Each edge \\\\(e\\_{ij} \\\\in \\\\mathbb{R}^{d\\_e}\\\\) is described by \\\\(d\\_e\\\\) number of features, which usually represent the bond type. A graph neural network is a network consisting of learnable and differentiable functions that are invariant for graph permutations. Graph neural networks consist of\nso-called message-passing layers which will be explained in more detail below, followed by more specific explanations of two different GNN architectures.\n\nFigure 1: Molecular graph overview. Figure taken from \\[1\\]\n\n#### GNN Tasks [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#GNN-Tasks)\n\nWe can perform different tasks with a GNN:\n\n- Graph-level tasks: one application would be to predict a specific property of the entire graph. This can be a classification task such as toxicity prediction or a regression task. In this tutorial, we will implement a regression task to predict molecular properties. Another graph-level task would be to predict entirely new graphs/molecules. This is especially relevant in the area of drug discovery, where new drug candidates are of interest.\n\n- Node-level tasks: we can predict a property of a specific node in the graph, e.g. the atomic charges of each atom. We could also predict a new node to be added to the graph. This is often done for molecule generation, where we want to add multiple atoms to form new molecules one after the other.\n\n- Edge-level tasks: we can predict edge properties, e.g. intramolecular forces between atoms, or a new edge in the graph. In the molecule generation context, we want to predict potential bonds between the atoms. Edge prediction can also be used to infer connections/interactions e.g. in a gene regulatory network.\n\n\n#### Message Passing [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Message-Passing)\n\nInstead of MLP layers in standard neural networks, GNNs have message-passing layers, where we collect information about the neighboring nodes. For each node \\\\(v\\\\), we look at the direct neighbors \\\\(N(v)\\\\) and gather information. Then all the information is aggregated, for example with summation. Then we update the node \\\\(v\\\\) with the aggregated messages. If we perform this aggregation and combining, each node contains the information about the direct neighbors (1-hop). If we repeat\nthis \\\\(n\\\\) times, we aggregate information about the \\\\(n\\_{th}\\\\) closest neighbors (\\\\(n\\\\) -hop).\n\n\\\\\\[a\\_v^{(k)} = \\\\text{aggregate}^{(k)} (\\\\{ h\\_u^{(k-1)}: u \\\\in N(v) \\\\})\\\\\\]\n\n\\\\\\[h\\_v^{(k)} = \\\\text{combine}^{(k)} (h\\_v^{(k-1)}, a\\_v^{(k)})\\\\\\]\n\nwhere \\\\(h\\_v^{(k)}\\\\) is the embedding of node \\\\(v\\\\) at layer \\\\(k\\\\), \\\\(N(v)\\\\) are the neighbors of node \\\\(v\\\\).\n\nFigure 2: Message passing overview. Figure taken from \\[2\\]\n\nOne important property of a GNN is permutation invariance. This means that changing the order of nodes in the graph should not affect the outcome. For example, when working with adjacency matrices, changing the order of nodes would mean swapping rows and/or columns. However, this does not change any properties of a graph, but the input would differ. In GNNs, we want to overcome this. We, therefore need an aggregation function and a combining function that are permutation invariant, such as using\nthe mean, the maximum or a sum. Using a permutation invariant aggregation function ensures that the graph-level outputs are also inva...",
      "url": "https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html"
    }
  ]
}