{
  "query": "How do you implement transfer learning for chemical reaction yield prediction when you have limited training data for new solvents?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Implementing transfer learning for reaction\u2011yield prediction with only a few examples in new solvents can be done in five core steps. The workflow below follows the strategies reported for low\u2011data chemistry domains\u202f([King\u2011Smith\u202f2024](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04928k);\u202f[Du\u202fet\u202fal.\u202f2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E);\u202f[Wen\u202fet\u202fal.\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06515g);\u202f[Shim\u202fet\u202fal.\u202f2022](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b);\u202f[Boyne\u202fet\u202fal.\u202f2024](https://arxiv.org/html/2506.07619v1)).\n\n1. **Assemble a large, diverse pre\u2011training corpus**  \n   - Collect publicly available reaction datasets that span many solvents, catalysts, and substrates (e.g., USPTO, high\u2011throughput experimentation collections).  \n   - Encode each reaction as a machine\u2011readable format (SMILES, reaction graphs, or tokenised strings).  \n   - This broad corpus supplies the \u201cfoundational chemistry model\u201d that can later be transferred to new solvent domains\u202f([King\u2011Smith\u202f2024](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04928k)).\n\n2. **Pre\u2011train a generic model on the full corpus**  \n   - Choose a flexible architecture such as a graph neural network (GNN) or a transformer\u2011based reaction encoder.  \n   - Train the model to predict yields (or related properties) on the full dataset.  \n   - Optionally apply unsupervised contrastive pre\u2011training to improve representations when data are noisy or heterogeneous\u202f([Wen\u202fet\u202fal.\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06515g)).\n\n3. **Enrich the limited new\u2011solvent data via augmentation**  \n   - Generate synthetic variants of the few available reactions (e.g., randomize reagent order, apply stereochemical flips, or use reaction\u2011template perturbations).  \n   - Combine these augmented examples with the real new\u2011solvent reactions to create a modestly larger fine\u2011tuning set\u202f([Du\u202fet\u202fal.\u202f2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)).\n\n4. **Fine\u2011tune the pretrained model on the new\u2011solvent set**  \n   - Freeze the early layers that capture general chemistry (bond types, functional\u2011group patterns) and only update the final layers responsible for yield mapping.  \n   - Use a low learning rate (e.g.,\u202f1e\u20114) and early\u2011stopping to avoid over\u2011fitting the tiny dataset.  \n   - If the new solvent is chemically similar to those seen during pre\u2011training, simple fine\u2011tuning often yields strong performance; otherwise, consider adding a small solvent\u2011embedding vector that is learned during this step\u202f([Shim\u202fet\u202fal.\u202f2022](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)).\n\n5. **Iterate with active (few\u2011shot) learning**  \n   - Deploy the fine\u2011tuned model to predict yields for a pool of untested solvent\u2011reaction combos.  \n   - Select the most uncertain or highest\u2011impact predictions, run those experiments, and add the results back to the fine\u2011tuning set.  \n   - This active transfer\u2011learning loop rapidly improves accuracy with only a handful of additional measurements\u202f([Boyne\u202fet\u202fal.\u202f2024](https://arxiv.org/html/2506.07619v1)).  \n\nFollowing these steps lets you leverage a large, pre\u2011trained chemistry model while extracting maximal predictive power from the scarce data available for new solvents.",
      "url": ""
    },
    {
      "title": "Transfer learning for a foundational chemistry model \u2020",
      "text": "Transfer learning for a foundational chemistry model - Chemical Science (RSC Publishing) DOI:10.1039/D3SC04928K\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2024/sc/d3sc04928k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d4sc01122h)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d4sc00090k)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D3SC04928K](https://doi.org/10.1039/D3SC04928K)(Edge Article)[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2024,**15**, 5143-5151\n# Transfer learning for a foundational chemistry model[\u2020](#fn1)\nEmma King-Smith[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2999-0955)\\*\nCavendish Laboratory, University of Cambridge, Cambridge, UK. E-mail:[esk34@cam.ac.uk](mailto:esk34@cam.ac.uk)\nReceived 19th September 2023, Accepted 15th November 2023\nFirst published on 24th November 2023\n## Abstract\nData-driven chemistry has garnered much interest concurrent with improvements in hardware and the development of new machine learning models. However, obtaining sufficiently large, accurate datasets of a desired chemical outcome for data-driven chemistry remains a challenge. The community has made significant efforts to democratize and curate available information for more facile machine learning applications, but the limiting factor is usually the laborious nature of generating large-scale data. Transfer learning has been noted in certain applications to alleviate some of the data burden, but this protocol is typically carried out on a case-by-case basis, with the transfer learning task expertly chosen to fit the finetuning. Herein, I develop a machine learning framework capable of accurate chemistry-relevant prediction amid general sources of low data. First, a chemical \u201cfoundational model\u201d is trained using a dataset of \u223c1 million experimental organic crystal structures. A task specific module is then stacked atop this foundational model and subjected to finetuning. This approach achieves state-of-the-art performance on a diverse set of tasks: toxicity prediction, yield prediction, and odor prediction.\n## Introduction\nThe implementation of computerized algorithms into organic chemistry has had a rich history, with early emphasis centered around deriving linear relationships from observed results.[1](#cit1)By the 1970s, synthetic chemists had turned their attention to utilizing more complex functions to model more abstract observations. In 1977, Coreyet al.published the first recognized retrosynthetic analyzer, LHASA (Logics and Heuristics Applied to Synthetic Analysis) which featured hand coded expert rules resulting in over 30![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)000 lines of FORTRAN code.[2](#cit2)With the turn of the century, improvements in computational hardware and the development of new computer learning algorithms, including machine learning (ML), have seeded new avenues for algorithm-based predictive chemistry.[3,4](#cit3)ML is the process of taking complex inputs, abstracting their relevant features through non-linear equations, and correlating those features to a given output. Despite its simplistic framework, variations upon this theme have yielded advances in numerous areas including fundamental molecular property prediction (e.g., quantum chemical, ADMET), reaction property prediction (e.g., regioselectivity, yield), and generative modeling.[5\u20137](#cit5)With these more powerful algorithms come higher data requirements. Where the first data-driven chemistry models may have necessitated a few experimental results, ML often demands tens of thousands of data points. Purely computational datasets from density functional theory (DFT) or semi-empirical methods have been generated and utilized in ML for prediction of quantum chemical properties (e.g., HOMO\u2013LUMO gaps, dipole moments)[8](#cit8)and in molecular scaffold generation.[9](#cit9)Benefits of using these datasets include larger sizes and less noise present within each observation. Whilst experimental data is inherently noisier, more expensive, and often more laborious to generate than computational data, it presents a more holistic representation of a chemical system, even if we do not fully understand the intricacies present within that system. It is therefore important for the community to find ways to incorporate these smaller, experimental datasets as a key feature into ML tools.\nOne method that has seen potential towards the utilization of smaller datasets in deep ML is transfer learning.[10](#cit10)In this process, a model is first trained on a large dataset. The target prediction of the first task (pretraining task) does not need to be directly related to the desired final task (finetuning task), however, the initial knowledge gained from pretraining must have some relevancy to the finetuning. In a neural network, each non-linear function is referred to as a layer; each layer in the neural network is responsible for extracting relevant chemical features for a given task. The first layer is defined as the input layer, the final layer as the output layer, and for this manuscript, the penultimate layer is defined as the latent space ([Fig. 1](#imgfig1)). One may imagine the latent space as a complete digitization of a molecule, whereby each molecular feature has been assigned a series of numbers. The key to effective transfer learning hinges around this latent space, whereby each molecule's chemically relevant features are so well characterized that the substitution of one output layer for another output layer results in accurate prediction of a different chemical property (see ESI: A non-expert's guide to transfer learning (CliffsNotes version) on p. S3 for further explanation of transfer learning[\u2020](#fn1)). In essence, the model transfers the knowledge it learnt from pretraining to finetuning. To date, transfer learning in data-driven chemistry has been applied on a case-by-case basis, where pretraining tasks are expertly chosen for specific finetunings to minimize domain mismatch.[10](#cit10)This limits the ML possibilities for smaller datasets.\n[![image file: d3sc04928k-f1.tif](https://pubs.rsc.org/image/article/2024/SC/d3sc04928k/d3sc04928k-f1.gif)](https://pubs.rsc.org/image/article/2024/SC/d3sc04928k/d3sc04928k-f1_hi-res.gif)|\n|**Fig. 1**Graphical overview of the framework. Top panel illustrates the pretraining process and the structure of deep learning neural networks. The bottom panel shows how the top large model (foundational model) can be used for new chemistry-relevant predictionsviathe foundational model's latent space.||\nHerein, I report the development of a general chemistry-centric foundational model utilizing transfer learning, capitalizing upon the molecular featurization from the resultant latent space. Rather than concocting a molecular representation through manual descriptor selection like in traditional QSAR, an underlying model, dubbed the \u201cfoundational model\u201d is utilized to generate the molecular representation, from which further training can be carried out to predict any endpoint properties of choice in a modular fashion, re-using (transferring) knowledge acquired in the first step. The goal of the foundational model is to ensure that enough relevant chemical information is present in the molecular representation (See ESI: A non-expert's guide to transfer learning (CliffsNotes version) on p. S3 for further explana...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04928k"
    },
    {
      "title": "Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes",
      "text": "[Jump to main content ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#maincontent) [Jump to site search ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#SearchText)\n\n[Issue 7, 2021](https://pubs.rsc.org/en/journals/journal/qo?issueid=qo008007&type=current)\n\n[![](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=CoverIssue&imageInfo.ImageIdentifier.SerCode=QO&imageInfo.ImageIdentifier.IssueId=QO008007)\\\n\\\nFrom the journal: **Organic Chemistry Frontiers**](https://pubs.rsc.org/en/journals/journal/qo)\n\n## Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes [\u2020](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E\\#fn1)\n\n![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg)\n\n[Yun\\\nZhang](https://pubs.rsc.org/en/results?searchtext=Author%3AYun%20Zhang), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0003-3353-4650)[\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Ling\\\nWang](https://pubs.rsc.org/en/results?searchtext=Author%3ALing%20Wang), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Xinqiao\\\nWang](https://pubs.rsc.org/en/results?searchtext=Author%3AXinqiao%20Wang), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#fn2) _a_[Chengyun\\\nZhang](https://pubs.rsc.org/en/results?searchtext=Author%3AChengyun%20Zhang),_a_[Jiamin\\\nGe](https://pubs.rsc.org/en/results?searchtext=Author%3AJiamin%20Ge),_a_[Jing\\\nTang](https://pubs.rsc.org/en/results?searchtext=Author%3AJing%20Tang),_a_[An\\\nSu](https://pubs.rsc.org/en/results?searchtext=Author%3AAn%20Su)\\*_b_\nand\n[Hongliang\\\nDuan](https://pubs.rsc.org/en/results?searchtext=Author%3AHongliang%20Duan)\\*_a_\n\n[Author affiliations](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n\\\\* Corresponding authors\n\naArtificial Intelligence Aided Drug Discovery Institute, College of Pharmaceutical Sciences, Zhejiang University of Technology, Hangzhou 310014, China\n\n**E-mail:** [hduan@zjut.edu.cn](mailto:hduan@zjut.edu.cn)\n\nbCollege of Chemical Engineering, Zhejiang University of Technology, Hangzhou 310014, China\n\n**E-mail:** [ansu@zjut.edu.cn](mailto:ansu@zjut.edu.cn)\n\n### Abstract\n\nEffective and rapid deep learning method to predict chemical reactions contributes to the research and development of organic chemistry and drug discovery. Despite the outstanding capability of deep learning in retrosynthesis and forward synthesis, predictions based on small chemical datasets generally result in a low accuracy due to an insufficiency of reaction examples. Here, we introduce a new state-of-the-art method, which integrates transfer learning with the transformer model to predict the outcomes of the Baeyer\u2013Villiger reaction which is a representative small dataset reaction. The results demonstrate that introducing a transfer learning strategy markedly improves the top-1 accuracy of the transformer-transfer learning model (81.8%) over that of the transformer-baseline model (58.4%). Moreover, we further introduce data augmentation to the input reaction SMILES, which allows for a better performance and improves the accuracy of the transformer-transfer learning model (86.7%). In summary, both transfer learning and data augmentation methods significantly improve the predictive performance of transformer models, which are powerful methods used in the field of chemistry to eliminate the restriction of limited training data.\n\n![Graphical abstract: Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&imageInfo.ImageIdentifier.ManuscriptID=D0QO01636E&imageInfo.ImageIdentifier.Year=2021)\n\nYou have access to this article\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\nPlease wait while we load your content...\nSomething went wrong. [Try again?](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n[About](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlAbstract)\n\n[Cited by](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlCitation)\n\n[Related](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E#pnlRelatedContent)\n\n[Download options Please wait...](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n## Supplementary files\n\n- [Supplementary information\\\nPDF (185K)](https://www.rsc.org/suppdata/d0/qo/d0qo01636e/d0qo01636e1.pdf)\n\n## Article information\n\nDOI[https://doi.org/10.1039/D0QO01636E](https://doi.org/10.1039/D0QO01636E)\n\n**Article type**Research Article\n\nSubmitted26 Dec 2020\n\nAccepted02 Feb 2021\n\nFirst published03 Feb 2021\n\n### Download Citation\n\n_**Org. Chem. Front.**_, 2021, **8**, 1415-1423\n\nBibTexEndNoteMEDLINEProCiteReferenceManagerRefWorksRIS\n\n### Permissions\n\n[Request permissions](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n### Data augmentation and transfer learning strategies for reaction prediction in low chemical data regimes\n\nY. Zhang, L. Wang, X. Wang, C. Zhang, J. Ge, J. Tang, A. Su and H. Duan,\n_Org. Chem. Front._, 2021,\u00a0**8**, 1415\n**DOI:** 10.1039/D0QO01636E\n\nTo request permission to reproduce material from this article, please go to the\n[Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD0QO01636E).\n\nIf you are **an author contributing to an RSC publication, you do not need to request permission**\nprovided correct acknowledgement is given.\n\nIf you are **the author of this article, you do not need to request permission to reproduce figures**\n**and diagrams** provided correct acknowledgement is given. If you want to reproduce the whole article\nin a third-party publication (excluding your thesis/dissertation for which permission is not required)\nplease go to the [Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD0QO01636E).\n\nRead more about [how to correctly acknowledge RSC content](https://www.rsc.org/journals-books-databases/journal-authors-reviewers/licences-copyright-permissions/#acknowledgements).\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E)\n\n### Social activity\n\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/twitter.svg)Tweet](https://twitter.com/intent/tweet/?text=Data+augmentation+and+transfer+learning+strategies+for+reaction+prediction+in+low+chemical+data+regimes+-+now+published+in+Organic+Chemistry+Frontiers&url=https%3a%2f%2fpubs.rsc.org%2fen%2fcontent%2farticlelanding%2f2021%2fqo%2fd0qo01636e)\n\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/wechat.svg)Share](https://pubs.rsc.org/en/Image/GetQrCode?url=https%3A%2F%2Fpubs.rsc.org%2Fen%2Fcontent%2Farticlelanding%2F2021%2Fqo%2Fd0qo01636e)\n\n## Search articles by author\n\nYun Zhang\n\nLing Wang\n\nXinqiao Wang\n\nChengyun Zhang\n\nJiamin Ge\n\nJing Tang\n\nAn Su\n\nHongliang Duan\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\nFetching data from CrossRef.\n\nThis may take some time to load.\n\nLoading related content![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\n\n### Spotlight\n\n### Advertisements",
      "url": "https://pubs.rsc.org/en/content/articlelanding/2021/QO/D0QO01636E"
    },
    {
      "title": "Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining \u2020",
      "text": "Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining - Chemical Science (RSC Publishing) DOI:10.1039/D1SC06515G\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06515g)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05660c)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc05792h)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D1SC06515G](https://doi.org/10.1039/D1SC06515G)(Edge Article)[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2022,**13**, 1446-1458\n# Improving machine learning performance on small chemical reaction data with unsupervised contrastive pretraining[\u2020](#fn1)\nMingjian Wen[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0013-575X)a,Samuel M. Blaua,Xiaowei Xie[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5618-8768)bc,Shyam DwaraknathdandKristin A. Persson[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2495-5509)\\*ef\naEnergy Technologies Area, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA\nbCollege of Chemistry, University of California, Berkeley, CA 94720, USA\ncMaterials Science Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA\ndLuxembourg Institute of Science and Technology, Luxembourg\neDepartment of Materials Science and Engineering, University of California, Berkeley, CA 94720, USA\nfMolecular Foundry, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA. E-mail:[kapersson@lbl.gov](mailto:kapersson@lbl.gov)\nReceived 22nd November 2021, Accepted 9th January 2022\nFirst published on 11th January 2022\n## Abstract\nMachine learning (ML) methods have great potential to transform chemical discovery by accelerating the exploration of chemical space and drawing scientific insights from data. However, modern chemical reaction ML models, such as those based on graph neural networks (GNNs), must be trained on a large amount of labelled data in order to avoid overfitting the data and thus possessing low accuracy and transferability. In this work, we propose a strategy to leverage unlabelled data to learn accurate ML models for small labelled chemical reaction data. We focus on an old and prominent problem\u2014classifying reactions into distinct families\u2014and build a GNN model for this task. We first pretrain the model on unlabelled reaction data using unsupervised contrastive learning and then fine-tune it on a small number of labelled reactions. The contrastive pretraining learns by making the representations of two augmented versions of a reaction similar to each other but distinct from other reactions. We propose chemically consistent reaction augmentation methods that protect the reaction center and find they are the key for the model to extract relevant information from unlabelled data to aid the reaction classification task. The transfer learned model outperforms a supervised model trained from scratch by a large margin. Further, it consistently performs better than models based on traditional rule-driven reaction fingerprints, which have long been the default choice for small datasets, as well as those based on reaction fingerprints derived from masked language modelling. In addition to reaction classification, the effectiveness of the strategy is tested on regression datasets; the learned GNN-based reaction fingerprints can also be used to navigate the chemical reaction space, which we demonstrate by querying for similar reactions. The strategy can be readily applied to other predictive reaction problems to uncover the power of unlabelled data for learning better models with a limited supply of labels.\n## 1. Introduction\nMachine learning methods, especially deep learning, have significantly expanded a chemist's toolbox, enabling the construction of quantitatively predictive models directly from data without explicitly designing rule-based models using chemical insights and intuitions. They have recently been successfully applied to address challenging chemical reaction problems, ranging from the prediction of reaction and activation energies,[1\u20135](#cit1)reaction products,[6,7](#cit6)and reaction conditions,[8,9](#cit8)as well as designing synthesis routes[10,11](#cit10)to name a few. A key ingredient underlying these successes is that modern machine learning methods excel in extracting the patterns in data from sufficient, labelled training examples.[12](#cit12)It has been shown that the performance of these chemical machine learning models can be systematically improved with the increase of training examples.[1,13](#cit1)Despite various recent efforts to generate large labelled reaction datasets that are suitable for modern machine learning,[3,14\u201317](#cit3)they are typically sparse and still small considering the size of the chemical reaction space.[18](#cit18)Many chemical reaction datasets, especially experimental ones, are rather limited, consisting of only thousands or even hundreds of labelled examples.[19,20](#cit19)For such small datasets, the machine learning models can easily become overfitted, resulting in low accuracy and transferability. Therefore, it would be of interest to seek new approaches to train the models using only a small number of reliable, labelled reactions while still retaining the accuracy.\nWhen the number of labelled reactions is small compared with the complexity of the machine learning model required to perform the task, it helps to seek some other source of information to initialize the feature detectors in the model and then to fine-tune these feature detectors using the limited supply of labels.[21](#cit21)In transfer learning, the source of information is another related supervised learning task that has an abundant number of labelled data. The model transfers beneficial information from the related task to aid its decision-making on the task with limited labels, resulting in improved performance. For example, transfer learning has enabled the molecular transformer to predict reaction outcomes with a small labelled dataset.[22,23](#cit22)Transfer learning, however, still requires a large labelled dataset to train the related task, which often is not readily available. Actually, it is possible to initialize the feature detectors using reactions without any labels at all. Although without explicit labels, unlabelled reactions contain extra information that can be leveraged to learn a better model and they are much easier to obtain. For example, the publicly available USPTO dataset[14](#cit14)contains \u223c3 million reactions, the commercial Reaxys database[24](#cit24)and the CAS database[25](#cit25)have \u223c56 millions and \u223c156 millions records of reactions, respectively. In this work, we present a generic unsupervised learning strategy to distill information from unlabelled chemical reactions. For the purpose of demonstration, we focus on the problem of classifying reactions into distinct families.\nReaction family classification has great value for chemists. It facilitates the communication of complex concepts like how a reaction happens in terms of atomic rearrangement and helps to efficiently navigate the chemical reaction space by systematic indexing of reactions in books and databases.[26\u201328](#cit26)Many iconic rules for reactivity prediction require reactions to be in the same fami...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/sc/d1sc06515g"
    },
    {
      "title": "",
      "text": "Predicting reaction conditions from limited data\nthrough active transfer learning\u2020\nEunjae Shim, a Joshua A. Kammeraad, ab Ziping Xu, b Ambuj Tewari, bc\nTim Cernak *ad and Paul M. Zimmerman *a\nTransfer and active learning have the potential to accelerate the development of new chemical reactions,\nusing prior data and new experiments to inform models that adapt to the target area of interest. This article\nshows how specifically tuned machine learning models, based on random forest classifiers, can expand the\napplicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First,\nmodel transfer is shown to be effective when reaction mechanisms and substrates are closely related, even\nwhen models are trained on relatively small numbers of data points. Then, a model simplification scheme is\ntested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen\nreagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit\nover random selection, an active transfer learning strategy is introduced to improve model predictions.\nSimple models, composed of a small number of decision trees with limited depths, are crucial for\nsecuring generalizability, interpretability, and performance of active transfer learning.\nIntroduction\nComputers are becoming increasingly capable of performing\nhigh-level chemical tasks.1\u20134 Machine learning approaches have\ndemonstrated viable retrosynthetic analyses,5\u20137 product predic\u0002tion,8\u201311 reaction condition suggestion,12\u201316 prediction of ster\u0002eoselectivity,17\u201320 regioselectivity,19,21\u201324 and reaction yield25,26\nand optimization of reaction conditions.27\u201330 These advances\nallow computers to assist synthesis planning for functional\nmolecules using well-established chemistry. For machine\nlearning to aid the development of new reactions, a model\nbased on established chemical knowledge must be able to\ngeneralize its predictions to reactivity that lies outside of the\ndataset. However, because most supervised learning algorithms\nlearn how features (e.g. reaction conditions) within a particular\ndomain relate to an outcome (e.g. yield), the model is not ex\u0002pected to be accurate outside its domain. This situation\nrequires chemists to consider other machine learning methods\nfor navigating new reactivity.\nExpert knowledge based on known reactions plays a central\nrole in the design of new reactions. The assumption that\nsubstrates with chemically similar reaction centers have trans\u0002ferable performance provides a plausible starting point for\nexperimental exploration. This concept of chemical similarity,\ntogether with literature data, guides expert chemists in the\ndevelopment of new reactions. Transfer learning, which\nassumes that data from a nearby domain, called the source\ndomain, can be leveraged to model the problem of interest in\na new domain, called the target domain,31 emulates a tactic\ncommonly employed by human chemists.\nTransfer learning is a promising strategy when limited data\nis available in the domain of interest, but a sizeable dataset is\navailable in a related domain.31,32 Models are \ue103rst created using\nthe source data, then transferred to the target domain using\nvarious algorithms.19,33\u201335 For new chemical targets where no\nlabeled data is available, the head start in predictivity a source\nmodel can provide becomes important. However, when a shi\ue09d\nin distribution of descriptor values occurs (e.g., descriptors\noutside of the original model ranges) in the target data, making\npredictions becomes challenging. For such a situation, the\nobjective of transfer learning becomes training a model that is\nas predictive in the target domain as possible.31,36 Toward this\nend, cross-validation is known to improve generalizability by\nproviding a procedure to avoid over\ue103tting on the training data.37\nThe reduction of generalization error, however, may not be\nsufficient outside the source domain. Accordingly, new\nmethods that enhance the applicability of a transferred model\nto new targets would be bene\ue103cial for reaction condition\nprediction.\nAnother machine learning method that can help tackle data\nscarcity is active learning. By making iterative queries of\na\nDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail:\npaulzim@umich.edu\nb\nDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\nc\nDepartment of Electrical Engineering and Computer Science, University of Michigan,\nAnn Arbor, MI, USA\nd\nDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA.\nE-mail: tcernak@med.umich.edu\n\u2020 Electronic supplementary information (ESI) available: Additional results. See\nhttps://doi.org/10.1039/d1sc06932b.\nCite this: Chem. Sci., 2022, 13, 6655\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 10th December 2021\nAccepted 10th May 2022\nDOI: 10.1039/d1sc06932b\nrsc.li/chemical-science\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2022, 13, 6655\u20136668 | 6655\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 11 May 2022. Downloaded on 1/15/2026 9:04:44 AM. This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence. View Article Online View Journal | View Issue\nlabeling a small number of datapoints, active learning updates\nmodels with knowledge from newly labeled data. As a result,\nexploration is guided into the most informative areas and\navoids collection of unnecessary data.38,39 Active learning is\ntherefore well-suited for reaction development, which greatly\nbene\ue103ts from efficient exploration and where chemists conduct\nthe next batch of reactions based on previous experimental\nresults. Based on this analogy, reaction optimization27,28 and\nreaction condition identi\ue103cation40 have been demonstrated to\nbene\ue103t from active learning. However, these prior works initiate\nexploration with randomly selected data points (Fig. 1A) which\ndoes not leverage prior knowledge, and therefore does not\nre\ue104ect how expert chemists initiate exploration. Initial search\ndirected by transfer learning could identify productive regions\nearly on, which in turn will help build more useful models for\nsubsequent active learning steps.\nTo align transfer and active learning closer to how expert\nchemists develop new reactions, appropriate chemical reaction\ndata is necessary.41 Available datasets42 that are o\ue09den used for\nmachine learning are overrepresented by positive reactions,\nfailing to re\ue104ect reactions with negative outcomes. On the other\nhand, reaction condition screening data of methodology\nreports\u2014which chemists o\ue09den refer to\u2014only constitute\na sparse subset of possible reagent combinations, making it\nhard for machine learning algorithms to extract meaningful\nknowledge.43\nHigh-throughput experimentation44\u201346 (HTE) data can \ue103ll\nthis gap. HTE provides reaction data16,25,27,47,48 with reduced\nvariations in outcome due to systematic experimentation. Pd\u0002catalyzed coupling data was therefore collected from reported\nwork using nanomole scale HTE in 1536 well plates.49\u201351 In the\ncurrent work, subsets of this data, classi\ue103ed by nucleophile type\nas shown in Fig. 2A, were selected to a dataset size of approxi\u0002mately 100 datapoints, which captured both positive and\nnegative reaction performance.\nReaction condition exploration could be made more efficient\nif algorithmic strategies could leverage prior knowledge. Toward\nthis goal, model transfer and its combination with active\nlearning were evaluated. Taking advantage of diverse campaigns,\nthis study will show that transferred models can be effective in\napplying prior reaction conditions to a new substrate type under\ncertain conditions. Next, the source model's ability to predict\nreaction conditions with new combinations of reagents will also\nbe evaluated. Lastly, challenging scenarios are considered where\nproductive reaction conditions for one class of substrate...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "Generalizing property prediction of ionic liquids from limited labeled data: a one-stop framework empowered by transfer learning",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2023/dd/d3dd00040k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00147k)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00121g)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D3DD00040K](https://doi.org/10.1039/D3DD00040K)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023, **2**, 591-601\n\n# Generalizing property prediction of ionic liquids from limited labeled data: a one-stop framework empowered by transfer learning [\u2020](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k\\#fn1)\n\nGuzhong\nChen\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-0515-8010)ab,\nZhen\nSong\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-9219-1833)\\*a,\nZhiwen\nQi\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2037-2234)\\*a and Kai\nSundmacher\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-3251-0593)bc\n\naState Key Laboratory of Chemical Engineering, School of Chemical Engineering, East China University of Science and Technology, 130 Meilong Road, Shanghai 200237, China. E-mail: [songz@ecust.edu.cn](mailto:songz@ecust.edu.cn); [zwqi@ecust.edu.cn](mailto:zwqi@ecust.edu.cn)\n\nbProcess Systems Engineering, Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstr. 1, D-39106 Magdeburg, Germany\n\ncProcess Systems Engineering, Otto-von-Guericke University Magdeburg, Universit\u00e4tsplatz 2, D-39106 Magdeburg, Germany\n\nReceived\n14th March 2023\n, Accepted 12th May 2023\n\nFirst published on 12th May 2023\n\n* * *\n\n## Abstract\n\nIonic liquids (ILs) could find use in almost every chemical process due to their wide spectrum of unique properties. The crux of the matter lies in whether a task-specific IL selection from enormous chemical space can be achieved by property prediction, for which limited labeled data represents a major obstacle. Here, we propose a one-stop ILTransR (IL transfer learning of representations) that employs large-scale unlabeled data for generalizing IL property prediction from limited labeled data. By first pre-training on \u223c10 million IL-like molecules, IL representations are derived from the encoder state of a transformer model. Employing the pre-trained IL representations, convolutional neural network (CNN) models for IL property prediction are trained and tested on eleven datasets of different IL properties. The obtained ILTransR presents superior performance as opposed to state-of-the-art models in all benchmarks. The application of ILTransR is exemplified by extensive screening of CO2 absorbent from a huge database of 8![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)333![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)096 synthetically-feasible ILs.\n\n* * *\n\n## 1 Introduction\n\nIonic liquids (ILs) are molten salts comprised fully of cations and anions, which can remain in liquid state around room temperature. In recent years, ILs have attracted remarkable attention in various applications, both in chemistry and engineering, [1,2](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit1) due to their unique physicochemical properties such as negligible vapor pressure, high thermal and electrochemical stability, wide liquidus range, etc. [2,3](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit2) More importantly, ILs also offer great potential to tune their physical and chemical properties by judicious selection of the cations and anions. For this reason, ILs could be designed to offer desirable properties to meet specific requirements for arbitrary given applications. The challenge, however, is to accurately evaluate various IL properties related to the target performance and identify optimal ILs from the nearly infinite combinations of possible cations and anions. [4,5](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit4)\n\nSo far, IL selection toward a specific process mainly relies on laborious trial-and-error experiments. However, such approaches are not only very time-consuming but also limited to a small IL chemical space, leaving many potentially promising structures unexplored. Alternatively, computational methods can be used for estimating the properties of ILs and IL-involved mixtures. [6](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit6) Traditional models such as equations of states (EoSs) [7](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit7) and group contribution models (GCMs) [8,9](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit8) have been widely employed for estimating the thermodynamic, transport, and EHS (environment, health, and safety) related properties of ILs. Nevertheless, both the two schemes are prone to the inherent weakness of limited predictive power and/or insufficient accuracy. [8](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit8) Another computational method for IL property prediction is the quantitative structure\u2013property relationship (QSPR) approach, wherein a property of interest is correlated quantitatively with certain descriptors of involved molecules [9,10](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit9) (for which machine learning methods have recently gained popularity [11\u201317](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit11)). Notably, the availability of IL property databases like ILThermo [18](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit18) has stimulated the use of ML methods for modeling IL properties, wherein diverse types of molecular descriptors were used as IL representation. [19\u201326](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit19) However, despite the high accuracy achieved by these models, such models still suffer from the inherent weakness of molecular descriptors for IL representation as well as the relatively limited databases of IL properties available for model development. Moreover, manually engineered IL descriptors usually require expert knowledge of specific types of ILs and the properties to be modeled, which could work well for specific tasks but may not generalize well for others. [27](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit27) In the past few years, there has been rapid progress in ML methods, particularly deep neural networks (DNNs). These DNN-based methods have garnered significant attention due to their ability to overcome the limitations of conventional models and achieve high accuracy in predicting complex tasks. [28\u201331](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit28) The growth of deep learning (DL) has offered excellent flexibility and performance to learn molecular representations from data, without explicit guides from experts. [32\u201334](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit32) Typically, a sufficiently large labeled training dataset is desirable for developing DL approaches. [35](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k#cit35) This is practical in areas like image classification as the number of labeled samples could easily reach se...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00040k"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2504.08874] Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2504.08874\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2504.08874**(cs)\n[Submitted on 11 Apr 2025]\n# Title:Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions\nAuthors:[Roshan Patel](https://arxiv.org/search/cs?searchtype=author&amp;query=Patel,+R),[Saeed Moayedpour](https://arxiv.org/search/cs?searchtype=author&amp;query=Moayedpour,+S),[Louis De Lescure](https://arxiv.org/search/cs?searchtype=author&amp;query=De+Lescure,+L),[Lorenzo Kogler-Anele](https://arxiv.org/search/cs?searchtype=author&amp;query=Kogler-Anele,+L),[Alan Cherney](https://arxiv.org/search/cs?searchtype=author&amp;query=Cherney,+A),[Sven Jager](https://arxiv.org/search/cs?searchtype=author&amp;query=Jager,+S),[Yasser Jangjou](https://arxiv.org/search/cs?searchtype=author&amp;query=Jangjou,+Y)\nView a PDF of the paper titled Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions, by Roshan Patel and 5 other authors\n[View PDF](https://arxiv.org/pdf/2504.08874)[HTML (experimental)](https://arxiv.org/html/2504.08874v1)> > Abstract:\n> Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2504.08874](https://arxiv.org/abs/2504.08874)[cs.LG]|\n|(or[arXiv:2504.08874v1](https://arxiv.org/abs/2504.08874v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2504.08874](https://doi.org/10.48550/arXiv.2504.08874)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yasser Jangjou [[view email](https://arxiv.org/show-email/a11d745f/2504.08874)]\n**[v1]**Fri, 11 Apr 2025 12:45:07 UTC (4,515 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions, by Roshan Patel and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2504.08874)\n* [HTML (experimental)](https://arxiv.org/html/2504.08874v1)\n* [TeX Source](https://arxiv.org/src/2504.08874)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-nd-4.0.png)view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2504.08874&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2504.08874&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-04](https://arxiv.org/list/cs.LG/2025-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2504.08874?context=cs)\n[cs.AI](https://arxiv.org/abs/2504.08874?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.08874)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.08874)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.08874)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2504.08874&amp;description=Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2504.08874&amp;title=Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommender...",
      "url": "https://arxiv.org/abs/2504.08874"
    }
  ]
}