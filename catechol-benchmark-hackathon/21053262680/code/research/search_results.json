{
  "query": "How do you improve leaderboard score when cross-validation score is already optimized but there is a large gap between CV and LB in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving a Kaggle leaderboard when your cross\u2011validation (CV) score is already near its best usually means you\u2019re over\u2011fitting to the CV folds or missing information that the hidden test set contains.\u202fThe most reliable way to close the CV\u2011LB gap is to treat the public leaderboard as a *validation* set and focus on robustness, extra data, and smarter ensembling\u202f([Slideshare \u2013 \u201ctrust CV over public scores\u201d](https://www.slideshare.net/DariusBaruasukas/tips-and-tricks-to-win-kaggle-data-science-competitions)\u202f|\u202f[Medium \u2013 \u201c5 Simple Tips To Improve Your Kaggle Models\u201d](https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418)).\n\n**Step\u2011by\u2011step actions**\n\n1. **Audit and tighten your CV scheme**  \n   - Use stratified or time\u2011aware splits that mimic the competition\u2019s test distribution.  \n   - Keep the same random seed across experiments so you compare apples\u2011to\u2011apples.  \n   - If possible, set aside a *hold\u2011out* slice (e.g., 10\u202f% of the training data) that you never touch during model tuning; treat it as a proxy for the private leaderboard.  \n   *Citation*: Slideshare stresses \u201cconsistent CV splits\u201d and trusting CV over public scores\u202f([Slideshare](https://www.slideshare.net/DariusBaruasukas/tips-and-tricks-to-win-kaggle-data-science-competitions)).\n\n2. **Add more signal**  \n   - Pull external datasets or generate synthetic data (e.g., image augmentations, text paraphrases).  \n   - Use domain\u2011specific preprocessing that the competition\u2019s organizers expect (e.g., medical image normalization, audio spectrograms).  \n   *Citation*: Hugo Dolan\u2019s guide lists \u201cGet more data\u201d and \u201cPreprocessing images\u201d as universal levers\u202f([LinkedIn \u2013 Hugo Dolan](https://www.linkedin.com/pulse/how-improve-your-kaggle-competition-leaderboard-ranking-hugo-dolan)).\n\n3. **Deepen feature engineering**  \n   - Create interaction features, target\u2011encode high\u2011cardinality categorical columns, and apply statistical aggregations on time\u2011series or grouped data.  \n   - Validate each new feature with the hold\u2011out slice to ensure it improves the proxy score, not just the CV average.  \n   *Citation*: The \u201c5 Simple Tips\u201d article notes that reviewing past winners often reveals recurring feature tricks that boost performance\u202f([Towards Data Science](https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418)).\n\n4. **Regularize aggressively**  \n   - Lower learning rates, increase `subsample`/`colsample_bytree` for tree\u2011based models, add dropout or weight decay for neural nets.  \n   - Reduce model depth or number of estimators to avoid memorizing CV folds.  \n   *Citation*: KDnuggets outlines key XGBoost parameters (`eta`, `subsample`, `colsample_bytree`, `max_depth`) that control over\u2011fitting\u202f([KDnuggets](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)).\n\n5. **Build strong ensembles**  \n   - Blend diverse model families (e.g., LightGBM, CNN, BERT) using simple averaging, weighted averaging, or stacking with a meta\u2011learner trained on out\u2011of\u2011fold predictions.  \n   - Ensembling smooths out idiosyncratic errors that cause CV\u2011LB divergence.  \n   *Citation*: Slideshare highlights \u201ceffective ensembling techniques\u201d as a core tip for competitive performance\u202f([Slideshare](https://www.slideshare.net/DariusBaruasukas/tips-and-tricks-to-win-kaggle-data-science-competitions)).\n\n6. **Leverage public kernels and past competition solutions**  \n   - Study top\u2011ranked notebooks from similar challenges; reuse proven preprocessing pipelines, model architectures, and feature sets.  \n   - Adapt those ideas to your data rather than reinventing from scratch.  \n   *Citation*: The \u201c5 Simple Tips\u201d article recommends reviewing past competitions to harvest winning strategies\u202f([Towards Data Science](https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418)).\n\n7. **Monitor the public leaderboard as a sanity check, not a target**  \n   - Submit only after a genuine improvement on your hold\u2011out slice; avoid chasing incremental public\u2011LB gains that may overfit to the small public test set (the \u201cladder\u201d problem).  \n   *Citation*: The Ladder paper warns that repeated leaderboard queries can cause over\u2011fitting, so a robust, parameter\u2011free evaluation strategy is preferred\u202f([MLR Proceedings \u2013 The Ladder](https://proceedings.mlr.press/v37/blum15.pdf)).\n\nFollowing these steps\u2014tightening CV, enriching data, engineering robust features, regularizing, ensembling, borrowing proven ideas, and treating the public leaderboard as a secondary validator\u2014will usually shrink the CV\u2011to\u2011leaderboard gap and lift your final ranking.",
      "url": ""
    },
    {
      "title": "5 Simple Tips To Improve Your Kaggle Models",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# 5 Simple Tips To Improve Your Kaggle Models\n\nHow To Get High Performing Models In Competitions\n\n[Louise Ferbach](https://towardsdatascience.com/author/lsferbach/)\n\nOct 2, 2020\n\n8 min read\n\nShare\n\nPhoto by [\u00f0\u009f\u0087\u00a8\u00f0\u009f\u0087\u00ad Claudio Schwarz \\| @purzlbaum](https://u%5Bnsplash%5D(https://unsplash.com?utm_source=medium&utm_medium=referral).com/@purzlbaum?utm_source=medium&utm_medium=referral) on Unsplash\n\nIf you recently got started on Kaggle, or if you are an old regular of the platform, you probably wonder how to easily improve the performance of your model. Here are some practical tips I\u2019ve accumulated through [my Kaggle journey](https://www.kaggle.com/louise2001). So, either build your own model or just start from a baseline public kernel, and try implementing these suggestions !\n\n## 1\\. Always review past competitions\n\nAlthough Kaggle\u2019s policy is to never feature twice an identical competition, there are often remakes of very similar problems. For example, some hosts propose a regular challenge on the same theme yearly (NFL\u2019s Big Data Bowl for example), with only small variations, or in some fields (like medical imaging for example) there are a lot of competitions with different targets but very similar spirit.\n\nReviewing winners\u2019 solutions (always made public after competition ends thanks to the incredible Kaggle community) can therefore be a great plus, as it gives you ideas to get started, and a winning strategy. If you have time to review a lot of them, you will also soon find out that, even in very different competitions, some popular baseline models seem to always do the job well enough :\n\n- Convolutional Neural Networks or the more complex ResNet or EfficientNet in **computer vision challenges**,\n- WaveNet in **audio processing challenges** (that can also very well be treated by image recognition models, if you just use a Mel Spectrogram),\n- BERT and its derivatives (RoBERTa, etc) in **natural language processing challenges**,\n- Light Gradient Boosting Method (or other Gradient Boosting or trees strategies) on **tabular data**\u2026\n\nYou can either look for similar competitions on the Kaggle platform directly, or take a look at [this great summary by Sundalai Rajkumar](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions).\n\nReviewing past competitions can also help you get hints on all the other steps explained in the following. For example, getting tips and tricks on preprocessing for similar problems, how people choose their hyperparameters, what additional tools they have implemented in their models to have them win the game, or if they focused on bagging only similar versions of their best models or rather ensembled a melting pot of all available public kernels.\n\n## 2\\. You never spend enough time on data preparation\n\nThis is far from being the most thrilling part of the job. However, the importance of this step cannot be overemphasized.\n\n- **Clean the data** : never assume the hosts worked on providing you with the cleanest possible data. Most of the time, it is wrong. Fill NaNs, remove outliers, split the data into categories of homogeneous observations\u2026\n- Do some easy **exploratory data analysis**, to get an overview of what you\u2019re working on (this will help you get insights and ideas). **This is the most important step at this stage**. Without proper knowledge of how your data is structured, what information you have, what general behavior features tend to have individually or collectively with respect to the target, you will walk blind and have no intuition of how to build your model. Draw plots, histograms, correlation matrices.\n- **Augment your data** : this is probably one of the best things to improve performance. However be careful not to make it so huge that your model won\u2019t be able to process it anymore. You can either find some additional datasets on the Internet (be very careful about rights, or you could suffer the same fate as [the winners of the $1M Deepfake Detection Challenge](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/157983)), or on the Kaggle platform (in similar past competitions !), or just work on the data you\u2019re being provided : flip and crop images, overlay audio recordings, back-translate or replace synonyms in texts\u2026\n\nPreprocessing is also the step where you have to carefully think about what **cross-validation method** you will rely on. Kaggle\u2019s motto could basically be : _Trust Your CV_. Working on your data will help you know how to split it : stratify on target values or on sample categories ? Is your data unbalanced ? If you have a clever CV strategy, and rely solely on it and not on leaderboard score (though it may be very tempting), then you\u2019re very likely to get good surprises on private final scores.\n\n## 3\\. Try hyperparameter searching\n\nHyperparameter searching helps you find the optimal parameters (learning rate, temperature of softmax, \u2026) your model should have in order to get the best possible performance, without having to run a thousand boring experiments by hand.\n\nThe most common hyperparameter searching strategies include :\n\n- **Grid Search** (please never do that) : the worst performing method to my sense since you can completely miss a pattern or a very local peak in performance for some values, it consists or testing hyperparameter values equally distributed on an interval of possible values you have defined ;\n- **Random Search** (and its Monte-Carlo derivatives) : you try random values of your parameters. The main issue with it lies in the fact that it is a parallel method and can quickly become very costly the more parameters you are testing. However, it has the advantage of enabling you to include prior knowledge in your testing : if you want to find the best learning rate between 1e-4 and 1e-1, but you suppose it must be around 1e-3, you can draw samples from a log-normal distribution centered on 1e-3.\n- **Bayesian Search** : basically the random search but improved in so far as it is iterative and therefore much less costly. It iteratively evaluates a promising hyperparameter configuration based on the current model, and then updates it. It is the best performing of the three.\n- Other methods including **gradient-based search** or **evolutionary optimization** are more hazardous and do not generally apply. They can be recommended in some special cases.\n\nThere are many **AutoML** tools that can do the job very well for you. Just take a look at the excellent Medium & TowardsDataScience ressources on this topic :\n\n- [https://towardsdatascience.com/how-to-beat-automl-hyperparameter-optimisation-with-flair-3b2f5092d9f5](https://towardsdatascience.com/how-to-beat-automl-hyperparameter-optimisation-with-flair-3b2f5092d9f5)\n- [https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)\n- [https://medium.com/@martsalz/automl-hyperparameter-tuning-with-nni-and-keras-ffbef61206cf](https://medium.com/@martsalz/automl-hyperparameter-tuning-with-nni-and-keras-ffbef61206cf)\n\nHowever, you have to be careful and keep a solid intuition of what hyperparameters values mean. If you don\u2019t have a solid validation set and homogeneous data, hyperparameter optimization pushed too far can lead into the overfitting-lion\u2019s den. Always prefer some rationally explainable parameter choice to a millidecimal accuracy win on training data.\n\n## 4\\. Simple practices can change the game\n\nI have found that there are some model wrappers you can use to get better results. They work on different levels :\n\n- In the optimization process, never forget to add a **Learning Rate Scheduler** that help get a more precise training (starting small, progressively increasing when your model is learning well, reducing the step on plateau for example).\n- Still in the optimization process...",
      "url": "https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418"
    },
    {
      "title": "How to improve your Kaggle competition leaderboard ranking",
      "text": "Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n### _Tips from a new \u2018Kaggler\u2019 building CNN\u2019s for blindness detection_\n\n_Originally Published in Towards Data Science_: [Link](https://towardsdatascience.com/how-to-improve-your-kaggle-competition-leaderboard-ranking-bcd16643eddf#612f-49adb057b0b1)\n\n_After recently competing in the 2019 APTOS Blindness Detection Kaggle Competition and_ [_finishing in top 32%_](https://www.kaggle.com/hugo1005) _, I thought I would share my process for training convolutional neural networks. My only prior deep learning experience was completing the Deeplearning.ai Specialisation, hence this is all you should need to read this article._\n\n### **Sections to this\u00a0article**\n\n1. Competition context\n2. Keeping a logbook\n3. Get more data\n4. Leveraging existing kernels\n5. Preprocessing images\n6. Training is a very very slow process (but don\u2019t worry)\n7. Transfer learning\n8. Model selection\n\n### Competition context\n\nI spent the last 2\u20133 months working on and off on the [APTOS 2019 Blindness Detection Competition](https://www.kaggle.com/c/aptos2019-blindness-detection) on Kaggle, which required you to grade images of people\u2019s eyes to 5 categories of diabetic retinopathy severity. In the remainder of this article I talk about some tips and tricks you can use in _any kaggle vision competition,_ as I feel that the things I learned from this competition are pretty much universally applicable _._\n\n### Keep a\u00a0logbook\n\nLike any good scientific experiment, we change one thing at a time and compare our results to our control. Hence when training a CNN (Convolutional Neural Network) we should do likewise and record the change and the results in a logbook. Heres the one I used from the blindness detection challenge.\n\nI don\u2019t claim that the exact table I use here is ideal (far from it), but I found it useful to be able to at least identify each time I made a change whether the model improved or not cumulatively on the previous changes. Regardless I highly recommend you keep some form of logbook as its very difficult to identify if anything your doing is working otherwise.\n\nSome ideas I have for my next competition logbook is to:\n\n1. Establish a single baseline model to compare all future changes to\n2. Come up with a bunch of tweaks you want to try and run modified versions of the baseline for each tweak independently rather than in a cumulative fashion.\n3. Maintain the same (and smallest) CNN Architecture for as long as possible as it will make iteration quicker and with some look many of the hyper-parameters should transfer decently to larger more complex models.\n\n### Get more\u00a0data\n\nDo some research before you start coding and see if a similar competition has been run before or if there are any databases of similar labelled training sets you can use. More data is never really harmful to your model (assuming the quality of labelling is decent), so get as much of it as you can, but just don\u2019t forget to keep your validation and test sets from the original dataset provided to you or you may end up with a train- test mismatch _._\n\n### Leveraging existing\u00a0kernels\n\nIf your new to deep learning competitions (like me) you probably don\u2019t want to write your entire notebook from scratch \u2014 especially when someone else has probably already posted a starter kernel for your competition (Why reinvent the wheel right?). This will probably save you a bunch of time on debugging and get you onto learning new stuff faster by just tweaking someone else\u2019s model.\n\n**This was a good starter kernel** that I used and retrofitted for almost all of my further trials:\n\n[**\\[APTOS\\] resnet50 baseline**](https://www.kaggle.com/mathormad/aptos-resnet50-baseline)\n\n**A word of warning:** If a kernel suggests a bunch of techniques to use for your model you should check if they state the resultant performance gains, otherwise be skeptical and conduct tests yourself before blindly incorporating them into your own models\u00a0:)\n\n### Preprocessing Images\n\n**Cropping & Other Augmentations:** This step is a must. Training images may be in a very raw state. For example in the blindness detection challenge the images were all cropped at different ratios which meant a dumb algorithm could overfit to the black space around the eye which was more prevalent in one class than another.\n\nHence cropping and resizing images in a robust way was a crucial part of this competition. There were also many image augmentation techniques such as random cropping, rotation, contrast and brightness etc, which I had varying degrees of success with.\n\n_Source:_ [_https://www.kaggle.com/taindow/be-careful-what-you-train-on_](https://www.kaggle.com/taindow/be-careful-what-you-train-on)\n\n**Imbalanced classes:** Invariably there are more training examples for some classes than others, so you need to fix this before you start training. A combination of techniques that work ok are _over / under-sampling_ as well as _mixup_(Zhang et al., 2019) during mini batch gradient descent.\n\n**Preprocessing Computation:** Often the dataset will be quite large and applying rudimentary procedures such as standardising size and cropping of images should be done in a separate kernel t (or offline dep. on the size of the dataset)and re-uploaded as a modified version of the original data \u2014 otherwise you will have to do this computation at every epoch / run of your model (which is a terrible waste of time).\n\n### Training is a very very slow\u00a0process\n\nNow that you\u2019ve written your first kernel you need to test it out! Kaggle kernels can run for up to 9 hours (the kernel time limit may vary by competition), the site is also running many models and can be slower at some times of the day than others as a result. My best advice is to first quickly run it in browser for 1 or 2 iterations to make sure you haven\u2019t made any errors then get several ideas you want to test out simultaneously and just hit commit on all of them and check back in a few hours. Note that if you hit commit rather than just running the kernel you don\u2019t have to keep your laptop running\u00a0:).\n\n### Transfer Learning\n\nYou won\u2019t be training any model from scratch which is sufficiently large. Typically we will just take a large model pre-trained on imagenet or some other large dataset and fine-tune it for our purposes. In almost all cases you should unfreeze all layers of the model during fine-tuning as the results are likely to be most stable.\n\n_This is nicely illustrated by this chart (Yosinski et al. 2014) where two networks are trained on datasets A and B then the network is chopped at layer n and the layers before are either frozen or fine tuned (indicated by +). The conclusion being seen in the second figure with the to line AnB+ with all 7 layers being tuned producing the best top-1 accuracy._\n\n### Model selection\n\nYour probably best off starting with a smaller model (like ResNet 50), then trying some larger architectures such as (Resnet-101, InceptionNets, EfficientNets). All of these networks have papers available and are definitely worth a read before you go ahead and use them, typically though you should expect to get better accuracy with newer models than older ones.\n\n### Closing Remarks\n\nWith the information i\u2019ve provided above you should be able to get a really decent score on both the public and private leaderboards.\n\nMy intuition from competing in this challenge would suggest that getting into the top 30 on the public leaderboard is sufficient to have a good chance at finishing in the top 10 on the private board due to the uncerta...",
      "url": "https://www.linkedin.com/pulse/how-improve-your-kaggle-competition-leaderboard-ranking-hugo-dolan"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) [2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2) 3 [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Model Training**\n\nWe can improve a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forset is the number of trees in the forest and the maximum number of features used in developing each tree.\u00a0**We need to understand how models work and what impact does each parameter have to the model\u2019s performance, be it accuracy, robustness or speed.**\n\nNormally we would find the best set of parameters by a process called\u00a0**[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**. Actually what it does is simply iterating through all the possible combinations and find the best one.\n\nBy the way, random forest usually reach optimum when\u00a0`max_features`\u00a0is set to the square root of the total number of features.\n\nHere I\u2019d like to stress some points about tuning XGB. These parameters are generally considered to have real impacts on its performance:\n\n- `eta`: Step size used in updating weights. Lower\u00a0`eta`\u00a0means slower training but better convergence.\n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration. This is to combat overfitting.\n- `colsample_bytree`: The ratio of features used in each iteration. This is like\u00a0`max_features`\u00a0in\u00a0`RandomForestClassifier`.\n- `max_depth`: The maximum depth of each tree. Unlike random forest,\u00a0**gradient boosting would eventually overfit if we do not limit its depth**.\n- `early_stopping_rounds`: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n2. Set\u00a0`eta`\u00a0to a relatively high value (e.g. 0.05 ~ 0.1),\u00a0`num_round`\u00a0to 300 ~ 500.\n3. Use grid search to find the best combination of other parameters.\n4. Gradually lower\u00a0`eta`\u00a0until we reach the optimum.\n5. **Use the validation set as\u00a0`watch_list`\u00a0to re-train the model with the best parameters. Observe how score changes on validation set in each iteration. Find the optimal value for\u00a0`early_stopping_rounds`.**\n\nFinally, note that models with randomness all have a parameter like\u00a0`seed`\u00a0or\u00a0`random_state`\u00a0to control the random seed.\u00a0**You must record this**\u00a0with all other parameters when you get a good model. Otherwise you wouldn\u2019t be able to reproduce it.\n\n**Cross Validation**\n\n**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**\u00a0is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse.\u00a0**It is widely believed that we should trust our CV scores under such situation.** Ideally we would want\u00a0**CV scores obtained by different approaches to improve in sync with each other and with the LB score**, but this is not always possible.\n\nUsually\u00a0**5-fold CV**\u00a0is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nHow to do CV properly is not a trivial problem. It requires constant experiment and case-by-case discussion. Many Kagglers share their CV approaches (like\u00a0[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)) after competitions when they feel that reliable CV is not easy.\n\n**Ensemble Generation**\n\n[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\u00a0refers to the technique of combining different models. It\u00a0**reduces both bias and variance of the final model**\u00a0(you can find a proof\u00a0[here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)), thus\u00a0**increasing the score and reducing the risk of overfitting**. Recently it became virtually impossible to win prize without using ensemble in Kaggle competitions.\n\nCommon approaches of ensemble learning are:\n\n- **Bagging**: Use different random subsets of training data to train each base model. Then all the base models vote to generate the final predictions. This is how random forest works.\n- **Boosting**: Train base models iteratively, modify the weights of training samples according to the last iteration. This is how gradient boosted trees work. (Actually it\u2019s not the whole story. Apart from boosting, GBTs try to learn the residuals of earlier iterations.) It performs better than bagging but is more prone to overfitting.\n- **Blending**: Use non-overlapping data to train different base models and take a weighted average of them to obtain the final predictions. This is easy to implement but uses less data.\n- **Stacking**: To be discussed next.\n\nIn theory, for the ensemble to perform well, two factors matter:\n\n- **Base models should be as unrelated as possibly**. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.\n- **Performance of base models shouldn\u2019t differ to much.**\n\nActually we have a\u00a0**trade-off**\u00a0here. In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.\n\n**Stacking**\n\nCompared with blending, stacking makes better use of training data. Here\u2019s a diagram of how it works:\n\n[![Stacking](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2099%200'%3E%3C/svg%3E)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)\n\n_(Taken from\u00a0[Faron](https://www.kaggle.com/mmueller). Many thanks!)_\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example. First we split the training data into 5 folds. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold.\u00a0**You have to keep the predictions on the testing data as well.**\u00a0This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data. After 5 iterations we will obtain a matrix of shape\u00a0`#(samples in training data) X #(base models)`. This matrix is then fed to the stacker (it\u2019s just another model) in the second level. After the stacker is fitted, use the predictions on testing data by base models ( **each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape**) as the input for the stacker and obtain our final predictions.\n\nMaybe it\u2019s better to just show the codes:\n\nPrize winners usually have larger and much more complicated ensembles. For beginner, implementing a correct 5-fold stacking is good enough.\n\n**\\*Pipeline**\n\nWe can see that the workflow for a Kaggle competition is quite complex, especially for model selection and ensemble. Ideally, we need a highly automated pipeline capable of:\n\n- **Modularized feature transformations**. We only need to write a few lines of codes (or better, rules / DSLs) and the new feature is added to the training set.\n- **Automated grid search**. We only need to set up models and parameter grid, the search w...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "Overfitting the leaderboard",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=77c6c69f1305124c7a10:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/caseyftw/overfitting-the-leaderboard"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    },
    {
      "title": "Kaggle Handbook: Fundamentals to Survive a Kaggle Shake-up",
      "text": "<div><div><div><h2>A guide to Kaggle competitions with tips &amp; tricks to get on the good side of the \u201cShake-up\u201d.</h2><div><a href=\"https://medium.com/@ertugruldemir?source=post_page---byline--3dec0c085bc8---------------------------------------\"><div><p></p></div></a></div></div><p>If you\u2019re interested in data science, you probably heard about Kaggle, a platform where hundreds of DS and ML enthusiasts meet, discuss, share and compete.</p><p>In the first post of this series, I\u2019m going to talk briefly about the format of Kaggle competitions and then move on to the fundamental techniques that will help you end up on the better side of the biggest pitfalls of Kaggle competitions: <em>Shake-up</em>s.</p><h2><strong>How do Kaggle competitions work?</strong></h2><p><strong><em>The competition process is basically as follows:</em></strong></p><ul><li>An individual, or more often an organization, identifies a problem and assumes that this issue can be solved by machine learning.</li><li>The data for the problem might already exist or be collected/generated for the competition. Then, a proper metric is established.</li><li>Depending on the sponsors and the complexity of the problem, a prize pool is set.</li><li>The organizers provide Kaggle staff with the necessary data and materials, and finally, the organizers become the Kaggle competition host.</li><li>Participants submit their solutions during the competition phase and receive their final ranking based on the competition metrics.</li></ul><h2>This looks straightforward<strong>. What\u2019s the catch?</strong></h2><p><strong><em>The catch is in the second stage of the competition where your models are tested on data they have never seen before.</em></strong></p><figure><figcaption>Public vs. Private LB, Source: Kaggle</figcaption></figure><p>Kaggle competitions usually consist of two stages. The results of the first stage are displayed on the <strong>Public Leaderboard</strong> \u2014 a live scoreboard \u2014 and the results of the second stage are displayed on the <strong>Private Leaderboard</strong>. The results of the first stage are not decisive for the final ranking, but they are your guide for the private leaderboard. Organizers usually leave only a small part of the test set for the public leaderboard, and the rest is reserved for the private leaderboard, which is the most important for the final rankings. The private leaderboard is hidden from the participants until the competition ends.</p><p>In terms of restrictions and complexity, there are similar contests on Kaggle called <strong>Code Competitions</strong>. In these contests, the test set features are hidden from the participants, which takes the restrictions one step further by limiting the inference times and preventing hand labeling or exploratory data analysis on test data.</p><h2>You were talking about shake-what?!</h2><p><strong><em>That\u2019s the part where your final outcome reveals.</em></strong></p><p>The shake-up is the difference in rank between public and private leaderboards. One of the main goals of Kaggle competitions is to <em>survive </em>the shake-ups. Is it sheer luck to end up on the good side of shake-ups, or are there better ways? How can we assess our chances of survival in shake-ups? What techniques can we use?</p><p>Data scientists are not the kind of people who shut their eyes and hope for the best. So let us figure out how to better <strong><em>estimate</em></strong> the unknown.</p><h2><strong>How do I know what my model will do on the private leaderboard?</strong></h2><p><strong><em>Let\u2019s talk about fundamentals.</em></strong></p><p><strong>Bias-Variance Tradeoff and Overfitting</strong></p><p>Today we can easily train models with a large number of parameters and create highly complex models. Complex models can be very useful, but they can also be very punishing if we are not careful about the <strong>bias-variance tradeoff</strong>. Such models are risky because they are prone to overfitting. Overfitting occurs when a model memorizes the noise of training data instead of learning about the actual pattern behind it. We want our models to <strong>generalize </strong>and capture the actual nature of the data. That\u2019s what we want in Kaggle competitions too, so we can make predictions on a test set that our model hasn\u2019t trained on. The results of these predictions determine your private leaderboard standings. To assess the generalization capabilities of our model, we need to apply <strong>validation techniques</strong> and understand how our model acts in different cases.</p><p>Bias-Variance Tradeoff is beyond the scope of this post, but I highly recommend you take a look if you are not familiar with this concept.</p><h2>What are these validation techniques?</h2><p><strong><em>There are numerous validation techniques but we\u2019re going to talk about more common and Kaggle-specific ones.</em></strong></p><p><strong>Validation Set Approach</strong></p><p>The validation set approach is pretty straightforward. You separate away a fraction of your training data and this new set is called <strong>Hold-Out</strong>:</p><blockquote><p>The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.</p></blockquote><p><em>\u2014 Hands-On Machine Learning with Scikit-Learn and TensorFlow, 2nd Edition.</em></p><p>The latter part of the quote above bears a resemblance to the public leaderboard concept. We can evaluate our final model on the public test set to get an estimate of the generalization error. However, this approach has its own downsides:</p><ul><li>The validation estimate of our model metric can be highly variable, depending on which observations are included in the training set and which are included in the validation set. This is especially true for the <strong>Public Test Set</strong> since we don\u2019t know if the Private Test Set is coming from similar distribution.</li><li>If we take a subset of our training data as the validation set, we decrease the number of observations to train on, which can hamper our model generalization again.</li></ul><p><strong>Cross Validation Approach</strong></p><figure><figcaption>General k-fold cross validation approach with Kaggle competition additions.</figcaption></figure><blockquote><p>This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k \u2212 1 folds.</p></blockquote><p><em>\u2014 Page 203, An Introduction to Statistical Learning, 2021.</em></p><p>This method can solve the problems with the single hold-out set approach that I mentioned above. By creating multiple <strong>folds</strong>, we can test the metric on different cases while observing the variance and eventually using the whole training data! This way, we can have stronger indicators for our model and estimate private leaderboard scores more confidently. Of course, we still assume training and private test set come from similar distributions. However, we train our models on different sets, so together they\u2019re more likely to generalize better than a single holdout set approach.</p><p><strong>Trust Your CV (Cross Validation)</strong></p><p><strong>Trust Your CV </strong>became a pretty popular tagline among Kaggle competitors, reminding them not to build their models based on public leaderboard scores, and I concur. I have participated in quite a few Kaggle competitions over the last few years, and...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    }
  ]
}