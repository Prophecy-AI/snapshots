{
  "query": "How do you reduce the gap between cross-validation and test set performance when predicting for completely unseen chemical compounds with distribution shift?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent studies of out\u2011of\u2011distribution (OOD) validation in drug\u2011discovery ML, you can narrow the CV\u2011test performance gap for completely unseen compounds by redesigning the validation protocol, adding uncertainty calibration, and using density\u2011aware prediction sets\u202f([Step Forward Cross Validation for Bioactivity Prediction](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006); [Evaluating Machine Learning Models for Molecular Property Prediction](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Replace random k\u2011fold CV with a forward\u2011time, n\u2011step CV scheme** \u2013 split the data chronologically (or by synthesis date) and train on earlier folds while validating on the next fold. This mimics prospective use and yields a more realistic estimate of OOD performance\u202f([Step Forward Cross Validation\u2026](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  \n\n2. **Choose OOD\u2011focused split strategies** \u2013 in addition to forward CV, create validation sets using scaffold\u2011based splits or chemical\u2011similarity clustering (e.g., K\u2011means on ECFP4 fingerprints). Scaffold splits give a strong ID\u2011OOD correlation (r\u22480.9), while cluster\u2011based splits present the hardest shift and expose weaknesses\u202f([Evaluating Machine Learning Models\u2026](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  \n\n3. **Build k\u2011fold ensembles for uncertainty estimation** \u2013 train a model on each CV fold and aggregate predictions (e.g., mean and variance). The ensemble variance serves as an uncertainty indicator and improves calibration on unseen chemistry\u202f([Large\u2011scale evaluation of k\u2011fold cross\u2011validation ensembles for uncertainty estimation](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9)).  \n\n4. **Calibrate probabilistic outputs on low\u2011data domains** \u2013 apply methods such as DIONYSUS to adjust predicted probabilities so that confidence scores reflect true likelihoods, especially when training data are scarce\u202f([Calibration and generalizability of probabilistic models\u2026](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  \n\n5. **Apply conformal prediction with density weighting to correct covariate shift** \u2013 train an energy\u2011based model (or KDE) on both labeled and large unlabeled chemical libraries, compute density scores for each test molecule, and use these scores to weight the conformal prediction set construction (CoDrug). This yields valid coverage despite distribution shift\u202f([Conformal Drug Property Prediction with Density Estimation under Covariate Shift](https://arxiv.org/abs/2310.12033)).  \n\n6. **Leverage meta\u2011learning on unlabeled data to bridge ID and OOD gaps** \u2013 use a meta\u2011learning framework that interpolates between ID and OOD representations using abundant unlabeled compounds, enabling the model to \u201clearn how to generalize\u201d beyond the training distribution\u202f([Robust Molecular Property Prediction via Densifying Scarce Labeled Data](https://arxiv.org/abs/2506.11877)).  \n\n7. **Validate the whole pipeline on an OOD benchmark** \u2013 run the final model on a curated OOD benchmark such as DrugOOD, which provides realistic distribution\u2011shift scenarios and noise annotations, to quantify the remaining performance gap and iterate on the above steps as needed\u202f([DrugOOD: Out\u2011of\u2011Distribution Dataset Curator and Benchmark\u2026](https://ojs.aaai.org/index.php/AAAI/article/download/25970/25742)).  \n\nFollowing these seven actions\u2014forward\u2011time CV, OOD\u2011aware splits, ensemble uncertainty, probability calibration, density\u2011weighted conformal prediction, meta\u2011learning with unlabeled data, and benchmark\u2011driven validation\u2014will systematically reduce the discrepancy between cross\u2011validation estimates and true test\u2011set performance on completely unseen chemical compounds.",
      "url": ""
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation",
      "text": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00709-9?)\n# Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 April 2023\n* Volume\u00a015, article\u00a0number49, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nLarge-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n* [Thomas-Martin Dutschmann](#auth-Thomas_Martin-Dutschmann-Aff1)[1](#Aff1),\n* [Lennart Kinzel](#auth-Lennart-Kinzel-Aff1)[1](#Aff1),\n* [Antonius ter Laak](#auth-Antonius-Laak-Aff2)[2](#Aff2)&amp;\n* \u2026* [Knut Baumann](#auth-Knut-Baumann-Aff1)[1](#Aff1)Show authors\n* 7359Accesses\n* 50Citations\n* 3Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9/metrics)\n## Abstract\nIt is insightful to report an estimator that describes how certain a model is in a prediction, additionally to the prediction alone. For regression tasks, most approaches implement a variation of the ensemble method, apart from few exceptions. Instead of a single estimator, a group of estimators yields several predictions for an input. The uncertainty can then be quantified by measuring the disagreement between the predictions, for example by the standard deviation. In theory, ensembles should not only provide uncertainties, they also boost the predictive performance by reducing errors arising from variance. Despite the development of novel methods, they are still considered the \u201cgolden-standard\u201d to quantify the uncertainty of regression models. Subsampling-based methods to obtain ensembles can be applied to all models, regardless whether they are related to deep learning or traditional machine learning. However, little attention has been given to the question whether the ensemble method is applicable to virtually all scenarios occurring in the field of cheminformatics. In a widespread and diversified attempt, ensembles are evaluated for 32 datasets of different sizes and modeling difficulty, ranging from physicochemical properties to biological activities. For increasing ensemble sizes with up to 200 members, the predictive performance as well as the applicability as uncertainty estimator are shown for all combinations of five modeling techniques and four molecular featurizations. Useful recommendations were derived for practitioners regarding the success and minimum size of ensembles, depending on whether predictive performance or uncertainty quantification is of more importance for the task at hand.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-45630-5?as&#x3D;webp)\n### [Ensemble Models](https://link.springer.com/10.1007/978-3-031-45630-5_12?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-48317-7?as&#x3D;webp)\n### [Ensemble Methods for Time Series Forecasting](https://link.springer.com/10.1007/978-3-319-48317-7_13?fromPaywallRec=false)\nChapter\u00a9 2017\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-12240-8?as&#x3D;webp)\n### [Ensemble Models Using Symbolic Regression and Genetic Programming for Uncertainty Estimation in ESG and Alternative Investments](https://link.springer.com/10.1007/978-3-031-12240-8_5?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://jcheminf.biomedcentral.com/subjects/applied-probability)\n* [Applied Statistics](https://jcheminf.biomedcentral.com/subjects/applied-statistics)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://jcheminf.biomedcentral.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Statistical Theory and Methods](https://jcheminf.biomedcentral.com/subjects/statistical-theory-and-methods)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nMachine learning (ML) for drug design purposes holds a long tradition\u00a0[[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR1)], but has recently started to gain further attention due to the success of deep learning (DL)\u00a0[[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR2)]. Yet, the prediction of chemical properties and activities is only one step in a long and resource-intense process of drug design, discovery, and development. When developing ML models, predictions alone are not sufficient and require further analysis\u00a0[[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR3)]. During model construction and testing, errors made by the model can easily be evaluated since the true target values are known. The error distribution allows the estimation of the quality of the model, but cannot be applied when predicting values for new compounds with unknown target values. In this case, it is good practice to provide an estimate of the uncertainty associated with the prediction. Measures quantifying the predictive uncertainty can be used to set a threshold which defines the model\u2019s applicability domain. The latter is defined as follows: \u201cThe applicability domain of a (Q)SAR model is the response and chemical structure space in which the model makes predictions with a given reliability.\u201d\u00a0[[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR4)]. The reliability of a model can either be addressed by quantifying its confidence, or, conversely, its uncertainty. Recent studies make use of the term uncertainty quantification (UQ)\u00a0[[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR5)]. Uncertainty can be of aleatoric nature, relating to the random process that generates the target values, or epistemic nature, implying model-related uncertainty\u00a0[[6](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR6)]. Usually, these two types cannot be fully distinguished\u00a0[[7](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR7)].\nClassification algorithms often provide built-in mechanisms or augmentations to measure their uncertainty\u00a0[[8](https://jcheminf.biomedcentral.com/...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2310.12033** (cs)\n\n\\[Submitted on 18 Oct 2023\\]\n\n# Title:Conformal Drug Property Prediction with Density Estimation under Covariate Shift\n\nAuthors: [Siddhartha Laghuvarapu](https://arxiv.org/search/cs?searchtype=author&query=Laghuvarapu,+S), [Zhen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+Z), [Jimeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+J)\n\nView a PDF of the paper titled Conformal Drug Property Prediction with Density Estimation under Covariate Shift, by Siddhartha Laghuvarapu and Zhen Lin and Jimeng Sun\n\n[View PDF](https://arxiv.org/pdf/2310.12033)\n\n> Abstract:In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.\n\n|     |     |\n| --- | --- |\n| Comments: | Accepted at NeurIPS 2023 |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2310.12033](https://arxiv.org/abs/2310.12033) \\[cs.LG\\] |\n|  | (or [arXiv:2310.12033v1](https://arxiv.org/abs/2310.12033v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2310.12033](https://doi.org/10.48550/arXiv.2310.12033)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Siddhartha Laghuvarapu \\[ [view email](https://arxiv.org/show-email/e46b4ff5/2310.12033)\\]\n\n**\\[v1\\]**\nWed, 18 Oct 2023 15:17:10 UTC (1,484 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Conformal Drug Property Prediction with Density Estimation under Covariate Shift, by Siddhartha Laghuvarapu and Zhen Lin and Jimeng Sun\n\n- [View PDF](https://arxiv.org/pdf/2310.12033)\n- [TeX Source](https://arxiv.org/src/2310.12033)\n- [Other Formats](https://arxiv.org/format/2310.12033)\n\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2310.12033&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2310.12033&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-10](https://arxiv.org/list/cs.LG/2023-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2310.12033?context=cs)\n\n[stat](https://arxiv.org/abs/2310.12033?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2310.12033?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.12033)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.12033)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.12033)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2310.12033&description=Conformal Drug Property Prediction with Density Estimation under Covariate Shift) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2310.12033&title=Conformal Drug Property Prediction with Density Estimation under Covariate Shift)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2310.12033) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2310.12033"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "DrugOOD: Out-of-Distribution Dataset Curator and Benchmark for AI-Aided Drug Discovery \u2013 a Focus on Affinity Prediction Problems with Noise Annotations",
      "text": "DrugOOD: Out-of-Distribution Dataset Curator and Benchmark for AI-Aided\nDrug Discovery \u2013 a Focus on Affinity Prediction Problems with Noise Annotations\nYuanfeng Ji1,3*, Lu Zhang1,2*, Jiaxiang Wu1\n, Bingzhe Wu1, Lanqing Li1,\nLong-Kai Huang1, Tingyang Xu1, Yu Rong1, Jie Ren1, Ding Xue1, Houtim Lai1,\nWei Liu1, Junzhou Huang1, Shuigeng Zhou2, Ping Luo3, Peilin Zhao1, Yatao Bian1\u2020\n1 Tencent AI Lab, China\n2 Fudan University, China\n3 The University of Hong Kong, China\nu3008013@connect.hku.hk\nAbstract\nAI-aided drug discovery (AIDD) is gaining popularity due to\nits potential to make the search for new pharmaceuticals faster,\nless expensive, and more effective. Despite its extensive use in\nnumerous fields (e.g., ADMET prediction, virtual screening),\nlittle research has been conducted on the out-of-distribution\n(OOD) learning problem with noise. We present DrugOOD,\na systematic OOD dataset curator and benchmark for AIDD.\nParticularly, we focus on the drug-target binding affinity pre\u0002diction problem, which involves both macromolecule (pro\u0002tein target) and small-molecule (drug compound). DrugOOD\noffers an automated dataset curator with user-friendly cus\u0002tomization scripts, rich domain annotations aligned with bio\u0002chemistry knowledge, realistic noise level annotations, and\nrigorous benchmarking of SOTA OOD algorithms, as opposed\nto only providing fixed datasets. Since the molecular data\nis often modeled as irregular graphs using graph neural net\u0002work (GNN) backbones, DrugOOD also serves as a valuable\ntestbed for graph OOD learning problems. Extensive empirical\nstudies have revealed a significant performance gap between\nin-distribution and out-of-distribution experiments, emphasiz\u0002ing the need for the development of more effective schemes\nthat permit OOD generalization under noise for AIDD.\nIntroduction\nTraditional drug discovery procedures are lengthy and ex\u0002pensive. To accelerate the drug development process, drug\u0002makers and investors are turning to artificial intelligence\ntechniques (Muratov et al. 2020) for drug discovery (e.g.,\nADMET prediction (Rong et al. 2020), target identification,\nprotein structure prediction(Shen et al. 2021)), which aim to\nrapidly identify new compounds and model complex mecha\u0002nisms to automate previously manual processes (Schneider\n2018). In this paper, we focus on one of the most challenging\napplications, called drug-target binding affinity prediction,\nwhich aims to identify a subset of compounds with high bind\u0002ing affinity for a given protein target among many candidate\ncompounds.\n*Equal contribution. Order was determined by tossing a coin.\n\u2020Corresponding author: Yatao Bian.\nCopyright \u00a9 2023, Association for the Advancement of Artificial\n(a) Structure-based Affinity Prediction (b) Protein/Compound Distribution\nTest (unseen domain)\nTrain (seen domain)\nAffinity\npred\nperformance\nFigure 1: (a) Structure-based affinity prediction aims to pre\u0002dict binding affinity values between a pair of target (protein)\nand compound (molecule), and (b) model performance is of\u0002ten severely degraded when the data distributions are shifted.\nDistribution shift is a ubiquitous problem in the field of\nAIDD, where the training distribution differs from the test\ndistribution. Typically, the prediction model is trained on\nknown target proteins when conducting virtual screening for\nhit findings. A \u201cblack swan\u201d event, such as COVID-19, may\nnevertheless occur, resulting in a new target with unseen data\ndistributions. Hence, the performance of the unseen target\nwill decline drastically. To address the performance degra\u0002dation (Koh et al. 2021) caused by distribution shift, it is\nnecessary to develop robust and generalizable algorithms\nfor this challenging issue in AIDD. Despite its importance\nin real-world problems, the community still lacks curated\nOOD datasets and benchmarks for inspiring relevant research.\nBesides, label noise is another critical issue. Generally, AI\nmodels are trained using publicly deposited datasets, such\nas ChEMBL, whereas the bioassay data are typically noisy\n(Kramer et al. 2012; Cort\u00e9s-Ciriano and Bender 2016). For in\u0002stance, the activity data from ChEMBL is manually extracted\nfrom the full texts of seven Medicinal Chemistry journals\n(Mendez et al. 2019). Various factors, including but not lim\u0002ited to different confidence levels for activities measured\nthrough experiments, unit-translation errors, repeated cita\u0002tions of single measurements, and different \u201ccut-off\u201d noise1\n,\nIntelligence (www.aaai.org). All rights reserved.\n1E.g., measurements could be recorded with <, \u2264, \u2248, >, \u2265,\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8023\nMolecular Scaffold Domain Molecular Size Domain\nx = \ny = activate \nd = scaffold \n24111 \n\u2026\nx = \ny = inactivate \nd = scaffold \n0 \nx = \ny = activate \nd = size\n228 \n\u2026\nx = \ny = activate \nd = size\n0 \nx = \ny = activate \nd = protein \n126 \n\u2026\nx = \ny = inactivate \nd = protein \n0 \nx = \ny = activate \nd = protein \nfamily 11 \n\u2026\nx = \ny = activate \nd = protein\nfamily 0 \nProtein Domain Protein family Domain\nx = \ny = activate \nd = scaffold \n34806 \n\u2026\nx = \ny = inactivate \nd = scaffold \n24112 \nx = \ny = activate \nd = size \n250 \n\u2026\nx = \ny = inactivate \nd = size\n229 \nx = \ny = activate \nd = protein \n415 \n\u2026\nx = \ny = inactivate \nd = protein \n127 \nx = \ny = inactivate \nd = protein\nfamily 14 \n\u2026\nx = \ny = activate \nd = protein\nfamily 12 \nTrain (seen domain) Test (unseen domain)\nMGAASGQ\nRGRWPLSP\nPLLMLSLLL\nLLLL\u2026\nMQRSPPG\nYGAQDDPP\nSRRDCAW\nAPGI\u2026\nMGSLLALL\nALLLLWGA\nVAEGPAKK\nVLTL\u2026\nMQSKVLLA\nVALWLCVE\nTRAASVGL\nPSV\u2026\nMGRPLHLV\nLLSASLAGL\nLLLGESLFIR\nREQ\u2026\nMHSKVTIIC\nIRFLFWFLL\nLCMLIGKS\nHTE\u2026\nMSTMHLLT\nFALLFSCSF\nARAACDPK\nIVNI\u2026\nMRALWVL\nGLCCVLLTF\nGSVRADDE\nVDV\u2026\nFigure 2: Exemplar curated datasets spanning different domain shifts from DrugOOD. Each data sample (x, y, d) in dataset is\nassociated with a domain annotation d, which corresponds to a distribution P over data points which are similar in some way,\ne.g., molecules with the same scaffold. Specifically, DrugOOD focuses on the problem of domain generalization, in which we\ntrain (seen domain) and test (unseen domain) the model on disjoint domains, e.g., molecules with a new scaffold. Additionally,\nDrugOOD identifies and annotates three noise levels (core, refined, general), whose level increases with data volume and noise\nsources.\ncan cause noise in these data. Figure 2 shows examples with\ndifferent noisy levels. Meanwhile, real-world data with noise\nlevel annotations is lacking for learning tasks under noise\nlabels (Angluin and Laird 1988; Han et al. 2020).\nTo help accelerate research by focusing community at\u0002tention and simplifying systematic comparisons between\ndata collection and implementation method, we present\nDrugOOD, a systematic OOD dataset curator and benchmark\nfor AI-assisted drug discovery that includes an open-source\nPython package which fully automates the data curation and\nOOD benchmarking processes. We focus on the most chal\u0002lenging OOD setting: domain generalization (Zhou et al.\n2021) problem in AI-aided drug discovery, though DrugOOD\ncan be easily adapted to other OOD settings, such as subpopu\u0002lation shift (Koh et al. 2021) and domain adaptation (Zhuang\net al. 2020). Our dataset is also the first AIDD dataset curator\nwith realistic noise level annotations that can serve as an\nimportant testbed for the setting of learning under noise.\nIn contrast to only providing fixed datasets, we present an\nautomated dataset curator based on the large-scale bioassay\ndeposition website ChEMBL (Mendez et al. 2019). Figure 3\nprovides a summary of the automated dataset curator. Using\nthis dataset curator, researchers/practitioners can generate\nnew OOD datasets based on their needs by simply modifying\nthe configuration files in the Python package. Specifically,\nwe also realize this dataset curator by generating 45 OOD\ndatasets spanning various domains, noise levels, and measure\u0002ment types. This mechanism offers two benefits: i) It ensures\naccurate reproduction of our datasets and benchmarks, ii...",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25970/25742"
    },
    {
      "title": "Characterizing Uncertainty in Machine Learning for Chemistry",
      "text": "Characterizing Uncertainty in Machine Learning for Chemistry | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Characterizing Uncertainty in Machine Learning for Chemistry\n08 February 2023, Version 1\nThis is not the most recent version. There is a[\nnewer version\n](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)of this content available\nWorking Paper\n## Authors\n* [Esther Heid](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Esther%20Heid)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-8404-6596),\n* [Charles J. McGill](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Charles%20J.%20McGill),\n* [Florence H. Vermeire](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Florence%20H.%20Vermeire),\n* [William H. Green](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=William%20H.%20Green)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nCharacterizing uncertainty in machine learning models has recently gained interest in the context of machine learning reliability, robustness, safety, and active learning. Here, we separate the total uncertainty into contributions from noise in the data (aleatoric) and shortcomings of the model (epistemic), further dividing epistemic uncertainty into model bias and variance contributions. We systematically address the influence of noise, model bias, and model variance in the context of chemical property predictions, where the diverse nature of target properties and the vast chemical chemical space give rise to many different distinct sources of prediction error. We demonstrate that different sources of error can each be significant in different contexts and must be individually addressed during model development. Through controlled experiments on datasets of molecular properties, we show important trends in model performance associated with the level of noise in the dataset, size of the dataset, model architecture, molecule representation, ensemble size, and dataset splitting. In particular, we show that 1) noise in the test set can limit a model's observed performance when the actual performance is much better, 2) using size-extensive model aggregation structures is crucial for extensive property prediction, 3) ensembling is a reliable tool for uncertainty quantification and improvement specifically for the contribution of model variance, and 4) evaluations of cross-validation models understate their performance. We develop general guidelines on how to improve an underperforming model when falling into different uncertainty contexts.\n## Keywords\n[Uncertainty](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Uncertainty)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[Chemical property prediction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Chemical%20property%20prediction)\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nCharacterizing Uncertainty in Machine Learning for Chemistry Scripts\n**Description**\nPython scripts to reproduce the results of the manuscript.\n**Actions**\n[**View**](https://github.com/cjmcgill/characterizing_uncertainty_scripts)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\n[May 17, 2023 Version\n3](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)\n[Feb 16, 2023 Version\n2](https://chemrxiv.org/engage/chemrxiv/article-details/63ecf74d1d2d1840638a8b75)\nFeb 08, 2023 Version 1\n## Metrics\n3,215\n1,200\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2023-00vcg\nD O I: 10.26434/chemrxiv-2023-00vcg [opens in a new tab]](https://doi.org/10.26434/chemrxiv-2023-00vcg)\n## Funding\n**Austrian Science Fund**\nJ-4415\n**Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS)**\n## Author\u2019s competing interest statement\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n## Ethics\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/63e2b82b6d032916bb650914"
    }
  ]
}