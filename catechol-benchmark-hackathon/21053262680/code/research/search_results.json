{
  "query": "How do you reduce distribution shift error when predicting molecular properties for unseen chemical compounds not in training data?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing distribution\u2011shift error for molecular\u2011property prediction can be achieved by combining data\u2011splitting strategies that expose the model to realistic shifts, training techniques that leverage both labeled and unlabeled chemistry, and uncertainty\u2011aware inference methods\u202f([Nature\u202f2025](https://www.nature.com/articles/s41524-025-01808-x); [arXiv\u202f2025](https://arxiv.org/html/2502.05970v1); [Blopig\u202f2021](https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction); [CoDrug\u202f2023](https://arxiv.org/abs/2310.12033)).\n\n1. **Create OOD\u2011focused splits** \u2013 use scaffold\u2011based splitting (or other chemically meaningful splits) instead of random splits so the test set contains scaffolds absent from training, thereby forcing the model to learn features that generalise across chemical space\u202f([Blopig\u202f2021](https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction)).  \n\n2. **Apply a transductive or meta\u2011learning framework** \u2013 incorporate abundant unlabeled molecules and train the predictor to interpolate between in\u2011distribution (ID) and out\u2011of\u2011distribution (OOD) data. This meta\u2011learning step reduces covariate shift by aligning ID and OOD representations\u202f([arXiv\u202f2025\u202f\u201cRobust Molecular Property Prediction via Densifying Scarce Labeled Data\u201d](https://arxiv.org/abs/2506.11877)).  \n\n3. **Leverage robust uncertainty quantification** \u2013 use conformal prediction with density estimation (e.g., the CoDrug method) to weight each molecule by its estimated density under the training distribution, producing calibrated prediction sets that remain valid under shift\u202f([CoDrug\u202f2023](https://arxiv.org/abs/2310.12033)).  \n\n4. **Distill knowledge from large teacher models** \u2013 train a high\u2011capacity graph\u2011neural\u2011network teacher on diverse datasets, then distill its embeddings into a smaller student model. The distilled student retains the teacher\u2019s general\u2011representation power, improving OOD accuracy without overfitting to the training scaffold distribution\u202f([NIH\u202f2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC12165064)).  \n\n5. **Validate on curated OOD benchmarks** \u2013 evaluate the final model on datasets such as DrugOOD or the OOD splits used in the Nature study to quantify the remaining distribution\u2011shift error and fine\u2011tune hyper\u2011parameters accordingly\u202f([DrugOOD\u202f2023](https://ojs.aaai.org/index.php/AAAI/article/download/25970/25742); [Nature\u202f2025](https://www.nature.com/articles/s41524-025-01808-x)).  \n\nFollowing these steps\u2014realistic splitting, transductive/meta\u2011learning with unlabeled chemistry, density\u2011aware uncertainty calibration, knowledge distillation, and rigorous OOD evaluation\u2014substantially reduces distribution\u2011shift error when predicting properties of unseen compounds.",
      "url": ""
    },
    {
      "title": "Out-of-Distribution Property Prediction in Materials and Molecules",
      "text": "Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules | npj Computational Materials\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![npj Computational Materials](https://media.springernature.com/full/nature-cms/uploads/product/npjcompumats/header-4baba14304e9c3518bdc0d6f35b470b9.svg)](https://www.nature.com/npjcompumats)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41524-025-01808-x?error=cookies_not_supported&code=700d2411-cae5-4b77-8d63-7e62cff2a842)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41524)\n* [RSS feed](https://www.nature.com/npjcompumats.rss)\nKnown Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:20 November 2025# Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n* [Nofit Segal](#auth-Nofit-Segal-Aff1)[1](#Aff1)[na1](#na1),\n* [Aviv Netanyahu](#auth-Aviv-Netanyahu-Aff2)[2](#Aff2)[na1](#na1),\n* [Kevin P. Greenman](#auth-Kevin_P_-Greenman-Aff3-Aff4)[3](#Aff3),[4](#Aff4),\n* [Pulkit Agrawal](#auth-Pulkit-Agrawal-Aff2)[2](#Aff2)&amp;\n* \u2026* [Rafael G\u00f3mez-Bombarelli](#auth-Rafael-G_mez_Bombarelli-Aff1)[1](#Aff1)Show authors\n[*npj Computational Materials*](https://www.nature.com/npjcompumats)**volume11**, Article\u00a0number:345(2025)[Cite this article](#citeas)\n* 3094Accesses\n* 1Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41524-025-01808-x/metrics)\n### Subjects\n* [Chemistry](https://www.nature.com/subjects/chemistry)\n* [Materials science](https://www.nature.com/subjects/materials-science)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n## Abstract\nDiscovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (OOD) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to OOD property prediction, achieving improvements in prediction accuracy. In particular, our method improves extrapolative precision by 1.8\u00d7 for materials and 1.5\u00d7 for molecules, and boosts recall of high-performing candidates by up to 3\u00d7. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-66685-w/MediaObjects/41467_2025_66685_Fig1_HTML.png)\n### [Molecular Motif Learning as a pretraining objective for molecular property prediction](https://www.nature.com/articles/s41467-025-66685-w?fromPaywallRec=false)\nArticleOpen access27 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-53751-y/MediaObjects/41467_2024_53751_Fig1_HTML.png)\n### [MolE: a foundation model for molecular graphs using disentangled attention](https://www.nature.com/articles/s41467-024-53751-y?fromPaywallRec=false)\nArticleOpen access12 November 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n## Introduction\nDesigning new materials and molecules is essential for the development of new technologies. Traditionally, this design process involves extensive experimental iteration or high-throughput methods to screen databases, which are time-consuming and resource-intensive[1](https://www.nature.com/articles/s41524-025-01808-x#ref-CR1),[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2). As a result, there is increasing interest in applying machine learning (ML) techniques to accelerate the discovery of materials and molecules with desired properties[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s41524-025-01808-x#ref-CR7). There is particular interest in property values that are outside the known property value distribution, as these will most likely lead to discovering new materials that will, in turn, unlock new capabilities and technologies.\nOne strategy for discovering materials and molecules with desired properties is inverse design via conditional generation, where the goal is to create candidates with out-of-distribution (OOD) property values that are absent from the training data[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2),[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[5](https://www.nature.com/articles/s41524-025-01808-x#ref-CR5),[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41524-025-01808-x#ref-CR10). A complementary strategy is virtual screening, where a large database of candidate materials is evaluated using predicted properties[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](https://www.nature.com/articles/s41524-025-01808-x#ref-CR16). In this case, the objective is to identify high-performing OOD candidates from a set of known compositions with unknown properties. Screening often involves applying a threshold, either an absolute value or a percentile cutoff, and selecting candidates whose predicted properties exceed it[17](#ref-CR17),[18](#ref-CR18),[19](https://www.nature.com/articles/s41524-025-01808-x#ref-CR19). However, both generative and screening approaches commonly face challenges when the target property values lie outside the distribution of the training data[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[13](https://www.nature.com/articles/s41524-025-01808-x#ref-CR13),[15](https://www.nature.com/articles/s41524-025-01808-x#ref-CR15),[20](#ref-CR20),[21](#ref-CR21),[22](https://www.nature.com/articles/s41524-025-01808-x#ref-CR22). Enhancing extrapolative capabilities in property prediction would improve the screening of large candidate spaces in terms of precision by identifying promising compounds and molecules with exceptional properties. This has the potential to streamline the identification process by reducing time and resource expenditure on low-potential candidates, thereby accelerating the discovery of materials and molecules with high synthesis viability.\nExtrapolation in materials science can refer to both the domain and the range of the predictive function. Typically, extrapolation is used to refer to*generalization*in the domain space, i.e., unseen classes of materials, structures, and chemical spaces, e.g., training on metals and predicting ceramics, or training on ar...",
      "url": "https://www.nature.com/articles/s41524-025-01808-x"
    },
    {
      "title": "Knowledge Distillation for Molecular Property Prediction - NIH",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Knowledge distillation (KD) is a powerful model compression technique that transfers knowledge from complex teacher models to compact student models, reducing computational costs while preserving predictive accuracy. This study investigated KD's efficacy in molecular property prediction across domain\u2010specific and cross\u2010domain tasks, leveraging state\u2010of\u2010the\u2010art graph neural networks (SchNet, DimeNet++, and TensorNet). In the domain\u2010specific setting, KD improved regression performance across diverse quantum mechanical properties in the QM9 dataset, with DimeNet++ student models achieving up to an 90% improvement in <span></span> compared to non\u2010KD baselines. Notably, in certain cases, smaller student models achieved comparable or even superior <span></span> improvements while being 2\u00d7 smaller, highlighting KD's ability to enhance efficiency without sacrificing predictive performance. Cross\u2010domain evaluations further demonstrated KD's adaptability, where embeddings from QM9\u2010trained teacher models enhanced predictions for ESOL (log<em>S</em>) and FreeSolv (\u0394<em>G<sub>hyd</sub>\n</em>), with SchNet exhibiting the highest gains of \u224865% in log<em>S</em>\u2005predictions. Embedding analysis revealed substantial student\u2010teacher alignment gains, with the relative shift in cosine similarity distribution peaks reaching up to 1.0 across student models. These findings highlighted KD as a robust strategy for enhancing molecular representation learning, with implications for cheminformatics, materials science, and drug discovery.</p>\n<section><p><strong>Keywords:</strong> graph neural networks, knowledge distillation, materials informatics, scalability</p></section></section><section><hr/>\n<p>This study explores the effectiveness of knowledge distillation (KD) for molecular property prediction using graph neural networks. By leveraging KD, student models achieve improved scalability and predictive accuracy across domain\u2010specific and cross\u2010domain molecular datasets. Embedding analysis highlights the enhanced alignment between teacher and student models, demonstrating KD's potential for efficient molecular representation learning.\n</p>\n<section><p></p></section></section><section><h2>1. Introduction</h2>\n<p>Molecular graphs are pivotal in cheminformatics, representing molecules as nodes (atoms) and edges (bonds), enabling advanced machine learning models to predict molecular properties efficiently. Datasets such as QM9<sup>[</sup>\n<a href=\"#advs12007-bib-0001\">\n<sup>1</sup>\n</a>\n<sup>]</sup> and MoleculeNet<sup>[</sup>\n<a href=\"#advs12007-bib-0002\">\n<sup>2</sup>\n</a>\n<sup>]</sup> provide a rich variety of molecular structures annotated with diverse chemical and physical properties, facilitating both predictive model development and performance evaluation. Leveraging these datasets, state\u2010of\u2010the\u2010art models like SchNet,<sup>[</sup>\n<a href=\"#advs12007-bib-0003\">\n<sup>3</sup>\n</a>, <a href=\"#advs12007-bib-0004\">\n<sup>4</sup>\n</a>\n<sup>]</sup> DimeNet++,<sup>[</sup>\n<a href=\"#advs12007-bib-0005\">\n<sup>5</sup>\n</a>, <a href=\"#advs12007-bib-0006\">\n<sup>6</sup>\n</a>\n<sup>]</sup> and TensorNet<sup>[</sup>\n<a href=\"#advs12007-bib-0007\">\n<sup>7</sup>\n</a>\n<sup>]</sup> have demonstrated significant success in molecular property prediction. SchNet captures continuous atomic interactions using convolutional layers, DimeNet++ integrates angular information to enhance geometric understanding, and TensorNet employs tensorial representations to capture higher\u2010order interactions, offering deeper structural insights.</p>\n<p>However, the increasing complexity of molecular models presents significant computational challenges,<sup>[</sup>\n<a href=\"#advs12007-bib-0008\">\n<sup>8</sup>\n</a>, <a href=\"#advs12007-bib-0009\">\n<sup>9</sup>\n</a>, <a href=\"#advs12007-bib-0010\">\n<sup>10</sup>\n</a>, <a href=\"#advs12007-bib-0011\">\n<sup>11</sup>\n</a>\n<sup>]</sup> particularly for large\u2010scale datasets such as QM9 and MoleculeNet. These datasets contain extensive quantum mechanical and experimental property annotations across thousands of molecular structures, requiring models to process high\u2010dimensional representations that encompass atomic positions, bond interactions, and electronic properties. As a result, training and deploying machine learning models on such datasets demands substantial computational resources, including high memory usage, prolonged training times, and intensive GPU processing.<sup>[</sup>\n<a href=\"#advs12007-bib-0012\">\n<sup>12</sup>\n</a>\n<sup>]</sup> Graph neural networks (GNNs) such as SchNet, DimeNet++, and TensorNet further intensify these challenges by incorporating intricate spatial and electronic interactions within molecular graphs. While these architectures provide state\u2010of\u2010the\u2010art performance in molecular property prediction, their computational demands hinder their scalability and practical application, particularly in high\u2010throughput screening pipelines and resource\u2010constrained environments.</p>\n<p>To address these bottlenecks, model compression techniques such as pruning,<sup>[</sup>\n<a href=\"#advs12007-bib-0013\">\n<sup>13</sup>\n</a>, <a href=\"#advs12007-bib-0014\">\n<sup>14</sup>\n</a>\n<sup>]</sup> transfer learning,<sup>[</sup>\n<a href=\"#advs12007-bib-0015\">\n<sup>15</sup>\n</a>, <a href=\"#advs12007-bib-0016\">\n<sup>16</sup>\n</a>, <a href=\"#advs12007-bib-0017\">\n<sup>17</sup>\n</a>, <a href=\"#advs12007-bib-0018\">\n<sup>18</sup>\n</a>\n<sup>]</sup> and knowledge distillation (KD)<sup>[</sup>\n<a href=\"#advs12007-bib-0019\">\n<sup>19</sup>\n</a>, <a href=\"#advs12007-bib-0020\">\n<sup>20</sup>\n</a>, <a href=\"#advs12007-bib-0021\">\n<sup>21</sup>\n</a>, <a href=\"#advs12007-bib-0022\">\n<sup>22</sup>\n</a>, <a href=\"#advs12007-bib-0023\">\n<sup>23</sup>\n</a>, <a href=\"#advs12007-bib-0024\">\n<sup>24</sup>\n</a>, <a href=\"#advs12007-bib-0025\">\n<sup>25</sup>\n</a>, <a href=\"#advs12007-bib-0026\">\n<sup>26</sup>\n</a>, <a href=\"#advs12007-bib-0027\">\n<sup>27</sup>\n</a>, <a href=\"#advs12007-bib-0028\">\n<sup>28</sup>\n</a>, <a href=\"#advs12007-bib-0029\">\n<sup>29</sup>\n</a>, <a href=\"#advs12007-bib-0030\">\n<sup>30</sup>\n</a>, <a href=\"#advs12007-bib-0031\">\n<sup>31</sup>\n</a>, <a href=\"#advs12007-bib-0032\">\n<sup>32</sup>\n</a>\n<sup>]</sup> have gained prominence. However, pruning can lead to the loss of structural information by removing essential connections, resulting in irregular sparsity that limits hardware efficiency.<sup>[</sup>\n<a href=\"#advs12007-bib-0033\">\n<sup>33</sup>\n</a>\n<sup>]</sup> Transfer learning, on the other hand, faces challenges such as negative transfer, where pre\u2010trained knowledge can hinder performance on new tasks.<sup>[</sup>\n<a href=\"#advs12007-bib-0033\">\n<sup>33</sup>\n</a>\n<sup>]</sup> Unlike these approaches, KD preserves the teacher model's structured knowledge, facilitates smooth adaptation across datasets, and produces dense yet compact models optimized for computational efficiency and generalization. This approach is particularly advantageous for molecular datasets, where efficient prediction of quantum and physicochemical properties is crucial for accelerating material discovery and drug design.<sup>[</sup>\n<a href=\"#advs12007-bib-0022\">\n<sup>22</sup>\n</a>, <a href=\"#advs12007-bib-0023\">\n<sup>23</sup>\n</a>, <a href=\"#advs12007-bib-0024\">\n<sup>24</sup>\n</a>, <a href=\"#advs12007-bib-0025\">\n<sup>25</sup>\n</a>, <a href=\"#advs12007-bib-0029\">\n<sup>29</sup>\n</a>\n<sup>]</sup> Beyond computational efficiency, KD also plays a crucial role in improving model generalizability,<sup>[</sup>\n<a href=\"#advs12007-bib-0025\">\n<sup>25</sup>\n</a>, <a href=\"#advs12007-bib-0031\">\n<sup>31</sup>\n</a>\n<sup>]</sup> interpretability,<sup>[</sup>\n<a href=\"#advs12007-bib-0027\">\n<sup>27</sup>\n</a>\n<sup>]</sup> and robustness.<sup>[</sup>\n<a href=\"#advs12007-bib-0030\">\n<sup>30</sup>\n</a>\n<sup>]</sup> In molecular property regression, different datasets often exhibit variations in numerical scales, feature importance, and noise ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12165064"
    },
    {
      "title": "Out-of-Distribution Property Prediction in Materials and Molecules",
      "text": "<div><div>\n<article>\n<p>[1]<span>\\fnm</span>Nofit <span>\\sur</span>Segal</p>\n<p>[2]<span>\\fnm</span>Aviv <span>\\sur</span>Netanyahu</p>\n<div>\n<p><span>\\equalcont</span></p><p>Equal Advising.</p>\n</div>\n<div>\n<p><span>\\equalcont</span></p><p>Equal Advising.</p>\n</div>\n<p>1]<span>\\orgdiv</span>Materials Science and Engineering, <span>\\orgname</span>MIT, <span>\\orgaddress</span><span>\\street</span>Memorial Dr, <span>\\city</span>Cambridge, <span>\\postcode</span>02139, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>2]<span>\\orgdiv</span>Electrical Engineering and Computer Science, <span>\\orgname</span>MIT, <span>\\orgaddress</span><span>\\street</span>Vassar St, <span>\\city</span>Cambridge, <span>\\postcode</span>02139, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>3]<span>\\orgdiv</span>Chemical Engineering, <span>\\orgname</span>Catholic Institute of Technology, <span>\\orgaddress</span><span>\\street</span>Broadway, <span>\\city</span>Cambridge, <span>\\postcode</span>02142, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>4]<span>\\orgdiv</span>Chemistry, <span>\\orgname</span>Catholic Institute of Technology, <span>\\orgaddress</span><span>\\street</span>Broadway, <span>\\city</span>Cambridge, <span>\\postcode</span>02142, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>\n<span>\u2003\u2003</span><span>\n<span>\n</span><span>\n<span><a href=\"mailto:rafagb@mit.edu\">rafagb@mit.edu</a>\n</span></span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Kevin P. <span>\\sur</span>Greenman\n</span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Pulkit <span>\\sur</span>Agrawal\n</span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Rafael <span>\\sur</span>G\u00f3mez-Bombarelli\n</span><span>\n<span>[\n</span>\n<span>[\n</span>\n<span>[\n</span>\n<span>[\n</span></span></span>\n</p>\n<div>\n<h6>Abstract</h6>\n<p>Discovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (<span>ood</span>) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to <span>ood</span> property prediction, achieving improvements in prediction accuracy. In particular, the True Positive Rate (TPR) of <span>ood</span> classification of materials and molecules improved by 3x and 2.5x, respectively, and precision improved by 2x and 1.5x compared to non-transductive baselines. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.</p>\n</div>\n<div>\n<h6>keywords: </h6><p>machine learning, materials property prediction, extrapolation, out-of-distribution, transduction\n</p></div>\n<section>\n<h2>\n<span>1 </span>Introduction</h2>\n<p>Designing new materials and molecules is essential for the development of new technologies. Traditionally, this design process involves extensive experimental iteration or high-throughput methods to screen databases, which are time-consuming and resource-intensive <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib1\">1</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>]</cite>. As a result, there is increasing interest in applying machine learning (ML) techniques to accelerate the discovery of materials and molecules with desired properties <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib1\">1</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib3\">3</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib5\">5</a>]</cite>.\nThere is particular interest in property values that are outside the known property value distribution as these will most likely lead to discovering new materials that will, in turn, unlock new capabilities and technologies.</p>\n<p>One strategy for finding materials and molecules with desired properties is inverse design through conditional generation where materials with out-of-distribution (<span>ood</span>) property values are unavailable, and the goal is to generate them.<cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib5\">5</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib6\">6</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib7\">7</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib8\">8</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>]</cite>.\nA complementary approach is screening a large database of candidates based on their predicted properties. <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib9\">9</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib10\">10</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib11\">11</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib12\">12</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib13\">13</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib14\">14</a>]</cite>. In this setting, the objective is to identify <span>ood</span> materials and molecules from a set of known candidates with unknown property values.\nHowever, both approaches typically struggle when property values fall outside the training distribution. <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib11\">11</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib13\">13</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib15\">15</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib16\">16</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib17\">17</a>]</cite>. Enhancing extrapolative capabilities in property prediction would improve the screening of large candidate spaces in terms of precision by identifying promising compounds and molecules with exceptional properties. This approach could help guide further synthesis and computational efforts, ultimately advancing materials and molecular design.</p>\n<p>Extrapolation in materials science can refer to both the domain (materials space) and range (property values) of the predictive function. It is often used to refer <span>generalization</span> to unseen classes of materials structures and chemical spaces \u2014 for example, training on metals and predicting ceramics or training on artificial molecules and predicting natural products. Here, we address extrapolation in material property values.</p>\n<p>When <span>ood</span> generalization is defined with respect to the materials space, extrapolation often reduces to interpolation. This occurs because test sets tend to remain within the same distribution as the training data representation space <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib18\">18</a>]</cite>. This includes predictive models employing leave-one-cluster-out extrapolation strategies <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib19\">19</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib20\">20</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib17\">17</a>]</cite>, as well as generative approaches designed to achieve <span>ood</span> generalization to structures with different atomic compositions or larger numbers of atoms <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib21\">21</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib22\">22</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib23\">23</a>]</cite>.</p>\n<p>When <span>ood</span> generalization is defined with respect to the range of the predictive function, classical machine learning models face significant challenges in extrapolating property pred...",
      "url": "https://arxiv.org/html/2502.05970v1"
    },
    {
      "title": "Out-of-distribution generalisation and scaffold splitting in molecular ...",
      "text": "The ability to successfully apply previously acquired knowledge to novel and unfamiliar situations is one of the main hallmarks of successful learning and general intelligence. This capability to effectively **generalise** is amongst the most desirable properties a prediction model (or a mind, for that matter) can have.\n\nIn supervised machine learning, the standard way to evaluate the generalisation power of a prediction model for a given task is to randomly split the whole available data set into two sets \u2013 a training set and a test set . The model is then subsequently trained on the examples in the training set and afterwards its prediction abilities are measured on the untouched examples in the test set via a suitable performance metric.\n\nSince in this scenario the model has never seen any of the examples in during training, its performance on must be indicative of its performance on novel data which it will encounter in the future. Right?\n\nNo.\n\nIn practise, one can regularly observe a situation where a machine learning model which performs well on a randomly selected test set fails spectacularly when confronted with novel data which was collected at a later point in time, by a different lab, in a different environment, or in some other context that differs from the original context in which the initial data set was collected. The reason for this can be found in the **distributional shift** between and which frequently occurs when the data collection context (and thus the data generating process) is altered in some way.\n\nIf the data split for the initial data set into training set and test set is done uniformly at random (as is usual), then both and follow the same distribution. This random uniform data split is very much in accordance with the framework of classical statistical learning theory \\[1\\], where one assumes that a learning model is primarily built to deal with training- and test data examples that have all been sampled independently from the same underlying probability distribution.\n\nUnfortunately, a random uniform data split is rarely a good simulation of practical reality where a newly collected data set which is fed into a machine learning model to obtain predictions almost never follows the data distribution of the data set on which the model was originally trained. This distributional shift between the initial training data set and the newly collected data set normally leads to a substantial drop in performance of the model on compared to its performance on a test set which follows the same distribution as . Thus, splitting the initial data set uniformly at random into a test set and a training set often leads to overoptimistic results when trying to estimate the predictive abilities of a machine learning model in a practical setting.\n\nTo get a more reliable picture of the real-world predictive capabilities of a trained machine learning model one must find a way to model a meaningful distributional shift and build it into the test set . Evaluating the model on can then provide a measure for the **out-of-distribution generalisation abilities** of the model.\n\nMeasuring out-of-distribution generalisation is of particular relevance in the field of **molecular property prediction** where distributional shifts tend to be large and difficult to handle for machine learning models. Different molecular data sets obtained by distinct pharmaceutical companies and research groups often contain compounds from vastly different areas of chemical space that exhibit high structural heterogeneity. An elegant solution for the modelling of such distributional shifts in chemical space is given by the idea of **scaffold splitting**.\n\nThe notion of a (two-dimensional) molecular scaffold is described in the article by Bemis and Murcko \\[2\\]. A molecular scaffold reduces the chemical structure of a compound to its core components, essentially by removing all side chains and only keeping ring systems and parts which link together ring systems. An additional option for making molecular scaffolds even more general is to \u201cforget\u201d the identities of the bonds and atoms by replacing all atoms with carbons and all bonds with single bonds.\n\nBemis-Murcko scaffolds can be automatically generated in RDKit via the following Python code:\n\n```\n# how to extract the Bemis-Murcko scaffold of a molecular compound via RDKit\n# import packages\nfrom rdkit import Chem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n# define compound via its SMILES string\nsmiles = \"CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC\"\n# convert SMILES string to RDKit mol object\nmol = Chem.MolFromSmiles(smiles)\n# create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound\nmol_scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n# make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds\nmol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold)\n# convert the generic scaffold mol object back to a SMILES string format\nsmiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic))\n# display compound and its generic Bemis-Murcko scaffold\ndisplay(mol)\nprint(smiles)\ndisplay(mol_scaffold_generic)\nprint(smiles_scaffold_generic)\n```\n\nIf we now have a molecular data set , we can map each compound in to its respective scaffold. Let us assume that a total number of pairwise distinct scaffolds appear in and that these scaffolds are numbered consecutively from to . We can then define an **equivalence relation** on by calling two compounds equivalent if they share the same scaffold. The associated equivalence classes consist of compound sets whereby a given set contains all compounds in which share the -th scaffold. It is not hard to see that the sets form a partition of the original data set . Without loss of generality, we assume that the equivalence classes are ordered by size in descending order, i.e. we assume that contains at least as many molecules as , and so on.\n\nOne appropriate way to now produce a scaffold split of the molecular data set into a training set and a test set for machine learning is to define as the union of the first (larger) sets and as the union of the last (smaller) sets . Here is a custom index parameter which can be used to control the respective sizes of and ; frequently is chosen such that contains approximately of the examples in .\n\nWhile a scaffold split is certainly not perfect, it is already a lot better than a uniform random split at providing a relevant measure of the practical utility of a molecular property prediction model. It mimics a situation where the training set was sampled from a structurally different area of chemical space than the test set . This creates a distributional shift between and which is comparable to the distributional shifts which are commonly observed in real chemical data sets. Evaluating a molecular machine learning model using a scaffold split rather than a uniform random split thus leads to significantly more robust results.\n\n**References:**\n\n\\[1\\] Poggio, Tomaso, and Christian R. Shelton. \u201cOn the mathematical foundations of learning.\u201d _American Mathematical Society_ 39.1 (2002): 1-49.\n\n\\[2\\] Bemis, Guy W., and Mark A. Murcko. \u201cThe properties of known drugs. 1. Molecular frameworks.\u201d _Journal of medicinal chemistry_ 39.15 (1996): 2887-2893.",
      "url": "https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "DrugOOD: Out-of-Distribution Dataset Curator and Benchmark for AI-Aided Drug Discovery \u2013 a Focus on Affinity Prediction Problems with Noise Annotations",
      "text": "DrugOOD: Out-of-Distribution Dataset Curator and Benchmark for AI-Aided\nDrug Discovery \u2013 a Focus on Affinity Prediction Problems with Noise Annotations\nYuanfeng Ji1,3*, Lu Zhang1,2*, Jiaxiang Wu1\n, Bingzhe Wu1, Lanqing Li1,\nLong-Kai Huang1, Tingyang Xu1, Yu Rong1, Jie Ren1, Ding Xue1, Houtim Lai1,\nWei Liu1, Junzhou Huang1, Shuigeng Zhou2, Ping Luo3, Peilin Zhao1, Yatao Bian1\u2020\n1 Tencent AI Lab, China\n2 Fudan University, China\n3 The University of Hong Kong, China\nu3008013@connect.hku.hk\nAbstract\nAI-aided drug discovery (AIDD) is gaining popularity due to\nits potential to make the search for new pharmaceuticals faster,\nless expensive, and more effective. Despite its extensive use in\nnumerous fields (e.g., ADMET prediction, virtual screening),\nlittle research has been conducted on the out-of-distribution\n(OOD) learning problem with noise. We present DrugOOD,\na systematic OOD dataset curator and benchmark for AIDD.\nParticularly, we focus on the drug-target binding affinity pre\u0002diction problem, which involves both macromolecule (pro\u0002tein target) and small-molecule (drug compound). DrugOOD\noffers an automated dataset curator with user-friendly cus\u0002tomization scripts, rich domain annotations aligned with bio\u0002chemistry knowledge, realistic noise level annotations, and\nrigorous benchmarking of SOTA OOD algorithms, as opposed\nto only providing fixed datasets. Since the molecular data\nis often modeled as irregular graphs using graph neural net\u0002work (GNN) backbones, DrugOOD also serves as a valuable\ntestbed for graph OOD learning problems. Extensive empirical\nstudies have revealed a significant performance gap between\nin-distribution and out-of-distribution experiments, emphasiz\u0002ing the need for the development of more effective schemes\nthat permit OOD generalization under noise for AIDD.\nIntroduction\nTraditional drug discovery procedures are lengthy and ex\u0002pensive. To accelerate the drug development process, drug\u0002makers and investors are turning to artificial intelligence\ntechniques (Muratov et al. 2020) for drug discovery (e.g.,\nADMET prediction (Rong et al. 2020), target identification,\nprotein structure prediction(Shen et al. 2021)), which aim to\nrapidly identify new compounds and model complex mecha\u0002nisms to automate previously manual processes (Schneider\n2018). In this paper, we focus on one of the most challenging\napplications, called drug-target binding affinity prediction,\nwhich aims to identify a subset of compounds with high bind\u0002ing affinity for a given protein target among many candidate\ncompounds.\n*Equal contribution. Order was determined by tossing a coin.\n\u2020Corresponding author: Yatao Bian.\nCopyright \u00a9 2023, Association for the Advancement of Artificial\n(a) Structure-based Affinity Prediction (b) Protein/Compound Distribution\nTest (unseen domain)\nTrain (seen domain)\nAffinity\npred\nperformance\nFigure 1: (a) Structure-based affinity prediction aims to pre\u0002dict binding affinity values between a pair of target (protein)\nand compound (molecule), and (b) model performance is of\u0002ten severely degraded when the data distributions are shifted.\nDistribution shift is a ubiquitous problem in the field of\nAIDD, where the training distribution differs from the test\ndistribution. Typically, the prediction model is trained on\nknown target proteins when conducting virtual screening for\nhit findings. A \u201cblack swan\u201d event, such as COVID-19, may\nnevertheless occur, resulting in a new target with unseen data\ndistributions. Hence, the performance of the unseen target\nwill decline drastically. To address the performance degra\u0002dation (Koh et al. 2021) caused by distribution shift, it is\nnecessary to develop robust and generalizable algorithms\nfor this challenging issue in AIDD. Despite its importance\nin real-world problems, the community still lacks curated\nOOD datasets and benchmarks for inspiring relevant research.\nBesides, label noise is another critical issue. Generally, AI\nmodels are trained using publicly deposited datasets, such\nas ChEMBL, whereas the bioassay data are typically noisy\n(Kramer et al. 2012; Cort\u00e9s-Ciriano and Bender 2016). For in\u0002stance, the activity data from ChEMBL is manually extracted\nfrom the full texts of seven Medicinal Chemistry journals\n(Mendez et al. 2019). Various factors, including but not lim\u0002ited to different confidence levels for activities measured\nthrough experiments, unit-translation errors, repeated cita\u0002tions of single measurements, and different \u201ccut-off\u201d noise1\n,\nIntelligence (www.aaai.org). All rights reserved.\n1E.g., measurements could be recorded with <, \u2264, \u2248, >, \u2265,\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8023\nMolecular Scaffold Domain Molecular Size Domain\nx = \ny = activate \nd = scaffold \n24111 \n\u2026\nx = \ny = inactivate \nd = scaffold \n0 \nx = \ny = activate \nd = size\n228 \n\u2026\nx = \ny = activate \nd = size\n0 \nx = \ny = activate \nd = protein \n126 \n\u2026\nx = \ny = inactivate \nd = protein \n0 \nx = \ny = activate \nd = protein \nfamily 11 \n\u2026\nx = \ny = activate \nd = protein\nfamily 0 \nProtein Domain Protein family Domain\nx = \ny = activate \nd = scaffold \n34806 \n\u2026\nx = \ny = inactivate \nd = scaffold \n24112 \nx = \ny = activate \nd = size \n250 \n\u2026\nx = \ny = inactivate \nd = size\n229 \nx = \ny = activate \nd = protein \n415 \n\u2026\nx = \ny = inactivate \nd = protein \n127 \nx = \ny = inactivate \nd = protein\nfamily 14 \n\u2026\nx = \ny = activate \nd = protein\nfamily 12 \nTrain (seen domain) Test (unseen domain)\nMGAASGQ\nRGRWPLSP\nPLLMLSLLL\nLLLL\u2026\nMQRSPPG\nYGAQDDPP\nSRRDCAW\nAPGI\u2026\nMGSLLALL\nALLLLWGA\nVAEGPAKK\nVLTL\u2026\nMQSKVLLA\nVALWLCVE\nTRAASVGL\nPSV\u2026\nMGRPLHLV\nLLSASLAGL\nLLLGESLFIR\nREQ\u2026\nMHSKVTIIC\nIRFLFWFLL\nLCMLIGKS\nHTE\u2026\nMSTMHLLT\nFALLFSCSF\nARAACDPK\nIVNI\u2026\nMRALWVL\nGLCCVLLTF\nGSVRADDE\nVDV\u2026\nFigure 2: Exemplar curated datasets spanning different domain shifts from DrugOOD. Each data sample (x, y, d) in dataset is\nassociated with a domain annotation d, which corresponds to a distribution P over data points which are similar in some way,\ne.g., molecules with the same scaffold. Specifically, DrugOOD focuses on the problem of domain generalization, in which we\ntrain (seen domain) and test (unseen domain) the model on disjoint domains, e.g., molecules with a new scaffold. Additionally,\nDrugOOD identifies and annotates three noise levels (core, refined, general), whose level increases with data volume and noise\nsources.\ncan cause noise in these data. Figure 2 shows examples with\ndifferent noisy levels. Meanwhile, real-world data with noise\nlevel annotations is lacking for learning tasks under noise\nlabels (Angluin and Laird 1988; Han et al. 2020).\nTo help accelerate research by focusing community at\u0002tention and simplifying systematic comparisons between\ndata collection and implementation method, we present\nDrugOOD, a systematic OOD dataset curator and benchmark\nfor AI-assisted drug discovery that includes an open-source\nPython package which fully automates the data curation and\nOOD benchmarking processes. We focus on the most chal\u0002lenging OOD setting: domain generalization (Zhou et al.\n2021) problem in AI-aided drug discovery, though DrugOOD\ncan be easily adapted to other OOD settings, such as subpopu\u0002lation shift (Koh et al. 2021) and domain adaptation (Zhuang\net al. 2020). Our dataset is also the first AIDD dataset curator\nwith realistic noise level annotations that can serve as an\nimportant testbed for the setting of learning under noise.\nIn contrast to only providing fixed datasets, we present an\nautomated dataset curator based on the large-scale bioassay\ndeposition website ChEMBL (Mendez et al. 2019). Figure 3\nprovides a summary of the automated dataset curator. Using\nthis dataset curator, researchers/practitioners can generate\nnew OOD datasets based on their needs by simply modifying\nthe configuration files in the Python package. Specifically,\nwe also realize this dataset curator by generating 45 OOD\ndatasets spanning various domains, noise levels, and measure\u0002ment types. This mechanism offers two benefits: i) It ensures\naccurate reproduction of our datasets and benchmarks, ii...",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25970/25742"
    },
    {
      "title": "",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2310.12033** (cs)\n\n\\[Submitted on 18 Oct 2023\\]\n\n# Title:Conformal Drug Property Prediction with Density Estimation under Covariate Shift\n\nAuthors: [Siddhartha Laghuvarapu](https://arxiv.org/search/cs?searchtype=author&query=Laghuvarapu,+S), [Zhen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+Z), [Jimeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+J)\n\nView a PDF of the paper titled Conformal Drug Property Prediction with Density Estimation under Covariate Shift, by Siddhartha Laghuvarapu and Zhen Lin and Jimeng Sun\n\n[View PDF](https://arxiv.org/pdf/2310.12033)\n\n> Abstract:In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.\n\n|     |     |\n| --- | --- |\n| Comments: | Accepted at NeurIPS 2023 |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2310.12033](https://arxiv.org/abs/2310.12033) \\[cs.LG\\] |\n|  | (or [arXiv:2310.12033v1](https://arxiv.org/abs/2310.12033v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2310.12033](https://doi.org/10.48550/arXiv.2310.12033)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Siddhartha Laghuvarapu \\[ [view email](https://arxiv.org/show-email/e46b4ff5/2310.12033)\\]\n\n**\\[v1\\]**\nWed, 18 Oct 2023 15:17:10 UTC (1,484 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Conformal Drug Property Prediction with Density Estimation under Covariate Shift, by Siddhartha Laghuvarapu and Zhen Lin and Jimeng Sun\n\n- [View PDF](https://arxiv.org/pdf/2310.12033)\n- [TeX Source](https://arxiv.org/src/2310.12033)\n- [Other Formats](https://arxiv.org/format/2310.12033)\n\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2310.12033&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2310.12033&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-10](https://arxiv.org/list/cs.LG/2023-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2310.12033?context=cs)\n\n[stat](https://arxiv.org/abs/2310.12033?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2310.12033?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.12033)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.12033)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.12033)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2310.12033&description=Conformal Drug Property Prediction with Density Estimation under Covariate Shift) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2310.12033&title=Conformal Drug Property Prediction with Density Estimation under Covariate Shift)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2310.12033) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2310.12033"
    },
    {
      "title": "Upgrading Reliability in Molecular Property Prediction by Robust ...",
      "text": "Upgrading Reliability in Molecular Property Prediction by Robust Quantification of Uncertainty from Machine Learning Models | Journal of Chemical Information and Modeling\nRecently Viewed[**close modal](javascript:void(0))\n* [ACS](http://www.acs.org)\n* [ACS Publications](https://pubs.acs.org/)\n* [C&amp;EN](https://cen.acs.org)\n* [CAS](https://www.cas.org)\n[Access through institution](https://pubs.acs.org/action/ssostart?redirectUri=/doi/10.1021/acs.jcim.5c00464)\n[Log In](https://pubs.acs.org/action/ssoRequestForLoginPage)\n[![ACS Publications. Most Trusted. Most Cited. Most Read](https://pubs.acs.org/pb-assets/ux3/pubs-logo-481x82-1523435513963.png)](https://pubs.acs.org/)\n[![Journal of Chemical Information and Modeling](https://pubs.acs.org/cms/10.1021/jcisd8/asset/16ad6e03-da16-d6e0-7da1-ad6e037da16a/title.png)](https://pubs.acs.org/journal/jcisd8)\nUpgrading Reliability in Molecular Property Prediction by Robust Quantification of Uncertainty from Machine Learning Models\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acs.jcim.5c00464&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464&amp;href=/doi/10.1021/acs.jcim.5c00464)\n* **Share\nShare on\n* **Facebook\n* **X\n* **Wechat\n* **LinkedIn\n* **Reddit\n* **Email\n* **Bluesky\n* **Jump to\n* **ExpandCollapse\n**Back to top\n**Close quick search form\n**clear**search\nJ. Chem. Inf. Model.All Publications/Website\n[Advanced Search](https://pubs.acs.org/search/advanced)\n**Close\nPublications[**](#)\n## CONTENT TYPES\n* All Types\n## SUBJECTS\nPublications: All Types[**](#)\n[Skip to article](#article__left-side)[Skip to sidebar](#article_content-right)\nClose## NextPrevious\n![Figure 1]()![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n[**Download Hi-Res Image](#)[**Download to MS-PowerPoint](#)[****Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acs.jcim.5c00464&amp;href=/doi/10.1021/acs.jcim.5c00464)*J. Chem. Inf. Model.*2025, 65, 20, 10819-10831\n[ADVERTISEMENT](http://acsmediakit.org)\n[![Journal Logo](specs/products/achs/releasedAssets/images/loading/loader.gif)](https://pubs.acs.org/journal/jcisd8)\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acs.jcim.5c00464&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acs.jcim.5c00464&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acs.jcim.5c00464&amp;href=/doi/10.1021/acs.jcim.5c00464)\n* **Share\nShare on\n* **Facebook\n* **X\n* **WeChat\n* **LinkedIn\n* **Reddit\n* **Email\n* **Bluesky\n* **Jump to\n* **ExpandCollapse\nMachine Learning and Deep LearningOctober 8, 2025\n# Upgrading Reliability in Molecular Property Prediction by Robust Quantification of Uncertainty from Machine Learning Models\n**Click to copy article linkArticle link copied!\n* Alex K\u00f6tter**\\***\nAlex K\u00f6tter\nDigital R&amp;D Large Molecule Research, Sanofi-Aventis Deutschland GmbH, 65926 Frankfurt am Main, Germany\n**\\***Email:[alexander.koetter@sanofi.com](mailto:alexander.koetter@sanofi.com)\nMore by[Alex K\u00f6tter](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Alex%20K%C3%B6tter)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0003-2977-6729](https://orcid.org/0000-0003-2977-6729)\n* Kanishka Singh\nKanishka Singh\nSynthetic Molecular Design, Integrated Drug Discovery, Sanofi-Aventis Deutschland GmbH, 65926 Frankfurt am Main, Germany\nMore by[Kanishka Singh](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Kanishka%20Singh)\n* Hans Matter\nHans Matter\nSynthetic Molecular Design, Integrated Drug Discovery, Sanofi-Aventis Deutschland GmbH, 65926 Frankfurt am Main, Germany\nMore by[Hans Matter](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Hans%20Matter)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-0249-6025](https://orcid.org/0000-0002-0249-6025)\n* Gerhard Hessler\nGerhard Hessler\nSynthetic Molecular Design, Integrated Drug Discovery, Sanofi-Aventis Deutschland GmbH, 65926 Frankfurt am Main, Germany\nMore by[Gerhard Hessler](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Gerhard%20Hessler)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0001-5602-0965](https://orcid.org/0000-0001-5602-0965)\n* Christoph Grebner\nChristoph Grebner\nSynthetic Molecular Design, Integrated Drug Discovery, Sanofi-Aventis Deutschland GmbH, 65926 Frankfurt am Main, Germany\nMore by[Christoph Grebner](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Christoph%20Grebner)\n[Other Access Options](#)[**Supporting Information (1)](#_i19)\n## Journal of Chemical Information and Modeling\nCite this:*J. Chem. Inf. Model.*2025, 65, 20, 10819\u201310831\n**Click to copy citationCitation copied!\n[https://pubs.acs.org/doi/10.1021/acs.jcim.5c00464](https://pubs.acs.org/doi/10.1021/acs.jcim.5c00464)\n[https://doi.org/10.1021/acs.jcim.5c00464](https://doi.org/10.1021/acs.jcim.5c00464)\nPublishedOctober 8, 2025\n[****](#)\n### Publication History\n* **\nReceived\n4 March 2025\n* **\nAccepted\n23 September 2025\n* **\nRevised\n19 September 2025\n* **\nPublished\nonline8 October 2025\n* **\nPublished\nin issue27 October 2025\nresearch-article\nCopyright \u00a92025 The Authors. Published by American Chemical Society\n[Request reuse permissions](https://pubs.acs.org/servlet/linkout?type=rightslink&amp;url=startPage=10819&pageCount=13&copyright=American+Chemical+Society&author=Alex+K%C3%B6tter%2C+Kanishka+Singh%2C+Hans+Matter%2C+et+al&orderBeanReset=true&imprint=American+Chemical+Society&volumeNum=65&issueNum=20&contentID=acs.jcim.5c00464&title=Upgrading+Reliability+in+Molecular+Property+Prediction+by+Robust+Quantification+of+Uncertainty+from+Machine+Learning+Models&numPages=13&pa=&issn=1549-9596&publisherName=acs&publication=jcisd8&rpt=n&endPage=10831&publicationDate=October+2025)\n## Abstract\n**Click to copy section linkSection link copied!\n![Abstract Image](https://pubs.acs.org/cms/10.1021/acs.jcim.5c00464/asset/images/medium/ci5c00464_0005.gif)\nReliable methods to quantify the predictive uncertainty of machine learning (ML) models can significantly increase the impact of molecular property prediction and are routinely used in applications like active learning and ML-guided property optimization. Poor predictive accuracy of ML models is often related to (i) regions of the chemical space, which are characterized by large property differences for structurally similar molecules, and (ii) a lack of representation of test molecules in the training data. Here, we analyze the relationship between these error sources and the predictive uncertainty of popular uncertainty quantification (UQ) methods on molecular activity data sets. We find that seve...",
      "url": "https://pubs.acs.org/doi/10.1021/acs.jcim.5c00464"
    }
  ]
}