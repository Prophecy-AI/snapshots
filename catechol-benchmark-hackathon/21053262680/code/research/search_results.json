{
  "query": "How do you handle distribution shift when predicting molecular properties for unseen chemical compounds in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Handling distribution shift in molecular\u2011property prediction requires a workflow that (i) detects out\u2011of\u2011distribution (OOD) compounds, (ii) adapts the model or data to reduce the shift, and (iii) makes conservative predictions when uncertainty is high. The steps below combine the most recent strategies reported in the literature\u202f([Nature\u202f2025](https://www.nature.com/articles/s41524-025-01808-x),\u202f[Blopig\u202f2021](https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction),\u202f[ArXiv\u202f2025](https://arxiv.org/abs/2506.11877)).\n\n1. **Assess the type of shift** \u2013 Determine whether the new compounds differ mainly in scaffold (core structure) or in overall chemical similarity. Use scaffold splitting to simulate scaffold\u2011level shift and clustering (e.g., K\u2011means on ECFP4 fingerprints) to simulate covariate shift\u202f([Blopig\u202f2021](https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction);\u202f[ChemRxiv\u202f2025](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  \n\n2. **Create OOD validation sets** \u2013 Split the training data using the identified strategy (scaffold split for mild shift, similarity\u2011cluster split for severe shift). Evaluate model performance on these OOD splits to quantify the baseline degradation\u202f([ChemRxiv\u202f2025](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  \n\n3. **Integrate heterogeneous data** \u2013 Augment the training set with external, possibly noisy, datasets and enforce consistency across overlapping molecules (e.g., multi\u2011task learning or data\u2011integration pipelines). This reduces distributional misalignment before model training\u202f([J.\u202fParrondo\u2011Pizarro\u202f2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC12574261)).  \n\n4. **Apply a transductive or meta\u2011learning adaptation** \u2013  \n   a. **Transductive OOD prediction**: During inference, jointly embed the unseen compounds with the training set and fine\u2011tune the predictor on the combined graph (the transductive approach improves accuracy on OOD molecules)\u202f([Nature\u202f2025](https://www.nature.com/articles/s41524-025-01808-x)).  \n   b. **Meta\u2011learning with unlabeled data**: Use a meta\u2011learner that interpolates between ID and OOD distributions by leveraging large pools of unlabeled molecules; the learner acquires a \u201chow\u2011to\u2011generalize\u201d update that mitigates covariate shift\u202f([ArXiv\u202f2025](https://arxiv.org/abs/2506.11877)).  \n\n5. **Detect OOD inputs at inference time** \u2013 Compute a distance metric (e.g., Mahalanobis distance in the latent space or similarity to training scaffolds). Flag compounds whose distance exceeds a calibrated threshold as OOD\u202f([Wu\u202f2021](https://ml.cmu.edu/research/phd-dissertation-pdfs/yw4_phd_ml_2021.pdf)).  \n\n6. **Adopt conservative prediction strategies** \u2013 For flagged OOD compounds, either (i) output higher uncertainty estimates (e.g., Bayesian GNN or ensemble variance) or (ii) defer to a fallback model trained on broader chemical space. This prevents over\u2011confident errors\u202f([Wu\u202f2021](https://ml.cmu.edu/research/phd-dissertation-pdfs/yw4_phd_ml_2021.pdf)).  \n\n7. **Iterate with active learning** \u2013 Select the most uncertain OOD compounds, obtain experimental labels, and add them to the training set. Re\u2011train or fine\u2011tune the model to progressively shrink the shift.  \n\nFollowing these steps\u2014characterizing the shift, validating on realistic OOD splits, enriching data, applying transductive/meta\u2011learning adaptation, detecting OOD inputs, and using conservative predictions\u2014provides a robust pipeline for molecular\u2011property prediction on unseen chemical compounds.",
      "url": ""
    },
    {
      "title": "Out-of-Distribution Property Prediction in Materials and ...",
      "text": "Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules | npj Computational Materials\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![npj Computational Materials](https://media.springernature.com/full/nature-cms/uploads/product/npjcompumats/header-4baba14304e9c3518bdc0d6f35b470b9.svg)](https://www.nature.com/npjcompumats)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41524-025-01808-x?error=cookies_not_supported&code=700d2411-cae5-4b77-8d63-7e62cff2a842)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41524)\n* [RSS feed](https://www.nature.com/npjcompumats.rss)\nKnown Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:20 November 2025# Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n* [Nofit Segal](#auth-Nofit-Segal-Aff1)[1](#Aff1)[na1](#na1),\n* [Aviv Netanyahu](#auth-Aviv-Netanyahu-Aff2)[2](#Aff2)[na1](#na1),\n* [Kevin P. Greenman](#auth-Kevin_P_-Greenman-Aff3-Aff4)[3](#Aff3),[4](#Aff4),\n* [Pulkit Agrawal](#auth-Pulkit-Agrawal-Aff2)[2](#Aff2)&amp;\n* \u2026* [Rafael G\u00f3mez-Bombarelli](#auth-Rafael-G_mez_Bombarelli-Aff1)[1](#Aff1)Show authors\n[*npj Computational Materials*](https://www.nature.com/npjcompumats)**volume11**, Article\u00a0number:345(2025)[Cite this article](#citeas)\n* 3094Accesses\n* 1Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41524-025-01808-x/metrics)\n### Subjects\n* [Chemistry](https://www.nature.com/subjects/chemistry)\n* [Materials science](https://www.nature.com/subjects/materials-science)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n## Abstract\nDiscovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (OOD) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to OOD property prediction, achieving improvements in prediction accuracy. In particular, our method improves extrapolative precision by 1.8\u00d7 for materials and 1.5\u00d7 for molecules, and boosts recall of high-performing candidates by up to 3\u00d7. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-66685-w/MediaObjects/41467_2025_66685_Fig1_HTML.png)\n### [Molecular Motif Learning as a pretraining objective for molecular property prediction](https://www.nature.com/articles/s41467-025-66685-w?fromPaywallRec=false)\nArticleOpen access27 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-53751-y/MediaObjects/41467_2024_53751_Fig1_HTML.png)\n### [MolE: a foundation model for molecular graphs using disentangled attention](https://www.nature.com/articles/s41467-024-53751-y?fromPaywallRec=false)\nArticleOpen access12 November 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n## Introduction\nDesigning new materials and molecules is essential for the development of new technologies. Traditionally, this design process involves extensive experimental iteration or high-throughput methods to screen databases, which are time-consuming and resource-intensive[1](https://www.nature.com/articles/s41524-025-01808-x#ref-CR1),[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2). As a result, there is increasing interest in applying machine learning (ML) techniques to accelerate the discovery of materials and molecules with desired properties[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s41524-025-01808-x#ref-CR7). There is particular interest in property values that are outside the known property value distribution, as these will most likely lead to discovering new materials that will, in turn, unlock new capabilities and technologies.\nOne strategy for discovering materials and molecules with desired properties is inverse design via conditional generation, where the goal is to create candidates with out-of-distribution (OOD) property values that are absent from the training data[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2),[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[5](https://www.nature.com/articles/s41524-025-01808-x#ref-CR5),[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41524-025-01808-x#ref-CR10). A complementary strategy is virtual screening, where a large database of candidate materials is evaluated using predicted properties[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](https://www.nature.com/articles/s41524-025-01808-x#ref-CR16). In this case, the objective is to identify high-performing OOD candidates from a set of known compositions with unknown properties. Screening often involves applying a threshold, either an absolute value or a percentile cutoff, and selecting candidates whose predicted properties exceed it[17](#ref-CR17),[18](#ref-CR18),[19](https://www.nature.com/articles/s41524-025-01808-x#ref-CR19). However, both generative and screening approaches commonly face challenges when the target property values lie outside the distribution of the training data[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[13](https://www.nature.com/articles/s41524-025-01808-x#ref-CR13),[15](https://www.nature.com/articles/s41524-025-01808-x#ref-CR15),[20](#ref-CR20),[21](#ref-CR21),[22](https://www.nature.com/articles/s41524-025-01808-x#ref-CR22). Enhancing extrapolative capabilities in property prediction would improve the screening of large candidate spaces in terms of precision by identifying promising compounds and molecules with exceptional properties. This has the potential to streamline the identification process by reducing time and resource expenditure on low-potential candidates, thereby accelerating the discovery of materials and molecules with high synthesis viability.\nExtrapolation in materials science can refer to both the domain and the range of the predictive function. Typically, extrapolation is used to refer to*generalization*in the domain space, i.e., unseen classes of materials, structures, and chemical spaces, e.g., training on metals and predicting ceramics, or training on ar...",
      "url": "https://www.nature.com/articles/s41524-025-01808-x"
    },
    {
      "title": "Learning to Predict and Make Decisions under Distribution Shift",
      "text": "Learning to Predict and Make Decisions\nunder Distribution Shift\nYifan Wu\nSeptember 2021\nCMU-ML-21-110\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\nThesis Committee:\nZachary Lipton, Chair\nAndrej Risteski\nSivaraman Balakrishnan\nAlexander Smola (Amazon)\nSubmitted in partial fulfillment of the requirements\nfor the degree of Doctor of Philosophy.\nCopyright \rc 2021 Yifan Wu\nThis research was supported by the Department of the Interior award D17AP00001 and grants from Pricewater\u0002houseCoopers, the University of California, and UPMC Enterprises.\nKeywords: Distribution Shift, Domain Adaptation, Label Shift, PU Learning, Reinforce\u0002ment Learning, Bandit\nAbstract\nA common use case of machine learning in real world settings is to learn a model\nfrom historical data and then deploy the model on future unseen examples. When\nthe data distribution for the future examples differs from the historical data distribu\u0002tion, machine learning techniques that depend precariously on the i.i.d. assumption\ntend to fail. So dealing with distribution shift is an important challenge when de\u0002veloping machine learning techniques for practical use. While it is unrealistic to\nexpect a learned model to predict accurately under any form of distribution shift,\nwell chosen research objectives may still lead to effective machine learning algo\u0002rithms that handle distribution shift properly. For example, when facing distribution\nshift, we may expect to build machine learning models that (i) make accurate predic\u0002tions under specific assumptions on how the distribution shift happens; (ii) identify\nout-of-distribution inputs where the model may not be able to predict well; and/or\n(iii) act conservatively according to what it can predict well. While recent research\nhas produced practically successful methods in machine learning settings with in\u0002dependent and identically distributed data, progress on settings where dealing with\ndistribution shift is necessary has remained in a comparatively developmental stage.\nIn this thesis, we study the problem of learning under distribution shift in two\nscenarios: prediction and decision making. The first part of the thesis addresses the\nprediction problem, focusing on exploiting specific assumptions such as covariate\nshift and label shift. We develop theoretical understanding and effective techniques\nin these scenarios. In the second part of this thesis, we study the problem of offline\npolicy optimization, where the goal is to learn a good policy from a fixed set of data,\nwhose distribution may not be rich enough to inform the optimal policy. We first\npresent an extensive empirical study on behavior regularized offline reinforcement\nlearning algorithms. We then present a theoretical study on whether/why one should\nfollow the pessimistic principle in the offline policy optimization problem.\niv\n\nAcknowledgments\nI would like to thank my advisor Zachary Lipton for agreeing to be my advi\u0002sor and providing all kinds of support during my PhD study. I would like to thank\nmy Master\u2019s degree advisors Csaba Szepesvari and Andr \u00b4 as Gy \u00b4 orgy for getting me \u00a8\nprepared for research before I started my PhD. I would like to thank Ofir Nachum,\nGeorge Tucker, Yanqi Zhou, and Hanxiao Liu for hosting my internships at Google.\nI would like to thank Chenjun Xiao, Saurabh Garg, and Ruosong Wang for the re\u0002search collaboration. I would like to thank Andrej Risteski, Sivaraman Balakrishnan,\nand Alex Smola for being on my thesis committee and providing insightful feedback\nfor my thesis proposal and dissertation. I would like to thank Diane Stidle for taking\ncare of the Machine Learning Department so that I never worry about any admin\u0002istrative process in the PhD program. I would also like to thank everyone I have\ninteracted with during my PhD, who has made my life as a PhD student better in\nvarious ways. Last but most importantly, I would like to thank my family for their\nlove and support.\nvi\n\nContents\n1 Introduction 1\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Outline and Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2.1 Part I Unsupervised Domain Adaptation . . . . . . . . . . . . . . . . . . 2\n1.2.2 Part II Offline Policy Optimization . . . . . . . . . . . . . . . . . . . . . 3\nI Unsupervised Domain Adaptation 7\n2 Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment 9\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.3 A Motivating Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4 Bounding the Target Domain Error . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.1 A general bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.2 Example of a perfect target domain classifier . . . . . . . . . . . . . . . 16\n2.5 Asymmetrically-relaxed distances . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.1 f-divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.2 Wasserstein distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.3 Reweighting distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.6.1 Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.7 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.9 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3 A Unified View of Label Shift Estimation 31\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.4 A Unified View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.4.1 A Unified Distribution Matching View . . . . . . . . . . . . . . . . . . . 33\n3.4.2 The Confusion Matrix Approach . . . . . . . . . . . . . . . . . . . . . . 34\n3.4.3 Maximum Likelihood Label Shift Estimation . . . . . . . . . . . . . . . 35\nvii\n3.4.4 MLLS with Confusion Matrix . . . . . . . . . . . . . . . . . . . . . . . 36\n3.5 Finite-Sample Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.8 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.8.1 Proof of Theorem 3.5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4 Learning from Positive and Unlabeled Data 49\n4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.3 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.4 Mixture Proportion Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n4.5 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.6 Combining MPE and Classification . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.7 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.9 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\nII Offline Policy Optimization 61\n5 An Empirical Study on Behavior Regularized Offline Reinforcement Learning 63\n5.1 Overview...",
      "url": "https://ml.cmu.edu/research/phd-dissertation-pdfs/yw4_phd_ml_2021.pdf"
    },
    {
      "title": "Enhancing molecular property prediction through data ...",
      "text": "[Skip to main content](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#main-content)\n\n**Official websites use .gov**\nA\n**.gov** website belongs to an official\ngovernment organization in the United States.\n\n**Secure .gov websites use HTTPS**\nA **lock** (\n\nLocked padlock icon\n) or **https://** means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n\nSearch PMC Full-Text ArchiveSearch in PMC\n\n- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n\n- ## PERMALINK\n\n\n\nCopy\n\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:\n[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nJ Cheminform\n\n. 2025 Oct 29;17:163. doi: [10.1186/s13321-025-01103-3](https://doi.org/10.1186/s13321-025-01103-3)\n\n# Enhancing molecular property prediction through data integration and consistency assessment\n\n[Raquel Parrondo-Pizarro](https://pubmed.ncbi.nlm.nih.gov/?term=%22Parrondo-Pizarro%20R%22%5BAuthor%5D)\n\n### Raquel Parrondo-Pizarro\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\nFind articles by [Raquel Parrondo-Pizarro](https://pubmed.ncbi.nlm.nih.gov/?term=%22Parrondo-Pizarro%20R%22%5BAuthor%5D)\n\n1,2, [Luca Menestrina](https://pubmed.ncbi.nlm.nih.gov/?term=%22Menestrina%20L%22%5BAuthor%5D)\n\n### Luca Menestrina\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Luca Menestrina](https://pubmed.ncbi.nlm.nih.gov/?term=%22Menestrina%20L%22%5BAuthor%5D)\n\n1, [Ricard Garcia-Serna](https://pubmed.ncbi.nlm.nih.gov/?term=%22Garcia-Serna%20R%22%5BAuthor%5D)\n\n### Ricard Garcia-Serna\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Ricard Garcia-Serna](https://pubmed.ncbi.nlm.nih.gov/?term=%22Garcia-Serna%20R%22%5BAuthor%5D)\n\n1, [Adri\u00e0 Fern\u00e1ndez-Torras](https://pubmed.ncbi.nlm.nih.gov/?term=%22Fern%C3%A1ndez-Torras%20A%22%5BAuthor%5D)\n\n### Adri\u00e0 Fern\u00e1ndez-Torras\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Adri\u00e0 Fern\u00e1ndez-Torras](https://pubmed.ncbi.nlm.nih.gov/?term=%22Fern%C3%A1ndez-Torras%20A%22%5BAuthor%5D)\n\n1,\u2709, [Jordi Mestres](https://pubmed.ncbi.nlm.nih.gov/?term=%22Mestres%20J%22%5BAuthor%5D)\n\n### Jordi Mestres\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\nFind articles by [Jordi Mestres](https://pubmed.ncbi.nlm.nih.gov/?term=%22Mestres%20J%22%5BAuthor%5D)\n\n1,2,\u2709\n\n- Author information\n- Article notes\n- Copyright and License information\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\n\u2709\n\nCorresponding author.\n\nReceived 2025 Jun 5; Accepted 2025 Sep 21; Collection date 2025.\n\n\u00a9 The Author(s) 2025\n\n**Open Access** This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by-nc-nd/4.0/](https://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nPMCID: PMC12574261\u00a0\u00a0PMID: [41163219](https://pubmed.ncbi.nlm.nih.gov/41163219/)\n\n## Abstract\n\nData heterogeneity and distributional misalignments pose critical challenges for machine learning models, often compromising predictive accuracy. These challenges are exemplified in preclinical safety modeling, a crucial step in early-stage drug discovery where limited data and experimental constraints exacerbate integration issues. Analyzing public ADME datasets, we uncovered significant misalignments as well as inconsistent property annotations between gold-standard and popular benchmark sources, such as Therapeutic Data Commons. These dataset discrepancies, which can arise from differences in various factors, including experimental conditions in data collection as well as chemical space coverage, can introduce noise and ultimately degrade model performance. Data standardization, despite harmonizing discrepancies and increasing the training set size, may not always lead to an improvement in predictive performance. This highlights the importance of rigorous data consistency assessment (DCA) prior to modeling. To facilitate a systematic DCA across diverse datasets, we developed AssayInspector, a model-agnostic package that leverages statistics, visualizations, and diagnostic summaries to identify outliers, batch effects, and discrepancies. Beyond preclinical safety, DCA can play a crucial role in federated learning scenarios, enabling effective transfer learning across heterogeneous data sources and supporting reliable integration across diverse scientific domains.\n\n### Supplementary Information\n\nThe online version contains supplementary material available at 10.1186/s13321-025-01103-3.\n\n**Keywords:** Data reporting, Molecular property, ADME, Physicochemical, Machine learning, Data aggregation, Predictive accuracy, Benchmark\n\n## Scientific contribution\n\nBy systematically analyzing public ADME datasets, we uncovered substantial distributional misalignments and annotation discrepancies between benchmark and gold-standard sources. These challenges were shown to undermine predictive modeling, as naive integration or standardization often degraded performance. To address them, we present AssayInspector, a tool that enables both consistency assessment and informed data integration, providing a foundation for more reliable predictive modelling in drug discovery.\n\n### Graphical Abstract\n\n### Supplementary Information\n\nThe online version contains supplementary material available at 10.1186/s13321-025-01103-3.\n\n## Introduction\n\nMachine learning (ML) has emerged as a powerful tool for scientific discovery, providing cost-effective predictions that accelerate research across various domains \\[ [1](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR1)\u2013 [3](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR3)\\]. Yet, the accuracy and reliability of ML models depend on the quality, size, and consistency of training data \\[ [4](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR4), [5](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR5)\\]. In this context, integrating publicly available datasets offers an opportunity to increase sample sizes and expand feature space ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12574261"
    },
    {
      "title": "Out-of-distribution generalisation and scaffold splitting in ...",
      "text": "The ability to successfully apply previously acquired knowledge to novel and unfamiliar situations is one of the main hallmarks of successful learning and general intelligence. This capability to effectively **generalise** is amongst the most desirable properties a prediction model (or a mind, for that matter) can have.\n\nIn supervised machine learning, the standard way to evaluate the generalisation power of a prediction model for a given task is to randomly split the whole available data set into two sets \u2013 a training set and a test set . The model is then subsequently trained on the examples in the training set and afterwards its prediction abilities are measured on the untouched examples in the test set via a suitable performance metric.\n\nSince in this scenario the model has never seen any of the examples in during training, its performance on must be indicative of its performance on novel data which it will encounter in the future. Right?\n\nNo.\n\nIn practise, one can regularly observe a situation where a machine learning model which performs well on a randomly selected test set fails spectacularly when confronted with novel data which was collected at a later point in time, by a different lab, in a different environment, or in some other context that differs from the original context in which the initial data set was collected. The reason for this can be found in the **distributional shift** between and which frequently occurs when the data collection context (and thus the data generating process) is altered in some way.\n\nIf the data split for the initial data set into training set and test set is done uniformly at random (as is usual), then both and follow the same distribution. This random uniform data split is very much in accordance with the framework of classical statistical learning theory \\[1\\], where one assumes that a learning model is primarily built to deal with training- and test data examples that have all been sampled independently from the same underlying probability distribution.\n\nUnfortunately, a random uniform data split is rarely a good simulation of practical reality where a newly collected data set which is fed into a machine learning model to obtain predictions almost never follows the data distribution of the data set on which the model was originally trained. This distributional shift between the initial training data set and the newly collected data set normally leads to a substantial drop in performance of the model on compared to its performance on a test set which follows the same distribution as . Thus, splitting the initial data set uniformly at random into a test set and a training set often leads to overoptimistic results when trying to estimate the predictive abilities of a machine learning model in a practical setting.\n\nTo get a more reliable picture of the real-world predictive capabilities of a trained machine learning model one must find a way to model a meaningful distributional shift and build it into the test set . Evaluating the model on can then provide a measure for the **out-of-distribution generalisation abilities** of the model.\n\nMeasuring out-of-distribution generalisation is of particular relevance in the field of **molecular property prediction** where distributional shifts tend to be large and difficult to handle for machine learning models. Different molecular data sets obtained by distinct pharmaceutical companies and research groups often contain compounds from vastly different areas of chemical space that exhibit high structural heterogeneity. An elegant solution for the modelling of such distributional shifts in chemical space is given by the idea of **scaffold splitting**.\n\nThe notion of a (two-dimensional) molecular scaffold is described in the article by Bemis and Murcko \\[2\\]. A molecular scaffold reduces the chemical structure of a compound to its core components, essentially by removing all side chains and only keeping ring systems and parts which link together ring systems. An additional option for making molecular scaffolds even more general is to \u201cforget\u201d the identities of the bonds and atoms by replacing all atoms with carbons and all bonds with single bonds.\n\nBemis-Murcko scaffolds can be automatically generated in RDKit via the following Python code:\n\n```\n# how to extract the Bemis-Murcko scaffold of a molecular compound via RDKit\n# import packages\nfrom rdkit import Chem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n# define compound via its SMILES string\nsmiles = \"CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC\"\n# convert SMILES string to RDKit mol object\nmol = Chem.MolFromSmiles(smiles)\n# create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound\nmol_scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n# make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds\nmol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold)\n# convert the generic scaffold mol object back to a SMILES string format\nsmiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic))\n# display compound and its generic Bemis-Murcko scaffold\ndisplay(mol)\nprint(smiles)\ndisplay(mol_scaffold_generic)\nprint(smiles_scaffold_generic)\n```\n\nIf we now have a molecular data set , we can map each compound in to its respective scaffold. Let us assume that a total number of pairwise distinct scaffolds appear in and that these scaffolds are numbered consecutively from to . We can then define an **equivalence relation** on by calling two compounds equivalent if they share the same scaffold. The associated equivalence classes consist of compound sets whereby a given set contains all compounds in which share the -th scaffold. It is not hard to see that the sets form a partition of the original data set . Without loss of generality, we assume that the equivalence classes are ordered by size in descending order, i.e. we assume that contains at least as many molecules as , and so on.\n\nOne appropriate way to now produce a scaffold split of the molecular data set into a training set and a test set for machine learning is to define as the union of the first (larger) sets and as the union of the last (smaller) sets . Here is a custom index parameter which can be used to control the respective sizes of and ; frequently is chosen such that contains approximately of the examples in .\n\nWhile a scaffold split is certainly not perfect, it is already a lot better than a uniform random split at providing a relevant measure of the practical utility of a molecular property prediction model. It mimics a situation where the training set was sampled from a structurally different area of chemical space than the test set . This creates a distributional shift between and which is comparable to the distributional shifts which are commonly observed in real chemical data sets. Evaluating a molecular machine learning model using a scaffold split rather than a uniform random split thus leads to significantly more robust results.\n\n**References:**\n\n\\[1\\] Poggio, Tomaso, and Christian R. Shelton. \u201cOn the mathematical foundations of learning.\u201d _American Mathematical Society_ 39.1 (2002): 1-49.\n\n\\[2\\] Bemis, Guy W., and Mark A. Murcko. \u201cThe properties of known drugs. 1. Molecular frameworks.\u201d _Journal of medicinal chemistry_ 39.15 (1996): 2887-2893.",
      "url": "https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction"
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Few-shot Molecular Property Prediction: A Survey",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2510.08900v1"
    },
    {
      "title": "Real-World Molecular Out-Of-Distribution: Specification and Investigation",
      "text": "We use cookies to distinguish you from other users and to provide you with a better experience on our websites.Close this message to accept cookies or find out how to manage your cookie settings. [Learn more about our Privacy Notice...\\\n\\[opens in a new tab\\]](https://www.cambridge.org/about-us/legal-notices/privacy-notice/)\n\n[Back to\\\nTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\n\nSearch within Theoretical and Computational Chemistry\n\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n\n# Real-World Molecular Out-Of-Distribution: Specification and Investigation\n\n05 June 2023, Version 1\n\nThis is not the most recent version. There is a [newer version](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932) of this content available\n\nWorking Paper\n\n## Authors\n\n- [Prudencio Tossou](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Prudencio%20Tossou),\n- [Cas Wognum](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Cas%20Wognum)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0009-0006-2742-4817),\n- [Michael Craig](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Michael%20Craig),\n- [Hadrien Mary](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Hadrien%20Mary),\n- [Emmanuel Noutahi](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Emmanuel%20Noutahi)\n\n[Show author details](https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1)\n\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nThis study presents a rigorous framework for investigating Molecular Out-Of-Distribution (MOOD) generalization in drug discovery. The concept of MOOD is first clarified through a problem specification that demonstrates how the covariate shifts encountered during real-world deployment can be characterized by the distribution of sample distances to the training set. We find that these shifts can cause performance to drop by up to 60% and uncertainty calibration by up to 40%. This leads us to propose a splitting protocol that aims to close the gap between deployment and testing. Then, using this protocol, a thorough investigation is conducted to assess the impact of model design, model selection and dataset characteristics on MOOD performance and uncertainty calibration. We find that appropriate representations and algorithms with built-in uncertainty estimation are crucial to improve performance and uncertainty calibration. This study sets itself apart by its exhaustiveness and opens an exciting avenue to benchmark meaningful, algorithmic progress in molecular scoring. All related code can be found on Github at https://github.com/cwognum/mood-experiments.\n\n## Keywords\n\n[Molecular Scoring](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20Scoring)\n\n[Out-of-Distribution](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Out-of-Distribution)\n\n[Applicability Domain](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Applicability%20Domain)\n\n[Drug Discovery](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Drug%20Discovery)\n\n## Supplementary materials\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\n\nMOOD: Supplementary Material\n\n**Description**\n\nProvides a variety of additional figures to support the results from the main text.\n\n**Actions**\n\n**Download**\n**(3 MB)**\n\n## Supplementary weblinks\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\n\nMOOD: Code base\n\n**Description**\n\nA Github repository with all the code that was used for the results in the MOOD paper.\n\n**Actions**\n\n[**View**](https://github.com/cwognum/mood-experiments)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\n[Jul 26, 2023 Version\\\n2](https://chemrxiv.org/engage/chemrxiv/article-details/64c012a1b053dad33ae21932)\n\nJun 05, 2023 Version 1\n\n## Metrics\n\n3,843\n\n1,822\n\n0\n\nViews\n\nDownloads\n\nCitations\n\n## License\n\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\n\nCC\n\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\n\nBY\n\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\n\nNC\n\nThe content is available under\n[CC BY NC 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by-nc/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2023-q11q4\\\nD O I: 10.26434/chemrxiv-2023-q11q4 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2023-q11q4)\n\n## Funding\n\n**Mitacs**\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/647a2fdbbe16ad5c575c7cb1"
    }
  ]
}