{
  "query": "What transfer learning and active learning approaches work best for predicting chemical reaction yields for unseen solvents in few-shot settings?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting reaction yields when a new solvent is introduced is a classic few\u2011shot problem: only a handful of experiments are available for the unseen solvent, yet the model must extrapolate from prior chemistry.  Recent work shows that the most reliable strategy is to **pair chemically\u2011aware transfer learning with uncertainty\u2011driven active learning**.  First, a model is pre\u2011trained on a modest, mechanistically\u2011related reaction set (e.g., Diels\u2013Alder, Cope, Claisen, Ene, or Nazarov reactions) rather than on a very large, unrelated corpus such as USPTO\u2011MIT; this \u201cchemically aware\u201d pre\u2011training yields the greatest data\u2011efficiency when the downstream task involves the same reaction class and solvent space\u202f([RSC](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00412d)).  After transfer, the model is fine\u2011tuned on a few solvent\u2011specific experiments.  Simple, shallow ensembles of decision trees (random\u2011forest classifiers with limited depth) have been found to retain interpretability and to generalise well in this active\u2011transfer setting\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577)).\n\nFor the active\u2011learning component, **uncertainty\u2011sampling (query\u2011by\u2011uncertainty) and output\u2011margin querying** consistently outperform random selection, reaching a target AUROC with far fewer solvent experiments.  In high\u2011throughput coupling datasets, uncertainty\u2011based selection of the most informative solvent\u2013catalyst combinations reduced the required data to <\u202f5\u202f% of the full space while still achieving high yield prediction accuracy\u202f([Nature](https://www.nature.com/articles/s42004-025-01434-0); [Chalmers](https://research.chalmers.se/en/publication/531294)).  Implementations that combine DFT\u2011derived solvent descriptors (e.g., solvent polarity, LUMO energy of the radical intermediate) with difference\u2011Morgan fingerprints in a random\u2011forest model further improve performance on unseen solvents, as demonstrated for Ni\u2011photoredox cross\u2011electrophile coupling\u202f([UCLA](https://doyle.chem.ucla.edu/wp-content/uploads/2025/05/souza-et-al-2025-applying-active-learning-toward-building-a-generalizable-model-for-ni-photoredox-cross-electrophile.pdf)).  The Catechol benchmark provides a dedicated few\u2011shot solvent\u2011selection dataset that can be used to validate these approaches\u202f([arXiv](https://arxiv.org/abs/2506.07619)).\n\nIn practice, the best pipeline therefore consists of: (1) **pre\u2011training a graph\u2011 or fingerprint\u2011based model on a small, mechanistically\u2011matched reaction set**, (2) **fine\u2011tuning with a few experimentally measured yields for the new solvent**, and (3) **iteratively selecting additional solvent experiments via uncertainty\u2011 or margin\u2011based active learning**.  This combination leverages prior chemical knowledge, keeps the model simple enough to avoid over\u2011fitting, and maximises information gain from each new solvent measurement, delivering accurate yield predictions even in the extreme few\u2011shot regime.",
      "url": ""
    },
    {
      "title": "Predicting reaction conditions from limited data through active ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Transfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.</p></section><section><hr/>\n<p>Transfer learning is combined with active learning to discover synthetic reaction conditions in a small-data regime. This strategy is tested on cross-coupling reactions from a high-throughput experimentation dataset and shows promising results.<a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-ga.jpg\"></a></p></section><section><h2>Introduction</h2>\n<p>Computers are becoming increasingly capable of performing high-level chemical tasks.<sup><a href=\"#cit1\">1\u20134</a></sup> Machine learning approaches have demonstrated viable retrosynthetic analyses,<sup><a href=\"#cit5\">5\u20137</a></sup> product prediction,<sup><a href=\"#cit8\">8\u201311</a></sup> reaction condition suggestion,<sup><a href=\"#cit12\">12\u201316</a></sup> prediction of stereoselectivity,<sup><a href=\"#cit17\">17\u201320</a></sup> regioselectivity,<sup><a href=\"#cit19\">19,21\u201324</a></sup> and reaction yield<sup><a href=\"#cit25\">25,26</a></sup> and optimization of reaction conditions.<sup><a href=\"#cit27\">27\u201330</a></sup> These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (<em>e.g.</em> reaction conditions) within a particular domain relate to an outcome (<em>e.g.</em> yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.</p>\n<p>Expert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain,<sup><a href=\"#cit31\">31</a></sup> emulates a tactic commonly employed by human chemists.</p>\n<p>Transfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain.<sup><a href=\"#cit31\">31,32</a></sup> Models are first created using the source data, then transferred to the target domain using various algorithms.<sup><a href=\"#cit19\">19,33\u201335</a></sup> For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (<em>e.g.</em>, descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible.<sup><a href=\"#cit31\">31,36</a></sup> Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data.<sup><a href=\"#cit37\">37</a></sup> The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.</p>\n<p>Another machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data.<sup><a href=\"#cit38\">38,39</a></sup> Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental results. Based on this analogy, reaction optimization<sup><a href=\"#cit27\">27,28</a></sup> and reaction condition identification<sup><a href=\"#cit40\">40</a></sup> have been demonstrated to benefit from active learning. However, these prior works initiate exploration with randomly selected data points (<a href=\"#fig1\">Fig. 1A</a>) which does not leverage prior knowledge, and therefore does not reflect how expert chemists initiate exploration. Initial search directed by transfer learning could identify productive regions early on, which in turn will help build more useful models for subsequent active learning steps.</p>\n<figure><h3>Fig. 1. Workflow of (A) previous active learning studies and (B) this work. Distinctions that arise from the different problem setting and incorporation of transfer learning are highlighted bold in (B).</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-f1.jpg\"></a></p>\n</figure><p>To align transfer and active learning closer to how expert chemists develop new reactions, appropriate chemical reaction data is necessary.<sup><a href=\"#cit41\">41</a></sup> Available datasets<sup><a href=\"#cit42\">42</a></sup> that are often used for machine learning are overrepresented by positive reactions, failing to reflect reactions with negative outcomes. On the other hand, reaction condition screening data of methodology reports\u2014which chemists often refer to\u2014only constitute a sparse subset of possible reagent combinations, making it hard for machine learning algorithms to extract meaningful knowledge.<sup><a href=\"#cit43\">43</a></sup></p>\n<p>High-throughput experimentation<sup><a href=\"#cit44\">44\u201346</a></sup> (HTE) data can fill this gap. HTE provides reaction data<sup><a href=\"#cit16\">16,25,27,47,48</a></sup> with reduced variations in outcome due to systematic experimentation. Pd-catalyzed coupling data was therefore collected from reported work using nanomole scale HTE in 1536 well plates.<sup><a href=\"#cit49\">49\u201351</a></sup> In the current work, subsets of this data, classified by nucleophile type as shown in <a href=\"#fig2\">Fig. 2A</a>, were selected to a dataset size of approximately 100 datapoints, which captured both positive and negative reaction performance.</p>\n<figure><h3>Fig. 2. (A) Structure of reactions in the dataset. A total of 1220 reactions across 10 t...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577"
    },
    {
      "title": "An active representation learning method for reaction yield ... - Nature",
      "text": "<div><div>\n \n <div><h2>Introduction</h2><div><p>The optimization of chemical reaction is a fundamental task that has numerous important applications in synthetic chemistry<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR1\">1</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR2\">2</a></sup>. For example, we often need to adopt some new structures to investigate the classic reactions, such as Buchwald\u2013Hartwig coupling, for discovering novel molecules with new functions<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR3\">3</a></sup>. But the real performance caused by those new structures are usually unreported and also not easy to predict. In traditional optimization process, chemists usually need to consider a \u201creaction space\u201d which contains a set of reaction combinations with several critical conditions, such as different catalysts, ligands, additives, solvents and other components (Fig.\u00a0<a href=\"https://www.nature.com/articles/s42004-025-01434-0#Fig1\">1A</a>). In the well-known Suzuki coupling dataset provided by Pfizer\u2019s group<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR4\">4</a></sup>, for example, the size of the reaction space is 5760\u2009=\u200915\u2009\u00d7\u200912\u2009\u00d7\u20098\u2009\u00d7\u20094. As an important optimization objective, reaction yield has been widely studied for evaluating experimental performance<sup><a href=\"#ref-CR5\">5</a>,<a href=\"#ref-CR6\">6</a>,<a href=\"#ref-CR7\">7</a>,<a href=\"#ref-CR8\">8</a>,<a href=\"#ref-CR9\">9</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR10\">10</a></sup>, since it can reflect the quality of a reaction and reveal the underlying principles in chemistry. Therefore, when studying a new reaction system, it is urgently needed to understand the patterns of reaction yield and explore high-yield reaction combinations. However, this process often needs to take a large amount of experimental time, and moreover, the performance heavily relies on the expertise of the experimenter. As a consequence, some potentially viable reaction conditions are very likely to be overlooked. For example, Buchwald\u2019s group recommended only a limited range of conditions on Buchwald\u2013Hartwig coupling in their early research<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR11\">11</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR12\">12</a></sup>, but they further discovered a series of important new combinations in the following study<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR13\">13</a></sup>.</p><div><figure><figcaption><b>Fig. 1: Reaction yield prediction framework for small-scale data.</b></figcaption><div><div><a href=\"https://www.nature.com/articles/s42004-025-01434-0/figures/1\"></a></div><p><b>A</b> Traditional yield optimization. <b>B</b> The overall framework of our model with RS-coreset. Our yield prediction result can be achieved via an iterative procedure, where each iteration includes 3 steps. Step 1 (\u201c<i>yield evaluation</i>\u201d), evaluate the yields of selected reactions by chemists. Step 2 (\u201c<i>representation learning</i>\u201d), update the representation learning model with the newly added experimental data. Step 3 (\u201c<i>data selection</i>\u201d), select the most informative reactions in the reaction space guided by our coreset method. After several iterations, the representation for reaction space becomes stable and then we can build the final yield prediction model upon it.</p></div><p><a href=\"https://www.nature.com/articles/s42004-025-01434-0/figures/1\"><span>Full size image</span></a></p></figure></div><p>The emerging high-throughput experimentation (HTE) technology, which is able to run a large number of reactions in parallel<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR14\">14</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR15\">15</a></sup>, has attracted a lot of attention for accelerating the traditional reaction optimization process<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR16\">16</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR17\">17</a></sup>. HTE can significantly promote the experimental efficiency and reduce the workload for exploring a new reaction system. The favorable parallelization of HTE can also generate a sufficiently large amount of experimental data to support reaction optimization, and thus greatly decreases the chance of missing high-yield reaction combinations.</p><p>To effectively utilize the experimental data generated from HTE, several different machine learning (ML) based methodologies have been proposed. Machine learning is an active research subarea of Artificial Intelligence<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR18\">18</a></sup>. In general, the goal of machine learning is to learn some effective model from observed data, where the model can be applied to solve various tasks, e.g., classification and regression. In particular, machine learning techniques have been successfully applied to different fields of scientific research<sup><a href=\"#ref-CR19\">19</a>,<a href=\"#ref-CR20\">20</a>,<a href=\"#ref-CR21\">21</a>,<a href=\"#ref-CR22\">22</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR23\">23</a></sup>. For example, we can exploit machine learning-assisted methods to address a variety of chemical tasks such as molecular design<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR24\">24</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR25\">25</a></sup>, reaction prediction<sup><a href=\"#ref-CR26\">26</a>,<a href=\"#ref-CR27\">27</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR28\">28</a></sup>, retrosynthetic analyses<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR29\">29</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR30\">30</a></sup>, reaction condition optimization<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR31\">31</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR32\">32</a></sup>, selectivity prediction<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR33\">33</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR34\">34</a></sup>, etc. In the area of yield prediction, Doyle\u2019s group provided an open-source chemical reaction optimization tool based on the Bayesian method, and they validated their approach through the experiments involving Palladium-catalysed C-N coupling and C\u2013H functionalization system, Mitsunobu reaction and deoxyfluorination reaction of alcohols<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR6\">6</a></sup>. Recently, Denmark\u2019s group designed a machine learning tool to predict substrate-adaptive conditions for Palladium-catalyzed C\u2013N couplings, where the tool can be used to optimize reaction combinations within a large reaction space of 450,000 possible reactions<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR17\">17</a></sup>.</p><p>Most of the current studies on reaction yield prediction rely on large amounts of data provided by HTE equipment. HTE, while powerful, its high cost makes it unaffordable for most research laboratories in the world. Consequently, the limited data and budget unfortunately hinder many chemists from leveraging ML methods to predict reaction yield or guide reaction screening. Therefore, our major task in this paper is to design a method that can provide guidance for yield prediction and optimization with small-scale experimental data. Through this method, we aim to interpret the entire reaction space and then achieve an overview of the yields across the space, which could help us to discover more potential reaction pathways that might otherwise be overlooked.</p><p>Our idea is to design some effective sampling strategy to approximate the reaction space. Moreo...",
      "url": "https://www.nature.com/articles/s42004-025-01434-0"
    },
    {
      "title": "Improving reaction prediction through chemically aware transfer ...",
      "text": "<div><div><p><span></span></p><div><div><p> DOI:\u00a0<a href=\"https://doi.org/10.1039/D4DD00412D\">10.1039/D4DD00412D</a>\n(Paper)\n<span><a href=\"https://doi.org/10.1039/2635-098X/2022\">Digital Discovery</a></span>, 2025, Advance Article </p></div><p><span>Received \n31st December 2024\n</span><span>, Accepted 26th March 2025</span></p><p>First published on 28th March 2025</p><hr/><div><h2>Abstract</h2><p>Practical applications of machine learning (ML) to new chemical domains are often hindered by data scarcity. Here we show how data gaps can be circumvented by means of transfer learning that leverages chemically relevant pre-training data. Case studies are presented in which the outcomes of two classes of pericyclic reactions are predicted: [3,3] rearrangements (Cope and Claisen rearrangements) and [4 + 2] cycloadditions (Diels\u2013Alder reactions). Using the graph-based generative algorithm NERF, we evaluate the data efficiencies achieved with different starting models that we pre-trained on datasets of different sizes and chemical scope. We show that the greatest data efficiency is obtained when the pre-training is performed on smaller datasets of mechanistically related reactions (Diels\u2013Alder, Cope and Claisen, Ene, and Nazarov) rather than &gt;50\u00d7 larger datasets of mechanistically unrelated reactions (USPTO-MIT). These small bespoke datasets were more efficient in both low re-training and low pre-training regimes, and are thus recommended alternatives to large diverse datasets for pre-training ML models.</p></div><hr/>\n<h2><span>Introduction</span></h2>\n<p><span>One of the most important bottlenecks for applications of machine learning (ML) in chemistry is the lack of access to reaction data. Even for widely used reactions such as amide, Suzuki, and SNAr reactions,<a href=\"#cit1\"><sup><span>1</span></sup></a> reaction datasets can be considered small by machine learning (ML) standards<a href=\"#cit2\"><sup><span>2,3</span></sup></a> or in comparison to datasets of molecules and their properties.<a href=\"#cit4\"><sup><span>4\u20137</span></sup></a> Even with popular reaction datasets such as USPTO,<a href=\"#cit8\"><sup><span>8</span></sup></a> Pistachio,<a href=\"#cit9\"><sup><span>9</span></sup></a> and Reaxys,<a href=\"#cit10\"><sup><span>10</span></sup></a> data filtering is required and for the latter two, commercial restrictions apply. The problem of data scarcity is especially acute for novel reactions, where only limited (small and/or homogeneous) reaction data are available for use in the training of predictive models. Experimentally generating datasets of significant size and diversity is a non-trivial task. We have recently shown that new generative ML algorithms featuring built-in \u201cchemical-awareness\u201d can efficiently predict chemical reactivity in low-data regimes.<a href=\"#cit11\"><sup><span>11</span></sup></a> Alternative, computational, strategies include data augmentation<a href=\"#cit12\"><sup><span>12\u201314</span></sup></a> and transfer learning.<a href=\"#cit15\"><sup><span>15\u201319</span></sup></a></span></p><p>Transfer learning<a href=\"#cit20\"><sup><span>20,21</span></sup></a> involves retraining an existing ML model on a new domain of chemistry (<a href=\"#imgfig1\">Fig. 1A</a>) and in many cases improves model accuracy while reducing training costs by not necessitating a brand new model. It requires two related datasets: one pre-training dataset that is used to train an initial model and another to re-train (fine-tune) the model on the target reactions/domain. In theory, the shared principles between these two domains can be learned during pre-training and leveraged during fine-tuning to produce a more effective model than training alone. However, the ideal relationship between the pre-training and re-training datasets in reaction prediction is not clear: Should the emphasis be on the size of the pre-training dataset, on molecular structure or on similarity of the reaction mechanisms? Chemical intuition would posit the mechanism, specifically the electron flow, contains the most applicable information but this requires a model that properly encodes this information. In contrast, the data hungry nature of neural networks would suggest that a significantly larger (one or more orders of magnitude) and more diverse dataset be more effective.</p>\n<br/><div><table><tbody><tr><td><a href=\"https://pubs.rsc.org/image/article/2025/DD/d4dd00412d/d4dd00412d-f1_hi-res.gif\"></a></td></tr><tr><td> </td><td><b>Fig. 1 </b> <span><span>(A) General transfer learning workflow. (B) Pericyclic reactions and their mechanistic similarities.</span></span></td><td> </td></tr></tbody></table></div>\n<p>We investigate the following question: in situations where data are scarce, do models pre-trained on mechanistically related reactions require less data than models trained on diverse reaction data? We address this question through studies of two target pericyclic reactions: [3,3] rearrangements (Cope<a href=\"#cit22\"><sup><span>22,23</span></sup></a> and Claisen<a href=\"#cit24\"><sup><span>24,25</span></sup></a> rearrangements) and [4 + 2] cycloadditions (Diels\u2013Alder<a href=\"#cit26\"><sup><span>26,27</span></sup></a> reactions) (<a href=\"#imgfig1\">Fig. 1B</a>). These reactions were chosen not just for their synthetic utility of atom-economy efficient transformations,<a href=\"#cit28\"><sup><span>28\u201331</span></sup></a> but crucially because they share a common mechanistic feature: the shuffling of electrons around a six-membered cyclic transition state. These reactions are compared and pre-trained with datasets from the Ene reaction, which shares the cyclic movement of six electrons, and the Nazarov cyclization, a 4-electron electrocyclic reaction. Our work examines whether ML models can recognize these shared mechanistic principles, in this case when predicting the major product of these reactions.</p>\n<h2><span>Methods</span></h2>\n<h3><span>Dataset curation</span></h3>\n<p><span>Pericyclic reaction datasets were generated by Reaxys<a href=\"#cit10\"><sup><span>10</span></sup></a> database searches and were curated using workflows that filtered based on atom-economy, bonding patterns, atom-mapping, and reaction templates (see ESI for more details<a href=\"#fn1\">\u2020</a>). The two target reaction datasets were: 3289 Cope and Claisen (CC) rearrangements and 9537 Diels\u2013Alder (DA) reactions.<a href=\"#cit11\"><sup><span>11</span></sup></a></span></p><p>For transfer learning, we also generated pre-training datasets representing different sizes and chemistry: (1) 80%-of-\u223c480000 diverse reactions from the USPTO-MIT database,<a href=\"#cit8\"><sup><span>8,32</span></sup></a> (2) 80%-of-9537 Diels\u2013Alder reactions (DA1), (3) 40%-of-9537 Diels\u2013Alder reactions (DA2), (4) 80%-of-3289 Cope and Claisen rearrangements, (5) 80%-of-2322 Ene reactions, (6) 80%-of-1029 Nazarov cyclizations where the reactant and product were represented as their charge-neutral forms (Naz1), (7) 80%-of-1029 Nazarov cyclizations with the reactant and product represented as their protonated forms (Naz2). The Jupyter notebooks to regenerate these datasets using a Reaxys license are available as described in the ESI.<a href=\"#fn1\">\u2020</a></p>\n<h3><span>Machine learning architectures</span></h3>\n<p><span>Machine learning models were trained with:</span></p><p>(1) NERF (non-autoregressive electron redistribution framework) algorithm.<a href=\"#cit33\"><sup><span>33</span></sup></a> NERF predicts the changes in edges of a molecular graph (corresponding to the changes in bond order that define a chemical reaction) using connectivity and nodes characterised by atom type, aromaticity, charge, and positional and segment embeddings. Its design principles<a href=\"#cit33\"><sup><span>33</span></sup></a> and performance<a href=\"#cit11\"><sup><span>11</span></sup></a> have been previously documented.</p>\n<p>(2) Chemformer,<a href=\"#cit17\"><sup><span>17</span></sup></a> a natural language processing (NLP) model built on the Bidirectional Auto-Regressive Transform...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00412d"
    },
    {
      "title": "[PDF] Applying Active Learning toward Building a Generalizable Model for ...",
      "text": "Applying Active Learning toward Building a Generalizable Model for\nNi-Photoredox Cross-Electrophile Coupling of Aryl and Alkyl\nBromides\nLucas W. Souza, Nathan D. Ricke,\u00a7 Braden C. Chaffin,\u00a7 Mike E. Fortunato, Shutian Jiang, Cihan Soylu,\nThomas C. Caya, Sii Hong Lau, Katherine A. Wieser, Abigail G. Doyle,* and Kian L. Tan*\nCite This: https://doi.org/10.1021/jacs.5c02218 Read Online\nACCESS Metrics & More Article Recommendations *s\u0131 Supporting Information\nABSTRACT: When developing machine learning models for yield\nprediction, the two main challenges are effectively exploring condition\nspace and substrate space. In this article, we disclose an approach for\nmapping the substrate space for Ni/photoredox-catalyzed cross\u0002electrophile coupling of alkyl bromides and aryl bromides in a high\u0002throughput experimentation (HTE) context. This model employs active\nlearning (in particular, uncertainty querying) as a strategy to rapidly\nconstruct a yield model. Given the vastness of substrate space, we\nfocused on an approach that builds an initial model and then uses a\nminimal data set to expand into new chemical spaces. In particular, we\nbuilt a model for a virtual space of 22,240 compounds using less than\n400 data points. We demonstrated that the model can be expanded to 33,312 compounds by adding information around 24 building\nblocks (<100 additional reactions). Comparing the active learning-based model to one constructed on randomly selected data\nshowed that the active learning model was significantly better at predicting which reactions will be successful. A combination of\ndensity function theory (DFT) and difference Morgan fingerprints was employed to construct the random forest model. Feature\nimportance analysis indicates that key DFT features that are related to the reaction mechanism (e.g., alkyl radical LUMO energy)\nwere crucial for model performance and predictions on aryl bromides outside the training set. We anticipate that combining DFT\nfeaturization and uncertainty-based querying will help the synthetic organic community build predictive models in a data-efficient\nmanner for other chemical reactions that feature large and diverse scopes.\n\u25a0 INTRODUCTION\nSupervised machine learning (ML) has emerged as a powerful\ntool for applications in synthetic organic chemistry. In recent\ndecades, numerous papers have explored retrosynthesis and\nforward synthesis prediction of complex reaction parameters\nsuch as stereoselectivity, reaction yield, and reaction\nconditions.1,2 While significant advances have been made in\nthe area of retrosynthesis, generalized forward prediction of\nyield and reaction success remains a significant challenge for\nML.3\nThe early perceptions that big data coupled with machine\nlearning would solve the yield prediction challenge have fallen\nshort of expectations. Attempts at leveraging large publicly\navailable databases such as literature data for the prediction of\nreaction yields have led to poor model performance (Figure\n1).4,5 While these data sets are large, they introduce a number\nof confounding variables, often resulting in low-quality\nmodels.6\u22128 Another large source of chemical data has been\nsourced from pharmaceutical companies via electronic\nlaboratory notebooks (ELN). These data, however, have\nbeen found to suffer from many of the same issues as public\ndatabases. Mainly, inaccurately reported yields and large\nvariations in reaction setup introduce confounding variables.9\nOne area of success for machine learning in organic\nchemistry has been using high-throughput experimentation\n(HTE) to generate de novo data sets. HTE data sets have been\nused to build accurate reaction yield models for Suzuki\u2212\nMiyaura cross-coupling reactions, Buchwald\u2212Hartwig amina\u0002tions,9\u221213 and many other reaction classes.13\u221223 However, due\nto the relatively small substrate scope of these works, models\nthat can generalize to unseen substrates have been challenging\nto identify. Recently, the Coley lab, in collaboration with\nAbbVie, published an exhaustive study utilizing an HTE data\nset for Suzuki reactions spanning over a decade of internal\nelectronic lab notebook (ELN) data from a select few number\nReceived: February 5, 2025\nRevised: April 24, 2025\nAccepted: April 28, 2025\npubs.acs.org/JACS Article\n\u00a9 XXXX American Chemical Society A\nhttps://doi.org/10.1021/jacs.5c02218\nJ. Am. Chem. Soc. XXXX, XXX, XXX\u2212XXX\nDownloaded via UNIV OF CALIFORNIA LOS ANGELES on May 22, 2025 at 16:26:38 (UTC).\nSee https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles.\nof chemists.2,24 While this work showed encouraging levels of\ngeneralizability, the rare and highly curated nature of the\ndataset represents a significant hurdle to access by the broader\nchemistry community. Additionally, the authors note that\nchanging sets of reaction conditions inherent to data spanning\nsuch a long period of time proved challenging and likely\nimpacted the degree of generalization observed.17 These\npublications highlight the potential of HTE data to facilitate\nyield prediction models of coupling reactions but also\ndemonstrate the challenge of modeling the vastness of\nsubstrate and/or reaction condition space.\nFigure 1. Strategy for the active learning generalization study of Ni/photoredox cross-electrophile coupling.\nFigure 2. (A) Reaction conditions; (B) aryl bromide selections. Note: two molecules are chosen from cluster 5, (C) UMAP of aryl bromide space. a\nReaction conditions; aryl bromide (0.033 mmol, 1 equiv), aryl bromide (0.049 mmol 1.5 equiv), NiCl2(dtbpy) (2.5 mol %),\nIr[df(CF3)ppy](dtbbpy)PF6 (2.5 mol %) 1,1,1,3,3,3-hexamethyl-2-(trimethylsilyl)trisilane (TTMSS, 1.2 equiv 0.039 mmoi), 2,4,6-\ntrimethylpyridine (3 equiv, 0.098 mmol), dimethylacetimide (DMA, 43.6 \u03bcL), dimethoxyethane (DME, 174.2 \u03bcL), compact fluorescent lamp\n(CFL).\nJournal of the American Chemical Society pubs.acs.org/JACS Article\nhttps://doi.org/10.1021/jacs.5c02218\nJ. Am. Chem. Soc. XXXX, XXX, XXX\u2212XXX\nB\nThe enormity of substrate space likely makes it expensive in\nterms of time and money to build fully generalized models.\nTherefore, we have been rethinking how to more efficiently\nacquire experimental data to train ML models for reaction\nprediction, as well as reconsidering how to approach the\nproblem of generalization. Traditionally, we would design a set\nof high-throughput experiments to learn substrate reaction\nmapping in a static space.25\u221228 Instead, in this study, we\nconsider substrate space as dynamic, in which we need to\nconstantly expand the model to learn new spaces that are of\ninterest.13 From this perspective, the approach to data\ncollection and model generation becomes the focal point\nrather than the model itself.\nIn this paper, we investigate the use of active learning (in\nparticular, uncertainty querying) to map out substrate space\n(Figure 1). Active learning is a branch of machine learning that\nuses an algorithm in concert with a human user to choose the\nnext most informative experiments to improve model perform\u0002ance. In particular, active learning has been shown to be an\neffective strategy for building a model with a limited amount of\ndata.29 This is similar to Bayesian optimization for reaction\ncondition optimization,30,31 where instead of optimizing for\nyield, model performance is the objective. Our approach\nfocuses on building an initial model for a reaction class and\nthen using a minimal set of experiments to extend the model to\na new desired chemical space.\nTo explore this approach, we focused our efforts on nickel\nphotoredox cross-electrophile coupling reactions of aryl and\nalkyl bromides (Figure 2A).32 This was a particularly appealing\nreaction class for the outlined approach because, being a\nrelatively new reaction methodology, literature data sets are\nsparser compared to data on other cross-coupling reactions.\nMoreover, the rapid adoption of C(sp2\n)\u2212C(sp3) coupling\nreactions in the pharmaceutical industry (especially within\nlibrary groups) demonstrates the high value...",
      "url": "https://doyle.chem.ucla.edu/wp-content/uploads/2025/05/souza-et-al-2025-applying-active-learning-toward-building-a-generalizable-model-for-ni-photoredox-cross-electrophile.pdf"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "Using Active Learning to Develop Machine Learning Models for Reaction Yield Prediction",
      "text": "Computer aided synthesis planning, suggesting synthetic routes for molecules of interest, is a rapidly growing field. The machine learning methods used are often dependent on access to large datasets for training, but finite experimental budgets limit how much data can be obtained from experiments. This suggests the use of schemes for data collection such as active learning, which identifies the data points of highest impact for model accuracy, and which has been used in recent studies with success. However, little has been done to explore the robustness of the methods predicting reaction yield when used together with active learning to reduce the amount of experimental data needed for training. This study aims to investigate the influence of machine learning algorithms and the number of initial data points on reaction yield prediction for two public high-throughput experimentation datasets. Our results show that active learning based on output margin reached a pre-defined AUROC faster than random sampling on both datasets. Analysis of feature importance of the trained machine learning models suggests active learning had a larger influence on the model accuracy when only a few features were important for the model prediction.\n\nReaction Yield Prediction\n\nBayesian Matrix Factorization\n\nRandom Forest\n\nActive Learning\n\nNeural Networks\n\n## Author\n\n### Simon Johansson\n\nChalmers, Computer Science and Engineering (Chalmers), Data Science and AI\n\nAstraZeneca AB\n\n### Hampus Gummesson Svensson\n\nAstraZeneca AB\n\nChalmers, Computer Science and Engineering (Chalmers), Data Science and AI\n\n### Esben Jannik Bjerrum\n\nAstraZeneca AB\n\n### Alexander Schliep\n\nUniversity of Gothenburg\n\n### Morteza Haghir Chehreghani\n\nChalmers, Computer Science and Engineering (Chalmers), Data Science and AI\n\n### Christian Tyrchan\n\nAstraZeneca AB\n\n### Ola Engkvist\n\nChalmers, Computer Science and Engineering (Chalmers)\n\nAstraZeneca AB\n\n#### Molecular Informatics\n\n1868-1743 (ISSN) 1868-1751 (eISSN)\n\nVol. 41 12\n2200043\n\n### Subject Categories (SSIF 2011)\n\nLanguage Technology (Computational Linguistics)\n\nBioinformatics (Computational Biology)\n\nBioinformatics and Systems Biology\n\n### DOI\n\n10.1002/minf.202200043\n\n## More information",
      "url": "https://research.chalmers.se/en/publication/531294"
    },
    {
      "title": "A transfer learning approach for reaction discovery in small data ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/iscience/vol/25/issue/7\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><div><h2>Highlights</h2><div><ul><li><span>\u2022</span><span><p>Dual pronged transfer learning, both to generate and predict yields of new molecules</p></span></li><li><span>\u2022</span><span><p>Demonstrated the utility for an important family of deoxyfluorination of alcohols</p></span></li><li><span>\u2022</span><span><p>Applicable for practically more likely situations with relatively smaller data</p></span></li><li><span>\u2022</span><span><p>Extendable to other reaction manifolds to facilitate expedited reaction discovery</p></span></li></ul></div></div><div><h2>Summary</h2><div><p>Sustainable practices in chemical sciences can be better realized by adopting interdisciplinary approaches that combine the advantages of machine learning (ML) on the initially acquired small data in reaction discovery. Developing new reactions generally remains heuristic and even time and resource intensive. For instance, synthesis of fluorine-containing compounds, which constitute \u223c20% of the marketed drugs, relies on deoxyfluorination of abundantly available alcohols. Herein, we demonstrate the use of a recurrent neural network-based deep generative model built on a library of just 37 alcohols for effective learning and exploration of the chemical space. The proof-of-concept ML model is able to generate good quality, synthetically accessible, higher-yielding novel alcohol molecules. This protocol would have superior utility for deployment into a practical reaction discovery pipeline.</p></div></div></div><ul><li></li><li></li></ul><div><h2>Subject areas</h2><p><span>Artificial intelligence</span></p><p><span>Computational chemistry</span></p><p><span>Functional group chemistry</span></p><p><span>Modeling chemical reactivity</span></p></div><section><h2>Data and code availability</h2><div><ul><li><span></span><span></span></li><li><span></span><span></span></li><li><span></span><span><p>Any additional information required to reanalyze the data reported in this paper is available from the <a href=\"#sec5.2.1\"><span><span>lead contact</span></span></a> upon request.</p></span></li></ul></div></section><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2022 The Authors.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2589004222009336"
    },
    {
      "title": "[PDF] Prediction of Chemical Reaction Yields using Deep Learning",
      "text": "Prediction of Chemical Reaction Yields using Deep\nLearning\nPhilippe Schwaller\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nE-mail: phs@zurich.ibm.com\nAlain C. Vaucher, Teodoro Laino\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nJean-Louis Reymond\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nAbstract. Artificial intelligence is driving one of the most important revolutions\nin organic chemistry. Multiple platforms, including tools for reaction prediction and\nsynthesis planning based on machine learning, successfully became part of the organic\nchemists\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike\nreaction prediction and retrosynthetic models, the prediction of reaction yields has\nreceived less attention in spite of the enormous potential of accurately predicting\nreaction conversion rates. Reaction yields models, describing the percentage of\nthe reactants converted to the desired products, could guide chemists and help\nthem select high-yielding reactions and score synthesis routes, reducing the number\nof attempts. So far, yield predictions have been predominantly performed for\nhigh-throughput experiments using a categorical (one-hot) encoding of reactants,\nconcatenated molecular fingerprints, or computed chemical descriptors. Here, we\nextend the application of natural language processing architectures to predict reaction\nproperties given a text-based representation of the reaction, using an encoder\ntransformer model combined with a regression layer. We demonstrate outstanding\nprediction performance on two high-throughput experiment reactions sets. An analysis\nof the yields reported in the open-source USPTO data set shows that their distribution\ndi\u21b5ers depending on the mass scale, limiting the dataset applicability in reaction yields\npredictions.\n1. Introduction\nChemical reactions in organic chemistry are described by writing the structural\nformula of reactants and products separated by an arrow, representing the chemical\ntransformation by specifying how the atoms rearrange between one or several reactant\nPrediction of Chemical Reaction Yields using Deep Learning 2\nmolecules and one or several product molecules [1]. Economic, logistic, and energetic\nconsiderations drive chemists to prefer chemical transformations capable of converting\nall reactant molecules into products with the highest yield possible. However, side\u0002reactions, degradation of reactants, reagents or products in the course of the reaction,\nequilibrium processes with incomplete conversion to a product, or simply by product\nisolation and purification undermine the quantitative conversion of reactants into\nproducts, rarely reaching optimal performance.\nReaction yields are usually reported as a percentage of the theoretical chemical\nconversion, i.e., the percentage of the reactant molecules successfully converted to the\ndesired product compared to the theoretical value. It is not uncommon for chemists\nto synthesise a molecule in a dozen or more reaction steps. Hence, low-yield reactions\nmay have a disastrous e\u21b5ect on the overall route yield because of the individual steps\u2019\nmultiplicative e\u21b5ect. Therefore, it is not surprising that designing new reactions with\nyields higher than existing ones attracts much e\u21b5ort in organic chemistry research.\nIn practice, specific chemical reaction classes are characterised by lower or higher\nyields, with the actual value depending on the reaction conditions (temperature,\nconcentrations, etc.) and on the specific substrates.\nEstimating the reaction yield can be a game-changing asset for synthesis planning.\nIt provides chemists with the ability to evaluate the overall yield of complex reaction\npaths, addressing possible shortcomings well ahead of investing hours and materials in\nwet-lab experiments. Computational models predicting reaction yields could support\nsynthetic chemists in choosing an appropriate synthesis route among many predicted\nby data-driven algorithms. Moreover, reaction yields prediction models could also be\nemployed as scoring functions in computer-assisted retrosynthesis route planning tools\n[2, 3, 4, 5], to complement forward prediction models [6, 4] and in-scope filters [2].\nMost of the existing e\u21b5orts in constructing models for the prediction of reactivity or\nof reaction yields focused on a particular reaction class: oxidative dehydrogenations of\nethylbenzene with tin oxide catalysts [7], reactions of vanadium selenites [8], Buchwald\u2013\nHartwig aminations [9, 10, 11], and Suzuki\u2013Miyaura cross-coupling reactions [12, 13, 14].\nTo the best of our knowledge, there was only one attempt to design a general-purpose\nprediction model for reactivity and yields, without applicability constraints to a specific\nreaction class [15]. In this work, the authors design a model predicting whether the\nreaction yield is above or below a threshold value and conclude that the models and\ndescriptors they consider cannot deliver satisfactory results.\nHere, we build on our legacy of treating organic chemistry as a language to introduce\na new model that predicts reaction yields starting from reaction SMILES [16]. More\nspecifically, we fine-tune the rxnfp models by Schwaller et al. [17] based on a BERT\u0002encoder [18] by extending it with a regression layer to predict reaction yields. BERT\nencoders belong to the transformer model family, which has revolutionised natural\nlanguage processing [19, 18]. These models take sequences of tokens as input to compute\ncontextualised representations of all the input tokens, and can be applied to reactions\nrepresented in the SMILES [20] format. In this work, we demonstrate for the first\nPrediction of Chemical Reaction Yields using Deep Learning 3\ntime, that these natural language architectures are very useful not only when working\nwith language tokens, but also to provide descriptors of high quality to predict reaction\nproperties such as reaction yields.\nIt is possible to train our approach both on data specific to a given reaction class\nor on data representing di\u21b5erent reaction types. Thus, we initially trained the model on\ntwo high-throughput experimentation (HTE) data sets. Among the few HTE reaction\ndata sets published in recent years, we selected the data sets for palladium-catalysed\nBuchwald\u2013Hartwig reactions provided by Ahneman et al. [9] and for Suzuki\u2013Miyaura\ncoupling reactions provided by Perera et al. [21]. Finally, we trained our model on\npatent data available in the USPTO data set [22, 23].\nHTE and Patent data sets are very di\u21b5erent in terms of content and quality. HTE\ndata sets typically cover a very narrow region in the chemical reaction space, with\nchemical reaction data related to one or a few reaction templates applied to large\ncombinations of selected precursors (reactants, solvents, bases, catalysts, etc.). In\ncontrast, patent reactions cover a much wider reaction space. In terms of quality, HTE\ndata sets report reactions represented uniformly and with yields measured using the\nsame analytical equipment, thus providing a consistent and high quality collection of\nknowledge. In comparison, the yields from patents were measured by di\u21b5erent scientists\nusing di\u21b5erent equipments. Incomplete information in the original documents, such\nas unreported reagents or reaction conditions, and the extensive limitation in text\nmining technologies makes the entire set of patent reactions quite noisy and sparse.\nAn extensive analysis of the USPTO data set revealed that the experimental conditions\nand reaction parameters, such as scale of the reaction, concentrations, temperature,\npressure, or reaction duration, may have a significant e\u21b5ect on the measured reaction\nyields. The functional dependency of the yields from the reaction conditions poses\nadditional constraints, as the model presented in this w...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60c750f2ee301c70b1c7a973/original/prediction-of-chemical-reaction-yields-using-deep-learning.pdf"
    }
  ]
}