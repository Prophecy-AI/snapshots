{
  "query": "How do you reduce prediction error for completely unseen test samples when cross-validation score is already optimized in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent guidance from the literature on model validation and common pitfalls, you can further lower error on truly unseen test samples even after achieving an optimal cross\u2011validation score by following these steps\u202f([Comet\u202f2023](https://www.comet.com/site/blog/understanding-hold-out-methods-for-training-machine-learning-models);\u202f[MachineLearningMastery\u202f2020](https://www.machinelearningmastery.com/data-leakage-machine-learning);\u202f[scikit\u2011learn\u202f2025](https://scikit-learn.org/stable/common_pitfalls.html);\u202f[Azure\u202f2025](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls);\u202f[FasterCapital\u202f2025](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html);\u202f[Stack\u202fOverflow\u202f2020](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels)):\n\n1. **Reserve a true hold\u2011out set**  \n   Split the original data once more (e.g., 10\u201320\u202f% of samples) and **never** use these points for training, hyper\u2011parameter tuning, or any cross\u2011validation folds. This final set provides an unbiased estimate of performance on completely unseen data\u202f([Comet\u202f2023](https://www.comet.com/site/blog/understanding-hold-out-methods-for-training-machine-learning-models)).\n\n2. **Fit preprocessing only on the training folds**  \n   Build a pipeline (e.g., scaling, encoding, feature extraction) that is **fit** on each training split and then **applied** to the validation or hold\u2011out data. In scikit\u2011learn this is done with `make_pipeline` or `Pipeline` so that the same transformations are used at inference time\u202f([scikit\u2011learn\u202f2025](https://scikit-learn.org/stable/common_pitfalls.html);\u202f[Stack\u202fOverflow\u202f2020](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels)).\n\n3. **Eliminate data leakage**  \n   Audit your features and preprocessing steps to ensure no information from the target or future observations leaks into the training data (e.g., using future timestamps, target\u2011derived statistics, or leakage through imputation). Removing leakage prevents overly optimistic CV scores that do not translate to real\u2011world data\u202f([MachineLearningMastery\u202f2020](https://www.machinelearningmastery.com/data-leakage-machine-learning)).\n\n4. **Use nested cross\u2011validation for hyper\u2011parameter tuning**  \n   Wrap the outer CV (used for performance estimation) around an inner CV that searches hyper\u2011parameters. This prevents the outer score from being biased by the same data used for tuning, yielding a more reliable estimate of generalization error\u202f([FasterCapital\u202f2025](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html)).\n\n5. **Apply regularization or simpler models**  \n   If the optimized CV score still overfits, increase regularization strength (e.g., higher `C` for SVM, stronger L1/L2 penalties) or switch to a less complex algorithm. Simpler models tend to generalize better when data are limited\u202f([Azure\u202f2025](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls)).\n\n6. **Address class imbalance and sampling bias**  \n   Use stratified splits, resampling (SMOTE, undersampling), or class\u2011weight adjustments so that the model sees a representative distribution during training and validation, reducing surprise on rare classes in the hold\u2011out set\u202f([Azure\u202f2025](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls)).\n\n7. **Iterate with more data or better features**  \n   When the hold\u2011out error remains high, collect additional labeled samples or perform feature engineering/selection (e.g., remove noisy features, add domain\u2011specific variables). More diverse data directly improves the model\u2019s ability to generalize\u202f([Comet\u202f2023](https://www.comet.com/site/blog/understanding-hold-out-methods-for-training-machine-learning-models)).\n\nBy systematically applying these actions\u2014especially a clean hold\u2011out evaluation, leakage checks, and nested CV\u2014you can identify and close the gap between an already\u2011optimized cross\u2011validation score and the true error on completely unseen test samples.",
      "url": ""
    },
    {
      "title": "Understanding Hold-Out Methods for Training Machine Learning ...",
      "text": "Understanding Hold-Out Methods for Training Machine Learning Models - Comet\nBuild AI tools in our[virtual hackathon](https://www.encodeclub.com/programmes/comet-resolution-v2-hackathon)| $30,000 in prizes!\n[![Comet logo](https://www.comet.com/site/wp-content/uploads/2025/07/comet-logo-dark.svg)](https://www.comet.com/site/)\n[Contact Us](https://www.comet.com/site/about-us/contact-us/)\n[Try Opik Free](https://comet.com/signup?from=llm)\n# Understanding Hold-Out Methods for Training Machine Learning Models\nWords By\n[Kailaash Devi](https://www.comet.com/site/blog/author/devikailaashgmail-com/)\nAugust 14, 2023\nDuring the evaluation of machine learning (ML) models, the following question might arise:\n* Is this model the best one available from the hypothesis space of the algorithm in terms of generalization error on an unknown/future data set?\n* What training and testing techniques are used for the model?\n* What model should be selected from the available ones?\nThe hold-out method is used to address these questions.\nConsider training a model using an algorithm on a given dataset. Using the same training data, you determine that the trained model has an accuracy of 95% or even 100%. What does this mean? Can this model be used for prediction?\nNo. This is because your model has been trained on the given data, i.e. it knows the data and has generalized over it very well. In contrast, when you try to predict over a new set of data, it will most likely give you very bad accuracy because it has never seen the data before and thus cannot generalize well over it. To deal with such problems, hold-out methods can be employed.\nIn this post, we will take a closer look at the hold-out method used during the process of training machine learning models.\n# What is the Hold-Out Method?\nThe hold-out method involves splitting the data into multiple parts and using one part for training the model and the rest for validating and testing it. It can be used for both model evaluation and selection.\nIn cases where every piece of data is used for training the model, there remains the problem of selecting the best model from a list of possible models. Primarily, we want to identify which model has the lowest generalization error or which model makes a better prediction on future or unseen datasets than all of the others. There is a need to have a mechanism that allows the model to be trained on one set of data and tested on another set of data. This is where hold-out comes into play.\n# Hold-Out Method for Model Evaluation\nModel evaluation using the hold-out method entails splitting the dataset into training and test datasets, evaluating model performance, and determining the most optimal model. This diagram illustrates the hold-out method for model evaluation.\n![](https://miro.medium.com/v2/resize:fit:700/1*iZpmWiVeFn0bcuMZ_yiEdw.jpeg)\nHold-out method for model evaluation (Source: By Author)\nThere are two parts to the dataset in the diagram above. One split is held aside as a training set. Another set is held back for testing or evaluation of the model. The percentage of the split is determined based on the amount of training data available. A typical split of 70\u201330% is used in which 70% of the dataset is used for training and 30% is used for testing the model.\nThe objective of this technique is to select the best model based on its accuracy on the testing dataset and compare it with other models. There is, however, the possibility that the model can be well fitted to the test data using this technique. In other words, models are trained to improve model accuracy on test datasets based on the assumption that the test dataset represents the population. As a result, the test error becomes an optimistic estimation of the generalization error. Obviously, this is not what we want. Since the final model is trained to fit well (or overfit) the test data, it won\u2019t generalize well to unknowns or future datasets.\n> Olcay Cirit and his team at Uber AI was able to build a neural network that outperformed XGBoost. [> Learn more by checking out this clip from our recent Comet customer roundtable.\n](https://www.youtube.com/watch?v=39GTCeqfvy8&amp;list=PLX9GmL8cVn_yout9BRYNj43XJco3gsZ3r&amp;index=1)\nFollow the steps below for using the hold-out method for model evaluation:\n1. Split the dataset in two (preferably 70\u201330%; however, the split percentage can vary and should be random).\n![](https://miro.medium.com/v2/resize:fit:514/1*yC50uai2UPOcpouXik05Dw.jpeg)\n2. Now, we train the model on the training dataset by selecting some fixed set of hyperparameters while training the model.\n![](https://miro.medium.com/v2/resize:fit:484/1*kxySBNHqqBC_CMZY7Zz2sg.jpeg)\n3. Use the hold-out test dataset to evaluate the model.\n![](https://miro.medium.com/v2/resize:fit:368/1*YXcAhXtP4vprP_XzMFrpqQ.jpeg)\n4. Use the entire dataset to train the final model so that it can generalize better on future datasets.\n![](https://miro.medium.com/v2/resize:fit:550/1*AejwyUqZ3spg_6N8JLsg6g.jpeg)\nIn this process, the dataset is split into training and test sets, and a fixed set of hyperparameters is used to evaluate the model. There is another process in which data can also be split into three sets, and these sets can be used to select a model or to tune hyperparameters. We will discuss that technique next.\n# Hold-Out Method for Model Selection\nSometimes the model selection process is referred to as hyperparameter tuning. During the hold-out method of selecting a model, the dataset is separated into three sets \u2014training, validation, and test.\n![](https://miro.medium.com/v2/resize:fit:700/1*TIvUOys2OoCdFOBcDjD9oA.jpeg)\nHold-out method for model selection (Source: By Author)\nFollow the steps below for using the hold-out method for model selection:\n1. Divide the dataset into three parts: training dataset, validation dataset, and test dataset.\n2. Now, different machine learning algorithms can be used to train different models. You can train your classification model, for example, using logistic regression, random forest, and XGBoost.\n3. Tune the hyperparameters for models trained with different algorithms. Change the hyperparameter settings for each algorithm mentioned in step 2 and come up with multiple models.\n4. On the validation dataset, test the performance of each of these models (associating with each of the algorithms).\n5. Choose the most optimal model from those tested on the validation dataset. The most optimal model will be set up with the most optimal hyperparameters. Using the example above, let\u2019s suppose the model trained with XGBoost with the most optimal hyperparameters is selected.\n6. Finally, on the test dataset, test the performance of the most optimal model.\nNow, let\u2019s see how to implement this in Python.\n# Implementing Python Code for Training/Test Split\nThe following Python code can be used to split the dataset into training and testing. Here is a code example that uses the[Sklearn Boston housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)to show how the train\\_test\\_split method from Sklearn.model\\_selection can be used to split the dataset into training and test. The test size is specified using the parameter test\\_size.\n```\n#Importing the dataset**from**sklearn**import**datasets**from**sklearn.model\\_selection**import**train\\_test\\_split#Then, loading the Boston Dataset\nbhp**=**datasets.load\\_boston()#Finally, creating the Training and Test Split\nX\\_train, X\\_test, y\\_train, y\\_test**=**train\\_test\\_split(bhp.data, bhp.target, random\\_state**=**42, test\\_size**=**0.3)\n```\n# **Conclusion**\nIf you have a large dataset, you\u2019re in a hurry, or you\u2019re just starting out with a data science project, you might benefit from the hold-out method. Hold-out methods can also be used to avoid overfitting or underfitting problems in machine learning models. Choosing a classifier is best done using hold-out methods. Additionally, these methods reduce error pruning for trees and fa...",
      "url": "https://www.comet.com/site/blog/understanding-hold-out-methods-for-training-machine-learning-models"
    },
    {
      "title": "Data Leakage in Machine Learning - MachineLearningMastery.com",
      "text": "Data Leakage in Machine Learning - MachineLearningMastery.comData Leakage in Machine Learning - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Data Preparation Crash-Course]()\n# Data Leakage in Machine Learning\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 15, 2020in[Data Preparation](https://machinelearningmastery.com/category/data-preparation/)[**98](https://machinelearningmastery.com/data-leakage-machine-learning/#comments)\nShare*Post*Share\nData leakage is a big problem in machine learning when developing[predictive models](https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/).\nData leakage is when information from outside the training dataset is used to create the model.\nIn this post you will discover the problem of data leakage in predictive modeling.\nAfter reading this post you will know:\n* What is data leakage is in predictive modeling.\n* Signs of data leakage and why it is a problem.\n* Tips and tricks that you can use to minimize data leakage on your predictive modeling problems.\n**Kick-start your project**with my new book[Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/), including*step-by-step tutorials*and the*Python source code*files for all examples.\nLet&#8217;s get started.\n![Data Leakage in Machine Learning](https://machinelearningmastery.com/wp-content/uploads/2016/07/Data-Leakage-in-Machine-Learning.jpg)\nData Leakage in Machine Learning\nPhoto by[DaveBleasdale](https://www.flickr.com/photos/sidelong/20147524535/), some rights reserved.\n## Goal of Predictive Modeling\nThe goal of predictive modeling is to develop a model that makes accurate predictions on new data, unseen during training.\nThis is a hard problem.\nIt&#8217;s hard because we cannot evaluate the model on something we don&#8217;t have.\nTherefore, we must estimate the performance of the model on unseen data by training it on only some of the data we have and evaluating it on the rest of the data.\nThis is the principle that underlies cross validation and more sophisticated techniques that try to reduce the variance in this estimate.\n### Want to Get Started With Data Preparation?\nTake my free 7-day email crash course now (with sample code).\nClick to sign-up and also get a free PDF Ebook version of the course.\nDownload Your FREE Mini-Course\n## What is Data Leakage in Machine Learning?\nData leakage can cause you to create overly optimistic if not completely invalid predictive models.\nData leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed.\n> if any other feature whose value would not actually be available in practice at the time you&#8217;d want to use the model to make a prediction, is a feature that can introduce leakage to your model\n&#8212;[Data Skeptic](http://dataskeptic.com/epnotes/leakage.php)\n> when the data you are using to train a machine learning algorithm happens to have the information you are trying to predict\n&#8212; Daniel Gutierrez,[Ask a Data Scientist: Data Leakage](http://insidebigdata.com/2014/11/26/ask-data-scientist-data-leakage/)\nThere is a topic in[computer security called data leakage and data loss prevention](https://en.wikipedia.org/wiki/Data_loss_prevention_software)which is related but not what we are talking about.\n### Data Leakage is a Problem\nIt is a serious problem for at least 3 reasons:\n1. **It is a problem if you are running a machine learning competition**. Top models will use the leaky data rather than be good general model of the underlying problem.\n2. **It is a problem when you are a company providing your data**. Reversing an anonymization and obfuscation can result in a privacy breach that you did not expect.\n3. **It is a problem when you are developing your own predictive models**. You may be creating overly optimistic models that are practically useless and cannot be used in production.\nAs machine learning practitioners, we are primarily concerned with this last case.\n### Do I have Data Leakage?\nAn easy way to know you have data leakage is if you are achieving performance that seems a little too good to be true.\nLike you can predict lottery numbers or pick stocks with high accuracy.\n> &#8220;too good to be true&#8221; performance is &#8220;a dead giveaway&#8221; of its existence\n&#8212; Chapter 13,[Doing Data Science: Straight Talk from the Frontline](https://amzn.to/3iKp4HU)\nData leakage is generally more of a problem with complex datasets, for example:\n* Time series datasets when creating training and test sets can be difficult.\n* Graph problems where random sampling methods can be difficult to construct.\n* Analog observations like sound and images where samples are stored in separate files that have a size and a time stamp.## Techniques To Minimize Data Leakage When Building Models\nTwo good techniques that you can use to minimize data leakage when developing predictive models are as follows:\n1. Perform data preparation within your cross validation folds.\n2. Hold back a validation dataset for final sanity check of your developed models.\nGenerally, it is good practice to use both of these techniques.\n### 1. Perform Data Preparation Within Cross Validation Folds\nYou can easily leak information when preparing your data for machine learning.\nThe effect is overfitting your training data and having an overly optimistic evaluation of your models performance on unseen data.\nFor example, if you normalize or standardize your entire dataset, then estimate the performance of your model using cross validation, you have committed the sin of data leakage.\nThe data rescaling process that you performed had knowledge of the full distribution of data in the training dataset when calculating the scaling factors (like min and max or mean and standard deviation). This knowledge was stamped into the rescaled values and exploited by all algorithms in your cross validation test harness.\nA non-leaky evaluation of machine learning algorithms in this situation would calculate the parameters for rescaling data within each fold of the cross validation and use those parameters to prepare the data on the held out test fold on each cycle.\n> The reality is that as a data scientist, you&#8217;re at risk of producing a data leakage situation any time you prepare, clean your data, impute missing values, remove outliers, etc. You might be distorting the data in the process of preparing it to the point that you&#8217;ll build a model that works well on your &#8220;clean&#8221; dataset, but will totally suck when applied in the real-world situation where you actually want to apply it.\n&#8212; Page 313,[Doing Data Science: Straight Talk from the Frontline](https://amzn.to/3iKp4HU)\nMore generally, non-leaky data preparation must happen within each fold of your cross validation cycle.\nYou may be able to relax this constraint\u00a0for some problems, for example if you can confidently estimate the distribution of your data because you have some other domain knowledge.\nIn general though, it is a good idea to re-prepare or re-calculate any required data preparation within your cross validation folds including tasks like feature selection, outlier removal, encoding, feature scaling and projection methods for dimensionality reduction, and more.\n> If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this is what biases the performance analysis.\n&#8...",
      "url": "https://www.machinelearningmastery.com/data-leakage-machine-learning"
    },
    {
      "title": "Generalization Error: The Ultimate Goal: Reducing Generalization Error with Cross Validation - FasterCapital",
      "text": "[Home](https://fastercapital.com) [Content](https://fastercapital.com/content/index.html) [Generalization Error: The Ultimate Goal: Reducing Generalization Error with Cross Validation](https://fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html)\n\n# Generalization Error: The Ultimate Goal: Reducing Generalization Error with Cross Validation\n\n**Updated: 07 Apr 2025****18 minutes**\n\nTable of Content\n\n[1\\. Introduction to Generalization Error](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Introduction-to-Generalization-Error)\n\n[2\\. The Importance of Model Validation](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#The-Importance-of-Model-Validation)\n\n[3\\. A Conceptual Overview](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#A-Conceptual-Overview)\n\n[4\\. Types of Cross-Validation Techniques](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Types-of-Cross-Validation-Techniques)\n\n[5\\. Implementing Cross-Validation in Model Training](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Implementing-Cross-Validation-in-Model-Training)\n\n[6\\. Analyzing Cross-Validation Results](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Analyzing-Cross-Validation-Results)\n\n[7\\. Common Pitfalls in Cross-Validation and How to Avoid Them](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Common-Pitfalls-in-Cross-Validation-and-How-to-Avoid-Them)\n\n[8\\. Advanced Strategies for Minimizing Generalization Error](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Advanced-Strategies-for-Minimizing-Generalization-Error)\n\n[9\\. Integrating Cross-Validation into Your Machine Learning Pipeline](https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html#Integrating-Cross-Validation-into-Your-Machine-Learning-Pipeline)\n\n# Generalization Error: The Ultimate Goal: Reducing Generalization Error with Cross Validation\n\n## 1\\. Introduction to Generalization Error\n\nIn the realm of machine learning, the concept of generalization error sits at the very heart of a model's ability to perform well on unseen data. It's a measure of how accurately an algorithm can predict outcomes for new, previously unobserved inputs. A model that generalizes well is not just memorizing the training data but has learned the underlying patterns that can be applied to new situations. This is crucial because the ultimate [test of a machine learning](https://www.fastercapital.com/content/Drivers--Test-Machine-Learning--Driving-Customer-Engagement--Harnessing-Machine-Learning-in-Driver-Test-Experiences.html) model is not how well it performs on the data it was trained on, but how it adapts to data it has never seen before.\n\n**1\\. Understanding Generalization Error:**\n\nGeneralization error, also known as out-of-sample error, is the difference between the error rates on the training data and the error rates on new data. It's a reflection of how well the model's learned patterns apply to fresh data.\n\n**2\\. The [bias-Variance tradeoff](https://www.fastercapital.com/content/Bias-Variance-Tradeoff--Walking-the-Tightrope--Cross-Validation-and-the-Bias-Variance-Tradeoff.html):**\n\nThe generalization error can be decomposed into two main components: bias and variance. Bias refers to the error that arises from incorrect assumptions in the learning algorithm. High bias can cause the model to miss relevant relations between features and target outputs (underfitting). Variance, on the other hand, refers to the error that arises from sensitivity to small fluctuations in the training set. High variance can cause 'overfitting,' where the model learns the noise in the training data as if it were a true signal.\n\n**3\\. Cross-Validation as a Solution:**\n\nCross-validation is a technique used to assess how the results of a statistical analysis will generalize to an independent data set. It is primarily used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n\n**4\\. K-Fold Cross-Validation:**\n\nOne common form of cross-validation is the k-fold cross-validation. The data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed.\n\n**5\\. Leave-One-Out Cross-Validation (LOOCV):**\n\nAnother form is LOOCV, where each observation is used once as a test set (singleton) while the remaining observations form the training set.\n\n**Example:**\n\nImagine we're trying to predict housing prices based on various features like size, location, and age of the property. A model that's been trained on a specific dataset from a particular city might have a low error rate on that data. However, when we try to apply this model to predict prices in a different city, the error rate could skyrocket if the model has not generalized well. This is where cross-validation comes in, helping us to estimate how well our model is likely to perform on different, unseen datasets.\n\nReducing generalization error is a fundamental goal in machine learning, and cross-validation is a powerful tool in achieving this. By understanding and applying these concepts, we can create models that are not only accurate on our current data but robust enough to handle new, unforeseen challenges.\n\n## 2\\. The Importance of Model Validation\n\nIn the quest to build predictive models that not only perform well on historical data but also generalize to new, unseen data, model validation emerges as a cornerstone of the machine learning process. It is the rigorous assessment of how our statistical models perform in terms of accuracy, robustness, and predictive power. Without proper validation, models are susceptible to overfitting, where they capture noise as if it were a signal, leading to overly optimistic performance estimates and poor generalization to new data. This is akin to memorizing the answers to a test rather than understanding the underlying principles; it might work for that one test, but it fails when presented with new questions.\n\nFrom a **statistician's perspective**, [model validation is about ensuring](https://www.fastercapital.com/content/Predictive-analytics--Model-Validation--Ensuring-Accuracy--Model-Validation-in-Predictive-Analytics.html) the integrity of the inferences drawn from the model. For a **data scientist**, it's a practical necessity to guarantee the model's utility in real-world applications. Meanwhile, from a **business standpoint**, it's about the bottom line\u2014ensuring that the model delivers value by making accurate predictions that lead to better decisions.\n\nHere are some key aspects of model validation:\n\n1\\. **Cross-Validation**: This technique involves partitioning the data into subsets, training the model on some subsets (training set), and validating the model on the remaining subsets (validation set). The most common form is k-fold cross-validation, where the data is divided into k subsets, and the model is trained and validated k times, each time using a different subset as the validation set and the remaining as the training set. This process mitigates the risk of overfitting ...",
      "url": "https://www.fastercapital.com/content/Generalization-Error--The-Ultimate-Goal--Reducing-Generalization-Error-with-Cross-Validation.html"
    },
    {
      "title": "11.  Common pitfalls and recommended practices #",
      "text": "11. Common pitfalls and recommended practices &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)](index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 11.Common pitfalls and recommended practices[#](#common-pitfalls-and-recommended-practices)\nThe purpose of this chapter is to illustrate some common pitfalls and\nanti-patterns that occur when using scikit-learn. It provides\nexamples of what**not**to do, along with a corresponding correct\nexample.\n## 11.1.Inconsistent preprocessing[#](#inconsistent-preprocessing)\nscikit-learn provides a library of[Dataset transformations](data_transforms.html#data-transforms), which\nmay clean (see[Preprocessing data](modules/preprocessing.html#preprocessing)), reduce\n(see[Unsupervised dimensionality reduction](modules/unsupervised_reduction.html#data-reduction)), expand (see[Kernel Approximation](modules/kernel_approximation.html#kernel-approximation))\nor generate (see[Feature extraction](modules/feature_extraction.html#feature-extraction)) feature representations.\nIf these data transforms are used when training a model, they also\nmust be used on subsequent datasets, whether it\u2019s test data or\ndata in a production system. Otherwise, the feature space will change,\nand the model will not be able to perform effectively.\nFor the following example, let\u2019s create a synthetic dataset with a\nsingle feature:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_regression&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;random\\_state=42&gt;&gt;&gt;X,y=make\\_regression(random\\_state=random\\_state,n\\_features=1,noise=1)&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,test\\_size=0.4,random\\_state=random\\_state)\n```\n**Wrong**\nThe train dataset is scaled, but not the test dataset, so model\nperformance on the test dataset is worse than expected:\n```\n&gt;&gt;&gt;fromsklearn.metricsimportmean\\_squared\\_error&gt;&gt;&gt;fromsklearn.linear\\_modelimportLinearRegression&gt;&gt;&gt;fromsklearn.preprocessingimportStandardScaler&gt;&gt;&gt;scaler=StandardScaler()&gt;&gt;&gt;X\\_train\\_transformed=scaler.fit\\_transform(X\\_train)&gt;&gt;&gt;model=LinearRegression().fit(X\\_train\\_transformed,y\\_train)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))62.80...\n```\n**Right**\nInstead of passing the non-transformed`X\\_test`to`predict`, we should\ntransform the test data, the same way we transformed the training data:\n```\n&gt;&gt;&gt;X\\_test\\_transformed=scaler.transform(X\\_test)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test\\_transformed))0.90...\n```\nAlternatively, we recommend using a[`Pipeline`](modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), which makes it easier to chain transformations\nwith estimators, and reduces the possibility of forgetting a transformation:\n```\n&gt;&gt;&gt;fromsklearn.pipelineimportmake\\_pipeline&gt;&gt;&gt;model=make\\_pipeline(StandardScaler(),LinearRegression())&gt;&gt;&gt;model.fit(X\\_train,y\\_train)Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),(&#39;linearregression&#39;, LinearRegression())])&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))0.90...\n```\nPipelines also help avoiding another common pitfall: leaking the test data\ninto the training data.\n## 11.2.Data leakage[#](#data-leakage)\nData leakage occurs when information that would not be available at prediction\ntime is used when building the model. This results in overly optimistic\nperformance estimates, for example from[cross-validation](modules/cross_validation.html#cross-validation), and thus poorer performance when the model is used\non actually novel data, for example during production.\nA common cause is not keeping the test and train data subsets separate.\nTest data should never be used to make choices about the model.**The general rule is to never call**`fit`**on the test data**. While this\nmay sound obvious, this is easy to miss in some cases, for example when\napplying certain pre-processing steps.\nAlthough both train and test data subsets should receive the same\npreprocessing transformation (as described in the previous section), it is\nimportant that these transformations are only learnt from the training data.\nFor example, if you have a\nnormalization step where you divide by the average value, the average should\nbe the average of the train subset,**not**the average of all the data. If the\ntest subset is included in the average calculation, information from the test\nsubset is influencing the model.\n### 11.2.1.How to avoid data leakage[#](#how-to-avoid-data-leakage)\nBelow are some tips on avoiding data leakage:\n* Always split the data into train and test subsets first, particularly\nbefore any preprocessing steps.\n* Never include test data when using the`fit`and`fit\\_transform`methods. Using all the data, e.g.,`fit(X)`, can result in overly optimistic\nscores.\nConversely, the`transform`method should be used on both train and test\nsubsets as the same preprocessing should be applied to all the data.\nThis can be achieved by using`fit\\_transform`on the train subset and`transform`on the test subset.\n* The scikit-learn[pipeline](modules/compose.html#pipeline)is a great way to prevent data\nleakage as it ensures that the appropriate method is performed on the\ncorrect data subset. The pipeline is ideal for use in cross-validation\nand hyper-parameter tuning functions.\nAn example of data leakage during preprocessing is detailed below.\n### 11.2.2.Data leakage during pre-processing[#](#data-leakage-during-pre-processing)\nNote\nWe here choose to illustrate data leakage with a feature selection step.\nThis risk of leakage is however relevant with almost all transformations\nin scikit-learn, including (but not limited to)[`StandardScaler`](modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler),[`SimpleImputer`](modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), and[`PCA`](modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA).\nA number of[Feature selection](modules/feature_selection.html#feature-selection)functions are available in scikit-learn.\nThey can help remove irrelevant, redundant and noisy features as well as\nimprove your model build time and performance. As with any other type of\npreprocessing, feature selection should**only**use the training data.\nIncluding the test data in feature selection will optimistically bias your\nmodel.\nTo demonstrate we will create this binary classification problem with\n10,000 randomly generated features:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;n\\_samples,n\\_features,n\\_classes=200,10000,2&gt;&gt;&gt;rng=np.random.RandomState(42)&gt;&gt;&gt;X=rng.standard\\_normal((n\\_samples,n\\_features))&gt;&gt;&gt;y=rng.choice(n\\_classes,n\\_samples)\n```\n**Wrong**\nUsing all the data to perform feature selection results in an accuracy score\nmuch higher than chance, even though our targets are completely random.\nThis randomness means that our`X`and`y`are independent and we thus expect\nthe accuracy to be around 0.5. However, since the feature selection step\n\u2018sees\u2019 the test data, the model has an unfair advantage. In the incorrect\nexample below we first use all the data for feature selection and then split\nthe data into training and test subsets for model fitting. The result is a\nmuch higher than expected accuracy score:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;fromsklearn.feature\\_selectionimportSelectKBest&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.metricsimportaccuracy\\_score&gt;&gt;&gt;# Incorrect preprocessing: the entire data is tr...",
      "url": "https://scikit-learn.org/stable/common_pitfalls.html"
    },
    {
      "title": "How to test unseen test data with cross validation and predict labels?",
      "text": "##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [How to test unseen test data with cross validation and predict labels?](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked4 years, 4 months ago\n\nModified [4 years, 3 months ago](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels?lastactivity)\n\nViewed\n1k times\n\n0\n\n1.The CSV that contains data(ie. text description) along with categorized labels\n\n```\ndf = pd.read_csv('./output/csv_sanitized_16_.csv', dtype=str)\nX = df['description_plus']\ny = df['category_id']\n\n```\n\n2.This CSV contains unseen data(ie. text description) for which labels need to be predicted\n\n```\ndf_2 = pd.read_csv('./output/csv_sanitized_2.csv', dtype=str)\nX2 = df_2['description_plus']\n\n```\n\nCross validation function that operates on the training data(item #1) above.\n\n```\ndef cross_val():\n    cv = KFold(n_splits=20)\n    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                     stop_words='english')\n    X_train = vectorizer.fit_transform(X)\n    clf = make_pipeline(preprocessing.StandardScaler(with_mean=False), svm.SVC(C=1))\n    scores = cross_val_score(clf, X_train, y, cv=cv)\n    print(scores)\n    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\ncross_val()\n\n```\n\nI need to know how to pass the unseen data(item #2) to the cross validation function and how to predict the labels?\n\n- [python-3.x](https://stackoverflow.com/questions/tagged/python-3.x)\n- [pandas](https://stackoverflow.com/questions/tagged/pandas)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [sklearn-pandas](https://stackoverflow.com/questions/tagged/sklearn-pandas)\n\n[Share](https://stackoverflow.com/q/60602393)\n\n[Improve this question](https://stackoverflow.com/posts/60602393/edit)\n\nFollow\n\nasked Mar 9, 2020 at 14:14\n\n[![neelmeg's user avatar](https://www.gravatar.com/avatar/cf5cce28048909e8dbe0df90bc710d7d?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/617779/neelmeg)\n\n[neelmeg](https://stackoverflow.com/users/617779/neelmeg) neelmeg\n\n2,47966 gold badges3434 silver badges4747 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n0\n\nUsing `scores = cross_val_score(clf, X_train, y, cv=cv)` you can only get the cross-validated scores of the model. `cross_val_score` will internally split the data into training and testing based on the `cv` parameter.\n\nSo the values that you get are the cross-validated accuracy of the SVC.\n\nTo get the score on the unseen data, you can first fit the model e.g.\n\n```\nclf = make_pipeline(preprocessing.StandardScaler(with_mean=False), svm.SVC(C=1))\nclf.fit(X_train, y) # the model is trained now\n\n```\n\nand then do `clf.score(X_unseen,y)`\n\nThe last will return the accuracy of the model on the unseen data.\n\n* * *\n\n## EDIT: The best way to do what you want is the following using a GridSearch to first find the best model using the training data and then evaluate the best model using the unseen (test) data:\n\n```\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# load some data\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n#split data to training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# hyperparameter tunig of the SVC model\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = svm.SVC()\n\n# fit the GridSearch using the TRAINING data\ngrid_searcher = GridSearchCV(svc, parameters)\ngrid_searcher.fit(X_train, y_train)\n\n#recover the best estimator (best parameters for the SVC, based on the GridSearch)\nbest_SVC_model = grid_searcher.best_estimator_\n\n# Now, check how this best model behaves on the test set\ncv_scores_on_unseen = cross_val_score(best_SVC_model, X_test, y_test, cv=5)\nprint(cv_scores_on_unseen.mean())\n\n```\n\n[Share](https://stackoverflow.com/a/60602520)\n\n[Improve this answer](https://stackoverflow.com/posts/60602520/edit)\n\nFollow\n\n[edited Mar 11, 2020 at 9:03](https://stackoverflow.com/posts/60602520/revisions)\n\nanswered Mar 9, 2020 at 14:22\n\n[![seralouk's user avatar](https://i.sstatic.net/7KIA9.png?s=64)](https://stackoverflow.com/users/5025009/seralouk)\n\n[seralouk](https://stackoverflow.com/users/5025009/seralouk) seralouk\n\n32.7k99 gold badges121121 silver badges137137 bronze badges\n\n4\n\n- Are we taking advantage of cross validation with this approach? should the lines you suggested be added after the cross\\_val\\_score function in case we want to perform cross validation and then fit the model and predict the score?\n\n\u2013\u00a0[neelmeg](https://stackoverflow.com/users/617779/neelmeg)\n\nCommentedMar 9, 2020 at 14:27\n\n- You can also use this function but then the scores that you will get are going to be the cross-validated scores under cv on the unseen set (that is going to be split internally again as explained before)\n\n\u2013\u00a0[seralouk](https://stackoverflow.com/users/5025009/seralouk)\n\nCommentedMar 10, 2020 at 8:00\n\n- 1\n\n\n\n\n\nSo you can either predict only or actually use again cross\\_val\\_score and then report these values as the CV-scores on the unseen dataset.\n\n\u2013\u00a0[seralouk](https://stackoverflow.com/users/5025009/seralouk)\n\nCommentedMar 10, 2020 at 8:09\n\n- so if I understand correctly we can use k-fold validation to choose which is a good model(by passing different models to k-fold validation) for my test classification problem and then use that model to train with labelled data and predict on unseen data. Is that right?\n\n\u2013\u00a0[neelmeg](https://stackoverflow.com/users/617779/neelmeg)\n\nCommentedMar 11, 2020 at 5:53\n\n\n[Add a comment](https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f60602393%2fhow-to-test-unseen-test-data-with-cross-validation-and-predict-labels%23new-answer)\n\nSign up using Google\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [python-3.x](https://stackoverflow.com/questions/tagged/python-3.x) - [pandas](https://stackoverflow.com/questions/tagged/pandas) - [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn) - [sklearn-pandas](https://stackoverflow.com/questions/tagged/sklearn-pandas)   or [ask your own question](https://sta...",
      "url": "https://stackoverflow.com/questions/60602393/how-to-test-unseen-test-data-with-cross-validation-and-predict-labels"
    },
    {
      "title": "Don't lose samples to estimation - PMC - NIH",
      "text": "Don\u2019t lose samples to estimation - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1016/j.patter.2022.100612)\n* [](pdf/main.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Patterns logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-patterns.png)\nPatterns (N Y)\n. 2022 Dec 9;3(12):100612. doi:[10.1016/j.patter.2022.100612](https://doi.org/10.1016/j.patter.2022.100612)\n# Don\u2019t lose samples to estimation\n[Ioannis Tsamardinos](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tsamardinos I\"[Author]>)\n### Ioannis Tsamardinos\n1Computer Science Department, University of Crete, Heraklion, Greece\n2JADBio \u2013Gnosis DA S.A, Heraklion, Greece\n3Institute of Applied and Computational Mathematics, Foundation for Research and Technology, Hellas, Heraklion, Greece\nFind articles by[Ioannis Tsamardinos](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tsamardinos I\"[Author]>)\n1,2,3,\u2217\n* Author information\n* Article notes\n* Copyright and License information\n1Computer Science Department, University of Crete, Heraklion, Greece\n2JADBio \u2013Gnosis DA S.A, Heraklion, Greece\n3Institute of Applied and Computational Mathematics, Foundation for Research and Technology, Hellas, Heraklion, Greece\n\u2217Corresponding authortsamard.it@gmail.com\nCollection date 2022 Dec 9.\n\u00a92022 The Author\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC9782254\u00a0\u00a0PMID:[36569551](https://pubmed.ncbi.nlm.nih.gov/36569551/)\n## Summary\nIn a typical predictive modeling task, we are asked to produce a final predictive model to employ operationally for predictions, as well as an estimate of its out-of-sample predictive performance. Typically, analysts hold out a portion of the available data, called a Test set, to estimate the model predictive performance on unseen (out-of-sample) records, thus \u201closing these samples to estimation.\u201d However, this practice is unacceptable when the total sample size is low. To avoid losing data to estimation, we need a shift in our perspective: we do not estimate the performance of a specific model instance; we estimate the performance of the pipeline that produces the model. This pipeline is applied on all available samples to produce the final model; no samples are lost to estimation. An estimate of its performance is provided by training the same pipeline on subsets of the samples. When multiple pipelines are tried, additional considerations that correct for the \u201cwinner\u2019s curse\u201d need to be in place.\n## The bigger picture\nEvery predictive model needs to be accompanied by an estimate of its predictive performance on unseen data. Typically, some samples are held out to estimate performance and thus, are \u201clost to estimation.\u201d This is impractical in low-sample datasets, precluding the development of advanced machine learning models. However, it can be avoided: (1) train your final model on all the data, (2) estimate its performance by training proxy models using the same machine learning pipeline on subsets of the data and testing on the rest, and (3) when trying numerous pipelines correct the estimate for multiple tries. Two protocols that abide to the above principles are presented: the Nested Cross Validation and the Bootstrap Bias Corrected Cross Validation, along with practical advice for small sample datasets. Computational experiments show that the performance of complex (e.g., non-linear and containing multiple steps) machine learning pipelines can be reliably estimated, even in small sample size scenarios.\nEvery predictive model needs to be accompanied by an estimate of its out-of-sample generalization) predictive performance. The typical Train-Test or Train-Validation-Test estimation protocols hold out portions of the available data for estimation purposes, thus \u201closing samples to estimation\u201d. When sample size is low this practice is inappropriate. Is there a way to accurately estimate the predictive performance of our model without losing any samples to estimation?.\n## Introduction\nWe just produced a predictive machine learning model for our\u00a0client or as part of scientific research of high accuracy. Unfortunately, our job as a data scientist is not done, yet! Almost invariably we don\u2019t just deliver the model to put into\u00a0production; we also need to provide an estimate of its predictive performance on new, unseen data called out-of-sample performance. Predictive performance may be measured by the area under the curve (AUC), accuracy, F1, mean squared error, or whatever metric is sensible for the problem at hand. Is performance better than random guessing, is it better than existing models, or is it as perfect as the Delphi Oracle?\n### Ideal performance estimation protocol\nIdeally, we would like to estimate performance prospectively in the exact operational environment where the model is to be used:\n* **Ideal direct estimation protocol**: Train a*model*given the available data, install it in its operating environment, wait until you*prospectively*gather a sizable future test dataset where the outcome also becomes known, estimate the performance of the model on the prospective data. This protocol considers anything that may go wrong when\u00a0we deploy the model, from batch effects to software bugs in retrieving the data to feed into the model. Of course, it is completely impractical to\u00a0perform a prospective evaluation on each model we consider. We need to know how predictive a model is before we deploy it and without having to wait to gather more data. Hence, several estimation protocols appeared that*simulate*this ideal estimation protocol, one or\u00a0multiple times, called*out-of-sample estimation protocols*.\n### Traditional model production and estimation\n#### The Train-Test protocol\nThe samples on which to measure performance should never be seen by the model generating algorithm. The standard way to provide an estimate of the model\u2019s out-of-sample predictive performance (i.e., on unseen data), hereafter simply referred to as*model performance*, is to hold out a portion of the data, typically called a*Test set*, for estimation purposes. The rest of the data are called the*Training set*, used to train (fit, learn) the model to return. The Train-Test protocol directly*simulates a single time*the ideal estimation method, pretending the Test set comes from the \u201cfuture.\u201d Unfortunately, the samples in the Test set are*l...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9782254"
    },
    {
      "title": "Minimizing error on unseen data - Data Science Stack Exchange",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Minimizing error on unseen data](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked3 years, 8 months ago\n\nModified [3 years, 8 months ago](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data?lastactivity)\n\nViewed\n418 times\n\n5\n\n$\\\\begingroup$\n\nThe classifier aims to minimize the loss function (($F(x)$ \\- $\\\\hat{F}(x)$)2), where $F(x)$ is unknown function and $\\\\hat{F}(x)$ is the predicted function. If $F(x)$ is not known for unseen data, how do we compute this loss? Why is the training error used to estimate the error for unseen data?\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n\n[Share](https://datascience.stackexchange.com/q/82723)\n\n[Improve this question](https://datascience.stackexchange.com/posts/82723/edit)\n\nFollow\n\n[edited Oct 9, 2020 at 21:49](https://datascience.stackexchange.com/posts/82723/revisions)\n\nSid\n\nasked Oct 8, 2020 at 7:11\n\n[![Sid's user avatar](https://www.gravatar.com/avatar/fe55d4965785dfb0eb0431baacc15e57?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/105502/sid)\n\n[Sid](https://datascience.stackexchange.com/users/105502/sid) Sid\n\n15588 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n6\n\n$\\\\begingroup$\n\n> if we don't know $F(x)$ for unseen data, how does a decision tree minimize this error?\n\nEvery supervised ML method relies on the assumption that the test data (any unseen data) follows the same distribution as the training data (note that this is not specific to Decision Trees). In fact both the training data and the test data are assumed to be sampled from the true population data. As a consequence $F(x)$ is assumed to be the same for the training data and the test (unseen) data.\n\nIf one uses a trained model on some unseen data which is not distributed like the training data, the results are simply unpredictable and the performance is likely to drop.\n\n> Why do we estimate the error for unseen data using the error observed in the training data?\n\nYou seem to suggest to use the \"unseen data\" in the training process. You would indeed get better results on the \"unseen data\" if you optimized on it, but then you would lose the point of having a portion of data set apart. \"Unseed data\" is necessary to estimate how good your model will perform on data never seen before. If you don't keep some data set apart you may have a better model but you have no way of estimating how good it will be when put into production.\n\n[Share](https://datascience.stackexchange.com/a/82741)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/82741/edit)\n\nFollow\n\n[edited Oct 8, 2020 at 17:32](https://datascience.stackexchange.com/posts/82741/revisions)\n\n[![mprouveur's user avatar](https://www.gravatar.com/avatar/4f0d3005d28e9443ba52ee46e03c06ef?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/104626/mprouveur)\n\n[mprouveur](https://datascience.stackexchange.com/users/104626/mprouveur)\n\n33811 silver badge77 bronze badges\n\nanswered Oct 8, 2020 at 13:52\n\n[![Erwan's user avatar](https://i.sstatic.net/mIAGP.jpg?s=64)](https://datascience.stackexchange.com/users/64377/erwan)\n\n[Erwan](https://datascience.stackexchange.com/users/64377/erwan) Erwan\n\n25.5k33 gold badges1414 silver badges3535 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data)\u00a0\\|\n\n1\n\n$\\\\begingroup$\n\nThe idea of using a test set is to mimic the real-world application of using machine learning to, say, do speech recognition for people who aren\u2019t born yet.\n\nThat\u2019s the situation where you don\u2019t know the label, so you can\u2019t calculate the error or loss.\n\nHowever, we mimic that (we hope) by withholding some data from the training. We fit the model on the training data and the estimate the error on unseen data by using our withheld data, since we know the correct label or value for those points, even though we didn\u2019t tell the model.\n\nOur estimate on the holdout data might be badly wrong when we deploy the model to, say, Siri or Alexa, but some kind of error calculation on withheld data is the next-best that we can do until it goes into production and we see how the model performs.\n\n[Share](https://datascience.stackexchange.com/a/82796)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/82796/edit)\n\nFollow\n\nanswered Oct 9, 2020 at 22:06\n\n[![Dave's user avatar](https://www.gravatar.com/avatar/2d2e852a5b039b0d1968a16bef768d49?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/73930/dave)\n\n[Dave](https://datascience.stackexchange.com/users/73930/dave) Dave\n\n3,96011 gold badge99 silver badges2929 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f82723%2fminimizing-error-on-unseen-data%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [classification](https://datascience.stackexchange.com/questions/tagged/classification)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[2](https://datascience.stackexchange.com/q/23690) [Fractions or probabilities as training labels](https://datascience.stackexchange.com/questions/23690/fractions-or-probabilities-as-training-labels)\n\n[1](https://datascience.stackexchange.com/q/27688) [asymmetric cost function for deep neural network binary classifier](https://datascience.stackexchange.com/questions/27688/asymmetric-cost-function-for-deep-neural-network-binary-classifier)\n\n[3](https://datascience.stackexchange.com/q/32001) [Neural Networks - Strategies for problems with high Bayes error rate](https://datascience.stackexchange.com/questions/32001/neural-networks-strategies-for-problems-with-high-bayes-error-rate)\n\n[1](https://datascience.stackexchange.com/q/51744) [difference betwen predicting seen and unseen data](https://datascience.stackexchange.com/questions/51744/difference-betwen-predicting-seen-and-unseen-data)\n\n[1](https://datascience.stackexchange.com/q/54208) [Keras model giving error when fields of unseen test data and train data are not same](https://datascience.stackexchange.com/questions/54208/keras-model-giving-error-when-fields-of-unseen-test-data-and-train-data-a...",
      "url": "https://datascience.stackexchange.com/questions/82723/minimizing-error-on-unseen-data"
    },
    {
      "title": "Prevent overfitting and imbalanced data with Automated ML",
      "text": "[Skip to main content](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls?view=azureml-api-2#main)\n\nThis browser is no longer supported.\n\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n\n[Download Microsoft Edge](https://go.microsoft.com/fwlink/p/?LinkID=2092881) [More info about Internet Explorer and Microsoft Edge](https://learn.microsoft.com/en-us/lifecycle/faq/internet-explorer-microsoft-edge)\n\nTable of contentsExit focus mode\n\n[Read in English](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls?view=azureml-api-2) Save\n\n- Add to Collections\n- Add to Plan\n\nTable of contents [Read in English](https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls?view=azureml-api-2)Add to CollectionsAdd to Plan[Edit](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning/concept-manage-ml-pitfalls.md)\n\n* * *\n\n#### Share via\n\n[Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview%3Dazureml-api-2%26WT.mc_id%3Dfacebook) [x.com](https://twitter.com/intent/tweet?original_referer=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview%3Dazureml-api-2%26WT.mc_id%3Dtwitter&text=Today%20I%20completed%20%22Avoid%20overfitting%20%26%20imbalanced%20data%20with%20Automated%20machine%20learning%20-%20Azure%20Machine%20Learning%20%7C%20Microsoft%20Learn%22!%20I'm%20so%20proud%20to%20be%20celebrating%20this%20achievement%20and%20hope%20this%20inspires%20you%20to%20start%20your%20own%20%40MicrosoftLearn%20journey!&tw_p=tweetbutton&url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview%3Dazureml-api-2%26WT.mc_id%3Dtwitter) [LinkedIn](https://www.linkedin.com/feed/?shareActive=true&text=Today%20I%20completed%20%22Avoid%20overfitting%20%26%20imbalanced%20data%20with%20Automated%20machine%20learning%20-%20Azure%20Machine%20Learning%20%7C%20Microsoft%20Learn%22!%20I'm%20so%20proud%20to%20be%20celebrating%20this%20achievement%20and%20hope%20this%20inspires%20you%20to%20start%20your%20own%20%40MicrosoftLearn%20journey!%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview%3Dazureml-api-2%26WT.mc_id%3Dlinkedin) [Email](mailto://https:%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview=azureml-api-2?subject=%5BShared%20Article%5D%20Avoid%20overfitting%20%26%20imbalanced%20data%20with%20Automated%20machine%20learning%20-%20Azure%20Machine%20Learning%20%7C%20Microsoft%20Learn&body=Today%20I%20completed%20%22Avoid%20overfitting%20%26%20imbalanced%20data%20with%20Automated%20machine%20learning%20-%20Azure%20Machine%20Learning%20%7C%20Microsoft%20Learn%22!%20I'm%20so%20proud%20to%20be%20celebrating%20this%20achievement%20and%20hope%20this%20inspires%20you%20to%20start%20your%20own%20%40MicrosoftLearn%20journey!%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fmachine-learning%2Fconcept-manage-ml-pitfalls%3Fview%3Dazureml-api-2%26WT.mc_id%3Demail)\n\n* * *\n\nPrint\n\nTable of contents\n\n# Prevent overfitting and imbalanced data with Automated ML\n\n- Article\n- 06/16/2023\n- 10 contributors\n\nFeedback\n\nOverfitting and imbalanced data are common pitfalls when you build machine learning models. By default, Azure Machine Learning's Automated ML provides charts and metrics to help you identify these risks, and implements best practices to help mitigate them.\n\n## Identify overfitting\n\nOverfitting in machine learning occurs when a model fits the training data too well, and as a result can't accurately predict on unseen test data. In other words, the model has memorized specific patterns and noise in the training data, but is not flexible enough to make predictions on real data.\n\nConsider the following trained models and their corresponding train and test accuracies.\n\nExpand table\n\n| Model | Train accuracy | Test accuracy |\n| --- | --- | --- |\n| A | 99.9% | 95% |\n| B | 87% | 87% |\n| C | 99.9% | 45% |\n\nConsider model **A**, there is a common misconception that if test accuracy on unseen data is lower than training accuracy, the model is overfitted. However, test accuracy should always be less than training accuracy, and the distinction for overfit vs. appropriately fit comes down to _how much_ less accurate.\n\nCompare models **A** and **B**, model **A** is a better model because it has higher test accuracy, and although the test accuracy is slightly lower at 95%, it is not a significant difference that suggests overfitting is present. You wouldn't choose model **B** because the train and test accuracies are closer together.\n\nModel **C** represents a clear case of overfitting; the training accuracy is high but the test accuracy isn't anywhere near as high. This distinction is subjective, but comes from knowledge of your problem and data, and what magnitudes of error are acceptable.\n\n## Prevent overfitting\n\nIn the most egregious cases, an overfitted model assumes that the feature value combinations seen during training always results in the exact same output for the target.\n\nThe best way to prevent overfitting is to follow ML best practices including:\n\n- Using more training data, and eliminating statistical bias\n- Preventing target leakage\n- Using fewer features\n- **Regularization and hyperparameter optimization**\n- **Model complexity limitations**\n- **Cross-validation**\n\nIn the context of Automated ML, the first three ways lists best practices you implement. The last three bolded items are **best practices Automated ML implements** by default to protect against overfitting. In settings other than Automated ML, all six best practices are worth following to avoid overfitting models.\n\n## Best practices you implement\n\n### Use more data\n\nUsing more data is the simplest and best possible way to prevent overfitting, and as an added bonus typically increases accuracy. When you use more data, it becomes harder for the model to memorize exact patterns, and it is forced to reach solutions that are more flexible to accommodate more conditions. It's also important to recognize statistical bias, to ensure your training data doesn't include isolated patterns that don't exist in live-prediction data. This scenario can be difficult to solve, because there could be overfitting present when compared to live test data.\n\n### Prevent target leakage\n\nTarget leakage is a similar issue, where you may not see overfitting between train/test sets, but rather it appears at prediction-time. Target leakage occurs when your model \"cheats\" during training by having access to data that it shouldn't normally have at prediction-time. For example, to predict on Monday what a commodity price will be on Friday, if your features accidentally included data from Thursdays, that would be data the model won't have at prediction-time since it can't see into the future. Target leakage is an easy mistake to miss, but is often characterized by abnormally high accuracy for your problem. If you're attempting to predict stock price and trained a model at 95% accuracy, there's likely target leakage somewhere in your features.\n\n### Use fewer features\n\nRemoving features can also help with overfitting by preventing the model from having too many fields to use to memorize specific patterns, thus causing it to be more flexible. It can be difficult to measure quantitatively, but if you can remove features and retain the same accuracy, you have likely made the model more flexible and have reduced the risk of overfitting.\n\n## Best practices Automated ML implements\n\n### Regularization and hyperparameter tuning\n\n**Regularization** is the process of minimizing a cost function to penalize complex and overfitted models. There's different types of regularization functions, but in gen...",
      "url": "https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls?view=azureml-api-2"
    }
  ]
}