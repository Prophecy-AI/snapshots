{
  "query": "How do you break the linear relationship between cross-validation score and leaderboard score in Kaggle competitions when all models fall on the same line?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the Kaggle \u201cdon\u2019t trust the\u202fcv scores\u201d write\u2011up and the discussion on DataScience\u202fStack\u2011Exchange, a straight\u2011line relationship between CV and leaderboard scores usually means the CV split does not reflect the test distribution.\u202fYou can break that line by changing how you validate and by weighting the data so the validation set mimics the leaderboard set\u202f([Kaggle\u202fSolution\u202f#3](https://www.kaggle.com/competitions/tabular-playground-series-nov-2021/writeups/ambrosm-3-solution-don-t-trust-the-cv-scores); [DataScience\u202fSE](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases)).\n\n**Steps to break the linear CV\u202f\u2194\u202fleaderboard relationship**\n\n1. **Check for train\u2011test leakage** \u2013 Train a simple classifier to distinguish training rows from test rows; if it succeeds, your CV scores are biased.\u202fWhen this happens, apply *weighted training* (give higher weight to samples that look like the test set) to reduce the leakage effect\u202f([Kaggle\u202fSolution\u202f#3](https://www.kaggle.com/competitions/tabular-playground-series-nov-2021/writeups/ambrosm-3-solution-don-t-trust-the-cv-scores)).\n\n2. **Switch to a more robust CV scheme** \u2013 Use stratified folds for classification or group\u2011aware folds (e.g.,\u202f`StratifiedKFold`, `GroupKFold`) so each fold preserves the target distribution and any grouping structure present in the leaderboard data\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/597808)).\n\n3. **Create a leaderboard\u2011style hold\u2011out** \u2013 Split a portion of the training data that matches the public leaderboard\u2019s class balance, time period, or other known characteristics, and evaluate models on this hold\u2011out instead of relying solely on generic K\u2011fold CV. This makes the validation metric more predictive of the leaderboard score\u202f([DataScience\u202fSE discussion](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases)).\n\n4. **Weight or re\u2011sample to align distributions** \u2013 After the hold\u2011out is defined, optionally apply sample weights or oversample/undersample to further align the training distribution with the hold\u2011out (the weighted training experiment in the Kaggle write\u2011up showed it can shift the CV\u2011LB line).\n\n5. **Avoid over\u2011fitting to the public leaderboard** \u2013 Limit the number of submissions and use the robust CV/hold\u2011out scores to guide model selection rather than chasing incremental public\u2011LB improvements; this prevents the CV and LB from staying locked on the same linear trend\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/597808)).\n\nFollowing these actions will make your CV metric diverge from the leaderboard metric, giving you a more reliable gauge of true generalisation and allowing you to break the linear relationship that occurs when all models fall on the same line.",
      "url": ""
    },
    {
      "title": "#3 Solution: Don't trust the cv scores - Kaggle",
      "text": "<div><div><p>Checking your browser before accessing www.kaggle.com ...</p><p>Click <a href=\"#\">here</a> if you are not automatically redirected after 5 seconds.</p></div></div>",
      "url": "https://www.kaggle.com/competitions/tabular-playground-series-nov-2021/writeups/ambrosm-3-solution-don-t-trust-the-cv-scores"
    },
    {
      "title": "How to get good score in kaggle competition?",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/questions-and-answers/597808"
    },
    {
      "title": "Why when my local cv of loss decreases, my leaderboard's loss ...",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Why when my local cv of loss decreases, my leaderboard's loss increases?](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years ago\n\nModified [4 years ago](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases?lastactivity)\n\nViewed\n236 times\n\n0\n\n$\\\\begingroup$\n\nI got a cv log\\_loss of **0.3025410331400577** when using 4-fold cross-validation and my leaderboard (with 30% of test dataset) got **0.26514**.\nI further did feature engineering and added some features to the model, which decreased my cv log\\_loss to **0.2946628055452142** but my leaderboard score increases to **0.30021**.\n\nWith all other techniques used, my cv log\\_loss decreased but my leaderboard loss increased.\n\nI used XGBoostClassifier model. I have removed all correlated features (corr > 0.8) also.\n\nUsually we will be judging whether our model generalizes or not, based on cv score. But here, cv score is not reliable. What may be the reason of this?\n\nAnd is it valid to judge my model performs better when my cv score decreases ?\n\nIf not, what are all the other techniques to judge my model?\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [xgboost](https://datascience.stackexchange.com/questions/tagged/xgboost)\n- [cross-validation](https://datascience.stackexchange.com/questions/tagged/cross-validation)\n- [generalization](https://datascience.stackexchange.com/questions/tagged/generalization)\n\n[Share](https://datascience.stackexchange.com/q/74791)\n\n[Improve this question](https://datascience.stackexchange.com/posts/74791/edit)\n\nFollow\n\n[edited May 25, 2020 at 7:02](https://datascience.stackexchange.com/posts/74791/revisions)\n\n[![fuwiak's user avatar](https://www.gravatar.com/avatar/23cb41cee0394217cebb85d242c46502?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/56354/fuwiak)\n\n[fuwiak](https://datascience.stackexchange.com/users/56354/fuwiak)\n\n1,37388 gold badges1313 silver badges2626 bronze badges\n\nasked May 25, 2020 at 6:10\n\n[![Haripriya R's user avatar](https://www.gravatar.com/avatar/3f57742e82560aa23ddb655cab8803c1?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/66867/haripriya-r)\n\n[Haripriya R](https://datascience.stackexchange.com/users/66867/haripriya-r) Haripriya R\n\n1311 bronze badge\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n0\n\n$\\\\begingroup$\n\nI think there are a few things going on in this question so I will take them one at a time. Just to note there are multiple reasons you could be faced with the issues you are noticing so I'm just going to give some possible reasons that come to mind. Just to note by leaderboard is something like a Kaggle competition where data is held back for use as a blind test. Overall, I think more information is needed to have a stab at troubleshooting which I will explain as I go.\n\n> I got a cv log\\_loss of 0.3025410331400577 when using 4-fold cross-validation and my leaderboard (with 30% of test dataset) got 0.26514. I further did feature engineering and added some features to the model, which decreased my cv log\\_loss to 0.2946628055452142 but my leaderboard score increases to 0.30021.\n\nYour approach to use cross-validation (cv) is good however 4-fold seems odd to me and also a little low. The standard would generally be 5 or 10-fold, see [Cross Validation](https://stats.stackexchange.com/questions/49692/why-do-researchers-use-10-fold-cross-validation-instead-of-testing-on-a-validati) for a nice discussion about cv and some advantages of 10-fold. My thought is that if you have a proportion of outliers/misclassified data in on your training set your low choice of 4-fold could mean the outliers are present in all training sets so your model is trained on these misclassified cases. Perhaps test increasing the number of folds on your model's performance. Conversely, cv does depend on sample size so if you are restricted on sample size this would necessitate reducing the number of folds or avoiding cv completely. The issue with not doing this is explained very nicely [here](https://datascience.stackexchange.com/questions/28158/how-to-calculate-the-fold-number-k-fold-in-cross-validation) but in short, each of your folds for training should have the same distribution as the test set so if you think this may not be the case avoid cross validation or drop the value of k. [Here](https://datascience.stackexchange.com/questions/17288/why-k-fold-cross-validation-cv-overfits-or-why-discrepancy-occurs-between-cv?rq=1) is a really nice discussion about k-fold cross validation and over-fitting but the bottom line is it can happen. This is also without being sure of what supervised machine learning technique you are using however which would also play a role as some machine learning techniques work better with larger training sets e.g. DNNs.\n\n> With all other techniques used, my cv log\\_loss decreased but my leaderboard loss increased.\n> I used XGBoostClassifier model. I have removed all correlated features (corr > 0.8) also.\n\nYour choice to use a gradient boosting technique makes me think you are doing logistic regression and as such your choice to remove one of the highly correlated variables is, in general, a good idea; removing the extra influence of the variables, [here](https://stats.stackexchange.com/questions/219169/removing-highly-correlated-variables-in-logistic-regression-in-r) is a good discussion.\n\nThe fact that you are using cv to compare your approaches is good, this is what it is essentially for but note that as you do this you risk over fitting to the training data similar to doing this with just a training set. An important step I think you have missed is to split your data and create your own test set that you do not touch until hyperparameter tuning is complete to use as a blind test. This should give you equivalent results to the leaderboard test set if you have big enough sample size, your test/train split has equivalent distributions and lastly, if the test set from the leaderboard is, in fact, the same as the data you are using for training.\n\n> And is it valid to judge my model performs better when my cv score decreases ?\n\nAssuming a high score is good (i.e. we want to maximise for accuracy etc not minimise some loss function) no, this is definitely not the way to go, I hope I have convinced you of possible reasons why here.\n\n**EDIT**\nAnother possible reason I thought of after discovering the dataset is quite small is that the issue could be caused by data leakage if an upsampling technique was used. [Here](https://datascience.stackexchange.com/questions/17288/why-k-fold-cross-validation-cv-overfits-or-why-discrepancy-occurs-between-cv?noredirect=1&lq=1) is a very good discussion but basically if you upsample from a pool of training data before performing cv then your model could learn some traits of other _real_ training data that was split into other cvs. This would cause your cv performance to be far higher than it really should. The way around this is t upsample within each cv.\n\nTo summarise, I think your question is generally as to why a machine ...",
      "url": "https://datascience.stackexchange.com/questions/74791/why-when-my-local-cv-of-loss-decreases-my-leaderboards-loss-increases"
    },
    {
      "title": "Cross-Validation with Linear Regression - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=19dcfcf639fa5d9631a1:1:11427)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/jnikhilsai/cross-validation-with-linear-regression"
    },
    {
      "title": "A note on \"proper\" Cross-Validation techniques for Kaggle ...",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/getting-started/398047"
    },
    {
      "title": "Cross-Validation Techniques for Classification Models - Keylabs",
      "text": "CrossValidation Techniques for Classification Models|Keylabs\n# Cross-Validation Techniques for Classification Models\nNov 13, 2024\n[](https://www.facebook.com/sharer/sharer.php?u=https://keylabs.ai/blog/cross-validation-techniques-for-classification-models/)[](<https://twitter.com/intent/tweet?text=Cross-Validation Techniques for Classification Models&url=https://keylabs.ai/blog/cross-validation-techniques-for-classification-models/>)\n**Cross-validation**is a cornerstone in**machine learning**, providing a solid framework for evaluating and refining**classification models**. By using different**cross-validation**methods, you can enhance your model's accuracy, avoid overfitting, and ensure it performs well on new data.\nIn this detailed guide, we'll dive into the world of cross-validation techniques for**classification models**. You'll learn from the basics of holdout validation to advanced methods like stratified k-fold and**time series cross-validation**. You'll discover how to select the best approach for your dataset and model needs.\n### **Key Takeaways**\n* Cross-validation is essential for accurate**model performance**assessment\n* **K-fold cross-validation**typically uses 5 or 10 folds for reliable results\n* Stratified k-fold maintains class balance in**imbalanced datasets**\n* **Time series cross-validation**accounts for sequential data patterns\n* Leave-one-out cross-validation is useful for**small datasets**\n* Different models may perform best with specific cross-validation techniques\n* Cross-validation helps prevent overfitting and improves model**generalization**[![Keylabs Demo](https://keylabs.ai/img/blog/kl.png)](https://keylabs.ai/contact_us.html?utm_source=blog&utm_medium=call&utm_campaign=meet)## **Introduction to Cross-Validation in Machine Learning**\nCross-validation is a key technique in**machine learning**, essential for evaluating**model performance**and avoiding overfitting. It involves splitting data into subsets for training and testing. This method provides a robust way to validate models. Let's dive into the core aspects of cross-validation and its importance in creating dependable machine learning models.\n### **Definition and Importance of Cross-Validation**\nCross-validation is a statistical method for[assessing a model's ability to generalize to unseen data](https://keylabs.ai/blog/improving-your-ai-models-accuracy-expert-tips/). It divides data into subsets, trains the model on some, and tests it on others. This approach gives a more accurate model performance estimate than a single train-test split.\n### **Role in Preventing Overfitting**\nOverfitting happens when a model excels on training data but fails on new data. Cross-validation prevents this by using multiple train-test splits. This ensures the model doesn't memorize specific patterns in a single dataset. It's critical for creating models that generalize well to new data.\n### **Significance in Model Evaluation**\nCross-validation is vital for**model evaluation**, providing a detailed assessment of performance across different data subsets. It helps identify issues with model stability and**generalization**capacity. Machine learning practitioners often use cross-validation to compare models and choose the most robust one for their problem.\n|Cross-Validation Technique|Description|Typical Usage|\nK-Fold|Splits data into K subsets, uses K-1 for training and 1 for testing|General purpose, balanced datasets|\nStratified K-Fold|Maintains**class distribution**in each fold|**Imbalanced datasets**|\nLeave-One-Out (**LOOCV**)|Uses N-1 samples for training, 1 for testing, repeated N times|**Small datasets**|\nTime Series Split|Respects temporal order of data|Time series data|\nBy using the right cross-validation techniques, you can enhance your**model validation**process. This leads to more reliable machine learning models that perform well across various datasets.\n## **Understanding the Basics of Classification Models**\n**Classification models**are vital in**supervised learning**for**predictive modeling**. They[sort data into set categories](https://keylabs.ai/blog/what-are-classification-models/), playing a key role in many fields. From identifying spam emails to diagnosing medical conditions, these models are indispensable.\nIn**supervised learning**, these models are trained on labeled data. They uncover patterns and connections between input features and output classes. This skill allows them to accurately predict outcomes on new, unseen data.\n* Support Vector Machines (**SVM**)\n* Logistic Regression\n* Decision Trees\n* Random Forests\nEach algorithm has its unique advantages and limitations. SVMs are great for high-dimensional data, while Random Forests are strong against overfitting. This diversity makes them suitable for various data types and problem areas.\nGrasping these foundational concepts is essential for successful**predictive modeling**projects. Knowing how classification models work helps you select the best algorithm and validation method for your needs.\n## **The Need for Robust Model Validation**\n**Model validation**is essential for creating dependable machine learning solutions. Simple train-test splits often fail to fully capture**data variability**. This can result in biased performance estimates and models that don't generalize well to new data.\n### **Limitations of Simple Train-Test Splits**\nTraditional holdout methods divide datasets into two parts, usually in a 70:30 or 80:20 ratio for training and testing. Though easy to set up, these methods can yield inconsistent results due to varying data point combinations. This highlights the need for more reliable validation techniques, as discussed in[model validation](https://www.linkedin.com/pulse/cross-validation-technique-ensuring-robust-model-performance).\n### **Addressing Data Variability**\nTo address**data variability**, data scientists often use**k-fold cross-validation**. This method divides the dataset into 'k' equal-sized folds and runs the model 'k' times. For example, with 1000 records and k = 10, the model is validated on 100 different datasets and trained on 900 in each iteration.\n### **Ensuring Model Generalization**\nRobust validation techniques ensure model**generalization**by exposing the algorithm to various data configurations. This method provides a more reliable performance assessment on unseen data. Stratified**k-fold cross-validation**, for instance, keeps**class distribution**consistent across folds, making it ideal for**imbalanced datasets**.\n|Validation Technique|Key Feature|Advantage|\nK-Fold Cross-Validation|Splits data into 'k' equal parts|Enhances consistency in evaluation|\nStratified K-Fold|Maintains**class distribution**|Handles imbalanced datasets|\nLeave-One-Out|Uses each data point as a fold|Maximizes training data usage|\n## **K-Fold Cross-Validation: A Cornerstone Technique**\n[![Embedded YouTube video](https://img.youtube.com/vi/3fzYdnuvEfk/0.jpg)](https://www.youtube.com/watch?v=3fzYdnuvEfk)\nK-fold cross-validation is a critical method in**model evaluation**. It divides data into K equal parts. The model is trained on K-1 parts and tested on the last part, repeated K times. With 87% of machine learning projects failing due to overfitting, this technique is essential for[detecting and addressing these issues](https://medium.com/@bididudy/the-essential-guide-to-k-fold-cross-validation-in-machine-learning-2bcb58c50578).\nChoosing k=10 is common among data scientists. It balances complexity and thorough evaluation. Averaging performance metrics across all folds gives a strong insight into model performance.\nK-fold cross-validation maximizes data usage. Unlike simple splits, it ensures every data point is used in both training and testing. This is beneficial with limited data, providing a more accurate model performance estimate.\nFor classification tasks, stratified K-fold cross-validation maintains class distribution. This is critical for imbalanced datasets, ensuring each fold mirrors the ov...",
      "url": "https://keylabs.ai/blog/cross-validation-techniques-for-classification-models"
    },
    {
      "title": "Overfitting the leaderboard",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=77c6c69f1305124c7a10:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/caseyftw/overfitting-the-leaderboard"
    },
    {
      "title": "How to Lead a Data Science Contest without Reading the Data - KDnuggets",
      "text": "# How to Lead a Data Science Contest without Reading the Data\n\n[<= Previous post](https://www.kdnuggets.com/2015/05/data-science-workforce-optimization-reducing-employee-attrition.html)\n\n[Next post =>](https://www.kdnuggets.com/2015/05/most-viewed-data-mining-videos-youtube.html)\n\n[Accuracy](https://www.kdnuggets.com/tag/accuracy), [Benchmark](https://www.kdnuggets.com/tag/benchmark), [Competition](https://www.kdnuggets.com/tag/competition), [Kaggle](https://www.kdnuggets.com/tag/kaggle), [Model Performance](https://www.kdnuggets.com/tag/model-performance)\n\nWe examine a \u201cwacky\u201d boosting method that lets you climb the public leaderboard without even looking at the data . But there is a catch, so read on before trying to win Kaggle competitions with this approach.\n\n* * *\n\n![c](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2012%2012'%3E%3C/svg%3E)[comments](https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html#comments)\n\n**By Moritz Hardt**\n\nMachine learning competitions have become an extremely popular format for\u00a0solving prediction and classification problems of all sorts. The most famous\u00a0example is perhaps the Netflix prize. An even better example is\u00a0[Kaggle](https://www.kaggle.com), an awesome startup that\u2019s\u00a0organized more than a hundred competitions over the past few years.\n\nThe central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of _holdout labels_ not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.\n\n![Heritage Prize public leaderboard](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20595%200'%3E%3C/svg%3E)\n\nPublic leaderboard of the Heritage Health Prize ( [Source](http://www.heritagehealthprize.com/c/hhp/leaderboard/public))\n\nIn this post, I will describe a method to climb the public leaderboard _without even looking at the data_. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle\u2019s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some challenges that while fundamental have only recently seen increased attention. A follow-up post will describe a [recent paper](http://arxiv.org/abs/1502.04585) with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.\n\nLet me be very clear that my point is _not_ to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I\u2019m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.\n\n**The Kaggle leaderboard mechanism**\n\nAt first sight, the Kaggle mechanism looks like the classic _holdout method_. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in \\[0,1\\]. Think of the score as prediction error (smaller is better). For concreteness, let\u2019s fix it to be the _misclassification rate_. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in \\[0,1\\].\n\nKaggle further splits its \\\\(N\\\\) private labels randomly into \\\\(n\\\\) holdout labels and \\\\(N-n\\\\) test labels. Typically, \\\\(n=0.3N\\\\). The public leaderboard is a sorting of all teams according to their score computed only on the \\\\(n\\\\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels. I will let \\\\(s\\_H(y)\\\\) denote the public score of a submission \\\\(y\\\\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.\n\n**The cautionary tale of wacky boosting**\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there\u2019s an unknown set of labels \\\\(y\\\\in\\\\{0,1\\\\}^N\\\\) that I need to predict. Well, I know nothing about \\\\(y\\\\). So here\u2019s what I\u2019m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we\u2019re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I\u2019m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here\u2019s what I do:\n\n**Algorithm** (Wacky Boosting):\n\n1. Choose \\\\(y\\_1,\\\\dots,y\\_k\\\\in\\\\{0,1\\\\}^N\\\\) uniformly at random.\n2. Let \\\\(I = \\\\{ i\\\\in\\[k\\] \\\\colon s\\_H(y\\_i) < 0.5 \\\\}\\\\).\n3. Output \\\\(\\\\hat y=\\\\mathrm{majority} \\\\{ y\\_i \\\\colon i \\\\in I \\\\} \\\\), where the majority is component-wise.\n\nLo and behold, this is what happens:\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20595%200'%3E%3C/svg%3E)\n\nIn this plot, \\\\(n=4000\\\\) and all numbers are averaged over 5 independent repetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would\u2019ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at _DeepCompeting.ly_, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso.\n\nTwo months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from _DeepCompeting.ly_ days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.\n\n**What just happened**\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \\\\(y\\_i\\\\) has loss around \\\\(1/2\\\\pm1/\\\\sqrt{n}\\\\). We\u2019re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \\\\(w\\_i\\\\) is roughly \\\\(1/2-c/\\\\sqrt{n}\\\\) for some positive constant \\\\(c>0\\\\). Put differently, each selected \\\\(y\\_i\\\\) is giving us a guess about each label in the unknown holdout set \\\\(H\\\\subseteq \\[N\\]\\\\) that\u2019s corre...",
      "url": "https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html"
    }
  ]
}