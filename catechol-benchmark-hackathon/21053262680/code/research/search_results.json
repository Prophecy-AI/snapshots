{
  "query": "What techniques do Kaggle winners use to reduce the gap between cross-validation and leaderboard score when predicting chemical properties for unseen molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners narrow the cross\u2011validation\u2011to\u2011leaderboard gap in molecular\u2011property contests by combining **graph\u2011based deep models with rigorous validation and post\u2011processing tricks**.  Most top solutions train several **graph neural networks (GNNs, message\u2011passing neural networks and SE\u2011GCNs)** that ingest the 3\u2011D geometry or SMILES strings of each molecule, because these architectures capture the relational chemistry that flat descriptors miss\u202f([github.com](https://github.com/mv-lab/CHAMPS-Kaggle19)).  To guard against over\u2011optimistic CV scores, they use **scaffold\u2011aware splits or other chemistry\u2011aware hold\u2011outs** that mimic the distribution of truly unseen compounds, and they often repeat the split many times to average out variance.\n\nAfter the individual models are tuned, winners **blend or stack** them into a strong ensemble and apply **label\u2011rescaling** (e.g., adjusting predictions to match the training\u2011set target distribution) before submission.  The 1st\u2011place solution for the 2025 Open Polymer Prediction competition explicitly describes a \u201cfairly strong ensemble \u2026 trained on only the host\u2019s original training data\u201d together with a label\u2011rescaling step that lifted the leaderboard score\u202f([kaggle.com](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/writeups/1st-place-solution)).  The NVIDIA \u201cKaggle Grandmasters Playbook\u201d further recommends systematic CV, careful feature leakage checks, and multi\u2011model stacking as proven ways to keep the public\u2011leaderboard metric aligned with true test performance\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\nFinally, experienced competitors treat the leaderboard as a **feedback loop**, monitoring score drift and iteratively refining their validation strategy rather than chasing every incremental leaderboard gain.  As a Kaggle participant who placed in the top\u202f5 of a molecular\u2011property challenge notes, this disciplined \u201cleaderboard\u2011aware\u201d workflow\u2014combined with robust CV, ensembling, and label adjustment\u2014has been key to shrinking the CV\u2011leaderboard gap\u202f([linkedin.com](https://www.linkedin.com/pulse/kaggle-molecular-competition-lessons-learned-from-finishing-nguyen)).",
      "url": ""
    },
    {
      "title": "1st Place Solution | Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/writeups/1st-place-solution"
    },
    {
      "title": "Predicting Molecular Properties. CHAMPS Kaggle competition 2019.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[mv-lab](https://github.com/mv-lab)/ **[CHAMPS-Kaggle19](https://github.com/mv-lab/CHAMPS-Kaggle19)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmv-lab%2FCHAMPS-Kaggle19) You must be signed in to change notification settings\n- [Fork\\\n3](https://github.com/login?return_to=%2Fmv-lab%2FCHAMPS-Kaggle19)\n- [Star\\\n2](https://github.com/login?return_to=%2Fmv-lab%2FCHAMPS-Kaggle19)\n\n\nPredicting Molecular Properties. CHAMPS Kaggle competition 2019.\n\n[2\\\nstars](https://github.com/mv-lab/CHAMPS-Kaggle19/stargazers) [3\\\nforks](https://github.com/mv-lab/CHAMPS-Kaggle19/forks) [Branches](https://github.com/mv-lab/CHAMPS-Kaggle19/branches) [Tags](https://github.com/mv-lab/CHAMPS-Kaggle19/tags) [Activity](https://github.com/mv-lab/CHAMPS-Kaggle19/activity)\n\n[Star](https://github.com/login?return_to=%2Fmv-lab%2FCHAMPS-Kaggle19)\n\n[Notifications](https://github.com/login?return_to=%2Fmv-lab%2FCHAMPS-Kaggle19) You must be signed in to change notification settings\n\n# mv-lab/CHAMPS-Kaggle19\n\nmaster\n\n[Branches](https://github.com/mv-lab/CHAMPS-Kaggle19/branches) [Tags](https://github.com/mv-lab/CHAMPS-Kaggle19/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[3 Commits](https://github.com/mv-lab/CHAMPS-Kaggle19/commits/master/) |\n| [GNN](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/GNN) | [GNN](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/GNN) |\n| [mpnn](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/mpnn) | [mpnn](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/mpnn) |\n| [semgcn](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/semgcn) | [semgcn](https://github.com/mv-lab/CHAMPS-Kaggle19/tree/master/semgcn) |\n| [.gitignore](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/.gitignore) | [.gitignore](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/.gitignore) |\n| [NN\\_model.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/NN_model.ipynb) | [NN\\_model.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/NN_model.ipynb) |\n| [README.md](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/README.md) | [README.md](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/README.md) |\n| [SchNet.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/SchNet.ipynb) | [SchNet.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/SchNet.ipynb) |\n| [lgbm\\_model.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/lgbm_model.ipynb) | [lgbm\\_model.ipynb](https://github.com/mv-lab/CHAMPS-Kaggle19/blob/master/lgbm_model.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# [CHAMPS Kaggle competition 2019](https://www.kaggle.com/c/champs-scalar-coupling)\n\n- [Kaggle](https://www.kaggle.com/)\n- [CHAMPS (CHemistry And Mathematics in Phase Space)](https://champsproject.com/)\n\n### Introduction\n\nThink you can use your data science smarts to make big predictions at a molecular level?\n\nThis challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.\nResearchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.\n\nThis competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds.\n\n**Your Challenge**\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\nUsing state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.\n\nA fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.\n\nUltimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.\n\n## About\n\nPredicting Molecular Properties. CHAMPS Kaggle competition 2019.\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/mv-lab/CHAMPS-Kaggle19/activity)\n\n### Stars\n\n[**2**\\\nstars](https://github.com/mv-lab/CHAMPS-Kaggle19/stargazers)\n\n### Watchers\n\n[**2**\\\nwatching](https://github.com/mv-lab/CHAMPS-Kaggle19/watchers)\n\n### Forks\n\n[**3**\\\nforks](https://github.com/mv-lab/CHAMPS-Kaggle19/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmv-lab%2FCHAMPS-Kaggle19&report=mv-lab+%28user%29)\n\n## [Releases](https://github.com/mv-lab/CHAMPS-Kaggle19/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/mv-lab/packages?repo_name=CHAMPS-Kaggle19)\n\nNo packages published\n\n## Languages\n\n- [Python68.7%](https://github.com/mv-lab/CHAMPS-Kaggle19/search?l=python)\n- [Jupyter Notebook31.3%](https://github.com/mv-lab/CHAMPS-Kaggle19/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/mv-lab/CHAMPS-Kaggle19"
    },
    {
      "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in ...",
      "text": "In the recent [Kaggle Predicting Molecular Properties Competition](https://www.kaggle.com/c/champs-scalar-coupling), my team ( [Lam](https://www.kaggle.com/lamdang), [Guillaume](https://www.kaggle.com/grjhuard) and me) has managed to finish in 5th place (out of 2749 teams). It is the first time I won a gold Kaggle medal and particularly in prize zone. Also it is the first competition that I spent seriously a lot of time and I learned a lot through it . Though I don't consider myself as a Kaggle expert by any means, I want to share some lessons, insights that hopefully can be helpful for others.\n\n## A little words about Kaggle\n\n### Kaggle is the best school, competitors are the best teachers and my teammates are the best competition companions I've had\n\nKaggle is undoubtedly a great platform with all sorts of interesting problems along with datasets, kernels and great discussions. Like what I saw in a post long time ago, Kaggle is definitely a home for data science enthusiasts all around the world where they spend their days and nights to challenge themselves. And now I would say that I'm very proud to be one of them. Since I'm not graduated from any school formation of data science like many of others, Kaggle comes to me as a place where I can learn many things, keep me motivated in the field which is moving a lot every day. The people like [Heng](https://www.kaggle.com/hengck23) are the best teachers I've had, not only from their insights and sharing about competitions but also from the way how they work.\n\nMoreover, one of the most important features I love about Kaggle is Leaderboard where we could see where we're standing compared to others. During this competition, I admit that the first thing I did when waking up every morning is looking at the leaderboard. Seeing many competitors passing above me in the ranking helps me to benchmark my skills, push me to learn and try new things which is very important in my career.\n\nSo if you really want to get into Machine learning or Data Science in general, I believe that Kaggle comes in as one of the best ways.\n\n## About the competition: Predicting Molecular Properties\n\nThis competition is sponsored by the the Chemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. It requires competitors to predict the magnetic interaction between two atoms in a molecule (i.e the scalar coupling constant). The objective of using machine learning models in this kind of project is to allow medicinal chemists to gain structural insights faster and cheaper than the quantum mechanic methods and to enable scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior. Such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. See more about it in [here](https://www.kaggle.com/c/champs-scalar-coupling)\n\n## Background\n\nAt first, my team didn't had any domain expertise, prior knowledge about chemistry or molecular properties. But we found this competition very interesting to discover a new application of machine learning. [Lam](https://www.kaggle.com/lamdang) is the first who started in our team and realized that using Graph Neural Network (GNN) gains better score than classical machine learning models (like Xgboost or LightGBM). He found out an interesting [paper](https://arxiv.org/abs/1812.05055) about using GNN for molecular properties and tried to implement it in this competition. After few submissions, he managed to get into top 3 in public Leaderboard at the beginning of the competition. [Guillaume](https://www.kaggle.com/grjhuard) and I joined it later with different angle of view. Firstly, I tried to experiment ideas of Heng, using [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) for which he published in the discussion forum (with his starter code). I also devoted an amount of my time to grasp some knowledge about molecular properties and their interaction. Thereafter, I read different relevant papers in this kind of task in order to better understand how GNN works. The interesting idea behind GNN from machine learning standpoint is finding a representation of a molecule as a graph with atoms as nodes and bonds as edges and how information flow between nodes-nodes, nodes-bonds or bonds-bonds. For example, the assumption that two nodes have a relationship and interact each other is expressed by an edge between them. Similarly, the absence of an edge expresses the assumption that the nodes have no relationship and should not influence each other directly. I also tried another deep learning architecture like [Schnet](https://arxiv.org/pdf/1806.01261.pdf) or adding another molecular features by reading discussions/kernels in kaggle forum before sticking into [MEGNET](https://arxiv.org/abs/1812.05055). Lam, Guillaume and I decided to merger as a team one month before competition end when each person has his own good competitive model. For more details, our solution write-up can be found [here](https://www.kaggle.com/c/champs-scalar-coupling/discussion/106864#614330)\n\nBelow are few lessons that I've learned so far from this competition:\n\n### 1\\. A good validation strategy is half of success\n\nI read an [interview](http://blog.kaggle.com/2018/05/07/profiling-top-kagglers-bestfitting-currently-1-in-the-world/) of first-ranking GrandMaster bestfitting about his strategy of winning competitions in Kaggle. He said that a good CV is half of success. I couldn't agree more. Every try we made, it should improve both on our local CV and on the public LB. In this competition, we set aside 5000 molecules for validation and 80000 ones for training. And luckily, validation score and public leaderboard score is very close so that we are very sure about evaluation of our models. This closeness makes us feel confident with our stacking strategy at the end.\n\nSo, always trust your CV more than the leaderboard. The public leaderboard represents only 29% of the actual test set, so you can't be sure about the quality of your solution based on this percentage. Sometimes your model might be great overall, but bad on the data, specially in the public test set.\n\nThe experience from [here](https://www.linkedin.com/pulse/my-team-won-20000-1st-place-kaggles-earthquake-corey-levinson/) and [recent finished competition](https://www.kaggle.com/c/severstal-steel-defect-detection) make this lesson more valuable.\n\n### 2\\. Classical Machine learning ideas possibly work in deep learning\n\nWhen using classical machine learning models like Xgboost or Lightgbm, we often heard many times about feature importance technique. While they are widely used in many tabular problems, it is less likely happen in deep learning. But my teammate Guillaume has proven the opposite. After testing feature importance (by randomize one feature at the prediction step), he noticed that the most important feature was by far the angle between an edge and the edge with the closest atom to the first edge. This insight gave us a 0.15 improvement for our best single model.\n\n### 3\\. Always keep updated state-of-the-art of the field (either NLP, Computer Vision or others)\n\nI encountered many data scientists who said that since they are only working in Computer Vision, they don't have any interest to invest their time in NLP models. For me, I don't feel that way. When this competition was finished and the solution of top teams were released, all top 6 teams, except our team, were using a technique that is recently very popular in NLP community - [Transformer](https://arxiv.org/abs/1706.03762). The way that they integrated Transformer in their model is quite eye opening for us.\n\n### 4\\. Test ideas with simple model before going bigger\n\nOne of the big mistakes I made during this competition is impl...",
      "url": "https://www.linkedin.com/pulse/kaggle-molecular-competition-lessons-learned-from-finishing-nguyen"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Molecular Property Prediction Challenge | Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://kaggle.com/competitions/molecular-property-prediction-challenge"
    },
    {
      "title": "Molecular Data Machine Learning | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=af403557b7a25be19e47:1:10825)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/competitions/molecular-machine-learning"
    },
    {
      "title": "Predicting molecule properties based on its SMILES - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p><div><a href=\"https://www.kaggle.com/cookies\"><p>Learn more</p></a><p>OK, Got it.</p></div></div><div><div><div><a href=\"https://www.kaggle.com/rmonge\"></a><p><span><span><span>Andr\u00e9 Oliveira </span></span><span> \u00b7 <span>4y ago</span> \u00b7 8,564 views</span></span></p><div><p></p></div></div></div><div><div><p></p><h2>Predicting molecule properties based on its SMILES</h2><p></p></div><div><p><a href=\"https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook\"><span>Notebook</span><span></span></a><a href=\"https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/input\"><span>Input</span><span></span></a><a href=\"https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/output\"><span>Output</span><span></span></a><a href=\"https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/log\"><span>Logs</span><span></span></a><a href=\"https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/comments\"><span>Comments (1)</span><span></span></a></p></div></div><div><div><div><h2>Runtime</h2><div><p>5s</p></div><div><h2>Input</h2><div><p><span>DATASETS</span></p><div><p><span></span></p><p>private-dataset</p><p></p></div><div><p><span></span></p><p>private-dataset</p><p></p></div></div></div><h2>Language</h2><p>Python</p></div><div><div></div></div></div><div><div><h2>License</h2><p>This Notebook has been released under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">Apache 2.0</a> open source license.</p></div><div><h2>Continue exploring</h2><ul><li><div><a><div><div><p></p></div><div><p>Input</p><p><span>2 files</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Output</p><p><span>1 file</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Logs</p><p><span>5.1 second run - successful</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Comments</p><p><span>1 comment</span></p></div></div></a></div></li></ul></div></div></div></div></div></div>",
      "url": "https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles"
    },
    {
      "title": "GitHub - tomruarol/Kaggle_Predicting_Molecular_Properties: Repository for the Kaggle Competition: Predicting Molecular Properties",
      "text": "[Skip to content](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[tomruarol](https://github.com/tomruarol)/ **[Kaggle\\_Predicting\\_Molecular\\_Properties](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties)\n- [Star\\\n2](https://github.com/login?return_to=%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties)\n\n\nRepository for the Kaggle Competition: Predicting Molecular Properties\n\n### License\n\n[MIT license](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/LICENSE)\n\n[2\\\nstars](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/stargazers) [1\\\nfork](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/forks) [Branches](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/branches) [Tags](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/tags) [Activity](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/activity)\n\n[Star](https://github.com/login?return_to=%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties)\n\n[Notifications](https://github.com/login?return_to=%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties) You must be signed in to change notification settings\n\n# tomruarol/Kaggle\\_Predicting\\_Molecular\\_Properties\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/branches) [Tags](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[52 Commits](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/commits/master/) |\n| ### [.ipynb\\_checkpoints](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/tree/master/.ipynb_checkpoints) | ### [.ipynb\\_checkpoints](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/tree/master/.ipynb_checkpoints) |  |  |\n| ### [LICENSE](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/LICENSE) | ### [LICENSE](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/LICENSE) |  |  |\n| ### [README.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/README.md) | ### [README.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/README.md) |  |  |\n| ### [XGB\\_V2.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/XGB_V2.ipynb) | ### [XGB\\_V2.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/XGB_V2.ipynb) |  |  |\n| ### [XGB\\_V3.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/XGB_V3.ipynb) | ### [XGB\\_V3.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/XGB_V3.ipynb) |  |  |\n| ### [c\\_XGB\\_V2.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/c_XGB_V2.ipynb) | ### [c\\_XGB\\_V2.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/c_XGB_V2.ipynb) |  |  |\n| ### [c\\_XGB\\_V3.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/c_XGB_V3.ipynb) | ### [c\\_XGB\\_V3.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/c_XGB_V3.ipynb) |  |  |\n| ### [custom\\_metrics.py](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/custom_metrics.py) | ### [custom\\_metrics.py](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/custom_metrics.py) |  |  |\n| ### [data.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/data.md) | ### [data.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/data.md) |  |  |\n| ### [description.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/description.md) | ### [description.md](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/description.md) |  |  |\n| ### [main.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main.ipynb) | ### [main.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main.ipynb) |  |  |\n| ### [main\\_LGBM.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main_LGBM.ipynb) | ### [main\\_LGBM.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main_LGBM.ipynb) |  |  |\n| ### [main\\_XGB.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main_XGB.ipynb) | ### [main\\_XGB.ipynb](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/main_XGB.ipynb) |  |  |\n| ### [modeling.py](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/modeling.py) | ### [modeling.py](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/blob/master/modeling.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle Predicting Molecular Properties\n\nRepository for the Kaggle Competition: Predicting Molecular Properties\n\n## About\n\nRepository for the Kaggle Competition: Predicting Molecular Properties\n\n### Resources\n\n[Readme](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties#MIT-1-ov-file)\n\n[Activity](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/activity)\n\n### Stars\n\n[**2**\\\nstars](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/watchers)\n\n### Forks\n\n[**1**\\\nfork](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ftomruarol%2FKaggle_Predicting_Molecular_Properties&report=tomruarol+%28user%29)\n\n## [Releases](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/tomruarol/packages?repo_name=Kaggle_Predicting_Molecular_Properties)\n\nNo packages published\n\n## Languages\n\n- [Jupyter Notebook99.6%](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/search?l=jupyter-notebook)\n- [Python0.4%](https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/tomruarol/Kaggle_Predicting_Molecular_Properties"
    }
  ]
}