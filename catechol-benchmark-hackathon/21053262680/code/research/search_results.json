{
  "query": "What techniques do Kaggle competition winners use to reduce the gap between cross-validation score and leaderboard score when there is distribution shift?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners narrow the CV\u2011to\u2011leaderboard gap mainly by **forcing their validation to look like the hidden test set** and then **adapting the model at test time**.  A common first step is to build a \u201clocal leaderboard\u201d \u2013 either by sampling a hold\u2011out slice that matches the public\u2011test distribution or by augmenting the training data so its feature and label statistics mirror those of the competition\u2019s test split\u202f([Kaggle Winning Solutions](https://www.kaggle.com/code/tahaalselwii/kaggle-winning-solutions-ai-trends-insights)).  When the shift is more subtle (e.g., changes in input space or label priors) competitors use **stratified or time\u2011aware CV folds** that respect the observed drift, and they often apply **label\u2011shift re\u2011weighting or prior correction** to align the training label distribution with the test one\u202f([6th\u2011place solution](https://www.kaggle.com/competitions/birdsong-recognition/writeups/deep-6th-place-solution-and-some-thoughts)).  \n\nBeyond better validation, top teams employ **test\u2011time adaptation**: lightweight refinement steps such as adjusting graph edges to the test\u2011time connectivity pattern or taking a few gradient steps on an auxiliary self\u2011supervised loss to improve representations without using true labels\u202f([OpenReview \u2013 test\u2011time refinement](https://openreview.net/forum?id=Xk9Q0CrJQc)).  They also **stack and blend many diverse models** (different architectures, feature sets, or data\u2011augmentation pipelines) so that any single distribution mismatch is averaged out, and they often **pseudo\u2011label the test data** to further align training and inference distributions\u202f([Grandmasters Playbook](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).  Together, these practices turn the CV score into a more reliable proxy for the leaderboard despite underlying distribution shifts.",
      "url": ""
    },
    {
      "title": "Kaggle Winning Solutions: AI Trends & Insights",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/tahaalselwii/kaggle-winning-solutions-ai-trends-insights"
    },
    {
      "title": "6th place solution and some thoughts - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions/birdsong-recognition/writeups/deep-6th-place-solution-and-some-thoughts"
    },
    {
      "title": "Understanding and Mitigating Distribution Shifts for Machine...",
      "text": "<div><div><p><strong>Keywords:</strong> <span>machine learning force fields, test-time training, distribution shifts</span></p><p><strong>TL;DR:</strong> <span>Benchmarks are established showing that machine learning forcefields suffer from common distribution shifts, and test-time refinement methods are developed to address the distribution shifts.</span></p><div><p><strong>Abstract:</strong></p><p>Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use ab initio labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. It can be applied to any existing pre-trained model to mitigate connectivity distribution shifts. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective. Inspired by previous test-time training works in computer vision, we replace self-supervised objectives at test time with an objective that uses an efficient prior to address distribution shifts. Our test-time refinement strategies can reduce force errors by an order of magnitude on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs.</p></div><p><strong>Primary Area:</strong> <span>applications to physical sciences (physics, chemistry, biology, etc.)</span></p><p><strong>Code Of Ethics:</strong> <span>I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.</span></p><p><strong>Reciprocal Reviewing:</strong> <span>I understand the reciprocal reviewing requirement as described on <a href=\"https://iclr.cc/Conferences/2025/CallForPapers\">https://iclr.cc/Conferences/2025/CallForPapers</a>. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at <a href=\"https://forms.gle/Huojr6VjkFxiQsUp6\">https://forms.gle/Huojr6VjkFxiQsUp6</a>.</span></p><p><strong>Anonymous Url:</strong> <span>I certify that there is no URL (e.g., github page) that could be used to find authors\u2019 identity.</span></p><p><strong>No Acknowledgement Section:</strong> <span>I certify that there is no acknowledgement section in this submission for double blind review.</span></p><p><strong>Submission Number:</strong> <span>8481</span></p></div></div>",
      "url": "https://openreview.net/forum?id=Xk9Q0CrJQc"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "How to Score better in Kaggle Competition",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/vithal2311/how-to-score-better-in-kaggle-competition"
    },
    {
      "title": "What methods do top Kagglers employ for score gain?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [What methods do top Kagglers employ for score gain?](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked1 year, 8 months ago\n\nModified [1 year, 8 months ago](https://datascience.stackexchange.com/datascience.stackexchange.com?lastactivity)\n\nViewed\n424 times\n\n2\n\n$\\\\begingroup$\n\nI\u2019m currently taking a course on ML and part of my final grade is my position on a Kaggle competition (private one) regarding a classification task. The majority of groups tend to have a similar public score but there are some that spike through the rest. I am left wondering what techniques data scientists employ to increase their scores.\n\nI am aware that a thoughtful pre-processing of the data is of chief importance as well as techniques such as grid search which help us find the best hyperparameters. This question is more about some out-of-the-box techniques that you employ when your aim is to increase your final score, for example, in the setting of a Kaggle competition.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/124709)\n\n[Improve this question](https://datascience.stackexchange.com/posts/124709/edit)\n\nFollow\n\n[edited Nov 22, 2023 at 23:36](https://datascience.stackexchange.com/posts/124709/revisions)\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac)\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\nasked Nov 22, 2023 at 19:32\n\n[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela) Frederico Portela\n\n4122 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n4\n\n$\\\\begingroup$\n\nTop Kagglers often employ a combination of traditional machine learning techniques and creative, out-of-the-box strategies to gain an edge in competitions. Here are some techniques:\n\n**1\\. Ensemble Methods**\n\n- _Stacking/Blending:_ Combining predictions from multiple models. This involves training several models and then using another model to predict the target variable based on the predictions of the initial models.\n\n- _Bagging and Boosting:_ Techniques like Random Forest (bagging) and Gradient Boosting (boosting) can significantly improve performance.\n\n\n**2\\. Feature Engineering**\n\n- _Creating new features:_ Deriving meaningful features from existing ones can provide valuable information to the model.\n\n- _Dimensionality Reduction:_ Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can be useful.\n\n\n**3\\. Advanced Preprocessing**\n\n- _Handling missing data:_ Creative ways to impute missing values.\n\n- _Outlier Detection and Treatment:_ Identifying and addressing outliers can improve model robustness.\n\n\n**4\\. Model Hyperparameter Tuning**\n\n- _Bayesian Optimization:_ Efficiently searching hyperparameter space.\n\n- _Optuna, Hyperopt:_ Libraries for hyperparameter optimization.\n\n\n**5\\. Neural Architecture Search (NAS)**\n\n- _Automated Model Design:_ Techniques to automatically search for the best neural network architecture.\n\n**6\\. Transfer Learning**\n\n- _Using pre-trained models:_ Leveraging models trained on large datasets and fine-tuning them for the specific task at hand.\n\n**7\\. Data Augmentation**\n\n- _Increasing Training Data:_ Applying transformations (rotations, flips, etc.) to artificially increase the size of the training dataset.\n\n**8\\. Domain-Specific Knowledge**\n\n- _Understanding the problem domain:_ Incorporating domain-specific knowledge can lead to better feature engineering and model performance.\n\n**9\\. Advanced Modeling Techniques**\n\n- _Neural Networks Architectures:_ Exploring different architectures, such as attention mechanisms, transformers, etc.\n- _XGBoost, LightGBM, CatBoost:_ Gradient boosting libraries that are often used for tabular data.\n\n**10\\. Time Series Techniques**\n\n- _LSTM, GRU:_ For sequential data.\n\n- _Feature lagging and rolling statistics:_ Utilizing information from past time points.\n\n\n**11\\. Post-Processing**\n\n- _Calibration:_ Adjusting predicted probabilities to improve the model's reliability.\n- _Threshold tuning:_ Adjusting the classification threshold based on the specific needs of the task.\n\n**12\\. Collaboration and Knowledge Sharing**\n\n- _Participating in discussions:_ Sharing insights and learning from others on Kaggle forums.\n- _Team Collaboration:_ Forming teams to combine diverse skills and perspectives.\n\n[Share](https://datascience.stackexchange.com/a/124714)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124714/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:43\n\n[lvvittor](https://datascience.stackexchange.com/users/156336/lvvittor) lvvittor\n\n10111 bronze badge\n\n$\\\\endgroup$\n\n1\n\n- 1\n\n$\\\\begingroup$Thank you very much. There are indeed here some techniques i was not aware of.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n4\n\n$\\\\begingroup$\n\nThere are many methods that can be employed to increase your score on a Kaggle competition. Here are a few examples:\n\n**Advanced classification techniques:** Using advanced classification techniques such as weighted average ensemble, stacked generalization ensemble, and power average ensemble can help improve your model's performance.\n\n**Data augmentation:** Techniques such as random rotation, hue adjustments, saturation adjustments, contrast adjustments, brightness adjustments, cropping, and more can help improve your model's accuracy when dealing with image data.\n\n**Text augmentation:** When dealing with text data, techniques such as exchanging words with synonyms, noising in RNN, and translating to other languages and back can help augment your training data and improve your model's performance.\n\n**External datasets:** Using external datasets that contain variables that influence the predicate variable can help increase the performance of your model pre-processing.\n\nI have even seen that in some cases, especially at the beginning of the site, teams took advantage of finding data leakage. For example, identifying customer's id that may potentially be linked to temporal patterns and thus model's prediction\n\nThis [discussion](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111308) may give you a fine summary of most of the techniques I mentioned above.\n\nI hope it helps.\n\n[Share](https://datascience.stackexchange.com/a/124715)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124715/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:44\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac) Multivac\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Thank you for this. I'll take a look at the discussion.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/logi...",
      "url": "https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain"
    },
    {
      "title": "How to win your first Kaggle competition? - dataroots",
      "text": "How to win your first Kaggle competition?\n[![symbol](https://dataroots.io/_next/static/media/symbol-rainbow.66f0e23b.svg)](https://dataroots.io/)\n![dataroots hero](https://dataroots.io/_next/static/media/glow-bottom-green.eb20c0f6.svg)\n# How to win your first Kaggle competition?\n[Get In Touch-&gt;](https://dataroots.io/contact-us)\n[Careers](https://dataroots.io/careers)\n[DNAOur DNA](https://dataroots.io/our-dna)\n[file-articleBlog](https://dataroots.io/blog)\n[podcastPodcast](https://dataroots.io/datatopics)\nByAdrien Debray, Johannes Lootens\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n## All you need to know about Kaggle competitions\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled-1.png)Kaggle competition overview page\n\ud83d\udca1Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\nFirstly there are the**getting-started competitions**, such as the[Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io)or[Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io)ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the**community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\nWhat actually tends to attract people to Kaggle are the**competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\nEvery single one of those competitions is defined by a**dataset**and an**evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\nWhile the********************train set********************is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the**public leaderboard test set**which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the**private leaderboard test set.**This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n### Notebooks\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n## How to take the W in a Kaggle competition ?\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\nWe participated in the[Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io)competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n### 1. Have a good understanding of the competition and how to tackle the problem\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n* Read the competition overview and linked resources thoroughly\n* Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n* Check existing literature on approaches that were tried/succeeded in solving this or similar problems### 2. Get inspired by other participants\u2019 work to get started\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled--1--1.png)Go in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\nBased on your readings, choose a clear and simple notebook with a decent LB score as**baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n### 3. Improve your model in an efficient way\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n##### Create datasets for intermediate results / preprocessed data\nSaved preprocessed datasets and trained models will make your results comparis...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    }
  ]
}