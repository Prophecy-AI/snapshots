{
  "query": "How do you reduce the intercept in a CV-LB relationship when predicting for unseen chemical compounds in machine learning competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the intercept (global bias) of a compound\u2011property model\u2014often called the CV\u2011LB (cross\u2011validation\u202f/\u202fleave\u2011blank) relationship\u2014can be done by explicitly controlling the model\u2019s base\u2011score and, if needed, recalibrating the predictions before they are applied to truly unseen chemicals\u202f([XGBoost docs](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html)).\n\n**Steps**\n\n1. **Train the model normally** so XGBoost estimates the intercept (`base_score`) from the training targets.  \n   ```python\n   import xgboost as xgb\n   reg = xgb.XGBRegressor()\n   reg.fit(X_train, y_train)\n   ```\n\n2. **Force a smaller (or zero) intercept** by overriding `base_score` after the first fit.  \n   ```python\n   reg.set_params(base_score=0.0)          # or any small constant you want\n   reg.fit(X_train, y_train)               # re\u2011fit with the new base_score\n   ```  \n   Setting `base_score` directly replaces the automatically\u2011estimated intercept\u202f([XGBoost docs](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html)).\n\n3. **Optionally supply a per\u2011sample bias (`base_margin`)** derived from a prior model or from the training\u2011set mean. This lets you \u201cstack\u201d a second model that starts from a reduced global bias.  \n   ```python\n   raw_pred = reg.predict(X_train, output_margin=True)   # raw (bias\u2011included) scores\n   reg2 = xgb.XGBRegressor()\n   reg2.fit(X_train, y_train, base_margin=raw_pred)      # raw_pred becomes the new bias\n   ```\n\n4. **Calibrate the final probabilities or regression outputs** on a held\u2011out validation split that mimics the distribution of unseen compounds. Use scikit\u2011learn\u2019s `CalibratedClassifierCV` (sigmoid or isotonic scaling) to adjust systematic offsets, which effectively lowers the intercept on new data\u202f([scikit\u2011learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)).\n\n5. **Guard against over\u2011optimistic similarity** between training and test compounds. When constructing the validation split, enforce low chemical similarity (e.g., Tanimoto\u202f<\u202f0.4) so the calibrated intercept truly reflects performance on novel structures\u202f([CPI\u2011N N paper](https://arxiv.org/pdf/2202.00001)).  \n\n6. **Evaluate on a truly unseen test set**; if the intercept is still high, repeat steps\u202f2\u20134 with a smaller `base_score` or stronger calibration until the mean prediction error on the blind set is minimized.  \n\nBy explicitly setting `base_score`, optionally using `base_margin`, and applying a calibration layer on a similarity\u2011controlled validation split, you can systematically reduce the intercept in the CV\u2011LB relationship for unseen chemical compounds in competition settings.",
      "url": ""
    },
    {
      "title": "[PDF] pound-protein interaction prediction methods - arXiv",
      "text": "Bioinformatics, YYYY, 0\u20130\ndoi: 10.1093/bioinformatics/xxxxx\nAdvance Access Publication Date: DD Month YYYY\nManuscript Category\nSubject Section\nInsights into performance evaluation of com\u0002pound-protein interaction prediction methods\nAdiba Yaseen1,*, Imran Amin2, Naeem Akhter1, Asa Ben-Hur3 and Fayyaz \nMinhas4*\n1Department of Computer and Information Sciences (DCIS), Pakistan Institute of Engineering and Ap\u0002plied Sciences (PIEAS), Islamabad, Pakistan.\n2National Institute for Biotechnology and Genetic Engineering, Faisalabad, Pakistan.\n3Department of Computer Science, Colorado State University, Fort Collins, USA.\n4Tissue Image Analytics Centre, Department of Computer Science, University of Warwick, Coventry, \nUK.\n*To whom correspondence should be addressed.\nReceived on XXXXX; revised on XXXXX; accepted on XXXXX\nAbstract\nMotivation:\nMachine learning based prediction of compound-protein interactions (CPIs) is important for drug de\u0002sign, screening and repurposing studies and can improve the efficiency and cost-effectiveness of wet \nlab assays. Despite the publication of many research papers reporting CPI predictors in the recent \nyears, we have observed a number of fundamental issues in experiment design that lead to over opti\u0002mistic estimates of model performance.\nResults:\nIn this paper, we analyze the impact of several important factors affecting generalization performance \nof CPI predictors that are overlooked in existing work:\n1. Similarity between training and test examples in cross-validation\n2. The strategy for generating negative examples, in the absence of experimentally verified neg\u0002ative examples.\n3. Choice of evaluation protocols and performance metrics and their alignment with real-world \nuse of CPI predictors in screening large compound libraries.\nUsing both an existing state-of-the-art method (CPI-NN) and a proposed kernel based approach,\nwe have found that assessment of predictive performance of CPI predictors requires careful control \nover similarity between training and test examples. We also show that random pairing for generating \nsynthetic negative examples for training and performance evaluation results in models with better gen\u0002eralization performance in comparison to more sophisticated strategies used in existing studies. Fur\u0002thermore, we have found that our kernel based approach, despite its simple design, exceeds the pre\u0002diction performance of CPI-NN. We have used the proposed model for compound screening of several \nproteins including SARS-CoV-2 Spike and Human ACE2 proteins and found strong evidence in support \nof its top hits.\nAvailability: Code and raw experimental results available at https://github.com/adibayaseen/HKRCPI\nContact: Fayyaz.minhas@warwick.ac.uk \nSupplementary information: Supplementary data files are available as part of the GitHub repository.\n1 Introduction \nCompound Protein Interaction (CPI) prediction is an important task in \nTarget Compound Screening for identifying protein targets of com\u0002pounds, drug design, and drug repurposing studies (Schirle and Jenkins \n2016). Affinity chromatography (Broach and Thorner 1996) and pro\u0002tein microarrays (Lee and Lee 2016; Zhao et al. 2021) are among the \nmost frequently used experimental methods for the identification of \nCPIs. However, such wet-lab approaches can be expensive and time\u0002taking (W. Zhang, Pei, and Lai 2017) (Paul et al. 2010). The emergence \nof pandemics such as Ebola and COVID-19 and the global challenge of \nantimicrobial resistance have highlighted the need of improving effi\u0002ciency and throughput in drug design (Thafar et al. 2019). Conse\u0002quently, CPI prediction using computational methods has become an \nattractive area of research (X. Chen et al. 2016) as such approaches can \nimprove the cost, time, and efficiency of drug discovery in contrast to \nexperimental methods (Mazandu et al. 2018). \n1.1 Approaches for Compound Protein Interaction \nPrediction\nConventionally, structure-based and ligand-based virtual screening are \nthe most well-researched areas of drug discovery (Lim et al. 2021). \nHowever, these methods require the three-dimensional (3D) structure \nof the protein of interest. As a consequence, machine learning (ML) \nbased methods that use sequence characteristics of proteins and chem\u0002ical structural representations of compounds for interaction prediction \nhave been developed (Bredel and Jacoby 2004) (Bleakley and \nYamanishi 2009; G\u00f6nen 2012; Y. Wang and Zeng 2013). Based on the \nrepresentation of proteins and compounds used in them, these compu\u0002tational methods can be categorized into three main classes: feature rep\u0002resentation-based methods, similarity-based methods, and end-to-end \nlearning methods. Similarity-based methods are based on the assump\u0002tion that similar drugs tend to target similar proteins and vice versa (R. \nChen et al. 2018). In feature representation-based approaches (Ding et \nal. 2014), features from compounds and proteins are extracted and fed \nto a machine learning model such as the nearest neighbor predictor, bi\u0002partite local models (Bleakley and Yamanishi 2009), Bayesian matrix \nfactorization-based kernels (G\u00f6nen 2012), gaussian contact profiling \n(van Laarhoven, Nabuurs, and Marchiori 2011), pairwise kernel \nmethod (Jacob and Vert 2008), etc. Comparative analysis by Ding et \nal. has shown that PKM outperforms other approaches (Ding et al. \n2014).\nIn recent years, researchers have developed multiple deep learning \nmodels for CPI prediction. DeepDTA (\u00d6zt\u00fcrk, \u00d6zg\u00fcr, and Ozkirimli \n2018) extracts real-valued sparse feature representations of proteins as \nwell as compounds using convolutional neural networks (CNNs) and \nappends these features through the final fully connected layer. Wid\u0002eDTA (\u00d6zt\u00fcrk, Ozkirimli, and \u00d6zg\u00fcr 2019) and Conv-DTI (S. Wang \net al. 2020) also used an analogous idea with additional features, ligand \nstructural similarity, and information about protein domains and motifs \nto enhance model accuracy. For representation compound structures, \nGraphDTA (Nguyen et al. 2021) and CPI\u2013GNN (Tsubaki, Tomii, and \nSese 2019) used novel graph neural networks (X.-M. Zhang et al. 2021) \n(GNNs) as an alternative to CNNs. CPI-NN was shown to outperform \nother embedding-based methods. \n1.2 Issues in performance assessment of CPI models\nDespite the increased sophistication of CPI models through deep learn\u0002ing, the generalization performance of existing approaches on \nindependent or real-world datasets is still not perfect (Riley 2019). One \nof the fundamental issues behind this is biased and overly-optimistic \nperformance assessment strategies arising from the use of unsuitable \ndatasets, poor non-redundancy control in train-test data splitting in \ncross-validation, improper procedures for generation of negative exam\u0002ple, lack of independent test sets, and choice of performance metrics. \nHere, we discuss each of these issues in further detail. \nA number of ML-based CPI prediction models have used the MUV \n(Rohrer and Baumann 2009), DUD-E (Mysinger et al. 2012) and Hu\u0002man-CPI datasets (Tsubaki, Tomii, and Sese 2019; Liu et al. 2015) for \nmodel training and performance evaluation. However, these datasets do \nnot contain true or experimentally verified negative examples and may \nhave a large degree of redundancy between proteins and compounds \nwhich can lead to biased machine learning models (Lieyang Chen et al. \n2019), (Sieg, Flachsenberg, and Rarey 2019) (Lifan Chen et al. 2020).\nAnother issue associated with the performance assessment of ML \nCPI models is the protocol used for generating negative examples. As \nthere is no standardized dataset of negative examples for compound\u0002protein interaction prediction, researchers in this domain resort to one \nof two approaches for the generation of \u201csynthetic\u201d negative examples \nfor training and performance assessment of machine learning models: \nRandom pairing and inter-class similarity-controlled negative example \ngeneration. In random pairing, proteins and compounds in the positiv...",
      "url": "https://arxiv.org/pdf/2202.00001"
    },
    {
      "title": "",
      "text": "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/index.html)\n- Intercept\n- [View page source](https://xgboost.readthedocs.io/en/stable/_sources/tutorials/intercept.rst.txt)\n* * *\n# Intercept [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#intercept)\nAdded in version 2.0.0.\nSince 2.0.0, XGBoost supports estimating the model intercept (named `base_score`)\nautomatically based on targets upon training. The behavior can be controlled by setting\n`base_score` to a constant value. The following snippet disables the automatic\nestimation:\n```\nimportxgboostasxgb\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\nIn addition, here 0.5 represents the value after applying the inverse link function. See\nthe end of the document for a description.\nOther than the `base_score`, users can also provide global bias via the data field\n`base_margin`, which is a vector or a matrix depending on the task. With multi-output\nand multi-class, the `base_margin` is a matrix with size `(n_samples, n_targets)` or\n`(n_samples, n_classes)`.\n```\nimportxgboostasxgb\nfromsklearn.datasetsimport make_regression\nX, y = make_regression()\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\nIt specifies the bias for each sample and can be used for stacking an XGBoost model on top\nof other models, see [Demo for boosting from prediction](https://xgboost.readthedocs.io/en/stable/python/examples/boost_from_prediction.html#sphx-glr-python-examples-boost-from-prediction-py) for a worked\nexample. When `base_margin` is specified, it automatically overrides the `base_score`\nparameter. If you are stacking XGBoost models, then the usage should be relatively\nstraightforward, with the previous model providing raw prediction and a new model using\nthe prediction as bias. For more customized inputs, users need to take extra care of the\nlink function. Let \\\\(F\\\\) be the model and \\\\(g\\\\) be the link function, since\n`base_score` is overridden when sample-specific `base_margin` is available, we will\nomit it here:\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i)\\\\\\]\nWhen base margin \\\\(b\\\\) is provided, it\u2019s added to the raw model output \\\\(F\\\\):\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i) + b\\_i\\\\\\]\nand the output of the final model is:\n\\\\\\[g^{-1}(F(x\\_i) + b\\_i)\\\\\\]\nUsing the gamma deviance objective `reg:gamma` as an example, which has a log link\nfunction, hence:\n\\\\\\[\\\\begin{split}\\\\ln{(E\\[y\\_i\\])} = F(x\\_i) + b\\_i \\\\\\\nE\\[y\\_i\\] = \\\\exp{(F(x\\_i) + b\\_i)}\\\\end{split}\\\\\\]\nAs a result, if you are feeding outputs from models like GLM with a corresponding\nobjective function, make sure the outputs are not yet transformed by the inverse link\n(activation).\nIn the case of `base_score` (intercept), it can be accessed through\n[`save_config()`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster.save_config) after estimation. Unlike the `base_margin`, the\nreturned value represents a value after applying inverse link. With logistic regression\nand the logit link function as an example, given the `base_score` as 0.5,\n\\\\(g(intercept) = logit(0.5) = 0\\\\) is added to the raw model output:\n\\\\\\[E\\[y\\_i\\] = g^{-1}{(F(x\\_i) + g(intercept))}\\\\\\]\nand 0.5 is the same as \\\\(base\\\\\\_score = g^{-1}(0) = 0.5\\\\). This is more intuitive if\nyou remove the model and consider only the intercept, which is estimated before the model\nis fitted:\n\\\\\\[\\\\begin{split}E\\[y\\] = g^{-1}{(g(intercept))} \\\\\\\nE\\[y\\] = intercept\\\\end{split}\\\\\\]\nFor some objectives like MAE, there are close solutions, while for others it\u2019s estimated\nwith one step Newton method.\n## Offset [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#offset)\nThe `base_margin` is a form of `offset` in GLM. Using the Poisson objective as an\nexample, we might want to model the rate instead of the count:\n\\\\\\[rate = \\\\frac{count}{exposure}\\\\\\]\nAnd the offset is defined as log link applied to the exposure variable:\n\\\\(\\\\ln{exposure}\\\\). Let \\\\(c\\\\) be the count and \\\\(\\\\gamma\\\\) be the exposure,\nsubstituting the response \\\\(y\\\\) in our previous formulation of base margin:\n\\\\\\[g(\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}) = F(x\\_i)\\\\\\]\nSubstitute \\\\(g\\\\) with \\\\(\\\\ln\\\\) for Poisson regression:\n\\\\\\[\\\\ln{\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}} = F(x\\_i)\\\\\\]\nWe have:\n\\\\\\[\\\\begin{split}E\\[c\\_i\\] &= \\\\exp{(F(x\\_i) + \\\\ln{\\\\gamma\\_i})} \\\\\\\nE\\[c\\_i\\] &= g^{-1}(F(x\\_i) + g(\\\\gamma\\_i))\\\\end{split}\\\\\\]\nAs you can see, we can use the `base_margin` for modeling with offset similar to GLMs",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html"
    },
    {
      "title": "CalibratedClassifierCV #",
      "text": "CalibratedClassifierCV &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# CalibratedClassifierCV[#](#calibratedclassifiercv)\n*class*sklearn.calibration.CalibratedClassifierCV(*estimator=None*,*\\**,*method='sigmoid'*,*cv=None*,*n\\_jobs=None*,*ensemble='auto'*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/calibration.py#L72)[#](#sklearn.calibration.CalibratedClassifierCV)\nCalibrate probabilities using isotonic, sigmoid, or temperature scaling.\nThis class uses cross-validation to both estimate the parameters of a\nclassifier and subsequently calibrate a classifier. With`ensemble=True`, for each cv split it\nfits a copy of the base estimator to the training subset, and calibrates it\nusing the testing subset. For prediction, predicted probabilities are\naveraged across these individual calibrated classifiers. When`ensemble=False`, cross-validation is used to obtain unbiased predictions,\nvia[`cross\\_val\\_predict`](sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict), which are then\nused for calibration. For prediction, the base estimator, trained using all\nthe data, is used. This is the prediction method implemented when`probabilities=True`for[`SVC`](sklearn.svm.SVC.html#sklearn.svm.SVC)and[`NuSVC`](sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)estimators (see[User Guide](../svm.html#scores-probabilities)for details).\nAlready fitted classifiers can be calibrated by wrapping the model in a[`FrozenEstimator`](sklearn.frozen.FrozenEstimator.html#sklearn.frozen.FrozenEstimator). In this case all provided\ndata is used for calibration. The user has to take care manually that data\nfor model fitting and calibration are disjoint.\nThe calibration is based on the[decision\\_function](../../glossary.html#term-decision_function)method of the`estimator`if it exists, else on[predict\\_proba](../../glossary.html#term-predict_proba).\nRead more in the[User Guide](../calibration.html#calibration).\nIn order to learn more on the CalibratedClassifierCV class, see the\nfollowing calibration examples:[Probability calibration of classifiers](../../auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py),[Probability Calibration curves](../../auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py), and[Probability Calibration for 3-class classification](../../auto_examples/calibration/plot_calibration_multiclass.html#sphx-glr-auto-examples-calibration-plot-calibration-multiclass-py).\nParameters:**estimator**estimator instance, default=None\nThe classifier whose output need to be calibrated to provide more\naccurate`predict\\_proba`outputs. The default classifier is\na[`LinearSVC`](sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC).\nAdded in version 1.2.\n**method**{\u2018sigmoid\u2019, \u2018isotonic\u2019, \u2018temperature\u2019}, default=\u2019sigmoid\u2019\nThe method to use for calibration. Can be:\n* \u2018sigmoid\u2019, which corresponds to Platt\u2019s method (i.e. a binary logistic\nregression model).\n* \u2018isotonic\u2019, which is a non-parametric approach.\n* \u2018temperature\u2019, temperature scaling.\nSigmoid and isotonic calibration methods natively support only binary\nclassifiers and extend to multi-class classification using a One-vs-Rest (OvR)\nstrategy with post-hoc renormalization, i.e., adjusting the probabilities after\ncalibration to ensure they sum up to 1.\nIn contrast, temperature scaling naturally supports multi-class calibration by\napplying`softmax(classifier\\_logits/T)`with a value of`T`(temperature)\nthat optimizes the log loss.\nFor very uncalibrated classifiers on very imbalanced datasets, sigmoid\ncalibration might be preferred because it fits an additional intercept\nparameter. This helps shift decision boundaries appropriately when the\nclassifier being calibrated is biased towards the majority class.\nIsotonic calibration is not recommended when the number of calibration samples\nis too low`(\u226a1000)`since it then tends to overfit.\nChanged in version 1.8:Added option \u2018temperature\u2019.\n**cv**int, cross-validation generator, or iterable, default=None\nDetermines the cross-validation splitting strategy.\nPossible inputs for cv are:\n* None, to use the default 5-fold cross-validation,\n* integer, to specify the number of folds.\n* [CV splitter](../../glossary.html#term-CV-splitter),\n* An iterable yielding (train, test) splits as arrays of indices.\nFor integer/None inputs, if`y`is binary or multiclass,[`StratifiedKFold`](sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)is used. If`y`is\nneither binary nor multiclass,[`KFold`](sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)is used.\nRefer to the[User Guide](../cross_validation.html#cross-validation)for the various\ncross-validation strategies that can be used here.\nChanged in version 0.22:`cv`default value if None changed from 3-fold to 5-fold.\n**n\\_jobs**int, default=None\nNumber of jobs to run in parallel.`None`means 1 unless in a[`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/generated/joblib.parallel_backend.html#joblib.parallel_backend)context.`-1`means using all processors.\nBase estimator clones are fitted in parallel across cross-validation\niterations.\nSee[Glossary](../../glossary.html#term-n_jobs)for more details.\nAdded in version 0.24.\n**ensemble**bool, or \u201cauto\u201d, default=\u201dauto\u201d\nDetermines how the calibrator is fitted.\n\u201cauto\u201d will use`False`if the`estimator`is a[`FrozenEstimator`](sklearn.frozen.FrozenEstimator.html#sklearn.frozen.FrozenEstimator), and`True`otherwise.\nIf`True`, the`estimator`is fitted using training data, and\ncalibrated using testing data, for each`cv`fold. The final estimator\nis an ensemble of`n\\_cv`fitted classifier and calibrator pairs, where`n\\_cv`is the number of cross-validation folds. The output is the\naverage predicted probabilities of all pairs.\nIf`False`,`cv`is used to compute unbiased predictions, via[`cross\\_val\\_predict`](sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict), which are then\nused for calibration. At prediction time, the classifier used is the`estimator`trained on all the data.\nNote that this method is also internally implemented in[`sklearn.svm`](../../api/sklearn.svm.html#module-sklearn.svm)estimators with the`probabilities=True`parameter.\nAdded in version 0.24.\nChanged in version 1.6:`&quot;auto&quot;`option is added and is the default.\nAttributes:**classes\\_**ndarray of shape (n\\_classes,)\nThe class labels.\n**n\\_features\\_in\\_**int\nNumber of features seen during[fit](../../glossary.html#term-fit). Only defined if the\nunderlying estimator exposes such an attribute when fit.\nAdded in version 0.24.\n**feature\\_names\\_in\\_**ndarray of shape (`n\\_features\\_in\\_`,)\nNames of features seen during[fit](../../glossary.html#term-fit). Only defined if the\nunderlying estimator exposes such an attribute when fit.\nAdded in version 1.0.\n**calibrated\\_classifiers\\_**list (len() equal to cv or 1 if`ensemble=False`)\nThe list of classifier and calibrator pairs.\n* When`ensemble=True`,`n\\_cv`fitted`estimator`and calibrator pairs.`n\\_cv`is the number of cross-validation folds.\n* When`ensemble=False`, the`estimator`, fitted on all the data, and fitted\ncalibrator.\nChanged in version 0.24:Single calibrated classifier case when`ensemble=False`.\nSee also\n[`calibration\\_curve`](sklearn.calibration.calibration_curve.html#sklearn.calibration.calibration_curve)\nCompute true and predicted probabilities for a calibration curve.\nReferences\n[1]\nB. Zadrozny &amp; C. Elkan.[Obtaining calibrated probability estimates from deci...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html"
    },
    {
      "title": "Improving Machine Learning Classification Predictions through ...",
      "text": "[Skip to main content](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#main-content)\n\n**Official websites use .gov**\nA\n**.gov** website belongs to an official\ngovernment organization in the United States.\n\n**Secure .gov websites use HTTPS**\nA **lock** (\n\nLocked padlock icon\n) or **https://** means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n\nSearch PMC Full-Text ArchiveSearch in PMC\n\n- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n\n- ## PERMALINK\n\n\n\nCopy\n\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:\n[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nJ Chem Inf Model\n\n. 2025 Oct 20;65(21):11716\u201311732. doi: [10.1021/acs.jcim.5c02015](https://doi.org/10.1021/acs.jcim.5c02015)\n\n# Improving Machine Learning Classification Predictions through SHAP and Features Analysis Interpretation\n\n[Leonardo Bernal](https://pubmed.ncbi.nlm.nih.gov/?term=%22Bernal%20L%22%5BAuthor%5D)\n\n### Leonardo Bernal\n\n\u2020\nDepartment\nof Life Sciences, University of Modena and\nReggio Emilia, Via Giuseppe Campi 103, 41125\nModena, Italy\n\n\u2021\nClinical\nand Experimental Medicine PhD Program, University\nof Modena and Reggio Emilia, Modena\n41125, Italy\n\nFind articles by [Leonardo Bernal](https://pubmed.ncbi.nlm.nih.gov/?term=%22Bernal%20L%22%5BAuthor%5D)\n\n\u2020,\u2021, [Giulio Rastelli](https://pubmed.ncbi.nlm.nih.gov/?term=%22Rastelli%20G%22%5BAuthor%5D)\n\n### Giulio Rastelli\n\n\u2020\nDepartment\nof Life Sciences, University of Modena and\nReggio Emilia, Via Giuseppe Campi 103, 41125\nModena, Italy\n\nFind articles by [Giulio Rastelli](https://pubmed.ncbi.nlm.nih.gov/?term=%22Rastelli%20G%22%5BAuthor%5D)\n\n\u2020, [Luca Pinzi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Pinzi%20L%22%5BAuthor%5D)\n\n### Luca Pinzi\n\n\u2020\nDepartment\nof Life Sciences, University of Modena and\nReggio Emilia, Via Giuseppe Campi 103, 41125\nModena, Italy\n\nFind articles by [Luca Pinzi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Pinzi%20L%22%5BAuthor%5D)\n\n\u2020,\\*\n\n- Author information\n- Article notes\n- Copyright and License information\n\n\u2020\nDepartment\nof Life Sciences, University of Modena and\nReggio Emilia, Via Giuseppe Campi 103, 41125\nModena, Italy\n\n\u2021\nClinical\nand Experimental Medicine PhD Program, University\nof Modena and Reggio Emilia, Modena\n41125, Italy\n\n\\*\n\nEmail luca.pinzi@unimore.it. Phone: +39 059\n2058625.\n\nReceived 2025 Aug 23; Accepted 2025 Oct 9; Revised 2025 Oct 7; Collection date 2025 Nov 10.\n\n\u00a9 2025 The Authors. Published by American Chemical Society\n\nThis article is licensed under CC-BY 4.0\n\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nPMCID: PMC12606625\u00a0\u00a0PMID: [41114446](https://pubmed.ncbi.nlm.nih.gov/41114446/)\n\n## Abstract\n\nTree-based machine\nlearning (ML) algorithms, such as Extra Trees\n(ET), Random Forest (RF), Gradient Boosting Machine (GBM), and XGBoost\n(XGB) are among the most widely used in early drug discovery, given\ntheir versatility and performance. However, models based on these\nalgorithms often suffer from misclassification and reduced interpretability\nissues, which limit their applicability in practice. To address these\nchallenges, several approaches have been proposed, including the use\nof SHapley Additive Explanations (SHAP). While SHAP values are commonly\nused to elucidate the importance of features driving models\u2019\npredictions, they can also be employed in strategies to improve their\nprediction performance. Building on these premises, we propose a novel\napproach that integrates SHAP and features value analyses to reduce\nmisclassification in model predictions. Specifically, we benchmarked\nclassifiers based on ET, RF, GBM, and XGB algorithms using data sets\nof compounds with known antiproliferative activity against three prostate\ncancer (PC) cell lines ( _i.e._, PC3, LNCaP, and DU-145).\nThe best-performing models, based on RDKit and ECFP4 descriptors with\nGBM and XGB algorithms, achieved MCC values above 0.58 and F1-score\nabove 0.8 across all data sets, demonstrating satisfactory accuracy\nand precision. Analyses of SHAP values revealed that many misclassified\ncompounds possess feature values that fall within the range typically\nassociated with the opposite class. Based on these findings, we developed\na misclassification-detection framework using four filtering rules,\nwhich we termed \u201cRAW\u201d, SHAP, \u201cRAW OR SHAP\u201d,\nand \u201cRAW AND SHAP\u201d. These filtering rules successfully\nidentified several potentially misclassified predictions, with the\n\u201cRAW OR SHAP\u201d rule retrieving up to 21%, 23%, and 63%\nof misclassified compounds in the PC3, DU-145, and LNCaP test sets,\nrespectively. The developed flagging rules enable the systematic exclusion\nof likely misclassified compounds, even across progressively higher\nprediction confidence levels, thus providing a valuable approach to\nimprove classifier performance in virtual screening applications.\n\n## Introduction\n\nIn recent years, artificial\nintelligence (AI) and in particular\nmachine learning (ML) have gained significant attention in drug discovery. _,_ These approaches have enabled the exploration of extensive chemical\nspaces and accelerated the development of new therapeutic candidates,\nfrom initial compound screenings _\u2212_ and bioactivity prediction to the optimization and prioritization of therapeutic\nleads. _\u2212_ Fueled by the increasing availability of large data\nsets, ML approaches have also contributed to significantly reduce\nexperimental costs and to accelerate timelines traditionally associated\nwith discovery and development, thereby\ndemonstrating their growing importance in both pharmaceutical industry\nand academic research.\n\nDespite\ntheir utility, several challenges often hamper the application\nof ML models in drug discovery, especially\nduring the early stages of virtual screening campaigns. Among these,\nmodel interpretability remains a key challenge, especially for complex architectures based on deep neural\nnetworks, which often provide outputs with limited interpretability\nand understanding of their inner functioning. _\u2212_ Such a limitation\ncomplicates the translation of model predictions into actionable chemical\ninsights, hampering rational chemical optimization of candidate molecules.\nTo address these limitations, novel deep learning models have been\nintroduced. However, the lack of large, high-quality training data\nsets containing bioactivity annotations for molecules, biological\ntargets, or cell lines often results in overfitting, generalization\nissues, and reduced chemical insights. These issues limit their application in early virtual screening\ncampaigns and _in silico_ phenotypic screenings. Conversely,\nsimpler tree-based ML algorithms remain widely used due to their lower\nimplementation complexity and inherent prediction interpretability, _,_ which facilitate the understanding of the models\u2019 predictions\nand provide clearer insights from their output.\n\nDespite these\nadvantages, model prediction outputs often contain\nfalse positives and false negatives, which can undermine the efficiency\nof the experimental validation. To mitigate this issue, many studies\nrely on probability-based confidence thresholds ( _e.g._, the _predict\\_proba_ function available in SciKit-Learn)\nto filter uncertain predictions. While effective in reducing globally\nuncertain classifications, such strategies inherently trade off predictive\ncoverage, often discarding a substantial fraction of compounds without\nresolving the problem of structurally inconsistent or locally misclassified\npredictions. This limitation highlights that reliance on global probability\nthresholds alone is insufficient to detect locally misclassified compounds,\nparticularly when the classifier confidence is artificially high.\nBesides, ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12606625"
    },
    {
      "title": "Will we ever be able to accurately predict solubility? | Scientific Data",
      "text": "Will we ever be able to accurately predict solubility? | Scientific Data\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Scientific Data](https://media.springernature.com/full/nature-cms/uploads/product/sdata/header-87021870c315c48063927b82055c12bc.svg)](https://www.nature.com/sdata)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41597-024-03105-6?error=cookies_not_supported&code=cc9ff490-8f5a-445e-abe3-402a4f6265ba)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41597)\n* [RSS feed](https://www.nature.com/sdata.rss)\nWill we ever be able to accurately predict solubility?\n[Download PDF](https://www.nature.com/articles/s41597-024-03105-6.pdf)\n[Download PDF](https://www.nature.com/articles/s41597-024-03105-6.pdf)\n* Analysis\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:18 March 2024# Will we ever be able to accurately predict solubility?\n* [P. Llompart](#auth-P_-Llompart-Aff1-Aff2)[1](#Aff1),[2](#Aff2),\n* [C. Minoletti](#auth-C_-Minoletti-Aff2)[2](#Aff2),\n* [S. Baybekov](#auth-S_-Baybekov-Aff1)[1](#Aff1),\n* [D. Horvath](#auth-D_-Horvath-Aff1)[1](#Aff1),\n* [G. Marcou](#auth-G_-Marcou-Aff1)[ORCID:orcid.org/0000-0003-1676-6708](https://orcid.org/0000-0003-1676-6708)[1](#Aff1)&amp;\n* \u2026* [A. Varnek](#auth-A_-Varnek-Aff1)[ORCID:orcid.org/0000-0003-1886-925X](https://orcid.org/0000-0003-1886-925X)[1](#Aff1)Show authors\n[*Scientific Data*](https://www.nature.com/sdata)**volume11**, Article\u00a0number:303(2024)[Cite this article](#citeas)\n* 22kAccesses\n* 36Citations\n* 4Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41597-024-03105-6/metrics)\n### Subjects\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n* [Physical chemistry](https://www.nature.com/subjects/physical-chemistry)\n## Abstract\nAccurate prediction of thermodynamic solubility by machine learning remains a challenge. Recent models often display good performances, but their reliability may be deceiving when used prospectively. This study investigates the origins of these discrepancies, following three directions: a historical perspective, an analysis of the aqueous solubility dataverse and data quality. We investigated over 20 years of published solubility datasets and models, highlighting overlooked datasets and the overlaps between popular sets. We benchmarked recently published models on a novel curated solubility dataset and report poor performances. We also propose a workflow to cure aqueous solubility data aiming at producing useful models for bench chemist. Our results demonstrate that some state-of-the-art models are not ready for public usage because they lack a well-defined applicability domain and overlook historical data sources. We report the impact of factors influencing the utility of the models: interlaboratory standard deviation, ionic state of the solute and data sources. The herein obtained models, and quality-assessed datasets are publicly available.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-022-01154-3/MediaObjects/41597_2022_1154_Fig1_HTML.png)\n### [Boosting the predictive performance with aqueous solubility dataset curation](https://www.nature.com/articles/s41597-022-01154-3?fromPaywallRec=false)\nArticleOpen access03 March 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-025-05559-8/MediaObjects/41597_2025_5559_Fig1_HTML.png)\n### [BigSolDB 2.0, dataset of solubility values for organic compounds in different solvents at various temperatures](https://www.nature.com/articles/s41597-025-05559-8?fromPaywallRec=false)\nArticleOpen access15 July 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-76850-8/MediaObjects/41598_2024_76850_Fig1_HTML.png)\n### [Estimation of hydrogen solubility in aqueous solutions using machine learning techniques for hydrogen storage in deep saline aquifers](https://www.nature.com/articles/s41598-024-76850-8?fromPaywallRec=false)\nArticleOpen access29 October 2024\n## Introduction\nAqueous solubility is a strategic parameter in synthetic, medicinal and environmental chemistry. It is one of the main parameters affecting bioavailability. Thus, a better understanding of this property is expected to improve success in drug design[1](https://www.nature.com/articles/s41597-024-03105-6#ref-CR1), as a key player in pharmacokinetics and ADME-Tox (Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiling[2](https://www.nature.com/articles/s41597-024-03105-6#ref-CR2). Solubility governs the fraction of the active substance available for absorption in the gastro-intestinal tract. Besides, a poor solubility of a compound or of a metabolite can be a threat for the patient: the substance may accumulate and crystalize, as exemplified by kidney stone diseases. Galenic formulation can improve the therapeutic potential of a compound[3](https://www.nature.com/articles/s41597-024-03105-6#ref-CR3), but a soluble drug candidate is always a safer option for clinical trials.\nHowever, measuring aqueous solubility is not always feasible at the early discovery stage because of the low throughput and large sample requirements[4](https://www.nature.com/articles/s41597-024-03105-6#ref-CR4),[5](https://www.nature.com/articles/s41597-024-03105-6#ref-CR5). For this reason,*in silico*predictive approaches have become highly valuable to prioritize drug candidates and reduce the number of experimental tests. Latest progress in this field is mainly due to (i) the organization of aqueous solubility prediction challenges, shedding a new light on existing tools; (ii) the public release of large aqueous solubility datasets; (iii) the advent of new machine learning methods promising unprecedented predictive performances. The current*status quo*in solubility prediction, which this study aims to analyze, is therefore very intricate.\nIn the first part of this study, we first remind the theoretical background of aqueous dissolution process, underlining the ambiguities and complexity of this measure. Next, we review the large number of datasets already published. Third, we critically discuss published models. This enables us, in a second part, to propose new guidelines to process thermodynamic aqueous solubility data. We applied them to existing datasets and proceed to a modeling exercise resulting in new QSAR models. All curated datasets and obtained models are publicly available at[https://doi.org/10.57745/CZVZIA](https://doi.org/10.57745/CZVZIA)[6](https://www.nature.com/articles/s41597-024-03105-6#ref-CR6).\n### Background of aqueous solubility\nSeveral types of solubility measurements are reported in the literature, depending on the method and conditions of measurement. The*thermodynamic solubility*is described as the maximum concentration of a compound in solution, at equilibrium with its most stable crystalline form. This solubility is usually measured during lead optimization phases and is used as source of*in silico*regression models[7](https://www.nature.com/articles/s41597-024-03105-6#ref-CR7). However, the above definition is not unambiguous, as the solute may, beyond physically dissolving, also*chemically*interact with water \u2013with significant impact on the equili...",
      "url": "https://www.nature.com/articles/s41597-024-03105-6"
    },
    {
      "title": "Finding a better path to drug selectivity - PMC - NIH",
      "text": ". Author manuscript; available in PMC: 2012 Nov 1.\n\n_Published in final edited form as:_ Drug Discov Today. 2011 Aug 2;16(21-22):985\u2013990. doi: [10.1016/j.drudis.2011.07.010](https://doi.org/10.1016/j.drudis.2011.07.010)\n\n## Abstract\n\nExtremely high affinity and selectivity are two of the most sought-after properties of drug molecules. Selectivity has been difficult to achieve, especially for targets that belong to large families of structurally and functionally related proteins. There are essentially two ways \\[AU1\\] by which selectivity can be improved during lead optimization: a chemical modification of the lead compound that improves the affinity towards the target to a higher extent than to off-target molecules; and a chemical modification that lowers the affinity of the lead compound towards off-target molecules. Maximal selectivity is achieved when both mechanisms can be combined synergistically. As we discuss here, analysis of several protease inhibitors that vary in a single functionality indicates that nonpolar functionalities preferentially follow the first mechanism, whereas polar functionalities follow the second, and that those features are imprinted in their thermodynamic signatures.\n\n## Introduction\n\nThe binding affinity of a drug for its target, Ka, is dictated by the Gibbs energy of binding (\u0394G),\n. \\[AU2\\]Maximal binding affinity occurs when \u0394G is large and negative. For example, picomolar binding affinity corresponds to \u221216.4 kcal/mol, whereas nanomolar affinity corresponds to \u221212.3 kcal/mol in Gibbs energy. A binding energy between \u221212 and \u221216 kcal/mol is the usual goal in lead optimization. For a starting compound with micromolar affinity, this goal implies the addition of an extra \u22128.2 kcal/mol to the binding energy of the compound. Given that the Gibbs energy of binding is composed of two contributions, the enthalpy (\u0394H) and entropy (\u0394S) changes (\u0394G = \u0394H \u2212 T\u0394S), the required energy is obtained by optimizing the enthalpy and entropy contributions to binding \\[ [1](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R1)\u2013 [4](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R4)\\].\n\nIn general, extremely high-binding affinity requires favorable enthalpic and entropic contributions. Given that binding implies transferring a compound from an aqueous environment into the binding pocket of a protein, the energy of desolvation as well as the energy of interaction of the drug molecule and its target need to be taken into consideration. The energies of all bonds that are made and broken during the process contribute to the enthalpy change, whereas all ordering and/or disordering processes in the protein, compound and solvent contribute to the entropy change. In the binding process, different chemical functionalities (polar or nonpolar) also contribute differently to the enthalpy and entropy changes. The specific enthalpic and entropic contributions associated with the binding of a compound to a target define its thermodynamic signature. Fortunately, the thermodynamic signature can be measured experimentally by isothermal titration calorimetry (ITC). Having access to the thermodynamic signature provides a useful roadmap for the optimization of a compound, because it immediately identifies the forces that require optimization \\[ [3](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R3), [5](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R5)\\].\n\nThe binding affinity of a drug molecule for its target results from the ultimate balance of desolvation and attractive forces. As a rule, polar and nonpolar chemical functionalities contribute differently to affinity and selectivity owing to differences in their thermodynamic signatures. Whereas the desolvation of both polar and nonpolar groups is coupled to favorable entropy changes, the desolvation of polar groups carries unfavorable enthalpy changes that are one order of magnitude larger than those of nonpolar groups \\[ [6](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R6)\\]. In addition to desolvation changes, binding involves enthalpic and entropic changes resulting from interactions of the compound with the target. Nonpolar groups can establish van der Waals contacts with the target that result in small enthalpic gains, whose magnitude depends on the degree of shape complementarity between compound and target. Polar groups are able to establish hydrogen bonds, which might contribute more significantly to the binding enthalpy. In general, only strong hydrogen bonds are able to overcome the unfavorable desolvation enthalpy of polar groups and contribute favorably to the binding enthalpy. ITC measurements of different compounds that vary by a single functionality and for which crystallographic structures of their bound complexes are available (e.g. \\[ [7](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R7), [8](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R8)\\]), indicate that strong hydrogen bonds contribute anywhere between \u22124 and \u22125 kcal/mol to the enthalpy change, whereas van der Waals interactions associated with a methyl group can contribute as much as \u22121 kcal/mol.\n\nThe interactions between the drug molecule and its target also result in a loss of conformational entropy owing to the structuring of protein residues and the drug molecule itself. The loss of conformational entropy of the compound can be minimized by introducing conformational constraints. All else being equal, a conformationally constrained compound will have a better binding affinity and also better selectivity than its flexible counterpart. Conformational constraints will restrict the ability of the compound to accommodate to binding site variations found in homologous off-target molecules. In this respect, Knight and Shokat \\[ [9](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R9)\\] noted that selective kinase inhibitors are usually entropically constrained with four or fewer rotatable bonds connecting any two ring systems.\n\n[Figure 1](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#F1) summarizes typical contributions to the thermodynamic signature of a drug molecule. The figure is by no means exhaustive. For example, if binding is associated with extensive protein structuring or folding, as is the case with intrinsically disordered proteins, the unfavorable conformational entropy might predominate, giving rise to an overall unfavorable entropy \\[ [10](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R10), [11](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R11)\\]. In addition, protonation and/or deprotonation effects that are buffer dependent need to be deconvoluted from the observed binding energy \\[ [12](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R12), [13](http://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R13)\\]. Because different forces have different thermodynamic profiles, it is possible for two compounds with similar affinities to have different selectivity profiles. From a design point of view, the goal is to maximize favorable interactions with the target while simultaneously minimizing or even making unfavorable the interactions of the compound with off-target molecules. Understanding the molecular origin of the selectivity differences of compounds with similar binding affinities becomes crucial to the design of more selective drug molecules. It is evident that all the forces that contribute to binding affinity do not contribute equally to selectivity. A tight fit between the ligand and its binding target not only maximizes van der Waals interactions, but also reduces the probability that the ligand will be accommodated equally well in off-target molecules. Affinity gains with off-target proteins will not be as large as with the target.\n\n### Figure 1.\n\nTypical contributions to the thermodynamic signature in drug design. Favorable enthalpic interactions (\u0394H; green bars) originate primarily from hydrogen bonding and van der Waals interactions. Unfavorable enthalpic interactions, by contrast, origin...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3210374"
    },
    {
      "title": "A Brief Introduction to Chemical Reaction Optimization",
      "text": "<div><p>\n This article references 355 other publications.\n </p><div><li><div><p><strong><a href=\"#ref1\">1</a></strong></p><div><p>Taylor, C. J.; Baker, A.; Chapman, M. R.; Reynolds, W. R.; Jolley, K. E.; Clemens, G.; Smith, G. E.; Blacker, A. J.; Chamberlain, T. W.; Christie, S. D. Flow Chemistry for Process Optimisation Using Design of Experiments. <i>J. Flow. Chem.</i> 2021, <i>11</i>, 75\u2013 86, \u00a0DOI: 10.1007/s41981-020-00135-0 </p><div><p>[<a href=\"/servlet/linkout?suffix=ref1/cit1&amp;dbid=16&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=10.1007%2Fs41981-020-00135-0\">Crossref</a>], [<a href=\"/servlet/linkout?suffix=ref1/cit1&amp;dbid=32&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=1%3ACAS%3A528%3ADC%252BB3MXht1eqtbnO\">CAS</a>],\u00a0<a href=\"http://scholar.google.com/scholar_lookup?hl=en&amp;volume=11&amp;publication_year=2021&amp;pages=75-86&amp;journal=J.+Flow.+Chem.&amp;author=C.+J.+Taylor&amp;author=A.+Baker&amp;author=M.+R.+Chapman&amp;author=W.+R.+Reynolds&amp;author=K.+E.+Jolley&amp;author=G.+Clemens&amp;author=G.+E.+Smith&amp;author=A.+J.+Blacker&amp;author=T.+W.+Chamberlain&amp;author=S.+D.+Christie&amp;title=Flow+Chemistry+for+Process+Optimisation+Using+Design+of+Experiments&amp;doi=10.1007%2Fs41981-020-00135-0\">Google Scholar</a></p><div><div><p>1</p><p>Flow chemistry for process optimisation using design of experiments</p><p>Taylor, Connor J.; Baker, Alastair; Chapman, Michael R.; Reynolds, William R.; Jolley, Katherine E.; Clemens, Graeme; Smith, Gill E.; Blacker, A. John; Chamberlain, Thomas W.; Christie, Steven D. R.; Taylor, Brian A.; Bourne, Richard A.</p><p>Journal of Flow Chemistry\n (2021),\n 11\n (1),\n 75-86CODEN:\n JFCOBJ;\n ISSN:2063-0212.\n \n (Akademiai Kiado)\n </p><p>Abstr.: Implementing statistical training into undergraduate or postgraduate chem. courses can provide high-impact learning experiences for students. However, the opportunity to reinforce this training with a combined lab. practical can significantly enhance learning outcomes by providing a practical bolstering of the concepts. This paper outlines a flow chem. lab. practical for integrating design of expts. optimization techniques into an org. chem. lab. session in which students construct a simple flow reactor and perform a structured series of expts. followed by computational processing and anal. of the results.</p></div><p>https://chemport.cas.org/services/resolver?origin=ACS&amp;resolution=options&amp;coi=1%3ACAS%3A528%3ADC%252BB3MXht1eqtbnO&amp;md5=2c5a72c760f1dc7f2921f6f2c5039fe0</p></div></div></div></div></li><li><div><p><strong><a href=\"#ref2\">2</a></strong></p><div><p>Tsay, C.; Pattison, R. C.; Piana, M. R.; Baldea, M. A Survey of Optimal Process Design Capabilities and Practices in the Chemical and Petrochemical Industries. <i>Comput. Chem. Eng.</i> 2018, <i>112</i>, 180\u2013 189, \u00a0DOI: 10.1016/j.compchemeng.2018.01.012 </p><div><p>[<a href=\"/servlet/linkout?suffix=ref2/cit2&amp;dbid=16&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=10.1016%2Fj.compchemeng.2018.01.012\">Crossref</a>], [<a href=\"/servlet/linkout?suffix=ref2/cit2&amp;dbid=32&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=1%3ACAS%3A528%3ADC%252BC1cXjtFeisrY%253D\">CAS</a>],\u00a0<a href=\"http://scholar.google.com/scholar_lookup?hl=en&amp;volume=112&amp;publication_year=2018&amp;pages=180-189&amp;journal=Comput.+Chem.+Eng.&amp;author=C.+Tsay&amp;author=R.+C.+Pattison&amp;author=M.+R.+Piana&amp;author=M.+Baldea&amp;title=A+Survey+of+Optimal+Process+Design+Capabilities+and+Practices+in+the+Chemical+and+Petrochemical+Industries&amp;doi=10.1016%2Fj.compchemeng.2018.01.012\">Google Scholar</a></p><div><div><p>2</p><p>A survey of optimal process design capabilities and practices in the chemical and petrochemical industries</p><p>Tsay, Calvin; Pattison, Richard C.; Piana, Michael R.; Baldea, Michael</p><p>Computers &amp; Chemical Engineering\n (2018),\n 112\n (),\n 180-189CODEN:\n CCENDW;\n ISSN:0098-1354.\n \n (Elsevier B.V.)\n </p><p>To examine industrial capabilities and practices in using process design optimization software tools, we conducted a series of over one hundred interviews with practitioners and industry experts in optimal process design, focusing on current techniques, workflows, and challenges. In this article, we analyze the findings of these interviews, providing a perspective into the status of optimal process design in the petrochem. and chem. industries. We first present the findings categorized by company type and personnel function, followed by industry-specific insights.</p></div><p>https://chemport.cas.org/services/resolver?origin=ACS&amp;resolution=options&amp;coi=1%3ACAS%3A528%3ADC%252BC1cXjtFeisrY%253D&amp;md5=48074c5c7e000041f6452ac1120adfaa</p></div></div></div></div></li><li><div><p><strong><a href=\"#ref3\">3</a></strong></p><div><p>Andraos, J. Designing a Green Organic Chemistry Lecture Course. In <i>Green Organic Chemistry in Lecture and Laboratory</i>, 1st ed.; CRC Press: Boca Raton, FL, 2012; pp 29\u2013 68.</p></div></div></li><li><div><p><strong><a href=\"#ref4\">4</a></strong></p><div><p>Brain, C. T.; Steer, J. T. An Improved Procedure for the Synthesis of Benzimidazoles, Using Palladium-Catalyzed Aryl-Amination Chemistry. <i>J. Org. Chem.</i> 2003, <i>68</i>, 6814\u2013 6816, \u00a0DOI: 10.1021/jo034824l </p><div><p>[<a href=\"/doi/10.1021/jo034824l\">ACS Full Text </a>], [<a href=\"/servlet/linkout?suffix=ref4/cit4&amp;dbid=32&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=1%3ACAS%3A528%3ADC%252BD3sXlvVGlt7o%253D\">CAS</a>],\u00a0<a href=\"http://scholar.google.com/scholar_lookup?hl=en&amp;volume=68&amp;publication_year=2003&amp;pages=6814-6816&amp;journal=J.+Org.+Chem.&amp;author=C.+T.+Brain&amp;author=J.+T.+Steer&amp;title=An+Improved+Procedure+for+the+Synthesis+of+Benzimidazoles%2C+Using+Palladium-Catalyzed+Aryl-Amination+Chemistry&amp;doi=10.1021%2Fjo034824l\">Google Scholar</a></p><div><div><p>4</p><p>An Improved Procedure for the Synthesis of Benzimidazoles, Using Palladium-Catalyzed Aryl-Amination Chemistry</p><p>Brain, Christopher T.; Steer, James T.</p><p>Journal of Organic Chemistry\n (2003),\n 68\n (17),\n 6814-6816CODEN:\n JOCEAH;\n ISSN:0022-3263.\n \n (American Chemical Society)\n </p><p>New, improved conditions were developed and optimized for the synthesis of benzimidazoles by intramol. Pd-catalyzed aryl-amination chem. This methodol., combined with a catch and release purifn. strategy, led to a range of these heterocycles being prepd. rapidly and in excellent yield. Thus, 2-BrC6H4N:CMeNHMe was cyclized to 1,2-dimethyl-1H-benzimidazole in &gt;98% conversion [Pd2(dba)3/PPh3 (dba = dibenzylideneacetone) catalyst, 160\u00b0, 2 equiv NaOH, H2O/DME, microwaves]. The PPh3 impurity in the product was removed by treatment of the crude product with Amberlyst 15 resin in DCM, washing the resin to remove the PPh3 contaminants, then releasing the benzimidazole with Et3N and filtering.</p></div><p>https://chemport.cas.org/services/resolver?origin=ACS&amp;resolution=options&amp;coi=1%3ACAS%3A528%3ADC%252BD3sXlvVGlt7o%253D&amp;md5=323543a981cfc5a10a0107c180eaaeb7</p></div></div></div></div></li><li><div><p><strong><a href=\"#ref5\">5</a></strong></p><div><p>Wang, H.; Jung, H.; Song, F.; Zhu, S.; Bai, Z.; Chen, D.; He, G.; Chang, S.; Chen, G. Nitrene-Mediated Intermolecular N\u2013N Coupling for Efficient Synthesis of Hydrazides. <i>Nat. Chem.</i> 2021, <i>13</i>, 378\u2013 385, \u00a0DOI: 10.1038/s41557-021-00650-0 </p><div><p>[<a href=\"/servlet/linkout?suffix=ref5/cit5&amp;dbid=16&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=10.1038%2Fs41557-021-00650-0\">Crossref</a>], [<a href=\"/servlet/linkout?suffix=ref5/cit5&amp;dbid=8&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=33753917\">PubMed</a>], [<a href=\"/servlet/linkout?suffix=ref5/cit5&amp;dbid=32&amp;doi=10.1021%2Facs.chemrev.2c00798&amp;key=1%3ACAS%3A528%3ADC%252BB3MXntFWgtLs%253D\">CAS</a>],\u00a0<a href=\"http://scholar.google.com/scholar_lookup?hl=en&amp;volume=13&amp;publication_year=2021&amp;pages=378-385&amp;journal=Nat.+Chem.&amp;author=H.+Wang&amp;author=H.+Jung&amp;author=F.+Song&amp;autho...",
      "url": "https://pubs.acs.org/doi/10.1021/acs.chemrev.2c00798"
    },
    {
      "title": "Solubility Prediction - an overview | ScienceDirect Topics",
      "text": "<div><div><h2>Chapters and Articles</h2><p>You might find these chapters and articles relevant to this topic.</p><article><div><div><p><span></span></p><section><h3>17.3.2.1 Solubility prediction</h3><p><span><span>Solubility prediction</span> remains an area of interest for many researchers. Various prediction models have been reported in the literature. The models for intrinsic solubility prediction can be divided into several categories</span><span><sup>34</sup></span>: (1) fragment or group contribution-based models,<span><sup>35\u201337</sup></span> (2) log<em>P</em> and melting-based models,<span><sup>38,39</sup></span> and (3) statistically derived models based on 2-D or 3-D chemical descriptors.<span><sup>40\u201342</sup></span> However, the accurate prediction of solubility remains challenging. The majority of organizations still relies on experimental solubility results for compound prioritization and decision making.</p></section><p></p></div><div><p><a href=\"https://www.sciencedirect.com/science/article/pii/B9780128024478000170\">Read full chapter</a></p><p>URL:\u00a0https://www.sciencedirect.com/science/article/pii/B9780128024478000170</p></div></div></article><article><div><div><p><span></span></p><section><h3>17.3.2 Solubility prediction and screen</h3><section><h4>17.3.2.1 Solubility prediction</h4><p><span><span>Solubility prediction</span> remains an area of interest for many researchers. Various prediction models have been reported in the literature. The models for intrinsic solubility prediction can be divided into several categories</span><span><sup>34</sup></span>: (1) fragment or group contribution-based models,<span><sup>35\u201337</sup></span> (2) log<em>P</em> and melting-based models,<span><sup>38,39</sup></span> and (3) statistically derived models based on 2-D or 3-D chemical descriptors.<span><sup>40\u201342</sup></span> However, the accurate prediction of solubility remains challenging. The majority of organizations still relies on experimental solubility results for compound prioritization and decision making.</p></section><section><h4>17.3.2.2 Solubility screen and measurement methods</h4><p>High-throughput (HT) solubility screening methods have been widely used to profile solubility during the early drug discovery stage. A large number of compounds to be profiled in various assays also requires standardized procedures in compound handling and distribution. In most cases, these are performed in 96-well or 384-well microtiter plates. For early HT solubility assays, the compounds are often dispensed by two methods. One method is to introduce the compounds as dimethyl sulfoxide (DMSO) stock solutions into aqueous media. The alternative method is to dispense the compounds as DMSO solutions, and then aqueous media are added after DMSO is removed from the plate. The final pH and solid form are typically not measured or characterized in either case. The HT solubility is determined by measuring the concentration of the saturated solutions using UV-Vis spectroscopy or liquid chromatography (UV or LC) after the solids are removed by filtration/centrifugation or by detecting precipitation formation using UV or nephelometric turbidity detector. These HT assays tend to overestimate the solubility due to the remaining organic solvent and/or formation of amorphous material during precipitation from DMSO stock solution. However, it is a very useful tool to rank the order of the compounds and flag the solubility risks at the early stage with only a small amount of compound required. In addition, the early solubility information also helps to interpret results from other in vitro assays.</p><p>During the lead optimization phase with a short list of compounds, it is recommended to conduct an equilibrium solubility study using crystalline solids with final pH and physical form measurements. This is because the pH and the physical form can significantly impact the solubility, as we previously discussed. In addition, the physical form can change during the course of solubility experiments. For examples, a free form can form a salt or a hydrate, or an amorphous material or metastable polymorph can convert to a more stable crystalline form. Therefore, it is important to understand the physical form of the initial and final solids. The most commonly used techniques to characterize the physical form of the <span></span>residual solids from solubility studies are polarized light microscopy (PLM) and powder X-ray diffraction (PXRD) (refer to <span>chapter: Solid-State Characterization and Techniques</span>, for techniques used for solid-state characterization).</p><p>During the drug development stage, the \u201cshake-flask\u201d method is typically considered as the standard method for determining the thermodynamic solubility. In the shake-flask method, a compound with known physical form (typically with most stable crystalline form) is added into a flask/vial with an excess amount to the medium, and the resulting suspension is agitated at a predetermined temperature and speed. Samples are collected at the predefined time point(s) typically at least 1\u20132 days. The residual solids are removed by filtration, the solubility of the compound is quantified by high-performance liquid chromatography (HPLC) or ultra-performance liquid chromatography (UPLC), the final pH is measured and recorded, and the physical form of the residual solids is characterized.</p><p>A detailed review of the solubility determination method is out of the scope of this chapter. There is a vast amount of information on this topic in literature for interested readers.<span><sup>43,44</sup></span></p></section><section><h4>17.3.2.3 Solubility screen in vehicles</h4><div><p>The pH solubility measurements are typically performed during the early stage of development. Some organizations also incorporate solubility in biorelevant media in the early-stage HT screens to understand the developability risk as oral candidates. Solubility in different organic solvents is sometimes conducted to facilitate polymorph, salt selection, and process optimization. For poorly soluble compounds, additional solubility screening is typically performed with different solubilizing vehicles to guide the formulation design and developability risk assessment. Solubilizing vehicles include cosolvents, surfactants, complexation agents, and oils/lipids. The common screening excipients are listed in <span>Table 17.4</span>. In addition, different counterions also impact solubility due to different <em>K</em><sub>sp</sub> values. In situ salt screening with different counterions can provide useful information on solubility enhancement and formulation development. The theory and practical aspects of in situ salt formation to improve solubility have been discussed in the literature.<span><sup>45,46</sup></span></p><div><p><span><span></span></span></p><p><span>Table 17.4</span>. Typical Excipients and Vehicles Screened for Solubility During Early Stage of Development</p><p></p><div><table><thead><tr><th>Type of Vehicles</th><th>Examples</th></tr></thead><tbody><tr><td>pH buffers</td><td>pH 1, pH 4, pH 6.5, pH 7.4, pH 9</td></tr><tr><td>Biorelevant media</td><td>SGF, FaSSIF, FeSSIF</td></tr><tr><td>Cosolvents</td><td>Polyethylene 400 (PEG 400), propylene glycol (PG), ethanol and dimethylacetamide (DMA)</td></tr><tr><td>Surfactants</td><td>Polysorbate 80 (Tween 80), Polyoxyl 15 Hydroxystearate (Solutol\u00ae HS 15/Kolliphor\u00ae HS 15), Pluronic\u00ae F-68, Polyoxyl-35 castor oil (Cremophor\u00ae EL/Kolliphor\u00ae EL), <span>d</span>-\u03b1-tocopheryl polyethylene glycol 1000 succinate (Vitamin E TPGS), caprylocaproyl polyoxyl-8 glycerides (Labrasol), sodium lauryl sulfate (SLS)</td></tr><tr><td>Complexation</td><td>Hydroxypropyl-\u03b2-cyclodextrin (HP-\u03b2-CD), sulfobutylether-\u03b2-cyclodextrin (SBE-\u03b2-CD/Captisol\u00ae)</td></tr><tr><td>Oils/Lipids</td><td>Medium-chain triglyceride (eg, Miglyol\u00ae 812), medium-chain monoglycerides &amp;amp; diglycerides (eg, Capmul\u00ae MCM), phospholipids (eg, P...",
      "url": "https://www.sciencedirect.com/topics/pharmacology-toxicology-and-pharmaceutical-science/solubility-prediction"
    }
  ]
}