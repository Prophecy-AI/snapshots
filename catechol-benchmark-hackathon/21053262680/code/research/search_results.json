{
  "query": "What are the best techniques to reduce the gap between cross-validation and leaderboard scores when predicting for completely unseen chemical compounds in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Cross\u2011validation is a standard way to estimate how a model will perform on new data, but in Kaggle competitions that focus on **completely unseen chemical compounds** the usual random\u2011split CV often over\u2011estimates the true score, creating a large gap between the public leaderboard and the final private leaderboard.  The most effective way to narrow this gap is to mimic the out\u2011of\u2011distribution (OOD) nature of the test set during validation.  A **k\u2011fold n\u2011step forward (or scaffold\u2011aware) split** has been shown to be far more predictive of real\u2011world performance than a random split, because each fold is built from compounds that are temporally or structurally earlier than those in the validation fold, thereby preserving the distribution shift that the competition will present\u202f([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  Complementary metrics such as **discovery yield** and **novelty error** can be tracked on these splits to quantify the model\u2019s applicability domain and to flag over\u2011optimistic CV scores\u202f([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\nBeyond the split strategy, **uncertainty quantification and calibration** are crucial.  The Chemprop package implements directed\u2011message\u2011passing neural networks with built\u2011in calibration methods, Bayesian ensembling, and explicit uncertainty estimates, which help identify predictions that are likely to be unreliable on OOD compounds\u202f([chemrxiv.org](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf)).  Leveraging **pre\u2011training on large public chemistry datasets** and then fine\u2011tuning on the competition data further improves generalisation, as does systematic **hyper\u2011parameter optimisation** guided by nested CV\u202f([chemrxiv.org](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf)).  Recent benchmark studies (BOOM) confirm that models with strong inductive bias\u2014e.g., graph\u2011based architectures\u2014tend to retain better OOD performance, especially when combined with the above calibration steps\u202f([arxiv.org](https://arxiv.org/html/2505.01912v1)).\n\nFinally, to prevent the leaderboard itself from becoming a source of over\u2011fitting, competition organizers can adopt **adaptive hold\u2011out schemes** such as the \u201cLadder\u201d algorithm, which limits the precision of feedback and the frequency of submissions, thereby encouraging participants to rely on robust validation rather than leaderboard chasing\u202f([mlr.press](https://proceedings.mlr.press/v37/blum15.pdf)).  For contestants, a practical workflow is: (1) create scaffold\u2011 or time\u2011aware forward CV splits, (2) train calibrated, uncertainty\u2011aware graph models with pre\u2011training and thorough hyper\u2011parameter search, (3) evaluate using discovery\u2011yield/novelty metrics, and (4) reserve a small, untouched hold\u2011out set for a final sanity check before the public leaderboard submission.  Following these steps consistently shrinks the CV\u2011to\u2011leaderboard gap for unseen chemical compounds.",
      "url": ""
    },
    {
      "title": "[PDF] Chemprop: Machine Learning Package for Chemical Property ...",
      "text": "Chemprop: Machine Learning Package for Chemical Property Prediction\nEsther Heid,1, 2 Kevin P. Greenman,1 Yunsie Chung,1 Shih-Cheng Li,1, 3 David E. Graff,4, 1 Florence H.\nVermeire,1, 5 Haoyang Wu,1 William H. Green,1and Charles J. McGill1, 6, a)\n1)Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139,\nUnited States\n2)Institute of Materials Chemistry, TU Wien, 1060 Vienna, Austria\n3)Department of Chemical Engineering, National Taiwan University, Taipei 10617,\nTaiwan\n4)Department of Chemistry and Chemical Biology, Harvard University, Cambridge, Massachusetts 02138,\nUnited States\n5)Department of Chemical Engineering, KU Leuven, Celestijnenlaan 200F, B-3001 Leuven,\nBelgium\n6)Department of Chemical and Life Science Engineering, Virginia Commonwealth University, Richmond,\nVirginia 23284, United States\nDeep learning has become a powerful and frequently employed tool for the prediction of molecular properties,\nthus creating a need for open-source and versatile software solutions that can be operated by non-experts.\nAmong current approaches, directed message-passing neural networks (D-MPNNs) have proven to perform\nwell on a variety of property prediction tasks. The software package Chemprop implements the D-MPNN\narchitecture, and offers simple, easy, and fast access to machine-learned molecular properties. Compared to its\ninitial version, we present a multitude of new Chemprop functionalities such as the support of multi-molecule\nproperties, reactions, atom/bond-level properties, and spectra. Further, we incorporate various uncertainty\nquantification and calibration methods along with related metrics, as well as pretraining and transfer learning\nworkflows, improved hyperparameter optimization, and other customization options concerning loss functions\nor atom/bond features. We benchmark D-MPNN models trained using Chemprop with the new reaction,\natom-level and spectra functionality on a variety of property prediction datasets, including MoleculeNet and\nSAMPL, and observe state-of-the-art performance on the prediction of water-octanol partition coefficients,\nreaction barrier heights, atomic partial charges, and absorption spectra. Chemprop enables out-of-the-box\ntraining of D-MPNN models for a variety of problem settings in a fast, user-friendly, and open-source software.\nI. INTRODUCTION\nMachine learning in general, and especially deep learn\u0002ing, has become a powerful tool in various fields of\nchemistry. Applications range from the prediction of\nphysico-chemical1\u20139 and pharmacological10 properties of\nmolecules to the design of molecules or materials with\ncertain properties,11\u201313 the exploration of chemical syn\u0002thesis pathways,14\u201327 or the prediction of properties im\u0002portant for chemical analysis like IR,28 UV/VIS29 or\nmass spectra.30\u201333\nMany combinations of molecular representation and\nmodel architecture have been developed to extract fea\u0002tures from molecules and predict molecular properties.\nMolecules can be represented as graphs, strings, pre\u0002computed feature vectors, or sets of atomic coordi\u0002nates and processed using graph-convolutional neural\nnetworks, transformers, or feed-forward neural networks\nto train predictive models. While early works focused\non hand-made features or simple fingerprinting methods\nlike circular fingerprints combined with kernel regres\u0002sion methods or neural networks,34 the current state\u0002of-the-art has shifted to end-to-end trainable models\nwhich directly learn to extract their own features.35\nHere, the model complexity can be nearly endless based\na)Electronic mail: mcgillc2@vcu.edu\non the mechanisms of information exchange between\nparts of the molecule. For example, graph convolu\u0002tional neural networks (GCNNs) extract local informa\u0002tion from the molecular graph for single or small groups\nof atoms, and use that information to update the imme\u0002diate neighborhood.1\u20133,36 Graph attention transformers\nallow for a less local information exchange via attention\nlayers, which learn to accumulate the features of atoms\nboth close and far away in the graph.37,38 Another impor\u0002tant line of research comprises the prediction of proper\u0002ties dependent on the three-dimensional conformation of\na molecule, such as the prediction of properties obtained\nfrom quantum mechanics.39\u201342 Finally, transformer mod\u0002els from natural language processing can be trained on\nstring representations such as SMILES or SELFIES, also\nleading to promising results.43\u201346\nIn general, larger models with more parameters suf\u0002fer from less restrictions in what and how they can learn\nfrom data, but require larger datasets and pre-training.47\nSmaller models can be very efficient if the right fea\u0002tures are known and used,47 but usually suffer from lim\u0002itations to their performance or generalization caused\nby a suboptimal representation or model architecture.\nFor example, models focusing only on the local struc\u0002ture of an atom and its immediate neighborhood usu\u0002ally fail to learn long-range interactions in a detailed\nfashion.48 In this work, we focus on GCNNs, which pre\u0002dict many chemical properties accurately49 at a moderate\nmodel size. They offer robust performance if the three\u00021\nhttps://doi.org/10.26434/chemrxiv-2023-3zcfl ORCID: https://orcid.org/0000-0002-8404-6596 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\ndimensional conformation of a molecule is not known or\nnot relevant for a prediction task. They perform best for\nmedium- to large-sized datasets (thousands to hundred\nthousands of data points). Many flavors of GCNNs have\nbeen published, among them our own approach called\nChemprop,36 a directed-message passing algorithm de\u0002rived from the seminal work of Gilmer et al.1\nAn early version of Chemprop was published in Ref. 36.\nSince then, the code has substantially evolved, and\nnow includes a vast collection of new features ranging\nfrom support of inputs beyond single molecules/targets\nand scalar molecular properties, to uncertainty estima\u0002tion, customized atom and bond features, and trans\u0002fer learning, among others. For example, Chemprop\nis now able to predict properties for systems contain\u0002ing multiple molecules, such as solute/solvent combina\u0002tions, or reactions with and without solvent. It can\ntrain on molecular targets, spectra, or atom/bond-level\ntargets, and output the latent representation for anal\u0002ysis of the learned feature embedding. Available uncer\u0002tainty metrics include popular approaches such as ensem\u0002bling, mean-variance estimation and evidential learning.\nChemprop is thus a general and versatile deep learning\ntoolbox, and enjoys a wide user base. Several studies\nhave been published based on Chemprop models rang\u0002ing from topics such as drug discovery,10,50,51 to spectro\u0002scopic properties,28,29 physico-chemical properties,4,52\u201358\nand reaction properties,59,60 with many of them making\nuse of the new features described in this manuscript, and\nachieving state-of-the-art performances in their respec\u0002tive tasks.\nIn this manuscript, we describe and benchmark the\nnew Chemprop features, as well as give usage examples\nand general advice on using Chemprop. The remainder\nof the article is structured as follows: The methods sec\u0002tion summarizes the architecture of Chemprop, as well as\nthe model details and parameters used in this study. We\nfurthermore describe the data acquisition, preprocessing\nand splitting of all datasets used in this study. We then\ndiscuss a selection of Chemprop features, with a focus on\nfeatures introduced after the initial release of Chemprop.\nIn the results section, we benchmark Chemprop on a\nlarge variety of datasets showcasing its performance for\nboth simple and advanced prediction tasks. We then\nconclude the article, and provide details on the data and\nsoftware, which we open-sourced including all scripts to\nallow for full reproducibility.\nII. METHODS\nIn the following, we describe the general architecture\nof Chemprop and a selection of the hyperparameters. We\nthen describe the hyperparamete...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models",
      "text": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n# BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nEvan R. AntoniukShehtab ZamanTal Ben-NunLawrence Livermore National LaboratoryPeggy LiLawrence Livermore National LaboratoryJames DiffenderferLawrence Livermore National LaboratoryBusra SahinBinghamton University, School of ComputingObadiah SmolenskiBinghamton University, School of ComputingTim HsuLawrence Livermore National LaboratoryAnna M. HiszpanskiLawrence Livermore National LaboratoryKenneth ChiuBinghamton University, School of ComputingBhavya KailkhuraLawrence Livermore National LaboratoryBrian Van EssenLawrence Livermore National Laboratory\n(January 2025)\n###### Abstract\nAdvances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations.\nAlthough the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks.\nWe present BOOM,benchmarks forout-of-distributionmolecular property predictions\u2014a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.\n## 1Introduction\nMolecular discovery is the process by which novel molecular structures with desirable application-specific properties are identified. Given the immense total search space of hypothetical small molecules enumerating approximately106010^{60}hypothetical molecules and 100 billion enumerated molecules, molecule discovery has increasingly relied upon machine learning models to efficiently navigate this space.> [\n[> 1\n](https://arxiv.org/html/2505.01912v1#bib.bib1)> , [> 2\n](https://arxiv.org/html/2505.01912v1#bib.bib2)> , [> 3\n](https://arxiv.org/html/2505.01912v1#bib.bib3)> ]\nTypically, molecule discovery is performed by first training a machine learning (ML) model on a molecule property dataset to learn the property-structure relationship. Then, this trained ML model is used to discover new molecules either by screening a list of enumerated molecules or by guiding a generative model towards molecules of interest> [\n[> 4\n](https://arxiv.org/html/2505.01912v1#bib.bib4)> ]\n.\nMolecule discovery is inherently an out-of-distribution (OOD) prediction problem. For the discovered molecules to constitute an exciting chemical discovery, the molecules need to either i) exhibit properties that extrapolate beyond those of the known molecules in the training dataset, or ii) possess a new chemical substructure that was previously not considered for the application of interest. In either case, the success of the molecule discovery campaign is dependent on the machine learning model\u2019s ability to make accurate predictions on samples that do not follow the same distribution as the known molecules (training data).\nDespite the importance of OOD performance to the problem of molecule discovery, the OOD performance of commonly used ML models for molecular property prediction has yet to be systematically explored. The majority of the standardized benchmarks used to assess the performance of chemical property prediction models do not include evaluations of model performance in the case where the test set is drawn from a different distribution than the training data. As a result of the lack of OOD chemistry benchmarks, the development of chemistry ML models are currently driven primarily by maximizing in-distribution performance, which may be hurting model generalization. The lack of OOD chemistry benchmarks has also hindered our understanding of how to develop generalizable chemistry foundation models. Currently, there is little empirical knowledge about how choices regarding the pretraining task, model architecture, and/or dataset diversity impact the generalization performance of chemistry foundation models that are expected to generalize across all chemical systems.\nIn this work, we develop BOOM,benchmarks forout-of-distributionmolecular property predictions, a standardized benchmark for assessing the OOD generalization performance of molecule property prediction models.\n### 1.1Main Findings\nOur work consists of the following main contributions:\n* \u2022We develop a robust methodology for evaluating the performance of chemical property prediction models to extrapolate to property values beyond their training distribution. Notably, this methodology is developed in a general manner, allowing it to apply to any material property dataset regardless of the specific model architecture, material property, or chemical system.\n* \u2022We perform the first large-scale benchmarking of the OOD performance of state-of-the-art ML chemical property prediction models. Across 10 diverse OOD tasks and 12 ML models, we do not find any existing models that show strong OOD generalization across all tasks. We therefore put forth BOOM OOD property prediction as a frontier challenge for chemical foundation models.\n* \u2022Our work highlights insights into how pretraining strategies, model architecture, molecule representation, and data augmentation impact OOD performance. These findings point towards strategies for the chemistry community to achieve chemical foundation models with strong OOD generalization across all chemical systems.\n## 2BOOM Overview\nIn general, one can define OOD with respect to either the model inputs (holding out a region of chemical space as the OOD test split) or with respect to the model outputs (holding out a range of chemical property values). In this work, we adopt the latter approach of benchmarking the performance of the models to extrapolate to property values not seen in training. Following the OOD definitions outlined by Farquhar et al., we here define OOD as a complement distribution with respect to the targets> [\n[> 5\n](https://arxiv.org/html/2505.01912v1#bib.bib5)> , [> 6\n](https://arxiv.org/html/2505.01912v1#bib.bib6)> ]\n. Specifically, given a molecule property dataset of chemical structures and their numerical property values, we create our OOD test set to consist of numerical values on the tail ends of the numerical property distribution (see Figure[1](https://arxiv.org/html/2505.01912v1#S2.F1)). In this way, our OOD benchmarking is directly aligned with the molecule discovery task in that it allows us to evaluate the consistency of ML models to discover molecules with state-of-the-art properties that extrapolate beyond the training data.\n### 2.1Datasets\nOverall, BOOM consists of 10 unique molecular property datasets. We collect 8 molecular property datasets from the QM9 Dataset: isotropic polarizability...",
      "url": "https://arxiv.org/html/2505.01912v1"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "A framework to evaluate machine learning crystal stability predictions",
      "text": "A framework to evaluate machine learning crystal stability predictions | Nature Machine Intelligence\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Machine Intelligence](https://media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-3a46d3127519f23b44cac085d4e82c58.svg)](https://www.nature.com/natmachintell)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42256-025-01055-1?error=cookies_not_supported&code=4b12e089-75e9-4c29-b9f2-45fc8c7d90d5)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42256)\n* [RSS feed](https://www.nature.com/natmachintell.rss)\nA framework to evaluate machine learning crystal stability predictions\n[Download PDF](https://www.nature.com/articles/s42256-025-01055-1.pdf)\n[Download PDF](https://www.nature.com/articles/s42256-025-01055-1.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:23 June 2025# A framework to evaluate machine learning crystal stability predictions\n* [Janosh Riebesell](#auth-Janosh-Riebesell-Aff1-Aff2)[ORCID:orcid.org/0000-0001-5233-3462](https://orcid.org/0000-0001-5233-3462)[1](#Aff1),[2](#Aff2),\n* [Rhys E. A. Goodall](#auth-Rhys_E__A_-Goodall-Aff1)[ORCID:orcid.org/0000-0002-6589-1700](https://orcid.org/0000-0002-6589-1700)[1](#Aff1),\n* [Philipp Benner](#auth-Philipp-Benner-Aff3)[ORCID:orcid.org/0000-0002-0912-8137](https://orcid.org/0000-0002-0912-8137)[3](#Aff3),\n* [Yuan Chiang](#auth-Yuan-Chiang-Aff2-Aff4)[2](#Aff2),[4](#Aff4),\n* [Bowen Deng](#auth-Bowen-Deng-Aff2-Aff4)[ORCID:orcid.org/0000-0003-4085-381X](https://orcid.org/0000-0003-4085-381X)[2](#Aff2),[4](#Aff4),\n* [Gerbrand Ceder](#auth-Gerbrand-Ceder-Aff2-Aff4)[ORCID:orcid.org/0000-0001-9275-3605](https://orcid.org/0000-0001-9275-3605)[2](#Aff2),[4](#Aff4),\n* [Mark Asta](#auth-Mark-Asta-Aff2-Aff4)[2](#Aff2),[4](#Aff4),\n* [Alpha A. Lee](#auth-Alpha_A_-Lee-Aff1)[1](#Aff1),\n* [Anubhav Jain](#auth-Anubhav-Jain-Aff2)[ORCID:orcid.org/0000-0001-5893-9967](https://orcid.org/0000-0001-5893-9967)[2](#Aff2)&amp;\n* \u2026* [Kristin A. Persson](#auth-Kristin_A_-Persson-Aff2-Aff4)[ORCID:orcid.org/0000-0003-2495-5509](https://orcid.org/0000-0003-2495-5509)[2](#Aff2),[4](#Aff4)Show authors\n[*Nature Machine Intelligence*](https://www.nature.com/natmachintell)**volume7**,pages836\u2013847 (2025)[Cite this article](#citeas)\n* 26kAccesses\n* 56Citations\n* 27Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42256-025-01055-1/metrics)\n### Subjects\n* [Atomistic models](https://www.nature.com/subjects/atomistic-models)\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n* [Combinatorial libraries](https://www.nature.com/subjects/combinatorial-libraries)\n* [Computational methods](https://www.nature.com/subjects/computational-methods)\nAn[Author Correction](https://doi.org/10.1038/s42256-025-01117-4)to this article was published on 02 September 2025\nThis article has been[updated](#change-history)\nA[preprint version](https://arxiv.org/abs/2308.14920)of the article is available at arXiv.\n## Abstract\nThe rapid adoption of machine learning in various scientific domains calls for the development of best practices and community agreed-upon benchmarking tasks and metrics. We present Matbench Discovery as an example evaluation framework for machine learning energy models, here applied as pre-filters to first-principles computed data in a high-throughput search for stable inorganic crystals. We address the disconnect between (1) thermodynamic stability and formation energy and (2) retrospective and prospective benchmarking for materials discovery. Alongside this paper, we publish a Python package to aid with future model submissions and a growing online leaderboard with adaptive user-defined weighting of various performance metrics allowing researchers to prioritize the metrics they value most. To answer the question of which machine learning methodology performs best at materials discovery, our initial release includes random forests, graph neural networks, one-shot predictors, iterative Bayesian optimizers and universal interatomic potentials. We highlight a misalignment between commonly used regression metrics and more task-relevant classification metrics for materials discovery. Accurate regressors are susceptible to unexpectedly high false-positive rates if those accurate predictions lie close to the decision boundary at 0\u2009eV per atom above the convex hull. The benchmark results demonstrate that universal interatomic potentials have advanced sufficiently to effectively and cheaply pre-screen thermodynamic stable hypothetical materials in future expansions of high-throughput materials databases.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs43588-023-00536-w/MediaObjects/43588_2023_536_Fig1_HTML.png)\n### [Accelerating the prediction of stable materials with machine learning](https://www.nature.com/articles/s43588-023-00536-w?fromPaywallRec=false)\nArticle09 November 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-024-01472-7/MediaObjects/41524_2024_1472_Fig1_HTML.png)\n### [Accelerating materials property prediction via a hybrid Transformer Graph framework that leverages four body interactions](https://www.nature.com/articles/s41524-024-01472-7?fromPaywallRec=false)\nArticleOpen access18 January 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-06735-9/MediaObjects/41586_2023_6735_Fig1_HTML.png)\n### [Scaling deep learning for materials discovery](https://www.nature.com/articles/s41586-023-06735-9?fromPaywallRec=false)\nArticleOpen access29 November 2023\n## Main\nThe challenge of evaluating, benchmarking and then applying the rapid evolution of machine learning (ML) models is common across scientific domains. Specifically, the lack of agreed-upon tasks and datasets can obscure the performance of the model, making comparisons difficult. Materials science is one such domain, where in the last decade, the numbers of ML publications and associated models have increased dramatically. Similar to other domains, such as drug discovery and protein design, the ultimate success is often associated with the discovery of a new material with specific functionality. In the combinatorial sense, materials science can be viewed as an optimization problem of mixing and arranging different atoms with a merit function that captures the complex range of properties that emerge. To date, \\~105combinations have been tested experimentally[1](https://www.nature.com/articles/s42256-025-01055-1#ref-CR1),[2](https://www.nature.com/articles/s42256-025-01055-1#ref-CR2), \\~107have been simulated[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s42256-025-01055-1#ref-CR7)and upwards of \\~1010possible quaternary materials are allowed by electronegativity and charge-balancing rules[8](https://www.nature.com/articles/s42256-025-01055-1#ref-CR8). The space of quinternaries and higher is even less explored, leaving vast numbers of potentially useful materials to be discovered. The discovery of new materials is a key driver of technological progress and lies on the path to more efficient solar cells, lighter and longer-lived batteries, and sm...",
      "url": "https://www.nature.com/articles/s42256-025-01055-1"
    },
    {
      "title": "Benchmarking ML in ADMET predictions: the practical impact ... - NIH",
      "text": "Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1186/s13321-025-01041-0)\n* [](pdf/13321_2025_Article_1041.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Journal of Cheminformatics logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jcheminfo.png)\nJ Cheminform\n. 2025 Jul 21;17:108. doi:[10.1186/s13321-025-01041-0](https://doi.org/10.1186/s13321-025-01041-0)\n# Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models\n[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n### Gintautas Kamuntavi\u010dius\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n1,\u2709,#,[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n### Tanya Paquet\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n1,#,[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n### Orestis Bastas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n1,#,[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n### Dainius \u0160alkauskas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n1,[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n### Alvaro Prat\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n1,[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n### Hisham Abdel Aty\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n1,[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n### Aurimas Pabrinkis\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n1,[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n### Povilas Norvai\u0161as\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n1,[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n### Roy Tal\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n1\n* Author information\n* Article notes\n* Copyright and License information\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\n\u2709Corresponding author.\n#\nContributed equally.\nReceived 2025 Jan 27; Accepted 2025 Jun 1; Collection date 2025.\n\u00a9The Author(s) 2025\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC12281724\u00a0\u00a0PMID:[40691635](https://pubmed.ncbi.nlm.nih.gov/40691635/)\n## Abstract\nThis study, focusing on predicting Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) properties, addresses the key challenges of ML models trained using ligand-based representations. We propose a structured approach to data feature selection, taking a step beyond the conventional practice of combining different representations without systematic reasoning. Additionally, we enhance model evaluation methods by integrating cross-validation with statistical hypothesis testing, adding a layer of reliability to the model assessments. Our final evaluations include a practical scenario, where models trained on one source of data are evaluated on a different one. This approach aims to bolster the reliability of ADMET predictions, providing more dependable and informative model evaluations.\n**Scientific contribution**\nThis study provided a structured approach to feature selection. We improve model evaluation by combining cross-validation with statistical hypothesis testing, making results more reliable. The methodology used in our study can be generalized beyond feature selection, boosting the confidence in selected models which is crucial in a noisy domain such as the ADMET prediction tasks. Additionally, we assess how well models trained on one dataset perform on another, offering practical insights for using external data in drug discovery.\n## Introduction\nThe Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) of compounds are commonly estimated throughout drug discovery projects, as the feasibility of a compound to become a viable drug highly depends on it. Through the years, a lot of work has gone into building and evaluating machine learning (ML) systems designed to predict molecular properties that are associated with ADMET. Public curated datasets and benchmarks for ADMET associated properties are becoming increasingly available to the community, creating the opportunity for more widespread exploration of ML algorithms and techniques in this space. The Therapeutics Data Commons (TDC) ADMET leaderboard showcases this [[1](#CR1)], highlighting a wide variety of models, features, and processing methods investigated...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724"
    },
    {
      "title": "Cross-Validation - Iterate.ai",
      "text": "## What is it?\n\nCross-validation is a technique used in the field of artificial intelligence and machine learning to assess how well a model performs on a given dataset. It involves dividing the dataset into multiple subsets, training the model on a portion of the data, and then testing it on the remaining subset. This process is repeated several times to ensure that the model is able to generalize well to new, unseen data.\n\nFor business people, cross-validation is relevant because it helps in ensuring the accuracy and reliability of predictive models used in various business applications. By using cross-validation, business executives can have confidence that the models they are using to make critical decisions, such as forecasting sales or predicting customer behavior, are robust and not overfit to the training data.\n\nThis can lead to more accurate insights and ultimately better business outcomes. Additionally, cross-validation can help in identifying and addressing potential issues with the model, leading to improvements in performance and overall efficiency. Overall, understanding and utilizing cross-validation can contribute to more effective and informed decision-making within the business context.\n\n## How does it work?\n\nCross-validation is a technique used in artificial intelligence to evaluate the performance of a machine learning model. Think of it like when you hire a new employee and you want to see how well they perform before giving them a permanent job.\n\nIn the case of cross-validation, the machine learning model is tested multiple times with different sets of data to make sure it can consistently make accurate predictions. This helps to ensure that the model isn\u2019t just good at predicting one specific set of data, but rather is generally good at making predictions in different situations.\n\nFor example, let\u2019s say you have a sales forecasting model for your business. You can use cross-validation to test the model\u2019s accuracy with different sales data from different time periods. This way, you can be more confident in the model\u2019s ability to predict future sales, even when the market conditions change.\n\nOverall, cross-validation is an important tool in ensuring that machine learning models are reliable and can be trusted to make accurate predictions in real-world business scenarios.\n\n### Pros\n\n1. Helps to reduce overfitting: Cross-validation helps to assess how the results of a model will generalize to an independent dataset, thereby reducing the risk of overfitting.\n2. Better use of data: By splitting the data into multiple subsets, cross-validation allows for better utilization of available data for both training and testing the model.\n3. Reliable performance estimation: Cross-validation provides a more reliable estimate of the model\u2019s performance compared to a single train-test split.\n\n### Cons\n\n1. Computational overhead: Cross-validation requires multiple iterations of model training and testing, which can be computationally expensive, especially for large datasets and complex models.\n2. Sensitivity to data splitting: The results of cross-validation can be sensitive to how the data is partitioned, and different splits may lead to different performance estimates.\n3. Not suitable for all types of data: Cross-validation may not be suitable for certain types of data, such as time-series data, where the sequence of data points is important and cannot be randomly split.\n\nStay Informed on How your Industry is Using AI\n\nDig deep into the latest AI trends and see how they can turbocharge your\u00a0future.\n\n[Learn more](https://www.aiexplored.ai/)\n\n## Applications and Examples\n\nCross-validation is a technique used in machine learning to assess the performance of a model. For example, in the context of building a spam filter for emails, cross-validation would be used to test the model\u2019s ability to accurately classify spam and non-spam emails by splitting the dataset into multiple subsets and testing the model on each subset to ensure that it generalizes well to new data.\n\nBy using cross-validation, the spam filter can be confidently deployed in real-world scenarios knowing that it has been rigorously tested for accuracy.\n\n## History and Evolution\n\nCross-validation is a term that originated in the field of statistics and machine learning, first discussed by researchers in the 1970s as a method for assessing the performance of predictive models.\n\nIt involves partitioning data into subsets, training the model on some of the subsets, and then testing it on the remaining data to evaluate its predictive accuracy. Today, cross-validation is essential for AI as it helps to prevent overfitting and ensure that machine learning models can generalize well to new, unseen data, leading to more robust and reliable AI systems.\n\n\u200d\n\n## FAQs\n\nWhat is cross-validation in AI?\n\nCross-validation is a technique used to evaluate machine learning models by training and testing on multiple subsets of the available data to avoid overfitting.\n\nWhy is cross-validation important in machine learning?\n\nCross-validation is important because it provides a more accurate estimate of the model's performance, helps in identifying overfitting, and ensures that the model generalizes well to new data.\n\nWhat are the different types of cross-validation methods?\n\nThe main types of cross-validation methods include k-fold cross-validation, leave-one-out cross-validation, and stratified cross-validation, each with its own pros and cons.\n\nHow do you implement cross-validation in practice?\n\nTo implement cross-validation, split the dataset into training and testing sets, choose a cross-validation method, and iterate through each fold to train and test the model, evaluating its performance.\n\nCan cross-validation be used for all types of machine learning models?\n\nYes, cross-validation can be used for various types of machine learning models, including regression, classification, and clustering, as a way to assess their performance.\n\n## Takeaways\n\nCross-validation is an important method used in business to assess the accuracy and generalizability of a predictive model. It involves splitting a data set into multiple subsets, training the model on some of the subsets, and testing it on the remaining subsets to measure its performance. The main goal of cross-validation is to ensure that a model can make accurate predictions on new, unseen data.\n\nUnderstanding cross-validation is crucial for businesses as it helps in selecting the best predictive model for making important business decisions. It also aids in avoiding overfitting, which occurs when a model performs well on the training data but poorly on new data. By utilizing cross-validation, businesses can enhance the reliability and robustness of their predictive models, ultimately leading to better decision-making and improved outcomes.\n\nTherefore, business people need to grasp the concept of cross-validation to make informed choices about the predictive models they use and to ensure the accuracy and effectiveness of their data-driven strategies.\n\n\u200d",
      "url": "https://www.iterate.ai/ai-glossary/cross-validation-explained"
    },
    {
      "title": "[PDF] BENCHMARKING BAND GAP PREDICTION FOR ...",
      "text": "Accepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nBENCHMARKING BAND GAP PREDICTION FOR\nSEMICONDUCTOR MATERIALS USING\nMULTIMODAL AND MULTI-FIDELITY DATA\nHaolin Wang1,2, Xianyuan Liu1,2, Anna Jungbluth3, Alex Ramadan4, Robert Oliver5,\nHaiping Lu1,2\n1 Centre for Machine Intelligence, University of Sheffield\n2 School of Computer Science, University of Sheffield\n3 Climate Office, European Space Agency\n4 School of Mathematical and Physical Sciences, University of Sheffield\n5 School of Chemical, Materials and Biological Engineering, University of Sheffield\n{haolin.wang, h.lu}@shef.ac.uk\nABSTRACT\nThe band gap is critical for understanding the electronic properties of materi\u0002als in semiconductor applications. While density functional theory is commonly\nused to estimate band gaps, it often underestimates values and remains compu\u0002tationally expensive, limiting its practical usefulness. Machine learning (ML)\nhas become a promising alternative for accurate and efficient band gap predic\u0002tions. However, existing datasets are limited in data modality, fidelity and sam\u0002ple size, and performance evaluation studies often lack direct comparisons be\u0002tween traditional and advanced ML models. Therefore, a more comprehensive\nevaluation is needed to make progress towards real-world impacts. In this pa\u0002per, we developed a benchmarking framework for ML-based band gap prediction\nto address this gap. We compiled a new multimodal, multi-fidelity dataset from\nthe Materials Project and BandgapDatabase1, consisting of 60,218 low-fidelity\ncomputational band gaps and 1,183 high-fidelity experimental band gaps across\n10 material categories. We evaluated seven ML models, from traditional meth\u0002ods to graph neural networks, assessing their ability to learn from atomic prop\u0002erties and structural information. To promote real-world applicability, we em\u0002ployed three metrics: mean absolute error, mean relative absolute error, and co\u0002efficient of determination R2\n. Moreover, we introduced a leave-one-material\u0002out evaluation strategy to better reflect real-world scenarios where new mate\u0002rials have little to no prior training data. Our findings offer valuable insights\ninto model selection and evaluation for band gap prediction across material cate\u0002gories, providing guidance for real-world applications in materials discovery and\nsemiconductor design. The data and code used in this work are available at:\nhttps://github.com/Shef-AIRE/bandgap-benchmark.\n1 INTRODUCTION\nThe band gap, defined as the energy difference between the valence and conduction bands, is a\nfundamental property of periodic solids and plays a critical role in determining their electrical con\u0002ductivity. This property is widely utilized in semiconductor applications (Yoder, 1996), including\nlight-emitting diodes (LEDs) (Lisensky et al., 1992), transistors (Ueno et al., 2004), and photovoltaic\ndevices (Goetzberger & Hebling, 2000). However, accurately determining the band gap of a mate\u0002rial remains a significant challenge. Theoretical methods, such as density functional theory (DFT),\nare commonly used but often underestimate band gaps due to limitations in exchange-correlation\nfunctionals. More advanced methods, such as the G0W0 approximation and hybrid functionals\n(Heyd et al., 2003), provide improved accuracy but are computationally intensive and require metic\u0002ulous parameter tuning. Recently, machine learning (ML) has emerged as a promising alternative\n1\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nML Pipeline\nCGCNN\nLEFTNet-Z\nLEFTNet-Prop\nNeural Networks\nCartNet\nTraditional ML Methods\nLinear Regression\nRandom Forest Regression\nSupport Vector Regression\nLow Fidelity (DFT) High Fidelity (Expt.)\nDataset\nMulti-Fidelity\nMultimodality Atomic Properties Crystal Structure\nSplitting K-fold Cross-Validation Leave-One-Material-Out\nEvaluation MAE MRAE\nFigure 1: Flowchart of our proposed benchmark. The benchmark categorizes data based on fi\u0002delity and modality, incorporating low-fidelity (DFT) and high-fidelity (experimental) band gaps,\nalong with multimodal features. Beyond traditional K-fold cross-validation, a leave-one-material\u0002out strategy is introduced to better reflect real-world scenarios. The machine learning (ML) pipeline\nstudies both traditional ML methods and more recent neural networks. For evaluation, mean relative\nabsolute error (MRAE) is introduced to enhance applicability, alongside mean absolute error (MAE)\nand the coefficient of determination R2.\nfor predicting band gaps. Unlike conventional theoretical methods, ML methods can capture com\u0002plex structure-property relationships from large datasets, enabling accurate and efficient predictions\nwithout expensive calculations.\nMachine learning predicts band gaps primarily using two complementary types of information:\natomic properties and crystal structure. These modalities represent the material from different per\u0002spectives, forming a multimodal data representation (Liu et al., 2025). Atomic properties capture\nintrinsic characteristics of individual atoms that influence electronic behavior and have been widely\nused in band gap modeling (Talapatra et al., 2023). For instance, Sabagh Moeini et al. (2024) used\neight atomic features to train linear models and identified the standard deviation of valence electrons\nas a key predictor for band gaps in perovskites.\nAdvanced graph representation learning models (Schutt et al., 2017; Choudhary & DeCost, 2021) \u00a8\nextract crystal structure information via graph neural networks (GNNs) and capture atomic inter\u0002actions by analyzing distance and orientation. These models better utilize the underlying physics\nof crystal structures via the three-dimensional arrangement of atoms, making them an intuitive and\nsuitable approach for accurate property prediction. Additionally, structural information comple\u0002ments atomic properties by providing a global context that connects the local information carried by\nindividual atoms. The Crystal Graph Convolutional Neural Network (CGCNN) (Xie & Grossman,\n2018) is one of the most widely used models for structure-based materials property prediction. It in\u0002corporates nine atomic properties along with interatomic distance information. Subsequent models,\nsuch as CartNet (Sole et al., 2025), extend this idea by explicitly encoding the full 3D structure of \u00b4\nmaterials. Another approach, LEFTNet (Du et al., 2023), further improves predictive performance\nby capturing higher-order geometric features, including bond angles and local orientations. How\u0002ever, none of these methods have been evaluated against traditional machine learning models within\na unified benchmark.\nSeveral benchmark studies have compared machine learning models for predicting various material\nproperties. For example, MatBench (Dunn et al., 2020) provides a leaderboard for structure-based\nproperty predictions in inorganic materials, covering 13 supervised learning tasks (including band\ngap prediction) and incorporating both DFT and experimental data. Similar ML benchmarking\nefforts for band gap prediction include MatDeepLearn (Fung et al., 2021), Varivoda et al. (2023),\nand the JARVIS-Leaderboard (Choudhary et al., 2024). These benchmarks primarily rely on band\n2\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\ngap databases, the Materials Project (Jain et al., 2013) and QMOF (Rosen et al., 2021; 2022), that\nprovide DFT-calculated band gaps only. On the other hand, experimental datasets, such as the\none from Zhuo et al. (2018), contain only compositional information, making them unsuitable for\nstructure-based approaches.\nMasood et al. (2023) introduced a multi-fidelity open-access dataset that includes 3D structures,\ncomputational band gaps, and experimental band gaps, offering a more suitable resource for\nstructure-based band gap prediction. However, the evaluation dataset is relatively small, containing\nonly 30 materials, and lacks representation of key material categories suc...",
      "url": "https://openreview.net/pdf?id=u8FripvaG5"
    }
  ]
}