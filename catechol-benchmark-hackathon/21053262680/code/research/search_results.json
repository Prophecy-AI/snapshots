{
  "query": "How do you handle distribution shift in molecular property prediction when test molecules are structurally different from training molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Handling distribution shift\u202f\u2014\u202fespecially when test molecules lie in a different region of chemical space than the training set\u202f\u2014\u202fcan be tackled with a few practical, research\u2011backed steps:\n\n1. **Quantify the shift before training**  \n   - Compute similarity\u2011based distances (e.g., Bemis\u2011Murcko scaffold overlap or ECFP4\u2011based K\u2011means clustering) between the training set and the intended test set. Scaffold\u2011based splits expose the hardest OOD scenario, while cluster\u2011based splits reveal covariate\u2011shift severity\u30108\u2020https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866\u3011.  \n\n2. **Create OOD\u2011aware validation splits**  \n   - Use *scaffold splitting* to hold out whole molecular scaffolds from training, mimicking structural novelty\u30105\u2020https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction\u3011.  \n   - Complement with *chemical\u2011similarity clustering* (K\u2011means on fingerprints) to generate a more challenging OOD split\u30108\u2020https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866\u3011.  \n\n3. **Learn environment\u2011invariant substructures**  \n   - Adopt a framework such as **MoleOOD**: train an environment\u2011inference model to discover latent factors that differentiate data sources, then regularize the molecular encoder to rely on substructures (scaffolds, size\u2011related motifs) that remain predictive across those environments\u30106\u2020https://proceedings.neurips.cc/paper_files/paper/2022/file/547108084f0c2af39b956f8eadb75d1b-Paper-Conference.pdf\u3011.  \n\n4. **Leverage transductive or semi\u2011supervised OOD techniques**  \n   - Apply a transductive approach that jointly embeds training and unlabeled test molecules, allowing the model to adjust its representation to the test distribution and improve prediction accuracy\u30101\u2020https://www.nature.com/articles/s41524-025-01808-x\u3011.  \n\n5. **Meta\u2011learn to adapt across distributions**  \n   - Use a meta\u2011learning scheme that trains on multiple \u201cepisodes\u201d of ID\u202f\u2192\u202fOOD shifts, exploiting abundant unlabeled data to interpolate between the two domains. This reduces covariate\u2011shift bias and yields more stable predictions on novel compounds\u30109\u2020https://arxiv.org/abs/2506.11877\u3011.  \n\n6. **Harmonize target distributions when possible**  \n   - If multiple datasets are combined, standardize each endpoint\u2019s distribution (e.g., z\u2011score or quantile\u2011transform) before model training to mitigate label\u2011space mismatches that can exacerbate OOD errors\u30102\u2020https://pmc.ncbi.nlm.nih.gov/articles/PMC12574261\u3011.  \n\n7. **Validate the correlation between ID and OOD performance**  \n   - After training, check the linear relationship (Pearson\u202fr) between ID and OOD metrics for each split. Strong correlation (e.g.,\u202fr\u202f\u2248\u202f0.9 for scaffold splits) indicates that good ID performance likely transfers, whereas weak correlation (r\u202f\u2248\u202f0.4 for cluster splits) signals the need for the above robustness tricks\u30108\u2020https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866\u3011.  \n\nFollowing these steps\u2014quantifying shift, using scaffold/cluster splits, enforcing substructure invariance, applying transductive/meta\u2011learning adaptations, and normalizing target distributions\u2014provides a systematic workflow to mitigate distribution shift when predicting properties of structurally novel molecules.",
      "url": ""
    },
    {
      "title": "Out-of-Distribution Property Prediction in Materials and ...",
      "text": "Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules | npj Computational Materials\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![npj Computational Materials](https://media.springernature.com/full/nature-cms/uploads/product/npjcompumats/header-4baba14304e9c3518bdc0d6f35b470b9.svg)](https://www.nature.com/npjcompumats)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41524-025-01808-x?error=cookies_not_supported&code=700d2411-cae5-4b77-8d63-7e62cff2a842)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41524)\n* [RSS feed](https://www.nature.com/npjcompumats.rss)\nKnown Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n[Download PDF](https://www.nature.com/articles/s41524-025-01808-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:20 November 2025# Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules\n* [Nofit Segal](#auth-Nofit-Segal-Aff1)[1](#Aff1)[na1](#na1),\n* [Aviv Netanyahu](#auth-Aviv-Netanyahu-Aff2)[2](#Aff2)[na1](#na1),\n* [Kevin P. Greenman](#auth-Kevin_P_-Greenman-Aff3-Aff4)[3](#Aff3),[4](#Aff4),\n* [Pulkit Agrawal](#auth-Pulkit-Agrawal-Aff2)[2](#Aff2)&amp;\n* \u2026* [Rafael G\u00f3mez-Bombarelli](#auth-Rafael-G_mez_Bombarelli-Aff1)[1](#Aff1)Show authors\n[*npj Computational Materials*](https://www.nature.com/npjcompumats)**volume11**, Article\u00a0number:345(2025)[Cite this article](#citeas)\n* 3094Accesses\n* 1Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41524-025-01808-x/metrics)\n### Subjects\n* [Chemistry](https://www.nature.com/subjects/chemistry)\n* [Materials science](https://www.nature.com/subjects/materials-science)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n## Abstract\nDiscovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (OOD) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to OOD property prediction, achieving improvements in prediction accuracy. In particular, our method improves extrapolative precision by 1.8\u00d7 for materials and 1.5\u00d7 for molecules, and boosts recall of high-performing candidates by up to 3\u00d7. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-66685-w/MediaObjects/41467_2025_66685_Fig1_HTML.png)\n### [Molecular Motif Learning as a pretraining objective for molecular property prediction](https://www.nature.com/articles/s41467-025-66685-w?fromPaywallRec=false)\nArticleOpen access27 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-53751-y/MediaObjects/41467_2024_53751_Fig1_HTML.png)\n### [MolE: a foundation model for molecular graphs using disentangled attention](https://www.nature.com/articles/s41467-024-53751-y?fromPaywallRec=false)\nArticleOpen access12 November 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n## Introduction\nDesigning new materials and molecules is essential for the development of new technologies. Traditionally, this design process involves extensive experimental iteration or high-throughput methods to screen databases, which are time-consuming and resource-intensive[1](https://www.nature.com/articles/s41524-025-01808-x#ref-CR1),[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2). As a result, there is increasing interest in applying machine learning (ML) techniques to accelerate the discovery of materials and molecules with desired properties[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s41524-025-01808-x#ref-CR7). There is particular interest in property values that are outside the known property value distribution, as these will most likely lead to discovering new materials that will, in turn, unlock new capabilities and technologies.\nOne strategy for discovering materials and molecules with desired properties is inverse design via conditional generation, where the goal is to create candidates with out-of-distribution (OOD) property values that are absent from the training data[2](https://www.nature.com/articles/s41524-025-01808-x#ref-CR2),[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[5](https://www.nature.com/articles/s41524-025-01808-x#ref-CR5),[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41524-025-01808-x#ref-CR10). A complementary strategy is virtual screening, where a large database of candidate materials is evaluated using predicted properties[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](https://www.nature.com/articles/s41524-025-01808-x#ref-CR16). In this case, the objective is to identify high-performing OOD candidates from a set of known compositions with unknown properties. Screening often involves applying a threshold, either an absolute value or a percentile cutoff, and selecting candidates whose predicted properties exceed it[17](#ref-CR17),[18](#ref-CR18),[19](https://www.nature.com/articles/s41524-025-01808-x#ref-CR19). However, both generative and screening approaches commonly face challenges when the target property values lie outside the distribution of the training data[4](https://www.nature.com/articles/s41524-025-01808-x#ref-CR4),[13](https://www.nature.com/articles/s41524-025-01808-x#ref-CR13),[15](https://www.nature.com/articles/s41524-025-01808-x#ref-CR15),[20](#ref-CR20),[21](#ref-CR21),[22](https://www.nature.com/articles/s41524-025-01808-x#ref-CR22). Enhancing extrapolative capabilities in property prediction would improve the screening of large candidate spaces in terms of precision by identifying promising compounds and molecules with exceptional properties. This has the potential to streamline the identification process by reducing time and resource expenditure on low-potential candidates, thereby accelerating the discovery of materials and molecules with high synthesis viability.\nExtrapolation in materials science can refer to both the domain and the range of the predictive function. Typically, extrapolation is used to refer to*generalization*in the domain space, i.e., unseen classes of materials, structures, and chemical spaces, e.g., training on metals and predicting ceramics, or training on ar...",
      "url": "https://www.nature.com/articles/s41524-025-01808-x"
    },
    {
      "title": "Enhancing molecular property prediction through data ...",
      "text": "[Skip to main content](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#main-content)\n\n**Official websites use .gov**\nA\n**.gov** website belongs to an official\ngovernment organization in the United States.\n\n**Secure .gov websites use HTTPS**\nA **lock** (\n\nLocked padlock icon\n) or **https://** means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n\nSearch PMC Full-Text ArchiveSearch in PMC\n\n- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n\n- ## PERMALINK\n\n\n\nCopy\n\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:\n[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nJ Cheminform\n\n. 2025 Oct 29;17:163. doi: [10.1186/s13321-025-01103-3](https://doi.org/10.1186/s13321-025-01103-3)\n\n# Enhancing molecular property prediction through data integration and consistency assessment\n\n[Raquel Parrondo-Pizarro](https://pubmed.ncbi.nlm.nih.gov/?term=%22Parrondo-Pizarro%20R%22%5BAuthor%5D)\n\n### Raquel Parrondo-Pizarro\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\nFind articles by [Raquel Parrondo-Pizarro](https://pubmed.ncbi.nlm.nih.gov/?term=%22Parrondo-Pizarro%20R%22%5BAuthor%5D)\n\n1,2, [Luca Menestrina](https://pubmed.ncbi.nlm.nih.gov/?term=%22Menestrina%20L%22%5BAuthor%5D)\n\n### Luca Menestrina\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Luca Menestrina](https://pubmed.ncbi.nlm.nih.gov/?term=%22Menestrina%20L%22%5BAuthor%5D)\n\n1, [Ricard Garcia-Serna](https://pubmed.ncbi.nlm.nih.gov/?term=%22Garcia-Serna%20R%22%5BAuthor%5D)\n\n### Ricard Garcia-Serna\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Ricard Garcia-Serna](https://pubmed.ncbi.nlm.nih.gov/?term=%22Garcia-Serna%20R%22%5BAuthor%5D)\n\n1, [Adri\u00e0 Fern\u00e1ndez-Torras](https://pubmed.ncbi.nlm.nih.gov/?term=%22Fern%C3%A1ndez-Torras%20A%22%5BAuthor%5D)\n\n### Adri\u00e0 Fern\u00e1ndez-Torras\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\nFind articles by [Adri\u00e0 Fern\u00e1ndez-Torras](https://pubmed.ncbi.nlm.nih.gov/?term=%22Fern%C3%A1ndez-Torras%20A%22%5BAuthor%5D)\n\n1,\u2709, [Jordi Mestres](https://pubmed.ncbi.nlm.nih.gov/?term=%22Mestres%20J%22%5BAuthor%5D)\n\n### Jordi Mestres\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\nFind articles by [Jordi Mestres](https://pubmed.ncbi.nlm.nih.gov/?term=%22Mestres%20J%22%5BAuthor%5D)\n\n1,2,\u2709\n\n- Author information\n- Article notes\n- Copyright and License information\n\n1Chemotargets SL, Parc Cientific de Barcelona, Baldiri Reixac 4 (TR-03), 08028 Barcelona, Catalonia Spain\n\n2Institut de Quimica Computacional i Catalisi, Facultat de Ciencies, Universitat de Girona, Maria Aurelia Capmany 69, 17003 Girona, Catalonia Spain\n\n\u2709\n\nCorresponding author.\n\nReceived 2025 Jun 5; Accepted 2025 Sep 21; Collection date 2025.\n\n\u00a9 The Author(s) 2025\n\n**Open Access** This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by-nc-nd/4.0/](https://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nPMCID: PMC12574261\u00a0\u00a0PMID: [41163219](https://pubmed.ncbi.nlm.nih.gov/41163219/)\n\n## Abstract\n\nData heterogeneity and distributional misalignments pose critical challenges for machine learning models, often compromising predictive accuracy. These challenges are exemplified in preclinical safety modeling, a crucial step in early-stage drug discovery where limited data and experimental constraints exacerbate integration issues. Analyzing public ADME datasets, we uncovered significant misalignments as well as inconsistent property annotations between gold-standard and popular benchmark sources, such as Therapeutic Data Commons. These dataset discrepancies, which can arise from differences in various factors, including experimental conditions in data collection as well as chemical space coverage, can introduce noise and ultimately degrade model performance. Data standardization, despite harmonizing discrepancies and increasing the training set size, may not always lead to an improvement in predictive performance. This highlights the importance of rigorous data consistency assessment (DCA) prior to modeling. To facilitate a systematic DCA across diverse datasets, we developed AssayInspector, a model-agnostic package that leverages statistics, visualizations, and diagnostic summaries to identify outliers, batch effects, and discrepancies. Beyond preclinical safety, DCA can play a crucial role in federated learning scenarios, enabling effective transfer learning across heterogeneous data sources and supporting reliable integration across diverse scientific domains.\n\n### Supplementary Information\n\nThe online version contains supplementary material available at 10.1186/s13321-025-01103-3.\n\n**Keywords:** Data reporting, Molecular property, ADME, Physicochemical, Machine learning, Data aggregation, Predictive accuracy, Benchmark\n\n## Scientific contribution\n\nBy systematically analyzing public ADME datasets, we uncovered substantial distributional misalignments and annotation discrepancies between benchmark and gold-standard sources. These challenges were shown to undermine predictive modeling, as naive integration or standardization often degraded performance. To address them, we present AssayInspector, a tool that enables both consistency assessment and informed data integration, providing a foundation for more reliable predictive modelling in drug discovery.\n\n### Graphical Abstract\n\n### Supplementary Information\n\nThe online version contains supplementary material available at 10.1186/s13321-025-01103-3.\n\n## Introduction\n\nMachine learning (ML) has emerged as a powerful tool for scientific discovery, providing cost-effective predictions that accelerate research across various domains \\[ [1](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR1)\u2013 [3](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR3)\\]. Yet, the accuracy and reliability of ML models depend on the quality, size, and consistency of training data \\[ [4](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR4), [5](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#CR5)\\]. In this context, integrating publicly available datasets offers an opportunity to increase sample sizes and expand feature space ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12574261"
    },
    {
      "title": "Out-of-distribution generalisation and scaffold splitting in ...",
      "text": "The ability to successfully apply previously acquired knowledge to novel and unfamiliar situations is one of the main hallmarks of successful learning and general intelligence. This capability to effectively **generalise** is amongst the most desirable properties a prediction model (or a mind, for that matter) can have.\n\nIn supervised machine learning, the standard way to evaluate the generalisation power of a prediction model for a given task is to randomly split the whole available data set into two sets \u2013 a training set and a test set . The model is then subsequently trained on the examples in the training set and afterwards its prediction abilities are measured on the untouched examples in the test set via a suitable performance metric.\n\nSince in this scenario the model has never seen any of the examples in during training, its performance on must be indicative of its performance on novel data which it will encounter in the future. Right?\n\nNo.\n\nIn practise, one can regularly observe a situation where a machine learning model which performs well on a randomly selected test set fails spectacularly when confronted with novel data which was collected at a later point in time, by a different lab, in a different environment, or in some other context that differs from the original context in which the initial data set was collected. The reason for this can be found in the **distributional shift** between and which frequently occurs when the data collection context (and thus the data generating process) is altered in some way.\n\nIf the data split for the initial data set into training set and test set is done uniformly at random (as is usual), then both and follow the same distribution. This random uniform data split is very much in accordance with the framework of classical statistical learning theory \\[1\\], where one assumes that a learning model is primarily built to deal with training- and test data examples that have all been sampled independently from the same underlying probability distribution.\n\nUnfortunately, a random uniform data split is rarely a good simulation of practical reality where a newly collected data set which is fed into a machine learning model to obtain predictions almost never follows the data distribution of the data set on which the model was originally trained. This distributional shift between the initial training data set and the newly collected data set normally leads to a substantial drop in performance of the model on compared to its performance on a test set which follows the same distribution as . Thus, splitting the initial data set uniformly at random into a test set and a training set often leads to overoptimistic results when trying to estimate the predictive abilities of a machine learning model in a practical setting.\n\nTo get a more reliable picture of the real-world predictive capabilities of a trained machine learning model one must find a way to model a meaningful distributional shift and build it into the test set . Evaluating the model on can then provide a measure for the **out-of-distribution generalisation abilities** of the model.\n\nMeasuring out-of-distribution generalisation is of particular relevance in the field of **molecular property prediction** where distributional shifts tend to be large and difficult to handle for machine learning models. Different molecular data sets obtained by distinct pharmaceutical companies and research groups often contain compounds from vastly different areas of chemical space that exhibit high structural heterogeneity. An elegant solution for the modelling of such distributional shifts in chemical space is given by the idea of **scaffold splitting**.\n\nThe notion of a (two-dimensional) molecular scaffold is described in the article by Bemis and Murcko \\[2\\]. A molecular scaffold reduces the chemical structure of a compound to its core components, essentially by removing all side chains and only keeping ring systems and parts which link together ring systems. An additional option for making molecular scaffolds even more general is to \u201cforget\u201d the identities of the bonds and atoms by replacing all atoms with carbons and all bonds with single bonds.\n\nBemis-Murcko scaffolds can be automatically generated in RDKit via the following Python code:\n\n```\n# how to extract the Bemis-Murcko scaffold of a molecular compound via RDKit\n# import packages\nfrom rdkit import Chem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n# define compound via its SMILES string\nsmiles = \"CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC\"\n# convert SMILES string to RDKit mol object\nmol = Chem.MolFromSmiles(smiles)\n# create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound\nmol_scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n# make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds\nmol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold)\n# convert the generic scaffold mol object back to a SMILES string format\nsmiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic))\n# display compound and its generic Bemis-Murcko scaffold\ndisplay(mol)\nprint(smiles)\ndisplay(mol_scaffold_generic)\nprint(smiles_scaffold_generic)\n```\n\nIf we now have a molecular data set , we can map each compound in to its respective scaffold. Let us assume that a total number of pairwise distinct scaffolds appear in and that these scaffolds are numbered consecutively from to . We can then define an **equivalence relation** on by calling two compounds equivalent if they share the same scaffold. The associated equivalence classes consist of compound sets whereby a given set contains all compounds in which share the -th scaffold. It is not hard to see that the sets form a partition of the original data set . Without loss of generality, we assume that the equivalence classes are ordered by size in descending order, i.e. we assume that contains at least as many molecules as , and so on.\n\nOne appropriate way to now produce a scaffold split of the molecular data set into a training set and a test set for machine learning is to define as the union of the first (larger) sets and as the union of the last (smaller) sets . Here is a custom index parameter which can be used to control the respective sizes of and ; frequently is chosen such that contains approximately of the examples in .\n\nWhile a scaffold split is certainly not perfect, it is already a lot better than a uniform random split at providing a relevant measure of the practical utility of a molecular property prediction model. It mimics a situation where the training set was sampled from a structurally different area of chemical space than the test set . This creates a distributional shift between and which is comparable to the distributional shifts which are commonly observed in real chemical data sets. Evaluating a molecular machine learning model using a scaffold split rather than a uniform random split thus leads to significantly more robust results.\n\n**References:**\n\n\\[1\\] Poggio, Tomaso, and Christian R. Shelton. \u201cOn the mathematical foundations of learning.\u201d _American Mathematical Society_ 39.1 (2002): 1-49.\n\n\\[2\\] Bemis, Guy W., and Mark A. Murcko. \u201cThe properties of known drugs. 1. Molecular frameworks.\u201d _Journal of medicinal chemistry_ 39.15 (1996): 2887-2893.",
      "url": "https://www.blopig.com/blog/2021/06/out-of-distribution-generalisation-and-scaffold-splitting-in-molecular-property-prediction"
    },
    {
      "title": "Learning Substructure Invariance for Out-of-Distribution ...",
      "text": "Learning Substructure Invariance for\nOut-of-Distribution Molecular Representations\nNianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, Junchi Yan\u2217\nDepartment of Computer Science and Engineering\nMoE Key Lab of Artificial Intelligence\nShanghai Jiao Tong University\n{yangnianzu,zengkaipeng,echo740,jiaxiaosong,yanjunchi}@sjtu.edu.cn\nAbstract\nMolecule representation learning (MRL) has been extensively studied and cur\u0002rent methods have shown promising power for various tasks, e.g., molecular\nproperty prediction and target identification. However, a common hypothesis of\nexisting methods is that either the model development or experimental evalua\u0002tion is mostly based on i.i.d. data across training and testing. Such a hypothesis\ncan be violated in real-world applications where testing molecules could come\nfrom new environments, bringing about serious performance degradation or unex\u0002pected prediction. We propose a new representation learning framework entitled\nMoleOOD to enhance the robustness of MRL models against such distribution\nshifts, motivated by an observation that the (bio)chemical properties of molecules\nare usually invariantly associated with certain privileged molecular substructures\nacross different environments (e.g., scaffolds, sizes, etc.). Specifically, We intro\u0002duce an environment inference model to identify the latent factors that impact\ndata generation from different distributions in a fully data-driven manner. We\nalso propose a new learning objective to guide the molecule encoder to lever\u0002age environment-invariant substructures that more stably relate with the labels\nacross environments. Extensive experiments on ten real-world datasets demon\u0002strate that our model has a stronger generalization ability than existing methods\nunder various out-of-distribution (OOD) settings, despite the absence of manual\nspecifications of environments. Particularly, our method achieves up to 5.9% and\n3.9% improvement over the strongest baselines on OGB and DrugOOD bench\u0002marks in terms of ROC-AUC, respectively. Our source code is publicly available at\nhttps://github.com/yangnianzu0515/MoleOOD.\n1 Introduction\nPredicting molecular properties plays an important role in many related applications like drug\ndiscovery [13] and material design [51]. These professional tasks conventionally take great efforts by\nexperts e.g. in chemistry and pharmacology. Recent years have witnessed inspiring breakthroughs on\nbuilding effective machine learning models for scientific discovery, and solid progress has been made\nalong the avenue of ML-based molecule representation learning (MRL). In general, MRL aims at\nembedding a molecule into a vector in latent space as a foundation model, on top of which the learned\nrepresentations could be used for a variety of downstream tasks, such as target identification [69],\nretrosynthetic analysis [65], search of antibiotics [54], virtual screening [40] for drug discovery, etc.\nThe challenge, however, is that existing MRL methods are mostly based on an underlying hypothesis\nthat training and testing molecules are independently sampled from an identical environment, yet\n\u2217\nJunchi Yan is the correspondence author who is also with Shanghai AI Laboratory.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\nOH\nOH\nOH\nCyclopropanol 1,4-Cyclohexanediol\nScaffold\t1\n3C-Ring\nScaffold\t2\n6C-Ring\nOH\nO\nO\nOH\nO\nOH\nOH\nO\nOH\nSmaller\tSize Larger\tSize\nEnvironment: Scaffold Environment: Size\nFormic Acid Citric\tAcid\nFigure 1: Two examples. Left: the shared substructure hydroxy (\u2212OH) invariantly contributes to the\nwater solubility of the two molecules which contain different scaffolds, i.e. sampled from different\nenvironments by definition. Right: the water solubility of the two molecules with different sizes can\nbe attributed to the shared substructure carboxy (\u2212COOH) invariantly, where different sizes are\nregarded as indicators to define different environments.\nreal-world environments are often dynamic and uncertain, which requires the model to effectively\nhandle distribution shifts. In fact, the available experimental molecule data are rather limited\nwhile the candidate molecules to be tested are often diverse, coming from unknown environments.\nTaking the virtual screening [40] as an example (which is a common protocol in drug discovery and\nusually for target identification), the prediction model is typically trained on some known target\nproteins. However, some unpredictable events like COVID-19 may occur, bringing new targets from\nunknown distributions. Similar scenarios where training and testing data are sampled from different\ndistributions are common in real world, posing an urgency for strengthening current MRL methods\nregarding out-of-distribution (OOD) generalization [41, 7, 44].\nExisting methods devised for out-of-distribution generalization mostly focus on Euclidean data such\nas images, while few endeavors OOD generalization on non-Euclidean data [61, 39]. In particular,\nmolecules, as a kind of typical non-Euclidean data, i.e., graph-structured data, is different from\nvisual data in nature. The work [27] point out that existing OOD models [3, 55, 18, 70, 50] fail\nto exhibit significant improvement on MRL tasks against distribution shifts and even the simple\nEmpirical Risk Minimization (ERM) [56] method outperforms these latest methods, which is also\nempirically verified by [16]. We aim to develop an OOD method tailored for molecules to solve the\nOOD generalization problem on MRL in this paper.\nWe incorporate an effective prior in the molecule domain into our model design: the (bio)chemical\nproperties of a molecule are usually associated with a few privileged molecular substructures, which\nhas been consistently shown by studies [31, 48, 72, 29] across bio-informatics, pharmacy, and\ndata mining. The common practice specifies environments as some prominent information of the\nmolecules e.g. scaffold pattern [32, 23] and molecule size [27]. Fig. 1 provides two illustrative\nexamples. Let\u2019s first take a look at the left example, where two molecules Cyclopropanol (C3H6O)\nand 1,4-Cyclohexanediol (C6H12O2) contain different scaffold patterns2: the former is 3C-ring\nand the latter is 6C-ring. Thus, the data-generating environments and the induced distributions\nwhich these two molecules are sampled from can be considered different [23]. Though sampled\nfrom different distributions, they are both readily soluble in water due to the invariant substructure\nhydroxy [24] shared across different environments. As for the example on the right of Fig. 1, the\nsizes of two molecules Formic Acid (CH2O2) and Citric Acid (C8H8O7) differ a lot. Consequently,\nthey can also be considered as being sampled from different environments. Owing to the shared\ninvarint substructure carboxy (\u2212COOH), they are both readily soluable in water, too. Hence, a\npromising paradigm would be to learn the causal data-generating invariance from the substructures\nacross environments, regarding a certain property, for the OOD generalization purpose.\nAnother important observation for consideration is that existing specifications for environments are\noften handcrafted or rule-based and not structured, which could provide insufficient information\nfor capturing the fundamental relations across domains from the casual data-generating perspective.\n2As a 2-D structural molecular framework [5], the scaffold reduces the chemical structure of a molecule to\nits core components, which can be obtained by removing side chains and only reserving the rings and parts\nconnecting rings [67]. The scaffold can be an indicator to define a specific environment [32, 23].\n2\nBesides, some studies [27, 16] show that directly utilizing such environment labels as input when\nadapting existing OOD generalization methods to MRL tasks can be problematic. Furthermore,\nmanual specifications of environments may be unavailable in reality. Hence, we aim to develop a\nlabel-free model that does not rely on the above ad-hoc environ...",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/547108084f0c2af39b956f8eadb75d1b-Paper-Conference.pdf"
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Few-shot Molecular Property Prediction: A Survey",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2510.08900v1"
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular ...",
      "text": "Evaluating Machine Learning Models for\nMolecular Property Prediction: Performance and\nRobustness on Out-of-Distribution Data\nHosein Fooladi,\u2020,\u2021,\u00b6 Thi Ngoc Lan Vu,\u2020,\u2021,\u00b6and Johannes Kirchmair\u2217,\u2020,\u2021,\u00b6\n\u2020Department of Pharmaceutical Sciences, Division of Pharmaceutical Chemistry, Faculty of\nLife Sciences, University of Vienna, Josef-Holaubek-Platz 2, 1090 Vienna, Austria\n\u2021Christian Doppler Laboratory for Molecular Informatics in the Biosciences, Department\nfor Pharmaceutical Sciences, University of Vienna, 1090 Vienna, Austria\n\u00b6Vienna Doctoral School of Pharmaceutical, Nutritional and Sport Sciences (PhaNuSpo),\nUniversity of Vienna, 1090 Vienna, Austria\nE-mail: johannes.kirchmair@univie.ac.at\nAbstract\nToday, machine learning models are employed extensively to predict the physico\u0002chemical and biological properties of molecules. Their performance is typically evalu\u0002ated on in-distribution (ID) data, i.e., data originating from the same distribution as\nthe training data. However, the real-world applications of such models often involve\nmolecules that are more distant from the training data, which necessitates assessing\ntheir performance on out-of-distribution (OOD) data. In this work, we investigate\nand evaluate the performance of twelve machine learning models, including classical\napproaches like random forests, as well as graph neural network (GNN) methods, such\nas message-passing graph neural networks, across eight data sets using seven splitting\nstrategies for OOD data generation. First, we investigate what constitutes OOD data\n1\nhttps://doi.org/10.26434/chemrxiv-2025-g1vjf-v2 ORCID: https://orcid.org/0000-0003-2667-5877 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nin the molecular domain for bioactivity and ADMET prediction tasks. In contrast to\nthe common point of view, we show that both classical machine learning and GNN\nmodels work well (not substantially different from random splitting) on data split\nbased on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering\n(K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both\ntypes of models. Second, we investigate the extent to which ID and OOD performance\nhave a positive linear relationship. If a positive correlation holds, models with the best\nperformance on the ID data can be selected with the promise of having the best perfor\u0002mance on OOD data. We show that the strength of this linear relationship is strongly\nrelated to how the OOD data is generated, i.e., which splitting strategies are used for\ngenerating OOD data. While the correlation between ID and OOD performance for\nscaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly\nfor cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more\nnuanced, and a strong positive correlation is not guaranteed for all OOD scenarios.\nThese findings suggest that OOD performance evaluation and model selection should\nbe carefully aligned with the intended application domain.\nIntroduction\nMachine learning (ML) plays a crucial role in the field of drug discovery, allowing the rapid\nprediction of molecular properties and biological activities. 1\u20134 By leveraging vast data sets\non chemical compounds, ML models have demonstrated remarkable success in identifying\npotential drug candidates, optimizing lead compounds, and predicting pharmacokinetic prop\u0002erties. This computational acceleration of early-stage drug development has substantially\ncontributed to the reduced need for extensive experimental testing. 5 Yet, a critical chal\u0002lenge persists in the field: while ML models typically work well on in-distribution (ID) data,\ni.e., molecules similar to those in their training sets, they are often challenged by out-of\u0002distribution (OOD) data, i.e., molecules that differ substantially from those in their training\n2\nhttps://doi.org/10.26434/chemrxiv-2025-g1vjf-v2 ORCID: https://orcid.org/0000-0003-2667-5877 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nsets.\nML models generally show significant performance degradation when tested on OOD data\nand often fail when tested on examples outside the training domain. 6\u20139 These limitations\nare particularly relevant in drug discovery, where exploring novel chemical spaces beyond\ntraining data boundaries is essential for identifying new therapeutic compounds. Although\nsuch exploration is crucial for innovation, it exposes ML models to OOD scenarios where\ntheir predictions may be unreliable. 10\u201312 The ability to maintain accuracy and robustness\nwhen faced with novel molecular structures is, therefore, critical for applications of ML in\ndrug discovery.\nThis study addresses the challenges posed by OOD data to ML by systematically evalu\u0002ating the performance of ML models on OOD data in drug discovery, particularly molecular\nproperty prediction (Figure 1). We examine a comprehensive range of approaches, from clas\u0002sical tabular methods such as random forests (RF) 13 operating on molecular fingerprints to\nmessage-passing graph neural networks (GNNs) 14 working directly with molecular graphs.\nFollowing recent findings by Gulrajani et al. 15 that demonstrate the effectiveness of Empirical\nRisk Minimization (ERM) 16 compared to specialized domain generalization methods, 17\u201321\nwe employ ERM as our main training algorithm for this benchmark study.\nIn the first part of this study, we explore what constitutes OOD data in the molecular\ndomain. While some researchers suggest that splitting based on Bemis-Murcko scaffolds 22\nsignificantly impacts model performance, as exemplified in the MoleculeNet benchmark, 23\nrecent studies have questioned this view. 24 To resolve this ambiguity, we systematically create\nand evaluate different types of test sets that could potentially be considered OOD, providing\na comprehensive analysis of their characteristics and impact on model performance.\nIn the second part of this study, we investigate the connection between ID and OOD\nperformance. While earlier studies suggest a substantial positive correlation between these\nmetrics, implying that ID performance could serve as a proxy for model selection, 25\u201327 recent\nwork has challenged this assumption. 28,29 Our analysis across various data sets and splitting\n3\nhttps://doi.org/10.26434/chemrxiv-2025-g1vjf-v2 ORCID: https://orcid.org/0000-0003-2667-5877 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nstrategies reveals that the relationship between ID and OOD performance can vary signif\u0002icantly, sometimes even showing negative correlations. These findings suggest that model\nselection strategies concentrating solely on ID performance may not consistently yield op\u0002timal generalization capabilities. More nuanced approaches are therefore needed for the\nevaluation and selection of models in drug discovery applications.\nMaterials and Methods\nIn this study, we evaluated the performance of GNNs and classical ML models, such as\nRF, on both ID and OOD test data sets. We assessed the model performance using the\nmetrics ROC-AUC (reported in the main text) and accuracy (reported in the Supporting\nInformation).\nThe experimental setup consisted of three major components: (1) curated molecular data\nsets for benchmarking, (2) a selection of ML models including both GNNs and classical ML\napproaches, and (3) data splitting methods (splitters) for generating OOD test sets. These\ncomponents are described in the following sections.\nData sets\nWe extracted our data set from the Therapeutic Data Commons (TDC) database, a bench\u0002mark resource for molecular machine learning. 30 TDC includes data sets for tasks such\nas small-molecule activity and property prediction, protein-protein interaction prediction,\nmolecular generation, and more. The data sets are organized into three categories: single\u0002instance prediction, multi-instance prediction, and generation. In this study, we used the\nsingle-instance prediction data sets, specifically focusing on ADMET prediction tasks (whic...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/67c8a90bfa469535b9148866/original/manuscript.pdf"
    }
  ]
}