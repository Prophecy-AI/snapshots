{
  "query": "What is the catechol benchmark dataset for few-shot machine learning and what MSE scores did the benchmark models achieve?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The Catechol Benchmark is a time\u2011series solvent\u2011selection dataset introduced for few\u2011shot regression, comprising more than 1,200 transient flow process conditions for yield prediction\u202f([arxiv.org](https://arxiv.org/abs/2506.07619)); the available sources do not provide the MSE scores achieved by the benchmarked models.",
      "url": ""
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n> jumper2021highly\n> ]\n, to fusion reactor control> [\n> degrave2022magnetic\n> ]\n, disease detection> [\n> zhou2023foundation\n> ]\n, battery design> [\n> folch2023combining\n> ]\n, and material discovery> [\n> raccuglia2016machine\n> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n> constable2007perspective\n> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n> karpov2019transformer\n> , > tetko2020state\n> ]\n, condition recommendations> [\n> gao2018using\n> ]\n, product predictions> [\n> coley2019graph\n> , > tu2022permutation\n> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n> wigh2024orderly\n> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n> schwaller2021prediction\n> , > griffiths2023gauche\n> , > rankovic2024bayesian\n> , > rankovic2025gollum\n> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v2#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We answer the call for more flow chemistry reaction data> [\n> deadman2025wanted\n> ]\n, further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify potential solutions.\n![Refer to caption](figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n> kearnes2021open\n> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n> lowe2012extraction\n> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n> wigh2024orderly\n> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> schwaller2021prediction\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive models for yield, and highlighting the importance of creating datasets with full (including potentially sub-optimal) reaction conditions.\nMore relevant to our work,> perera2018platform\nintroduced a dataset of 5760 Suzuki-Miyaura cross-coupling reactions,> ahneman2018predicting\nintroduced a dataset of 3956 Buchwald\u2013Hartwig aminations, and> prieto2022accelerating\ninvestigated screening additives for Ni-catalysed reactions, all for the purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes and Bayesian neural networks> [\n> gr...",
      "url": "https://arxiv.org/html/2506.07619v2"
    },
    {
      "title": "blaiszik/awesome-matchem-datasets - GitHub",
      "text": "GitHub - blaiszik/awesome-matchem-datasets\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/blaiszik/awesome-matchem-datasets)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/blaiszik/awesome-matchem-datasets)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=blaiszik/awesome-matchem-datasets)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[blaiszik](https://github.com/blaiszik)/**[awesome-matchem-datasets](https://github.com/blaiszik/awesome-matchem-datasets)**Public\n* [Notifications](https://github.com/login?return_to=/blaiszik/awesome-matchem-datasets)You must be signed in to change notification settings\n* [Fork34](https://github.com/login?return_to=/blaiszik/awesome-matchem-datasets)\n* [Star272](https://github.com/login?return_to=/blaiszik/awesome-matchem-datasets)\n### License\n[MIT license](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/LICENSE)\n[272stars](https://github.com/blaiszik/awesome-matchem-datasets/stargazers)[34forks](https://github.com/blaiszik/awesome-matchem-datasets/forks)[Branches](https://github.com/blaiszik/awesome-matchem-datasets/branches)[Tags](https://github.com/blaiszik/awesome-matchem-datasets/tags)[Activity](https://github.com/blaiszik/awesome-matchem-datasets/activity)\n[Star](https://github.com/login?return_to=/blaiszik/awesome-matchem-datasets)\n[Notifications](https://github.com/login?return_to=/blaiszik/awesome-matchem-datasets)You must be signed in to change notification settings\n# blaiszik/awesome-matchem-datasets\nmain\n[Branches](https://github.com/blaiszik/awesome-matchem-datasets/branches)[Tags](https://github.com/blaiszik/awesome-matchem-datasets/tags)\n[](https://github.com/blaiszik/awesome-matchem-datasets/branches)[](https://github.com/blaiszik/awesome-matchem-datasets/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[107 Commits](https://github.com/blaiszik/awesome-matchem-datasets/commits/main/)\n[](https://github.com/blaiszik/awesome-matchem-datasets/commits/main/)\n|\n[.llms](https://github.com/blaiszik/awesome-matchem-datasets/tree/main/.llms)\n|\n[.llms](https://github.com/blaiszik/awesome-matchem-datasets/tree/main/.llms)\n|\n|\n|\n[LICENSE](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/README.md)\n|\n[README.md](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/README.md)\n|\n|\n|\n[matchem-datasets.png](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/matchem-datasets.png)\n|\n[matchem-datasets.png](https://github.com/blaiszik/awesome-matchem-datasets/blob/main/matchem-datasets.png)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Awesome Materials &amp; Chemistry Datasets\n[](#awesome-materials--chemistry-datasets)\n## About\n[](#about)\nA curated list of the most useful datasets in**materials science**and**chemistry**for training**machine learning**and**AI foundation models**. This includes experimental, computational, and literature-mined datasets\u2014prioritizing**open-access**resources and community contributions.\nThis project aims to:\n* Catalog the best datasets by domain, type, quality, and size\n* Support reproducible research in AI for chemistry and materials\n* Provide a community-driven resource with contributions from researchers and developers\n## Table of Contents\n[](#table-of-contents)\n* [About](#about)\n* [How to Use](#how-to-use)\n* [Contributing](#contributing)\n* [Datasets](#datasets)\n* [Computational (DFT, MD)](#computational-datasets)\n* [Experimental](#experimental-datasets)\n* [LLM Training](#llm-training-datasets)\n* [Literature-mined &amp; Text](#literature-mined--text-datasets)\n* [Physics &amp; Engineering (PDE, CFD)](#physics--engineering-pde-cfd-datasets)\n* [Proprietary](#proprietary-datasets)\n* [License](#license)\n* [Acknowledgements](#acknowledgements)\n## Contributing\n[](#contributing)\nWant to add a new dataset or improve metadata?\n1. Fork the repository\n2. Edit the appropriate dataset list or add a new entry\n3. Submit a pull request with a brief description and download link\nOR\n4. Submit as an issue\n## Datasets\n[](#datasets)\n### Computational Datasets\n[](#computational-datasets)\n|Dataset|Domain|Size|Type|Format|\n[BOOM: Benchmarks for Out-Of-distribution Molecules](https://github.com/FLASK-LLNL/BOOM)|Small molecules|10 Out-Of-Distribution Tasks (1M+ entries)|Computational|CSV|\n[MSR-ACC/TAE25](https://doi.org/10.5281/zenodo.15387279)|Small molecules|77k CCSD(T)/CBS atomization energies|Computational|JSON|\n[OMat24 (Meta)](https://huggingface.co/datasets/fairchem/OMAT24)|Inorganic crystals|110M DFT entries|Computational|JSON/HDF5|\n[OMol25 (Meta)](https://huggingface.co/facebook/OMol25)|Molecular chemistry|100M+ DFT calculations|Computational|LMDB|\n[OMC25](https://huggingface.co/facebook/OMC25)|Molecular crystals|&gt;27M structures|Computational|Zarr|\n[Materials Project (LBL)](https://materialsproject.org)|Inorganic crystals|500k+ compounds|Computational|JSON/API|\n[Open Catalyst 2020 (OC20)](https://opencatalystproject.org)|Catalysis (surfaces)|1.2M relaxations|Computational|JSON/HDF5|\n[AFLOW](https://aflow.org)|Inorganic materials|3.5M materials|Computational|REST API|\n[OQMD](https://oqmd.org)|Inorganic solids|1M+ compounds|Computational|SQL/CSV|\n[JARVIS-DFT (NIST)](https://jarvis.nist.gov)|3D/2D materials|40k+ entries|Computational|JSON/API|\n[Carolina Materials DB](http://www.carolinamatdb.org)|Hypothetical crystals|214k structures|Computational|JSON|\n[NOMAD](https://nomad-lab.eu/prod/v1/gui/search/entries/search/entries)|Various DFT/MD|&gt;19M calculations|Computational|JSON|\n[MatPES](https://matpes.ai)|DFT Potential Energy Surfaces|\\~400,000 structures from 300K MD simulations|Computational|JSON|\n[Vector-QM24](https://doi.org/10.5281/zenodo.11164951)|Small organic and inorganic molecules|836k conformational isomers|Computational|JSON|\n[AIMNet2 Dataset](https://doi.org/10.1184/R1/27629937.v1)|Non-metallic compounds|20M hybrid DFT calculations|Computational|JSON|\n[RDB7](https://zenodo.org/records/13328872)|Barrier height and enthalpy for small organic reactions|12k CCSD(T)-F12 calculations|Computational|CSV|\n[RDB19-Rad](https://zenodo.org/records/11493786)|\u0394G of activation and of reaction for organic reactions in 40 common solvents|5.6k DFT + COSMO-RS calculations|Computational|CSV|\n[QCML](https://zenodo.org/records/14859804)|Small molecules consisting of up to 8 heavy atoms|14.7B Semi-empirical + 33.5M DFT calculations|Computational|TFDS|\n[QM9](http://quantum-machine.org/datasets/)|Small organic molecules|134k molecules with quantum properties|Experimental|SDF/CSV|\n[QM7/QM7b](http://quantum-mach...",
      "url": "https://github.com/blaiszik/awesome-matchem-datasets"
    },
    {
      "title": "Enhancing drug property prediction with dual-channel transfer ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<section><h3>Background</h3>\n<p>Accurate prediction of molecular property holds significance in contemporary drug discovery and medical research. Recent advances in AI-driven molecular property prediction have shown promising results. Due to the costly annotation of in vitro and in vivo experiments, transfer learning paradigm has been gaining momentum in extracting general self-supervised information to facilitate neural network learning. However, prior pretraining strategies have overlooked the necessity of explicitly incorporating domain knowledge, especially the molecular fragments, into model design, resulting in the under-exploration of the molecular semantic space.</p></section><section><h3>Results</h3>\n<p>We propose an effective model with FRagment-based dual-channEL pretraining (FREL). Equipped with molecular fragments, FREL comprehensively employs masked autoencoder and contrastive learning to learn intra- and inter-molecule agreement, respectively. We further conduct extensive experiments on ten public datasets to demonstrate its superiority over state-of-the-art models. Further investigations and interpretations manifest the underlying relationship between molecular representations and molecular properties.</p></section><section><h3>Conclusions</h3>\n<p>Our proposed model FREL achieves state-of-the-art performance on the benchmark datasets, emphasizing the importance of incorporating molecular fragments into model design. The expressiveness of learned molecular representations is also investigated by visualization and correlation analysis. Case studies indicate that the learned molecular representations better capture the drug property variation and fragment semantics.</p></section><section><h3>Supplementary information</h3>\n<p>The online version contains supplementary material available at 10.1186/s12859-023-05413-x.</p></section><section><p><strong>Keywords:</strong> Drug property prediction, Transfer learning, Molecular representation learning</p></section></section><section><h2>Introduction</h2>\n<p>One of the most foundational and crucial tasks in the domain of drug discovery pertains to the accurate prediction of molecular properties. Compared with conventional in vitro and in vivo experiments, computational methods have the potential to expedite the overall process of identifying better drug candidates with specific characteristics [<a href=\"#CR1\">1</a>, <a href=\"#CR2\">2</a>]. In general, the performance of molecular property prediction is mainly affected by two stages. The initial stage involves molecular featurization design [<a href=\"#CR3\">3</a>\u2013<a href=\"#CR5\">5</a>], which aims to translate chemical information into structured data recognizable by machine learning algorithms. The subsequent stage, known as molecular representation learning [<a href=\"#CR6\">6</a>\u2013<a href=\"#CR8\">8</a>], focuses on the development of methods for representing molecules as numerical vectors that encapsulate rich semantic biochemical information, either through manual [<a href=\"#CR9\">9</a>] or automatic means [<a href=\"#CR10\">10</a>]. Our paper, situated within the second stage, delves into self-supervised molecular representation learning techniques that implicitly extract biomedical domain knowledge via drug molecular fragments.</p>\n<p>Due to the inherent benefits of graphs in representing molecules, graph-based models, ranging from convolutional [<a href=\"#CR11\">11</a>] to spatial neural networks [<a href=\"#CR12\">12</a>, <a href=\"#CR13\">13</a>], have garnered attention in initial efforts towards supervised molecular representation learning. However, it is hampered by the lack of labeled property [<a href=\"#CR14\">14</a>] and the out-of-distribution problem [<a href=\"#CR10\">10</a>, <a href=\"#CR15\">15</a>], which have spurred the development of transfer learning approaches. A common framework involves pretraining the model with proxy tasks on extensive unlabeled molecular datasets, followed by fine-tuning the learned model on labeled downstream tasks. Prior studies [<a href=\"#CR16\">16</a>\u2013<a href=\"#CR20\">20</a>] employ various augmentation methods to construct molecular view pairs for contrastive learning, maximizing the agreement between different augmented views. Some models, on the other hand, use generative learning [<a href=\"#CR21\">21</a>] to reconstruct partial information of the sample itself [<a href=\"#CR22\">22</a>, <a href=\"#CR23\">23</a>], enabling the model to learn the molecular semantic space.</p>\n<p>Despite of some encouraging headway, most of the prior studies tend to overlooked the potential benefits of incorporating domain knowledge into model architecture, which can explicitly integrate biochemical information into model training. In the domain of pharmaceuticals, molecular fragments are of vital importance in determining molecular properties. For example, adrenergic receptor agonists with catechol structure (catechol hydroxyl group) are easily decomposed by COMT (catechol O-methyltransferase) in vivo, with poor stability and short action time, which affects the effectiveness of the drug. In comparison, adrenergic receptor agonists with non-catechol structure have much stronger stability [<a href=\"#CR24\">24</a>]. Moreover, we further present a exploratory experiment to verify the feasibility and effectiveness of fragment-based model design in the Additional file <a href=\"#MOESM1\">1</a>.</p>\n<p>Motivated by intuitive inspiration and exploratory experiment, we propose a novel and effective framework with FRagment-based dual-channEL pretraining (FREL), that comprehensively employs generative learning and contrastive learning to achieve intra- and inter-molecular agreement, respectively. The overall framework is demonstrated in Fig.\u00a0<a href=\"#Fig1\">1</a>. Specifically, for the contrastive learning channel, we generate two correlated molecular views of the same molecule. Then, we define a contrastive loss to maximize the inter-molecular agreement. For generative learning channel, we randomly mask partial node features and leverage a decoder to reconstruct the masked features based on intra-fragment information. By combining aforementioned contrastive and generative loss, FREL is expected to learn both intra- and inter-molecular agreement. We further support the effectiveness of our approach with theoretical analysis from the perspective of information theory.</p>\n<figure><h3>Fig. 1.</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=10360281_12859_2023_5413_Fig1_HTML.jpg\"></a></p>\n<figcaption><p>The proposed FREL model. In the pre-training phase, the GNN encoder takes molecular graph and fragments as inputs, which are respectively fed into the subsequent contrastive channel and generative channel. The model parameters are optimized with the sum of contrastive loss and generative loss to learn intra- and inter-molecule agreement. We express our gratitude for the use of the illustration of the blood-brain barrier and HIV virus, which were obtained from the websites <a href=\"https://smart.servier.com/\">https://smart.servier.com/</a> and <a href=\"https://www.vecteezy.com/\">https://www.vecteezy.com/</a>, respectively. We confirm that permission was given to reproduce these works</p></figcaption></figure><p>We evaluate the performance of our FREL model on 10 widely-used benchmark datasets from MoleculeNet [<a href=\"#CR10\">10</a>] and malaria [<a href=\"#CR25\">25</a>] that cover a wide range of molecular property prediction tasks, including classification and regression. The results reveal that FREL improves non-pretraining baselines without negative transfer and achieve the state-of-the-art (SOTA) performance. Moreover, we conduct extensive experiments to evaluate the expressiveness of molecular representations by visualization and statistical methods. The m...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10360281"
    },
    {
      "title": "[PDF] Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\nMachine learning has promised to change the landscape of laboratory chem\u0002istry, with impressive results in molecular property prediction and reaction retro\u0002synthesis. However, chemical datasets are often inaccessible to the machine\nlearning community as they tend to require cleaning, thorough understanding of the\nchemistry, or are simply not available. In this paper, we introduce a novel dataset\nfor yield prediction, providing the first-ever transient flow dataset for machine\nlearning benchmarking, covering over 1200 process conditions. While previous\ndatasets focus on discrete parameters, our experimental set-up allow us to sample\na large number of continuous process conditions, generating new challenges for\nmachine learning models. We focus on solvent selection, a task that is particularly\ndifficult to model theoretically and therefore ripe for machine learning applica\u0002tions. We showcase benchmarking for regression algorithms, transfer-learning\napproaches, feature engineering, and active learning, with important applications\ntowards solvent replacement and sustainable manufacturing.\n1 Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u0002powering the world of the natural sciences: from famous examples such as AlphaFold for protein\npredictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\ndiscovery [5], among many more. However, we seldom see the machine learning community bench\u0002mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\ndata, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\nexpensive the data can be to produce, resulting in many datasets being locked behind closed doors by\nlarge companies.\nAIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u0002ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\naddressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\nChemistry (https://www.solvechemistry.com), we present a first important step into addressing\nthe dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\nmachine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\nbeing the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\na drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nPreprint.\narXiv:2506.07619v2 [cs.LG] 27 Nov 2025\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\nsolvents and for improved solvent replacement tools. However, most of the solvent replacement tools\nfocus purely on learning unsupervised representations of solvents, with the hope that experimentalists\ncan find solvents with similar properties to replace those with environmental concerns. A much\nstronger approach would consider the interaction of a variety of different solvents with a reaction of\ninterest to directly predict reaction yields, in such a way that the best possible solvent can be selected\naccording to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical\nreaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n[9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\ndue to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\nprediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\nlack the continuous reaction conditions, such as temperature and residence time, that are required to\nscale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that\nallows for quick and efficient screening of continuous reaction conditions. We specifically provide\nyield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\nmeasurements across the residence time, temperature, and solvent space. We answer the call for\nmore flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\nchallenges to current machine learning methods for chemistry, and identify potential solutions.\n1.1 Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning\nbenchmarking tends to be poor. This can be a result of improper formatting or documentation,\nincomplete information about reaction conditions or the experimental set-up, or the lack of machine\nreadability, leading to limited usage by the ML community. However, some effort has been made\nto address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n[18], a repository containing over 2M different reactions, many of which come from US patent data\n(USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\nlearning readiness and data inconsistencies across reactions.\nORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\ndataset for forward and retro-synthetic prediction using transformers; however, it also shows that\nyield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\nconclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\nconcentrations, and duration have a significant effect on yield. The assumption that every reaction in\nthe dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\nmodels for yield, and highlighting the importance of creating datasets with full (including potentially\nsub-optimal) reaction conditions.\nMore relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u0002coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\nand Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\npurposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\nand Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n[16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\nfocus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\nand residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\nour case temperature and residence time), as well as providing a pseudo-continuous representation of\nsolvents themselves through the use of solvent mixtures.\nPerhaps the closest example to our dataset is presented in Nguyen et al. [24], who used high\u0002throughput experimentation to screen 12708 catal...",
      "url": "https://arxiv.org/pdf/2506.07619"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    },
    {
      "title": "NVIDIA BioNeMo Framework",
      "text": "Model Benchmarks &#8212; NVIDIA BioNeMo Framework\nToggle navigation sidebar\nToggle in-page Table of Contents\n[![logo](../_static/nvidia-logo-horiz-rgb-blk-for-screen.png)# NVIDIA BioNeMo Framework\n](../index.html)\n**\n**\n****\n**Contents\n# Model Benchmarks\n## Contents\n# Model Benchmarks[#](#model-benchmarks)\n## Protein Sequence Representation[#](#protein-sequence-representation)\nMetrics and datasets are from[FLIP](https://www.biorxiv.org/content/10.1101/2021.11.09.467890v2.full).\n|\nMetric\n|\n|\n|\nDataset\n|\n|\nType\n|\nName\n|\nDefinition\n|\nSplit Name\n|\nSplit Definition\n|\nClassification\n|\nSecondary Structure\n|\nPredict one of three secondary structure classes (helix, sheet, coil) for each amino acid in a protein sequence.\n|\nSampled\n|\nRandomly split sequences into train/test with 95/5% probability.\n|\nClassification\n|\nSubcellular Localization (SCL)\n|\nFor each protein, predict one of ten subcellular locations (cytoplasm, nucleus, cell membrane, mitochondrion, endoplasmic reticulum, lysosome/vacuole, golgi apparatus, peroxisome, extracellular, and plastid).\n|\nMixed Soft\n|\nThe mixed soft split uses train, validation, and test splits as provided in the DeepLoc 1.0 publication.\n|\nClassification\n|\nConservation\n|\nPredict one of nine possible conservation classes (1 = most variable to 9 = highly conserved) for each amino acid in a protein sequence\n|\nSampled\n|\nRandomly split sequences into train/test with 95/5% probability.\n|\nRegression\n|\nMeltome\n|\nPredict melting degree, which is the temperature at which &gt;50% of a protein is denatured.\n|\nMixed Split\n|\nProtein sequences were clustered by seq identity with 80% of clusters used for training, 20% for testing. The mixed split uses sequences from clusters for training and the representative cluster sequence for testing. The objective is to minimize performance overestimation on large clusters in the test set.\n|\nRegression\n|\nGB1 Binding Activity\n|\nThe impact of amino acid substitutions for one or more of four GB1 positions (V39, D40, G41, and V54) was measured in a binding assay. Values &gt; 1 indicate more binding than wildtype, equal to 1 indicate equivalent binding, and &lt; 1 indicate less binding than wildtype.\n|\nTwo vs Rest\n|\nThe training split includes wild type sequence and all single and double mutations. Everything else is put into the test set.\n|\n**Classification Metric Values**\nESM models listed below are tested as deployed in BioNeMo.\n|\nSecondary Structure\n|\n|\nSubcellular Localization (SCL)\n|\n|\nConservation\n|\n|\n**Model**\n|\n**Accuracy**\n|\n**Model**\n|\n**Accuracy**\n|\n**Model**\n|\n**Accuracy**\n|\nOne Hot\n|\n0.643\n|\nOne Hot\n|\n0.386\n|\nOne Hot\n|\n0.202\n|\nESM1nv\n|\n0.773\n|\nESM1nv\n|\n0.720\n|\nESM1nv\n|\n0.249\n|\nProtT5nv\n|\n0.793\n|\nProtBERT\n|\n0.740\n|\nProtT5nv\n|\n0.256\n|\nProtBERT\n|\n0.818\n|\nProtT5nv\n|\n0.764\n|\nProtBERT\n|\n0.326\n|\nProtT5\n|\n0.854\n|\nESM2 T33 650M UR50D\n|\n0.791\n|\nESM2 T33 650M UR50D\n|\n0.329\n|\nESM2 T33 650M UR50D\n|\n0.855\n|\nESM2 T36 3B UR50D\n|\n0.812\n|\nESM2 T36 3B UR50D\n|\n0.337\n|\nESM2 T36 3B UR50D\n|\n0.861\n|\nProtT5\n|\n0.820\n|\nESM2 T48 15B UR50D\n|\n0.340\n|\nESM2 T48 15B UR50D\n|\n0.867\n|\nESM2 T48 15B UR50D\n|\n0.839\n|\nProtT5\n|\n0.343\n|\n**Regression Metric Values**\n|\nMeltome\n|\n|\nGB1 Binding Activity\n|\n|\n**Model**\n|\n**MSE**\n|\n**Model**\n|\n**MSE**\n|\nOne Hot\n|\n128.21\n|\nOne Hot\n|\n2.56\n|\nESM1nv\n|\n82.85\n|\nProtT5\n|\n1.69\n|\nProtT5nv\n|\n77.39\n|\nESM2 T33 650M UR50D\n|\n1.67\n|\nProtBERT\n|\n58.87\n|\nESM2 T36 3B UR50D\n|\n1.64\n|\nESM2 T33 650M UR50D\n|\n53.38\n|\nProtBERT\n|\n1.61\n|\nESM2 T36 3B UR50D\n|\n45.78\n|\nProtT5nv\n|\n1.60\n|\nProtT5\n|\n44.76\n|\nESM1nv\n|\n1.58\n|\nESM2 T48 15B UR50D\n|\n39.49\n|\nESM2 T48 15B UR50D\n|\n1.52\n|\n## SMILES Representation[#](#smiles-representation)\n**Metric Definitions and Dataset**\n|\nType\n|\nMetric\n|\nMetric Definition\n|\nDataset\n|\nPhyschem Properties\n|\nLipophilicity\n|\nMSE from best performing SVM and Random Forest model, as determined by hyperparameter optimization with 20-fold nested cross-validation.\n|\nMoleculeNet datasets: Lipophilicity: 4,200 molecules FreeSolv: 642 molecules ESOL: 1,128 molecules\n|\n|\nFreeSolv\n|\n|\n|\n|\nESOL\n|\n|\n|\nBioactivities\n|\nActivity\n|\n|\nExCAPE database filtered on a subset of protein targets (28 genes). The set of ligands for each target comprise one dataset, with the number of ligands ranging from 1,341 to 367,067 molecules (total = 1,203,479). A model is fit for each dataset and the resulting MSE values are averaged.\n|\n**Metric Values**\n|\nType\n|\nMetric\n|\nSVM MSE\n|\nRandom Forest MSE\n|\nPhyschem Properties\n|\nLipophilicity\n|\n0.491\n|\n0.811\n|\n|\nFreeSolv\n|\n1.991\n|\n4.832\n|\n|\nESOL\n|\n0.474\n|\n0.862\n|\nBioactivities\n|\nActivity\n|\n0.520\n|\n0.616\n|\n## SMILES Generation[#](#smiles-generation)\n**Metric Definitions and Dataset**\n|\nType\n|\nMetric\n|\nMetric Definition\n|\nDataset\n|\nSampling\n|\nValidity\n|\nPercentage of molecules generated which are valid SMILES, as determined by RDKit.\n|\nThe dataset was 10k molecules randomly selected from ChEMBL that are not present in the training data for MoFlow or MegaMolBART and pass drug-likeness filters. For each of these seed molecules, sample 512 molecules from MoFlow with a temperature of 0.25. For MegaMolBART, sample 10 molecules with a radius of 1.0. For each seed molecule, calculate metric or properties as described on its samples. The metric value is the percentage of molecules which meet the metric definition.\n|\n|\nNovelty\n|\nPercentage of valid molecules that are not present in training data and don\u2019t match the seed molecule.\n|\n|\n|\nUniqueness\n|\nPercentage of valid molecules that are unique.\n|\n|\n|\nNUV\n|\nPercentage of molecules generated which meet all sampling metrics (novelty, uniqueness, validity).\n|\n|\nDrug-Likeness\n|\nQED\n|\nQuantitative estimate of drug-likeness.\n|\n|\n|\nSAS\n|\nSynthetic accessibility score.\n|\n|\n|\nPass Filters\n|\nFraction of valid molecules which meet all of the following drug-likeness criteria: (1) SAS between 2.0 and 4.0, inclusive; (2) QED &gt;= 0.65; (3) Maximum ring size &lt;= 6; (4) Number of rings &gt;= 2; (5) No rings with fewer than 5 atoms.\n|\n|\n**Metric Values**\n|\nType\n|\nMetric\n|\nMegaMolBART\n|\n|\nMoFlow\n|\n|\n|\n|\nMean\n|\nStandard Deviation\n|\nMean\n|\nStandard Deviation\n|\nSampling\n|\nValidity\n|\n0.819\n|\n0.034\n|\n1.000\n|\n0.000\n|\n|\nNovelty\n|\n1.000\n|\n0.000\n|\n1.000\n|\n0.000\n|\n|\nUniqueness\n|\n0.513\n|\n0.069\n|\n0.841\n|\n0.190\n|\n|\nNUV\n|\n0.395\n|\n0.037\n|\n0.841\n|\n0.190\n|\nDrug-Likeness\n|\nQED\n|\n0.746\n|\n0.007\n|\n0.583\n|\n0.009\n|\n|\nSAS\n|\n2.654\n|\n0.204\n|\n4.150\n|\n0.254\n|\n|\nPass Filters\n|\n0.766\n|\n0.074\n|\n0.215\n|\n0.020\n|",
      "url": "https://docs.nvidia.com/bionemo-framework/1.10/models/model-benchmarks.html"
    },
    {
      "title": "FSL-CP: a benchmark for small molecule activity few-shot prediction using cell microscopy images",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2024/dd/d3dd00205e)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/dd/d3dd00194f)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/dd/d4dd00010b)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D3DD00205E](https://doi.org/10.1039/D3DD00205E)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2024, **3**, 719-727\n\n# FSL-CP: a benchmark for small molecule activity few-shot prediction using cell microscopy images [\u2020](https://pubs.rsc.org/pubs.rsc.org\\#fn1)\n\nSon V.\nHa\n,\nLucas\nLeuschner\nand Paul\nCzodrowski\n\\*\nJGU Mainz, Germany. E-mail: [czodpaul@uni-mainz.de](mailto:czodpaul@uni-mainz.de)\n\nReceived\n11th October 2023\n, Accepted 15th February 2024\n\nFirst published on 26th February 2024\n\n## Abstract\n\nPredicting small molecule activities using information from high-throughput microscopy images has been shown to tremendously increase hit rates and chemical diversity of the hits in previous drug discovery projects. However, due to high cost of acquiring data or ethical reasons, data sparsity remains a big challenge in drug discovery. This opens up the opportunity for few-shot prediction: fine-tuning a model on a low-data assay of interest after pretraining on other more populated assays. Previous efforts have been made to establish a benchmark for few-shot learning of molecules based on molecular structures. With cell images as a molecular representation, methods in the computer vision domain are also applicable for activity prediction. In this paper, we make two contributions: (a) a public data set for few-shot learning with cell microscopy images for the scientific community and (b) a range of baseline models encompassing different existing single-task, multi-task and meta-learning approaches.\n\n## 1 Introduction\n\nHigh-throughput imaging (HTI) has been a powerful tool in drug discovery, having yielded many biological discoveries. [1\u20133](https://pubs.rsc.org/pubs.rsc.org#cit1) It often involves capturing the morphological changes of cells induced by chemical compounds and quantifying these changes into a large set of numerical features [4](https://pubs.rsc.org/pubs.rsc.org#cit4) such as staining intensity, texture, shape and spatial correlations. They act as \u2018fingerprints\u2019 that can be used to characterise compounds in a relatively unbiased way. This technique, known as morphological profiling, has proven to be useful for a variety of applications, such as optimizing the diversity of compound libraries, [5](https://pubs.rsc.org/pubs.rsc.org#cit5) determining the mechanism of action of compounds, [6\u20138](https://pubs.rsc.org/pubs.rsc.org#cit6) and clustering genes based on their biological functions. [9,10](https://pubs.rsc.org/pubs.rsc.org#cit9)\n\nCell painting is a morphological profiling method in which cells are perturbed with a compound, have their different compartments stained using six dyes, and have their images of the five fluorescence channels captured. [4](https://pubs.rsc.org/pubs.rsc.org#cit4) Cell painting data have been used for a range of applications, from predicting mitochondrial toxicity, [3,11](https://pubs.rsc.org/pubs.rsc.org#cit3) in vitro toxicity, [12](https://pubs.rsc.org/pubs.rsc.org#cit12) hit identification, [13](https://pubs.rsc.org/pubs.rsc.org#cit13) and more. In addition, cell painting can be used in combination with other modalities to enhance prediction such as chemical structure [14](https://pubs.rsc.org/pubs.rsc.org#cit14) and gene expression data. [15](https://pubs.rsc.org/pubs.rsc.org#cit15)\n\n### 1.1 Small molecule activity prediction\n\nPrediction of small molecule activity against a drug target is an important task in drug discovery. It helps identify and optimise compounds for a desired activity, as well as recognise and avoid off-target activities. This leads to the identification of compounds with the highest potential in the early drug discovery pipeline.\n\nHTI data have been used in bioactivity prediction by Simm et al. [16](https://pubs.rsc.org/pubs.rsc.org#cit16) in two drug discovery projects, which led to a tremendous increase in hit rates by 50- to 250-fold, while increasing the chemical structure diversity of the hits. In these projects, only 1.6% of the label matrix is filled, for over 500000 compounds and 1200 prediction tasks. This reflects the need for a modeling paradigm which can not only adapt to new tasks quickly with little data, but also leverage the availability of many low-data related tasks. We find that this setting is ideal to form a few-shot learning challenge.\n\n### 1.2 Few-shot learning\n\nIn the few-shot learning setting, there is not one big dataset D to learn from, but instead many small datasets we called tasks, denoted T. The aim of few-shot methods is to generalise over new tasks {Tu}Uu=1 \u2208 Dtest efficiently with only a small number of available datapoints. Each task Tu consists of a support set S for learning and a query set Q for evaluation, Tu = \u3008S, Q\u3009. Typically the size of support set S is very small to reflect the low-data setting.\n\nFew-shot models adapt efficiently to low-data tasks by using an advantage initialisation of their parameters, normally through some sort of pretraining on a large data corpus such as a set of auxiliary tasks {Tv}Vv=1 \u2208 Dtrain. We expect that knowledge gained from pretraining can be transferred effectively to new unseen tasks, so that models can quickly learn these new tasks using only little data. This can be compared to, for example, a person who already has prior knowledge of music picking up a new musical instrument relativelyquickly with little demonstration.\n\nMost state-of-the-art few-shot methods come from computer vision and natural language processing domains. [17\u201321](https://pubs.rsc.org/pubs.rsc.org#cit17) Drug discovery is another field where there is growing interest in few-shot learning, [22](https://pubs.rsc.org/pubs.rsc.org#cit22) since data scarcity is a common setting for many prediction tasks. In this paper, we propose to expand another challenge in drug discovery to the few-shot learning area: the aforementioned small molecule activity prediction with cell imaging data. We find that this is a real-world scientific problem with an ideal few-shot setting: there are many related low-data tasks convenient for knowledge transfer between each other. Furthermore, with cell images as a molecular representation, methods in the computer vision domain can be adapted for activity prediction. We believe that the field of cell imaging/analysis would greatly benefit from these algorithmic innovations.\n\nIn this paper, we make two contributions:\n\n\u2022 A data set for few-shot prediction of small molecule activity using cell microscopy images, which we named FSL-CP. The dataset is curated so that it is easy for future researchers to experiment with their few-shot methods.\n\n\u2022 A benchmark of models encompassing different existing single-task, multi-task and meta-learning approaches on the dataset. This acts as both a diverse baseline for future algorithms and a means to study the strengths and weaknesses of different modelling paradigms.\n\n## 2 Methods\n\n### 2.1 FSL-CP: few-shot learning data set with cell microscopy images\n\nThe FSL-CP dataset comprises compounds at the intersection of ChEMBL [23](https://pubs.rsc.org/pubs.rsc.org#cit23) version 31 and the cell painting [4](https://pubs.rsc.org/pubs.rsc.org#cit4) public dataset. We provide an overview of the data construction process below ( [Fig. 1](https://pubs.rsc.org/pubs.rsc.org#imgfig1)), and the exact reproducible source code is available on GitHub, at https://github.com/czodrowskilab/FSL\\_CP\\_DataPrep.\n\n|     |\n| --- |\n| **Fig. 1** FSL-CP data curation and processing. Cell painting images and features from CellProfiler [24](https://pubs.rsc.org/pubs.rsc.org#cit24) come ...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2024/dd/d3dd00205e"
    }
  ]
}