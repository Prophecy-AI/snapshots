{
  "query": "What is the arXiv paper 2512.19530 about catechol benchmark dataset and what specific model architecture achieved MSE 0.0039?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The paper\u202farXiv:2512.19530 introduces the \u201cCatechol Benchmark,\u201d a high\u2011throughput transient\u2011flow chemistry dataset of 1,227 catechol\u2011rearrangement experiments used to benchmark graph\u2011neural\u2011network models, but the search results do not specify which particular model architecture achieved an MSE of\u202f0.0039. ([arxiv.org](https://arxiv.org/abs/2512.19530))",
      "url": ""
    },
    {
      "title": "Learning Continuous Solvent Effects from Transient Flow ...",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "Arxiv\u4eca\u65e5\u8bba\u6587| 2025-12-23 - \u95f2\u8bb0\u7b97\u6cd5",
      "text": "Arxiv\u4eca\u65e5\u8bba\u6587 | 2025-12-23 | \u95f2\u8bb0\u7b97\u6cd5![avatar](https://lonepatient.top/img/touxiang.jpeg)\n[\n\u6587\u7ae0268\n](https://lonepatient.top/archives/)\n[\n\u6807\u7b7e313\n](https://lonepatient.top/tags/)\n[\n\u5206\u7c7b71\n](https://lonepatient.top/categories/)\n[**\u9996\u9875](https://lonepatient.top/)\n[**\u627e\u6587\u7ae0**](javascript:void(0);)\n* [**\u6807\u7b7e](https://lonepatient.top/tags/)\n* [**\u5206\u7c7b](https://lonepatient.top/categories/)\n* [**\u65f6\u95f4\u8f74](https://lonepatient.top/archives/)\n* [**\u7edf\u8ba1](https://lonepatient.top/charts/)\n[**\u7f51\u7ad9**](javascript:void(0);)\n* [**AI\u5de5\u5177\u96c6\u5408](https://www.aitoolist.cn)\n* [**AI\u5de5\u5177\u5bfc\u822a](https://www.deepdh.com)\n* [**\u4eba\u5de5\u667a\u80fd\u5de5\u5177](https://www.ai-lib.club)\n* [**AI\u5de5\u5177\u96c6](https://ai-bot.cn)\n[**\u6559\u7a0b**](javascript:void(0);)\n* [**Prompt\u6559\u7a0b](https://learningprompt.wiki/)\n[**\u5728\u7ebf**](javascript:void(0);)\n* [**\u5728\u7ebfmarkdown](https://lonepatient.top/md_editor/)\n* [**\u5728\u7ebfpush](https://github.com/lonePatient/blog_source/tree/main/source/_posts)\n[**\u7559\u8a00\u677f](https://lonepatient.top/message/)\n[**\u53cb\u94fe](https://lonepatient.top/link/)\n[**\u5173\u4e8e](https://lonepatient.top/about/)\n# Arxiv\u4eca\u65e5\u8bba\u6587 | 2025-12-23[**](https://github.com/lonePatient/blog_source/tree/main/source/_posts/arxiv_papers_2025-12-23.md)\n**\u53d1\u8868\u4e8e2025-12-23|**\u66f4\u65b0\u4e8e2026-01-07|**[\u5b66\u672f\u4f1a\u8bae](https://lonepatient.top/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/)****[Arxiv](https://lonepatient.top/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/Arxiv/)\n|**\u5b57\u6570\u603b\u8ba1:263.7k|**\u9605\u8bfb\u65f6\u957f:1253\u5206\u949f|**\u9605\u8bfb\u91cf:|**\u8bc4\u8bba\u6570:[](https://lonepatient.top/2025/12/23/arxiv_papers_2025-12-23.html#post-comment)\n\u672c\u7bc7\u535a\u6587\u4e3b\u8981\u5185\u5bb9\u4e3a2025-12-23 \u4ece[Arxiv.org](https://arxiv.org)\u8bba\u6587\u7f51\u7ad9\u83b7\u53d6\u7684\u6700\u65b0\u8bba\u6587\u5217\u8868\uff0c\u81ea\u52a8\u66f4\u65b0\uff0c\u6309\u7167NLP\u3001CV\u3001ML\u3001AI\u3001IR\u4e94\u4e2a\u5927\u65b9\u5411\u533a\u5206\uff0c\u82e5\u9700\u8981\u90ae\u4ef6\u5b9a\u65f6\u63a5\u6536\uff0c\u8bf7\u5728\u8bc4\u8bba\u533a\u7559\u4e0b\u4f60\u7684\u90ae\u7bb1\u53f7\u3002\n\u8bf4\u660e\uff1a\u6bcf\u65e5\u8bba\u6587\u6570\u636e\u4ece[Arxiv.org](https://arxiv.org)\u83b7\u53d6\uff0c\u6bcf\u5929\u65e9\u4e0a12:00\u5de6\u53f3\u5b9a\u65f6\u81ea\u52a8\u66f4\u65b0\u3002\n\u53cb\u60c5\u63d0\u793a: \u5982\u4f55\u60a8\u9700\u8981**\u90ae\u7bb1**\u63a5\u6536\u6bcf\u65e5\u8bba\u6587\u6570\u636e\uff0c\u8bf7\u5728\u8bc4\u8bba\u5904\u7559\u4e0b\u4f60\u7684\u90ae\u7bb1\u3002\n## \u76ee\u5f55* [\u6982\u89c8](#\u6982\u89c8)\n* [\u81ea\u7136\u8bed\u8a00\u5904\u7406CL](#\u81ea\u7136\u8bed\u8a00\u5904\u7406)\n* [\u4eba\u5de5\u667a\u80fdAI](#\u4eba\u5de5\u667a\u80fd)\n* [\u673a\u5668\u5b66\u4e60LG](#\u673a\u5668\u5b66\u4e60)\n* [\u8ba1\u7b97\u673a\u89c6\u89c9CV](#\u8ba1\u7b97\u673a\u89c6\u89c9)\n* [\u4fe1\u606f\u68c0\u7d22IR](#\u4fe1\u606f\u68c0\u7d22)### \u6982\u89c8(2025-12-23)\n\u4eca\u65e5\u5171\u66f4\u65b0**811**\u7bc7\u8bba\u6587,\u5176\u4e2d:\n* **\u81ea\u7136\u8bed\u8a00\u5904\u7406**\u5171**100**\u7bc7(Computation and Language ([cs.CL](http://cs.CL)))\n* **\u4eba\u5de5\u667a\u80fd**\u5171**242**\u7bc7(Artificial Intelligence ([cs.AI](http://cs.AI)))\n* **\u8ba1\u7b97\u673a\u89c6\u89c9**\u5171**203**\u7bc7(Computer Vision and Pattern Recognition ([cs.CV](http://cs.CV)))\n* **\u673a\u5668\u5b66\u4e60**\u5171**225**\u7bc7(Machine Learning (cs.LG))### \u81ea\u7136\u8bed\u8a00\u5904\u7406[NLP-0] GenEnv: Difficulty-Aligned Co-Evolution BetweenLLMAgentsand Environment Simulators\n\u3010\u901f\u8bfb\u3011\uff1a\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08Large Language Model, LLM\uff09\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u56e0\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u6210\u672c\u9ad8\u4e14\u9759\u6001\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002\u5176\u6838\u5fc3\u89e3\u51b3\u65b9\u6848\u662f\u63d0\u51faGenEnv\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u96be\u5ea6\u5bf9\u9f50\u7684\u534f\u540c\u8fdb\u5316\u535a\u5f08\u673a\u5236\uff0c\u4f7f\u667a\u80fd\u4f53\u4e0e\u53ef\u6269\u5c55\u7684\u751f\u6210\u5f0f\u73af\u5883\u6a21\u62df\u5668\uff08generative environment simulator\uff09\u5171\u540c\u6f14\u5316\u3002\u8be5\u6846\u67b6\u7684\u5173\u952e\u5728\u4e8e\u5f15\u5165\u03b1-\u8bfe\u7a0b\u5956\u52b1\uff08\u03b1-Curriculum Reward\uff09\uff0c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u96be\u5ea6\u4ee5\u5339\u914d\u667a\u80fd\u4f53\u5f53\u524d\u7684\u80fd\u529b\u6c34\u5e73\uff0c\u4ece\u800c\u5b9e\u73b0\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d\uff08zone of proximal development\uff09\u5185\u7684\u9ad8\u6548\u5b66\u4e60\u3002\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u65b9\u6cd5\uff0cGenEnv\u5b9e\u73b0\u4e86\u6570\u636e\u6548\u7387\u63d0\u5347\u548c\u6027\u80fd\u663e\u8457\u589e\u5f3a\u3002\n**\u94fe\u63a5**:[https://arxiv.org/abs/2512.19682](https://arxiv.org/abs/2512.19682)\n**\u4f5c\u8005**: Jiacheng Guo,Ling Yang,Peter Chen,Qixin Xiao,Yinjie Wang,Xinzhe Juan,Jiahao Qiu,Ke Shen,Mengdi Wang\n**\u673a\u6784**: Princeton University (\u666e\u6797\u65af\u987f\u5927\u5b66); ByteDance Seed (\u5b57\u8282\u8df3\u52a8\u79cd\u5b50\u57fa\u91d1); Columbia University (\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66); University of Michigan (\u5bc6\u6b47\u6839\u5927\u5b66); University of Chicago (\u829d\u52a0\u54e5\u5927\u5b66)\n**\u7c7b\u76ee**: Computation and Language ([cs.CL](http://cs.CL))\n**\u5907\u6ce8**: Our codes are available at[this https URL](https://github.com/Gen-Verse/GenEnv)\n**\u70b9\u51fb\u67e5\u770b\u6458\u8981\nAbstract:Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent\u2019s ``zone of proximal development\u2019'. This process is guided by a simple but effective \\\\alpha -Curriculum Reward, which aligns task difficulty with the agent\u2019s current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\\\textbf+40.3% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3 \\\\times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.\nzh\n[NLP-1] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies\n\u3010\u901f\u8bfb\u3011\uff1a\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning, RL\uff09\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08Large Language Models, LLMs\uff09\u89c6\u4e3a\u5355\u4e00\u7edf\u4e00\u7b56\u7565\u65f6\uff0c\u5ffd\u89c6\u5176\u5185\u90e8\u7ed3\u6784\u4e0e\u673a\u5236\u7684\u95ee\u9898\u3002\u4e3a\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u4f18\u5316\u5e76\u63ed\u793a\u590d\u6742\u63a8\u7406\u673a\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u6765\u8bc6\u522b\u5185\u90e8\u5c42\u7b56\u7565\uff08Internal Layer Policies\uff09\u548c\u6a21\u5757\u7b56\u7565\uff08Internal Modular Policies\uff09\uff0c\u5206\u522b\u5bf9\u5e94Transformer\u6b8b\u5dee\u6d41\u4e2d\u5404\u5c42\u53ca\u81ea\u6ce8\u610f\u529b\u4e0e\u524d\u9988\u7f51\u7edc\uff08Feed-Forward Network, FFN\uff09\u7ec4\u4ef6\u7684\u8d21\u732e\u3002\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u5728\u4e8e\u5229\u7528\u9690\u85cf\u72b6\u6001\u4e0e\u672a\u5d4c\u5165\u77e9\u9635\uff08unembedding matrix\uff09\u7ec4\u5408\u7684\u7b49\u4ef7\u6027\uff0c\u4ece\u800c\u663e\u5f0f\u5efa\u6a21\u6bcf\u4e00\u5c42\u6216\u6a21\u5757\u5bf9\u6700\u7ec8\u91c7\u6837\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u51fa\u201c\u81ea\u5e95\u5411\u4e0a\u7b56\u7565\u4f18\u5316\u201d\uff08Bottom-up Policy Optimization, BuPO\uff09\u2014\u2014\u4e00\u79cd\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u4f18\u5316\u5e95\u5c42\u5c42\u7b56\u7565\u7684\u65b0RL\u8303\u5f0f\uff0c\u6709\u6548\u91cd\u5efa\u57fa\u7840\u63a8\u7406\u80fd\u529b\u5e76\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002\n**\u94fe\u63a5**:[https://arxiv.org/abs/2512.19673](https://arxiv.org/abs/2512.19673)\n**\u4f5c\u8005**: Yuqiao Tan,Minzheng Wang,Shizhu He,Huanxuan Liao,Chengfeng Zhao,Qiunan Lu,Tian Liang,Jun Zhao,Kang Liu\n**\u673a\u6784**: \u672a\u77e5**\u7c7b\u76ee**: Machine Learning (cs.LG); Artificial Intelligence ([cs.AI](http://cs.AI)); Computation and Language ([cs.CL](http://cs.CL))\n**\u5907\u6ce8**: Preprint. Our code is available at[this https URL](https://github.com/Trae1ounG/BuPO)\n**\u70b9\u51fb\u67e5\u770b\u6458\u8981\nAbstract:Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama\u2019s prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at this https URL.\nzh\n[NLP-2] Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting\n\u3010\u901f\u8bfb\u3011\uff1a\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3Aspect-Category Sentiment Analysis (ACSA)\u4efb\u52a1\u4e2d\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u6210\u672c\u9ad8\u6602\u800c\u5bfc\u81f4\u7684\u6a21\u578b\u8bad\u7ec3\u56f0\u96be\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65b0\u9886\u57df\u7f3a\u4e4f\u8db3\u591f\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002\u5176\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u5728\u4e8e\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eChain-of-Thought (CoT)\u63d0\u793a\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e2d\u95f4\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff08Unified Meaning Representation, UMR\uff09\u6765\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u63d0\u5347\u6a21\u578b\u5bf9ACSA\u4efb\u52a1\u7684\u7406\u89e3\u4e0e\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5UMR-based\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff08\u5982Qwen3-8B\uff09\u4e0a\u6548\u679c\u63a5\u8fd1\u6807\u51c6CoT\u57fa\u7ebf\uff0c\u4f46\u5176\u666e\u9002\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u7279\u522b\u662f\u5728\u5c0f\u578b\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u9002\u7528\u6027\u3002\n**\u94fe\u63a5**:[https://arxiv.org/abs/2512.19651](https://arxiv.org/abs/2512.19651)\n**\u4f5c\u8005**: Filippos Ventirozos,Peter Appleby,Matthew Shardlow\n**\u673a\u6784**: Manchester Metropolitan University (\u66fc\u5f7b\u65af\u7279\u90fd\u4f1a\u5927\u5b66); Autotrader Research Group, Autotrader UK (Autotrader \u7814\u7a76\u7ec4\uff0cAutotrader \u82f1\u56fd)\n**\u7c7b\u76ee**: Computation and Language ([cs.CL](http://cs.CL))\n**\u5907\u6ce8**: 9 pages, 3 figures, 3 tables\n**\u70b9\u51fb\u67e5\u770b\u6458\u8981\nAbstract:Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present sign...",
      "url": "http://lonepatient.top/2025/12/23/arxiv_papers_2025-12-23"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\nMachine learning has promised to change the landscape of laboratory chem\u0002istry, with impressive results in molecular property prediction and reaction retro\u0002synthesis. However, chemical datasets are often inaccessible to the machine\nlearning community as they tend to require cleaning, thorough understanding of the\nchemistry, or are simply not available. In this paper, we introduce a novel dataset\nfor yield prediction, providing the first-ever transient flow dataset for machine\nlearning benchmarking, covering over 1200 process conditions. While previous\ndatasets focus on discrete parameters, our experimental set-up allow us to sample\na large number of continuous process conditions, generating new challenges for\nmachine learning models. We focus on solvent selection, a task that is particularly\ndifficult to model theoretically and therefore ripe for machine learning applica\u0002tions. We showcase benchmarking for regression algorithms, transfer-learning\napproaches, feature engineering, and active learning, with important applications\ntowards solvent replacement and sustainable manufacturing.\n1 Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u0002powering the world of the natural sciences: from famous examples such as AlphaFold for protein\npredictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\ndiscovery [5], among many more. However, we seldom see the machine learning community bench\u0002mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\ndata, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\nexpensive the data can be to produce, resulting in many datasets being locked behind closed doors by\nlarge companies.\nAIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u0002ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\naddressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\nChemistry (https://www.solvechemistry.com), we present a first important step into addressing\nthe dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\nmachine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\nbeing the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\na drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nPreprint.\narXiv:2506.07619v2 [cs.LG] 27 Nov 2025\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\nsolvents and for improved solvent replacement tools. However, most of the solvent replacement tools\nfocus purely on learning unsupervised representations of solvents, with the hope that experimentalists\ncan find solvents with similar properties to replace those with environmental concerns. A much\nstronger approach would consider the interaction of a variety of different solvents with a reaction of\ninterest to directly predict reaction yields, in such a way that the best possible solvent can be selected\naccording to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical\nreaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n[9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\ndue to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\nprediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\nlack the continuous reaction conditions, such as temperature and residence time, that are required to\nscale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that\nallows for quick and efficient screening of continuous reaction conditions. We specifically provide\nyield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\nmeasurements across the residence time, temperature, and solvent space. We answer the call for\nmore flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\nchallenges to current machine learning methods for chemistry, and identify potential solutions.\n1.1 Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning\nbenchmarking tends to be poor. This can be a result of improper formatting or documentation,\nincomplete information about reaction conditions or the experimental set-up, or the lack of machine\nreadability, leading to limited usage by the ML community. However, some effort has been made\nto address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n[18], a repository containing over 2M different reactions, many of which come from US patent data\n(USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\nlearning readiness and data inconsistencies across reactions.\nORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\ndataset for forward and retro-synthetic prediction using transformers; however, it also shows that\nyield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\nconclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\nconcentrations, and duration have a significant effect on yield. The assumption that every reaction in\nthe dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\nmodels for yield, and highlighting the importance of creating datasets with full (including potentially\nsub-optimal) reaction conditions.\nMore relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u0002coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\nand Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\npurposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\nand Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n[16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\nfocus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\nand residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\nour case temperature and residence time), as well as providing a pseudo-continuous representation of\nsolvents themselves through the use of solvent mixtures.\nPerhaps the closest example to our dataset is presented in Nguyen et al. [24], who used high\u0002throughput experimentation to screen 12708 catal...",
      "url": "https://www.arxiv.org/pdf/2506.07619"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - deepchem/moleculenet: Moleculenet.ai Datasets And Splits\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/moleculenet)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/moleculenet)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=deepchem/moleculenet)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[deepchem](https://github.com/deepchem)/**[moleculenet](https://github.com/deepchem/moleculenet)**Public\n* [Notifications](https://github.com/login?return_to=/deepchem/moleculenet)You must be signed in to change notification settings\n* [Fork22](https://github.com/login?return_to=/deepchem/moleculenet)\n* [Star105](https://github.com/login?return_to=/deepchem/moleculenet)\nMoleculenet.ai Datasets And Splits\n### License\n[MIT license](https://github.com/deepchem/moleculenet/blob/master/LICENSE)\n[105stars](https://github.com/deepchem/moleculenet/stargazers)[22forks](https://github.com/deepchem/moleculenet/forks)[Branches](https://github.com/deepchem/moleculenet/branches)[Tags](https://github.com/deepchem/moleculenet/tags)[Activity](https://github.com/deepchem/moleculenet/activity)\n[Star](https://github.com/login?return_to=/deepchem/moleculenet)\n[Notifications](https://github.com/login?return_to=/deepchem/moleculenet)You must be signed in to change notification settings\n# deepchem/moleculenet\nmaster\n[Branches](https://github.com/deepchem/moleculenet/branches)[Tags](https://github.com/deepchem/moleculenet/tags)\n[](https://github.com/deepchem/moleculenet/branches)[](https://github.com/deepchem/moleculenet/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[78 Commits](https://github.com/deepchem/moleculenet/commits/master/)\n[](https://github.com/deepchem/moleculenet/commits/master/)\n|\n[examples](https://github.com/deepchem/moleculenet/tree/master/examples)\n|\n[examples](https://github.com/deepchem/moleculenet/tree/master/examples)\n|\n|\n|\n[.gitignore](https://github.com/deepchem/moleculenet/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/deepchem/moleculenet/blob/master/.gitignore)\n|\n|\n|\n[LICENSE](https://github.com/deepchem/moleculenet/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/deepchem/moleculenet/blob/master/LICENSE)\n|\n|\n|\n[MolNet\\_Benchmark\\_Tutorial.ipynb](https://github.com/deepchem/moleculenet/blob/master/MolNet_Benchmark_Tutorial.ipynb)\n|\n[MolNet\\_Benchmark\\_Tutorial.ipynb](https://github.com/deepchem/moleculenet/blob/master/MolNet_Benchmark_Tutorial.ipynb)\n|\n|\n|\n[README.md](https://github.com/deepchem/moleculenet/blob/master/README.md)\n|\n[README.md](https://github.com/deepchem/moleculenet/blob/master/README.md)\n|\n|\n|\n[setup.cfg](https://github.com/deepchem/moleculenet/blob/master/setup.cfg)\n|\n[setup.cfg](https://github.com/deepchem/moleculenet/blob/master/setup.cfg)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# MoleculeNet Leaderboard\n[](#moleculenet-leaderboard)\n## Physical Chemistry\n[](#physical-chemistry)\n### Delaney (ESOL)\n[](#delaney-esol)\n|Rank|Model|Featurization|Test RMSE|Validation RMSE|Contact|References|Date|\n1|GCN|GraphConv|0.8851 +- 0.0292|0.9405 +- 0.0310|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://arxiv.org/abs/1609.02907),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Jan 28th, 2020|\n2|Random Forest|1024-bit ECFP4|1.7406 +- 0.0261|1.7932 +- 0.0153|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Jan 28th, 2020|\n### Lipo\n[](#lipo)\n|Rank|Model|Featurization|Test RMSE|Validation RMSE|Contact|References|Date|\n1|GCN|GraphConv|0.7806 +- 0.0404|0.7897 +- 0.0553|[Yuanqi Du](https://github.com/deepchem/moleculenet/blob/master/ydu6@gmu.edu)|[Paper](https://arxiv.org/abs/1609.02907),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Feb 13th, 2021|\n2|Random Forest|1024-bit ECFP4|0.9621 +- 0.0030|1.0024 +- 0.0029|[Yuanqi Du](https://github.com/deepchem/moleculenet/blob/master/ydu6@gmu.edu)|[Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Feb 13th, 2021|\n## Biophysics\n[](#biophysics)\n### BACE Classification\n[](#bace-classification)\n|Rank|Model|Featurization|Test ROC-AUC|Validation ROC-AUC|Contact|References|Date|\n1|Random Forest|1024-bit ECFP4|0.8507 +- 0.0072|0.7368 +- 0.0066|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Dec 2nd, 2020|\n2|GCN|GraphConv|0.8175 +- 0.0193|0.7430 +- 0.0194|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://arxiv.org/abs/1609.02907),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Dec 20th, 2020|\n### BACE Regression\n[](#bace-regression)\n|Rank|Model|Featurization|Test RMSE|Validation RMSE|Contact|References|Date|\n1|Random Forest|1024-bit ECFP4|1.3178 +- 0.0081|0.6716 +- 0.0059|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Dec 26th, 2020|\n2|GCN|GraphConv|1.6450 +- 0.1325|0.5244 +- 0.0200|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://arxiv.org/abs/1609.02907),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Dec 26th, 2020|\n### PDBbind (core)\n[](#pdbbind-core)\n|Rank|Model|Featurization|Test RMSE (R^2)|Validation RMSE (R^2)|Contact|References|Date|\n1|ACNN|AtomicConv|1.8604 +- 0.0540 (0.5405 +- 0.0124)|1.5885 +- 0.0865 (0.6015 +- 0.0330)|[Nathan Frey](https://github.com/deepchem/moleculenet/blob/master/n.frey@seas.upenn.edu)|[Paper](https://arxiv.org/pdf/1703.10603.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|March 24, 2021|\n## Physiology\n[](#physiology)\n### BBBP\n[](#bbbp)\n|Rank|Model|Featurization|Test ROC-AUC|Validation ROC-AUC|Contact|References|Date|\n1|Random Forest|1024-bit ECFP4|0.7194 +- 0.0068|0.9713 +- 0.0013|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),[Code](https://github.com/deepchem/moleculenet/blob/master/examples)|Dec 30th, 2020|\n2|GCN|GraphConv|0.6760 +- 0.0195|0.9736 +- 0.0027|[Mufei Li](https://github.com/deepchem/moleculenet/blob/master/mufeili1996@gmail.com)|[Paper](https://arxiv.org/abs/1609.02907),...",
      "url": "https://github.com/deepchem/moleculenet"
    }
  ]
}