{
  "query": "How do you use pre-trained molecular embeddings from ChemBERTa or MolBERT as features for downstream regression tasks in chemistry?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the DeepChem tutorial and the\u202fmolfeat\u202fdocumentation, you can turn ChemBERTa\u202for\u202fMolBERT\u202fembeddings into feature vectors and feed them to any regression model in a few steps ([DeepChem tutorial](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers);\u202f[molfeat finetuning guide](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n1. **Install the required libraries**  \n   ```bash\n   mamba install -c conda-forge transformers \"tokenizers <0.13.2\" molfeat\n   ```  \n   (molfeat provides the wrapper around HuggingFace models)\u202f([molfeat finetuning guide](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n2. **Load the pretrained transformer**  \n   ```python\n   from molfeat.store import ModelStore\n   store = ModelStore()\n   chemberta = store.get_pretrained(\"chemberta\")   # or \"molbert\"\n   ```  \n   The store fetches the ChemBERTa/MolBERT weights from HuggingFace\u202f([molfeat usage](https://molfeat-docs.datamol.io/stable/usage.html)).\n\n3. **Create a SMILES\u2011to\u2011embedding pipeline**  \n   ```python\n   from molfeat.trans import MoleculeTransformer\n   transformer = MoleculeTransformer(chemberta, n_jobs=-1)   # parallelizable\n   embeddings = transformer(smiles_list)   # shape: (n_molecules, d)\n   ```  \n   The `MoleculeTransformer` tokenizes each SMILES string, runs it through the model, and returns the pooled representation (default is the CLS token)\u202f([molfeat finetuning guide](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).\n\n4. **Prepare the regression dataset**  \n   ```python\n   import pandas as pd\n   df = pd.read_csv(\"my_data.csv\")          # columns: smiles, target\n   X = transformer(df[\"smiles\"].tolist())\n   y = df[\"target\"].values\n   ```  \n   The embeddings `X` are now fixed\u2011size feature vectors ready for any downstream learner\u202f([DeepChem tutorial](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers)).\n\n5. **Train a regression model** (e.g., scikit\u2011learn)  \n   ```python\n   from sklearn.model_selection import train_test_split\n   from sklearn.ensemble import RandomForestRegressor\n   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n   model = RandomForestRegressor(n_estimators=200, random_state=0)\n   model.fit(X_train, y_train)\n   preds = model.predict(X_test)\n   ```  \n   Any regressor (linear, gradient\u2011boosted, neural net) can be used because the embeddings already capture rich molecular information\u202f([DeepChem tutorial](https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers)).\n\n6. **(Optional) Fine\u2011tune the transformer on your task**  \n   If you have enough labeled data, wrap the pretrained model in a PyTorch training loop and back\u2011propagate the regression loss; molfeat provides examples of this workflow\u202f([molfeat finetuning guide](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html)).  \n\nFollowing these steps, ChemBERTa or MolBERT embeddings become straightforward input features for any downstream regression problem in chemistry.",
      "url": ""
    },
    {
      "title": "Tutorial: ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction using a Smiles Tokenization Strategy\n       \n       \u00b6",
      "text": "DeepChem\n[\n![DeepChem Logo](https://deepchem.io/deepchem.github.io//_next/static/media/deepchem-logo.95f3f074.png)\nDeepChem\n](https://deepchem.io/)\n**\n**## Tutorials\n# Tutorial: ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction using a Smiles Tokenization Strategy[\u00b6](#Tutorial:-ChemBERTa:-Large-Scale-Self-Supervised-Pretraining-for-Molecular-Property-Prediction-using-a-Smiles-Tokenization-Strategy)\n![alt text](https://huggingface.co/front/assets/huggingface_mask.svg)\nBy Seyone Chithrananda ([Twitter](https://twitter.com/SeyoneC))\nDeep learning for chemistry and materials science remains a novel field with lots of potiential. However, the popularity of transfer learning based methods in areas such as natural language processing (NLP) and computer vision have not yet been effectively developed in computational chemistry + machine learning. Using HuggingFace's suite of models and the ByteLevel tokenizer, we are able to train a large-transformer model, RoBERTa, on a large corpus of 10,000,000 SMILES strings from a commonly known benchmark chemistry dataset, PubChem.\nTraining RoBERTa over 10 epochs, the model achieves a pretty good loss of 0.198, and may likely continue to converge if trained for a larger number of epochs. The model can predict masked/corrupted tokens within a SMILES sequence/molecule, allowing for variants of a molecule within discoverable chemical space to be predicted.\nBy applying the representations of functional groups and atoms learned by the model, we can try to tackle problems of toxicity, solubility, drug-likeness, and synthesis accessibility on smaller datasets using the learned representations as features for graph convolution and attention models on the graph structure of molecules, as well as fine-tuning of BERT. Finally, we propose the use of attention visualization as a helpful tool for chemistry practitioners and students to quickly identify important substructures in various chemical properties.\nAdditionally, visualization of the attention mechanism have been seen through previous research as incredibly valuable towards chemical reaction classification. The applications of open-sourcing large-scale transformer models such as RoBERTa with HuggingFace may allow for the acceleration of these individual research directions.\nA link to a repository which includes the training, uploading and evaluation notebook (with sample predictions on compounds such as Remdesivir) can be found[here](https://github.com/seyonechithrananda/bert-loves-chemistry). All of the notebooks can be copied into a new Colab runtime for easy execution. This repository will be updated with new features, such as attention visualization, easier benchmarking infrastructure, and more. The work behind this tutorial has been published on[Arxiv](https://arxiv.org/abs/2010.09885), and was accepted for a**poster presentation at NeurIPS 2020's ML for Molecules Workshop**.\nFor the sake of this tutorial, we'll be fine-tuning a pre-trained ChemBERTa on a small-scale molecule dataset, Clintox, to show the potiential and effectiveness of HuggingFace's NLP-based transfer learning applied to computational chemistry. Output for some cells are purposely cleared for readability, so do not worry if some output messages for your cells differ!\nIn short, there are three major components we'll be going over in this notebook.\n1. Masked token inference predictions on SMILES strings\n2. Attention visualizaiton of the PubChem-10M model\n3. Fine-tuninhg BPE-ChemBERTa and Smiles-Tokenizer ChemBERTa model's on the CLintox toxicity dataset.\n**Don't worry if you aren't familiar with some of these terms. We will explain them later in the tutorial!**\nIf you're looking to dive deeper, check out the poster[here](https://seyonechithrananda.com/ChemBERTa-DeepChem-e1244b82c8fb40bca8c3c882acb9baa5).\n## Colab[\u00b6](#Colab)\nThis tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\n## Setup[\u00b6](#Setup)\nTo run DeepChem within Colab, you'll need to run the following cell of installation commands. This will take about 5 minutes to run to completion and install your environment.\nIn\u00a0[1]:\n```\n!curl -Lo conda\\_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab\\_install.pyimportconda\\_installerconda\\_installer.install()!/root/miniconda/bin/conda info -e\n```\n```\n% Total % Received % Xferd Average Speed Time Time Time Current\nDload Upload Total Spent Left Speed\n100 3501 100 3501 0 0 16995 0 --:--:-- --:--:-- --:--:-- 16995\n```\n```\nadd /root/miniconda/lib/python3.7/site-packages to PYTHONPATH\npython version: 3.7.10\nremove current miniconda\nfetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86\\_64.sh\ndone\ninstalling miniconda to /root/miniconda\ndone\ninstalling rdkit, openmm, pdbfixer\nadded omnia to channels\nadded conda-forge to channels\ndone\nconda packages installation finished!\n```\n```\n# conda environments:\n#\nbase \\* /root/miniconda\n```\nIn\u00a0[2]:\n```\n!pip install --pre deepchemimportdeepchemdeepchem.\\_\\_version\\_\\_\n```\n```\nRequirement already satisfied: deepchem in /usr/local/lib/python3.7/dist-packages (2.5.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.4.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem) (0.22.2.post1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.19.5)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.0.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;deepchem) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;deepchem) (2018.9)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;deepchem) (1.15.0)\n```\n```\nwandb:WARNINGW&amp;&amp;B installed but not logged in. Run `wandb login` or set the WANDB\\_API\\_KEY env variable.wandb:WARNINGW&amp;&amp;B installed but not logged in. Run `wandb login` or set the WANDB\\_API\\_KEY env variable.\n```\nOut[2]:\n```\n'2.5.0'\n```\nIn\u00a0[3]:\n```\nfromrdkitimportChem\n```\nWe want to install NVIDIA's Apex tool, for the training pipeline used by`simple-transformers`and Weights and Biases. This package enables us to use 16-bit training, mixed precision, and distributed training without any changes to our code. Generally GPUs are good at doing 32-bit(single precision) math, not at 16-bit(half) nor 64-bit(double precision). Therefore traditionally deep learning model trainings are done in 32-bit. By switching to 16-bit, we\u2019ll be using half the memory and theoretically less computation at the expense of the available number range and precision. However, pure 16-bit training creates a lot of problems for us (imprecise weight updates, gradient underflow and overflow).**Mixed precision training, with Apex, alleviates these problems**.\nWe will be installing`simple-transformers`, a library which builds ontop of HuggingFace's`transformers`package specifically for fine-tuning ChemBERTa.\nIn\u00a0[\u00a0]:\n```\n!git clone https://github.com/NVIDIA/apex!cd/content/apex!pip install -v --no-cache-dir /content/apex!pip install transformers!pip install simpletransformers!pip install wandb!cd..\n```\nIn\u00a0[5]:\n```\nimportsys!test-d bertviz\\_repo&amp;&amp;echo\"FYI: bertviz\\_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz\\_repo\"# !rm -r b...",
      "url": "https://deepchem.io/tutorials/transfer-learning-with-chemberta-transformers"
    },
    {
      "title": "Finetuning a pretrained transformer - molfeat",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html#huggingface-transformer-finetuning)\n\n# Finetuning a pretrained transformer\n\nIn\u00a0\\[1\\]:\n\nCopied!\n\n```\n%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n```\n\n%load\\_ext autoreload\n%autoreload 2\nimport torch\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n## HuggingFace Transformer Finetuning [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#huggingface-transformer-finetuning)\n\nCommunity contribution\n\nCurious how one would run this tutorial on [Graphcore IPUs](https://www.graphcore.ai/products/ipu)? See this tutorial contributed by [@s-maddrellmander](https://github.com/s-maddrellmander):\n[![Run on Gradient](https://camo.githubusercontent.com/c9931a1689c37ab786edd3e1e5f59b9a6f7d097628c4689ce2432563ef884524/68747470733a2f2f6173736574732e706170657273706163652e696f2f696d672f6772616469656e742d62616467652e737667)](https://ipu.dev/yoyy6N)\n\nWe have previously shown how [Molfeat integrates with PyTorch in general](https://molfeat-docs.datamol.io/stable/tutorials/integration.html) and even with [Pytorch Geometric](https://molfeat-docs.datamol.io/stable/tutorials/pyg_integration.html). Now we will demonstrate how to use molfeat to finetune a pretrained transformer. This tutorial will walk you through an example of finetuning the ChemBERTa pretrained model for molecular property prediction. These same principles can be applied to any pretrained transformers available in molfeat.\n\nTo run this tutorial, you will need to install `transformers` and `tokenizers`.\n\n`mamba install -c conda-forge transformers \"tokenizers <0.13.2\"`\n\nAdvanced users\n\nThis tutorial is for advanced users that are comfortable with the APIs of molfeat and Hugging Face transformers.\n\nIn\u00a0\\[2\\]:\n\nCopied!\n\n```\nfrom molfeat.utils.converters import SmilesConverter\nfrom molfeat.trans.pretrained import PretrainedHFTransformer\n\n```\n\nfrom molfeat.utils.converters import SmilesConverter\nfrom molfeat.trans.pretrained import PretrainedHFTransformer\n\n### Featurizer [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#featurizer)\n\nPretrained Transformer Featurizer in molfeat have an underlying object `featurizer` that can handle both tokenization and embedding.\n\nWe will leverage this structure in molfeat to initialize our transformer model, but also to tokenize our molecules\n\nWe first start by defining our featurizer. Here we will use the ChemBERTa pretrained model.\n\nIn\u00a0\\[3\\]:\n\nCopied!\n\n```\nfeaturizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)\n\n```\n\nfeaturizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)\n\n- Note the use of preload to preload the model in the `__init__`\n- Note how we define a pooling mechanism here. Molfeat provides [several poolers that you can explore in the API](https://molfeat-docs.datamol.io/stable/tutorials/api/molfeat.utils.html#pooling). Because a pooling layer can already be specified and will be accessible through the `_pooling_obj` attribute we will not bother defining one later. Instead we will just retrieve the one from the featurizer.\n\n### Dataset [\u00b6](https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html\\#dataset)\n\nFor the dataset, we will use the `BBBP` dataset, which contains binary labels of blood-brain barrier penetration.\n\nIn\u00a0\\[4\\]:\n\nCopied!\n\n```\ndf = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")\n\n```\n\ndf = pd.read\\_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")\n\nIn\u00a0\\[5\\]:\n\nCopied!\n\n```\ndf.head()\n\n```\n\ndf.head()\n\nOut\\[5\\]:\n\n|  | num | name | p\\_np | smiles |\n| --- | --- | --- | --- | --- |\n| 0 | 1 | Propanolol | 1 | \\[Cl\\].CC(C)NCC(O)COc1cccc2ccccc12 |\n| 1 | 2 | Terbutylchlorambucil | 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl |\n| 2 | 3 | 40730 | 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... |\n| 3 | 4 | 24 | 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C |\n| 4 | 5 | cloxacillin | 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N\\[C@H\\]3\\[C@H\\]4SC(C)(C)... |\n\nNow we just need to define our PyTorch Dataset. As discussed above, we will leverage the internal structure of our transformer\n\nIn\u00a0\\[6\\]:\n\nCopied!\n\n```\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\nclass DTset(Dataset):\n    def __init__(self, smiles, y, mf_featurizer):\n        super().__init__()\n        self.smiles = smiles\n        self.mf_featurizer = mf_featurizer\n        self.y = torch.tensor(y).float()\n        # here we use the molfeat mf_featurizer to convert the smiles to\n        # corresponding tokens based on the internal tokenizer\n        # we just want the data from the batch encoding object\n        self.transformed_mols = self.mf_featurizer._convert(smiles)\n\n    @property\n    def embedding_dim(self):\n        return len(self.mf_featurizer)\n\n    @property\n    def max_length(self):\n        return self.transformed_mols.shape[-1]\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def collate_fn(self, **kwargs):\n        # the default collate fn self.mf_featurizer.get_collate_fn(**kwargs)\n        # returns None, which should just concatenate the inputs\n        # You could also use `transformers.default_data_collator` instead\n        return self.mf_featurizer.get_collate_fn(**kwargs)\n\n    def __getitem__(self, index):\n        datapoint = dict((name, val[index]) for name, val in self.transformed_mols.items())\n        datapoint[\"y\"] = self.y[index]\n        return datapoint\n\n```\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import default\\_data\\_collator\nclass DTset(Dataset):\ndef \\_\\_init\\_\\_(self, smiles, y, mf\\_featurizer):\nsuper().\\_\\_init\\_\\_()\nself.smiles = smiles\nself.mf\\_featurizer = mf\\_featurizer\nself.y = torch.tensor(y).float()\n\\# here we use the molfeat mf\\_featurizer to convert the smiles to\n\\# corresponding tokens based on the internal tokenizer\n\\# we just want the data from the batch encoding object\nself.transformed\\_mols = self.mf\\_featurizer.\\_convert(smiles)\n@property\ndef embedding\\_dim(self):\nreturn len(self.mf\\_featurizer)\n@property\ndef max\\_length(self):\nreturn self.transformed\\_mols.shape\\[-1\\]\ndef \\_\\_len\\_\\_(self):\nreturn self.y.shape\\[0\\]\ndef collate\\_fn(self, \\*\\*kwargs):\n\\# the default collate fn self.mf\\_featurizer.get\\_collate\\_fn(\\*\\*kwargs)\n\\# returns None, which should just concatenate the inputs\n\\# You could also use \\`transformers.default\\_data\\_collator\\` instead\nreturn self.mf\\_featurizer.get\\_collate\\_fn(\\*\\*kwargs)\ndef \\_\\_getitem\\_\\_(self, index):\ndatapoint = dict((name, val\\[index\\]) for name, val in self.transformed\\_mols.items())\ndatapoint\\[\"y\"\\] = self.y\\[index\\]\nreturn datapoint\n\nIn\u00a0\\[7\\]:\n\nCopied!\n\n```\ndataset = DTset(df.smiles.values, df.p_np.values, featurizer)\ngenerator = torch.Generator().manual_seed(42)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dt, test_dt = torch.utils.data.random_split(dataset, [train_size, test_size], generator=generator)\n\n```\n\ndataset = DTset(df.smiles.values, df.p\\_np.values, featurizer)\ngenerator = torch.Generator().manual\\_seed(42)\ntrain\\_size = int(0.8 \\* len(dataset))\ntest\\_size = len(dataset) - train\\_size\ntrain\\_dt, test\\_dt = torch.utils.data.random\\_split(dataset, \\[train\\_size, test\\_size\\], generator=generator)\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\n```\n  0%|          | 0/2050 [00:00<?, ?it/s]\n```\n\nIn\u00a0\\[8\\]:\n\nCopied!\n\n```\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dt, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn())\ntest_loader = DataLoader(test_dt, batch_size=BATCH_SIZE, shuffle=False, collate_fn=dataset.collate_fn())\n\n```\n\nBATCH\\_SIZE = 64\ntrain\\_loader = DataLoader(train\\_dt, batch\\_size=BATCH\\_SIZE, shuffle=True, collate\\_fn=dat...",
      "url": "https://molfeat-docs.datamol.io/stable/tutorials/transformer_finetuning.html"
    },
    {
      "title": "How to use molfeat? - molfeat",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/stable/usage.html#usage)\n\n# Usage [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#usage)\n\n## Structure [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#structure)\n\nMolfeat is organized in three main modules:\n\n- `molfeat.store`: The model store loads, lists and registers all featurizers.\n- `molfeat.calc`: A calculator is a callable that featurizes a single molecule.\n- `molfeat.trans`: A transformer is a scikit-learn compatible class that wraps a calculator in a featurization pipeline.\n\nLearn more about the different types of featurizers\n\nConsult [this tutorial](https://molfeat-docs.datamol.io/stable/tutorials/types_of_featurizers.html) to dive deeper into the differences between the calculator and transformer.\nIt provides a good overview of the different types of featurizers and has pointers for learning about more advanced features.\n\n## Quick API Tour [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#quick-api-tour)\n\nCommunity contribution\n\nCurious how molfeat can simplify training QSAR models? See this tutorial contributed by [@PatWalters](https://github.com/PatWalters): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PatWalters/practical_cheminformatics_tutorials/blob/main/ml_models/QSAR_in_8_lines.ipynb)\n\n```\nimport datamol as dm\nfrom molfeat.calc import FPCalculator\nfrom molfeat.trans import MoleculeTransformer\nfrom molfeat.store.modelstore import ModelStore\n\n# Load some dummy data\ndata = dm.data.freesolv().sample(100).smiles.values\n\n# Featurize a single molecule\ncalc = FPCalculator(\"ecfp\")\ncalc(data[0])\n\n# Define a parallelized featurization pipeline\nmol_transf = MoleculeTransformer(calc, n_jobs=-1)\nmol_transf(data)\n\n# Easily save and load featurizers\nmol_transf.to_state_yaml_file(\"state_dict.yml\")\nmol_transf = MoleculeTransformer.from_state_yaml_file(\"state_dict.yml\")\nmol_transf(data)\n\n# List all available featurizers\nstore = ModelStore()\nstore.available_models\n\n# Find a featurizer and learn how to use it\nmodel_card = store.search(name=\"ChemBERTa-77M-MLM\")[0]\nmodel_card.usage()\n\n```\n\n## FAQ [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#faq)\n\n#### What is a molecular featurizer ? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#what-is-a-molecular-featurizer)\n\nA molecular featurizer is a function or model that provides numerical representations for molecular structures. These numerical features serve as inputs for machine learning models, enabling them to predict molecular properties and activities, design novel molecules, perform molecular analyses, or conduct searches for similar molecules.\n\n#### Why so many molecular featurizers in `molfeat`? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#why-so-many-molecular-featurizers-in-molfeat)\n\nThe reason for providing a diverse range of molecular featurizers in `molfeat` is to address the inherent uncertainty in determining which molecular representation performs best for a given task. Different featurization methods exist, such as using physico-chemical descriptors, molecular structure fingerprints, deep learning embeddings, and more. The effectiveness of these representations varies depending on the specific application. Therefore, the availability of multiple featurizers in `molfeat` ensures that users can access the most suitable featurizer for their unique needs.\n\n#### What is the difference between a calculator and a featurizer in `molfeat`? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#what-is-the-difference-between-a-calculator-and-a-featurizer-in-molfeat)\n\nIn `molfeat`,\n\n- a `calculator` operates on individual molecules and specifies the process of transforming an input molecule into a numerical representation.\n- a `featurizer` works with batches of molecules, leveraging the efficiency of deep learning models on batch processing. Some `featurizers` uses a `calculator` internally to feature each molecule individually and then stitch their outputs together. Additionally, `featurizers` offer convenient tools, such as parallelism and caching, to optimize the computation of molecular representations efficiently.\n\n`molfeat` has been designed with utmost flexibility, recognizing that the actions users wish to perform with molecular data can be vast and diverse, and there often isn't a single \"right\" way to approach them.\n\n#### What functions should I be familiar with when using the featurizer classes ? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#what-functions-should-i-be-familiar-with-when-using-the-featurizer-classes)\n\nWhen using a `featurizer` in `molfeat`, you should be familiar with the following functions:\n\n- `preprocess()`: This method performs preprocessing of your input molecules to ensure compatibility with the expected featurizer class you are using. It's essential to note that the preprocessing steps **are not automatically applied to your inputs** to maintain independence from the molecular transformation. The preprocess function takes your molecule inputs, along with optional labels, and can be redefined when creating a custom featurizer.\n\n- `transform()`: This method operates on a batch of molecules and returns a list of representations, where the actual featurization occurs. In cases where featurization fails, the position can be denoted as `None`, especially when you choose to `ignore_errors`.\n- `_transform()`: This method operates on a single input molecule, performing the actual featurization.\n- `__call__()`: This method uses `transform()` under the hood and provides convenient arguments, such as enforcing the datatype defined during the initialization of your model, to the outputs. If you specify `ignore_errors`, a vector of indexes where featurization did not fail will also be returned.\n\nIn addition to the methods described above, PretrainedMolTransformer introduces the following functions:\n\n- `_embed()`: For pre-trained models that benefit from batched featurization, this method is internally called during transform instead of an internal calculator.\n- `_convert()`: This method is called by the transformer to convert the molecule input into the expected format of the underlying ML model. For example, for a pre-trained language model expecting SELFIES strings, we will perform the conversion to SELFIES strings here.\n\n#### I am getting an error and I am not sure what to do ? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#i-am-getting-an-error-and-i-am-not-sure-what-to-do)\n\nWhen encountering an error during the featurization process, you have a couple of options to handle it:\n\n- Ignore Errors: You can choose to set the `ignore_errors` parameter to `True` when using the featurizer. This allows the featurizer to continue processing even if it encounters errors on some molecules in your dataset. The featurizer will still attempt to calculate representations for all molecules, and any molecules that failed featurization will have their position in the output list marked as `None`.\n\n- Increase Verbosity: If you're unsure about the specific errors occurring during featurization, you can set the verbosity of the featurizer to True. This will enable the featurizer to log all errors encountered during the process, providing more detailed information about the cause of the issue, since because of the above features, some silent errors are often caught but not propagated.\n\nFor example, the following will ensure that all errors are logged.\n\n```\nfrom molfeat.trans.concat import FeatConcat\nfrom molfeat.trans.fp import FPVecTransformer\nimport numpy as np\nfeaturizer = MoleculeTransformer(..., dtype=np.float32, verbose=True)\nfeaturizer([\"CSc1nc2cc3c(cc2[nH]1)N(Cc1ccc(S(=O)(=O)c2ccccc2)cc1)CCC3\"], enforce_dtype=True)\n\n```\n\n#### What are the base featurizers class in molfeat and how to use them ? [\u00b6](https://molfeat-docs.datamol.io/stable/usage.html\\#what-are-the-base-featurizers-class-in-molfeat-and-how-to-use-t...",
      "url": "https://molfeat-docs.datamol.io/stable/usage.html"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "deepchem/examples/tutorials/Transfer\\_Learning\\_With\\_ChemBERTa\\_Transformers.ipynb at master \u00b7deepchem/deepchem \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=deepchem/deepchem)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[deepchem](https://github.com/deepchem)/**[deepchem](https://github.com/deepchem/deepchem)**Public\n* [Notifications](https://github.com/login?return_to=/deepchem/deepchem)You must be signed in to change notification settings\n* [Fork2k](https://github.com/login?return_to=/deepchem/deepchem)\n* [Star6.5k](https://github.com/login?return_to=/deepchem/deepchem)\n</turbo-frame></main>\nYou can\u2019t perform that action at this time.\n</div>",
      "url": "https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb"
    },
    {
      "title": "Why should you care?",
      "text": "Why should you care? - molfeat\n[Skip to content](#use-cases)\n# Why should you care?\nIn\u00a0[1]:\nCopied!\n```\n%load\\_extautoreload%autoreload2\n```\n%load\\_ext autoreload\n%autoreload 2\nUnlike many other machine learning domains, molecular featurization (i.e. the process of transforming a molecule into a vector) lacks a consistently good default.\nIt is still an open question how to best capture the complexity of molecular data with a unified representation. Which molecular representation works best depends largely on which task you are modeling. To achieve optimal performance, it is wise to experiment with a variety of featurization schemes, from structural fingerprints to physicochemical descriptors and pre-trained embeddings.\n## Use cases[\u00b6](#use-cases)\nMolecular representations / featurizers are an integral part of any molecular modelling workflow and are commonly used for:\n* Search - to find molecules with similar electronic properties, similar structures, or similar biological activity for a target.\n* Clustering - to group molecules based on their features and derive hypotheses around the relationship between structure and activity\n* Modeling - to build QSAR model for molecular property/activity prediction## Importance of the choice of molecular representation[\u00b6](#importance-of-the-choice-of-molecular-representation)\nTo demonstrate the impact a featurizer can have, we establish two simple benchmarks:\n1. To demonstrate the impact on modeling, we will use two datasets from[MoleculeNet](https://moleculenet.org/datasets-1)[1].\n2. To demonstrate the impact on search, we will use the[RDKit Benchmarking Platform](https://github.com/rdkit/benchmarking_platform)[2, 3].\nWe will compare the performance of three different featurizers:\n* **ECFP6**[4]: Binary, circular fingerprints where each bit indicates the presence of particular substructures of a radius up to 3 bonds away from an atom.\n* **Mordred**[5]: Continuous descriptors with more than 1800 2D and 3D descriptors.\n* **ChemBERTa**[6]: Learned representations from a pre-trained SMILES transformer model.\nTl;dr - Importance of molecular representation\nNo featurizer consistently stood out for either task or even within a task category:\n* **Modeling:**The Mordred featurizer outperforms the next best featurizer by \\~20% on the*Lipophilicity*prediction task. On*ClinTox*, however, things are reversed and ChemBERTa outperforms the other featurizers by \\~about 18%.\n* **Search**: ECFP outperforms ChemBERTa and Mordred and is the best option across the board, although it's target dependent.\nThese quick examples show the context-dependent nature and thus the importance of experimenting with trying different featurizers.**In short, the perfect molecular featurizer doesn\u2019t exist (yet!)**. All have their pros and cons depending on the data and the downstream task.\n### Modeling[\u00b6](#modeling)\nWe will compare the performance on two datasets using scikit-learn[AutoML](https://github.com/automl/auto-sklearn)[7, 8] models.\nIn\u00a0[\u00a0]:\nCopied!\n```\n!pipinstallautosklearn\n```\n! pip install autosklearn\nIn\u00a0[2]:\nCopied!\n```\nimportosimporttqdmimportfsspecimportpickleimportwarningsimportnumpyasnpimportpandasaspdimportdatamolasdmimportmatplotlib.pyplotaspltimportautosklearn.classificationimportautosklearn.regressionfromcollectionsimportdefaultdictfromrdkit.ChemimportSaltRemoverfromsklearn.metricsimportmean\\_absolute\\_error,roc\\_auc\\_scorefromsklearn.model\\_selectionimportGroupShuffleSplitfromsklearn.neighborsimportKNeighborsClassifierfrommolfeat.trans.fpimportFPVecTransformerfrommolfeat.trans.pretrained.hf\\_transformersimportPretrainedHFTransformer\n```\nimport os\nimport tqdm\nimport fsspec\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport matplotlib.pyplot as plt\nimport autosklearn.classification\nimport autosklearn.regression\nfrom collections import defaultdict\nfrom rdkit.Chem import SaltRemover\nfrom sklearn.metrics import mean\\_absolute\\_error, roc\\_auc\\_score\nfrom sklearn.model\\_selection import GroupShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf\\_transformers import PretrainedHFTransformer\nIn\u00a0[3]:\nCopied!\n```\n# Making the output less verbosewarnings.simplefilter(\"ignore\")os.environ[\"PYTHONWARNINGS\"]=\"ignore\"os.environ[\"TOKENIZERS\\_PARALLELISM\"]=\"false\"dm.disable\\_rdkit\\_log()\n```\n# Making the output less verbose\nwarnings.simplefilter(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\nos.environ[\"TOKENIZERS\\_PARALLELISM\"] = \"false\"\ndm.disable\\_rdkit\\_log()\nIn\u00a0[4]:\nCopied!\n```\ndefload\\_dataset(uri:str,readout\\_col:str):\"\"\"Loads the MoleculeNet dataset\"\"\"df=pd.read\\_csv(uri)smiles=df[\"smiles\"].valuesy=df[readout\\_col].valuesreturnsmiles,ydefpreprocess\\_smiles(smi):\"\"\"Preprocesses the SMILES string\"\"\"mol=dm.to\\_mol(smi,ordered=True,sanitize=False)try:mol=dm.sanitize\\_mol(mol)except:# noqa: E722mol=NoneifmolisNone:returnmol=dm.standardize\\_mol(mol,disconnect\\_metals=True)remover=SaltRemover.SaltRemover()mol=remover.StripMol(mol,dontRemoveEverything=True)returndm.to\\_smiles(mol)defscaffold\\_split(smiles):\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"scaffolds=[dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi)))forsmiinsmiles]splitter=GroupShuffleSplit(n\\_splits=1,test\\_size=0.2,random\\_state=42)returnnext(splitter.split(smiles,groups=scaffolds))\n```\ndef load\\_dataset(uri: str, readout\\_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\ndf = pd.read\\_csv(uri)\nsmiles = df[\"smiles\"].values\ny = df[readout\\_col].values\nreturn smiles, y\ndef preprocess\\_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\nmol = dm.to\\_mol(smi, ordered=True, sanitize=False)\ntry:\nmol = dm.sanitize\\_mol(mol)\nexcept: # noqa: E722\nmol = None\nif mol is None:\nreturn\nmol = dm.standardize\\_mol(mol, disconnect\\_metals=True)\nremover = SaltRemover.SaltRemover()\nmol = remover.StripMol(mol, dontRemoveEverything=True)\nreturn dm.to\\_smiles(mol)\ndef scaffold\\_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\nscaffolds = [dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi))) for smi in smiles]\nsplitter = GroupShuffleSplit(n\\_splits=1, test\\_size=0.2, random\\_state=42)\nreturn next(splitter.split(smiles, groups=scaffolds))\nIn\u00a0[5]:\nCopied!\n```\n# Setup the featurizerstrans\\_ecfp=FPVecTransformer(kind=\"ecfp:6\",n\\_jobs=-1)trans\\_mordred=FPVecTransformer(kind=\"mordred\",replace\\_nan=True,n\\_jobs=-1)trans\\_chemberta=PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\",notation=\"smiles\")\n```\n# Setup the featurizers\ntrans\\_ecfp = FPVecTransformer(kind=\"ecfp:6\", n\\_jobs=-1)\ntrans\\_mordred = FPVecTransformer(kind=\"mordred\", replace\\_nan=True, n\\_jobs=-1)\ntrans\\_chemberta = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", notation=\"smiles\")\n#### Lipophilicity[\u00b6](#lipophilicity)\nLipophilicity is a regression task with 4200 molecules\nIn\u00a0[6]:\nCopied!\n```\n# Prepare the Lipophilicity datasetsmiles,y\\_true=load\\_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\",\"exp\")smiles=np.array([preprocess\\_smiles(smi)forsmiinsmiles])smiles=np.array([smiforsmiinsmilesifdm.to\\_mol(smi)isnotNone])feats\\_ecfp,ind\\_ecfp=trans\\_ecfp(smiles,ignore\\_errors=True)feats\\_mordred,ind\\_mordred=trans\\_mordred(smiles,ignore\\_errors=True)feats\\_chemberta,ind\\_chemberta=trans\\_chemberta(smiles,ignore\\_errors=True)X={\"ECFP\":feats\\_ecfp[ind\\_ecfp],\"Mordred\":feats\\_mordred[ind\\_mordred],\"ChemBERTa\":feats\\_chemberta[ind\\_chemberta],}\n```\n# Prepare the Lipophilicity dataset\nsmiles, y\\_true = load\\_dataset(\n\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\"\n)\nsmiles = np.array([preprocess\\_smiles(smi) for smi in smiles])\nsmiles = np.array([smi for smi in smiles if dm.to\\_mol(smi) is not None])\nfeats\\_ecfp, ind\\_ecfp = trans\\_ecfp(smiles, ignore\\_errors=True)\nfeats\\_mordred, ind\\_mordred = trans\\_mordred(smiles, ignore\\_errors=True)\nfeats\\_chem...",
      "url": "https://molfeat-docs.datamol.io/stable/benchmark.html"
    },
    {
      "title": "Why bother? - molfeat",
      "text": "[Skip to content](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io#one-featurizer-to-rule-them-all)\n\n# Why bother?\n\nIn\u00a0\\[1\\]:\n\nCopied!\n\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n%load\\_ext autoreload\n%autoreload 2\n\n## One featurizer to rule them all? [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#One-featurizer-to-rule-them-all?)\n\nContrary to many other machine learning domains, _molecular_ featurization (i.e. the process of transforming a molecule into a vector) lacks a good default. It remains unclear how we can effectively capture the richness of molecular data in a unified representation and what works best heavily depends on the nature and constraints of the task you are trying to model. It is therefore good practice to try different featurization schemes: From structural fingerprints, to physico-chemical descriptors and pre-trained embeddings.\n\n## Don't take our word for it [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Don't-take-our-word-for-it)\n\nTo demonstrate the impact a featurizer can have, we setup two simple benchmarks.\n\n1. To demonstrate the impact on modeling, we will use two datasets from [MoleculeNet](https://moleculenet.org/datasets-1).\n2. To demonstrate the impact on search, we will use the [RDKit Benchmarking Platform](https://github.com/rdkit/benchmarking_platform).\n\nWe will compare the performance of three different featurizers:\n\n- **ECFP6** \\[1\\]: Binary, circular fingerprints where each bit indicates the presence of particular substructures of a radius up to 3 bonds away from an atom.\n- **Mordred** \\[2\\]: Continuous descriptors with more than 1800 2D and 3D descriptors.\n- **ChemBERTa** \\[3\\]: Learned representations from a pre-trained SMILES transformer model.\n\n### Modeling [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Modeling)\n\nWe will compare the performance on two datasets using scikit-learn [AutoML](https://github.com/automl/auto-sklearn) \\[4, 5\\] models.\n\nIn\u00a0\\[3\\]:\n\nCopied!\n\n```\nimport os\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport autosklearn.classification\nimport autosklearn.regression\nfrom sklearn.metrics import mean_absolute_error, roc_auc_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom rdkit.Chem import SaltRemover\n\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\n```\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\nimport autosklearn.classification\nimport autosklearn.regression\nfrom sklearn.metrics import mean\\_absolute\\_error, roc\\_auc\\_score\nfrom sklearn.model\\_selection import GroupShuffleSplit\nfrom rdkit.Chem import SaltRemover\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf\\_transformers import PretrainedHFTransformer\n\nIn\u00a0\\[4\\]:\n\nCopied!\n\n```\ndef load_dataset(uri: str, readout_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\n    df = pd.read_csv(uri)\n    smiles = df[\"smiles\"].values\n    y = df[readout_col].values\n    return smiles, y\n\ndef preprocess_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\n    with dm.without_rdkit_log():\n        mol = dm.to_mol(smi, ordered=True, sanitize=False)\n        mol = dm.sanitize_mol(mol)\n        if mol is None:\n            return\n\n        mol = dm.standardize_mol(mol, disconnect_metals=True)\n        remover = SaltRemover.SaltRemover()\n        mol = remover.StripMol(mol, dontRemoveEverything=True)\n\n    return dm.to_smiles(mol)\n\ndef scaffold_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\n    scaffolds = [dm.to_smiles(dm.to_scaffold_murcko(dm.to_mol(smi))) for smi in smiles]\n    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    return next(splitter.split(smiles, groups=scaffolds))\n```\n\ndef load\\_dataset(uri: str, readout\\_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\ndf = pd.read\\_csv(uri)\nsmiles = df\\[\"smiles\"\\].values\ny = df\\[readout\\_col\\].values\nreturn smiles, y\ndef preprocess\\_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\nwith dm.without\\_rdkit\\_log():\nmol = dm.to\\_mol(smi, ordered=True, sanitize=False)\nmol = dm.sanitize\\_mol(mol)\nif mol is None:\nreturn\nmol = dm.standardize\\_mol(mol, disconnect\\_metals=True)\nremover = SaltRemover.SaltRemover()\nmol = remover.StripMol(mol, dontRemoveEverything=True)\nreturn dm.to\\_smiles(mol)\ndef scaffold\\_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\nscaffolds = \\[dm.to\\_smiles(dm.to\\_scaffold\\_murcko(dm.to\\_mol(smi))) for smi in smiles\\]\nsplitter = GroupShuffleSplit(n\\_splits=1, test\\_size=0.2, random\\_state=42)\nreturn next(splitter.split(smiles, groups=scaffolds))\n\nIn\u00a0\\[5\\]:\n\nCopied!\n\n```\n# Setup the featurizers\ntrans_ecfp = FPVecTransformer(kind=\"ecfp:6\", n_jobs=-1)\ntrans_mordred = FPVecTransformer(kind=\"mordred\", replace_nan=True, n_jobs=-1)\ntrans_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles')\n```\n\n\\# Setup the featurizers\ntrans\\_ecfp = FPVecTransformer(kind=\"ecfp:6\", n\\_jobs=-1)\ntrans\\_mordred = FPVecTransformer(kind=\"mordred\", replace\\_nan=True, n\\_jobs=-1)\ntrans\\_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles')\n\n```\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\n  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n\n```\n\n#### Lipophilicity [\u00b6](https://molfeat-docs.datamol.io/molfeat-docs.datamol.io\\#Lipophilicity)\n\nLipophilicity is a regression task with 4200 molecules\n\nIn\u00a0\\[6\\]:\n\nCopied!\n\n```\n# Prepare the Lipophilicity dataset\nsmiles, y_true = load_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\nsmiles = np.array([preprocess_smiles(smi) for smi in smiles])\nsmiles = np.array([smi for smi in smiles if smi != \"\"])\n\nX = {\n    \"ECFP\": trans_ecfp(smiles),\n    \"Mordred\": trans_mordred(smiles),\n    \"ChemBERTa\": trans_chemberta(smiles),\n}\n```\n\n\\# Prepare the Lipophilicity dataset\nsmiles, y\\_true = load\\_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\nsmiles = np.array(\\[preprocess\\_smiles(smi) for smi in smiles\\])\nsmiles = np.array(\\[smi for smi in smiles if smi != \"\"\\])\nX = {\n\"ECFP\": trans\\_ecfp(smiles),\n\"Mordred\": trans\\_mordred(smiles),\n\"ChemBERTa\": trans\\_chemberta(smiles),\n}\n\n```\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/home/cas/local/conda/envs/molfeat-benchmark/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **pass...",
      "url": "https://molfeat-docs.datamol.io/0.8.0/benchmark.html"
    },
    {
      "title": "ChemBERTa-2: Towards Chemical Foundation Models",
      "text": "[2209.01712] ChemBERTa-2: Towards Chemical Foundation Models\n\\\\newfloatcommand\ncapbtabboxtable[][\\\\FBwidth]\n# ChemBERTa-2: Towards Chemical Foundation Models\nWalid Ahmad\nReverie Labs\nwalid@reverielabs.com\nElana Simon11footnotemark:1\nReverie Labs\nelana@reverielabs.com\nSeyone Chithrananda\nUC Berkeley\nseyonec@berkeley.edu\nGabriel Grand\nReverie Labs &amp; MIT CSAIL\ngg@mit.edu\nBharath Ramsundar\nDeep Forest Sciences\nbharath@deepforestsci.com\nEqual contribution\n###### Abstract\nLarge pretrained models such as GPT-3 have had tremendous impact on modern natural language processing by leveraging self-supervised learning to learn salient representations that can be used to readily finetune on a wide variety of downstream tasks> [\n[> 1\n](#bib.bib1)> ]\n. We investigate the possibility of transferring such advances tomolecularmachine learning by building a chemical foundation model, ChemBERTa-2, using the \u201clanguage\" of SMILES. While labeled data for molecular prediction tasks is typically scarce, libraries of SMILES strings are readily available.\nIn this work, we build upon ChemBERTa> [\n[> 2\n](#bib.bib2)> ]\nby optimizing the pretraining process. We compare multi-task and self-supervised pretraining by varying hyperparameters and pretraining dataset size, up to 77M compounds from PubChem. To our knowledge, the 77M set constitutes one of the largest datasets used for molecular pretraining to date. We find that with these pretraining improvements, we are competitive with existing state-of-the-art architectures on the MoleculeNet> [\n[> 3\n](#bib.bib3)> ]\nbenchmark suite. We analyze the degree to which improvements in pretraining translate to improvement on downstream tasks.\n## 1Motivation\nOver the past few years, transformers> [\n[> 4\n](#bib.bib4)> , [> 5\n](#bib.bib5)> ]\nhave emerged as popular architectures for learning self-supervised representations of molecules from text representations. ChemBERTa> [\n[> 2\n](#bib.bib2)> ]\nintroduced a BERT-like transformer model that learns molecular fingerprints through semi-supervised pretraining and pretrained it on a dataset of 10M compounds. MolBERT> [\n[> 6\n](#bib.bib6)> ]\nexperiments with a number of different pretraining objectives on a dataset of 1.6M compounds. SMILES-BERT> [\n[> 7\n](#bib.bib7)> ]\npretrains on 18.7M compounds from Zinc.\nChemBERTa-2 is a BERT-like transformer model> [\n[> 8\n](#bib.bib8)> ]\nthat learns molecular fingerprints through semi-supervised pretraining of the language model. ChemBERTa-2 employs masked-language modelling (MLM) and multi-task regression (MTR) over a large corpus of 77 million SMILES strings, a well-known text representation of molecules. SMILES, is its own language, with a simple vocabulary, consisting of a series of characters representing atom and bond symbols, and very few grammar rules.> [\n[> 9\n](#bib.bib9)> ]\n. ChemBERTa-2 explores the scaling hypothesis that pretraining effectively on larger datasets can yield improved performance, using the largest training dataset in molecular representation learning.\n## 2Related Work\nWhile this paper and its preceding works explore transformer-based pretraining for molecular models, a parallel line of work has explored the used of graph-based pretraining methods. SNAP> [\n[> 10\n](#bib.bib10)> ]\nintroduces graph pretraining methods based on node attribute masking and structural similarity. Grover> [\n[> 11\n](#bib.bib11)> ]\nscales graph-transformer pretraining to a 100 million parameter model pretrained on 10M compounds. MolGNet> [\n[> 12\n](#bib.bib12)> ]\nuses a message passing architecture to pretrain a 53 million parameter model on 11M compounds.\nA number of recent works have explored alternative pretraining methodologies including contrastive learning> [\n[> 13\n](#bib.bib13)> ]\n. Other work attempts to combine molecular graph and transformer based pretraining methodologies into a unified \"dual\" framework> [\n[> 14\n](#bib.bib14)> ]\n, or considers techniques inspired by neural machine translation by learning to translate between SMILES and InChi representations of a molecule> [\n[> 15\n](#bib.bib15)> ]\n. Very recent work investigates whether large language models such as GPT-3, trained on non-chemical corpuses have learned meaningful chemistry> [\n[> 16\n](#bib.bib16)> ]\n.\n## 3Methods\n![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.01712/assets/figures/pretrain_task_diagram.png)(a)MLM vs. MTR\n![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.01712/assets/figures/transfer_learning_diagram.png)(b)Training pipeline\nFigure 1:a) An illustration of masked language modeling (MLM) and multitask regression (MTR) pretraining tasks. b) The training pipeline implemented to achieve results in this paper.\nChemBERTa-2 is based on the RoBERTa> [\n[> 8\n](#bib.bib8)> ]\ntransformer implementation in HuggingFace> [\n[> 17\n](#bib.bib17)> ]\n. We use the same training dataset of 77M unique SMILES from ChemBERTa> [\n[> 2\n](#bib.bib2)> ]\n. We canonicalize and globally shuffle the SMILES to facilitate large-scale pretraining. For validation, we first set aside a fixed set of 100k compounds. We divide the remaining dataset by sampling subsets of 5M, 10M and 77M (the full set), constituting three datasets to be used across both pretraining tasks.\n### 3.1Pretraining Strategies and Setup\nMasked Language Modeling:We adopt the masked language modeling (MLM) pretraining procedure from RoBERTa, which masks 15% of the tokens in each input string and trains the model to correctly identify them. We use a maximum vocab size of 591 tokens based on a dictionary of common SMILES characters and a maximum sequence length of 512 tokens.\nMulti-task Regression:We compute a set of 200 molecular properties for each compound in our training dataset. These properties do not require any experimental measurements and can each be calculated from SMILES alone using RDKit> [\n[> 18\n](#bib.bib18)> ]\n. We then train a multitask regression (MTR) architecture to predict these properties simultaneously. Because these tasks have very different scales and ranges, we mean-normalize the labels for each task prior to training.\nPretraining Setup:Models are trained on AWS EC2 instances equipped with Nvidia T4 GPUs. We set early stopping patience to equal one pass through the dataset, to ensure that for any dataset size, the model has an opportunity to see each compound at least once.\n### 3.2Hyperparameter Search\nMost language modeling architectures have hyperparameters that are tuned on datasets comprised of written and spoken language, such as English. SMILES on the other hand, have a very different grammatical structure. To ensure an adequate assessment of ChemBERTa-v2 performance, we conduct a thorough hyperparameter search (subject to compute constraints).\nWe select 50 random hyperparameter configurations, varying the hidden size, number of attention heads, dropout, intermediate size, number of hidden layers, and the learning rate. Models have between 5M and 46M parameters. Each configuration is trained on each of the MLM and MTR pretraining tasks, with the 5M dataset. Using the smallest dataset size ensures that we can train until convergence (as dictated by early stopping). From pretraining results, we select five configurations, with varying validation loss values, to train on the 10M and 77M sets. Five configurations are selected for MLM and MTR, independently from one another, with the objective of evaluating how pretraining loss can affect downstream performance.\n### 3.3Finetuning on MoleculeNet\nWe evaluate our models on several regression and classification tasks from MoleculeNet> [\n[> 3\n](#bib.bib3)> ]\nselected to cover a range of dataset sizes (1.5K - 8.0K examples) and medicinal chemistry applications (brain penetrability, toxicity, solubility, and on-target inhibition). These included the BACE, Clearance, Delaney, Lipophilicity, BBBP, ClinTox, HIV, Delaney, and Tox21 datasets. For datasets with multiple tasks, we selected a single representative task: the clinical toxicity (CT\\_TOX) task fro...",
      "url": "https://ar5iv.labs.arxiv.org/html/2209.01712"
    },
    {
      "title": "",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/pdf/2404.14986"
    }
  ]
}