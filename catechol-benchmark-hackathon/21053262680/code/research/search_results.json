{
  "query": "How do you reduce the gap between cross-validation and leaderboard score when predicting for completely unseen chemical compounds in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent studies of out\u2011of\u2011distribution validation for molecular data and on best\u2011practice recommendations for reliable model evaluation, you can narrow the CV\u2011to\u2011leaderboard gap for completely unseen compounds by using a forward\u2011looking, scaffold\u2011aware validation scheme and by explicitly measuring applicability\u2011domain metrics\u202f([Step Forward Cross Validation for Bioactivity Prediction](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006); [Benchmarking ML in ADMET predictions](https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724)).\n\n**Key steps**\n\n1. **Split the data by chemical scaffold (or by time\u2011ordered \u201cn\u2011step forward\u201d folds).**  \n   Replace the usual random k\u2011fold CV with a *k\u2011fold n\u2011step forward* (or scaffold\u2011split) CV, which keeps structurally similar molecules together in the same fold and forces the model to predict truly novel chemotypes. This has been shown to better reflect real\u2011world performance on unseen compounds\u202f([Step Forward Cross Validation](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\n2. **Run multiple forward folds and average the scores.**  \n   Perform the forward split repeatedly (e.g., 5\u2011fold) so each fold serves once as the \u201cfuture\u201d test set. Compute the mean and standard deviation of the metric (MAE, R\u00b2, etc.) across folds to obtain a stable estimate\u202f([Step Forward Cross Validation](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\n3. **Add applicability\u2011domain metrics.**  \n   Alongside the primary error metric, calculate *discovery yield* (fraction of predicted actives that are truly active) and *novelty error* (how far predictions lie from the training chemical space). These metrics highlight when the model is extrapolating beyond its reliable domain\u202f([Step Forward Cross Validation](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\n4. **Validate the CV results statistically.**  \n   Use hypothesis\u2011testing (e.g., paired t\u2011test or bootstrap) to confirm that performance differences between CV folds are significant and not due to random variation. Combining CV with statistical testing improves confidence that the CV estimate will translate to the leaderboard\u202f([Benchmarking ML in ADMET predictions](https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724)).\n\n5. **Iterate feature engineering with domain\u2011specific representations.**  \n   Experiment with richer molecular descriptors (graph neural networks, physicochemical fingerprints, multi\u2011modal data) and re\u2011evaluate using the forward CV pipeline; better representations reduce over\u2011optimistic CV scores and align them with true test\u2011set performance\u202f([Benchmarking ML in ADMET predictions](https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724)).\n\n6. **Ensemble the top\u2011performing models evaluated under forward CV.**  \n   Combine several models that each performed well in the forward\u2011split validation; ensembles tend to be more robust to distribution shift and often close the CV\u2011leaderboard gap\u202f([Benchmarking ML in ADMET predictions](https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724)).\n\n7. **Monitor the public\u2011leaderboard drift and stop over\u2011submitting.**  \n   Because repeated submissions can cause leaderboard over\u2011fitting, limit the number of submissions and use the public score only as a sanity check; rely on the forward\u2011CV estimate for final model selection\u202f([The Ladder: A Reliable Leaderboard](https://proceedings.mlr.press/v37/blum15.pdf)).\n\nFollowing this forward\u2011split, statistically\u2011grounded validation workflow will give you a CV estimate that more closely matches the final Kaggle leaderboard when the test set contains completely new chemical compounds.",
      "url": ""
    },
    {
      "title": "Benchmarking ML in ADMET predictions: the practical impact ... - NIH",
      "text": "Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1186/s13321-025-01041-0)\n* [](pdf/13321_2025_Article_1041.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Journal of Cheminformatics logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jcheminfo.png)\nJ Cheminform\n. 2025 Jul 21;17:108. doi:[10.1186/s13321-025-01041-0](https://doi.org/10.1186/s13321-025-01041-0)\n# Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models\n[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n### Gintautas Kamuntavi\u010dius\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n1,\u2709,#,[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n### Tanya Paquet\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n1,#,[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n### Orestis Bastas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n1,#,[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n### Dainius \u0160alkauskas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n1,[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n### Alvaro Prat\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n1,[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n### Hisham Abdel Aty\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n1,[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n### Aurimas Pabrinkis\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n1,[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n### Povilas Norvai\u0161as\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n1,[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n### Roy Tal\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n1\n* Author information\n* Article notes\n* Copyright and License information\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\n\u2709Corresponding author.\n#\nContributed equally.\nReceived 2025 Jan 27; Accepted 2025 Jun 1; Collection date 2025.\n\u00a9The Author(s) 2025\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC12281724\u00a0\u00a0PMID:[40691635](https://pubmed.ncbi.nlm.nih.gov/40691635/)\n## Abstract\nThis study, focusing on predicting Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) properties, addresses the key challenges of ML models trained using ligand-based representations. We propose a structured approach to data feature selection, taking a step beyond the conventional practice of combining different representations without systematic reasoning. Additionally, we enhance model evaluation methods by integrating cross-validation with statistical hypothesis testing, adding a layer of reliability to the model assessments. Our final evaluations include a practical scenario, where models trained on one source of data are evaluated on a different one. This approach aims to bolster the reliability of ADMET predictions, providing more dependable and informative model evaluations.\n**Scientific contribution**\nThis study provided a structured approach to feature selection. We improve model evaluation by combining cross-validation with statistical hypothesis testing, making results more reliable. The methodology used in our study can be generalized beyond feature selection, boosting the confidence in selected models which is crucial in a noisy domain such as the ADMET prediction tasks. Additionally, we assess how well models trained on one dataset perform on another, offering practical insights for using external data in drug discovery.\n## Introduction\nThe Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) of compounds are commonly estimated throughout drug discovery projects, as the feasibility of a compound to become a viable drug highly depends on it. Through the years, a lot of work has gone into building and evaluating machine learning (ML) systems designed to predict molecular properties that are associated with ADMET. Public curated datasets and benchmarks for ADMET associated properties are becoming increasingly available to the community, creating the opportunity for more widespread exploration of ML algorithms and techniques in this space. The Therapeutics Data Commons (TDC) ADMET leaderboard showcases this [[1](#CR1)], highlighting a wide variety of models, features, and processing methods investigated...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "[PDF] BENCHMARKING BAND GAP PREDICTION FOR ...",
      "text": "Accepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nBENCHMARKING BAND GAP PREDICTION FOR\nSEMICONDUCTOR MATERIALS USING\nMULTIMODAL AND MULTI-FIDELITY DATA\nHaolin Wang1,2, Xianyuan Liu1,2, Anna Jungbluth3, Alex Ramadan4, Robert Oliver5,\nHaiping Lu1,2\n1 Centre for Machine Intelligence, University of Sheffield\n2 School of Computer Science, University of Sheffield\n3 Climate Office, European Space Agency\n4 School of Mathematical and Physical Sciences, University of Sheffield\n5 School of Chemical, Materials and Biological Engineering, University of Sheffield\n{haolin.wang, h.lu}@shef.ac.uk\nABSTRACT\nThe band gap is critical for understanding the electronic properties of materi\u0002als in semiconductor applications. While density functional theory is commonly\nused to estimate band gaps, it often underestimates values and remains compu\u0002tationally expensive, limiting its practical usefulness. Machine learning (ML)\nhas become a promising alternative for accurate and efficient band gap predic\u0002tions. However, existing datasets are limited in data modality, fidelity and sam\u0002ple size, and performance evaluation studies often lack direct comparisons be\u0002tween traditional and advanced ML models. Therefore, a more comprehensive\nevaluation is needed to make progress towards real-world impacts. In this pa\u0002per, we developed a benchmarking framework for ML-based band gap prediction\nto address this gap. We compiled a new multimodal, multi-fidelity dataset from\nthe Materials Project and BandgapDatabase1, consisting of 60,218 low-fidelity\ncomputational band gaps and 1,183 high-fidelity experimental band gaps across\n10 material categories. We evaluated seven ML models, from traditional meth\u0002ods to graph neural networks, assessing their ability to learn from atomic prop\u0002erties and structural information. To promote real-world applicability, we em\u0002ployed three metrics: mean absolute error, mean relative absolute error, and co\u0002efficient of determination R2\n. Moreover, we introduced a leave-one-material\u0002out evaluation strategy to better reflect real-world scenarios where new mate\u0002rials have little to no prior training data. Our findings offer valuable insights\ninto model selection and evaluation for band gap prediction across material cate\u0002gories, providing guidance for real-world applications in materials discovery and\nsemiconductor design. The data and code used in this work are available at:\nhttps://github.com/Shef-AIRE/bandgap-benchmark.\n1 INTRODUCTION\nThe band gap, defined as the energy difference between the valence and conduction bands, is a\nfundamental property of periodic solids and plays a critical role in determining their electrical con\u0002ductivity. This property is widely utilized in semiconductor applications (Yoder, 1996), including\nlight-emitting diodes (LEDs) (Lisensky et al., 1992), transistors (Ueno et al., 2004), and photovoltaic\ndevices (Goetzberger & Hebling, 2000). However, accurately determining the band gap of a mate\u0002rial remains a significant challenge. Theoretical methods, such as density functional theory (DFT),\nare commonly used but often underestimate band gaps due to limitations in exchange-correlation\nfunctionals. More advanced methods, such as the G0W0 approximation and hybrid functionals\n(Heyd et al., 2003), provide improved accuracy but are computationally intensive and require metic\u0002ulous parameter tuning. Recently, machine learning (ML) has emerged as a promising alternative\n1\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nML Pipeline\nCGCNN\nLEFTNet-Z\nLEFTNet-Prop\nNeural Networks\nCartNet\nTraditional ML Methods\nLinear Regression\nRandom Forest Regression\nSupport Vector Regression\nLow Fidelity (DFT) High Fidelity (Expt.)\nDataset\nMulti-Fidelity\nMultimodality Atomic Properties Crystal Structure\nSplitting K-fold Cross-Validation Leave-One-Material-Out\nEvaluation MAE MRAE\nFigure 1: Flowchart of our proposed benchmark. The benchmark categorizes data based on fi\u0002delity and modality, incorporating low-fidelity (DFT) and high-fidelity (experimental) band gaps,\nalong with multimodal features. Beyond traditional K-fold cross-validation, a leave-one-material\u0002out strategy is introduced to better reflect real-world scenarios. The machine learning (ML) pipeline\nstudies both traditional ML methods and more recent neural networks. For evaluation, mean relative\nabsolute error (MRAE) is introduced to enhance applicability, alongside mean absolute error (MAE)\nand the coefficient of determination R2.\nfor predicting band gaps. Unlike conventional theoretical methods, ML methods can capture com\u0002plex structure-property relationships from large datasets, enabling accurate and efficient predictions\nwithout expensive calculations.\nMachine learning predicts band gaps primarily using two complementary types of information:\natomic properties and crystal structure. These modalities represent the material from different per\u0002spectives, forming a multimodal data representation (Liu et al., 2025). Atomic properties capture\nintrinsic characteristics of individual atoms that influence electronic behavior and have been widely\nused in band gap modeling (Talapatra et al., 2023). For instance, Sabagh Moeini et al. (2024) used\neight atomic features to train linear models and identified the standard deviation of valence electrons\nas a key predictor for band gaps in perovskites.\nAdvanced graph representation learning models (Schutt et al., 2017; Choudhary & DeCost, 2021) \u00a8\nextract crystal structure information via graph neural networks (GNNs) and capture atomic inter\u0002actions by analyzing distance and orientation. These models better utilize the underlying physics\nof crystal structures via the three-dimensional arrangement of atoms, making them an intuitive and\nsuitable approach for accurate property prediction. Additionally, structural information comple\u0002ments atomic properties by providing a global context that connects the local information carried by\nindividual atoms. The Crystal Graph Convolutional Neural Network (CGCNN) (Xie & Grossman,\n2018) is one of the most widely used models for structure-based materials property prediction. It in\u0002corporates nine atomic properties along with interatomic distance information. Subsequent models,\nsuch as CartNet (Sole et al., 2025), extend this idea by explicitly encoding the full 3D structure of \u00b4\nmaterials. Another approach, LEFTNet (Du et al., 2023), further improves predictive performance\nby capturing higher-order geometric features, including bond angles and local orientations. How\u0002ever, none of these methods have been evaluated against traditional machine learning models within\na unified benchmark.\nSeveral benchmark studies have compared machine learning models for predicting various material\nproperties. For example, MatBench (Dunn et al., 2020) provides a leaderboard for structure-based\nproperty predictions in inorganic materials, covering 13 supervised learning tasks (including band\ngap prediction) and incorporating both DFT and experimental data. Similar ML benchmarking\nefforts for band gap prediction include MatDeepLearn (Fung et al., 2021), Varivoda et al. (2023),\nand the JARVIS-Leaderboard (Choudhary et al., 2024). These benchmarks primarily rely on band\n2\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\ngap databases, the Materials Project (Jain et al., 2013) and QMOF (Rosen et al., 2021; 2022), that\nprovide DFT-calculated band gaps only. On the other hand, experimental datasets, such as the\none from Zhuo et al. (2018), contain only compositional information, making them unsuitable for\nstructure-based approaches.\nMasood et al. (2023) introduced a multi-fidelity open-access dataset that includes 3D structures,\ncomputational band gaps, and experimental band gaps, offering a more suitable resource for\nstructure-based band gap prediction. However, the evaluation dataset is relatively small, containing\nonly 30 materials, and lacks representation of key material categories suc...",
      "url": "https://openreview.net/pdf?id=u8FripvaG5"
    },
    {
      "title": "Cross-Validation - Iterate.ai",
      "text": "## What is it?\n\nCross-validation is a technique used in the field of artificial intelligence and machine learning to assess how well a model performs on a given dataset. It involves dividing the dataset into multiple subsets, training the model on a portion of the data, and then testing it on the remaining subset. This process is repeated several times to ensure that the model is able to generalize well to new, unseen data.\n\nFor business people, cross-validation is relevant because it helps in ensuring the accuracy and reliability of predictive models used in various business applications. By using cross-validation, business executives can have confidence that the models they are using to make critical decisions, such as forecasting sales or predicting customer behavior, are robust and not overfit to the training data.\n\nThis can lead to more accurate insights and ultimately better business outcomes. Additionally, cross-validation can help in identifying and addressing potential issues with the model, leading to improvements in performance and overall efficiency. Overall, understanding and utilizing cross-validation can contribute to more effective and informed decision-making within the business context.\n\n## How does it work?\n\nCross-validation is a technique used in artificial intelligence to evaluate the performance of a machine learning model. Think of it like when you hire a new employee and you want to see how well they perform before giving them a permanent job.\n\nIn the case of cross-validation, the machine learning model is tested multiple times with different sets of data to make sure it can consistently make accurate predictions. This helps to ensure that the model isn\u2019t just good at predicting one specific set of data, but rather is generally good at making predictions in different situations.\n\nFor example, let\u2019s say you have a sales forecasting model for your business. You can use cross-validation to test the model\u2019s accuracy with different sales data from different time periods. This way, you can be more confident in the model\u2019s ability to predict future sales, even when the market conditions change.\n\nOverall, cross-validation is an important tool in ensuring that machine learning models are reliable and can be trusted to make accurate predictions in real-world business scenarios.\n\n### Pros\n\n1. Helps to reduce overfitting: Cross-validation helps to assess how the results of a model will generalize to an independent dataset, thereby reducing the risk of overfitting.\n2. Better use of data: By splitting the data into multiple subsets, cross-validation allows for better utilization of available data for both training and testing the model.\n3. Reliable performance estimation: Cross-validation provides a more reliable estimate of the model\u2019s performance compared to a single train-test split.\n\n### Cons\n\n1. Computational overhead: Cross-validation requires multiple iterations of model training and testing, which can be computationally expensive, especially for large datasets and complex models.\n2. Sensitivity to data splitting: The results of cross-validation can be sensitive to how the data is partitioned, and different splits may lead to different performance estimates.\n3. Not suitable for all types of data: Cross-validation may not be suitable for certain types of data, such as time-series data, where the sequence of data points is important and cannot be randomly split.\n\nStay Informed on How your Industry is Using AI\n\nDig deep into the latest AI trends and see how they can turbocharge your\u00a0future.\n\n[Learn more](https://www.aiexplored.ai/)\n\n## Applications and Examples\n\nCross-validation is a technique used in machine learning to assess the performance of a model. For example, in the context of building a spam filter for emails, cross-validation would be used to test the model\u2019s ability to accurately classify spam and non-spam emails by splitting the dataset into multiple subsets and testing the model on each subset to ensure that it generalizes well to new data.\n\nBy using cross-validation, the spam filter can be confidently deployed in real-world scenarios knowing that it has been rigorously tested for accuracy.\n\n## History and Evolution\n\nCross-validation is a term that originated in the field of statistics and machine learning, first discussed by researchers in the 1970s as a method for assessing the performance of predictive models.\n\nIt involves partitioning data into subsets, training the model on some of the subsets, and then testing it on the remaining data to evaluate its predictive accuracy. Today, cross-validation is essential for AI as it helps to prevent overfitting and ensure that machine learning models can generalize well to new, unseen data, leading to more robust and reliable AI systems.\n\n\u200d\n\n## FAQs\n\nWhat is cross-validation in AI?\n\nCross-validation is a technique used to evaluate machine learning models by training and testing on multiple subsets of the available data to avoid overfitting.\n\nWhy is cross-validation important in machine learning?\n\nCross-validation is important because it provides a more accurate estimate of the model's performance, helps in identifying overfitting, and ensures that the model generalizes well to new data.\n\nWhat are the different types of cross-validation methods?\n\nThe main types of cross-validation methods include k-fold cross-validation, leave-one-out cross-validation, and stratified cross-validation, each with its own pros and cons.\n\nHow do you implement cross-validation in practice?\n\nTo implement cross-validation, split the dataset into training and testing sets, choose a cross-validation method, and iterate through each fold to train and test the model, evaluating its performance.\n\nCan cross-validation be used for all types of machine learning models?\n\nYes, cross-validation can be used for various types of machine learning models, including regression, classification, and clustering, as a way to assess their performance.\n\n## Takeaways\n\nCross-validation is an important method used in business to assess the accuracy and generalizability of a predictive model. It involves splitting a data set into multiple subsets, training the model on some of the subsets, and testing it on the remaining subsets to measure its performance. The main goal of cross-validation is to ensure that a model can make accurate predictions on new, unseen data.\n\nUnderstanding cross-validation is crucial for businesses as it helps in selecting the best predictive model for making important business decisions. It also aids in avoiding overfitting, which occurs when a model performs well on the training data but poorly on new data. By utilizing cross-validation, businesses can enhance the reliability and robustness of their predictive models, ultimately leading to better decision-making and improved outcomes.\n\nTherefore, business people need to grasp the concept of cross-validation to make informed choices about the predictive models they use and to ensure the accuracy and effectiveness of their data-driven strategies.\n\n\u200d",
      "url": "https://www.iterate.ai/ai-glossary/cross-validation-explained"
    },
    {
      "title": "Cross-Validation in Data Science - Medium",
      "text": "Cross-Validation in Data Science. In the world of data science, model\u2026 | by Brandon Wohlwend | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@brandon93.w/cross-validation-in-data-science-c87974f8f7d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@brandon93.w/cross-validation-in-data-science-c87974f8f7d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Cross-Validation in Data Science\n[\n![Brandon Wohlwend](https://miro.medium.com/v2/resize:fill:64:64/1*5aXj5U2R2t2tFf2uGMFIJA.jpeg)\n](https://medium.com/@brandon93.w?source=post_page---byline--c87974f8f7d---------------------------------------)\n[Brandon Wohlwend](https://medium.com/@brandon93.w?source=post_page---byline--c87974f8f7d---------------------------------------)\n16 min read\n\u00b7Jul 19, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/c87974f8f7d&amp;operation=register&amp;redirect=https://medium.com/@brandon93.w/cross-validation-in-data-science-c87974f8f7d&amp;user=Brandon+Wohlwend&amp;userId=bfd61666d8cd&amp;source=---header_actions--c87974f8f7d---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/c87974f8f7d&amp;operation=register&amp;redirect=https://medium.com/@brandon93.w/cross-validation-in-data-science-c87974f8f7d&amp;source=---header_actions--c87974f8f7d---------------------bookmark_footer------------------)\nListen\nShare\nIn the world of data science, model evaluation serves as the cornerstone for ensuring the reliability and effectiveness of predictive models. It is the stage where we assess the performance of our models, using a variety of statistical methods, to gauge how well they can predict or classify unseen data. Model evaluation is the critical phase that separates a theoretical data experiment from a practical, deployable model. It offers us invaluable insights, allowing us to discern whether our model will be a success in real-world scenarios or if it needs further fine-tuning.\nBut how do we ensure that our model\u2019s evaluation is robust and reliable? This is where the concept of cross-validation comes into play. Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. Its primary function is to gauge how well a model will perform on an independent dataset, thereby providing us with a reliable estimation of its performance metrics.\nBy using cross-validation, we\u2019re not just making sure that our model works well with our current data, but we\u2019re also ensuring that it can generalize to new data. This helps to prevent common pitfalls in machine learning such as overfitting, where the model is too complex and performs well on the training data but poorly on new, unseen data.\nIn this article, we will delve deeper into the importance, methods, and application of cross-validation in data science, shedding light on why it\u2019s such a fundamental aspect of building robust and reliable predictive models.\n## The Importance of Cross-Validation\nIn the field of machine learning and data science, the performance of a model is determined not only by its ability to capture the patterns and relationships in the training data but also by its capacity to generalize well to unseen data. This is where two notorious problems come into play: overfitting and underfitting.\n### Explanation of the Risks of Overfitting and Underfitting\nOverfitting occurs when our model performs well on the training data but poorly on new, unseen data. In essence, the model becomes so intricately tailored to the training data that it fails to capture the broader patterns, instead of getting lost in the noise and outliers. Imagine learning word for word from a textbook to pass an exam, only to fail when asked to apply this knowledge to new problems. This is analogous to overfitting in machine learning.\nOn the other end of the spectrum, underfitting happens when the model is too simple to capture the underlying structure of the data. It neither performs well on the training data nor generalizes effectively to new data. If we stick with the exam analogy, underfitting is like trying to pass an advanced physics exam after only studying basic principles. The study material (model) is too simplistic to capture the complexity of the exam questions (data).\n### The Role of Cross-Validation in Mitigating These Risks\nCross-validation plays a crucial role in mitigating the risks of both overfitting and underfitting. The beauty of this technique lies in its simple yet effective process. By partitioning the original sample into a training set to train the model, and a validation set to evaluate it, we can assess how the results of the model will generalize to an independent data set.\nCross-validation provides a more robust measure of a model\u2019s predictive performance by ensuring that it is tested on unseen data. In doing so, it helps us find the balance between a model that is too complex (leading to overfitting) and a model that is too simple (leading to underfitting).\nMoreover, cross-validation allows us to tune hyperparameters with only the original training set. This helps us to more reliably estimate the effectiveness of the model and avoid \u201cleaking\u201d information from our validation set into our model training process, a common pitfall when incorrectly applying cross-validation.\nThus, by using cross-validation in the model training process, we increase our chances of developing a model that not only fits the training data well but also accurately generalizes to new, unseen data.\n## Understanding Cross-Validation\nCross-validation is a powerful technique in the field of machine learning and statistics, aimed at assessing how well a predictive model will perform on unseen data. Before we delve deeper into the types of cross-validation and its application, let\u2019s take a moment to understand what cross-validation really is.\n### Definition and Explanation of Cross-Validation\nAt its core, cross-validation is a resampling procedure used to evaluate a model if we have a limited amount of data. To perform cross-validation, we divide our data into a set of \u2018folds\u2019, usually of roughly equal size. The exact number of folds depends on the specific type of cross-validation used and the size of your dataset.\nIn each round of cross-validation, one of the folds is held out as a validation (or \u2018test\u2019) set, and the remaining folds are used as a training set. The model is trained on the training set and then its predictive performance is evaluated on the validation set. This process is repeated for each fold, ensuring that each one serves as the validation set exactly once.\nThe results from each fold\u2019s validation performance are then averaged to provide an overall measure of the model\u2019s predictive capability. The key advantage of cross-validation is that every observation in our dataset gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This helps in providing a more accurate and robust measure of model pe...",
      "url": "https://medium.com/@brandon93.w/cross-validation-in-data-science-c87974f8f7d"
    },
    {
      "title": "Validation strategies for target prediction methods - Oxford Academic",
      "text": "<div><div>\n<div>\n <div>\n \n \n <p>Journal Article\n </p>\n <div>\n \n <div>\n <p>\n Neann Mathai, \n \n<div>\n \n <div>\n <div><p>Department of Chemistry</p><p>, University of Bergen, Bergen, </p><p>Norway</p></div><div><p>Computational Biology Unit (CBU)</p><p>, University of Bergen, Bergen, </p><p>Norway</p></div><div><p>Center for Bioinformatics (ZBH)</p><p>, Department of Computer Science, Faculty of Mathematics, Informatics and Natural Sciences, Universit\u00e4t Hamburg, Hamburg, </p><p>Germany</p></div>\n </div>\n <p>\n Search for other works by this author on:\n </p>\n \n \n</div> \n \n \n Ya Chen, \n \n<div>\n \n <div><p>Center for Bioinformatics (ZBH)</p><p>, Department of Computer Science, Faculty of Mathematics, Informatics and Natural Sciences, Universit\u00e4t Hamburg, Hamburg, </p><p>Germany</p></div>\n <p>\n Search for other works by this author on:\n </p>\n \n \n</div> \n \n \n Johannes Kirchmair<i></i>\n \n<div>\n \n <div>\n <div><p>Department of Chemistry</p><p>, University of Bergen, Bergen, </p><p>Norway</p></div><div><p>Computational Biology Unit (CBU)</p><p>, University of Bergen, Bergen, </p><p>Norway</p></div><div><p>Center for Bioinformatics (ZBH)</p><p>, Department of Computer Science, Faculty of Mathematics, Informatics and Natural Sciences, Universit\u00e4t Hamburg, Hamburg, </p><p>Germany</p></div>\n </div>\n <div>\n <p>Corresponding author: Johannes Kirchmair, Department of Chemistry and Computational Biology Unit (CBU), University of Bergen, N-5020 Bergen, Norway and Center for Bioinformatics (ZBH), Department of Computer Science, Faculty of Mathematics, Informatics and Natural Sciences, Universit\u00e4t Hamburg, Hamburg, 20146, Germany. Tel.: +47-55-58-34-64; E-mail: <a href=\"mailto:johannes.kirchmair@uib.no\">johannes.kirchmair@uib.no</a></p>\n </div>\n \n <p>\n Search for other works by this author on:\n </p>\n \n \n</div> \n \n </p>\n </div>\n<div>\n <div>\n <p>Received:</p>\n <p>26 November 2018</p>\n </div>\n <div>\n <p>Revision received:</p>\n <p>14 January 2019</p>\n </div>\n <div>\n <p>Accepted:</p>\n <p>17 February 2019</p>\n </div>\n \n </div>\n </div>\n</div>\n \n <div>\n <div>\n <ul>\n <li>\n <a href=\"/bib/article-pdf/21/3/791/33398993/bbz026.pdf\">\n PDF\n </a>\n </li>\n \n <li>\n \n <i></i>\n \n \n <ul>\n \n \n <li>\n Figures &amp; tables\n </li>\n \n \n \n </ul>\n </li>\n <li>\n <div>\n <p><a href=\"#\">\n <i></i>\n Cite\n</a></p><div>\n <h3>Cite</h3>\n <p>Neann Mathai, Ya Chen, Johannes Kirchmair, Validation strategies for target prediction methods, <em>Briefings in Bioinformatics</em>, Volume 21, Issue 3, May 2020, Pages 791\u2013802, <a href=\"https://doi.org/10.1093/bib/bbz026\">https://doi.org/10.1093/bib/bbz026</a></p>\n \n <p><i></i>Close\n</p></div>\n \n </div>\n </li>\n <li>\n \n </li>\n <li>\n \n \n \n \n <ul>\n \n <li><a href=\"#\">Email</a></li>\n <li><a href=\"#\">Twitter</a></li>\n <li><a href=\"#\">Facebook</a></li>\n <li><a href=\"#\">More</a></li>\n </ul>\n </li>\n </ul>\n \n </div>\n <div>\n <h2>Abstract</h2>\n<section><p>Computational methods for target prediction, based on molecular similarity and network-based approaches, machine learning, docking and others, have evolved as valuable and powerful tools to aid the challenging task of mode of action identification for bioactive small molecules such as drugs and drug-like compounds. Critical to discerning the scope and limitations of a target prediction method is understanding how its performance was evaluated and reported. Ideally, large-scale prospective experiments are conducted to validate the performance of a model; however, this expensive and time-consuming endeavor is often not feasible. Therefore, to estimate the predictive power of a method, statistical validation based on retrospective knowledge is commonly used. There are multiple statistical validation techniques that vary in rigor. In this review we discuss the validation strategies employed, highlighting the usefulness and constraints of the validation schemes and metrics that are employed to measure and describe performance. We address the limitations of measuring only generalized performance, given that the underlying bioactivity and structural data are biased towards certain small-molecule scaffolds and target families, and suggest additional aspects of performance to consider in order to produce more detailed and realistic estimates of predictive power. Finally, we describe the validation strategies that were employed by some of the most thoroughly validated and accessible target prediction methods.</p></section> \n <h2>Introduction</h2>\n<p>Fueled by the growing amount of chemical and biological data, the availability of powerful phenotypic screening technologies [1], and a shift in small-molecule drug discovery from the \u2018one drug one target\u2019 paradigm to \u2018polypharmacology\u2019 [2\u20135], <em>in silico</em> methods for the prediction of the biomacromolecular targets of small molecules have become one of the most intensely researched areas of cheminformatics in recent years. These methods are useful not only for the discovery of new medicines but also in the repositioning of existing approved drugs [6\u20139].</p><p>Target prediction methods are typically pair-input problems, in that they classify a query compound and a biomacromolecule pair as an interacting (positive) or a non-interacting (negative) pair. One categorization of target prediction methods, based on the types of data used, classifies methods into three overarching approaches: ligand-based, structure-based and chemogenomic approaches [10, 11]. Ligand-based approaches make predictions based on the similarity principle, which states that similar ligands (in the context of this review, small molecules) are likely to have similar targets. These methods typically make use of a variety of molecular descriptors to quantify and compare the physicochemical properties of small molecules. They do not rely on structural information on biomacromolecules. Their applicability domain is limited primarily by the available chemical and biological data. Structure-based approaches, such as ligand docking, use structural data on biomacromolecules as the main source of information to make predictions. They are generally more computationally expensive than ligand-based methods, and their primary limitations are defined by the availability of relevant target structures and accuracy of scoring functions. Chemogenomics approaches (or proteochemometric approaches) are defined here as methods that combine information from both ligands and targets to make their predictions [10\u201312].</p><p>There are several publications discussing techniques that can be used in validating target prediction models [13\u201320]. However, among the many recently published reviews on <em>in silico</em> target prediction, only few include a discussion of validation strategies [6, 10, 11, 21\u201326]. With this review we aim to provide a comprehensive reference of strategies for the validation of target prediction models. The review begins with a discussion of data partitioning schemes that are used to train and test models to measure their performance, highlighting their appropriateness and limitations. This is followed by an analysis of the metrics that are used to measure this performance and of established benchmark data sets. Building up on these components, we point out strategies to obtain more realistic estimates of the performance of target prediction models that account for the biases present in the underlying reference data. Finally, we describe the validation strategies that were employed by some of the most thoroughly validated and accessible target prediction methods.</p> <h2>Strategies for validating target prediction methods</h2>\n<p>Validation primarily serves two purposes: the selection of an optimal model and the evaluation of its generalized predictive performance [13, 14]. Model selection is commonly a result of an iterative model building process, during which models based on various algorithms and parameters are built on a training set and validated on a testing set. This validation procedure is generally referred to as internal ...",
      "url": "https://academic.oup.com/bib/article/21/3/791/5428023"
    },
    {
      "title": "Cross Validation. Cross-validation is a technique for\u2026 | by Omkar Hankare",
      "text": "Cross Validation. Cross-validation is a technique for\u2026 | by Omkar Hankare | Medium\n[Sitemap](https://ompramod.medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://ompramod.medium.com/cross-validation-623620ff84c2&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://ompramod.medium.com/cross-validation-623620ff84c2&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# **Cross Validation**\n[\n![Omkar Hankare](https://miro.medium.com/v2/resize:fill:64:64/1*RmJTXC-dvXWkdEN2b9T56w.png)\n](https://ompramod.medium.com/?source=post_page---byline--623620ff84c2---------------------------------------)\n[Omkar Hankare](https://ompramod.medium.com/?source=post_page---byline--623620ff84c2---------------------------------------)\n16 min read\n\u00b7Jan 21, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/623620ff84c2&amp;operation=register&amp;redirect=https://ompramod.medium.com/cross-validation-623620ff84c2&amp;user=Omkar+Hankare&amp;userId=5cc07c79d006&amp;source=---header_actions--623620ff84c2---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/623620ff84c2&amp;operation=register&amp;redirect=https://ompramod.medium.com/cross-validation-623620ff84c2&amp;source=---header_actions--623620ff84c2---------------------bookmark_footer------------------)\nListen\nShare\nCross-validation is a technique for evaluating a machine learning model and testing its performance. Cross-validation is a technique used to evaluate the performance of a machine learning model by training it on different subsets of the data and testing it on the remaining subset. This is something different from the general train-test split\n**> Introduction:\n**\nCross-validation is also known as rotation estimation or out-of-sample testing. Rotation estimation refers to the process of rotating, or splitting, the data into different subsets. Simply put, in the process of cross-validation, the original data sample is randomly divided into several subsets. The machine learning model trains on all subsets, except one. After training, the model is tested by making predictions on the remaining subset. The goal is to estimate the model\u2019s performance on unseen data, similar to how cross-validation works.\nOut-of-sample testing is another term used for cross-validation, which refers to the process of testing a model\u2019s performance on data that was not used to train the model. This is important for determining how well the model is likely to perform on new, unseen data.\n**> Why to use Cross Validation?\n**\nSuppose you build a machine learning model to solve a problem, and you have trained the model on a given dataset. When you check the accuracy of the model on the training data, it is close to 95%. Does this mean that your model has trained very well, and it is the best model because of the high accuracy?\nNo, it\u2019s not! Because your model is trained on the given data, it knows the data well, captured even the minute variations(noise), and has generalized very well over the given data. If you expose the model to completely new, unseen data, it might not predict with the same accuracy and it might fail to generalize over the new data. This problem is called over-fitting.\nSometimes the model doesn\u2019t train well on the training set as it\u2019s not able to find patterns. In this case, it wouldn\u2019t perform well on the test set as well. This problem is called Under-fitting.\nPress enter or click to view image in full size\n![]()\n[Reference](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\nCross validation is a way to check if a machine learning model is overfitting or underfitting on a given dataset.\n**> Methods used for Cross-Validation:\n**\nThere are several methods used for cross-validation, each with its own advantages and disadvantages. Some of the most common methods are:\n**> Holdout Validation:\n**\nThe hold-out method, also known as train-test split, is the simplest type of cross-validation. It involves randomly dividing the dataset into two subsets: a training set and a holdout set (or test set). The model is trained on the training set, and its performance is evaluated on the holdout set.\nPress enter or click to view image in full size\n![]()\nA common rule of thumb is to use about 70% of the dataset as a training set and 30% as a validation set. Here\u2019s how you can implement holdout cross-validation using the iris dataset:\n```\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom sklearn.metrics import accuracy\\_score\nX, y = datasets.load\\_iris(return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X, y, test\\_size=0.3, random\\_state=42)\nclf = DecisionTreeClassifier(random\\_state=42)\nclf.fit(X\\_train, y\\_train)\ny\\_pred = clf.predict(X\\_test)\naccuracy = accuracy\\_score(y\\_test, y\\_pred)\nprint(f&quot;Accuracy: {accuracy}&quot;)\n```\nThis method is straightforward and easy to implement but can be less accurate than other cross-validation techniques primarily because it does not account for the variability of the data split.\n1. The performance of the model evaluated through holdout cross-validation can vary significantly based on the specific split of the data. This variability arises because the data used for training and testing is not guaranteed to be representative of the entire dataset. This can lead to an overly optimistic or pessimistic evaluation of the model\u2019s performance.\n2. Holdout cross-validation does not allow for a comprehensive assessment of model performance across different hyperparameters. Since the model is trained and tested only once, it\u2019s challenging to determine the optimal hyperparameters for the model.\n3. Holdout cross-validation does not leverage the entire dataset for training and testing. This can be particularly problematic when dealing with small datasets or when the data is imbalanced. In such cases, a single split might not adequately represent the diversity of the data, leading to inaccurate performance evaluations.\nThus, Holdout cross-validation is a good starting point for model evaluation, but for more reliable performance estimates, other cross-validation techniques like k-fold cross-validation are recommended. However, it\u2019s worth noting that for datasets with a large number of observations, holdout cross-validation can be computationally efficient and may still provide a reasonable estimate of model performance.\n**> 1.\n**> **> K-fold cross validation:\n**\nK-fold cross validation is a method of evaluating the performance of a machine learning model by splitting the data into k subsets or \u201cfolds\u201d of roughly equal size. The model is trained on k-1 of the folds and tested on the remaining fold. This process is repeated k times, with each fold being used as the test set once. The performance of the model is then averaged across all k iterations. This gives an estimate of how well the model is likely to perform on new, unseen ...",
      "url": "https://ompramod.medium.com/cross-validation-623620ff84c2"
    }
  ]
}