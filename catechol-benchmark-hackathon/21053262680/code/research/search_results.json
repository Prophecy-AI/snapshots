{
  "query": "How do you implement extrapolation detection for molecular property prediction to reduce test set error when predicting for unseen chemical compounds?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Implementing extrapolation detection to improve molecular\u2011property predictions for unseen compounds** (based on recent benchmarks, validation schemes, and descriptor\u2011enhanced models)\u202f([Nature\u202f2024](https://www.nature.com/articles/s41524-023-01194-2),\u202f[Digital\u202fDiscovery\u202f2024](https://pubs.rsc.org/en/Content/ArticleLanding/2024/DD/D3DD00256J),\u202f[PMCID\u202f2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006))  \n\n1. **Generate a rich descriptor set**  \n   * Compute conventional fingerprints/graph features *and* a quantum\u2011mechanical (QM) descriptor matrix (the QMex dataset).  \n   * These QM descriptors capture electronic and structural information that helps the model extrapolate beyond the training range\u202f([Nature\u202f2024](https://www.nature.com/articles/s41524-023-01194-2)).  \n\n2. **Create an extrapolation\u2011aware validation split**  \n   * Use **Extrapolation Validation (EV)** or **k\u2011fold n\u2011step forward cross\u2011validation** to mimic prospective prediction: order compounds by a relevant scalar (e.g., molecular weight or a target property) and reserve the highest\u2011range segment as the \u201cextrapolation test\u201d set.  \n   * This split reveals how error grows when the test points lie outside the training distribution\u202f([Digital\u202fDiscovery\u202f2024](https://pubs.rsc.org/en/Content/ArticleLanding/2024/DD/D3DD00256J);\u202f[PMCID\u202f2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  \n\n3. **Quantify extrapolation risk for each candidate**  \n   * For every molecule, compute its distance to the training cloud in the combined descriptor space (e.g., Mahalanobis or cosine distance).  \n   * Flag a molecule as \u201cextrapolative\u201d when the distance exceeds a calibrated threshold or when its target value lies outside the training property range (the definition used in the benchmark\u202f([Nature\u202f2024](https://www.nature.com/articles/s41524-023-01194-2))).  \n\n4. **Train an interaction\u2011aware model**  \n   * Fit the **Interactive Linear Regression (ILR)** that includes interaction terms between QM descriptors and categorical structure information.  \n   * Alternatively, use a graph\u2011neural network (GNN) with **Norm or Sum aggregation** (both additive\u2011property friendly) rather than Mean aggregation, which has been shown to improve MW\u2011type extrapolation\u202f([Medium\u202f2025](https://medium.com/@dinhlong240600/chemprop-aggregation-and-extrapolation-57c3169ee273)).  \n\n5. **Apply extrapolation gating at inference**  \n   * For a new compound, first compute its extrapolation score (step\u202f3).  \n   * If the score is below the threshold, return the model\u2019s prediction.  \n   * If above, either (a) flag the prediction as low\u2011confidence, or (b) blend it with a QM\u2011based linear predictor (the ILR) to obtain a more reliable estimate.  \n\n6. **Validate the workflow**  \n   * Compare test\u2011set MAE/R\u00b2 on the random\u2011split validation versus the EV/forward\u2011CV split.  \n   * A reduction in the extrapolation\u2011test error (often >30\u202f% improvement reported for QM\u2011assisted models) confirms that detection and gating are effective\u202f([Nature\u202f2024](https://www.nature.com/articles/s41524-023-01194-2);\u202f[PMCID\u202f2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  \n\n7. **Optional meta\u2011learning refinement**  \n   * If abundant unlabeled compounds are available, employ a meta\u2011learning scheme that learns to interpolate between in\u2011distribution (ID) and out\u2011of\u2011distribution (OOD) data, further stabilizing predictions for extreme chemistries\u202f([arXiv\u202f2025](https://arxiv.org/abs/2506.11877)).  \n\nFollowing these steps equips you with a systematic extrapolation\u2011detection pipeline that flags high\u2011risk predictions and leverages QM\u2011enhanced models to keep test\u2011set error low when forecasting properties of truly novel chemical compounds.",
      "url": ""
    },
    {
      "title": "Extrapolative prediction of small-data molecular property using ...",
      "text": "[![npj Computational Materials](https://media.springernature.com/full/nature-cms/uploads/product/npjcompumats/header-4baba14304e9c3518bdc0d6f35b470b9.svg)](https://www.nature.com/npjcompumats)\n\nExtrapolative prediction of small-data molecular property using quantum mechanics-assisted machine learning\n\n[Download PDF](https://www.nature.com/articles/s41524-023-01194-2.pdf)\n\n[Download PDF](https://www.nature.com/articles/s41524-023-01194-2.pdf)\n\n### Subjects\n\n- [Computational methods](https://www.nature.com/subjects/computational-methods)\n- [Theoretical chemistry](https://www.nature.com/subjects/theoretical-chemistry)\n\n## Abstract\n\nData-driven materials science has realized a new paradigm by integrating materials domain knowledge and machine-learning (ML) techniques. However, ML-based research has often overlooked the inherent limitation in predicting unknown data: extrapolative performance, especially when dealing with small-scale experimental datasets. Here, we present a comprehensive benchmark for assessing extrapolative performance across 12 organic molecular properties. Our large-scale benchmark reveals that conventional ML models exhibit remarkable performance degradation beyond the training distribution of property range and molecular structures, particularly for small-data properties. To address this challenge, we introduce a quantum-mechanical (QM) descriptor dataset, called QMex, and an interactive linear regression (ILR), which incorporates interaction terms between QM descriptors and categorical information pertaining to molecular structures. The QMex-based ILR achieved state-of-the-art extrapolative performance while preserving its interpretability. Our benchmark results, QMex dataset, and proposed model serve as valuable assets for improving extrapolative predictions with small experimental datasets and for the discovery of novel materials/molecules that surpass existing candidates.\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-024-01339-x/MediaObjects/41524_2024_1339_Fig1_HTML.png)\n\n### [Learning together: Towards foundation models for machine learning interatomic potentials with meta-learning](https://www.nature.com/articles/s41524-024-01339-x?fromPaywallRec=false)\n\nArticleOpen access17 July 2024\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41570-022-00416-3/MediaObjects/41570_2022_416_Figa_HTML.png)\n\n### [Extending machine learning beyond interatomic potentials for predicting molecular properties](https://www.nature.com/articles/s41570-022-00416-3?fromPaywallRec=false)\n\nArticle25 August 2022\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-025-04720-7/MediaObjects/41597_2025_4720_Fig1_HTML.png)\n\n### [The QCML dataset, Quantum chemistry reference data from 33.5M DFT and 14.7B semi-empirical calculations](https://www.nature.com/articles/s41597-025-04720-7?fromPaywallRec=false)\n\nArticleOpen access08 March 2025\n\n## Introduction\n\nMaterials science has greatly benefited from advancements in machine learning (ML) and deep learning (DL) techniques[1](https://www.nature.com/www.nature.com#ref-CR1), [2](https://www.nature.com/www.nature.com#ref-CR2), [3](https://www.nature.com/www.nature.com#ref-CR3), [4](https://www.nature.com/www.nature.com#ref-CR4), [5](https://www.nature.com/www.nature.com#ref-CR5), [6](https://www.nature.com/articles/s41524-023-01194-2#ref-CR6). These techniques have revolutionized the prediction of molecular properties, leveraging traditional computational approaches, such as the group contribution (GC) method[7](https://www.nature.com/www.nature.com#ref-CR7), [8](https://www.nature.com/www.nature.com#ref-CR8), [9](https://www.nature.com/articles/s41524-023-01194-2#ref-CR9), quantitative structure-activity/property relationship (QSAR/QSPR) method[10](https://www.nature.com/www.nature.com#ref-CR10), [11](https://www.nature.com/www.nature.com#ref-CR11), [12](https://www.nature.com/www.nature.com#ref-CR12), [13](https://www.nature.com/articles/s41524-023-01194-2#ref-CR13), quantum mechanics (QM), and molecular dynamics (MD) calculations[14](https://www.nature.com/www.nature.com#ref-CR14), [15](https://www.nature.com/www.nature.com#ref-CR15), [16](https://www.nature.com/www.nature.com#ref-CR16), [17](https://www.nature.com/www.nature.com#ref-CR17), [18](https://www.nature.com/www.nature.com#ref-CR18), [19](https://www.nature.com/www.nature.com#ref-CR19), [20](https://www.nature.com/www.nature.com#ref-CR20), [21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/www.nature.com#ref-CR22), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/www.nature.com#ref-CR25), [26](https://www.nature.com/articles/s41524-023-01194-2#ref-CR26). Graph neural networks (GNNs) have emerged as a promising DL-based method for property prediction by embedding molecular structures in a graph architecture[27](https://www.nature.com/articles/s41524-023-01194-2#ref-CR27), [28](https://www.nature.com/articles/s41524-023-01194-2#ref-CR28). Moreover, pre-trained GNNs, which employ self-supervised or transfer learning, have demonstrated the highest accuracies across various benchmarks in molecular property prediction[29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/www.nature.com#ref-CR32), [33](https://www.nature.com/articles/s41524-023-01194-2#ref-CR33). ML/DL techniques continue to enhance the accuracy and speed of property prediction, serving as indispensable tools for data-driven materials science.\n\nHowever, a fundamental contradiction persists in ML/DL techniques regarding their inherent extrapolation difficulty, i.e., the ability to predict beyond the available data. In molecular property prediction, two main limitations arise from the range of molecular properties and the diversity of molecular structures (see Fig. [1](https://www.nature.com/articles/s41524-023-01194-2#Fig1)). The primary objective of data-driven materials exploration is to identify high-performance molecules/materials that are not yet represented in databases. Hence, ML/DL models must possess the capability to extrapolate unexplored data solely from the available data. However, materials datasets often consist of small experimental results, typically containing fewer than 500 data points[34](https://www.nature.com/www.nature.com#ref-CR34), [35](https://www.nature.com/www.nature.com#ref-CR35), [36](https://www.nature.com/www.nature.com#ref-CR36), [37](https://www.nature.com/www.nature.com#ref-CR37), [38](https://www.nature.com/www.nature.com#ref-CR38), [39](https://www.nature.com/articles/s41524-023-01194-2#ref-CR39), which inevitably carries biases due to molecular structures and property ranges. It is crucial to determine whether ML/DL models can overcome these biases and effectively extrapolate molecular properties, even when dealing with limited data, to discover novel materials/molecules that outperform existing ones.\n\n**Fig. 1: Overview of extrapolative prediction of molecular property based on the range of molecular properties and the diversity of molecular structures.**\n\n[![figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41524-023-01194-2/MediaObjects/41524_2023_1194_Fig1_HTML.png)](https://www.nature.com/articles/s41524-023-01194-2/figures/1)\n\nThe interpolation task represents predictions within the available property range and molecular structures, while the extrapolation task represents predictions outside the training distribution of existing data. Plot illustrates the relationship between 1D-UMAP (Uniform Manifold Approximation and Projection[90](https://www.nature.com/articles/s41524-023-01194-2#ref-CR90)) compo...",
      "url": "https://www.nature.com/articles/s41524-023-01194-2"
    },
    {
      "title": "Chemprop: Aggregation and Extrapolation | by Dinh Long Huynh",
      "text": "## Context\n\nI have just finished a first version of my Master\u2019s Thesis and waiting for my supervisor\u2019s opinion. That\u2019s why I again have tones of time for reading my favorite topics and seek some junior job position (\ud83d\ude2d). Recently, I came across Dr. [Pat Walters](https://www.linkedin.com/in/wpwalters/)\u2019s blogs about the extrapolation capacity of Chemprop model.\n\nPart 1: [https://patwalters.github.io/Why-Dont-Machine-Learning-Models-Extrapolate/](https://patwalters.github.io/Why-Dont-Machine-Learning-Models-Extrapolate/)\n\nPart 2 (guest post from [Dr. Alan Cheng](https://www.linkedin.com/in/alancheng/) and [Jeffery Zhou](https://www.linkedin.com/in/jeffery-zhou-b8523816a/)): [https://patwalters.github.io/GNNs-Can-Extrapolate/](https://patwalters.github.io/GNNs-Can-Extrapolate/)\n\nIn Part 1, Dr. Walters suggested an interesting point: Chemprop models struggle with the extrapolation of Molecular Weights (MW). Specifically, models trained on compounds with MW below 400 g/mol have difficulty making predictions for those with MW higher than 500 g/mol. I refer to this as extrapolation on the target space, since it does not involve the input space, such as molecule clusters or chemical spaces, in this context.\n\nIn the follow-up blog, Dr. Cheng and Zhou raised an even more interesting point: using **Mean Aggregation** is the reason why model struggles with extrapolating MW. They proposed that using **Norm Aggregation** could greatly enhance extrapolation for MW. Based on their [scripts](https://github.com/iGotsIt/chemprop-extrapolation), I recreated the analysis.\n\nScatter plot of y\\_true versus y\\_pred. Blue points show the interpolation test set, orange points show the extrapolation test set.\n\nI then gave **Sum Aggregation** a try, which also performed well for MW extrapolation, as I expected. Because I noticed that **MW is an additive property**, it makes both Sum Aggregation and Norm Aggregation naturally advantageous.\n\nThis led me to question **whether non-additive molecular properties can also benefit from Sum Aggregation or Norm Aggregation for extrapolation**. This motivated me to write this blog. The focus of this blog is on the effect of aggregation layer on the Chemprop models\u2019 extrapolation capacity. I used the default hyperparameters for the models in my analysis. Further investigation into hyperparameter tuning and extrapolation concerning the input space, such as scaffold or cluster-based splitting, would be out of scope but worth to try (maybe another blog).\n\nThe scripts for this blog is on my [GitHub](https://github.com/DinhLongHuynh/extrapolation_blog/). Those scripts are mainly inherited from [Jeffery notebook](https://github.com/iGotsIt/chemprop-extrapolation) with a little modification to conduct my analysis.\n\n## **Chemprop Aggregation Layer**\n\nChemprop is a nice package for molecular property prediction. The fundamental architecture of Chemprop include 4 modules: (1) a **local feature encoder** to turn atoms and bonds into matrix representations, (2) a **Directed Message Passing Neural Network** (D-MPNN) to capture the relationship between atoms and bonds, (3) an **aggregation function** to combine atomic embedding into the molecular embedding, and (4) a standard FFN to generate final prediction from molecular embedding.\n\nChemprop architecture overview\n\nRecently, Chemprop supports 4 aggregation functions:\n\n- **Sum Aggregation:** simply sums the atom matrix along the second dimension (the dimension of number of atoms).\n- **Norm Aggregation:** performs sum aggregation and then divides the result by a **constant norm factor**, i.e. 100.\n- **Mean Aggregation:** performs sum aggregation and then divides the result the **number of atoms** in the given molecule.\n- **Attentive Aggregation:** assign **attention scores for each atom** using a learnable linear layer, then performs a weighted sum of atom features based on these attention scores, giving more importance to certain atoms in the aggregation.\n\n## **Experiments**\n\nI applied the same approach to Dr. Walters, which consists of 3 datasets for each experiment: training-validating set, interpolating testing set, and extrapolating testing set. In my investigation, I examined **four aggregation** layers and **seven target values**: molecular weight (MW), logP, norm\\_MW (calculated as MW/number of atoms), TPSA, BalabanJ, BertzCT, and LabuteASA. I intentionally computed norm\\_MW as a way to reduce size-dependent property of MW.\n\nDiagnostic plots were created for the testing sets, with the interpolating set represented in blue and the extrapolating set in orange.\n\nScatter plot of y\\_true versus y\\_pred across experiments, organized by aggregation method (rows) and molecular property (columns). Blue points show the interpolation test set, orange points show the extrapolation test set.\n\nTo clarify, among the seven target values provided, MW, logP, TPSA, BertzCT, and LabuteASA are dependent on molecular size, while norm\\_MW and BalabanJ are size-independent. It is important to emphasize that my classification is based on **how RDKit calculates** these values, rather than their nature, as I derived these target values using RDKit.\n\nThere is an interesting trend: while **Norm and Sum Aggregation** perform well for extrapolating **size-dependent** properties, they fail for size-independent factors. Interestingly, **Attentive and Mean Aggregation** are effective for extrapolating **size-independent** properties but not for size-dependent factors. Norm\\_MW is a challenging task in general and might require hyperparameter tuning to boost the performance. However, looking at those that are highly correlated with y\\_true (points close to the diagonal line), Attentive and Mean Aggregation still perform better than the other two in terms of extrapolation.\n\n## Final thoughts\n\nThis blog shows an intuitive perspective that the selection of Aggregation layers is task-specific and worth for a very deep investigation when start training a Chemprop model. To strongly conclude, I believe that my toy models (I borrow this term from Dr. Walters) are not enough, as it might need a stronger statistical framework to draw the final conclusion.",
      "url": "https://medium.com/@dinhlong240600/chemprop-aggregation-and-extrapolation-57c3169ee273"
    },
    {
      "title": "Extrapolation validation (EV): a universal validation method for mitigating machine learning extrapolation risk - Digital Discovery (RSC Publishing)",
      "text": "Extrapolation validation (EV): a universal validation method for mitigating machine learning extrapolation risk - Digital Discovery (RSC Publishing)\n[\nJump to main content![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)\n](#maincontent)[\nJump to site search![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)\n](#SearchText)\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/menu-light.png)](#)\n[Publishing](https://pubs.rsc.org/)\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/search-light.png)](#)\nSearch\n![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right.png)[Advanced](https://pubs.rsc.org/en/search/advancedsearch)\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/user-light.png)](https://pubs.rsc.org/en/account/logon)\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/trolley-light.png)](https://www.rsc.org/basket/shoppingcart/orderitems?returnurl=https://pubs.rsc.org/en/Content/ArticleLanding/2024/DD/D3DD00256J)\n[![Royal Society of Chemistry homepage](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/rsc-logo-rev-pubs.svg)](https://www.rsc.org)\nSearch\n**![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right.png)\nYou must enter a search term\n[Advanced search](https://pubs.rsc.org/en/search/advancedsearch)\n[Issue 5, 2024](https://pubs.rsc.org/en/journals/journal/dd?issueid=dd003005&amp;type=current)\n[\n![](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=CoverIssue&amp;imageInfo.ImageIdentifier.SerCode=DD&amp;imageInfo.ImageIdentifier.IssueId=DD003005)\nFrom the journal:### Digital Discovery\n](https://pubs.rsc.org/en/journals/journal/dd)\n## Extrapolation validation (EV): a universal validation method for mitigating machine learning extrapolation risk[&#8224;](#fn1)\n![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg)\n[Mengxian\rYu](https://pubs.rsc.org/en/results?searchtext=Author:Mengxian%20Yu),*a*[Yin-Ning\rZhou](https://pubs.rsc.org/en/results?searchtext=Author:Yin-Ning%20Zhou),[![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0003-3509-3983)*b*[Qiang\rWang](https://pubs.rsc.org/en/results?searchtext=Author:Qiang%20Wang)*a*and[Fangyou\rYan](https://pubs.rsc.org/en/results?searchtext=Author:Fangyou%20Yan)[![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/orcid_16x16.png)](https://orcid.org/0009-0009-6394-4600)\\**a*\n[Author affiliations](#)\n\\*Corresponding authors\naSchool of Chemical Engineering and Material Science, Tianjin University of Science and Technology, Tianjin 300457, P. R. China\n**E-mail:**[yanfangyou@tust.edu.cn](mailto:yanfangyou@tust.edu.cn)\nbDepartment of Chemical Engineering, School of Chemistry and Chemical Engineering, Shanghai Jiao Tong University, Shanghai 200240, P. R. China\n### Abstract\nMachine learning (ML) can provide decision-making advice for major challenges in science and engineering, and its rapid development has led to advances in fields like chemistry &amp; medicine, earth &amp; life sciences, and communications &amp; transportation. Grasping the trustworthiness of the decision-making advice given by ML models remains challenging, especially when applying them to samples outside the domain-of-application. Here, an untrustworthy application situation (*i.e.*, complete extrapolation-failure) that would occur in models developed by ML methods involving tree algorithms is confirmed, and the root cause of its difficulty in discovering novel materials &amp; chemicals is revealed. Furthermore, a universal extrapolation risk evaluation scheme, termed the extrapolation validation (EV) method, is proposed, which is not restricted to specific ML methods and model architecture in its applicability. The EV method quantitatively evaluates the extrapolation ability of 11 popularly applied ML methods and digitalizes the extrapolation risk arising from variations of the independent variables in each method. Meanwhile, the EV method provides insights and solutions for evaluating the reliability of out-of-distribution sample prediction and selecting trustworthy ML methods.\n![Graphical abstract: Extrapolation validation (EV): a universal validation method for mitigating machine learning extrapolation risk](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&amp;imageInfo.ImageIdentifier.ManuscriptID=D3DD00256J&amp;imageInfo.ImageIdentifier.Year=2024)\nThis article is Open Access\n![](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/Ajax-GA-Loader.gif)Please wait while we load your content...Something went wrong.[Try again?](#)\n[About](#pnlAbstract)\n[Cited by](#pnlCitation)\n[Related](#pnlRelatedContent)\n[Download optionsPlease wait...](#)\n## Supplementary files\n* [Supplementary informationPDF (2680K)](https://www.rsc.org/suppdata/d3/dd/d3dd00256j/d3dd00256j1.pdf)\n## Article information\nDOI[https://doi.org/10.1039/D3DD00256J](https://doi.org/10.1039/D3DD00256J)\n**Article type**Paper\nSubmitted29 Dec 2023\nAccepted17 Apr 2024\nFirst published19 Apr 2024\n![](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/open-access-icon-orange.png)\n**This article is Open Access**[![Creative Commons BY-NC license](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/CCBY-NC.svg)](http://creativecommons.org/licenses/by-nc/3.0/)\n### DownloadCitation\n***Digital Discovery***, 2024,**3**, 1058-1067\nBibTexEndNoteMEDLINEProCiteReferenceManagerRefWorksRIS\n### Permissions\n[Request permissions](#)\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](#)### Extrapolation validation (EV): a universal validation method for mitigating machine learning extrapolation risk\nM. Yu, Y. Zhou, Q. Wang and F. Yan,*Digital Discovery*, 2024,**3**, 1058**DOI:**10.1039/D3DD00256J\nThis article is licensed under a[Creative Commons Attribution-NonCommercial 3.0 Unported Licence](https://creativecommons.org/licenses/by-nc/3.0/).**You can use material from this article in other publications, without requesting further permission**from the RSC, provided that the correct acknowledgement is given and it is not used for commercial purposes.\nTo request permission**to reproduce material from this article in a commercial publication**, please go to the[Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039/D3DD00256J).\nIf you are**an author contributing to an RSC publication, you do not need to request permission**provided correct acknowledgement is given.\nIf you are**the author of this article, you do not need to request permission to reproduce figures and diagrams**provided correct acknowledgement is given. If you want to reproduce the whole article in a third-party commercial publication (excluding your thesis/dissertation for which permission is not required) please go to the[Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039/D3DD00256J).\nRead more about[how to correctly acknowledge RSC content](https://www.rsc.org/journals-books-databases/journal-authors-reviewers/licences-copyright-permissions/#acknowledgements).\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](#)\n### Social activity\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/twitter.svg)Tweet](https://twitter.com/intent/tweet/?text=Extrapolation+validation+(EV):+a+universal+validation+method+for+mitigating+machine+learning+extrapolation+risk+-+now+published+in+Digital+Discovery&url=https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00256j)\n[![](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/wechat.svg)Share](https://pubs.rsc.org/en/Image/GetQrCode?url=https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00256j)\n## Search articles by author\nMengxian Yu\nYin-Ning Zhou\nQiang Wang\nFangyou Yan\n![](https://www.rsc-cdn.org/pubs-core/2022.0.191/content/NewImages/Ajax-GA-Loader.gif)Fetching data from CrossRef.\nThis may ta...",
      "url": "https://pubs.rsc.org/en/Content/ArticleLanding/2024/DD/D3DD00256J"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Forecasting",
      "text": "Forecasting\n**Planning Analysis:\nForecasting**\n|\n> > > > > > > > > > > > > > > > **\nYou should also visit the[Calculating Growth Rates](class8a.htm)page.\nEstimates and Forecasts**\n* *Estimates*- refers to current\rpopulation. are calculated in lieu of of an actual census count and are used to update\rpopulation figures of the last census</li></ul>\n* *Projections*- refers to future population levels.\rprojections indicate what population changes might occur, given assumptions inherent in\rthe projection method and data - typically presented in ranges - it, then</li></ul>**\nTypes of Forecasts\n</font>A. Extrapolation**</p>> > Extrapolative Forecasting\n> > - a method of prediction which assumes that the patterns that existed in the past will> continue on into the future, and that those patterns are regular and can be measured. In other words, the past is a good indicator of the future. > *\n**> Applications\n*> :> good for developing baseline data. Not as good for assessing the future impacts of a policy change, unless historic data for a similar policy change is available.\n</p>> *> Attributes &amp; Limits\n*> - simple, cheap and often as or more accurate that complex theoritical models.\n> *> Process\n*> - plot data and observe patterns. > > Key: having a good base of data and understanding the pattern within it--easier said than done\n> > *> Technique\n*> s: best fit, ratios, etc\n> </font>> **\nB. Theoretical Models**</p>> Theoretical\rForecasting- modeling, or using a construct of how some subsystem of the world\rfunctions to predict how things will happen in the future. There are empirical models,\rwhich reduce the problem to calculations, and nonempirical models, which do not quantify\rthe problem under scrutiny.</p>\nLargely involves modeling.\rModels specify the linkages between relevant variables and are nothing more than informed\rspeculation about the behavior of some system.\n*Applications*:\rmany rational decisions are made on the basis of models e.g., the law of gravity.\r(discounting, certain pop models, etc)\n*Attributes &amp;\rLimits*- more complex than extrapolation.\n*Process -*based\ron longer-term relationships and research.\n*Techniques:*econometric models, cohort survival, Reilly&#146;s gravity model, discounting\n</blockquote>**\nC. Intuitive Prediction**</p>> </font>> Inductive Forecasting > - a set of methods in which the future state is predicted (by persons who> have some knowledge that makes them likely to do this accurately), and then data and assumptions necessary to achieve this outcome are deducted.\n> > The most common forecasting methods. Rely primarily on subjective judgement. (seat-of-the-pants planning).\n> *> Attributes &amp; Limits\n*> - uses retroductive logic eg a future state is described and the retroductive logic is used to find data and assumptions consistent with the forecasted end state.\n> *> Process\n*> - 1.> choose knowledgable persons to interview. 2. anonymity; types - Delphi, cross impact analysis, feasibility assessment, etc.\n> </font>\n**Guidelines for Selecting a forecasting\rmethod**\n1. relative accuracy</li>\n2. type of population data available\n3. quality of data available\n4. scale of the analysis\n5. length of projection period\n6. budget &amp; time</li></ol>**\nComparison of Population\rProjection and Estimation Models**</p>|**\nRelative Complexity**|**\nType of Data that Can Be Used**|**\nEstimating or Projecting Technique**|\n**Type of\rModel**|**\nSimple**|**\nModerate**|**\nComplex**|**\nHistorical Counts**|**\nVital Components**|**\nOther Indicies**|**\nEstimation**|**\nProjection**|\n**Noncomponent:**|||||||||\nTrend Extrapolation|\n\u00fc|\n\u00fc||\n\u00fc||\n\u00fc|\n\u00fc|\n\u00fc|\nComparative Forecast|\n\u00fc|||\n\u00fc||||\n\u00fc|\nRatio Trend|\n\u00fc|||\n\u00fc|||\n\u00fc|\n\u00fc|\nDensity Ceiling||\n\u00fc|\n\u00fc|\n\u00fc||||\n\u00fc|\nRation Correlation||\n\u00fc|\n\u00fc|\n\u00fc||\n\u00fc|\n\u00fc||\nHousing Unit||\n\u00fc|\n\u00fc|\n\u00fc||\n\u00fc|\n\u00fc|\n\u00fc|\nMarket Force||\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc||\n\u00fc|\n**Component:**|||||||||\nResidual|\n\u00fc||||\n\u00fc||\n\u00fc||\nVital Rates||\n\u00fc|||\n\u00fc||\n\u00fc|\n\u00fc|\nCohort-Survival||\n\u00fc|\n\u00fc||\n\u00fc||\n\u00fc|\n\u00fc|\nCohort-Component||\n\u00fc|\n\u00fc||\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\nComposite||\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\nSource*: Local Population and\rEmployment Projection Techniques*. Greenburg, et al. Center for Urban Policy Research,\r1979.**</p>> > > > > > > > > &nbsp;\n> > > > > > Comparison of Population Projection and Estimation Models**</p></font>|**\nAppropriate Estimation and Projection Period**|**\nAppropriate\rEstimation and Projection Scale**|**\nAppropriate for Studies Involving Numerous Entities**|\n**Type of\rModel**|**Short**|**Middle**|**Long**|**Nation**|**State**|**County**|**Local**||\n**Noncomponent:**|||||||||\nTrend Extrapolation|\n\u00fc|||||\n\u00fc|\n\u00fc|Yes|\nComparative Forecast|\n\u00fc|||||\n\u00fc|\n\u00fc|Maybe|\nRatio Trend|\n\u00fc|\n\u00fc|\n\u00fc||\n\u00fc|\n\u00fc|\n\u00fc|Yes|\nDensity Ceiling||\n\u00fc|\n\u00fc|||\n\u00fc|\n\u00fc|Yes|\nRatio Correlation|\n\u00fc|||||\n\u00fc|\n\u00fc|No|\nHousing Unit`|\n\u00fc|\n\u00fc||||\n\u00fc|\n\u00fc|No|\nMarket Force|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|Yes|\n**Component:**|||||||||\nResidual|\n\u00fc|||\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|Yes|\nVital Rates|\n\u00fc||||\n\u00fc|\n\u00fc|\n\u00fc|Maybe|\nCohort-Survival|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|Yes|\nCohort-Component|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|\n\u00fc|Depends on\rMigration Model|\nComposite|\n\u00fc|\n\u00fc|\n\u00fc||\n\u00fc|\n\u00fc|\n\u00fc|Yes|\nSource*: Local Population and\rEmployment Projection Techniques*. Greenburg, et al. Center for Urban Policy Research,\r1979.**</p>**\n[[Home](IndexMain.htm)|[Syllabus](Syllabus.htm)|[Schedule](Schedule.htm)|[Assignments](Assignments.htm)|[Project](Project.htm)|[Links](Links.htm)]\nThis page maintained by[Bob Parker](mailto:rgp@darkwing.uoregon.edu),\r\u00a92002\nOctober 21, 2003",
      "url": "https://pages.uoregon.edu/rgp/PPPM613/class8.htm"
    },
    {
      "title": "Generative Models for Extrapolation Prediction in Materials Informatics",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p></p>\n<p>We report a deep\ngenerative model for regression tasks in materials\ninformatics. The model is introduced as a component of a data imputer\nand predicts more than 20 diverse experimental properties of organic\nmolecules. The imputer is designed to predict material properties\nby \u201cimagining\u201d the missing data in the database, enabling\nthe use of incomplete material data. Even removing 60% of the data\ndoes not diminish the prediction accuracy in a model task. Moreover,\nthe model excels at extrapolation prediction, where target values\nof the test data are out of the range of the training data. Such an\nextrapolation has been regarded as an essential technique for exploring\nnovel materials but has hardly been studied to date due to its difficulty.\nWe demonstrate that the prediction performance can be improved by\n&gt;30% by using the imputer compared with traditional linear regression\nand boosting models. The benefit becomes especially pronounced with\nfew records for an experimental property (&lt;100 cases) when prediction\nwould be difficult by conventional methods. The presented approach\ncan be used to more efficiently explore functional materials and break\nthrough previous performance limits.</p></section><section><h2>1. Introduction</h2>\n<p>The aim of materials informatics is to reveal the underlying trends\nin materials science by using machine learning tools to enable efficient\nexploration of functional materials, including those for use in energy-related\ndevices.<sup><a href=\"#ref1\">1</a>\u2212<a href=\"#ref5\">5</a></sup> Since the properties of materials are uniquely determined by the\nstates of their constituent atoms, their observed structure\u2013property\nrelationships can be mimicked by machine learning models (<a href=\"#fig1\">Figure </a><a href=\"#fig1\">1</a>).<sup><a href=\"#ref3\">3</a>\u2212<a href=\"#ref5\">5</a></sup> Their predictions\nare often more accurate than traditional theory-based predictions\nand simulations, especially in the cases of complex systems where\nthe computational costs of the traditional approaches increase exponentially.<sup><a href=\"#ref5\">5</a>\u2212<a href=\"#ref7\">7</a></sup></p>\n<figure><h3>Figure 1.</h3>\n<p></p>\n<figcaption><p>Scheme\nfor processing material data. The material information is\nobservable as a chemical formula, experimental properties, and various\nother characteristics. The conventional machine learning models typically\ntreat the relationship between only two factors (e.g., fingerprint\nand experimental property). On the other hand, humans and generative\nmodels can integrate and imagine versatile information and consider\nvarious underlying relationships.</p></figcaption></figure><p>To describe the structures of materials, researchers typically\nconsider their characteristic representations, such as molecular formulas\nand crystal structures.<sup><a href=\"#ref1\">1</a>,<a href=\"#ref3\">3</a></sup> In particular, numeric array-type\nexpressions are frequently used in materials informatics because of\ntheir high computational processability.<sup><a href=\"#ref1\">1</a>,<a href=\"#ref3\">3</a></sup> Molecular descriptors,\nfingerprints, and neural network outputs are representative ideas\nfor efficiently expressing the structural information of organic molecules\nand could be alternatives to molecular structures or their character\nstrings (e.g., simplified molecular input line entry system: SMILES).<sup><a href=\"#ref1\">1</a>,<a href=\"#ref3\">3</a>,<a href=\"#ref8\">8</a>\u2212<a href=\"#ref11\">11</a></sup> Machine learning models can connect\nstructural information and material properties via statistical relationships\n(<a href=\"#fig1\">Figure </a><a href=\"#fig1\">1</a>).<sup><a href=\"#ref1\">1</a></sup> Diverse features, including mechanical properties,<sup><a href=\"#ref12\">12</a></sup> permittivity,<sup><a href=\"#ref6\">6</a></sup> electric\nconductivity,<sup><a href=\"#ref5\">5</a>,<a href=\"#ref7\">7</a>,<a href=\"#ref13\">13</a></sup> thermal\nconductivity,<sup><a href=\"#ref14\">14</a></sup> and photoconversion efficiency,<sup><a href=\"#ref15\">15</a></sup> have been successfully predicted from structural\ndata with reasonable accuracy.</p>\n<p>Although conventional machine\nlearning models can predict the properties\nof materials at higher levels of accuracy than traditional approaches,\nthe processable information has been strictly limited. Usually, one\nmodel processes only a single property of a limited number of material\nspecies,<sup><a href=\"#ref1\">1</a></sup> whereas humans can judge material\ncharacteristics not only from the specific species but also from various\nmaterial data and general knowledge of science.<sup><a href=\"#ref5\">5</a></sup> The lack of such prior knowledge often becomes problematic\nfor prediction models, especially when using small material databases,\nleading to inferior prediction accuracy. More flexible and humanlike\ndata processing algorithms are needed to allow the models to acquire\na broad knowledge of materials and achieve more reliable predictions.<sup><a href=\"#ref5\">5</a></sup></p>\n<p>Several approaches have been proposed to\nimprove prediction accuracy.<sup><a href=\"#ref1\">1</a>,<a href=\"#ref5\">5</a>,<a href=\"#ref10\">10</a>,<a href=\"#ref11\">11</a></sup> Transfer learning has become\na powerful deep learning technique:\nhere, a neural network is pretrained with a large database, and the\nmodel is reused with a smaller target database.<sup><a href=\"#ref10\">10</a>,<a href=\"#ref16\">16</a>,<a href=\"#ref17\">17</a></sup> Increased prediction accuracy has been reported\nin the fields of image processing,<sup><a href=\"#ref17\">17</a></sup> text\nprocessing,<sup><a href=\"#ref16\">16</a></sup> and even materials science.<sup><a href=\"#ref7\">7</a>,<a href=\"#ref10\">10</a>,<a href=\"#ref18\">18</a>,<a href=\"#ref19\">19</a></sup> On the other hand, the amount of accessible material data (e.g.,\n10<sup>1</sup>\u201310<sup>4</sup> cases for one experimental database)<sup><a href=\"#ref3\">3</a>,<a href=\"#ref5\">5</a></sup> might, in general, be too small to be considered as big data for\ndeep learning. In contrast, over 10<sup>7</sup> records are used in\ntypical transfer learning tasks.<sup><a href=\"#ref16\">16</a>,<a href=\"#ref17\">17</a></sup> More efficient\napproaches are needed to realize robust predictions with smaller databases.</p>\n<p>Here, we propose generative models<sup><a href=\"#ref11\">11</a>,<a href=\"#ref20\">20</a>\u2212<a href=\"#ref23\">23</a></sup> for effectively processing broad material data. The models learn\nthe distribution of inputted data, not a specific relationship between\nan explanatory variable <strong><em>x</em></strong> and target <em>y</em>.<sup><a href=\"#ref20\">20</a></sup> The introduction of the\n\u201cimagined\u201d databases made by generative models was found\nto be a key to predicting various material properties from small experimental\nmaterial databases, even for databases missing many records. Compared\nwith standard models, generative models led to improved regression\naccuracy in both interpolation and extrapolation tasks (i.e., for\npredicted <em>y</em> targets, respectively, within and outside\nof the range of the training dataset). Except for linear regression\n(including the Gaussian process with linear kernel) and theory-based\nmodeling (e.g., scaling laws and group contribution methods),<sup><a href=\"#ref24\">24</a>,<a href=\"#ref25\">25</a></sup> no practical method has been developed to carry out such extrapolations\nin materials informatics,<sup><a href=\"#ref1\">1</a></sup> and hence, a\npromising alternative is proposed in this study. The present results\nare expected to open a new path for more efficiently exploring new\nmaterials that can overcome the performance limits of previous materials.</p></section><section><h2>2. Results and Discussion</h2>\n<section><h3>2.1. Descriptor Selection</h3>\n<p>The objective\nof this study was to introduce a generative model to predict diverse\nmaterial properties from structural information. Since the model can,\nin principle, proc...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8190893"
    },
    {
      "title": "Out-of-Distribution Property Prediction in Materials and Molecules",
      "text": "<div><div>\n<article>\n<p>[1]<span>\\fnm</span>Nofit <span>\\sur</span>Segal</p>\n<p>[2]<span>\\fnm</span>Aviv <span>\\sur</span>Netanyahu</p>\n<div>\n<p><span>\\equalcont</span></p><p>Equal Advising.</p>\n</div>\n<div>\n<p><span>\\equalcont</span></p><p>Equal Advising.</p>\n</div>\n<p>1]<span>\\orgdiv</span>Materials Science and Engineering, <span>\\orgname</span>MIT, <span>\\orgaddress</span><span>\\street</span>Memorial Dr, <span>\\city</span>Cambridge, <span>\\postcode</span>02139, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>2]<span>\\orgdiv</span>Electrical Engineering and Computer Science, <span>\\orgname</span>MIT, <span>\\orgaddress</span><span>\\street</span>Vassar St, <span>\\city</span>Cambridge, <span>\\postcode</span>02139, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>3]<span>\\orgdiv</span>Chemical Engineering, <span>\\orgname</span>Catholic Institute of Technology, <span>\\orgaddress</span><span>\\street</span>Broadway, <span>\\city</span>Cambridge, <span>\\postcode</span>02142, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>4]<span>\\orgdiv</span>Chemistry, <span>\\orgname</span>Catholic Institute of Technology, <span>\\orgaddress</span><span>\\street</span>Broadway, <span>\\city</span>Cambridge, <span>\\postcode</span>02142, <span>\\state</span>MA, <span>\\country</span>USA</p>\n<p>\n<span>\u2003\u2003</span><span>\n<span>\n</span><span>\n<span><a href=\"mailto:rafagb@mit.edu\">rafagb@mit.edu</a>\n</span></span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Kevin P. <span>\\sur</span>Greenman\n</span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Pulkit <span>\\sur</span>Agrawal\n</span></span>\n<span>\u2003\u2003</span><span>\n<span><span>\\fnm</span>Rafael <span>\\sur</span>G\u00f3mez-Bombarelli\n</span><span>\n<span>[\n</span>\n<span>[\n</span>\n<span>[\n</span>\n<span>[\n</span></span></span>\n</p>\n<div>\n<h6>Abstract</h6>\n<p>Discovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (<span>ood</span>) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to <span>ood</span> property prediction, achieving improvements in prediction accuracy. In particular, the True Positive Rate (TPR) of <span>ood</span> classification of materials and molecules improved by 3x and 2.5x, respectively, and precision improved by 2x and 1.5x compared to non-transductive baselines. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.</p>\n</div>\n<div>\n<h6>keywords: </h6><p>machine learning, materials property prediction, extrapolation, out-of-distribution, transduction\n</p></div>\n<section>\n<h2>\n<span>1 </span>Introduction</h2>\n<p>Designing new materials and molecules is essential for the development of new technologies. Traditionally, this design process involves extensive experimental iteration or high-throughput methods to screen databases, which are time-consuming and resource-intensive <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib1\">1</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>]</cite>. As a result, there is increasing interest in applying machine learning (ML) techniques to accelerate the discovery of materials and molecules with desired properties <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib1\">1</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib3\">3</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib5\">5</a>]</cite>.\nThere is particular interest in property values that are outside the known property value distribution as these will most likely lead to discovering new materials that will, in turn, unlock new capabilities and technologies.</p>\n<p>One strategy for finding materials and molecules with desired properties is inverse design through conditional generation where materials with out-of-distribution (<span>ood</span>) property values are unavailable, and the goal is to generate them.<cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib5\">5</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib6\">6</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib7\">7</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib8\">8</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib2\">2</a>]</cite>.\nA complementary approach is screening a large database of candidates based on their predicted properties. <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib9\">9</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib10\">10</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib11\">11</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib12\">12</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib13\">13</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib14\">14</a>]</cite>. In this setting, the objective is to identify <span>ood</span> materials and molecules from a set of known candidates with unknown property values.\nHowever, both approaches typically struggle when property values fall outside the training distribution. <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib4\">4</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib11\">11</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib13\">13</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib15\">15</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib16\">16</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib17\">17</a>]</cite>. Enhancing extrapolative capabilities in property prediction would improve the screening of large candidate spaces in terms of precision by identifying promising compounds and molecules with exceptional properties. This approach could help guide further synthesis and computational efforts, ultimately advancing materials and molecular design.</p>\n<p>Extrapolation in materials science can refer to both the domain (materials space) and range (property values) of the predictive function. It is often used to refer <span>generalization</span> to unseen classes of materials structures and chemical spaces \u2014 for example, training on metals and predicting ceramics or training on artificial molecules and predicting natural products. Here, we address extrapolation in material property values.</p>\n<p>When <span>ood</span> generalization is defined with respect to the materials space, extrapolation often reduces to interpolation. This occurs because test sets tend to remain within the same distribution as the training data representation space <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib18\">18</a>]</cite>. This includes predictive models employing leave-one-cluster-out extrapolation strategies <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib19\">19</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib20\">20</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib17\">17</a>]</cite>, as well as generative approaches designed to achieve <span>ood</span> generalization to structures with different atomic compositions or larger numbers of atoms <cite>[<a href=\"https://arxiv.org/html/2502.05970v1#bib.bib21\">21</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib22\">22</a>, <a href=\"https://arxiv.org/html/2502.05970v1#bib.bib23\">23</a>]</cite>.</p>\n<p>When <span>ood</span> generalization is defined with respect to the range of the predictive function, classical machine learning models face significant challenges in extrapolating property pred...",
      "url": "https://arxiv.org/html/2502.05970v1"
    }
  ]
}