{
  "query": "How do you predict chemical reaction yields for unseen solvents using domain adaptation or transfer learning techniques?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting reaction yields for solvents that were not present in the training set can be done by treating the new solvent domain as a **target domain** and adapting a model that was trained on a large, solvent\u2011diverse **source domain**.  The workflow below follows the strategies reported in recent transfer\u2011learning studies on reaction yields and solvent effects\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a\u3011\u30102\u2020https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577\u3011\u30105\u2020https://www.sciencedirect.com/science/article/pii/S2589004222009336\u3011\u30106\u2020https://link.springer.com/article/10.1186/s13321-024-00815-2\u3011\u30109\u2020https://arxiv.org/abs/2506.07619\u3011.\n\n---\n\n### 1. Assemble a source\u2011domain dataset  \n* Gather a large collection of reactions with **atom\u2011mapped reaction SMILES** and the corresponding **solvent SMILES** (or a descriptor vector).  \n* Include yield values (experimental or high\u2011level calculated, e.g., COSMO\u2011RS) for many solvents (hundreds) so the model learns general solvent\u2011reaction interactions\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a\u3011.  \n\n### 2. Pre\u2011train a deep model on the source data  \n* Use a transformer\u2011based \u201cMolecular Transformer\u201d or a multi\u2011view architecture (e.g., ReaMVP) that ingests the reaction SMILES and solvent SMILES jointly.  \n* Train the model to regress the yield (or \u0394\u0394G\u2021/\u0394\u0394H\u2021) using a standard loss (MSE).  Large\u2011scale pre\u2011training improves generalisation across reaction families\u30106\u2020https://link.springer.com/article/10.1186/s13321-024-00815-2\u3011.  \n\n### 3. Define the target\u2011solvent domain  \n* Collect a **small set** of reactions run in the *unseen* solvent (10\u201150 examples).  \n* If experimental data are scarce, generate approximate yields with a physics\u2011based method (e.g., COSMO\u2011RS) to bootstrap the domain\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a\u3011.  \n\n### 4. Adapt the model to the new solvent  \n* **Fine\u2011tune** the pre\u2011trained network on the target\u2011solvent data while keeping most layers frozen; only the solvent\u2011embedding layer and the final regression head are updated.  \n* Alternatively, apply **domain\u2011adversarial training**: add a solvent\u2011domain classifier that tries to predict whether a sample comes from source or target; back\u2011propagate a gradient\u2011reversal layer so the shared encoder learns solvent\u2011invariant features\u30102\u2020https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577\u3011.  \n\n### 5. (Optional) Use a simple, interpretable model for active transfer learning  \n* Train a shallow random\u2011forest on the same features (reaction fingerprints + solvent descriptors).  \n* Deploy an **active\u2011learning loop**: the model suggests the most uncertain reactions in the new solvent, you run those experiments, add the results, and re\u2011train.  This strategy was shown to boost performance when the source\u2011target gap is large\u30102\u2020https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577\u3011.  \n\n### 6. Evaluate and calibrate  \n* Test the adapted model on a held\u2011out set of reactions in the unseen solvent.  \n* Report MAE (target <\u202f1\u202fkcal\u202fmol\u207b\u00b9 for \u0394\u0394G\u2021 or <\u202f10\u202f% absolute yield error) as achieved in the cited solvent\u2011effect study\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a\u3011.  \n* If performance is insufficient, iterate steps\u202f4\u20135 with more target data or stronger regularisation.  \n\n### 7. Deploy for rapid prediction  \n* Once validated, the model can predict yields for **any new reaction** combined with the unseen solvent simply by providing the reaction SMILES and the solvent SMILES string.  Predictions are near\u2011instantaneous, enabling high\u2011throughput solvent screening or self\u2011driving laboratory optimisation\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a\u3011\u30109\u2020https://arxiv.org/abs/2506.07619\u3011.  \n\nFollowing these steps leverages domain adaptation / transfer learning to extrapolate yield predictions to solvents that were not represented in the original training set, while keeping the workflow data\u2011efficient and interpretable.",
      "url": ""
    },
    {
      "title": "Predicting reaction conditions from limited data through active ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Transfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.</p></section><section><hr/>\n<p>Transfer learning is combined with active learning to discover synthetic reaction conditions in a small-data regime. This strategy is tested on cross-coupling reactions from a high-throughput experimentation dataset and shows promising results.<a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-ga.jpg\"></a></p></section><section><h2>Introduction</h2>\n<p>Computers are becoming increasingly capable of performing high-level chemical tasks.<sup><a href=\"#cit1\">1\u20134</a></sup> Machine learning approaches have demonstrated viable retrosynthetic analyses,<sup><a href=\"#cit5\">5\u20137</a></sup> product prediction,<sup><a href=\"#cit8\">8\u201311</a></sup> reaction condition suggestion,<sup><a href=\"#cit12\">12\u201316</a></sup> prediction of stereoselectivity,<sup><a href=\"#cit17\">17\u201320</a></sup> regioselectivity,<sup><a href=\"#cit19\">19,21\u201324</a></sup> and reaction yield<sup><a href=\"#cit25\">25,26</a></sup> and optimization of reaction conditions.<sup><a href=\"#cit27\">27\u201330</a></sup> These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (<em>e.g.</em> reaction conditions) within a particular domain relate to an outcome (<em>e.g.</em> yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.</p>\n<p>Expert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain,<sup><a href=\"#cit31\">31</a></sup> emulates a tactic commonly employed by human chemists.</p>\n<p>Transfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain.<sup><a href=\"#cit31\">31,32</a></sup> Models are first created using the source data, then transferred to the target domain using various algorithms.<sup><a href=\"#cit19\">19,33\u201335</a></sup> For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (<em>e.g.</em>, descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible.<sup><a href=\"#cit31\">31,36</a></sup> Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data.<sup><a href=\"#cit37\">37</a></sup> The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.</p>\n<p>Another machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data.<sup><a href=\"#cit38\">38,39</a></sup> Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental results. Based on this analogy, reaction optimization<sup><a href=\"#cit27\">27,28</a></sup> and reaction condition identification<sup><a href=\"#cit40\">40</a></sup> have been demonstrated to benefit from active learning. However, these prior works initiate exploration with randomly selected data points (<a href=\"#fig1\">Fig. 1A</a>) which does not leverage prior knowledge, and therefore does not reflect how expert chemists initiate exploration. Initial search directed by transfer learning could identify productive regions early on, which in turn will help build more useful models for subsequent active learning steps.</p>\n<figure><h3>Fig. 1. Workflow of (A) previous active learning studies and (B) this work. Distinctions that arise from the different problem setting and incorporation of transfer learning are highlighted bold in (B).</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-f1.jpg\"></a></p>\n</figure><p>To align transfer and active learning closer to how expert chemists develop new reactions, appropriate chemical reaction data is necessary.<sup><a href=\"#cit41\">41</a></sup> Available datasets<sup><a href=\"#cit42\">42</a></sup> that are often used for machine learning are overrepresented by positive reactions, failing to reflect reactions with negative outcomes. On the other hand, reaction condition screening data of methodology reports\u2014which chemists often refer to\u2014only constitute a sparse subset of possible reagent combinations, making it hard for machine learning algorithms to extract meaningful knowledge.<sup><a href=\"#cit43\">43</a></sup></p>\n<p>High-throughput experimentation<sup><a href=\"#cit44\">44\u201346</a></sup> (HTE) data can fill this gap. HTE provides reaction data<sup><a href=\"#cit16\">16,25,27,47,48</a></sup> with reduced variations in outcome due to systematic experimentation. Pd-catalyzed coupling data was therefore collected from reported work using nanomole scale HTE in 1536 well plates.<sup><a href=\"#cit49\">49\u201351</a></sup> In the current work, subsets of this data, classified by nucleophile type as shown in <a href=\"#fig2\">Fig. 2A</a>, were selected to a dataset size of approximately 100 datapoints, which captured both positive and negative reaction performance.</p>\n<figure><h3>Fig. 2. (A) Structure of reactions in the dataset. A total of 1220 reactions across 10 t...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577"
    },
    {
      "title": "Machine learning from quantum chemistry to predict experimental ...",
      "text": "<div><div><p><span></span></p><div><div><p> DOI:\u00a0<a href=\"https://doi.org/10.1039/D3SC05353A\">10.1039/D3SC05353A</a>\n(Edge Article)\n<span><a href=\"https://doi.org/10.1039/2041-6539/2010\">Chem. Sci.</a></span>, 2024, <strong>15</strong>, 2410-2424</p></div><p><span>Received \n 10th October 2023\n </span><span>, Accepted 4th January 2024</span></p><p>First published on 10th January 2024</p><hr/><div><h2>Abstract</h2><p>Fast and accurate prediction of solvent effects on reaction rates are crucial for kinetic modeling, chemical process design, and high-throughput solvent screening. Despite the recent advance in machine learning, a scarcity of reliable data has hindered the development of predictive models that are generalizable for diverse reactions and solvents. In this work, we generate a large set of data with the COSMO-RS method for over 28000 neutral reactions and 295 solvents and train a machine learning model to predict the solvation free energy and solvation enthalpy of activation (\u0394\u0394<span>G</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small>, \u0394\u0394<span>H</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small>) for a solution phase reaction. On unseen reactions, the model achieves mean absolute errors of 0.71 and 1.03 kcal mol<small><sup>\u22121</sup></small> for \u0394\u0394<span>G</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small> and \u0394\u0394<span>H</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small>, respectively, relative to the COSMO-RS calculations. The model also provides reliable predictions of relative rate constants within a factor of 4 when tested on experimental data. The presented model can provide nearly instantaneous predictions of kinetic solvent effects or relative rate constants for a broad range of neutral closed-shell or free radical reactions and solvents only based on atom-mapped reaction SMILES and solvent SMILES strings.</p></div><hr/>\n \n \n <h2><span>1 Introduction</span></h2>\n <p><span>Accurate prediction of reaction rates is essential for modeling a variety of chemical kinetic systems such as pyrolysis,<a href=\"#cit1\"><sup><span>1,2</span></sup></a> polymerization,<a href=\"#cit3\"><sup><span>3</span></sup></a> oxidative degradation,<a href=\"#cit4\"><sup><span>4,5</span></sup></a> and atmospheric chemistry.<a href=\"#cit6\"><sup><span>6</span></sup></a> Detailed kinetic models enable one to predict key products, identify major kinetic pathways, and optimize reaction conditions for complex chemical systems. Kinetic mechanisms often involve hundreds to tens of thousands of elementary reactions,<a href=\"#cit7\"><sup><span>7</span></sup></a> and a fast, high-throughput method to estimate reaction rates is thus needed. <span>Ab initio</span> methods like quantum mechanics/molecular mechanics (QM/MM) can provide accurate predictions of rate constants, but their high computational cost has been a major limiting factor for large-scale, automated predictions. As more kinetic data become available, data-driven approaches such as linear group contribution,<a href=\"#cit8\"><sup><span>8\u201310</span></sup></a> decision tree based rate rules,<a href=\"#cit11\"><sup><span>11,12</span></sup></a> and machine learning (ML) models<a href=\"#cit13\"><sup><span>13\u201319</span></sup></a> have emerged as more popular choices for estimating kinetic parameters. Several ML models<a href=\"#cit15\"><sup><span>15\u201317</span></sup></a> have successfully predicted barrier heights and rate constants of diverse gas phase reactions only based on readily available 2D information (<span>e.g.</span> SMILES strings) of reactants and products. However, such data-driven models for liquid/solution phase reactions have been lightly investigated with limited applicability,<a href=\"#cit20\"><sup><span>20</span></sup></a> and most approaches rely on the <span>ab initio</span> methods with either implicit or explicit solvation models.<a href=\"#cit21\"><sup><span>21,22</span></sup></a></span></p><p>Solvents can have significant impacts on reaction rates and outcomes, and it is crucial to accurately predict these kinetic solvent effects. Recent research efforts have been devoted to employing ML (<span>e.g.</span> deep neural network) for free energy predictions of condensed phase reactions.<a href=\"#cit15\"><sup><span>15,18,19,23\u201328</span></sup></a> Many of these studies<a href=\"#cit18\"><sup><span>18,19,23\u201325,28</span></sup></a> combine the ML models with semi-empirical or lower-level QM/MM methods to obtain the energy predictions that match the accuracy of higher-level QM/MM methods. For example, G\u00f3mez-Flores <span>et al.</span><a href=\"#cit19\"><sup><span>19</span></sup></a> used a ML approach to predict the energy difference between the density functional tight-binding model and other higher level QM methods for a thiol-disulfide exchange reaction in water. In a study by Pan <span>et al.</span>,<a href=\"#cit18\"><sup><span>18</span></sup></a> a ML model was trained to reproduce <span>ab initio</span> QM/MM potentials in free energy simulations for the aqueous Menshutkin reaction between ammonia and chloromethane. Farrar and Grayson<a href=\"#cit28\"><sup><span>28</span></sup></a> employed ML models to predict DFT-quality activation barriers for various nitro-Michael addition reactions in toluene based on the features generated from semi-empirical methods. These approaches, however, require semi-empirical QM/MM steps that are less suitable for instantaneous, automatic rate predictions. Furthermore, their models are limited to a single solvent and need the 3D coordinates or QM features of reactants and transition states as inputs, which are not readily available.</p>\n <p>The ML models by Jorner <span>et al.</span><a href=\"#cit26\"><sup><span>26</span></sup></a> and by Heid and Green<a href=\"#cit15\"><sup><span>15</span></sup></a> are the few cases that can predict reaction properties in multiple solvents only based on the 2D structural information of molecules. Jorner <span>et al.</span><a href=\"#cit26\"><sup><span>26</span></sup></a> employed a Gaussian process regression model and compared several 2D structural features to predict the barrier height of 443 S<small><sub>N</sub></small>Ar reactions in different solvents. In their work, the best accuracy was reached by adopting the BERT<a href=\"#cit29\"><sup><span>29</span></sup></a> reaction fingerprint. Heid and Green,<a href=\"#cit15\"><sup><span>15</span></sup></a> on the other hand, used the condensed graph of reaction (CGR) as an input reaction representation for a graph convolutional neural network (GCNN). They applied the CGR GCNN model to the same S<small><sub>N</sub></small>Ar data set and were able to achieve better barrier height predictions compared to the other models that used the BERT fingerprint or different reaction representations. While these models can provide fast kinetic estimations for solution-phase reactions at a low computational cost, only one reaction family was considered with a relatively small training set. A larger data set that contains more diverse types of reactions and solvents is needed in order to train a more generalized model for kinetic solvent effect predictions. Moreover, both models used fixed descriptors to represent solvents, but prior studies<a href=\"#cit15\"><sup><span>15,30,31</span></sup></a> revealed that the learned molecular representations based on a graph convolutional approach outperform fixed molecular descriptors in many property prediction tasks.</p>\n <p>In this study, we present a ML model that can predict kinetic solvent effects for a wide range of neutral reactions and solvents only based on atom-mapped reaction SMILES and solvent SMILES strings. More precisely, the model predicts the solvation free energy and solvation enthalpy of activation (\u0394\u0394<span>G</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small>, \u0394\u0394<span>H</span><small><sup>\u2021</sup></small><small><sub>solv</sub></small>) for a reaction\u2013solvent pair, which can be used to estimate a relative rate con...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc05353a"
    },
    {
      "title": "A transfer learning approach for reaction discovery in small data ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/iscience/vol/25/issue/7\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><div><h2>Highlights</h2><div><ul><li><span>\u2022</span><span><p>Dual pronged transfer learning, both to generate and predict yields of new molecules</p></span></li><li><span>\u2022</span><span><p>Demonstrated the utility for an important family of deoxyfluorination of alcohols</p></span></li><li><span>\u2022</span><span><p>Applicable for practically more likely situations with relatively smaller data</p></span></li><li><span>\u2022</span><span><p>Extendable to other reaction manifolds to facilitate expedited reaction discovery</p></span></li></ul></div></div><div><h2>Summary</h2><div><p>Sustainable practices in chemical sciences can be better realized by adopting interdisciplinary approaches that combine the advantages of machine learning (ML) on the initially acquired small data in reaction discovery. Developing new reactions generally remains heuristic and even time and resource intensive. For instance, synthesis of fluorine-containing compounds, which constitute \u223c20% of the marketed drugs, relies on deoxyfluorination of abundantly available alcohols. Herein, we demonstrate the use of a recurrent neural network-based deep generative model built on a library of just 37 alcohols for effective learning and exploration of the chemical space. The proof-of-concept ML model is able to generate good quality, synthetically accessible, higher-yielding novel alcohol molecules. This protocol would have superior utility for deployment into a practical reaction discovery pipeline.</p></div></div></div><ul><li></li><li></li></ul><div><h2>Subject areas</h2><p><span>Artificial intelligence</span></p><p><span>Computational chemistry</span></p><p><span>Functional group chemistry</span></p><p><span>Modeling chemical reactivity</span></p></div><section><h2>Data and code availability</h2><div><ul><li><span></span><span></span></li><li><span></span><span></span></li><li><span></span><span><p>Any additional information required to reanalyze the data reported in this paper is available from the <a href=\"#sec5.2.1\"><span><span>lead contact</span></span></a> upon request.</p></span></li></ul></div></section><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2022 The Authors.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2589004222009336"
    },
    {
      "title": "Prediction of chemical reaction yields with large-scale multi-view pre ...",
      "text": "Prediction of chemical reaction yields with large-scale multi-view pre-training | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Account]()\n# Prediction of chemical reaction yields with large-scale multi-view pre-training\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:25 February 2024\n* Volume\u00a016, article\u00a0number22, (2024)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://link.springer.com/content/pdf/10.1186/s13321-024-00815-2.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://link.springer.com/journal/13321)[Aims and scope](https://link.springer.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nPrediction of chemical reaction yields with large-scale multi-view pre-training\n[Download PDF](https://link.springer.com/content/pdf/10.1186/s13321-024-00815-2.pdf)\n* [Runhan Shi](#auth-Runhan-Shi-Aff1)[1](#Aff1),\n* [Gufeng Yu](#auth-Gufeng-Yu-Aff1)[1](#Aff1),\n* [Xiaohong Huo](#auth-Xiaohong-Huo-Aff2)[2](#Aff2)&amp;\n* \u2026* [Yang Yang](#auth-Yang-Yang-Aff1)[1](#Aff1)Show authors\n* 7923Accesses\n* 24Citations\n* 1Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1186/s13321-024-00815-2/metrics)\n## Abstract\nDeveloping machine learning models with high generalization capability for predicting chemical reaction yields is of significant interest and importance. The efficacy of such models depends heavily on the representation of chemical reactions, which has commonly been learned from SMILES or graphs of molecules using deep neural networks. However, the progression of chemical reactions is inherently determined by the molecular 3D geometric properties, which have been recently highlighted as crucial features in accurately predicting molecular properties and chemical reactions. Additionally, large-scale pre-training has been shown to be essential in enhancing the generalization capability of complex deep learning models. Based on these considerations, we propose the Reaction Multi-View Pre-training (ReaMVP) framework, which leverages self-supervised learning techniques and a two-stage pre-training strategy to predict chemical reaction yields. By incorporating multi-view learning with 3D geometric information, ReaMVP achieves state-of-the-art performance on two benchmark datasets. Notably, the experimental results indicate that ReaMVP has a significant advantage in predicting out-of-sample data, suggesting an enhanced generalization ability to predict new reactions. Scientific Contribution: This study presents the ReaMVP framework, which improves the generalization capability of machine learning models for predicting chemical reaction yields. By integrating sequential and geometric views and leveraging self-supervised learning techniques with a two-stage pre-training strategy, ReaMVP achieves state-of-the-art performance on benchmark datasets. The framework demonstrates superior predictive ability for out-of-sample data and enhances the prediction of new reactions.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1186%2Fs13321-024-00818-z/MediaObjects/13321_2024_818_Fig1_HTML.png)\n### [Improving chemical reaction yield prediction using pre-trained graph neural networks](https://link.springer.com/10.1186/s13321-024-00818-z?fromPaywallRec=true)\nArticleOpen access01 March 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1186%2Fs13321-024-00805-4/MediaObjects/13321_2024_805_Figa_HTML.png)\n### [Enhancing chemical synthesis: a two-stage deep neural network for predicting feasible reaction conditions](https://link.springer.com/10.1186/s13321-024-00805-4?fromPaywallRec=true)\nArticleOpen access24 January 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-55082-4/MediaObjects/41467_2024_55082_Fig1_HTML.png)\n### [Multi-channel learning for integrating structural hierarchies into context-dependent molecular representation](https://link.springer.com/10.1038/s41467-024-55082-4?fromPaywallRec=true)\nArticleOpen access06 January 2025\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Biochemical Reaction Network](https://link.springer.com/subjects/biochemical-reaction-network)\n* [Process Chemistry](https://link.springer.com/subjects/process-chemistry)\n* [Reactive Precursor](https://link.springer.com/subjects/reactive-precursor)\n* [Reaction Kinetics](https://link.springer.com/subjects/reaction-kinetics)\n* [Reaction Mechanisms](https://link.springer.com/subjects/reaction-mechanisms)\n* [Structure Prediction](https://link.springer.com/subjects/structure-prediction)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nThe prediction of chemical reaction yields, which refer to the percentage of product formed in relation to the reactant consumed, is an important research topic in organic chemistry [[1](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR1),[2](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR2)]. In the field of organic synthesis, chemists often synthesize a target molecule through several or a dozen reaction steps [[3](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR3)]. Consequently, low-yield reactions in the intermediate steps can have a negative impact on the total yield of the synthesis route due to the cumulative effect of each step. The estimation of chemical reaction yields plays an important role in guiding synthetic chemists to choose appropriate molecular synthesis routes, particularly in the case of identifying highly active and selective catalysts efficiently. Traditionally, chemists depend on empirical predictions or specific wet experiments to determine yields, which require extensive domain knowledge and are both time-consuming and labor-intensive. Therefore, data-driven machine learning techniques are needed to provide an efficient alternative.\nIt remains a great challenge to accurately predict chemical reaction yields due to the complexity of reaction space and the diverse factors that influence chemical experiments [[4](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR4),[5](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR5)]. To develop machine learning-based approaches, it is crucial to establish effective methods for representing chemical reactions. Conventional studies typically rely on feature engineering to represent chemical reactions using fingerprints or descriptors. This involves creating customized descriptors based on domain expertise to capture molecular, atomic, vibrational, or physicochemical properties [[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](#ref-CR11),[12](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR12)]. Some researchers derived descriptors from circular substructures present in the simplified molecular-input line-entry system (SMILES) [[13](https://link.springer.com/article/10.1186/s13321-024-00815-2#ref-CR13)] strings of the reactions [[14](https:/...",
      "url": "https://link.springer.com/article/10.1186/s13321-024-00815-2"
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "Prediction of chemical reaction yields using deep learning",
      "text": "<div><div>\n<h2>We apologize for the inconvenience...</h2>\n<p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p>\n<p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>\n<p>\n<a href=\"https://ioppublishing.org/contacts/\">https://ioppublishing.org/contacts/</a>\n</p>\n</div></div>",
      "url": "https://iopscience.iop.org/article/10.1088/2632-2153/abc81d"
    },
    {
      "title": "Transfer learning enables the molecular transformer to predict regio",
      "text": "<div><div>\n \n <div><h2>Introduction</h2><div><p>Organic synthesis is a complex problem-solving task in which the vast knowledge accumulated in the field of organic chemistry is used to create new molecules, starting from simple commercially available building blocks<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR1\">1</a></sup>. Because of its complexity, organic synthesis is believed to be one of the main bottlenecks in pharmaceutical research and development<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR2\">2</a></sup>, and having accurate models to predict reaction outcome could boost chemists\u2019 productivity by reducing the number of experiments to perform.</p><p>Machine learning has long been present in the chemical domain, tackling challenges than range, for example for quantitative structure\u2013activity relationship predictions<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR3\">3</a></sup>, virtual screening<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR4\">4</a></sup> and quantum chemistry<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR5\">5</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR6\">6</a></sup>. Enabled by algorithmic advances in deep learning<sup><a href=\"#ref-CR7\">7</a>,<a href=\"#ref-CR8\">8</a>,<a href=\"#ref-CR9\">9</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR10\">10</a></sup> and the availability of large reaction data sets<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR11\">11</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR12\">12</a></sup>, reaction prediction methods have emerged in recent years<sup><a href=\"#ref-CR13\">13</a>,<a href=\"#ref-CR14\">14</a>,<a href=\"#ref-CR15\">15</a>,<a href=\"#ref-CR16\">16</a>,<a href=\"#ref-CR17\">17</a>,<a href=\"#ref-CR18\">18</a>,<a href=\"#ref-CR19\">19</a>,<a href=\"#ref-CR20\">20</a>,<a href=\"#ref-CR21\">21</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR22\">22</a></sup>. Those reaction prediction methods can be divided into two categories<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR23\">23</a></sup>, bond change prediction methods using graph neural networks<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR14\">14</a>,<a href=\"#ref-CR16\">16</a>,<a href=\"#ref-CR17\">17</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR18\">18</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR22\">22</a></sup> and product SMILES generation using sequence-2-sequence models<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR15\">15</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR19\">19</a></sup>.</p><p>Reaction prediction tasks are typically evaluated on the USPTO_MIT benchmark<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR14\">14</a></sup>, which does not contain molecules with defined stereocenters. Currently, the best prediction algorithm in terms of performance is the Molecular Transformer<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR10\">10</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR19\">19</a></sup>. The architecture is based on the ground-breaking work by Vaswani et al.<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR10\">10</a></sup>, which revolutionised the field of neural machine translation, where sentences in one language are translated into another language. In contrast, for reaction prediction, the model learns to translate the precursors\u2019 Simplified molecular-input line-entry system (SMILES)<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR24\">24</a></sup> representation into the product SMILES.</p><p>The Molecular Transformer can be accessed for free through the IBM RXN for Chemistry platform<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR25\">25</a></sup>. Compared to other methods, such as graph neural networks-based ones, the advantages of the Molecular Transformer approaches are that they do not require mapping between the product and reactant atoms in the training<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR26\">26</a></sup> and inputs can contain stereochemistry. In fact, sequence-2-sequence approaches, like the Molecular Transformer<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR10\">10</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR19\">19</a></sup>, are currently the only large-scale reaction prediction approaches capable of handling stereochemistry. Stereochemistry is systematically avoided in graph-based methods, as the connection table and adjacency matrix of two stereoisomers is identical. Although stereoselectivity can theoretically be predicted by the Molecular Transformers<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR19\">19</a></sup>, it is one of their most significant weaknesses because of the lack of clean training data. To date, their performance on predicting specific stereochemical reactions has not been investigated.</p><p>In this work, we investigate the adaptation of the Molecular Transformer to correctly predict regio- and stereoselective reactions. As study case we focus on carbohydrates, a class of molecules for which the stereochemistry and the high degree of functionalization are key reactivity factors. Carbohydrate chemistry is essential for accessing complex glycans that are used as tool compounds to investigate fundamental biological processes such as protein glycosylation<sup><a href=\"#ref-CR27\">27</a>,<a href=\"#ref-CR28\">28</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR29\">29</a></sup>, as well as for the preparation of synthetic vaccines<sup><a href=\"#ref-CR30\">30</a>,<a href=\"#ref-CR31\">31</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR32\">32</a></sup>. Predicting the outcome of carbohydrate transformations, such as regioselective protection/deprotection of multiple hydroxyl groups or the stereospecificity of glycosylation reactions, is a very difficult task even for experienced carbohydrate chemists<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR33\">33</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR34\">34</a></sup>, implying that this field of research might particularly benefit from computer-assisted reaction prediction tools.</p><p>First, we investigate transfer learning with a specialized subset of reactions as a means to adapt the Molecular Transformer to achieve high performance on carbohydrate reactions. Transfer learning, where a model is trained on a task with abundant data and either simultaneously trained or subsequently fine-tuned on another task with less data available<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR35\">35</a></sup>, has recently led to significant advancements in Natural Language Processing<sup><a href=\"#ref-CR36\">36</a>,<a href=\"#ref-CR37\">37</a>,<a href=\"#ref-CR38\">38</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR39\">39</a></sup>. For instance, it has been used to improve translation performance in low-resource languages<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR36\">36</a></sup>. More recently, unsupervised pretraining transfer learning strategies have successfully been applied to sequence-2-sequence models<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR37\">37</a>,<a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR40\">40</a></sup>. In the chemical domain, transfer learning has enabled the development of accurate neural network potential for quantum mechanical calculations<sup><a href=\"https://www.nature.com/articles/s41467-020-18671-7#ref-CR41\">41</a></sup> and shows gre...",
      "url": "https://www.nature.com/articles/s41467-020-18671-7"
    },
    {
      "title": "Spectroscopy-Guided Deep Learning Predicts Solid\u2013Liquid Surface Adsorbate Properties in Unseen Solvents",
      "text": "![](https://d.adroll.com/cm/b/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/g/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/index/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/n/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/o/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/outbrain/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/pubmatic/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/r/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/taboola/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/triplelift/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/x/out?adroll_fpc=ff8b72eb7a295a66281c0d03561c4051-1720306675079&pv=14480128167.775885&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Fjacs.3c10921&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)\n\nRecently Viewed [close modal](javascript:void(0))\n\nRecently Viewed\n\n#### You have not visited any articles yet, Please visit some articles to see contents here.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nYou\u2019ve supercharged your research process with ACS and Mendeley!\n\nContinue\n\n###### STEP 1:\n\nLogin with ACS IDLogged in SuccessClick to create an ACS ID\n\n###### STEP 2:\n\nLogin with MendeleyLogged in Success [Create a Mendeley account](https://id.elsevier.com/as/authorization.oauth2?state=c33c27125763433d4d32a15accaacc18&prompt=login&scope=openid%20email%20profile%20els_auth_info&authType=SINGLE_SIGN_IN&response_type=code&platSite=MDY%2Fmendeley&redirect_uri=https%3A%2F%2Fwww.mendeley.com%2Fcallback%2F&client_id=MENDELEY)\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease login with your ACS ID before connecting to your Mendeley account.\n\nLogin with ACS ID\n\nMENDELEY PAIRING EXPIREDReconnect\n\nYour Mendeley pairing has expired. Please reconnect\n\n![Figure 1](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921)![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n\n[Download Hi-Res Image](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) [Download to MS-PowerPoint](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) [**Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/jacs.3c10921&href=/doi/full/10.1021/jacs.3c10921) _J. Am. Chem. Soc._ 2024, 146, 1, 811-823\n\n[ADVERTISEMENT](http://acsmediakit.org)\n\n[RETURN TO ISSUE](https://pubs.acs.org/toc/jacsat/146/1) [PREV](https://pubs.acs.org/doi/10.1021/jacs.3c10864) Article [NEXT](https://pubs.acs.org/doi/10.1021/jacs.3c10963)\n\n[![Journal Logo](https://pubs.acs.org/doi/full/10.1021/specs/products/achs/releasedAssets/images/loading/loader-128b5db1cc3a83761a15cf2e5c9b452d.gif)](https://pubs.acs.org/journal/jacsat)\n\n[Get e-Alerts](https://pubs.acs.org/doi/full/10.1021/jacs.3c10921) close\n\n# Spectroscopy-Guided Deep Learning Predicts Solid\u2013Liquid Surface Adsorbate Properties in Unseen Solvents\n\n- Wenjie Du\n\n\n\n\nWenjie Du\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Wenjie Du](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Wenjie++Du)\n\n- ,\n- Fenfen Ma\n\n\n\n\nFenfen Ma\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nGusu Laboratory of Materials, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Fenfen Ma](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Fenfen++Ma)\n\n- ,\n- Baicheng Zhang\n\n\n\n\nBaicheng Zhang\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\n\n\nMore by [Baicheng Zhang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Baicheng++Zhang)\n\n\n\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-1899-028X](https://orcid.org/0000-0002-1899-028X)\n\n- ,\n- Jiahui Zhang\n\n\n\n\nJiahui Zhang\n\n\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Jiahui Zhang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Jiahui++Zhang)\n\n- ,\n- Di Wu\n\n\n\n\nDi Wu\n\n\n\n\n\nSchool of Software Engineering, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu 215123, China\n\n\n\n\n\nMore by [Di Wu](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Di++Wu)\n\n- ,\n- Edward Sharman\n\n\n\n\nEdward Sharman\n\n\n\n\n\nDepartment of Neurology, University of California, Irvine, California 92697, United States\n\n\n\n\n\nMore by [Edward Sharman](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Edward++Sharman)\n\n- ,\n- Jun Jiang **\\***\n\n\n\n\nJun Jiang\n\n\n\n\n\nKey Laboratory of Precision and Intelligent Chemistry, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\nSchool of Chemistry and Materials Science, University of Science and Technology of China, Hefei, Anhui 230026, China\n\n\n\n**\\*** Email: [jiangj1@ustc.edu.cn](mailto:jiangj1@ustc.edu.cn)\n\nMore by [Jun Jiang](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Jun++Jiang)\n\n\n\n![Orcid](https://pubs.acs.org/products/a...",
      "url": "https://pubs.acs.org/doi/full/10.1021/jacs.3c10921"
    }
  ]
}