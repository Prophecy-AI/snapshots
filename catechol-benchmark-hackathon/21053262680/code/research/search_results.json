{
  "query": "How do you implement a true Graph Neural Network for molecular property prediction using PyTorch Geometric with atom and bond features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Based on the TeachOpenCADD GNN tutorial and the\u202fGNN\u2011QM9\u202fGitHub implementation, a \u201ctrue\u201d graph neural network for molecular property prediction can be built in PyTorch\u202fGeometric by explicitly feeding atom\u2011level node features and bond\u2011level edge attributes** ([TeachOpenCADD](https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html); [GNN\u2011QM9](https://github.com/niayeshzrf/gnn-qm9)).\n\n1. **Install the required libraries**  \n   ```bash\n   conda install -c conda-forge rdkit pytorch torchvision torchaudio\n   pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv\n   ```  \n   (RDKit provides atom/bond chemistry; PyTorch\u202fGeometric supplies GNN layers.)  \n\n2. **Create the molecular graph with atom and bond features**  \n   * Load the raw\u202f`.sdf`\u202for other format with RDKit.  \n   * For each atom, compute a feature vector (e.g., atomic number, degree, hybridization, aromaticity).  \n   * For each bond, compute edge attributes (e.g., bond type, conjugation, whether it is in a ring).  \n   * Build `edge_index` (source\u2011target pairs) and `edge_attr` tensors, then wrap everything in a `torch_geometric.data.Data` object.  \n   * Optionally store many graphs in an `InMemoryDataset` for fast loading.  \n   (All of these steps are illustrated in the GNN\u2011QM9 repo) ([GNN\u2011QM9](https://github.com/niayeshzrf/gnn-qm9)).\n\n3. **Define a GNN model that consumes both node and edge features**  \n   ```python\n   import torch.nn as nn\n   from torch_geometric.nn import GINConv, global_mean_pool\n\n   class GIN(torch.nn.Module):\n       def __init__(self, hidden_dim=128):\n           super().__init__()\n           # MLP used inside each GINConv\n           self.mlp = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n                                    nn.ReLU(),\n                                    nn.Linear(hidden_dim, hidden_dim))\n           self.convs = nn.ModuleList([\n               GINConv(self.mlp), GINConv(self.mlp), GINConv(self.mlp)\n           ])\n           self.pool = global_mean_pool\n           self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n                                   nn.ReLU(),\n                                   nn.Dropout(0.2),\n                                   nn.Linear(hidden_dim, 1))   # regression output\n       def forward(self, x, edge_index, edge_attr, batch):\n           for conv in self.convs:\n               x = conv(x, edge_index)          # edge_attr can be incorporated via custom message passing if needed\n               x = nn.functional.relu(x)\n           x = self.pool(x, batch)              # graph\u2011level embedding\n           return self.fc(x)\n   ```  \n   This follows the 3\u2011layer GIN architecture used in the QM9 example and the GCN/GIN definitions from the TeachOpenCADD tutorial ([TeachOpenCADD](https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html); [GNN\u2011QM9](https://github.com/niayeshzrf/gnn-qm9)).\n\n4. **Set up the training loop**  \n   * Split the dataset into train/val/test `DataLoader`s (`torch_geometric.loader.DataLoader`).  \n   * Use an optimizer such as `Adam` and a regression loss like `MSELoss`.  \n   * For each epoch, forward\u2011pass the batched graphs, compute loss, back\u2011propagate, and step the optimizer.  \n   * Track validation MAE/RMSE to monitor performance.  \n   (The tutorial walks through exactly this workflow.) ([TeachOpenCADD](https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html))\n\n5. **Evaluate and use the trained model**  \n   * After training, evaluate on the held\u2011out test set to report final metrics.  \n   * For inference on a new molecule, repeat step\u202f2 to obtain its `Data` object, then call `model(x, edge_index, edge_attr, batch)` to obtain the predicted property.  \n\nThese five steps give a complete, reproducible pipeline that leverages true atom\u2011 and bond\u2011level graph representations for molecular property prediction with PyTorch\u202fGeometric.",
      "url": ""
    },
    {
      "title": "T035 \u00b7 GNN-based molecular property prediction",
      "text": "[Skip to content](https://projects.volkamerlab.org/projects.volkamerlab.org#talktorials/T035_graph_neural_networks)\n\n# T035 \u00b7 GNN-based molecular property prediction [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#talktorials-t035-graph-neural-networks--page-root)\n\n**Note**: This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n\nAuthors:\n\n- Paula Linh Kramer, 2022, [Volkamer Lab](https://volkamerlab.org/), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University\n\n\n## Aim of this talktorial [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Aim-of-this-talktorial)\n\nIn this tutorial, we will first explain the basic concepts of graph neural networks (GNNs) and present two different GNN architectures. We apply our neural networks to the `QM9` dataset, which is a dataset containing small molecules. With this dataset, we want to predict molecular properties. We demonstrate how to train and evaluate GNNs step by step using PyTorch Geometric.\n\n### Contents in _Theory_ [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Contents-in-Theory)\n\n- GNN Tasks\n\n- Message Passing\n\n- Graph Convolutional Network (GCN)\n\n- Graph Isomorphism Network (GIN)\n\n- Training a GNN\n\n- Applications of GNNs\n\n\n### Contents in _Practical_ [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Contents-in-Practical)\n\n- Dataset\n\n- Defining a GCN and GIN\n\n- Training a GNN\n\n- Evaluating the model\n\n\n### References [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#References)\n\n- Articles:\n\n  - Atz, Kenneth, Francesca Grisoni, and Gisbert Schneider. _Geometric Deep Learning on Molecular Representations_, [Nature Machine Intelligence 3.12 (2021): 1023-1032](https://arxiv.org/pdf/2107.12375.pdf)\n\n  - Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. _How Powerful are Graph Neural Networks?_, [International Conference on Learning Representations (ICLR 2019)](https://arxiv.org/abs/1810.00826v3)\n\n  - Welling, Max, and Thomas N. Kipf. _Semi-supervised classification with graph convolutional networks_, [International Conference on Learning Representations (ICLR 2017)](https://arxiv.org/pdf/1609.02907.pdf)\n\n  - Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. _Neural Message Passing for Quantum Chemistry_, [International conference on machine learning. PMLR, 2017](https://arxiv.org/pdf/1704.01212.pdf)\n- Blog posts:\n\n  - Maxime Labonne, _Graph Convolutional Networks: Introduction to GNNs_, [Maxime Labonne](https://mlabonne.github.io/blog/intrognn/)\n\n  - Maxime Labonne, _GIN: How to Design the Most Powerful Graph Neural Network_, [Maxime Labonne](https://mlabonne.github.io/blog/gin/)\n\n  - Vortana Say, _How To Save and Load Model In PyTorch With A Complete Example_, [towardsdatascience](https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee)\n\n  - Michael Bronstein, _Expressive power of graph neural networks and the Weisfeiler-Lehman test_, [towardsdatascience](https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)\n\n  - Benjamin Sanchez-Lengeling, Emily Reif, _A Gentle Introduction to Graph Neural Networks_, [Distill](https://distill.pub/2021/gnn-intro/)\n- Tutorials:\n\n  - _Pytorch Geometric Documentation_, [Colab Notebooks and Video Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)\n\n  - _Pytorch Geometric Documentation_, [Introduction by Example](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#learning-methods-on-graphs)\n\n## Theory [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Theory)\n\n### Graph Neural Networks [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Graph-Neural-Networks)\n\nThere are several ways to represent molecules which are explained and discussed in **Talktorial T033**. If we work with molecules, one intuitive approach to apply deep learning to certain tasks is to make use of the graph structure of molecules. Graph neural networks can directly work on given graphs. Molecules can easily be represented as a graph, as seen in Figure 1. Given a graph \\\\(G=(V, E)\\\\), \\\\(V\\\\) describes the vertices or nodes. In molecular graphs, a node\n\\\\(v\\_i \\\\in \\\\mathbb{R}^{d\\_v}\\\\) represents an atom. Nodes can have \\\\(d\\_v\\\\) different features, such as atomic number and chirality. Edges usually correspond to covalent bonds between the atoms. Each edge \\\\(e\\_{ij} \\\\in \\\\mathbb{R}^{d\\_e}\\\\) is described by \\\\(d\\_e\\\\) number of features, which usually represent the bond type. A graph neural network is a network consisting of learnable and differentiable functions that are invariant for graph permutations. Graph neural networks consist of\nso-called message-passing layers which will be explained in more detail below, followed by more specific explanations of two different GNN architectures.\n\nFigure 1: Molecular graph overview. Figure taken from \\[1\\]\n\n#### GNN Tasks [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#GNN-Tasks)\n\nWe can perform different tasks with a GNN:\n\n- Graph-level tasks: one application would be to predict a specific property of the entire graph. This can be a classification task such as toxicity prediction or a regression task. In this tutorial, we will implement a regression task to predict molecular properties. Another graph-level task would be to predict entirely new graphs/molecules. This is especially relevant in the area of drug discovery, where new drug candidates are of interest.\n\n- Node-level tasks: we can predict a property of a specific node in the graph, e.g. the atomic charges of each atom. We could also predict a new node to be added to the graph. This is often done for molecule generation, where we want to add multiple atoms to form new molecules one after the other.\n\n- Edge-level tasks: we can predict edge properties, e.g. intramolecular forces between atoms, or a new edge in the graph. In the molecule generation context, we want to predict potential bonds between the atoms. Edge prediction can also be used to infer connections/interactions e.g. in a gene regulatory network.\n\n\n#### Message Passing [\u00b6](https://projects.volkamerlab.org/projects.volkamerlab.org\\#Message-Passing)\n\nInstead of MLP layers in standard neural networks, GNNs have message-passing layers, where we collect information about the neighboring nodes. For each node \\\\(v\\\\), we look at the direct neighbors \\\\(N(v)\\\\) and gather information. Then all the information is aggregated, for example with summation. Then we update the node \\\\(v\\\\) with the aggregated messages. If we perform this aggregation and combining, each node contains the information about the direct neighbors (1-hop). If we repeat\nthis \\\\(n\\\\) times, we aggregate information about the \\\\(n\\_{th}\\\\) closest neighbors (\\\\(n\\\\) -hop).\n\n\\\\\\[a\\_v^{(k)} = \\\\text{aggregate}^{(k)} (\\\\{ h\\_u^{(k-1)}: u \\\\in N(v) \\\\})\\\\\\]\n\n\\\\\\[h\\_v^{(k)} = \\\\text{combine}^{(k)} (h\\_v^{(k-1)}, a\\_v^{(k)})\\\\\\]\n\nwhere \\\\(h\\_v^{(k)}\\\\) is the embedding of node \\\\(v\\\\) at layer \\\\(k\\\\), \\\\(N(v)\\\\) are the neighbors of node \\\\(v\\\\).\n\nFigure 2: Message passing overview. Figure taken from \\[2\\]\n\nOne important property of a GNN is permutation invariance. This means that changing the order of nodes in the graph should not affect the outcome. For example, when working with adjacency matrices, changing the order of nodes would mean swapping rows and/or columns. However, this does not change any properties of a graph, but the input would differ. In GNNs, we want to overcome this. We, therefore need an aggregation function and a combining function that are permutation invariant, such as using\nthe mean, the maximum or a sum. Using a permutation invariant aggregation function ensures that the graph-level outputs are also inva...",
      "url": "https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html"
    },
    {
      "title": "GitHub - niayeshzrf/GNN-QM9",
      "text": "<div><div><article>\n<p>\n<a href=\"https://github.com/niayeshzrf/GNN-QM9/blob/main/notebooks/Cover.png\"></a>\n</p>\n<p></p><h2>\ud83e\uddea Graph Neural Network for Molecular Property Prediction (QM9)</h2><a href=\"#-graph-neural-network-for-molecular-property-prediction-qm9\"></a><p></p>\n<p>This project implements a Graph Neural Network (GNN) trained from scratch on the <a href=\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/gdb9.tar.gz\">QM9 dataset</a> to predict quantum chemical properties \u2014 specifically the <strong>HOMO energy</strong> of molecules.</p>\n<p>Molecular graphs are built from raw <code>.sdf</code> files using <strong>RDKit</strong> and trained using <strong>PyTorch Geometric</strong>. All preprocessing, training, and visualizations are implemented from scratch to maximize learning and reproducibility.</p>\n<hr/>\n<p></p><h2>\ud83d\udd2c Why QM9? Why GNN?</h2><a href=\"#-why-qm9-why-gnn\"></a><p></p>\n<ul>\n<li><strong>QM9</strong> provides 130,000+ small organic molecules with rich quantum properties.</li>\n<li>Molecules are naturally graphs \u2014 atoms as nodes, bonds as edges.</li>\n<li>GNNs leverage this structure, making them ideal for property prediction.</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83e\uddec Data Pipeline</h2><a href=\"#-data-pipeline\"></a><p></p>\n<p>\u2705 Parse raw <code>.sdf</code> using RDKit<br/>\n\u2705 Create node, edge, and adjacency features<br/>\n\u2705 Build PyTorch Geometric <code>Data</code> objects<br/>\n\u2705 Save &amp; load efficiently with <code>InMemoryDataset</code></p>\n<p><strong>Custom graph features:</strong></p>\n<hr/>\n<p></p><h2>\ud83e\udde0 Model: Graph Isomorphism Network (GIN)</h2><a href=\"#-model-graph-isomorphism-network-gin\"></a><p></p>\n<p>We use a simple GIN-based model with:</p>\n<ul>\n<li>3 GINConv layers</li>\n<li>Global mean pooling</li>\n<li>Fully connected layers for regression</li>\n<li>Dropout + ReLU activations</li>\n</ul>\n<p>Target values are <strong>z-score normalized</strong> for training.</p>\n<hr/>\n<p></p><h2>\ud83d\udcc8 Results</h2><a href=\"#-results\"></a><p></p>\n<p></p><h3>\u2705 Loss Curves</h3><a href=\"#-loss-curves\"></a><p></p>\n<p>\n<a href=\"https://github.com/niayeshzrf/GNN-QM9/blob/main/notebooks/loss_curve.png\"></a>\n</p>\n<p></p><h2>\ud83e\uddfe File Structure</h2><a href=\"#-file-structure\"></a><p></p>\n<div><pre>GNN-QM9/\n\u251c\u2500\u2500 data/ # QM9 raw + processed data\n\u251c\u2500\u2500 src/ # All code (loader, dataset, model, training)\n\u251c\u2500\u2500 notebooks/ # Jupyter visualizations\n\u251c\u2500\u2500 results/ # Predictions, losses\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n<span>---</span>\n<span>## <span>\ud83d\ude80 How to Run</span></span>\n<span>```</span><span>bash</span>\ngit clone https://github.com/niayeshzrf/GNN-QM9.git\n<span>cd</span> GNN-QM9\npip install -r requirements.txt\n<span><span>#</span> Build dataset</span>\npython src/build_dataset.py\n<span><span>#</span> Train model</span>\npython src/train_GNN.py\n=======\n<span><span>#</span> GNN-QM9</span>\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 07a631be07503ca51155bf58a8bfaa9f583bf06f</pre></div>\n</article></div></div>",
      "url": "https://github.com/niayeshzrf/gnn-qm9"
    },
    {
      "title": "geometric-gnn-dojo/geometric_gnn_101.ipynb at main",
      "text": "geometric-gnn-dojo/geometric\\_gnn\\_101.ipynb at main \u00b7chaitjo/geometric-gnn-dojo \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=chaitjo/geometric-gnn-dojo)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[chaitjo](https://github.com/chaitjo)/**[geometric-gnn-dojo](https://github.com/chaitjo/geometric-gnn-dojo)**Public\n* [Notifications](https://github.com/login?return_to=/chaitjo/geometric-gnn-dojo)You must be signed in to change notification settings\n* [Fork50](https://github.com/login?return_to=/chaitjo/geometric-gnn-dojo)\n* [Star518](https://github.com/login?return_to=/chaitjo/geometric-gnn-dojo)\n</turbo-frame></main>\nYou can\u2019t perform that action at this time.\n</div>",
      "url": "https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb"
    },
    {
      "title": "Building A Graph Convolutional Network for Molecular ...",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F978b0ae10ec4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-978b0ae10ec4---------------------------------------)\n\n\u00b7\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n## Artificial Intelligence\n\n# Building A Graph Convolutional Network for Molecular Property Prediction\n\n## Tutorial to make molecular graphs and develop a simple PyTorch-based GCN\n\n[Gaurav Deshmukh](https://medium.com/@ChemAndCode?source=post_page---byline--978b0ae10ec4---------------------------------------)\n\n17 min read\n\n\u00b7\n\nDec 23, 2023\n\n--\n\n6\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nPhoto by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nArtificial intelligence has taken the world by storm. Every week, new models, tools, and applications emerge that promise to push the boundaries of human endeavor. The availability of open-source tools that enable users to train and employ complex machine learning models in a modest number of lines of code have truly democratized AI; at the same time, while many of these off-the-shelf models may provide excellent predictive capabilities, their usage as black box models may deprive inquisitive students of AI of a deeper understanding of how they work and why they were developed in the first place. This understanding is particularly important in the natural sciences, where knowing that a model is accurate is not enough \u2014 it is also essential to know its connection to other physical theories, its limitations, and its generalizability to other systems. In this article, we will explore the basics of one particular ML model \u2014 a graph convolutional network \u2014 through the lens of chemistry. This is not meant to be a mathematically rigorous exploration; instead, we will try to compare features of the network with traditional models in the natural sciences and think about why it works as well as it does.\n\n## 1\\. The need for graphs and graph neural networks\n\nA model in chemistry or physics is usually a continuous function, say _y=f(x\u2081, x\u2082, x\u2083, \u2026, x\u2099)_, in which _x\u2081, x\u2082, x\u2083, \u2026, x\u2099_ are the inputs and _y_ is the output. An example of such a model is the equation that determines the electrostatic interaction (or force) between two point charges _q\u2081_ and _q\u2082_ separated by a distance _r_ present in a medium with relative permittivity _\u03b5\u1d63_, commonly termed as Coulomb\u2019s law.\n\nPress enter or click to view image in full size\n\nFigure 1: The Coulomb equation as a model for electrostatic interactions between point charges (Image by author)\n\nIf we did not know this relationship but, hypothetically, had multiple datapoints each including the interaction between point charges (the output) and the corresponding inputs, we could fit an artificial neural network to predict the interaction for any given point charges for any given separation in a medium with a specified permittivity. In the case of this problem, admittedly ignoring some important caveats, creating a data-driven model for a physical problem is relatively straightforward.\n\nNow consider the problem of prediction of a particular property, say solubility in water, from the structure of a molecule. First, there is no obvious set of inputs to describe a molecule. You could use various features, such as bond lengths, bond angles, number of different types of elements, number of rings, and so forth. However, there is no guarantee that any such arbitrary set is bound to work well for all molecules.\n\nSecond, unlike the example of the point charges, the inputs may not necessarily reside in a continuous space. For example, we can think of methanol, ethanol, and propanol as a set of molecules with increasing chain lengths; there is no notion, however, of anything between them \u2014 chain length is a discrete parameter and there is no way to interpolate between methanol and ethanol to get other molecules. Having a continuous space of inputs is essential to calculate derivatives of the model, which can then be used for optimization of the chosen property.\n\nTo overcome these problems, various methods for encoding molecules have been proposed. One such method is textual representation using schemes such as SMILES and SELFIES. There is a large body of literature on this representation, and I direct the interested reader to this [helpful review](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf). The second method involves representing molecules as graphs. While each method has its advantages and shortcomings, graph representations feel more intuitive for chemistry.\n\nA graph is a mathematical structure consisting of nodes connected by edges that represent relationships between nodes. Molecules fit naturally into this structure \u2014 atoms become nodes, and bonds become edges. Each node in the graph is represented by a vector that encodes properties of the corresponding atom. Usually, a one-hot encoding scheme suffices (more on this in the next section). These vectors can be stacked to create a _node matrix._ Relationships between nodes \u2014 denoted by edges \u2014 can be delineated through a square _adjacency matrix,_ wherein every element _a\u1d62\u2c7c_ is either 1 or 0 depending on whether the two nodes _i_ and _j_ are connected by an edge or not respectively. The diagonal elements are set to 1, indicating a self-connection, which makes the matrix amenable to convolutions (as you will see in the next section). More complex graph representations can be developed, in which edge properties are also one-hot encoded in a separate matrix, but we shall leave that for another article. These node and adjacency matrices will serve as inputs to our model.\n\nPress enter or click to view image in full size\n\nFigure 2: Representation of an acetamide molecule as a graph with one-hot encodings of atomic numbers of nodes (Image by author)\n\nTypically, artificial neural network models accept a 1-dimensional vector of inputs. For multidimensional inputs, such as images, a class of models called convolutional neural networks was developed. In our case too we have 2-dimensional matrices as inputs, and therefore, need a modified network that can accept these as inputs. Graph neural networks were developed to operate on such node and adjacency matrices to convert them into appropriate 1-dimensional vectors that can then be passed through hidden layers of a vanilla artificial neural network to generate outputs. There are many types of graph neural networks, such as graph convolutional networks, message passing networks, graph attention networks, and so forth, which primarily differ ...",
      "url": "https://medium.com/data-science/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4"
    },
    {
      "title": "Tutorial 6: Basics of Graph Neural Networks",
      "text": "Tutorial 6: Basics of Graph Neural Networks &mdash; PyTorch Lightning 2.6.0 documentation\nTable of Contents[](#)\n* [Overview](https://lightning.ai/docs/overview/)\n* [Team management](https://lightning.ai/docs/team-management/team-management)\n* [Production](https://lightning.ai/docs/production/automate-workflows)\n* [Security](https://lightning.ai/docs/security/security)\n* [Open source](#)\n* [Overview](https://lightning.ai/docs/opensource/open-source)\n* [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)\n* [Fabric](https://lightning.ai/docs/fabric/stable/)\n* [Lit-GPT](https://github.com/Lightning-AI/litgpt)\n* [Torchmetrics](https://lightning.ai/docs/torchmetrics/stable/)\n* [Litdata](https://github.com/Lightning-AI/litdata)\n* [Lit LLaMA](https://github.com/Lightning-AI/lit-llama)\n* [Litserve](https://github.com/Lightning-AI/litserve)\n* [Examples](https://lightning.ai/docs/examples)\n* [Glossary](https://lightning.ai/docs/glossary)\n* [FAQ](https://lightning.ai/docs/faq)\n* [Docs](../../index.html)&gt;\n* Tutorial 6: Basics of Graph Neural Networks\n* [![](../../_static/images/view-page-source-icon.svg)](../../_sources/notebooks/course_UvA-DL/06-graph-neural-networks.ipynb.txt)\nShortcuts\n# Tutorial 6: Basics of Graph Neural Networks[\u00b6](#Tutorial-6:-Basics-of-Graph-Neural-Networks)\n* **Author:**Phillip Lippe\n* **License:**CC BY-SA\n* **Generated:**2025-05-01T10:33:44.559710\nIn this tutorial, we will discuss the application of neural networks on graphs. Graph Neural Networks (GNNs) have recently gained increasing popularity in both applications and research, including domains such as social networks, knowledge graphs, recommender systems, and bioinformatics. While the theory and math behind GNNs might first seem complicated, the implementation of those models is quite simple and helps in understanding the methodology. Therefore, we will discuss the implementation of\nbasic network layers of a GNN, namely graph convolutions, and attention layers. Finally, we will apply a GNN on semi-supervised node classification and molecule categorization. This notebook is part of a lecture series on Deep Learning at the University of Amsterdam. The full list of tutorials can be found at[https://uvadlc-notebooks.rtfd.io](https://uvadlc-notebooks.rtfd.io).\nOpen in[](https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/06-graph-neural-networks.ipynb)\nGive us a \u2b50[on Github](https://www.github.com/Lightning-AI/lightning/)| Check out[the documentation](https://lightning.ai/docs/)| Join us[on Discord](https://discord.com/invite/tfXFetEZxv)\n## Setup[\u00b6](#Setup)\nThis notebook requires some packages besides pytorch-lightning.\n```\n[1]:\n```\n```\n!pipinstall--quiet&quot;&quot;torch-geometric ==2.1.\\*&quot;&quot;&quot;&quot;torch-sparse ==0.6.\\*&quot;&quot;&quot;&quot;torch-scatter ==2.1.\\*&quot;&quot;&quot;torchmetrics &gt;=1.0,&lt;1.8&quot;&quot;torch ==2.1.2&quot;&quot;torchvision&quot;&quot;pytorch-lightning &gt;=2.0,&lt;2.6&quot;&quot;matplotlib&quot;&quot;seaborn&quot;&quot;torch &gt;=1.8.1,&lt;2.8&quot;&quot;numpy &lt;2.0&quot;&quot;&quot;torch-cluster ==1.6.\\*&quot;&quot;&quot;&quot;torch-spline-conv ==1.2.\\*&quot;&quot;&quot;numpy &lt;3.0&quot;\n```\n```\nWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[notice]A new release of pip is available:24.2-&gt;25.1[notice]To update, run:python -m pip install --upgrade pip\n```\n[![Embedded YouTube video](https://img.youtube.com/vi/fK7d56Ly9q8/0.jpg)](https://www.youtube.com/watch?v=fK7d56Ly9q8)\nWe start by importing our standard libraries below.\n```\n[2]:\n```\n```\n# Standard librariesimportos# For downloading pre-trained modelsimporturllib.requestfromurllib.errorimportHTTPError# PyTorch Lightningimportpytorch\\_lightningaspl# PyTorchimporttorchimporttorch.nnasnnimporttorch.nn.functionalasFimporttorch.optimasoptim# PyTorch geometricimporttorch\\_geometricimporttorch\\_geometric.dataasgeom\\_dataimporttorch\\_geometric.nnasgeom\\_nn# PL callbacksfrompytorch\\_lightning.callbacksimportModelCheckpointfromtorchimportTensorAVAIL\\_GPUS=min(1,torch.cuda.device\\_count())BATCH\\_SIZE=256ifAVAIL\\_GPUSelse64# Path to the folder where the datasets are/should be downloadedDATASET\\_PATH=os.environ.get(&quot;&quot;PATH\\_DATASETS&quot;&quot;,&quot;data/&quot;)# Path to the folder where the pretrained models are savedCHECKPOINT\\_PATH=os.environ.get(&quot;&quot;PATH\\_CHECKPOINT&quot;&quot;,&quot;&quot;saved\\_models/GNNs/&quot;&quot;)# Setting the seedpl.seed\\_everything(42)# Ensure that all operations are deterministic on GPU (if used) for reproducibilitytorch.backends.cudnn.deterministic=Truetorch.backends.cudnn.benchmark=False\n```\n```\nSeed set to 42\n```\nWe also have a few pre-trained models we download below.\n```\n[3]:\n```\n```\n# Github URL where saved models are stored for this tutorialbase\\_url=&quot;&quot;https://raw.githubusercontent.com/phlippe/saved\\_models/main/tutorial7/&quot;&quot;# Files to downloadpretrained\\_files=[&quot;NodeLevelMLP.ckpt&quot;,&quot;NodeLevelGNN.ckpt&quot;,&quot;GraphLevelGraphConv.ckpt&quot;]# Create checkpoint path if it doesn&#39;t exist yetos.makedirs(CHECKPOINT\\_PATH,exist\\_ok=True)# For each file, check whether it already exists. If not, try downloading it.forfile\\_nameinpretrained\\_files:file\\_path=os.path.join(CHECKPOINT\\_PATH,file\\_name)if&quot;/&quot;infile\\_name:os.makedirs(file\\_path.rsplit(&quot;/&quot;,1)[0],exist\\_ok=True)ifnotos.path.isfile(file\\_path):file\\_url=base\\_url+file\\_nameprint(f&quot;Downloading{file\\_url}...&quot;)try:urllib.request.urlretrieve(file\\_url,file\\_path)exceptHTTPErrorase:print(&quot;Something went wrong. Please try to download the file from the GDrive folder,&quot;&quot; or contact the author with the full output including the following error:\\\\n&quot;,e,)\n```\n```\nDownloading https://raw.githubusercontent.com/phlippe/saved\\_models/main/tutorial7/NodeLevelMLP.ckpt...\nDownloading https://raw.githubusercontent.com/phlippe/saved\\_models/main/tutorial7/NodeLevelGNN.ckpt...\nDownloading https://raw.githubusercontent.com/phlippe/saved\\_models/main/tutorial7/GraphLevelGraphConv.ckpt...\n```\n## Graph Neural Networks[\u00b6](#Graph-Neural-Networks)\n### Graph representation[\u00b6](#Graph-representation)\nBefore starting the discussion of specific neural network operations on graphs, we should consider how to represent a graph. Mathematically, a graph\\\\(\\\\mathcal{G}\\\\)is defined as a tuple of a set of nodes/vertices\\\\(V\\\\), and a set of edges/links\\\\(E\\\\):\\\\(\\\\mathcal{G}=(V,E)\\\\). Each edge is a pair of two vertices, and represents a connection between them. For instance, let\u2019s look at the following graph:\n![c07c134044854d87bb6a2cd1c8716372](https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/06-graph-neural-networks/example_graph.svg)\nThe vertices are\\\\(V=\\\\{1,2,3,4\\\\}\\\\), and edges\\\\(E=\\\\{(1,2), (2,3), (2,4), (3,4)\\\\}\\\\). Note that for simplicity, we assume the graph to be undirected and hence don\u2019t add mirrored pairs like\\\\((2,1)\\\\). In application, vertices and edge can often have specific attributes, and edges can even be directed. The question is how we could represent this diversity in an efficient way for matrix operations. Usually, for the edges, we decide between two variants: an adjacency matrix, or a list of\npaired vertex indices.\nThe**adjacency matrix**\\\\(A\\\\)is a square matrix whose elements indicate whether pairs of vertices are adjacent, i.e. connected, or not. In the simplest case,\\\\(A\\_{ij}\\\\)is 1 if there is a connection from node\\\\(i\\\\)to\\\\(j\\\\), and otherwise 0. If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well. For an ...",
      "url": "https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/06-graph-neural-networks.html"
    },
    {
      "title": "Using GNN property predictors as molecule generators",
      "text": "Using GNN property predictors as molecule generators | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-025-59439-1?error=cookies_not_supported&code=7bd376b0-babf-4d1c-9a07-b834222a7e20)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nUsing GNN property predictors as molecule generators\n[Download PDF](https://www.nature.com/articles/s41467-025-59439-1.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-025-59439-1.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:08 May 2025# Using GNN property predictors as molecule generators\n* [F\u00e9lix Therrien](#auth-F_lix-Therrien-Aff1)[ORCID:orcid.org/0000-0003-1074-7805](https://orcid.org/0000-0003-1074-7805)[1](#Aff1),\n* [Edward H. Sargent](#auth-Edward_H_-Sargent-Aff1)[ORCID:orcid.org/0000-0003-0396-6495](https://orcid.org/0000-0003-0396-6495)[1](#Aff1)&amp;\n* [Oleksandr Voznyy](#auth-Oleksandr-Voznyy-Aff1)[ORCID:orcid.org/0000-0002-8656-5074](https://orcid.org/0000-0002-8656-5074)[1](#Aff1)\n[*Nature Communications*](https://www.nature.com/ncomms)**volume16**, Article\u00a0number:4301(2025)[Cite this article](#citeas)\n* 16kAccesses\n* 8Citations\n* 3Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-025-59439-1/metrics)\n### Subjects\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n* [Computational science](https://www.nature.com/subjects/computational-science)\n## Abstract\nGraph neural networks (GNNs) have emerged as powerful tools to accurately predict materials and molecular properties in computational and automated discovery pipelines. In this article, we exploit the invertible nature of these neural networks to directly generate molecular structures with desired electronic properties. Starting from a random graph or an existing molecule, we perform a gradient ascent while holding the GNN weights fixed in order to optimize its input, the molecular graph, towards the target property. Valence rules are enforced strictly through a judicious graph construction. The method relies entirely on the property predictor; no additional training is required on molecular structures. We demonstrate the application of this method by generating molecules with specific energy gaps verified with density functional theory (DFT) and\u00a0with specific octanol-water partition coefficients (logP). Our approach hits target properties with rates comparable to or better than state-of-the-art generative models while consistently generating more diverse molecules. Moreover, while validating our framework we created a dataset of 1617 new molecules and their corresponding DFT-calculated properties that could serve as an out-of-distribution test set for QM9-trained models.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-022-04967-9/MediaObjects/41598_2022_4967_Fig1_HTML.png)\n### [Selecting molecules with diverse structures and properties by maximizing submodular functions of descriptors learned with graph neural networks](https://www.nature.com/articles/s41598-022-04967-9?fromPaywallRec=false)\nArticleOpen access21 January 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-99785-0/MediaObjects/41598_2025_99785_Fig1_HTML.png)\n### [Targeted molecular generation with latent reinforcement learning](https://www.nature.com/articles/s41598-025-99785-0?fromPaywallRec=false)\nArticleOpen access30 April 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42003-025-09064-x/MediaObjects/42003_2025_9064_Fig1_HTML.png)\n### [MoleculeFormer is a GCN-transformer architecture for molecular property prediction](https://www.nature.com/articles/s42003-025-09064-x?fromPaywallRec=false)\nArticleOpen access25 November 2025\n## Introduction\nOne of the ultimate goals of computational materials science is to rapidly identify promising material structures and compositions with specific properties to guide experimental scientists and automated laboratories[1](https://www.nature.com/articles/s41467-025-59439-1#ref-CR1). This is particularly true in the time-critical fields of pharmacy and materials for energy and sustainability where a number of large scale autonomous experimentation initiatives are underway[2](https://www.nature.com/articles/s41467-025-59439-1#ref-CR2),[3](https://www.nature.com/articles/s41467-025-59439-1#ref-CR3). In the past decades, computational materials discovery has been achieved by going through large databases of existing materials and computing properties from first principles using methods such as density functional theory (DFT) and molecular dynamics[4](https://www.nature.com/articles/s41467-025-59439-1#ref-CR4),[5](https://www.nature.com/articles/s41467-025-59439-1#ref-CR5). These methods have proven to be successful in many cases[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](https://www.nature.com/articles/s41467-025-59439-1#ref-CR9), but suffer from two main limitations: (1) they are computationally expensive and (2) they cover a small subspace of all possible materials.\nIn an effort to alleviate the first problem, machine learning (ML) based property prediction methods have become an integral part of materials science[10](https://www.nature.com/articles/s41467-025-59439-1#ref-CR10),[11](https://www.nature.com/articles/s41467-025-59439-1#ref-CR11). Various models exploit different materials representations (e.g., graphs, fingerprints)[12](https://www.nature.com/articles/s41467-025-59439-1#ref-CR12),[13](https://www.nature.com/articles/s41467-025-59439-1#ref-CR13), model architectures (e.g., neural networks, random forests)[14](https://www.nature.com/articles/s41467-025-59439-1#ref-CR14)and datasets (experimental or computed) and their accuracy has been steadily increasing, often competing with that of DFT[10](https://www.nature.com/articles/s41467-025-59439-1#ref-CR10). The success and adoption of these models is due largely to the powerful tools developed by the ML and data science communities (such as Pytorch[15](https://www.nature.com/articles/s41467-025-59439-1#ref-CR15), Tensorflow[16](https://www.nature.com/articles/s41467-025-59439-1#ref-CR16), Pandas[17](https://www.nature.com/articles/s41467-025-59439-1#ref-CR17), etc.). However, despite their promising performance on benchmark datasets, ML property predictors still suffer from poor generalizability[18](https://www.nature.com/articles/s41467-025-59439-1#ref-CR18), exhibiting much lower performance on out-of-distribution data, i.e., materials that are different from what they have been trained on.\nMaterials and molecule generation can alleviate the second limitation: it can theoretically explore the full space of all possible materials. Traditionally this has been done using minima hopping[19](https://www.nature.com/articles/s41467-025-59439-1#ref-CR19), metadynamics[20](https://www.nature.com/articles/s41467-025-59439-1#ref-CR20)and evolutionary approaches[21](#r...",
      "url": "https://www.nature.com/articles/s41467-025-59439-1"
    },
    {
      "title": "pyg-team/pytorch_geometric: Graph Neural Network ...",
      "text": "GitHub - pyg-team/pytorch\\_geometric: Graph Neural Network Library for PyTorch\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=pyg-team/pytorch_geometric)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[pyg-team](https://github.com/pyg-team)/**[pytorch\\_geometric](https://github.com/pyg-team/pytorch_geometric)**Public\n* [Notifications](https://github.com/login?return_to=/pyg-team/pytorch_geometric)You must be signed in to change notification settings\n* [Fork3.9k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n* [Star23.4k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\nGraph Neural Network Library for PyTorch\n[pyg.org](https://pyg.org)\n### License\n[MIT license](https://github.com/pyg-team/pytorch_geometric/blob/master/LICENSE)\n[23.4kstars](https://github.com/pyg-team/pytorch_geometric/stargazers)[3.9kforks](https://github.com/pyg-team/pytorch_geometric/forks)[Branches](https://github.com/pyg-team/pytorch_geometric/branches)[Tags](https://github.com/pyg-team/pytorch_geometric/tags)[Activity](https://github.com/pyg-team/pytorch_geometric/activity)\n[Star](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n[Notifications](https://github.com/login?return_to=/pyg-team/pytorch_geometric)You must be signed in to change notification settings\n# pyg-team/pytorch\\_geometric\nmaster\n[Branches](https://github.com/pyg-team/pytorch_geometric/branches)[Tags](https://github.com/pyg-team/pytorch_geometric/tags)\n[](https://github.com/pyg-team/pytorch_geometric/branches)[](https://github.com/pyg-team/pytorch_geometric/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[7,919 Commits](https://github.com/pyg-team/pytorch_geometric/commits/master/)\n[](https://github.com/pyg-team/pytorch_geometric/commits/master/)\n|\n[.github](https://github.com/pyg-team/pytorch_geometric/tree/master/.github)\n|\n[.github](https://github.com/pyg-team/pytorch_geometric/tree/master/.github)\n|\n|\n|\n[benchmark](https://github.com/pyg-team/pytorch_geometric/tree/master/benchmark)\n|\n[benchmark](https://github.com/pyg-team/pytorch_geometric/tree/master/benchmark)\n|\n|\n|\n[docker](https://github.com/pyg-team/pytorch_geometric/tree/master/docker)\n|\n[docker](https://github.com/pyg-team/pytorch_geometric/tree/master/docker)\n|\n|\n|\n[docs](https://github.com/pyg-team/pytorch_geometric/tree/master/docs)\n|\n[docs](https://github.com/pyg-team/pytorch_geometric/tree/master/docs)\n|\n|\n|\n[examples](https://github.com/pyg-team/pytorch_geometric/tree/master/examples)\n|\n[examples](https://github.com/pyg-team/pytorch_geometric/tree/master/examples)\n|\n|\n|\n[graphgym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)\n|\n[graphgym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)\n|\n|\n|\n[test](https://github.com/pyg-team/pytorch_geometric/tree/master/test)\n|\n[test](https://github.com/pyg-team/pytorch_geometric/tree/master/test)\n|\n|\n|\n[torch\\_geometric](https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric)\n|\n[torch\\_geometric](https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric)\n|\n|\n|\n[.gitignore](https://github.com/pyg-team/pytorch_geometric/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/pyg-team/pytorch_geometric/blob/master/.gitignore)\n|\n|\n|\n[.pre-commit-config.yaml](https://github.com/pyg-team/pytorch_geometric/blob/master/.pre-commit-config.yaml)\n|\n[.pre-commit-config.yaml](https://github.com/pyg-team/pytorch_geometric/blob/master/.pre-commit-config.yaml)\n|\n|\n|\n[CHANGELOG.md](https://github.com/pyg-team/pytorch_geometric/blob/master/CHANGELOG.md)\n|\n[CHANGELOG.md](https://github.com/pyg-team/pytorch_geometric/blob/master/CHANGELOG.md)\n|\n|\n|\n[CITATION.cff](https://github.com/pyg-team/pytorch_geometric/blob/master/CITATION.cff)\n|\n[CITATION.cff](https://github.com/pyg-team/pytorch_geometric/blob/master/CITATION.cff)\n|\n|\n|\n[LICENSE](https://github.com/pyg-team/pytorch_geometric/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/pyg-team/pytorch_geometric/blob/master/LICENSE)\n|\n|\n|\n[README.md](https://github.com/pyg-team/pytorch_geometric/blob/master/README.md)\n|\n[README.md](https://github.com/pyg-team/pytorch_geometric/blob/master/README.md)\n|\n|\n|\n[codecov.yml](https://github.com/pyg-team/pytorch_geometric/blob/master/codecov.yml)\n|\n[codecov.yml](https://github.com/pyg-team/pytorch_geometric/blob/master/codecov.yml)\n|\n|\n|\n[pyproject.toml](https://github.com/pyg-team/pytorch_geometric/blob/master/pyproject.toml)\n|\n[pyproject.toml](https://github.com/pyg-team/pytorch_geometric/blob/master/pyproject.toml)\n|\n|\n|\n[readthedocs.yml](https://github.com/pyg-team/pytorch_geometric/blob/master/readthedocs.yml)\n|\n[readthedocs.yml](https://github.com/pyg-team/pytorch_geometric/blob/master/readthedocs.yml)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n[![](https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true)](https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true)\n[![PyPI Version](https://camo.githubusercontent.com/ee318372ebfb14375aee3ed9611cfeadeaf7c81a4d2e61a1510b3c0565fbb119/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f746f7263682d67656f6d65747269633f636f6c6f723d344232364134)](https://pypi.python.org/pypi/torch-geometric)[![PyPI Download](https://camo.githubusercontent.com/ccb14adc523eb6bef1c885fdbb74f10ae2aed3928a893ead1a0a0ba6ffce30b0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f746f7263685f67656f6d65747269633f636f6c6f723d344232364134)](https://pepy.tech/projects/torch_geometric)[![Slack](https://camo.githubusercontent.com/771fd162a09c79f5de5f0bf8a853c635efa269070a3f6a72c52f72f9d9d87597/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d6a6f696e2d77686974652e7376673f6c6f676f3d736c61636b26636f6c6f723d344232364134)](https://data.pyg.org/slack.html)[![Contributing](https://camo.githubusercontent.com/596661da782f9b35a60900e95293923e77d113745ae96e28bbc3f6419a3f069e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c617426636f6c6f723d344232364134)](https://github.com/pyg-team/pytorch_geometric/blob/master/.github/CONTRIBUTING.md)\n**[Documentation](https://pytorch-geometric.readthedocs.io)**|**[PyG 1.0 Paper](https://arxiv.org/abs/1903.02428)**|**[PyG 2.0 Paper](https://arxiv.org/abs/2507.16991)**|**[Colab Notebooks](https://pytorch-geometric.readthedocs.io/en/lates...",
      "url": "https://github.com/pyg-team/pytorch_geometric"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "pytorch\\_geometric/examples/attentive\\_fp.py at master \u00b7pyg-team/pytorch\\_geometric \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=pyg-team/pytorch_geometric)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[pyg-team](https://github.com/pyg-team)/**[pytorch\\_geometric](https://github.com/pyg-team/pytorch_geometric)**Public\n* [Notifications](https://github.com/login?return_to=/pyg-team/pytorch_geometric)You must be signed in to change notification settings\n* [Fork3.9k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n* [Star23.3k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n</turbo-frame></main>\nYou can\u2019t perform that action at this time.\n</div>",
      "url": "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py"
    }
  ]
}