{
  "query": "How do you reduce the intercept in a CV-LB linear relationship when predicting for unseen chemical compounds?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the intercept (bias) of a cross\u2011validated linear model for out\u2011of\u2011sample chemical predictions can be done in a few focused steps:\n\n1. **Center the training descriptors** \u2013 subtract the mean of each molecular feature from both the training and any future test set so the data are zero\u2011centered. When the inputs are centered, the fitted intercept will be close to zero.  \n   *(The intercept term disappears if the data are already centered, as `fit_intercept=False` expects\u202f[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)).*\n\n2. **Disable automatic intercept estimation** \u2013 tell the linear estimator not to learn an intercept at all:  \n   ```python\n   from sklearn.linear_model import LinearRegression\n   model = LinearRegression(fit_intercept=False)   # no intercept\n   ```  \n   This forces the model to rely solely on the learned slopes, eliminating any systematic offset\u202f[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n\n3. **If using XGBoost\u2011based linear boosters, fix the base score** \u2013 set `base_score` (the model\u2019s global bias) to a constant such as\u202f0.0 before training, or supply a custom `base_margin` that reflects a zero intercept.  \n   ```python\n   import xgboost as xgb\n   reg = xgb.XGBRegressor()\n   reg.set_params(base_score=0.0)   # forces intercept to zero\n   ```  \n   This overrides the automatic intercept estimation introduced in XGBoost\u202f2.0.0\u202f[XGBoost intercept](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html).\n\n4. **Apply a post\u2011hoc intercept\u2011bias correction** \u2013 after fitting, compute the average residual on a small validation set of known compounds and subtract that mean bias from all future predictions (the \u201cintercept\u2011bias correction\u201d described for spectroscopy models).  \n   ```python\n   preds = model.predict(X_val)\n   bias  = preds.mean() - y_val.mean()\n   corrected_pred = model.predict(X_new) - bias\n   ```  \n   This step directly reduces systematic offset when the model is deployed on unseen chemicals\u202f[Optimizing the Regression Model: The Challenge of Intercept\u2013Bias and Slope \u201cCorrection\u201d](https://www.spectroscopyonline.com/view/optimizing-regression-model-challenge-intercept-bias-and-slope-correction).\n\n5. **Validate on an external hold\u2011out set** \u2013 evaluate the adjusted model on truly unseen compounds; if a residual intercept remains, repeat steps\u202f1\u20114 with a larger calibration set or stronger regularization (e.g., ridge/elastic\u2011net) to shrink coefficients and further dampen bias.  \n\nFollowing these actions\u2014centering features, disabling the intercept term, fixing any global bias in tree\u2011based boosters, and applying a simple bias\u2011correction based on a validation set\u2014will systematically lower the intercept of a CV\u2011LB linear relationship and improve predictions for new chemical compounds.",
      "url": ""
    },
    {
      "title": "",
      "text": "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/index.html)\n- Intercept\n- [View page source](https://xgboost.readthedocs.io/en/stable/_sources/tutorials/intercept.rst.txt)\n* * *\n# Intercept [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#intercept)\nAdded in version 2.0.0.\nSince 2.0.0, XGBoost supports estimating the model intercept (named `base_score`)\nautomatically based on targets upon training. The behavior can be controlled by setting\n`base_score` to a constant value. The following snippet disables the automatic\nestimation:\n```\nimportxgboostasxgb\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\nIn addition, here 0.5 represents the value after applying the inverse link function. See\nthe end of the document for a description.\nOther than the `base_score`, users can also provide global bias via the data field\n`base_margin`, which is a vector or a matrix depending on the task. With multi-output\nand multi-class, the `base_margin` is a matrix with size `(n_samples, n_targets)` or\n`(n_samples, n_classes)`.\n```\nimportxgboostasxgb\nfromsklearn.datasetsimport make_regression\nX, y = make_regression()\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\nIt specifies the bias for each sample and can be used for stacking an XGBoost model on top\nof other models, see [Demo for boosting from prediction](https://xgboost.readthedocs.io/en/stable/python/examples/boost_from_prediction.html#sphx-glr-python-examples-boost-from-prediction-py) for a worked\nexample. When `base_margin` is specified, it automatically overrides the `base_score`\nparameter. If you are stacking XGBoost models, then the usage should be relatively\nstraightforward, with the previous model providing raw prediction and a new model using\nthe prediction as bias. For more customized inputs, users need to take extra care of the\nlink function. Let \\\\(F\\\\) be the model and \\\\(g\\\\) be the link function, since\n`base_score` is overridden when sample-specific `base_margin` is available, we will\nomit it here:\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i)\\\\\\]\nWhen base margin \\\\(b\\\\) is provided, it\u2019s added to the raw model output \\\\(F\\\\):\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i) + b\\_i\\\\\\]\nand the output of the final model is:\n\\\\\\[g^{-1}(F(x\\_i) + b\\_i)\\\\\\]\nUsing the gamma deviance objective `reg:gamma` as an example, which has a log link\nfunction, hence:\n\\\\\\[\\\\begin{split}\\\\ln{(E\\[y\\_i\\])} = F(x\\_i) + b\\_i \\\\\\\nE\\[y\\_i\\] = \\\\exp{(F(x\\_i) + b\\_i)}\\\\end{split}\\\\\\]\nAs a result, if you are feeding outputs from models like GLM with a corresponding\nobjective function, make sure the outputs are not yet transformed by the inverse link\n(activation).\nIn the case of `base_score` (intercept), it can be accessed through\n[`save_config()`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster.save_config) after estimation. Unlike the `base_margin`, the\nreturned value represents a value after applying inverse link. With logistic regression\nand the logit link function as an example, given the `base_score` as 0.5,\n\\\\(g(intercept) = logit(0.5) = 0\\\\) is added to the raw model output:\n\\\\\\[E\\[y\\_i\\] = g^{-1}{(F(x\\_i) + g(intercept))}\\\\\\]\nand 0.5 is the same as \\\\(base\\\\\\_score = g^{-1}(0) = 0.5\\\\). This is more intuitive if\nyou remove the model and consider only the intercept, which is estimated before the model\nis fitted:\n\\\\\\[\\\\begin{split}E\\[y\\] = g^{-1}{(g(intercept))} \\\\\\\nE\\[y\\] = intercept\\\\end{split}\\\\\\]\nFor some objectives like MAE, there are close solutions, while for others it\u2019s estimated\nwith one step Newton method.\n## Offset [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#offset)\nThe `base_margin` is a form of `offset` in GLM. Using the Poisson objective as an\nexample, we might want to model the rate instead of the count:\n\\\\\\[rate = \\\\frac{count}{exposure}\\\\\\]\nAnd the offset is defined as log link applied to the exposure variable:\n\\\\(\\\\ln{exposure}\\\\). Let \\\\(c\\\\) be the count and \\\\(\\\\gamma\\\\) be the exposure,\nsubstituting the response \\\\(y\\\\) in our previous formulation of base margin:\n\\\\\\[g(\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}) = F(x\\_i)\\\\\\]\nSubstitute \\\\(g\\\\) with \\\\(\\\\ln\\\\) for Poisson regression:\n\\\\\\[\\\\ln{\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}} = F(x\\_i)\\\\\\]\nWe have:\n\\\\\\[\\\\begin{split}E\\[c\\_i\\] &= \\\\exp{(F(x\\_i) + \\\\ln{\\\\gamma\\_i})} \\\\\\\nE\\[c\\_i\\] &= g^{-1}(F(x\\_i) + g(\\\\gamma\\_i))\\\\end{split}\\\\\\]\nAs you can see, we can use the `base_margin` for modeling with offset similar to GLMs",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html"
    },
    {
      "title": "LinearRegression #",
      "text": "LinearRegression &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# LinearRegression[#](#linearregression)\n*class*sklearn.linear\\_model.LinearRegression(*\\**,*fit\\_intercept=True*,*copy\\_X=True*,*tol=1e-06*,*n\\_jobs=None*,*positive=False*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/linear_model/_base.py#L470)[#](#sklearn.linear_model.LinearRegression)\nOrdinary least squares Linear Regression.\nLinearRegression fits a linear model with coefficients w = (w1, \u2026, wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\nParameters:**fit\\_intercept**bool, default=True\nWhether to calculate the intercept for this model. If set\nto False, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n**copy\\_X**bool, default=True\nIf True, X will be copied; else, it may be overwritten.\n**tol**float, default=1e-6\nThe precision of the solution (`coef\\_`) is determined by`tol`which\nspecifies a different convergence criterion for the`lsqr`solver.`tol`is set as`atol`and`btol`of[`scipy.sparse.linalg.lsqr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html#scipy.sparse.linalg.lsqr)when\nfitting on sparse training data. This parameter has no effect when fitting\non dense data.\nAdded in version 1.7.\n**n\\_jobs**int, default=None\nThe number of jobs to use for the computation. This will only provide\nspeedup in case of sufficiently large problems, that is if firstly`n\\_targets&gt;1`and secondly`X`is sparse or if`positive`is set\nto`True`.`None`means 1 unless in a[`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/generated/joblib.parallel_backend.html#joblib.parallel_backend)context.`-1`means using all\nprocessors. See[Glossary](../../glossary.html#term-n_jobs)for more details.\n**positive**bool, default=False\nWhen set to`True`, forces the coefficients to be positive. This\noption is only supported for dense arrays.\nFor a comparison between a linear regression model with positive constraints\non the regression coefficients and a linear regression without such constraints,\nsee[Non-negative least squares](../../auto_examples/linear_model/plot_nnls.html#sphx-glr-auto-examples-linear-model-plot-nnls-py).\nAdded in version 0.24.\nAttributes:**coef\\_**array of shape (n\\_features, ) or (n\\_targets, n\\_features)\nEstimated coefficients for the linear regression problem.\nIf multiple targets are passed during the fit (y 2D), this\nis a 2D array of shape (n\\_targets, n\\_features), while if only\none target is passed, this is a 1D array of length n\\_features.\n**rank\\_**int\nRank of matrix`X`. Only available when`X`is dense.\n**singular\\_**array of shape (min(X, y),)\nSingular values of`X`. Only available when`X`is dense.\n**intercept\\_**float or array of shape (n\\_targets,)\nIndependent term in the linear model. Set to 0.0 if`fit\\_intercept=False`.\n**n\\_features\\_in\\_**int\nNumber of features seen during[fit](../../glossary.html#term-fit).\nAdded in version 0.24.\n**feature\\_names\\_in\\_**ndarray of shape (`n\\_features\\_in\\_`,)\nNames of features seen during[fit](../../glossary.html#term-fit). Defined only when`X`has feature names that are all strings.\nAdded in version 1.0.\nSee also\n[`Ridge`](sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\nRidge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization.\n[`Lasso`](sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\nThe Lasso is a linear model that estimates sparse coefficients with l1 regularization.\n[`ElasticNet`](sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)\nElastic-Net is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients.\nNotes\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares ([`scipy.linalg.lstsq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq)) or Non Negative Least Squares\n([`scipy.optimize.nnls`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html#scipy.optimize.nnls)) wrapped as a predictor object.\nExamples\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.linear\\_modelimportLinearRegression&gt;&gt;&gt;X=np.array([[1,1],[1,2],[2,2],[2,3]])&gt;&gt;&gt;# y = 1 \\* x\\_0 + 2 \\* x\\_1 + 3&gt;&gt;&gt;y=np.dot(X,np.array([1,2]))+3&gt;&gt;&gt;reg=LinearRegression().fit(X,y)&gt;&gt;&gt;reg.score(X,y)1.0&gt;&gt;&gt;reg.coef\\_array([1., 2.])&gt;&gt;&gt;reg.intercept\\_np.float64(3.0)&gt;&gt;&gt;reg.predict(np.array([[3,5]]))array([16.])\n```\nfit(*X*,*y*,*sample\\_weight=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/linear_model/_base.py#L602)[#](#sklearn.linear_model.LinearRegression.fit)\nFit linear model.\nParameters:**X**{array-like, sparse matrix} of shape (n\\_samples, n\\_features)\nTraining data.\n**y**array-like of shape (n\\_samples,) or (n\\_samples, n\\_targets)\nTarget values. Will be cast to X\u2019s dtype if necessary.\n**sample\\_weight**array-like of shape (n\\_samples,), default=None\nIndividual weights for each sample.\nAdded in version 0.17:parameter*sample\\_weight*support to LinearRegression.\nReturns:**self**object\nFitted Estimator.\nget\\_metadata\\_routing()[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/utils/_metadata_requests.py#L1550)[#](#sklearn.linear_model.LinearRegression.get_metadata_routing)\nGet metadata routing of this object.\nPlease check[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nReturns:**routing**MetadataRequest\nA[`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest)encapsulating\nrouting information.\nget\\_params(*deep=True*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/base.py#L240)[#](#sklearn.linear_model.LinearRegression.get_params)\nGet parameters for this estimator.\nParameters:**deep**bool, default=True\nIf True, will return the parameters for this estimator and\ncontained subobjects that are estimators.\nReturns:**params**dict\nParameter names mapped to their values.\npredict(*X*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/linear_model/_base.py#L297)[#](#sklearn.linear_model.LinearRegression.predict)\nPredict using the linear model.\nParameters:**X**array-like or sparse matrix, shape (n\\_samples, n\\_features)\nSamples.\nReturns:**C**array, shape (n\\_samples,)\nReturns predicted values.\nscore(*X*,*y*,*sample\\_weight=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/base.py#L610)[#](#sklearn.linear_model.LinearRegression.score)\nReturn[coefficient of determination](../model_evaluation.html#r2-score)on test data.\nThe coefficient of determination,\\\\(R^2\\\\), is defined as\\\\((1 - \\\\frac{u}{v})\\\\), where\\\\(u\\\\)is the residual\nsum of squares`((y\\_true-y\\_pred)\\*\\*2).sum()`and\\\\(v\\\\)is the total sum of squares`((y\\_true-y\\_true.mean())\\*\\*2).sum()`.\nThe best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\nthe expected value of`y`, disregarding the input features, would get\na\\\\(R^2\\\\)score of 0.0.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nTest samples. For some estimators this may be a precomputed\nkernel matrix or a list of generic objects instead with shape`(n\\_samples,n\\_samples\\_fitted)`, where`n\\_samples\\_fitted`is the nu...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
    },
    {
      "title": "Optimizing the Regression Model: The Challenge of Intercept\u2013Bias and Slope \u201cCorrection\u201d",
      "text": "Optimizing the Regression Model: The Challenge of Intercept\u2013Bias and Slope \u201cCorrection\u201d | Spectroscopy Online\n[![](https://www.spectroscopyonline.com/logo.webp)](https://www.spectroscopyonline.com/)\n[\nSubscribe\n](https://one.spectroscopyonline.com/subscribe/)\n* Multimedia\n* Publications\n* Columns\n* News\n* App Notes\n* Conferences\n* [Webcasts](https://www.spectroscopyonline.com/webcasts)\n* Resources\n* [Subscribe](https://one.spectroscopyonline.com/subscribe/)\n* [Directory](https://spectroscopydirectory.com/)\n* [Analytical Instrumentation](https://www.spectroscopyonline.com/topic/analytical-instrumentation)\n* [Analytical Method Validation](https://www.spectroscopyonline.com/topic/analytical-method-validation)\n* [Analytical Theory](https://www.spectroscopyonline.com/topic/analytical-theory)\n* [Annual Salary Survey](https://www.spectroscopyonline.com/topic/annual-salary-survey)\n* [Atomic Absorption](https://www.spectroscopyonline.com/topic/atomic-absorption)\n* [Atomic Spectroscopy](https://www.spectroscopyonline.com/topic/atomic-spectroscopy)\n* [Biological, Medical, and Clinical Analysis](https://www.spectroscopyonline.com/topic/biological-medical-and-clinical-analysis)\n* [Biopharmaceuticals Biotechnology and Protein Analysis](https://www.spectroscopyonline.com/topic/biopharmaceuticals-and-protein-analysis)\n* [Cannabis Analysis](https://www.spectroscopyonline.com/topic/cannabis-analysis)\n* [Corporate Profiles](https://www.spectroscopyonline.com/topic/corporate-profiles)\n* [Data Analytics, Statistics, Chemometrics, and Artificial Intelligence](https://www.spectroscopyonline.com/topic/data-analysis-statistics-chemometrics-artificial-intelligence)\n* [Dietary Supplements Analysis](https://www.spectroscopyonline.com/topic/dietary-supplements-analysis)\n* [Energy, Petroleum, and Bio Energy](https://www.spectroscopyonline.com/topic/energy-petroleum-and-bio-energy)\n* [Environmental Analysis](https://www.spectroscopyonline.com/topic/environmental-analysis)\n* [Far-IR/Terahertz Spectroscopy](https://www.spectroscopyonline.com/topic/far-irterahertz-spectroscopy)\n* [Fluorescence](https://www.spectroscopyonline.com/topic/fluorescence)\n* [Food and Beverage Analysis](https://www.spectroscopyonline.com/topic/food-and-beverage-analysis)\n* [Forensics, Narcotics](https://www.spectroscopyonline.com/topic/forensics-narcotics)\n* [GC-MS](https://www.spectroscopyonline.com/topic/gc-ms)\n* [Homeland Security](https://www.spectroscopyonline.com/topic/homeland-security)\n* [ICP-MS](https://www.spectroscopyonline.com/topic/icp-ms)\n* [ICP-OES](https://www.spectroscopyonline.com/topic/icp-oes)\n* [Imaging](https://www.spectroscopyonline.com/topic/imaging)\n* [Infrared (IR) Spectroscopy](https://www.spectroscopyonline.com/topic/infrared-ir-spectroscopy)\n* [LC-MS](https://www.spectroscopyonline.com/topic/lc-ms)\n* [LIBS](https://www.spectroscopyonline.com/topic/libs)\n* [Lasers and Laser-Source Technologies](https://www.spectroscopyonline.com/topic/lasers-and-source-technologies)\n* [Market Profiles](https://www.spectroscopyonline.com/topic/market-profiles)\n* [Mass Spectrometry](https://www.spectroscopyonline.com/topic/mass-spectrometry)\n* [Molecular Spectroscopy](https://www.spectroscopyonline.com/topic/molecular-spectroscopy)\n* [NMR](https://www.spectroscopyonline.com/topic/nmr)\n* [Near Infrared (NIR) Spectroscopy](https://www.spectroscopyonline.com/topic/near-infrared-nir-spectroscopy)\n* [Optics](https://www.spectroscopyonline.com/topic/optics)\n* [Peer-reviewed Articles](https://www.spectroscopyonline.com/topic/peer-reviewed-articles)\n* [Pharmaceutical Analysis](https://www.spectroscopyonline.com/topic/pharmaceutical-analysis)\n* [Plastics Polymers and Rubber](https://www.spectroscopyonline.com/topic/plastics-polymers-and-rubber)\n* [Portable and Handheld Spectroscopy](https://www.spectroscopyonline.com/topic/portable-and-handheld-spectroscopy)\n* [Process Control and Analysis](https://www.spectroscopyonline.com/topic/process-control-and-analysis)\n* [Quality Control/Quality Assurance (QA/QC)](https://www.spectroscopyonline.com/topic/quality-controlquality-assurance-qaqc)\n* [Quality by Design (QbD)](https://www.spectroscopyonline.com/topic/quality-design-qbd)\n* [Raman Spectroscopy](https://www.spectroscopyonline.com/topic/raman-spectroscopy)\n* [Regulatory Standards/GLP/GMP Compliance](https://www.spectroscopyonline.com/topic/regulatory-standardsglpgmp-compliance)\n* [Sample Preparation](https://www.spectroscopyonline.com/topic/sample-preparation)\n* [Spectroscopy Interviews](https://www.spectroscopyonline.com/topic/spectroscopy-interviews)\n* [Surface-enhanced Raman spectroscopy (SERS)](https://www.spectroscopyonline.com/topic/surface-enhanced-raman-spectroscopy)\n* [Technology Forum](https://www.spectroscopyonline.com/topic/technology-forum)\n* [Trends](https://www.spectroscopyonline.com/topic/trends)\n* [Tutorials](https://www.spectroscopyonline.com/topic/tutorials)\n* [UV-vis Spectroscopy](https://www.spectroscopyonline.com/topic/uv-vis-spectroscopy)\n* [Vendor Tips &amp; Tricks](https://www.spectroscopyonline.com/topic/vendor-tips-tricks)\n* [Web of Science](https://www.spectroscopyonline.com/topic/web-of-science)\n* [X-ray Analysis](https://www.spectroscopyonline.com/topic/x-ray-analysis)\nChoose Topic\nSpotlight -\n[Emerging Trends in Pharma &amp; Biopharma](https://globalmeet.webcasts.com/starthere.jsp?ei=1740514&amp;tp_key=e18836e4d8&amp;sti=mjhspotlight)|\n[Analysis Along the Packaging Value Chain](https://www.spectroscopyonline.com/view/analysis-along-the-packaging-value-chain-using-spectroscopy-microscopy)|\n[Miniaturized Spectroscopy for Biomedicine](https://www.spectroscopyonline.com/view/miniaturized-spectroscopy-for-biomedical-applications)\nAdvertisement\n|Articles|July 1, 2015\n[Spectroscopy](https://www.spectroscopyonline.com/journals/spectroscopy)\n* Spectroscopy-07-01-2015\n* Volume30\n* Issue7\n# Optimizing the Regression Model: The Challenge of Intercept\u2013Bias and Slope \u201cCorrection\u201d\nAuthor(s)[*Howard Mark*](https://www.spectroscopyonline.com/authors/howard-mark),[*Jerome Workman Jr.*](https://www.spectroscopyonline.com/authors/jerome-workman-jr)\nThe archnemesis of calibration modeling and the routine use of multivariate models for quantitative analysis in spectroscopy is the confounded bias or slope adjustments that must be continually implemented to maintain calibration prediction accuracy over time. A perfectly developed calibration model that predicted well on day one suddenly has to be bias adjusted on a regular basis to pass a simple bias test when predicted values are compared to reference values at a later date. Why does this problem continue to plague researchers and users of chemometrics and spectroscopy?\nAdvertisement\n**The archnemesis of calibration modeling and the routine use of multivariate models for quantitative analysis in spectroscopy is the confounded bias or slope adjustments that must be continually implemented to maintain calibration prediction accuracy over time. A perfectly developed calibration model that predicted well on day one suddenly has to be bias adjusted on a regular basis to pass a simple bias test when predicted values are compared to reference values at a later date. Why does this problem continue to plague researchers and users of chemometrics and spectroscopy?**\nThe subject of bias and slope, also known as intercept and slope adjustments, following calibration transfer has been an integral part of the application of multivariate calibrations since the very beginning. It is well understood, and widely accepted, that following the transfer of multivariate calibrations from one instrument to another there is a bias adjustment required for the predicted results to conform to the reference values from a set of reference transfer samples. There have been many attempts to reduce this bias or intercept requirement, but the fact remains that intercept and slope are routinely used for calibration transfer from one instrument to another.\nOne may derive a number of exp...",
      "url": "https://www.spectroscopyonline.com/view/optimizing-regression-model-challenge-intercept-bias-and-slope-correction"
    },
    {
      "title": "Intercept \u2014 xgboost 3.1.0-dev documentation",
      "text": "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/latest/tutorials/index.html)\n- Intercept\n- [View page source](https://xgboost.readthedocs.io/en/latest/_sources/tutorials/intercept.rst.txt)\n* * *\n# Intercept [\uf0c1](https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html\\#intercept)\nAdded in version 2.0.0.\nSince 2.0.0, XGBoost supports estimating the model intercept (named `base_score`)\nautomatically based on targets upon training. The behavior can be controlled by setting\n`base_score` to a constant value. The following snippet disables the automatic\nestimation:\n```\nimportxgboostasxgb\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\nIn addition, here 0.5 represents the value after applying the inverse link function. See\nthe end of the document for a description.\nOther than the `base_score`, users can also provide global bias via the data field\n`base_margin`, which is a vector or a matrix depending on the task. With multi-output\nand multi-class, the `base_margin` is a matrix with size `(n_samples, n_targets)` or\n`(n_samples, n_classes)`.\n```\nimportxgboostasxgb\nfromsklearn.datasetsimport make_regression\nX, y = make_regression()\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\nIt specifies the bias for each sample and can be used for stacking an XGBoost model on top\nof other models, see [Demo for boosting from prediction](https://xgboost.readthedocs.io/en/latest/python/examples/boost_from_prediction.html#sphx-glr-python-examples-boost-from-prediction-py) for a worked\nexample. When `base_margin` is specified, it automatically overrides the `base_score`\nparameter. If you are stacking XGBoost models, then the usage should be relatively\nstraightforward, with the previous model providing raw prediction and a new model using\nthe prediction as bias. For more customized inputs, users need to take extra care of the\nlink function. Let \\\\(F\\\\) be the model and \\\\(g\\\\) be the link function, since\n`base_score` is overridden when sample-specific `base_margin` is available, we will\nomit it here:\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i)\\\\\\]\nWhen base margin \\\\(b\\\\) is provided, it\u2019s added to the raw model output \\\\(F\\\\):\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i) + b\\_i\\\\\\]\nand the output of the final model is:\n\\\\\\[g^{-1}(F(x\\_i) + b\\_i)\\\\\\]\nUsing the gamma deviance objective `reg:gamma` as an example, which has a log link\nfunction, hence:\n\\\\\\[\\\\begin{split}\\\\ln{(E\\[y\\_i\\])} = F(x\\_i) + b\\_i \\\\\\\nE\\[y\\_i\\] = \\\\exp{(F(x\\_i) + b\\_i)}\\\\end{split}\\\\\\]\nAs a result, if you are feeding outputs from models like GLM with a corresponding\nobjective function, make sure the outputs are not yet transformed by the inverse link\n(activation).\nIn the case of `base_score` (intercept), it can be accessed through\n[`save_config()`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.save_config) after estimation. Unlike the `base_margin`, the\nreturned value represents a value after applying inverse link. With logistic regression\nand the logit link function as an example, given the `base_score` as 0.5,\n\\\\(g(intercept) = logit(0.5) = 0\\\\) is added to the raw model output:\n\\\\\\[E\\[y\\_i\\] = g^{-1}{(F(x\\_i) + g(intercept))}\\\\\\]\nand 0.5 is the same as \\\\(base\\\\\\_score = g^{-1}(0) = 0.5\\\\). This is more intuitive if\nyou remove the model and consider only the intercept, which is estimated before the model\nis fitted:\n\\\\\\[\\\\begin{split}E\\[y\\] = g^{-1}{(g(intercept))} \\\\\\\nE\\[y\\] = intercept\\\\end{split}\\\\\\]\nFor some objectives like MAE, there are close solutions, while for others it\u2019s estimated\nwith one step Newton method.\n## Offset [\uf0c1](https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html\\#offset)\nThe `base_margin` is a form of `offset` in GLM. Using the Poisson objective as an\nexample, we might want to model the rate instead of the count:\n\\\\\\[rate = \\\\frac{count}{exposure}\\\\\\]\nAnd the offset is defined as log link applied to the exposure variable:\n\\\\(\\\\ln{exposure}\\\\). Let \\\\(c\\\\) be the count and \\\\(\\\\gamma\\\\) be the exposure,\nsubstituting the response \\\\(y\\\\) in our previous formulation of base margin:\n\\\\\\[g(\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}) = F(x\\_i)\\\\\\]\nSubstitute \\\\(g\\\\) with \\\\(\\\\ln\\\\) for Poisson regression:\n\\\\\\[\\\\ln{\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}} = F(x\\_i)\\\\\\]\nWe have:\n\\\\\\[\\\\begin{split}E\\[c\\_i\\] &= \\\\exp{(F(x\\_i) + \\\\ln{\\\\gamma\\_i})} \\\\\\\nE\\[c\\_i\\] &= g^{-1}(F(x\\_i) + g(\\\\gamma\\_i))\\\\end{split}\\\\\\]\nAs you can see, we can use the `base_margin` for modeling with offset similar to GLMs",
      "url": "https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html"
    },
    {
      "title": "LogisticRegressionCV #",
      "text": "LogisticRegressionCV &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# LogisticRegressionCV[#](#logisticregressioncv)\n*class*sklearn.linear\\_model.LogisticRegressionCV(*\\**,*Cs=10*,*l1\\_ratios='warn'*,*fit\\_intercept=True*,*cv=None*,*dual=False*,*penalty='deprecated'*,*scoring=None*,*solver='lbfgs'*,*tol=0.0001*,*max\\_iter=100*,*class\\_weight=None*,*n\\_jobs=None*,*verbose=0*,*refit=True*,*intercept\\_scaling=1.0*,*random\\_state=None*,*use\\_legacy\\_attributes='warn'*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/linear_model/_logistic.py#L1363)[#](#sklearn.linear_model.LogisticRegressionCV)\nLogistic Regression CV (aka logit, MaxEnt) classifier.\nSee glossary entry for[cross-validation estimator](../../glossary.html#term-cross-validation-estimator).\nThis class implements regularized logistic regression with implicit cross\nvalidation for the penalty parameters`C`and`l1\\_ratio`, see[`LogisticRegression`](sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), using a set of available solvers.\nThe solvers \u2018lbfgs\u2019, \u2018newton-cg\u2019, \u2018newton-cholesky\u2019 and \u2018sag\u2019 support only L2\nregularization with primal formulation. The \u2018liblinear\u2019\nsolver supports both L1 and L2 regularization (but not both, i.e. elastic-net),\nwith a dual formulation only for the L2 penalty. The Elastic-Net (combination of L1\nand L2) regularization is only supported by the \u2018saga\u2019 solver.\nFor the grid of`Cs`values and`l1\\_ratios`values, the best hyperparameter\nis selected by the cross-validator[`StratifiedKFold`](sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), but it can be changed\nusing the[cv](../../glossary.html#term-cv)parameter. All solvers except \u2018liblinear\u2019 can warm-start the\ncoefficients (see[Glossary](../../glossary.html#term-warm_start)).\nRead more in the[User Guide](../linear_model.html#logistic-regression).\nParameters:**Cs**int or list of floats, default=10\nEach of the values in Cs describes the inverse of regularization\nstrength. If Cs is as an int, then a grid of Cs values are chosen\nin a logarithmic scale between 1e-4 and 1e4.\nLike in support vector machines, smaller values specify stronger\nregularization.\n**l1\\_ratios**array-like of shape (n\\_l1\\_ratios), default=None\nFloats between 0 and 1 passed as Elastic-Net mixing parameter (scaling between\nL1 and L2 penalties). For`l1\\_ratio=0`the penalty is an L2 penalty. For`l1\\_ratio=1`it is an L1 penalty. For`0&lt;l1\\_ratio&lt;1`, the penalty is a\ncombination of L1 and L2.\nAll the values of the given array-like are tested by cross-validation and the\none giving the best prediction score is used.\nWarning\nCertain values of`l1\\_ratios`, i.e. some penalties, may not work with some\nsolvers. See the parameter`solver`below, to know the compatibility between\nthe penalty and solver.\nDeprecated since version 1.8:`l1\\_ratios=None`is deprecated in 1.8 and will raise an error\nin version 1.10. Default value will change from`None`to`(0.0,)`in version 1.10.\n**fit\\_intercept**bool, default=True\nSpecifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.\n**cv**int or cross-validation generator, default=None\nThe default cross-validation generator used is Stratified K-Folds.\nIf an integer is provided, it specifies the number of folds,`n\\_folds`, used.\nSee the module[`sklearn.model\\_selection`](../../api/sklearn.model_selection.html#module-sklearn.model_selection)module for the\nlist of possible cross-validation objects.\nChanged in version 0.22:`cv`default value if None changed from 3-fold to 5-fold.\n**dual**bool, default=False\nDual (constrained) or primal (regularized, see also[this equation](../linear_model.html#regularized-logistic-loss)) formulation. Dual formulation\nis only implemented for l2 penalty with liblinear solver. Prefer dual=False when\nn\\_samples &gt;&gt; n\\_features.\n**penalty**{\u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019}, default=\u2019l2\u2019\nSpecify the norm of the penalty:\n* `'l2'`: add a L2 penalty term (used by default);\n* `'l1'`: add a L1 penalty term;\n* `'elasticnet'`: both L1 and L2 penalty terms are added.\nWarning\nSome penalties may not work with some solvers. See the parameter`solver`below, to know the compatibility between the penalty and\nsolver.\nDeprecated since version 1.8:`penalty`was deprecated in version 1.8 and will be removed in 1.10.\nUse`l1\\_ratio`instead.`l1\\_ratio=0`for`penalty='l2'`,`l1\\_ratio=1`for`penalty='l1'`and`l1\\_ratio`set to any float between 0 and 1 for`'penalty='elasticnet'`.\n**scoring**str or callable, default=None\nThe scoring method to use for cross-validation. Options:\n* str: see[String name scorers](../model_evaluation.html#scoring-string-names)for options.\n* callable: a scorer callable object (e.g., function) with signature`scorer(estimator,X,y)`. See[Callable scorers](../model_evaluation.html#scoring-callable)for details.\n* `None`:[accuracy](../model_evaluation.html#accuracy-score)is used.\n**solver**{\u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018newton-cg\u2019, \u2018newton-cholesky\u2019, \u2018sag\u2019, \u2018saga\u2019}, default=\u2019lbfgs\u2019\nAlgorithm to use in the optimization problem. Default is \u2018lbfgs\u2019.\nTo choose a solver, you might want to consider the following aspects:\n* \u2018lbfgs\u2019 is a good default solver because it works reasonably well for a wide\nclass of problems.\n* For[multiclass](../../glossary.html#term-multiclass)problems (`n\\_classes&gt;=3`), all solvers except\n\u2018liblinear\u2019 minimize the full multinomial loss, \u2018liblinear\u2019 will raise an\nerror.\n* \u2018newton-cholesky\u2019 is a good choice for`n\\_samples`&gt;&gt;`n\\_features\\*n\\_classes`, especially with one-hot encoded\ncategorical features with rare categories. Be aware that the memory usage\nof this solver has a quadratic dependency on`n\\_features\\*n\\_classes`because it explicitly computes the full Hessian matrix.\n* For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019\nand \u2018saga\u2019 are faster for large ones;\n* \u2018liblinear\u2019 might be slower in[`LogisticRegressionCV`](#sklearn.linear_model.LogisticRegressionCV)because it does not handle warm-starting.\n* \u2018liblinear\u2019 can only handle binary classification by default. To apply a\none-versus-rest scheme for the multiclass setting one can wrap it with the[`OneVsRestClassifier`](sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier).\nWarning\nThe choice of the algorithm depends on the penalty (`l1\\_ratio=0`for\nL2-penalty,`l1\\_ratio=1`for L1-penalty and`0&lt;l1\\_ratio&lt;1`for\nElastic-Net) chosen and on (multinomial) multiclass support:\n|\nsolver\n|\nl1\\_ratio\n|\nmultinomial multiclass\n|\n\u2018lbfgs\u2019\n|\nl1\\_ratio=0\n|\nyes\n|\n\u2018liblinear\u2019\n|\nl1\\_ratio=1 or l1\\_ratio=0\n|\nno\n|\n\u2018newton-cg\u2019\n|\nl1\\_ratio=0\n|\nyes\n|\n\u2018newton-cholesky\u2019\n|\nl1\\_ratio=0\n|\nyes\n|\n\u2018sag\u2019\n|\nl1\\_ratio=0\n|\nyes\n|\n\u2018saga\u2019\n|\n0&lt;&lt;=l1\\_ratio&lt;&lt;=1\n|\nyes\n|\nNote\n\u2018sag\u2019 and \u2018saga\u2019 fast convergence is only guaranteed on features\nwith approximately the same scale. You can preprocess the data with\na scaler from[`sklearn.preprocessing`](../../api/sklearn.preprocessing.html#module-sklearn.preprocessing).\nAdded in version 0.17:Stochastic Average Gradient (SAG) descent solver. Multinomial support in\nversion 0.18.\nAdded in version 0.19:SAGA solver.\nAdded in version 1.2:newton-cholesky solver. Multinomial support in version 1.6.\n**tol**float, default=1e-4\nTolerance for stopping criteria.\n**max\\_iter**int, default=100\nMaximum number of iterations of the optimization algorithm.\n**class\\_weight**dict or \u2018balanced\u2019, default=None\nWeights associated with classes in the form`{class\\_label:weight}`.\nIf not given, all classes are supposed to have weight one.\nThe \u201cbalanced\u201d mode uses the values of y to automaticall...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html"
    },
    {
      "title": "6 \u00a0  Linear Regression",
      "text": "6 Linear Regression \u2013Interpretable Machine Learning\n# 6Linear Regression\nA linear regression model predicts the target as a weighted sum of the feature inputs. The linearity of the learned relationship makes the interpretation easy. Linear regression models have long been used by statisticians, computer scientists, and other people who tackle quantitative problems.\nLinear models can be used to model the dependence of a regression target y on features\\\\(\\\\mathbf{x}\\\\). The learned relationships can be written for a single instance i as follows:\n\\\\[y=\\\\beta\\_{0}+\\\\beta\\_{1}x\\_{1}+\\\\ldots+\\\\beta\\_{p}x\\_{p}+\\\\epsilon\\\\]\nThe predicted outcome of an instance is a weighted sum of its p features. The betas\\\\(\\\\beta\\_{j}, j \\\\in 1, \\\\ldots, p\\\\)represent the learned feature weights or coefficients. The first weight in the sum,\\\\(\\\\beta\\_0\\\\), is called the intercept and is not multiplied with a feature. The epsilon\\\\(\\\\epsilon\\\\)is the error we still make, i.e.the difference between the prediction and the actual outcome.[1](#fn1)These errors are assumed to follow a Gaussian distribution, which means that we make errors in both negative and positive directions and make many small errors and few large errors.\nTo find the best coefficient, we typically minimize the squared differences between the actual and the estimated outcomes:\n\\\\[\\\\hat{\\\\boldsymbol{\\\\beta}} = \\\\arg\\\\!\\\\min\\_{\\\\beta\\_0, \\\\ldots, \\\\beta\\_p} \\\\sum\\_{i=1}^n \\\\left( y^{(i)} - \\\\left( \\\\beta\\_0 + \\\\sum\\_{j=1}^p \\\\beta\\_j x^{(i)}\\_{j} \\\\right) \\\\right)^{2}\\\\]\nWe will not discuss in detail how the optimal weights can be found, but if you are interested, you can read chapter 3.2 of the book \u201cThe Elements of Statistical Learning\u201d([Hastie 2009](references.html#ref-hastie2009elements))or one of the other online resources on linear regression models.\nThe biggest advantage of linear regression models is linearity: It makes the estimation procedure simple, and most importantly, these linear equations have an easy-to-understand interpretation on a modular level (i.e., the weights). This is one of the main reasons why the linear model and all similar models are so widespread in academic fields such as medicine, sociology, psychology, and many other quantitative research fields. For example, in the medical field, it is not only important to predict the clinical outcome of a patient, but also to quantify the influence of the drug and at the same time take sex, age, and other features into account in an interpretable way.\nEstimated weights come with confidence intervals. A confidence interval is a range for the weight estimate that covers the \u201ctrue\u201d weight with a certain confidence. For example, a 95% confidence interval for a weight of 2 could range from 1 to 3. The interpretation of this interval would be: If we repeated the estimation 100 times with newly sampled data, the confidence interval would include the true weight in 95 out of 100 cases, given that the linear regression model is the correct model for the data.\nWhether the model is the \u201ccorrect\u201d model depends on whether the relationships in the data meet certain assumptions, which are linearity, normality, homoscedasticity, independence, fixed features, and absence of multicollinearity.\n**\nAssumptions are optional.\nYou only need the assumptions to get further things out of the linear model like confidence intervals.\n**Linearity**\nThe linear regression model forces the prediction to be a linear combination of features, which is both its greatest strength and its greatest limitation. Linearity leads to interpretable models. Linear effects are easy to quantify and describe. They are additive, so it\u2019s easy to separate the effects. If you suspect feature interactions or a nonlinear association of a feature with the target value, you can add interaction terms or use regression splines.\n**Normality**\nIt\u2019s assumed that the target outcome given the features follows a normal distribution. If this assumption is violated, the estimated confidence intervals of the feature weights are invalid.\n**Homoscedasticity**(constant variance)\nThe variance of the error terms is assumed to be constant over the entire feature space. Suppose you want to predict the value of a house given the living area in square meters. You estimate a linear model that assumes that, regardless of the size of the house, the error around the predicted response has the same variance. This assumption is often violated in reality. In the house example, it\u2019s plausible that the variance of error terms around the predicted price is higher for larger houses since with higher prices there is more room for price fluctuations. Suppose the average error (difference between predicted and actual price) in your linear regression model is 50,000 Euros. If you assume homoscedasticity, you assume that the average error of 50,000 is the same for houses that cost 1 million and for houses that cost only 40,000. This is unreasonable because it would mean that we can expect negative house prices.\n**Independence**\nIt\u2019s assumed that each instance is independent of any other instance.\nIf you perform repeated measurements, such as multiple blood tests per patient, the data points are not independent. For dependent data, you need special linear regression models, such as mixed effect models or GEEs.\nIf you use the \u201cnormal\u201d linear regression model, you might draw wrong conclusions from the model.\n**Fixed features**\nThe input features are considered \u201cfixed\u201d. Fixed means that they are treated as \u201cgiven constants\u201d and not as statistical variables. This implies that they are free of measurement errors. This is a rather unrealistic assumption. Without that assumption, however, you would have to fit very complex measurement error models that account for the measurement errors of your input features. And usually you don\u2019t want to do that.\n**Absence of multicollinearity**\nYou do not want strongly correlated features because this messes up the estimation of the weights. In a situation where two features are strongly correlated, it becomes problematic to estimate the weights because the feature effects are additive and it becomes indeterminable to which of the correlated features to attribute the effects.\n## Interpretation\nThe interpretation of a weight in the linear regression model depends on the type of the corresponding feature.\n* Numerical feature: Increasing the numerical feature by one unit changes the estimated outcome by its weight. An example of a numerical feature is the size of a house.\n* Binary feature: A feature that takes one of two possible values for each instance. An example is the feature \u201cHouse comes with a garden\u201d. One of the values counts as the reference category (in some programming languages encoded with 0), such as \u201cNo garden\u201d. Changing the feature from the reference category to the other category changes the estimated outcome by the feature\u2019s weight.\n* Categorical feature with multiple categories: A feature with a fixed number of possible values. An example is the feature \u201cfloor type,\u201d with possible categories \u201ccarpet,\u201d \u201claminate,\u201d and \u201cparquet.\u201d A solution to deal with many categories is one-hot encoding, meaning that each category has its own binary column. For a categorical feature with L categories, you only need L-1 columns because the L-th column would have redundant information. For example, when columns 1 to L-1 all have value 0 for one instance, we know that the categorical feature of this instance takes on category L. The interpretation for each category is then the same as the interpretation for binary features. Some languages, such as R, allow you to encode categorical features in various ways, as[described later in this chapter](#cat-code).\n* Intercept\\\\(\\\\beta\\_0\\\\): The intercept is the feature weight for the \u201cconstant feature,\u201d which is always 1 for all instances. Most software packages automatically add this \u201c1\u201d feature to estimate the intercept. The interpretation is: For an instance wit...",
      "url": "https://christophm.github.io/interpretable-ml-book/limo.html"
    },
    {
      "title": "Meta-Learning Linear Models for Molecular Property Prediction",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2509.13527** (cs)\n\n\\[Submitted on 16 Sep 2025\\]\n\n# Title:Meta-Learning Linear Models for Molecular Property Prediction\n\nAuthors: [Yulia Pimonova](https://arxiv.org/search/cs?searchtype=author&query=Pimonova,+Y), [Michael G. Taylor](https://arxiv.org/search/cs?searchtype=author&query=Taylor,+M+G), [Alice Allen](https://arxiv.org/search/cs?searchtype=author&query=Allen,+A), [Ping Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+P), [Nicholas Lubbers](https://arxiv.org/search/cs?searchtype=author&query=Lubbers,+N)\n\nView a PDF of the paper titled Meta-Learning Linear Models for Molecular Property Prediction, by Yulia Pimonova and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2509.13527) [HTML (experimental)](https://arxiv.org/html/2509.13527v1)\n\n> Abstract:Chemists in search of structure-property relationships face great challenges due to limited high quality, concordant datasets. Machine learning (ML) has significantly advanced predictive capabilities in chemical sciences, but these modern data-driven approaches have increased the demand for data. In response to the growing demand for explainable AI (XAI) and to bridge the gap between predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear Algorithm for Meta-Learning that preserves interpretability while improving the prediction accuracy across multiple properties. While most approaches treat each chemical prediction task in isolation, LAMeL leverages a meta-learning framework to identify shared model parameters across related tasks, even if those tasks do not share data, allowing it to learn a common functional manifold that serves as a more informed starting point for new unseen tasks. Our method delivers performance improvements ranging from 1.1- to 25-fold over standard ridge regression, depending on the domain of the dataset. While the degree of performance enhancement varies across tasks, LAMeL consistently outperforms or matches traditional linear methods, making it a reliable tool for chemical property prediction where both accuracy and interpretability are critical.\n\n|     |     |\n| --- | --- |\n| Comments: | 26 pages, 16 figures |\n| Subjects: | Machine Learning (cs.LG); Chemical Physics (physics.chem-ph) |\n| Report\u00a0number: | LA-UR-25-28399 |\n| Cite as: | [arXiv:2509.13527](https://arxiv.org/abs/2509.13527) \\[cs.LG\\] |\n| (or [arXiv:2509.13527v1](https://arxiv.org/abs/2509.13527v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2509.13527](https://doi.org/10.48550/arXiv.2509.13527) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yulia Pimonova \\[ [view email](https://arxiv.org/show-email/959c963c/2509.13527)\\] **\\[v1\\]**\nTue, 16 Sep 2025 20:41:45 UTC (6,105 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Meta-Learning Linear Models for Molecular Property Prediction, by Yulia Pimonova and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2509.13527)\n- [HTML (experimental)](https://arxiv.org/html/2509.13527v1)\n- [TeX Source](https://arxiv.org/src/2509.13527)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2509.13527&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2509.13527&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-09](https://arxiv.org/list/cs.LG/2025-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2509.13527?context=cs) [physics](https://arxiv.org/abs/2509.13527?context=physics) [physics.chem-ph](https://arxiv.org/abs/2509.13527?context=physics.chem-ph)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2509.13527)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2509.13527)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2509.13527)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2509.13527) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2509.13527"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    }
  ]
}