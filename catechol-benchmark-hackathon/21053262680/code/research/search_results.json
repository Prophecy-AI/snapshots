{
  "query": "How do you implement a Graph Attention Network for solvent property prediction that generalizes to unseen molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Based on recent studies of graph\u2011attention models for solubility (e.g., MoGAT\u202f[1] and other attention\u2011based GNNs\u202f[2]) and an open\u2011source PyTorch/DGL implementation\u202f[3], you can build a Graph Attention Network (GAT) that predicts solvent\u2011related properties and generalizes to unseen molecules by following these core steps:**\n\n1. **Prepare the molecular dataset**  \n   - Collect SMILES strings for solute\u2013solvent pairs and the target property (e.g., log\u202fS, solvation energy).  \n   - Convert each SMILES to a graph: nodes\u202f=\u202fatoms (features\u202f=\u202f atom type, hybridization, partial charge, etc.), edges\u202f=\u202fbonds (bond type, aromaticity). Libraries such as **RDKit** can generate these features.  \n   - Split the data so that the test set contains molecules **not present** in the training set (scaffold split or random split with low overlap) to evaluate true generalization\u202f[1][3].\n\n2. **Build a multi\u2011order graph attention architecture**  \n   - Stack several **graph attention layers** (e.g., `GATConv` from DGL or PyTorch Geometric).  \n   - After each layer, extract the node embeddings; these capture information from 1\u2011hop, 2\u2011hop, \u2026 neighborhoods.  \n   - Merge the embeddings from all orders with an **attention\u2011based read\u2011out** (a learned weighted sum) to obtain a single graph\u2011level vector, as done in the **MoGAT** model\u202f[1].  \n   - Optionally add a lightweight **convolutional block** before attention (as in **CoAtGIN**) to enrich local patterns\u202f[4].\n\n3. **Define the prediction head**  \n   - Feed the final graph embedding into a small fully\u2011connected network (e.g., two linear layers with ReLU) that outputs the scalar property.  \n   - Use **Mean Squared Error** (MSE) loss for regression tasks.\n\n4. **Train with regularization that promotes generalization**  \n   - Optimize with Adam (learning rate\u202f\u2248\u202f1e\u20113) and apply dropout (\u2248\u202f0.2) on attention coefficients.  \n   - Include **early stopping** on a validation scaffold split.  \n   - If a large pre\u2011trained message\u2011passing model is available (e.g., the solvation\u2011energy MP\u2011NN from the QM9 study\u202f[5]), fine\u2011tune it on your solvent data to leverage transfer learning\u202f[5].\n\n5. **Evaluate on truly unseen molecules**  \n   - Compute MAE / RMSE on the held\u2011out test set.  \n   - Inspect the learned **attention scores** to verify that the model highlights chemically relevant atoms (MoGAT provides atomic\u2011importance scores)\u202f[1].  \n   - Compare against baseline models (e.g., standard GCN, random forest on fingerprints) to confirm improved generalization.\n\n6. **Deploy or infer on new compounds**  \n   - Use the same preprocessing pipeline to convert new SMILES to graphs.  \n   - Run the trained GAT model to obtain property predictions; the attention mechanism ensures the model can extrapolate to novel chemical scaffolds because it bases decisions on local atomic environments rather than memorized whole\u2011molecule patterns\u202f[2][3].\n\n**References**  \n[1]\u202fLee\u202fet\u202fal., \u201cMulti\u2011order graph attention network for water solubility prediction and interpretation\u201d (2022) \u2013 introduces multi\u2011order attention and atomic importance scores.  \n[2]\u202fChong\u202fet\u202fal., \u201cAttention\u2011Based Graph Neural Network for Molecular Solubility Prediction\u201d (2023) \u2013 demonstrates GAT for solubility.  \n[3]\u202flowkc, **solv_gnn** GitHub repo (PyTorch\u202f+\u202fDGL implementation for solvent solubility)\u202f[2021].  \n[4]\u202fIEEE, \u201cCoAtGIN: Marrying Convolution and Attention for Graph\u2011based Molecule Property Prediction\u201d (2025) \u2013 shows convolution\u2011plus\u2011attention design.  \n[5]\u202fWard\u202fet\u202fal., \u201cGraph\u2011Based Approaches for Predicting Solvation Energy\u201d (2021) \u2013 provides a pre\u2011trained message\u2011passing model useful for transfer learning.",
      "url": ""
    },
    {
      "title": "Multi-order graph attention network for water solubility prediction and interpretation",
      "text": "1\nVol.:(0123456789)\nScientifc Reports | (2023) 13:957 | https://doi.org/10.1038/s41598-022-25701-5\nwww.nature.com/scientificreports\nMulti\u2011order graph attention \nnetwork for water solubility \nprediction and interpretation\nSangho Lee1,2,10, Hyunwoo Park1,2,10, Chihyeon Choi1,2, Wonjoon Kim3, Ki Kang Kim4,5, \nYoung\u2011Kyu Han6, Joohoon Kang7,8, Chang\u2011Jong Kang9* & Youngdoo Son1,2*\nThe water solubility of molecules is one of the most important properties in various chemical \nand medical research felds. Recently, machine learning-based methods for predicting molecular \nproperties, including water solubility, have been extensively studied due to the advantage of \nefectively reducing computational costs. Although machine learning-based methods have made \nsignifcant advances in predictive performance, the existing methods were still lacking in interpreting \nthe predicted results. Therefore, we propose a novel multi-order graph attention network (MoGAT) for \nwater solubility prediction to improve the predictive performance and interpret the predicted results. \nWe extracted graph embeddings in every node embedding layer to consider the information of diverse \nneighboring orders and merged them by attention mechanism to generate a fnal graph embedding. \nMoGAT can provide the atomic-specifc importance scores of a molecule that indicate which atoms \nsignifcantly infuence the prediction so that it can interpret the predicted results chemically. It also \nimproves prediction performance because the graph representations of all neighboring orders, \nwhich contain diverse range of information, are employed for the fnal prediction. Through extensive \nexperiments, we demonstrated that MoGAT showed better performance than the state-of-the-art \nmethods, and the predicted results were consistent with well-known chemical knowledge.\nSince most chemical and biological reactions occur when dissolved in water, the water solubility of a molecule \nor polymer is an important factor in various academic and industrial felds such as chemistry, biochemistry, \nfood engineering, medical, and pharmaceutical industries. For example, biological activities such as the reac\u0002tion between proteins1\n, protein and nucleic acid structures2, protein-substrate binding3, and protein folding3 are \nconducted in the liquid state4\n; thus, solubility plays an important role in dosage forms and desired concentration \nof drugs to achieve the required pharmacological response5\n.\nAccurate measurement of the water solubility of a molecule involves rigorous and time-consuming experi\u0002ments that are highly sensitive to the external environment. Furthermore, although there are several theoretical \nmodels for computing solubility6,7\n, these models were empirically constructed using only a small amount of \nexperimental data. Terefore, building a general empirical model is challenging for a large set of experimental \ndata using the existing theoretical models.\nRecently, to overcome this limitation, various machine learning (ML)-based methods have been widely intro\u0002duced to predict solubility and other molecular properties using molecular features, including molecular weights, \nring structures, and aromatic properties8\u201311. Some studies have improved the prediction performance with graphs \nconsisting of nodes and edges representing atoms and bonds, respectively, as inputs12\u201315. To efectively capture \nthe structural characteristics of the constructed graphs, they used various graph neural network (GNN)-based \nmethods, including message passing neural network (MPNN)13 and its variants with attention16,17, to predict \nOPEN\n1\nDepartment of Industrial and Systems Engineering, Dongguk University-Seoul, Seoul 04620, South \nKorea. 2\nData Science Laboratory (DSLAB), Dongguk University-Seoul, Seoul 04620, South Korea. 3Division of \nFuture Convergence (HCI Science Major), Dongduk Women\u2019s University, Seoul 02748, South Korea. 4\nDepartment \nof Energy Science, Sungkyunkwan University (SKKU), Suwon 16419, South Korea. 5\nCenter for Integrated \nNanostructure Physics (CINAP), Institute for Basic Science (IBS), Sungkyunkwan University (SKKU), Suwon 16419, \nSouth Korea. 6\nDepartment of Energy and Materials Engineering, Dongguk University-Seoul, Seoul 04620, \nSouth Korea. 7\nSchool of Advanced Materials Science and Engineering, Sungkyunkwan University (SKKU), \nSuwon 16419, South Korea. 8\nKIST\u2011SKKU Carbon\u2011Neutral Research Center, Sungkyunkwan University (SKKU), \nSuwon 16419, South Korea. 9\nDepartment of Physics, Chungnam National University, Daejeon 34134, South \nKorea. 10These authors contributed equally: Sangho Lee and Hyunwoo Park. *email: cjkang87@cnu.ac.kr; \nyoungdoo@dongguk.edu\n2\nVol:.(1234567890)\nScientifc Reports | (2023) 13:957 | https://doi.org/10.1038/s41598-022-25701-5\nwww.nature.com/scientificreports/\nmolecular property. Tey also demonstrated the graph could efectively represent the structural characteristics \nof the molecules. Although the GNN-based methods improved the predictive performance of molecular proper\u0002ties, most of them failed to interpret factors that substantially impact molecular properties prediction18. Analyz\u0002ing the impacts of each factor in molecules provides confdence in the results; thus, the GNN-based methods \nshould interpret which atoms in a molecule highly afect the prediction in a similar way to the well-known \nchemical knowledge. AttentiveFP14, one of the GNN-based chemical property prediction methods, can provide \nthe importance of each atom in predicting molecular properties. Specifcally, AttentiveFP well-interpreted the \npredicted results through the importance of each atom obtained from neighbors\u2019 information of the fnal node \nembedding layer, as well as achieved state-of-the-art performance. However, it can only consider the neighbors\u2019 \ninformation from the last node embedding layer and cannot directly refect the information of the diferent \nneighboring orders obtained from the other layers.\nTerefore, we propose a multi-order graph attention network (MoGAT) for water solubility prediction to \nimprove the performances of prediction and interpretation with diverse aspects of neighbors\u2019 information. \nFirst, for each node embedding layer, we derived node embeddings, which imply the hidden states of each atom, \nupdated by refecting information of its neighbors. Ten, graph embeddings representing the whole molecule \nat every node embedding layer are calculated. Finally, a fnal graph embedding is derived by giving weights \ncalculated with the sofmax function to the graph embeddings. Te graph embeddings obtained from every \nnode embedding layer refect the information of diferent neighboring orders; thus, the fnal graph embedding \nprovides useful information in predicting water solubility. In addition, the weights calculated with the sofmax \nfunction to the graph embeddings imply the importance scores of each atom so that we can interpret the efect \nof each atom on the predicted results.\nTo verify the predictive performance of MoGAT, we performed several experiments with extensive datasets. \nAs a result, we demonstrated that MoGAT achieved better performance than the existing GNN-based methods. \nFurthermore, we interpreted which atoms in a molecule are important for water solubility by deriving atomic\u0002specifc importance by integrating information of diverse neighboring orders. Te importance scores of atoms \nwere also consistent with the chemical intuitions from the existing calculation results19.\nTe rest of this paper is organized as follows. In the next section, the preliminaries and a detailed algorithm of \nthe proposed method are described. Ten, we present the experimental results on various benchmark datasets, \nwhich demonstrate the efectiveness of MoGAT. Finally, we conclude with a discussion on the limitations of \nMoGAT and mention of future research directions.\nMethodology\nIn this section, we frst briefy explain the attention mechanism applied to GNNs. Ten, we propose a novel graph \nattention network...",
      "url": "https://www.nature.com/articles/s41598-022-25701-5.pdf?error=cookies_not_supported&code=2785c419-d838-4604-b192-b75db15678b9"
    },
    {
      "title": "Attention-Based Graph Neural Network for Molecular Solubility Prediction",
      "text": "![](https://d.adroll.com/cm/b/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/g/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/index/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/n/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/o/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/outbrain/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/pubmatic/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/r/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/taboola/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/triplelift/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)![](https://d.adroll.com/cm/x/out?adroll_fpc=d1d19fcc6b48b9827a1336e1cb69c0be-1720385875868&pv=29668980875.68539&arrfrr=https%3A%2F%2Fpubs.acs.org%2Fdoi%2Ffull%2F10.1021%2Facsomega.2c06702&advertisable=3LBZJ4KXKBF2PJ46QCMT3X)\n\nRecently Viewed [close modal](javascript:void(0))\n\nRecently Viewed\n\n#### You have not visited any articles yet, Please visit some articles to see contents here.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nPair your accounts.\n\nExport articles to Mendeley\n\nGet article recommendations from ACS based on references in your Mendeley library.\n\nYou\u2019ve supercharged your research process with ACS and Mendeley!\n\nContinue\n\n###### STEP 1:\n\nLogin with ACS IDLogged in SuccessClick to create an ACS ID\n\n###### STEP 2:\n\nLogin with MendeleyLogged in Success [Create a Mendeley account](https://id.elsevier.com/as/authorization.oauth2?state=c33c27125763433d4d32a15accaacc18&prompt=login&scope=openid%20email%20profile%20els_auth_info&authType=SINGLE_SIGN_IN&response_type=code&platSite=MDY%2Fmendeley&redirect_uri=https%3A%2F%2Fwww.mendeley.com%2Fcallback%2F&client_id=MENDELEY)\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease note: If you switch to a different device, you may be asked to login again with only your ACS ID.\n\nPlease login with your ACS ID before connecting to your Mendeley account.\n\nLogin with ACS ID\n\nMENDELEY PAIRING EXPIREDReconnect\n\nYour Mendeley pairing has expired. Please reconnect\n\n![Figure 1](https://pubs.acs.org/doi/full/10.1021/acsomega.2c06702)![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n\n[Download Hi-Res Image](https://pubs.acs.org/doi/full/10.1021/acsomega.2c06702) [Download to MS-PowerPoint](https://pubs.acs.org/doi/full/10.1021/acsomega.2c06702) [**Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.2c06702&href=/doi/full/10.1021/acsomega.2c06702) _ACS Omega_ 2023, 8, 3, 3236-3244\n\n[ADVERTISEMENT](http://acsmediakit.org)\n\n[RETURN TO ISSUE](https://pubs.acs.org/toc/acsodf/8/3) [PREV](https://pubs.acs.org/doi/10.1021/acsomega.2c06691) Article [NEXT](https://pubs.acs.org/doi/10.1021/acsomega.2c06705)\n\n[![Journal Logo](https://pubs.acs.org/doi/full/10.1021/specs/products/achs/releasedAssets/images/loading/loader-128b5db1cc3a83761a15cf2e5c9b452d.gif)](https://pubs.acs.org/journal/acsodf)\n\n[Get e-Alerts](https://pubs.acs.org/doi/full/10.1021/acsomega.2c06702) close\n\n# Attention-Based Graph Neural Network for Molecular Solubility Prediction\n\n- Waqar Ahmad\n\n\n\n\nWaqar Ahmad\n\n\n\n\n\nDepartment of Electronics and Information Engineering, Jeonbuk National University, Jeonju54896, South Korea\n\n\n\n\n\nMore by [Waqar Ahmad](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Waqar++Ahmad)\n\n- ,\n- Hilal Tayara **\\***\n\n\n\n\nHilal Tayara\n\n\n\n\n\nSchool of International Engineering and Science, Jeonbuk National University, Jeonju54896, South Korea\n\n\n\n**\\*** Email: [hilaltayara@jbnu.ac.kr](mailto:hilaltayara@jbnu.ac.kr)\n\nMore by [Hilal Tayara](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Hilal++Tayara)\n\n\n\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0001-5678-3479](https://orcid.org/0000-0001-5678-3479)\n\n- ,\u00a0and\n- Kil To Chong **\\***\n\n\n\n\nKil To Chong\n\n\n\n\n\nDepartment of Electronics and Information Engineering, Jeonbuk National University, Jeonju54896, South Korea\n\n\n\nAdvanced Electronics and Information Research Center, Jeonbuk National University, Jeonju54896, South Korea\n\n\n\n**\\*** Email: [kitchong@jbnu.ac.kr](mailto:kitchong@jbnu.ac.kr)\n\nMore by [Kil To Chong](https://pubs.acs.org/action/doSearch?field1=Contrib&text1=Kil+To++Chong)\n\n\n[**Cite this:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021%2Facsomega.2c06702&href=/doi/10.1021%2Facsomega.2c06702) _ACS Omega_2023, 8, 3, 3236\u20133244\n\nPublication Date (Web):January 12, 2023\n\n#### Publication History\n\n- **Received**18 October 2022\n- **Accepted**23 December 2022\n- **Published** online12 January 2023\n- **Published** inissue 24 January 2023\n\n[https://pubs.acs.org/doi/10.1021/acsomega.2c06702](https://pubs.acs.org/doi/10.1021/acsomega.2c06702)\n\n[https://doi.org/10.1021/acsomega.2c06702](https://doi.org/10.1021/acsomega.2c06702)\n\nresearch-article\n\nACS Publications**Copyright \u00a9 2023 The Authors. Published by American Chemical Society**. This publication is licensed under\n\n[CC-BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n### License Summary\\*\n\nYou are free to share (copy and redistribute) this article in any medium or format within the parameters below:\n\n- ![cc licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/cc.svg)\nCreative Commons (CC): This is a Creative Commons license.\n\n- ![ny licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/by.svg)\nAttribution (BY): Credit must be given to the creator.\n\n- ![nc licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/nc.svg)\nNon-Commercial (NC): Only non-commercial uses of the work are\npermitted.\n\n- ![nd licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/nd.svg)\nNo Derivatives (ND): Derivative works may be created for\nnon-commercial purposes, but sharing is prohibited.\n\n\n[View full license](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n\n\\*Disclaimer\n\nThis summary highlights only some of the key feature...",
      "url": "https://pubs.acs.org/doi/full/10.1021/acsomega.2c06702"
    },
    {
      "title": "GitHub - VanshRamani/Molmerger-Solubility-Prediction",
      "text": "<div><div><article><p></p><h2>MolMerger Solubility Prediction</h2><a href=\"#molmerger-solubility-prediction\"></a><p></p>\n<p><a href=\"https://camo.githubusercontent.com/0999daed0faac008583eab199f7d10e8eec0793fabb32f66dd5fe5b677e2d369/68747470733a2f2f692e6962622e636f2f77464b5237737a462f696d6167652e706e67\"></a></p>\n<p></p><h2>Overview</h2><a href=\"#overview\"></a><p></p>\n<p>This repository contains the implementation of <em>MolMerger</em>, a Graph Neural Network (GNN)-based approach for predicting solubility in diverse solvents by incorporating explicit solute-solvent interactions. The method is described in the paper:</p>\n<blockquote>\n<p><strong>Graph Neural Networks for Predicting Solubility in Diverse Solvents Using MolMerger Incorporating Solute\u2013Solvent Interactions</strong><br/>\n<em>Vansh Ramani and Tarak Karmakar</em><br/>\nJournal of Chemical Theory and Computation, 2024, 20 (15), 6549-6558<br/>\nDOI: <a href=\"https://doi.org/10.1021/acs.jctc.4c00382\">10.1021/acs.jctc.4c00382</a></p>\n</blockquote>\n<p>The <em>MolMerger</em> approach captures solute-solvent interactions using Gasteiger charges to construct molecular graphs, which are then processed by a graph neural network to predict LogS values.</p>\n<p></p><h2>Repository Structure</h2><a href=\"#repository-structure\"></a><p></p>\n<div><pre><code>\u251c\u2500\u2500 MolMergerModel.ipynb # Jupyter Notebook with implementation and usage details\n\u251c\u2500\u2500 FinalTrainedModel.pt # Pretrained model for inference\n\u251c\u2500\u2500 trainset.csv # Training dataset\n\u251c\u2500\u2500 EvalData.csv # Evaluation dataset\n\u251c\u2500\u2500 Results.csv # Model predictions\n\u251c\u2500\u2500 README.md # This file\n</code></pre></div>\n<p></p><h2>Getting Started</h2><a href=\"#getting-started\"></a><p></p>\n<p>All necessary steps to run the model, including data preprocessing, model training, and inference, are provided in <strong>MolMergerModel.ipynb</strong>. Please follow the instructions in the notebook to reproduce the results.</p>\n<p></p><h2>Dataset</h2><a href=\"#dataset\"></a><p></p>\n<p>The dataset used in this study was compiled from multiple sources, including <strong>BigSolDB</strong>, <strong>BNNLabs Solubility</strong>, and <strong>ESOL</strong>. The dataset consists of <strong>6,975 solute-solvent pairs</strong>, carefully curated to ensure high-quality data. The dataset was split into:</p>\n<ul>\n<li><strong>Train-Test Set</strong> (5,493 pairs): Used for training and validation.</li>\n<li><strong>Robust Evaluation Set</strong> (1,482 pairs): Used to assess the generalization ability of the model, ensuring that predictions are reliable for unseen solvents.</li>\n</ul>\n<p>The dataset includes a variety of solvents, both aqueous and organic, covering a broad range of solubility conditions. The features used include molecular structure representations, Gasteiger charges, and solute-solvent interaction descriptors.</p>\n<p></p><h2>Model Architecture</h2><a href=\"#model-architecture\"></a><p></p>\n<p>The <em>MolMerger</em> model leverages <strong>AttentiveFP</strong>, a graph attention-based neural network optimized for molecular graph processing. The key components include:</p>\n<ul>\n<li><strong>Graph Construction:</strong> Molecules are represented as graphs, where atoms form the nodes, and chemical bonds form the edges.</li>\n<li><strong>Feature Encoding:</strong> Atom-level features (such as atomic number, hybridization, charge) and bond-level features (such as bond type and conjugation) are incorporated.</li>\n<li><strong>Attention Mechanism:</strong> The model applies attention-based message passing to highlight critical interactions within solute-solvent pairs.</li>\n<li><strong>Prediction Layer:</strong> The final embedding is processed through fully connected layers to predict <strong>LogS (log solubility)</strong> values.</li>\n</ul>\n<p></p><h2>Results</h2><a href=\"#results\"></a><p></p>\n<p>The <em>MolMerger</em> model demonstrated <strong>state-of-the-art performance</strong> in predicting solubility across diverse solvent environments. Key findings include:</p>\n<ul>\n<li><strong>Test Set Performance:</strong> The model achieved an <strong>R\u00b2 score of 0.767</strong> and a <strong>Mean Absolute Error (MAE) of 0.78</strong>, showing strong predictive accuracy.</li>\n<li><strong>Robust Evaluation Performance:</strong> Across 65 unseen solvents, the model maintained an <strong>average MAE of 0.79</strong>, with 46 solvents having an MAE below 1.0.</li>\n<li><strong>Comparison with Experimental Data:</strong> Predictions for widely used pharmaceutical compounds, such as <strong>paracetamol, alprazolam, and aspirin</strong>, closely matched experimental solubility values.</li>\n</ul>\n<p></p><h3>Sample Predictions</h3><a href=\"#sample-predictions\"></a><p></p>\n<p></p><h2>Citation</h2><a href=\"#citation\"></a><p></p>\n<p>If you use this repository in your research, please cite:</p>\n<div><pre><code>@article{Ramani2024MolMerger,\n author = {Vansh Ramani and Tarak Karmakar},\n title = {Graph Neural Networks for Predicting Solubility in Diverse Solvents Using MolMerger Incorporating Solute\u2013Solvent Interactions},\n journal = {Journal of Chemical Theory and Computation},\n year = {2024},\n volume = {20},\n number = {15},\n pages = {6549-6558},\n doi = {10.1021/acs.jctc.4c00382}\n}\n</code></pre></div>\n<p></p><h2>License</h2><a href=\"#license\"></a><p></p>\n<p>This repository is for research purposes only. Please refer to the original paper for details on the methodology and usage rights.</p>\n<p></p><h2>Contact</h2><a href=\"#contact\"></a><p></p>\n<p>For any queries, please contact <strong>Vansh Ramani</strong> or <strong>Tarak Karmakar</strong> via their respective institutional affiliations.</p>\n</article></div></div>",
      "url": "https://github.com/vanshramani/molmerger-solubility-prediction"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - lowkc/solv\\_gnn: GNNs for predicting solubility of molecules in organic solvents using PyTorch and DGL\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/lowkc/solv_gnn)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/lowkc/solv_gnn)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=lowkc/solv_gnn)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[lowkc](https://github.com/lowkc)/**[solv\\_gnn](https://github.com/lowkc/solv_gnn)**Public\n* [Notifications](https://github.com/login?return_to=/lowkc/solv_gnn)You must be signed in to change notification settings\n* [Fork3](https://github.com/login?return_to=/lowkc/solv_gnn)\n* [Star14](https://github.com/login?return_to=/lowkc/solv_gnn)\nGNNs for predicting solubility of molecules in organic solvents using PyTorch and DGL\n[14stars](https://github.com/lowkc/solv_gnn/stargazers)[3forks](https://github.com/lowkc/solv_gnn/forks)[Branches](https://github.com/lowkc/solv_gnn/branches)[Tags](https://github.com/lowkc/solv_gnn/tags)[Activity](https://github.com/lowkc/solv_gnn/activity)\n[Star](https://github.com/login?return_to=/lowkc/solv_gnn)\n[Notifications](https://github.com/login?return_to=/lowkc/solv_gnn)You must be signed in to change notification settings\n# lowkc/solv\\_gnn\nmaster\n[Branches](https://github.com/lowkc/solv_gnn/branches)[Tags](https://github.com/lowkc/solv_gnn/tags)\n[](https://github.com/lowkc/solv_gnn/branches)[](https://github.com/lowkc/solv_gnn/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[44 Commits](https://github.com/lowkc/solv_gnn/commits/master/)\n[](https://github.com/lowkc/solv_gnn/commits/master/)\n|\n[gnn](https://github.com/lowkc/solv_gnn/tree/master/gnn)\n|\n[gnn](https://github.com/lowkc/solv_gnn/tree/master/gnn)\n|\n|\n|\n[trained\\_models](https://github.com/lowkc/solv_gnn/tree/master/trained_models)\n|\n[trained\\_models](https://github.com/lowkc/solv_gnn/tree/master/trained_models)\n|\n|\n|\n[.gitignore](https://github.com/lowkc/solv_gnn/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/lowkc/solv_gnn/blob/master/.gitignore)\n|\n|\n|\n[README.md](https://github.com/lowkc/solv_gnn/blob/master/README.md)\n|\n[README.md](https://github.com/lowkc/solv_gnn/blob/master/README.md)\n|\n|\n|\n[example\\_prediction\\_and\\_interaction\\_maps.ipynb](https://github.com/lowkc/solv_gnn/blob/master/example_prediction_and_interaction_maps.ipynb)\n|\n[example\\_prediction\\_and\\_interaction\\_maps.ipynb](https://github.com/lowkc/solv_gnn/blob/master/example_prediction_and_interaction_maps.ipynb)\n|\n|\n|\n[prediction.py](https://github.com/lowkc/solv_gnn/blob/master/prediction.py)\n|\n[prediction.py](https://github.com/lowkc/solv_gnn/blob/master/prediction.py)\n|\n|\n|\n[run\\_optuna.py](https://github.com/lowkc/solv_gnn/blob/master/run_optuna.py)\n|\n[run\\_optuna.py](https://github.com/lowkc/solv_gnn/blob/master/run_optuna.py)\n|\n|\n|\n[train\\_gpu.py](https://github.com/lowkc/solv_gnn/blob/master/train_gpu.py)\n|\n[train\\_gpu.py](https://github.com/lowkc/solv_gnn/blob/master/train_gpu.py)\n|\n|\n|\n[train\\_gpu\\_partialcharge.py](https://github.com/lowkc/solv_gnn/blob/master/train_gpu_partialcharge.py)\n|\n[train\\_gpu\\_partialcharge.py](https://github.com/lowkc/solv_gnn/blob/master/train_gpu_partialcharge.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Graph neural networks for solvation free energy prediction\n[](#graph-neural-networks-for-solvation-free-energy-prediction)\n## Installation\n[](#installation)\nDependencies:`pytorch==1.6.0`,`dgl==0.5.0`,`rdkit=2020.03.5`\n## Train the model\n[](#train-the-model)\nTrain on a GPU using the`run\\_training.py`script. CPU training is also possible, just make sure to modify the code accordingly. Hyperparameter optimisation is available via Optuna in`run\\_optuna.py`.\n### Dataset\n[](#dataset)\nThe CompSolv-Exp dataset was used to train our model. The data (excluding proprietary MNSol data) can be found in the ESI of the Vermeire and Green (2021) Chem. Eng. J. paper:[https://doi.org/10.1016/j.cej.2021.129307](https://doi.org/10.1016/j.cej.2021.129307)\n## Predict using a pre-trained model\n[](#predict-using-a-pre-trained-model)\nCheck out the notebook`example\\_prediction\\_and\\_interaction\\_maps.ipynb`for examples on how to load a trained model and how to visualise the weights of the interaction map.\nTo predict the solvation free energy for a list of solute-solvent pairs, you can load in the file using the code:`python prediction.py --dataset-file='/path/to/prediction/.csv' --model-path=/path/to/saved/model/dir`Example files for DAAQ solubility (as in the paper) are in the*trained\\_models/data*, and a zipped pre-trained model checkpoint can be downloaded in release v0.1.0.\n## About\nGNNs for predicting solubility of molecules in organic solvents using PyTorch and DGL\n### Topics\n[machine-learning](https://github.com/topics/machine-learning)[chemistry](https://github.com/topics/chemistry)\n### Resources\n[Readme](#readme-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/lowkc/solv_gnn/activity)\n### Stars\n[**14**stars](https://github.com/lowkc/solv_gnn/stargazers)\n### Watchers\n[**1**watching](https://github.com/lowkc/solv_gnn/watchers)\n### Forks\n[**3**forks](https://github.com/lowkc/solv_gnn/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/lowkc/solv_gnn&amp;report=lowkc+(user))\n## [Releases1](https://github.com/lowkc/solv_gnn/releases)\n[\nv0.1.0Latest\nFeb 23, 2022\n](https://github.com/lowkc/solv_gnn/releases/tag/v0.1.0)\n## [Packages0](https://github.com/users/lowkc/packages?repo_name=solv_gnn)\nNo packages published\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n## Languages\n* [Jupyter Notebook73.0%](https://github.com/lowkc/solv_gnn/search?l=jupyter-notebook)\n* [Python27.0%](https://github.com/lowkc/solv_gnn/search?l=python)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/lowkc/solv_gnn"
    },
    {
      "title": "CoAtGIN: Marrying Convolution and Attention for Graph-based Molecule Property Prediction",
      "text": "CoAtGIN: Marrying Convolution and Attention for Graph-based Molecule Property Prediction \\| IEEE Conference Publication \\| IEEE Xplore\n### IEEE Account\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n### Purchase Details\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n### Need Help?\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
      "url": "https://ieeexplore.ieee.org/document/9995324"
    },
    {
      "title": "Graph-Based Approaches for Predicting Solvation Energy in Multiple Solvents: Open Datasets and Machine Learning Models",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>The solvation properties of molecules, often estimated using quantum chemical simulations, are important in the synthesis of energy storage materials, drugs, and industrial chemicals. Here, we develop machine learning models of solvation energies to replace expensive quantum chemistry calculations with inexpensive-to-compute message-passing neural network models that require only the molecular graph as inputs. Our models are trained on a new database of solvation energies for 130,258 molecules taken from the QM9 dataset computed in five solvents (acetone, ethanol, acetonitrile, dimethyl sulfoxide, and water) via an implicit solvent model. Our best model achieves a mean absolute error of 0.5 kcal/mol for molecules with nine or fewer non-hydrogen atoms and 1 kcal/mol for molecules with between 10 and 14 non-hydrogen atoms. We make the entire dataset of 651,290 computed entries openly available and provide simple web and programmatic interfaces to enable others to run our solvation energy model on new molecules. This model calculates the solvation energies for molecules using only the SMILES string and also provides an estimate of whether each molecule is within the domain of applicability of our model. We envision that the dataset and models will provide the functionality needed for the rapid screening of large chemical spaces to discover improved molecules for many applications.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/60dcc135379e8d40eff79459"
    },
    {
      "title": "Transfer Learning Graph Representations of Molecules for pKa, 13C-NMR, and Solubility",
      "text": "Transfer Learning Graph Representations of Molecules for pKa, 13C-NMR, and Solubility | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Transfer Learning Graph Representations of Molecules for pKa, 13C-NMR, and Solubility\n01 September 2023, Version 1\nThis is not the most recent version. There is a[\nnewer version\n](https://chemrxiv.org/engage/chemrxiv/article-details/6584fe019138d231613a8a68)of this content available\nWorking Paper\n## Authors\n* [Amer Marwan El Samman](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Amer%20Marwan%20El%20Samman),\n* [Stefano De Castro](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stefano%20De%20Castro),\n* [Brooke Morton](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Brooke%20Morton)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0009-0006-9396-992X),\n* [Stijn De Baerdemacker](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stijn%20De%20Baerdemacker)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0001-7933-3227)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nWe explore transfer learning models from a pre-trained graph convoluntional neural network representation of molecules, obtained from SchNet, 1 to predict 13 C-NMR, pKa, and logS sol- ubility. SchNet learns a graph representation of a molecule by associating each atom with an \u201cembedding vector\u201d and interacts the atom-embeddings with each other by leveraging graph- convolutional filters on their interatomic distances. We pre-trained SchNet on molecular energy and demonstrate that the pre-trained atomistic embeddings can then be used as a transferable representation for a wide array of properties. On the one hand, for atomic properties such as micro-pK1 and 13 C-NMR, we investigate two models, one linear and one neural net, that inputs pre-trained atom-embeddings of a particular atom (e.g. carbon) and predicts a local property (e.g. 13 C-NMR). On the other hand, for molecular properties such as solubility, a size-extensive graph model is built using the embeddings of all atoms in the molecule as input. For all cases, qualitatively correct predictions are made with relatively little training data (&lt; 1000 training points), showcasing the ease with which pre-trained embeddings pick up on important chemical patterns. The proposed models successfully capture well-understood trends of pK1 and solu- bility. This study advances our understanding of current neural net graph representations and their capacity for transfer learning applications in chemistry.\n## Keywords\n[machine learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=machine%20learning)\n[transfer learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=transfer%20learning)\n[pKa](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=pKa)\n[NMR](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=NMR)\n[logS](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=logS)\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nSchNet Model Embedding Vectors of QM9 Atoms Labelled According to Functional Groups Designation\n**Description**\nEmbedding vectors for all atoms in the first 10k molecules in the QM9 dataset, generated by a trained SchNet model Also contains the model which the embedding vectors were extracted from . Model was trained on 100k training points (molecules) and 10k validation points of QM9.\n**Actions**\n[**View**](https://doi.org/10.25545/EK1EQA)\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nTransfer Learning Graph Representations of Molecules for pKa 13C NMR and Solubility.\n**Description**\nJupyter Notebooks with models used for Transfer Learning Graph Representations of Molecules for pKa 13C NMR and Solubility.\n**Actions**\n[**View**](<https://github.com/amerelsamman/Transfer-Learning-Graph-Representations- of-Molecules-for-pKa-13C-NMR-and-Solubility.>)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\n[Dec 22, 2023 Version\n2](https://chemrxiv.org/engage/chemrxiv/article-details/6584fe019138d231613a8a68)\nSep 01, 2023 Version 1\n## Metrics\n1,641\n841\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\nNC\nThe content is available under[CC BY NC 4.0[opens in a new tab]](https://creativecommons.org/licenses/by-nc/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2023-hfcm5\nD O I: 10.26434/chemrxiv-2023-hfcm5 [opens in a new tab]](https://doi.org/10.26434/chemrxiv-2023-hfcm5)\n## Funding\n**Canada Research Chairs**\n**Canada Foundatio...",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/64f167e379853bbd78d62c1e"
    },
    {
      "title": "MolPROP: Molecular Property prediction with multimodal language and graph fusion",
      "text": "MolPROP: Molecular Property prediction with multimodal language and graph fusion | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-024-00846-9?)\n# MolPROP: Molecular Property prediction with multimodal language and graph fusion\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:22 May 2024\n* Volume\u00a016, article\u00a0number56, (2024)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-024-00846-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nMolPROP: Molecular Property prediction with multimodal language and graph fusion\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-024-00846-9.pdf)\n* [Zachary A. Rollins](#auth-Zachary_A_-Rollins-Aff1)[1](#Aff1),\n* [Alan C. Cheng](#auth-Alan_C_-Cheng-Aff1)[1](#Aff1)&amp;\n* [Essam Metwally](#auth-Essam-Metwally-Aff1)[1](#Aff1)\n* 7110Accesses\n* 31Citations\n* 5Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9/metrics)\n### Abstract\nPretrained deep learning models self-supervised on large datasets of language, image, and graph representations are often fine-tuned on downstream tasks and have demonstrated remarkable adaptability in a variety of applications including chatbots, autonomous driving, and protein folding. Additional research aims to improve performance on downstream tasks by fusing high dimensional data representations across multiple modalities. In this work, we explore a novel fusion of a pretrained language model, ChemBERTa-2, with graph neural networks for the task of molecular property prediction. We benchmark the MolPROP suite of models on seven scaffold split MoleculeNet datasets and compare with state-of-the-art architectures. We find that (1) multimodal property prediction for small molecules can match or significantly outperform modern architectures on hydration free energy (FreeSolv), experimental water solubility (ESOL), lipophilicity (Lipo), and clinical toxicity tasks (ClinTox), (2) the MolPROP multimodal fusion is predominantly beneficial on regression tasks, (3) the ChemBERTa-2 masked language model pretraining task (MLM) outperformed multitask regression pretraining task (MTR) when fused with graph neural networks for multimodal property prediction, and (4) despite improvements from multimodal fusion on regression tasks MolPROP significantly underperforms on some classification tasks. MolPROP has been made available at[https://github.com/merck/MolPROP](https://github.com/merck/MolPROP).\n### Scientific contribution\nThis work explores a novel multimodal fusion of learned language and graph representations of small molecules for the supervised task of molecular property prediction. The MolPROP suite of models demonstrates that language and graph fusion can significantly outperform modern architectures on several regression prediction tasks and also provides the opportunity to explore alternative fusion strategies on classification tasks for multimodal molecular property prediction.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-96-8173-0?as&#x3D;webp)\n### [Deep Interactions for\u00a0Multimodal Molecular Property Prediction](https://link.springer.com/10.1007/978-981-96-8173-0_26?fromPaywallRec=false)\nChapter\u00a9 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs11030-025-11350-z/MediaObjects/11030_2025_11350_Fig1_HTML.png)\n### [TMolNet: a task-aware multimodal neural network for molecular property prediction](https://link.springer.com/10.1007/s11030-025-11350-z?fromPaywallRec=false)\nArticle21 September 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-024-01155-w/MediaObjects/42004_2024_1155_Fig1_HTML.png)\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://link.springer.com/10.1038/s42004-024-01155-w?fromPaywallRec=false)\nArticleOpen access05 April 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Cheminformatics](https://jcheminf.biomedcentral.com/subjects/cheminformatics)\n* [Computational Linguistics](https://jcheminf.biomedcentral.com/subjects/computational-linguistics)\n* [Language Processing](https://jcheminf.biomedcentral.com/subjects/language-processing)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Predictive markers](https://jcheminf.biomedcentral.com/subjects/predictive-markers)\n* [Structure Prediction](https://jcheminf.biomedcentral.com/subjects/structure-prediction)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nLearned molecular representations have undergone rapid evolution in recent years exploring a variety of encoding mechanisms including string line annotations (e.g., SMILES [[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR1)], SMARTS [[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR2)], or SELFIES [[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR3)]) and graph representations [[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR4)]. These representations are commonly pretrained in a self-supervised fashion and/or supervised on downstream tasks such as molecular property prediction. While line annotations such as SMILES (Simplified Molecular-Input Line Entry System) [[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR1)] strings are compact, have a well-defined grammar, and contain large accessible datasets for self-supervised pretraining, language models do not explicitly encode physical information about molecular topology. This has prompted efforts to represent molecules as graphs to explicitly capture connectivity information. Indeed, significant progress has been demonstrated in terms of sample efficiency and generalizability to new molecules by explicitly representing molecules as graph neural networks (GNNs) where local information is aggregated and passed (i.e., message passing) across the graph structure according to its connectivity [[5](#ref-CR5),[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR11)]. Additional work has also explored incorporating 3D information such as bond distances [[12](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR12),[13](https://jcheminf.biomedcentral.com/article/10.1186/s13321-024-00846-9#ref-CR13)] or pretraining GNNs on various tasks such as atom masking or subgraph removal [[14](https://jcheminf.biomedcentral...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00846-9"
    }
  ]
}